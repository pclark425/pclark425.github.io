<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9533 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9533</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9533</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-cc8ffedfdea6fb92fe839d0799ffe6f82391305b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cc8ffedfdea6fb92fe839d0799ffe6f82391305b" target="_blank">A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models</a></p>
                <p><strong>Paper Venue:</strong> Scientific Reports</p>
                <p><strong>Paper TL;DR:</strong> An article-level key-insight extraction system based on Large Language Models, calling it ArticleLLM, which demonstrates not only the feasibility of LLMs in key-insight extraction, but also the effectiveness of cooperation of multiple fine-tuned LLMs, leading to efficient academic literature survey and knowledge discovery.</p>
                <p><strong>Paper Abstract:</strong> The exponential growth of scientific articles has presented challenges in information organization and extraction. Automation is urgently needed to streamline literature reviews and enhance insight extraction. We explore the potential of Large Language Models (LLMs) in key-insights extraction from scientific articles, including OpenAI’s GPT-4.0, MistralAI’s Mixtral 8 × 7B, 01AI’s Yi, and InternLM’s InternLM2. We have developed an article-level key-insight extraction system based on LLMs, calling it ArticleLLM. After evaluating the LLMs against manual benchmarks, we have enhanced their performance through fine-tuning. We propose a multi-actor LLM approach, merging the strengths of multiple fine-tuned LLMs to improve overall key-insight extraction performance. This work demonstrates not only the feasibility of LLMs in key-insight extraction, but also the effectiveness of cooperation of multiple fine-tuned LLMs, leading to efficient academic literature survey and knowledge discovery.</p>
                <p><strong>Cost:</strong> 0.04</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9533.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9533.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ArticleLLM (multi-actor)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ArticleLLM: multi-actor key-insight extraction system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An article-level key-insight extraction system that uses multiple fine-tuned open-source LLMs (Mixtral FT, Yi FT, InternLM2 FT) whose independent outputs are synthesized by a dedicated actor (InternLM2) to produce structured, article-level key-insights (aim, motivation, problem, method, evaluation metrics, findings, contributions, limitations, future work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Multi-actor ensemble: InternLM2 FT (synthesizer) + Mixtral FT + Yi FT (extractors); GPT-4 used as label/judge</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>An ensemble of fine-tuned open-source transformer LLMs: Mixtral 8x7B (Mistral family, Hugging Face compatible), Yi (01AI), InternLM2 (InternLM family). All were fine-tuned with instruction-response pairs using LoRA adapters (PEFT) and 4-bit GPTQ quantized loading; InternLM2 FT was further fine-tuned as the synthesizer that merges outputs. GPT-4 (proprietary) was used to generate reference labels and as an evaluator, not fine-tuned in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scholarly literature analysis (healthcare arXiv subset; general scientific articles)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Training / fine-tuning dataset: 1,000 arXiv articles filtered by keyword 'healthcare' and length < 7000 words (700 used for training, 300 for test). Preprocessing: PDF→text via tika-python, sentence boundary disambiguation with pySBD; instruction–response pairs created where GPT-4 outputs served as supervised labels.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>thematic/generalizable research insights (structured key-insights synthesis across articles)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Not formal scientific 'laws' but generalizable synthesis patterns such as: multi-actor fusion increases coverage and relevance of article-level key-insights (aim, methods, findings) compared to individual fine-tuned LLMs; extraction of 'limitations' is systematically more difficult and under-recovered by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Supervised instruction fine-tuning (Instruction-Response pairs) using LoRA (PEFT) with 4-bit GPTQ quantized model loading; independent extraction by three fine-tuned LLMs followed by a synthesizer LLM (InternLM2 FT) to merge outputs (multi-actor pipeline). GPT-4 outputs were used as responses (labels) for supervised fine-tuning and as an automated semantic judge.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Three evaluation channels: (1) Manual evaluation by two human researchers assigning relevance scores [0–1] on a 34-article subset; (2) GPT-4 semantic similarity scoring [0–100] using prompt-based evaluation; (3) Vector (cosine) similarity of Sentence-Transformer embeddings (all-MiniLM-L6-v2) normalized to [0–100]; statistical significance tested with Wilcoxon Signed-Rank Test.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Multi-actor ArticleLLM outperformed individual fine-tuned open-source models across key-insight categories: the multi-actor ensemble achieved the highest GPT-4 semantic-similarity and vector-similarity metrics and showed statistical significance across most categories (Wilcoxon test p<0.05). InternLM2 FT was the best single open-source model (GPT-4 score after fine-tuning: 77.8 average). GPT-4 (proprietary) scored near human level (manual avg 0.97) but is costly; multi-actor provides a locally deployable, cost-effective alternative. Limitations include weaker extraction for 'evaluation metrics' and 'limitations' fields and runtime inefficiency (~10 min per article on the reported hardware).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Baselines: (a) pre-fine-tuned open-source models (Mixtral, Yi, InternLM2) and (b) GPT-4 and manual human annotation. GPT-4 outperformed open-source models before fine-tuning (manual avg 0.97). Fine-tuning improved open-source models (InternLM2 FT best), and the multi-actor fusion surpassed single fine-tuned models, narrowing the gap with GPT-4. No comparison to classical meta-analysis methods was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Dataset limited to 1,000 arXiv healthcare articles and articles under 7,000 words (affects generalizability); hardware constraints required 4-bit quantization (GPTQ) which may reduce performance; multi-actor ensemble increases computational cost and latency (~10 minutes per article on tested setup); difficulty extracting implicit/noisy elements like 'limitations' when not explicitly stated in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Authors note that open-source LLMs sometimes synthesize plausible but not strictly accurate 'limitations' (i.e., speculative or hallucinated content). They also note variability in GPT-4 scoring across runs and potential sensitivity of semantic-evaluator judgments; no systematic bias audit was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9533.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9533.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (label/judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4 (Generative Pre-trained Transformer 4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art proprietary large language model used in this work as a source of supervised labels (to generate expected key-insight responses) and as an automated semantic judge for scoring open-source LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Proprietary transformer-based LLM (multibillion-parameter class; exact size not reported in paper). Used in zero-shot/instruction mode to produce reference key-insight outputs for supervised fine-tuning of open-source models and to score semantic similarity between generated and reference key-insights via prompt-based evaluation (GPT-4 score 0–100).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scholarly literature analysis (used cross-domain as label/judge for healthcare arXiv articles)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Used to generate labels for 1,000 article instances (a subset used to fine-tune open-source LLMs) and to evaluate 300 test articles: GPT-4 consumed article text + structured instruction prompts to produce reference key-insights.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>reference-standard synthesis outputs (used as surrogate human labels for key-insight content)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>GPT-4 produced article-level structured key-insights that served as labels; the system considered GPT-4 outputs as near-human gold standard (manual avg 0.97). Example role: produce concise 'limitations' or 'future work' descriptions from an article.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Prompted generation: carefully designed instructions (table of instructions in paper) fed entire article or preprocessed article text to GPT-4 to elicit structured key-insights. Outputs were used as labels for supervised fine-tuning of open-source models and as an evaluator to score semantic similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Used as the semantic judge: prompts (Table 5 in paper) instruct GPT-4 to score similarity [0–100] between an open-source model output and the GPT-4 reference; also, human manual evaluation compared GPT-4 outputs to human annotations (GPT-4 close to human). Repeated GPT-4 scoring (30 replicates) to quantify variability (mean 88.4 ± 0.63).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 produced high-quality reference key-insights, approximating human annotations (manual average score 0.97). The authors leveraged GPT-4 labels to bootstrap supervised fine-tuning for open-source LLMs, enabling good downstream results without large-scale human annotation. They observed small but measurable variability across repeated GPT-4 scoring runs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>GPT-4 served both as an upper baseline (near-human) and as label source; it outperformed pre-fine-tuned open-source LLMs and set the target for fine-tuning; however, cost makes it impractical at scale compared to locally deployed ArticleLLM.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Cost and operational expense for large-scale processing (~$0.15 per ~10k-word article cited); score variability across repeated runs; proprietary constraints (not locally deployable) and dependence on GPT-4 may introduce annotation biases when used as the sole label source.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Paper acknowledges potential variability and that GPT-4's outputs were treated as labels (which could propagate GPT-4-specific biases into fine-tuned models); no detailed bias audit but authors note variability and sensitivity of GPT-4 judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9533.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9533.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternLM2 FT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternLM2 fine-tuned (InternLM2 FT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source LLM (InternLM2) fine-tuned with instruction-response pairs (LoRA, GPTQ 4-bit loading) and used both as an extractor and as the central synthesizer in the multi-actor pipeline; it achieved the highest GPT-4 scoring among fine-tuned open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>InternLM2 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>InternLM2 base model (HuggingFace-compatible transformer) adapted with PEFT LoRA parameters and quantized via GPTQ to 4-bit for efficient local fine-tuning and inference; further fine-tuned both as an extractor (InternLM2 FT) and as the integrating/summarizing actor.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scholarly literature analysis (healthcare arXiv subset)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Fine-tuned on 700 training articles (out of 1,000) with instruction-response pairs where GPT-4 outputs served as responses; additional 1,000 entities used to fine-tune InternLM2 for synthesis from multiple LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>synthesized article-level key-insights (thematic synthesis capable of merging multiple extractor outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>InternLM2 FT synthesis yields more semantically-aligned key-insights (GPT-4 score avg 77.8) and, when used to merge outputs from multiple extractors, reduces omission of critical elements compared to single extractors.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Instruction fine-tuning (supervised) with LoRA adapters; used InternLM2 FT as both an independent extractor and the final synthesizer that summarizes/merges the outputs of Mixtral FT, Yi FT, and itself into a unified key-insight set.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluated by manual relevance scoring (humans), GPT-4 semantic scoring, and vector-similarity (Sentence-Transformer). Statistical tests (Wilcoxon) showed InternLM2 FT significant improvements across many key-insight categories.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>InternLM2 FT was the top-performing single open-source model after fine-tuning (GPT-4 score 77.8, vector similarity highest among single models). As the synthesizer in the multi-actor system, it produced the best fused outputs, contributing to the multi-actor approach's superior performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Outperformed Mixtral FT and Yi FT after fine-tuning; approached GPT-4 performance for many categories but remained below GPT-4 in manual evaluation. Multi-actor fusion including InternLM2 FT outperformed single InternLM2 FT.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Performance gains may be limited by 4-bit quantization and dataset domain (healthcare arXiv only); resource constraints influenced model choices and input length caps (<7000 words).</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Similar risks as other LLMs: potential to synthesize plausible but inaccurate content when article elements are not explicit; no in-depth bias analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9533.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9533.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral FT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral 8×7B fine-tuned (Mixtral FT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MistralAI's Mixtral model family (an open-source transformer) fine-tuned via instruction-response pairs with LoRA and GPTQ 4-bit quantization; used as one extractor actor in the ArticleLLM ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Mixtral 8×7B (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Mixtral (Mistral-derived) model compatible with Hugging Face; fine-tuned with LoRA adapters and quantized to 4-bit for memory-efficient local fine-tuning; used for article-level extraction following the same instruction templates as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scholarly literature analysis (healthcare arXiv subset)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Fine-tuned on the same instruction–response dataset (GPT-4 generated labels) from ~700 training articles drawn from 1,000 arXiv healthcare articles (length filter <7000 words).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>article-level thematic extraction (key-insights per article)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Post-fine-tuning Mixtral FT showed marginal improvements (GPT-4 score increased from ~49.4 to ~51.6) but lagged behind InternLM2 FT, and contributed complementary perspectives in the multi-actor fusion.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Instruction fine-tuning using LoRA; independent inference on article text to extract structured key-insights; outputs combined later in synthesizer actor.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same evaluation pipeline: manual annotation, GPT-4 scoring, vector embedding similarity; Wilcoxon tests applied to compare model differences.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mixtral FT produced modest improvements after fine-tuning (small increase in GPT-4 score) and contributed complementary content to the multi-actor fusion, but was the weakest of the three fine-tuned open-source models in absolute performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Underperformed InternLM2 FT and GPT-4; small gains from fine-tuning observed but lower absolute semantic-similarity scores.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Lower baseline capability relative to InternLM2 and GPT-4; fine-tuning and 4-bit quantization may have limited model capacity in this task.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>No model-specific audit reported; general observation that open-source LLMs can synthesize plausible but incorrect items (noted especially for 'limitations').</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9533.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9533.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yi FT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yi fine-tuned (Yi FT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>01AI's Yi model fine-tuned through instruction–response supervised learning (LoRA + GPTQ 4-bit) and used as one of the extractor actors in the multi-actor ArticleLLM pipeline; showed modest improvement after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>Yi (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Yi (01AI) transformer model adapted with LoRA adapters and 4-bit GPTQ quantization for efficient local fine-tuning; served as an extractor in the multi-actor system.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scholarly literature analysis (healthcare arXiv subset)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Trained/fine-tuned on instruction–response pairs derived from GPT-4 outputs across the curated 1,000 arXiv healthcare articles (700 for training, 300 test).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>article-level key-insight extraction (thematic patterns per article)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Yi FT improved slightly after fine-tuning (GPT-4 score from ~68.5 to ~71.4) and contributed extractive sentences for the multi-actor synthesizer to merge.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Instruction fine-tuning (LoRA) with GPT-4-generated responses as labels; independent extraction followed by synthesizer merging.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Manual human relevance scoring, GPT-4 semantic scoring, vector similarity; Wilcoxon tests to assess differences.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Yi FT showed noticeable but modest gains post-fine-tuning and contributed to improved multi-actor performance; it had significance in certain key-insight categories per Wilcoxon tests (e.g., aim, question addressed, findings).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Improved from pre-fine-tune baseline; still below InternLM2 FT and overall multi-actor fusion performance; multi-actor ensemble outperformed each single Yi FT.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Marginal single-model gains; same dataset/hardware/quantization constraints; occasional synthesis of non-explicit 'limitations'.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>No dedicated bias study; paper remarks open-source LLMs can invent plausible limitations when not explicitly present in text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction <em>(Rating: 2)</em></li>
                <li>LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion <em>(Rating: 2)</em></li>
                <li>Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models <em>(Rating: 2)</em></li>
                <li>MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework <em>(Rating: 2)</em></li>
                <li>Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate <em>(Rating: 2)</em></li>
                <li>Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering <em>(Rating: 1)</em></li>
                <li>SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9533",
    "paper_id": "paper-cc8ffedfdea6fb92fe839d0799ffe6f82391305b",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [
        {
            "name_short": "ArticleLLM (multi-actor)",
            "name_full": "ArticleLLM: multi-actor key-insight extraction system",
            "brief_description": "An article-level key-insight extraction system that uses multiple fine-tuned open-source LLMs (Mixtral FT, Yi FT, InternLM2 FT) whose independent outputs are synthesized by a dedicated actor (InternLM2) to produce structured, article-level key-insights (aim, motivation, problem, method, evaluation metrics, findings, contributions, limitations, future work).",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "Multi-actor ensemble: InternLM2 FT (synthesizer) + Mixtral FT + Yi FT (extractors); GPT-4 used as label/judge",
            "llm_model_description": "An ensemble of fine-tuned open-source transformer LLMs: Mixtral 8x7B (Mistral family, Hugging Face compatible), Yi (01AI), InternLM2 (InternLM family). All were fine-tuned with instruction-response pairs using LoRA adapters (PEFT) and 4-bit GPTQ quantized loading; InternLM2 FT was further fine-tuned as the synthesizer that merges outputs. GPT-4 (proprietary) was used to generate reference labels and as an evaluator, not fine-tuned in this study.",
            "application_domain": "Scholarly literature analysis (healthcare arXiv subset; general scientific articles)",
            "input_corpus_description": "Training / fine-tuning dataset: 1,000 arXiv articles filtered by keyword 'healthcare' and length &lt; 7000 words (700 used for training, 300 for test). Preprocessing: PDF→text via tika-python, sentence boundary disambiguation with pySBD; instruction–response pairs created where GPT-4 outputs served as supervised labels.",
            "qualitative_law_type": "thematic/generalizable research insights (structured key-insights synthesis across articles)",
            "qualitative_law_example": "Not formal scientific 'laws' but generalizable synthesis patterns such as: multi-actor fusion increases coverage and relevance of article-level key-insights (aim, methods, findings) compared to individual fine-tuned LLMs; extraction of 'limitations' is systematically more difficult and under-recovered by LLMs.",
            "extraction_methodology": "Supervised instruction fine-tuning (Instruction-Response pairs) using LoRA (PEFT) with 4-bit GPTQ quantized model loading; independent extraction by three fine-tuned LLMs followed by a synthesizer LLM (InternLM2 FT) to merge outputs (multi-actor pipeline). GPT-4 outputs were used as responses (labels) for supervised fine-tuning and as an automated semantic judge.",
            "evaluation_method": "Three evaluation channels: (1) Manual evaluation by two human researchers assigning relevance scores [0–1] on a 34-article subset; (2) GPT-4 semantic similarity scoring [0–100] using prompt-based evaluation; (3) Vector (cosine) similarity of Sentence-Transformer embeddings (all-MiniLM-L6-v2) normalized to [0–100]; statistical significance tested with Wilcoxon Signed-Rank Test.",
            "results_summary": "Multi-actor ArticleLLM outperformed individual fine-tuned open-source models across key-insight categories: the multi-actor ensemble achieved the highest GPT-4 semantic-similarity and vector-similarity metrics and showed statistical significance across most categories (Wilcoxon test p&lt;0.05). InternLM2 FT was the best single open-source model (GPT-4 score after fine-tuning: 77.8 average). GPT-4 (proprietary) scored near human level (manual avg 0.97) but is costly; multi-actor provides a locally deployable, cost-effective alternative. Limitations include weaker extraction for 'evaluation metrics' and 'limitations' fields and runtime inefficiency (~10 min per article on the reported hardware).",
            "comparison_to_baseline": "Baselines: (a) pre-fine-tuned open-source models (Mixtral, Yi, InternLM2) and (b) GPT-4 and manual human annotation. GPT-4 outperformed open-source models before fine-tuning (manual avg 0.97). Fine-tuning improved open-source models (InternLM2 FT best), and the multi-actor fusion surpassed single fine-tuned models, narrowing the gap with GPT-4. No comparison to classical meta-analysis methods was performed.",
            "reported_limitations": "Dataset limited to 1,000 arXiv healthcare articles and articles under 7,000 words (affects generalizability); hardware constraints required 4-bit quantization (GPTQ) which may reduce performance; multi-actor ensemble increases computational cost and latency (~10 minutes per article on tested setup); difficulty extracting implicit/noisy elements like 'limitations' when not explicitly stated in the text.",
            "bias_or_hallucination_issues": "Authors note that open-source LLMs sometimes synthesize plausible but not strictly accurate 'limitations' (i.e., speculative or hallucinated content). They also note variability in GPT-4 scoring across runs and potential sensitivity of semantic-evaluator judgments; no systematic bias audit was reported.",
            "uuid": "e9533.0",
            "source_info": {
                "paper_title": "A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "GPT-4 (label/judge)",
            "name_full": "OpenAI GPT-4 (Generative Pre-trained Transformer 4)",
            "brief_description": "A state-of-the-art proprietary large language model used in this work as a source of supervised labels (to generate expected key-insight responses) and as an automated semantic judge for scoring open-source LLM outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4",
            "llm_model_description": "Proprietary transformer-based LLM (multibillion-parameter class; exact size not reported in paper). Used in zero-shot/instruction mode to produce reference key-insight outputs for supervised fine-tuning of open-source models and to score semantic similarity between generated and reference key-insights via prompt-based evaluation (GPT-4 score 0–100).",
            "application_domain": "Scholarly literature analysis (used cross-domain as label/judge for healthcare arXiv articles)",
            "input_corpus_description": "Used to generate labels for 1,000 article instances (a subset used to fine-tune open-source LLMs) and to evaluate 300 test articles: GPT-4 consumed article text + structured instruction prompts to produce reference key-insights.",
            "qualitative_law_type": "reference-standard synthesis outputs (used as surrogate human labels for key-insight content)",
            "qualitative_law_example": "GPT-4 produced article-level structured key-insights that served as labels; the system considered GPT-4 outputs as near-human gold standard (manual avg 0.97). Example role: produce concise 'limitations' or 'future work' descriptions from an article.",
            "extraction_methodology": "Prompted generation: carefully designed instructions (table of instructions in paper) fed entire article or preprocessed article text to GPT-4 to elicit structured key-insights. Outputs were used as labels for supervised fine-tuning of open-source models and as an evaluator to score semantic similarity.",
            "evaluation_method": "Used as the semantic judge: prompts (Table 5 in paper) instruct GPT-4 to score similarity [0–100] between an open-source model output and the GPT-4 reference; also, human manual evaluation compared GPT-4 outputs to human annotations (GPT-4 close to human). Repeated GPT-4 scoring (30 replicates) to quantify variability (mean 88.4 ± 0.63).",
            "results_summary": "GPT-4 produced high-quality reference key-insights, approximating human annotations (manual average score 0.97). The authors leveraged GPT-4 labels to bootstrap supervised fine-tuning for open-source LLMs, enabling good downstream results without large-scale human annotation. They observed small but measurable variability across repeated GPT-4 scoring runs.",
            "comparison_to_baseline": "GPT-4 served both as an upper baseline (near-human) and as label source; it outperformed pre-fine-tuned open-source LLMs and set the target for fine-tuning; however, cost makes it impractical at scale compared to locally deployed ArticleLLM.",
            "reported_limitations": "Cost and operational expense for large-scale processing (~$0.15 per ~10k-word article cited); score variability across repeated runs; proprietary constraints (not locally deployable) and dependence on GPT-4 may introduce annotation biases when used as the sole label source.",
            "bias_or_hallucination_issues": "Paper acknowledges potential variability and that GPT-4's outputs were treated as labels (which could propagate GPT-4-specific biases into fine-tuned models); no detailed bias audit but authors note variability and sensitivity of GPT-4 judgments.",
            "uuid": "e9533.1",
            "source_info": {
                "paper_title": "A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "InternLM2 FT",
            "name_full": "InternLM2 fine-tuned (InternLM2 FT)",
            "brief_description": "An open-source LLM (InternLM2) fine-tuned with instruction-response pairs (LoRA, GPTQ 4-bit loading) and used both as an extractor and as the central synthesizer in the multi-actor pipeline; it achieved the highest GPT-4 scoring among fine-tuned open-source models.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "InternLM2 (fine-tuned)",
            "llm_model_description": "InternLM2 base model (HuggingFace-compatible transformer) adapted with PEFT LoRA parameters and quantized via GPTQ to 4-bit for efficient local fine-tuning and inference; further fine-tuned both as an extractor (InternLM2 FT) and as the integrating/summarizing actor.",
            "application_domain": "Scholarly literature analysis (healthcare arXiv subset)",
            "input_corpus_description": "Fine-tuned on 700 training articles (out of 1,000) with instruction-response pairs where GPT-4 outputs served as responses; additional 1,000 entities used to fine-tune InternLM2 for synthesis from multiple LLM outputs.",
            "qualitative_law_type": "synthesized article-level key-insights (thematic synthesis capable of merging multiple extractor outputs)",
            "qualitative_law_example": "InternLM2 FT synthesis yields more semantically-aligned key-insights (GPT-4 score avg 77.8) and, when used to merge outputs from multiple extractors, reduces omission of critical elements compared to single extractors.",
            "extraction_methodology": "Instruction fine-tuning (supervised) with LoRA adapters; used InternLM2 FT as both an independent extractor and the final synthesizer that summarizes/merges the outputs of Mixtral FT, Yi FT, and itself into a unified key-insight set.",
            "evaluation_method": "Evaluated by manual relevance scoring (humans), GPT-4 semantic scoring, and vector-similarity (Sentence-Transformer). Statistical tests (Wilcoxon) showed InternLM2 FT significant improvements across many key-insight categories.",
            "results_summary": "InternLM2 FT was the top-performing single open-source model after fine-tuning (GPT-4 score 77.8, vector similarity highest among single models). As the synthesizer in the multi-actor system, it produced the best fused outputs, contributing to the multi-actor approach's superior performance.",
            "comparison_to_baseline": "Outperformed Mixtral FT and Yi FT after fine-tuning; approached GPT-4 performance for many categories but remained below GPT-4 in manual evaluation. Multi-actor fusion including InternLM2 FT outperformed single InternLM2 FT.",
            "reported_limitations": "Performance gains may be limited by 4-bit quantization and dataset domain (healthcare arXiv only); resource constraints influenced model choices and input length caps (&lt;7000 words).",
            "bias_or_hallucination_issues": "Similar risks as other LLMs: potential to synthesize plausible but inaccurate content when article elements are not explicit; no in-depth bias analysis reported.",
            "uuid": "e9533.2",
            "source_info": {
                "paper_title": "A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Mixtral FT",
            "name_full": "Mixtral 8×7B fine-tuned (Mixtral FT)",
            "brief_description": "MistralAI's Mixtral model family (an open-source transformer) fine-tuned via instruction-response pairs with LoRA and GPTQ 4-bit quantization; used as one extractor actor in the ArticleLLM ensemble.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "Mixtral 8×7B (fine-tuned)",
            "llm_model_description": "Mixtral (Mistral-derived) model compatible with Hugging Face; fine-tuned with LoRA adapters and quantized to 4-bit for memory-efficient local fine-tuning; used for article-level extraction following the same instruction templates as other models.",
            "application_domain": "Scholarly literature analysis (healthcare arXiv subset)",
            "input_corpus_description": "Fine-tuned on the same instruction–response dataset (GPT-4 generated labels) from ~700 training articles drawn from 1,000 arXiv healthcare articles (length filter &lt;7000 words).",
            "qualitative_law_type": "article-level thematic extraction (key-insights per article)",
            "qualitative_law_example": "Post-fine-tuning Mixtral FT showed marginal improvements (GPT-4 score increased from ~49.4 to ~51.6) but lagged behind InternLM2 FT, and contributed complementary perspectives in the multi-actor fusion.",
            "extraction_methodology": "Instruction fine-tuning using LoRA; independent inference on article text to extract structured key-insights; outputs combined later in synthesizer actor.",
            "evaluation_method": "Same evaluation pipeline: manual annotation, GPT-4 scoring, vector embedding similarity; Wilcoxon tests applied to compare model differences.",
            "results_summary": "Mixtral FT produced modest improvements after fine-tuning (small increase in GPT-4 score) and contributed complementary content to the multi-actor fusion, but was the weakest of the three fine-tuned open-source models in absolute performance.",
            "comparison_to_baseline": "Underperformed InternLM2 FT and GPT-4; small gains from fine-tuning observed but lower absolute semantic-similarity scores.",
            "reported_limitations": "Lower baseline capability relative to InternLM2 and GPT-4; fine-tuning and 4-bit quantization may have limited model capacity in this task.",
            "bias_or_hallucination_issues": "No model-specific audit reported; general observation that open-source LLMs can synthesize plausible but incorrect items (noted especially for 'limitations').",
            "uuid": "e9533.3",
            "source_info": {
                "paper_title": "A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Yi FT",
            "name_full": "Yi fine-tuned (Yi FT)",
            "brief_description": "01AI's Yi model fine-tuned through instruction–response supervised learning (LoRA + GPTQ 4-bit) and used as one of the extractor actors in the multi-actor ArticleLLM pipeline; showed modest improvement after fine-tuning.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_model_name": "Yi (fine-tuned)",
            "llm_model_description": "Yi (01AI) transformer model adapted with LoRA adapters and 4-bit GPTQ quantization for efficient local fine-tuning; served as an extractor in the multi-actor system.",
            "application_domain": "Scholarly literature analysis (healthcare arXiv subset)",
            "input_corpus_description": "Trained/fine-tuned on instruction–response pairs derived from GPT-4 outputs across the curated 1,000 arXiv healthcare articles (700 for training, 300 test).",
            "qualitative_law_type": "article-level key-insight extraction (thematic patterns per article)",
            "qualitative_law_example": "Yi FT improved slightly after fine-tuning (GPT-4 score from ~68.5 to ~71.4) and contributed extractive sentences for the multi-actor synthesizer to merge.",
            "extraction_methodology": "Instruction fine-tuning (LoRA) with GPT-4-generated responses as labels; independent extraction followed by synthesizer merging.",
            "evaluation_method": "Manual human relevance scoring, GPT-4 semantic scoring, vector similarity; Wilcoxon tests to assess differences.",
            "results_summary": "Yi FT showed noticeable but modest gains post-fine-tuning and contributed to improved multi-actor performance; it had significance in certain key-insight categories per Wilcoxon tests (e.g., aim, question addressed, findings).",
            "comparison_to_baseline": "Improved from pre-fine-tune baseline; still below InternLM2 FT and overall multi-actor fusion performance; multi-actor ensemble outperformed each single Yi FT.",
            "reported_limitations": "Marginal single-model gains; same dataset/hardware/quantization constraints; occasional synthesis of non-explicit 'limitations'.",
            "bias_or_hallucination_issues": "No dedicated bias study; paper remarks open-source LLMs can invent plausible limitations when not explicitly present in text.",
            "uuid": "e9533.4",
            "source_info": {
                "paper_title": "A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction",
            "rating": 2,
            "sanitized_title": "llmensemble_optimal_large_language_model_ensemble_method_for_ecommerce_product_attribute_value_extraction"
        },
        {
            "paper_title": "LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion",
            "rating": 2,
            "sanitized_title": "llmblender_ensembling_large_language_models_with_pairwise_ranking_and_generative_fusion"
        },
        {
            "paper_title": "Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models",
            "rating": 2,
            "sanitized_title": "routing_to_the_expert_efficient_rewardguided_ensemble_of_large_language_models"
        },
        {
            "paper_title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
            "rating": 2,
            "sanitized_title": "metagpt_meta_programming_for_a_multiagent_collaborative_framework"
        },
        {
            "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
            "rating": 2,
            "sanitized_title": "encouraging_divergent_thinking_in_large_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering",
            "rating": 1,
            "sanitized_title": "extracting_accurate_materials_data_from_research_papers_with_conversational_language_models_and_prompt_engineering"
        },
        {
            "paper_title": "SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis",
            "rating": 2,
            "sanitized_title": "sciassess_benchmarking_llm_proficiency_in_scientific_literature_analysis"
        }
    ],
    "cost": 0.03998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><!DOCTYPE html>
<html lang="en" class="grade-c">
<head>
    <title>A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models | Scientific Reports</title>


<link rel="alternate" type="application/rss+xml" href="https://www.nature.com/srep.rss"/>




<link rel="preconnect" href="https://cmp.nature.com" crossorigin>

<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="applicable-device" content="pc,mobile">
<meta name="viewport" content="width=device-width,initial-scale=1.0,maximum-scale=5,user-scalable=yes">
<meta name="360-site-verification" content="5a2dc4ab3fcb9b0393241ffbbb490480" />

<script data-test="dataLayer">
    window.dataLayer = [{"content":{"category":{"contentType":"article","legacy":{"webtrendsPrimaryArticleType":"research","webtrendsSubjectTerms":"computer-science;scientific-data","webtrendsContentCategory":null,"webtrendsContentCollection":null,"webtrendsContentGroup":"Scientific Reports","webtrendsContentGroupType":null,"webtrendsContentSubGroup":"Article","status":null}},"article":{"doi":"10.1038/s41598-025-85715-7"},"attributes":{"cms":null,"deliveryPlatform":"oscar","copyright":{"open":true,"legacy":{"webtrendsLicenceType":"http://creativecommons.org/licenses/by-nc-nd/4.0/"}}},"contentInfo":{"authors":["Zihan Song","Gyo-Yeob Hwang","Xin Zhang","Shan Huang","Byung-Kwon Park"],"publishedAt":1736467200,"publishedAtString":"2025-01-10","title":"A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models","legacy":null,"publishedAtTime":null,"documentType":"aplusplus","subjects":"Computer science,Scientific data"},"journal":{"pcode":"srep","title":"scientific reports","volume":"15","issue":"1","id":41598,"publishingModel":"Open Access"},"authorization":{"status":true},"features":[{"name":"furtherReadingSection","present":true}],"collection":null},"page":{"category":{"pageType":"article"},"attributes":{"template":"mosaic","featureFlags":[{"name":"download-collection-test","active":false},{"name":"nature-onwards-journey","active":false}],"testGroup":null},"search":null},"privacy":{},"version":"1.0.0","product":null,"session":null,"user":null,"backHalfContent":true,"country":"NO","hasBody":true,"uneditedManuscript":false,"twitterId":["o3xnx","o43y9","o3ef7"],"baiduId":"d38bce82bcb44717ccc29a90c4b781ea","japan":false}];
    window.dataLayer.push({
        ga4MeasurementId: 'G-ERRNTNZ807',
        ga360TrackingId: 'UA-71668177-1',
        twitterId: ['3xnx', 'o43y9', 'o3ef7'],
        baiduId: 'd38bce82bcb44717ccc29a90c4b781ea',
        ga4ServerUrl: 'https://collect.nature.com',
        imprint: 'nature'
    });
</script>

<script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;


        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>









    <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) {  .c-card--major .c-card__title,.u-h1,.u-h2,h1,h2,h2.app-access-wall__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-weight:700}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h3,h4,h5,h6{letter-spacing:-.0117156rem}html{line-height:1.15;text-size-adjust:100%;box-sizing:border-box;font-size:100%;height:100%;overflow-y:scroll}body{background:#eee;color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;line-height:1.76;margin:0;min-height:100%}details,main{display:block}h1{font-size:2em;margin:.67em 0}a,sup{vertical-align:baseline}a{background-color:transparent;color:#069;overflow-wrap:break-word;text-decoration:underline;text-decoration-skip-ink:auto;word-break:break-word}b{font-weight:bolder}sup{font-size:75%;line-height:0;position:relative;top:-.5em}img{border:0;height:auto;max-width:100%;vertical-align:middle}button,input,select{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=submit],button{-webkit-appearance:button}[type=checkbox]{box-sizing:border-box;padding:0}summary{display:list-item}[hidden]{display:none}button{border-radius:0;cursor:pointer}h1{font-size:2rem;font-weight:700;letter-spacing:-.0390625rem;line-height:2.25rem}.c-card--major .c-card__title,.u-h1,.u-h2,button,h1,h2,h2.app-access-wall__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-card--major .c-card__title,.u-h2,h2{font-size:1.5rem;font-weight:700;letter-spacing:-.0117156rem;line-height:1.6rem}.u-h3{letter-spacing:-.0117156rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-card__title,.c-reading-companion__figure-title,.u-h3,.u-h4,h3,h4,h5,h6{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:700;line-height:1.4rem}.c-article-editorial-summary__container .c-article-editorial-summary__article-title,.c-reading-companion__figure-title,.u-h4,h3,h4,h5,h6{letter-spacing:-.0117156rem}.c-reading-companion__figure-title,.u-h4{font-size:1.125rem}input+label{padding-left:.5em}nav ol,nav ul{list-style:none none}p:empty{display:none}.c-nature-box{background-color:#fff;border:1px solid #d5d5d5;border-radius:2px;box-shadow:0 0 5px 0 rgba(51,51,51,.1);line-height:1.3;margin-bottom:24px;padding:16px 16px 3px}.c-nature-box__text{font-size:1rem;margin-bottom:16px}.c-nature-box--access-to-pdf{display:none}@media only screen and (min-width:1024px){.c-nature-box--mobile{display:none}}.c-nature-box .c-pdf-download{margin-bottom:16px!important}.c-nature-box svg+.c-article__button-text{margin-left:8px}.c-nature-box--version{background-color:#eee}.c-nature-box__wrapper{transform:translateZ(0)}.c-nature-box__wrapper--placeholder{min-height:165px}.sans-serif{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.article-page{background:#fff}p{overflow-wrap:break-word;word-break:break-word}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;padding:0}.c-article-identifiers__item{list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-associated-content__container .c-article-associated-content__title,.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-associated-content__container .c-article-associated-content__title{margin-bottom:8px}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #fff;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-editorial-summary__container{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem}.c-article-editorial-summary__container .c-article-editorial-summary__content p:last-child{margin-bottom:0}.c-article-editorial-summary__container .c-article-editorial-summary__content--less{max-height:9.5rem;overflow:hidden}.c-article-editorial-summary__container .c-article-editorial-summary__button{background-color:#fff;border:0;color:#069;font-size:.875rem;margin-bottom:16px}.c-article-editorial-summary__container .c-article-editorial-summary__button.active,.c-article-editorial-summary__container .c-article-editorial-summary__button.hover,.c-article-editorial-summary__container .c-article-editorial-summary__button:active,.c-article-editorial-summary__container .c-article-editorial-summary__button:hover{text-decoration:underline;text-decoration-skip-ink:auto}.c-article-associated-content__container .c-article-associated-content__collection-label{font-size:.875rem;line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#0067c5;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fff;border-bottom:1px solid #fff;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-cod,.c-reading-companion__panel--active{display:block}.c-cod{font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex:1 1 auto;margin:0;padding:13px}.c-cod__input--submit{background-color:#069;border:1px solid #069;color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff;color:#069}.c-pdf-download__link .u-icon{padding-top:2px}.c-pdf-download{display:flex;margin-bottom:24px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__link:hover{text-decoration:none}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-context-bar--sticky .c-pdf-download{display:block;margin-bottom:0;white-space:nowrap}@media only screen and (max-width:539px){.c-pdf-download .u-sticky-visually-hidden{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:16px}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.c-article-extras .c-pdf-container .c-pdf-download+.c-pdf-download{margin-left:0}@media only screen and (min-width:540px){.c-context-bar--sticky .c-pdf-download__link{align-items:center;flex:1 1 183px}}@media only screen and (max-width:320px){.c-context-bar--sticky .c-pdf-download__link{padding:16px}}.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:none}@media only screen and (max-width:1023px){.article-page--commercial .c-article-main-column .c-pdf-button__container .c-pdf-download{display:block}}.c-recommendations-column-switch .c-meta{margin-top:auto}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__container{margin:0 auto;max-width:1280px;padding:0 16px}.c-context-bar__title{display:none}.app-researcher-popup__link.hover,.app-researcher-popup__link.visited,.app-researcher-popup__link:hover,.app-researcher-popup__link:visited,.c-article-metrics__heading a,.c-article-metrics__posts .c-card__title a{color:inherit}.c-article-authors-search__list{align-items:center;display:flex;flex-wrap:wrap;gap:16px 16px;justify-content:center}@media only screen and (min-width:320px){.c-article-authors-search__list{justify-content:normal}}.c-article-authors-search__text{align-items:center;display:flex;flex-flow:column wrap;font-size:14px;justify-content:center}@media only screen and (min-width:320px){.c-article-authors-search__text{flex-direction:row;font-size:16px}}.c-article-authors-search__links-text{font-weight:700;margin-right:8px;text-align:center}@media only screen and (min-width:320px){.c-article-authors-search__links-text{text-align:left}}.c-article-authors-search__list-item--left{flex:1 1 100%}@media only screen and (min-width:320px){.c-article-authors-search__list-item--left{flex-basis:auto}}.c-article-authors-search__list-item--right{flex:1 1 auto}.c-article-identifiers{margin:0}.c-article-identifiers__item{border-right:2px solid #cedbe0;color:#222;font-size:14px}@media only screen and (min-width:320px){.c-article-identifiers__item{font-size:16px}}.c-article-identifiers__item:last-child{border-right:none}.c-article-metrics__posts .c-card__title{font-size:1.05rem}.c-article-metrics__posts .c-card__title+span{color:#6f6f6f;font-size:1rem}.app-author-list{color:#222;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;line-height:1.4;list-style:none;margin:0;padding:0}.app-author-list>li,.c-breadcrumbs>li,.c-footer__links>li,.js .app-author-list,.u-list-comma-separated>li,.u-list-inline>li{display:inline}.app-author-list>li:not(:first-child):not(:last-child):before{content:", "}.app-author-list>li:not(:only-child):last-child:before{content:" & "}.app-author-list--compact{font-size:.875rem;line-height:1.4}.app-author-list--truncated>li:not(:only-child):last-child:before{content:" ... "}.js .app-author-list__hide{display:none;visibility:hidden}.js .app-author-list__hide:first-child+*{margin-block-start:0}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad__label{color:#333;font-weight:400;line-height:1.5;margin-bottom:4px}.c-ad__label,.c-meta{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem}.c-meta{color:inherit;line-height:1.4;list-style:none;margin:0;padding:0}.c-meta--large{font-size:1rem}.c-meta--large .c-meta__item{margin-bottom:8px}.c-meta__item{display:inline-block;margin-bottom:4px}.c-meta__item:not(:last-child){border-right:1px solid #d5d5d5;margin-right:4px;padding-right:4px}@media only screen and (max-width:539px){.c-meta__item--block-sm-max{display:block}.c-meta__item--block-sm-max:not(:last-child){border-right:none;margin-right:0;padding-right:0}}@media only screen and (min-width:1024px){.c-meta__item--block-at-lg{display:block}.c-meta__item--block-at-lg:not(:last-child){border-right:none;margin-right:0;padding-right:0}}.c-meta__type{font-weight:700;text-transform:none}.c-skip-link{background:#069;bottom:auto;color:#fff;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#fff}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-card__summary>p:last-child,.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #eee;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__heading{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;font-weight:700}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message__icon--top{align-self:flex-start}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-status-message--error .c-status-message__icon{color:#c40606}.c-status-message--boxed.c-status-message--error{border-bottom:4px solid #c40606}.c-status-message--success .c-status-message__icon{color:#00b8b0}.c-status-message--boxed.c-status-message--success{border-bottom:4px solid #00b8b0}.c-status-message--warning .c-status-message__icon{color:#edbc53}.c-status-message--boxed.c-status-message--warning{border-bottom:4px solid #edbc53}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{margin:4px 4px 0;fill:#888;height:10px;width:10px}@media only screen and (max-width:539px){.c-breadcrumbs .c-breadcrumbs__item{display:none}.c-breadcrumbs .c-breadcrumbs__item:last-child,.c-breadcrumbs .c-breadcrumbs__item:nth-last-child(2){display:inline}}.c-card{background-color:transparent;border:0;box-shadow:none;display:flex;flex-direction:column;font-size:14px;min-width:0;overflow:hidden;padding:0;position:relative}.c-card--no-shape{background:0 0;border:0;box-shadow:none}.c-card__image{display:flex;justify-content:center;overflow:hidden;padding-bottom:56.25%;position:relative}@supports (aspect-ratio:1/1){.c-card__image{padding-bottom:0}}.c-card__image img{left:0;min-height:100%;min-width:100%;position:absolute}@supports ((-o-object-fit:cover) or (object-fit:cover)){.c-card__image img{height:100%;object-fit:cover;width:100%}}.c-card__body{flex:1 1 auto;padding:16px}.c-card--no-shape .c-card__body{padding:0}.c-card--no-shape .c-card__body:not(:first-child){padding-top:16px}.c-card__title{letter-spacing:-.01875rem;margin-bottom:8px;margin-top:0}[lang=de] .c-card__title{hyphens:auto}.c-card__summary{line-height:1.4}.c-card__summary>p{margin-bottom:5px}.c-card__summary a{text-decoration:underline}.c-card__link:not(.c-card__link--no-block-link):before{bottom:0;content:"";left:0;position:absolute;right:0;top:0}.c-card--flush .c-card__body{padding:0}.c-card--major{font-size:1rem}.c-card--dark{background-color:#29303c;border-width:0;color:#e3e4e5}.c-card--dark .c-card__title{color:#fff}.c-card--dark .c-card__link,.c-card--dark .c-card__summary a{color:inherit}.c-header{background-color:#fff;border-bottom:5px solid #000;font-size:1rem;line-height:1.4;margin-bottom:16px}.c-header__row{padding:0;position:relative}.c-header__row:not(:last-child){border-bottom:1px solid #eee}.c-header__split{align-items:center;display:flex;justify-content:space-between}.c-header__logo-container{flex:1 1 0px;line-height:0;margin:8px 24px 8px 0}.c-header__logo{transform:translateZ(0)}.c-header__logo img{max-height:32px}.c-header__container{margin:0 auto;max-width:1280px}.c-header__menu{align-items:center;display:flex;flex:0 1 auto;flex-wrap:wrap;font-weight:700;gap:8px 8px;line-height:1.4;list-style:none;margin:0 -4px;padding:0}@media print{.c-header__menu{display:none}}@media only screen and (max-width:1023px){.c-header__menu--hide-lg-max{display:none;visibility:hidden}}.c-header__menu--global{font-weight:400;justify-content:flex-end}.c-header__menu--global svg{display:none;visibility:hidden}.c-header__menu--global svg:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__menu--global svg{display:block;visibility:visible}}.c-header__menu--journal{font-size:.875rem;margin:8px 0 8px -8px}@media only screen and (min-width:540px){.c-header__menu--journal{flex-wrap:nowrap;font-size:1rem}}.c-header__item{padding-bottom:0;padding-top:0;position:static}.c-header__item--pipe{border-left:2px solid #eee;padding-left:8px}.c-header__item--padding{padding-bottom:8px;padding-top:8px}@media only screen and (min-width:540px){.c-header__item--dropdown-menu{position:relative}}@media only screen and (min-width:1024px){.c-header__item--hide-lg{display:none;visibility:hidden}}@media only screen and (max-width:767px){.c-header__item--hide-md-max{display:none;visibility:hidden}.c-header__item--hide-md-max:first-child+*{margin-block-start:0}}.c-header__link{align-items:center;color:inherit;display:inline-flex;gap:4px 4px;padding:8px;white-space:nowrap}.c-header__link svg{transition-duration:.2s}.c-header__show-text{display:none;visibility:hidden}.has-tethered .c-header__heading--js-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:540px){.c-header__show-text{display:inline;visibility:visible}}.c-header__show-text-sm{display:inline;visibility:visible}@media only screen and (min-width:540px){.c-header__show-text-sm{display:none;visibility:hidden}.c-header__show-text-sm:first-child+*{margin-block-start:0}}.c-header__dropdown{background-color:#000;border-bottom:1px solid #2f2f2f;color:#eee;font-size:.875rem;line-height:1.2;padding:16px 0}@media print{.c-header__dropdown{display:none}}.c-header__heading{display:inline-block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.25rem;font-weight:400;line-height:1.4;margin-bottom:8px}.c-header__heading--keyline{border-top:1px solid;border-color:#2f2f2f;margin-top:16px;padding-top:16px;width:100%}.c-header__list{display:flex;flex-wrap:wrap;gap:0 16px;list-style:none;margin:0 -8px}.c-header__flush{margin:0 -8px}.c-header__visually-hidden{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.c-header__search-form{margin-bottom:8px}.c-header__search-layout{display:flex;flex-wrap:wrap;gap:16px 16px}.c-header__search-layout>:first-child{flex:999 1 auto}.c-header__search-layout>*{flex:1 1 auto}.c-header__search-layout--max-width{max-width:720px}.c-header__search-button{align-items:center;background-color:transparent;background-image:none;border:1px solid #fff;border-radius:2px;color:#fff;cursor:pointer;display:flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.15;margin:0;padding:8px 16px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:100%}.c-header__input,.c-header__select{border:1px solid;border-radius:3px;box-sizing:border-box;font-size:1rem;padding:8px 16px;width:100%}.c-header__select{-webkit-appearance:none;background-image:url("data:image/svg+xml,%3Csvg height='16' viewBox='0 0 16 16' width='16' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z' fill='%23333' fill-rule='evenodd' transform='matrix(0 1 -1 0 11 3)'/%3E%3C/svg%3E");background-position:right .7em top 50%;background-repeat:no-repeat;background-size:1em;box-shadow:0 1px 0 1px rgba(0,0,0,.04);display:block;margin:0;max-width:100%;min-width:150px}@media only screen and (min-width:540px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:auto;right:0}}@media only screen and (min-width:768px){.c-header__menu--journal .c-header__item--dropdown-menu:last-child .c-header__dropdown.has-tethered{left:0;right:auto}}.c-header__dropdown.has-tethered{border-bottom:0;border-radius:0 0 2px 2px;left:0;position:absolute;top:100%;transform:translateY(5px);width:100%;z-index:1}@media only screen and (min-width:540px){.c-header__dropdown.has-tethered{transform:translateY(8px);width:auto}}@media only screen and (min-width:768px){.c-header__dropdown.has-tethered{min-width:225px}}.c-header__dropdown--full-width.has-tethered{padding:32px 0 24px;transform:none;width:100%}.has-tethered .c-header__heading--js-hide{display:none;visibility:hidden}.has-tethered .c-header__list--js-stack{flex-direction:column}.has-tethered .c-header__item--keyline,.has-tethered .c-header__list~.c-header__list .c-header__item:first-child{border-top:1px solid #d5d5d5;margin-top:8px;padding-top:8px}.c-header__item--snid-account-widget{display:flex}.c-header__container{padding:0 4px}.c-header__list{padding:0 12px}.c-header__menu .c-header__link{font-size:14px}.c-header__item--snid-account-widget .c-header__link{padding:8px}.c-header__menu--journal{margin-left:0}@media only screen and (min-width:540px){.c-header__container{padding:0 16px}.c-header__menu--journal{margin-left:-8px}.c-header__menu .c-header__link{font-size:16px}.c-header__link--search{gap:13px 13px}}.u-button{align-items:center;background-color:transparent;background-image:none;border-radius:2px;cursor:pointer;display:inline-flex;font-family:sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button svg,.u-button--primary svg{fill:currentcolor}.u-button{border:1px solid #069;color:#069}.u-button--primary{background-color:#069;background-image:none;border:1px solid #069;color:#fff}.u-button--full-width{display:flex;width:100%}.u-display-none{display:none}.js .u-js-hide,.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}.u-visually-hidden{border:0;clip:rect(0,0,0,0);height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}@media print{.u-hide-print{display:none}}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}.u-hide-at-lg:first-child+*{margin-block-start:0}}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-color-open-access{color:#b74616}.u-float-left{float:left}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-full-height{height:100%}.u-link-inherit{color:inherit}.u-list-reset{list-style:none;margin:0;padding:0}.u-text-bold{font-weight:700}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-justify-content-space-between{justify-content:space-between}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-mb-48{margin-bottom:48px}.u-pa-16{padding:16px}html *,html :after,html :before{box-sizing:inherit}.c-article-section__title,.c-article-title{font-weight:700}.c-card__title{line-height:1.4em}.c-article__button{background-color:#069;border:1px solid #069;border-radius:2px;color:#fff;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;line-height:1.4;margin-bottom:16px;padding:13px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-article__button,.c-article__button:hover{text-decoration:none}.c-article__button--inverted,.c-article__button:hover{background-color:#fff;color:#069}.c-article__button--inverted:hover{background-color:#069;color:#fff}.c-header__link{text-decoration:inherit}.grade-c-hide{display:block}.c-pdf-download__link{padding:13px 24px} } </style>





        <link data-test="critical-css-handler" data-inline-css-source="critical-css" rel="stylesheet" href="/static/css/enhanced-article-b8c2953b86.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">

    <noscript>
        <link rel="stylesheet" type="text/css" href="/static/css/enhanced-article-b8c2953b86.css" media="only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)">
    </noscript>

<link rel="stylesheet" type="text/css" href="/static/css/article-print-fb7cb72232.css" media="print">




<link rel="apple-touch-icon" sizes="180x180" href=/static/images/favicons/nature/apple-touch-icon-f39cb19454.png>
<link rel="icon" type="image/png" sizes="48x48" href=/static/images/favicons/nature/favicon-48x48-b52890008c.png>
<link rel="icon" type="image/png" sizes="32x32" href=/static/images/favicons/nature/favicon-32x32-3fe59ece92.png>
<link rel="icon" type="image/png" sizes="16x16" href=/static/images/favicons/nature/favicon-16x16-951651ab72.png>
<link rel="manifest" href=/static/manifest.json crossorigin="use-credentials">
<link rel="mask-icon" href=/static/images/favicons/nature/safari-pinned-tab-69bff48fe6.svg color="#000000">
<link rel="shortcut icon" href=/static/images/favicons/nature/favicon.ico>
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-config" content=/static/browserconfig.xml>
<meta name="theme-color" content="#000000">
<meta name="application-name" content="Nature">


<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>



<!-- Google Tag Manager -->
<script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://sgtm.nature.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
                performance.mark('SN GPT Ads gtm-container-fired');
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
<!-- End Google Tag Manager -->

    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            if (h === 'preview-www.nature.com') return;
            var e = d.createElement(t),
                s = d.getElementsByTagName(t)[0];
            if (h === 'nature.com' || h.endsWith('.nature.com')) {
                e.src = 'https://cmp.nature.com/production_live/en/consent-bundle-8-99.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = '/static/js/cookie-consent-es5-bundle-8d962b73c2.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }
        cc();
    })(window,document,'script');
</script>


<script id="js-position0">
    (function(w, d) {
        w.idpVerifyPrefix = 'https://verify.nature.com';
        w.ra21Host = 'https://wayf.springernature.com';
        var moduleSupport = (function() {
            return 'noModule' in d.createElement('script');
        })();

        if (w.config.mustardcut === true) {
            w.loader = {
                index: 0,
                registered: [],
                scripts: [

                        {src: '/static/js/global-article-es6-bundle-e979e7e7bc.js', test: 'global-article-js', module: true},
                        {src: '/static/js/global-article-es5-bundle-597536b0e3.js', test: 'global-article-js', nomodule: true},
                        {src: '/static/js/shared-es6-bundle-175c7a3024.js', test: 'shared-js', module: true},
                        {src: '/static/js/shared-es5-bundle-9786f91175.js', test: 'shared-js', nomodule: true},
                        {src: '/static/js/header-150-es6-bundle-5bb959eaa1.js', test: 'header-150-js', module: true},
                        {src: '/static/js/header-150-es5-bundle-994fde5b1d.js', test: 'header-150-js', nomodule: true}

                ].filter(function (s) {
                    if (s.src === null) return false;
                    if (moduleSupport && s.nomodule) return false;
                    return !(!moduleSupport && s.module);
                }),

                register: function (value) {
                    this.registered.push(value);
                },

                ready: function () {
                    if (this.registered.length === this.scripts.length) {
                        this.registered.forEach(function (fn) {
                            if (typeof fn === 'function') {
                                setTimeout(fn, 0); 
                            }
                        });
                        this.ready = function () {};
                    }
                },

                insert: function (s) {
                    var t = d.getElementById('js-position' + this.index);
                    if (t && t.insertAdjacentElement) {
                        t.insertAdjacentElement('afterend', s);
                    } else {
                        d.head.appendChild(s);
                    }
                    ++this.index;
                },

                createScript: function (script, beforeLoad) {
                    var s = d.createElement('script');
                    s.id = 'js-position' + (this.index + 1);
                    s.setAttribute('data-test', script.test);
                    if (beforeLoad) {
                        s.defer = 'defer';
                        s.onload = function () {
                            if (script.noinit) {
                                loader.register(true);
                            }
                            if (d.readyState === 'interactive' || d.readyState === 'complete') {
                                loader.ready();
                            }
                        };
                    } else {
                        s.async = 'async';
                    }
                    s.src = script.src;
                    return s;
                },

                init: function () {
                    this.scripts.forEach(function (s) {
                        loader.insert(loader.createScript(s, true));
                    });

                    d.addEventListener('DOMContentLoaded', function () {
                        loader.ready();
                        var conditionalScripts;

                            conditionalScripts = [
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es6-bundle-464a2af269.js', test: 'pan-zoom-js',  module: true },
                                {match: 'div[data-pan-container]', src: '/static/js/pan-zoom-es5-bundle-98fb9b653b.js', test: 'pan-zoom-js',  nomodule: true },
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es6-bundle-55688a0084.js', test: 'math-js', module: true},
                                {match: 'math,span.mathjax-tex', src: '/static/js/math-es5-bundle-6a270012ec.js', test: 'math-js', nomodule: true}
                            ];


                        if (conditionalScripts) {
                            conditionalScripts.filter(function (script) {
                                return !!document.querySelector(script.match) && !((moduleSupport && script.nomodule) || (!moduleSupport && script.module));
                            }).forEach(function (script) {
                                loader.insert(loader.createScript(script));
                            });
                        }
                    }, false);
                }
            };
            loader.init();
        }
    })(window, document);
</script>










<meta name="robots" content="noarchive">
<meta name="access" content="Yes">


<link rel="search" href="https://www.nature.com/search">
<link rel="search" href="https://www.nature.com/opensearch/opensearch.xml" type="application/opensearchdescription+xml" title="nature.com">
<link rel="search" href="https://www.nature.com/opensearch/request" type="application/sru+xml" title="nature.com">






    <script type="application/ld+json">{"mainEntity":{"headline":"A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models","description":"The exponential growth of scientific articles has presented challenges in information organization and extraction. Automation is urgently needed to streamline literature reviews and enhance insight extraction. We explore the potential of Large Language Models (LLMs) in key-insights extraction from scientific articles, including OpenAI’s GPT-4.0, MistralAI’s Mixtral 8 × 7B, 01AI’s Yi, and InternLM’s InternLM2. We have developed an article-level key-insight extraction system based on LLMs, calling it ArticleLLM. After evaluating the LLMs against manual benchmarks, we have enhanced their performance through fine-tuning. We propose a multi-actor LLM approach, merging the strengths of multiple fine-tuned LLMs to improve overall key-insight extraction performance. This work demonstrates not only the feasibility of LLMs in key-insight extraction, but also the effectiveness of cooperation of multiple fine-tuned LLMs, leading to efficient academic literature survey and knowledge discovery.","datePublished":"2025-01-10T00:00:00Z","dateModified":"2025-01-10T00:00:00Z","pageStart":"1","pageEnd":"11","license":"http://creativecommons.org/licenses/by-nc-nd/4.0/","sameAs":"https://doi.org/10.1038/s41598-025-85715-7","keywords":["Computer science","Scientific data","Key-insight extraction","Large Language Model fine-tuning","Automatic information extraction","Science","Humanities and Social Sciences","multidisciplinary"],"image":["https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-025-85715-7/MediaObjects/41598_2025_85715_Fig1_HTML.png","https://media.springernature.com/lw1200/springer-static/image/art%3A10.1038%2Fs41598-025-85715-7/MediaObjects/41598_2025_85715_Fig2_HTML.png"],"isPartOf":{"name":"Scientific Reports","issn":["2045-2322"],"volumeNumber":"15","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Nature Publishing Group UK","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Zihan Song","affiliation":[{"name":"Dong-A University","address":{"name":"Dong-A University, Busan, Republic of Korea","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Gyo-Yeob Hwang","affiliation":[{"name":"Dong-A University","address":{"name":"Dong-A University, Busan, Republic of Korea","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Xin Zhang","affiliation":[{"name":"Dong-A University","address":{"name":"Dong-A University, Busan, Republic of Korea","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Shan Huang","affiliation":[{"name":"Dong-A University","address":{"name":"Dong-A University, Busan, Republic of Korea","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Byung-Kwon Park","affiliation":[{"name":"Dong-A University","address":{"name":"Dong-A University, Busan, Republic of Korea","@type":"PostalAddress"},"@type":"Organization"}],"email":"bpark@dau.ac.kr","@type":"Person"}],"isAccessibleForFree":true,"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>










    <link rel="canonical" href="https://www.nature.com/articles/s41598-025-85715-7">


    <meta name="journal_id" content="41598"/>
    <meta name="dc.title" content="A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models"/>
    <meta name="dc.source" content="Scientific Reports 2025 15:1"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Nature Publishing Group"/>
    <meta name="dc.date" content="2025-01-10"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2025 The Author(s)"/>
    <meta name="dc.rights" content="2025 The Author(s)"/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="The exponential growth of scientific articles has presented challenges in information organization and extraction. Automation is urgently needed to streamline literature reviews and enhance insight extraction. We explore the potential of Large Language Models (LLMs) in key-insights extraction from scientific articles, including OpenAI&#8217;s GPT-4.0, MistralAI&#8217;s Mixtral 8&#8201;&#215;&#8201;7B, 01AI&#8217;s Yi, and InternLM&#8217;s InternLM2. We have developed an article-level key-insight extraction system based on LLMs, calling it ArticleLLM. After evaluating the LLMs against manual benchmarks, we have enhanced their performance through fine-tuning. We propose a multi-actor LLM approach, merging the strengths of multiple fine-tuned LLMs to improve overall key-insight extraction performance. This work demonstrates not only the feasibility of LLMs in key-insight extraction, but also the effectiveness of cooperation of multiple fine-tuned LLMs, leading to efficient academic literature survey and knowledge discovery."/>
    <meta name="prism.issn" content="2045-2322"/>
    <meta name="prism.publicationName" content="Scientific Reports"/>
    <meta name="prism.publicationDate" content="2025-01-10"/>
    <meta name="prism.volume" content="15"/>
    <meta name="prism.number" content="1"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="1608"/>
    <meta name="prism.endingPage" content=""/>
    <meta name="prism.copyright" content="2025 The Author(s)"/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://www.nature.com/articles/s41598-025-85715-7"/>
    <meta name="prism.doi" content="doi:10.1038/s41598-025-85715-7"/>
    <meta name="citation_pdf_url" content="https://www.nature.com/articles/s41598-025-85715-7.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://www.nature.com/articles/s41598-025-85715-7"/>
    <meta name="citation_journal_title" content="Scientific Reports"/>
    <meta name="citation_journal_abbrev" content="Sci Rep"/>
    <meta name="citation_publisher" content="Nature Publishing Group"/>
    <meta name="citation_issn" content="2045-2322"/>
    <meta name="citation_title" content="A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models"/>
    <meta name="citation_volume" content="15"/>
    <meta name="citation_issue" content="1"/>
    <meta name="citation_online_date" content="2025/01/10"/>
    <meta name="citation_firstpage" content="1608"/>
    <meta name="citation_lastpage" content=""/>
    <meta name="citation_article_type" content="Article"/>
    <meta name="citation_fulltext_world_readable" content=""/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1038/s41598-025-85715-7"/>
    <meta name="DOI" content="10.1038/s41598-025-85715-7"/>
    <meta name="size" content="156834"/>
    <meta name="citation_doi" content="10.1038/s41598-025-85715-7"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1038/s41598-025-85715-7&amp;api_key="/>
    <meta name="description" content="The exponential growth of scientific articles has presented challenges in information organization and extraction. Automation is urgently needed to streamline literature reviews and enhance insight extraction. We explore the potential of Large Language Models (LLMs) in key-insights extraction from scientific articles, including OpenAI&#8217;s GPT-4.0, MistralAI&#8217;s Mixtral 8&#8201;&#215;&#8201;7B, 01AI&#8217;s Yi, and InternLM&#8217;s InternLM2. We have developed an article-level key-insight extraction system based on LLMs, calling it ArticleLLM. After evaluating the LLMs against manual benchmarks, we have enhanced their performance through fine-tuning. We propose a multi-actor LLM approach, merging the strengths of multiple fine-tuned LLMs to improve overall key-insight extraction performance. This work demonstrates not only the feasibility of LLMs in key-insight extraction, but also the effectiveness of cooperation of multiple fine-tuned LLMs, leading to efficient academic literature survey and knowledge discovery."/>
    <meta name="dc.creator" content="Song, Zihan"/>
    <meta name="dc.creator" content="Hwang, Gyo-Yeob"/>
    <meta name="dc.creator" content="Zhang, Xin"/>
    <meta name="dc.creator" content="Huang, Shan"/>
    <meta name="dc.creator" content="Park, Byung-Kwon"/>
    <meta name="dc.subject" content="Computer science"/>
    <meta name="dc.subject" content="Scientific data"/>
    <meta name="citation_reference" content="citation_journal_title=Humanit. Soc. Sci. Commun.; citation_title=Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases; citation_author=L Bornmann, R Haunschild, R Mutz; citation_volume=8; citation_publication_date=2021; citation_pages=224; citation_doi=10.1057/s41599-021-00903-w; citation_id=CR1"/>
    <meta name="citation_reference" content="citation_journal_title=BMJ Open.; citation_title=Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the PROSPERO registry; citation_author=R Borah, AW Brown, PL Capers, KA Kaiser; citation_volume=7; citation_publication_date=2017; citation_pages=e012545; citation_doi=10.1136/bmjopen-2016-012545; citation_id=CR2"/>
    <meta name="citation_reference" content="citation_journal_title=Scientometrics; citation_title=Information extraction from scientific articles: a survey; citation_author=Z Nasar, SW Jaffry, MK Malik; citation_volume=117; citation_publication_date=2018; citation_pages=1931-1990; citation_doi=10.1007/s11192-018-2921-5; citation_id=CR3"/>
    <meta name="citation_reference" content="Boukhers, Z. &amp; Bouabdallah, A. Vision and natural language for metadata extraction from scientific PDF documents. In Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries 1&#8211;5. 
                  https://doi.org/10.1145/3529372.3533295

                 (ACM, 2022)."/>
    <meta name="citation_reference" content="Tateisi, Y., Ohta, T., Miyao, Y., Pyysalo, S. &amp; Aizawa, A. Typed entity and relation annotation on computer science papers. In Proceedings of the 10th International Conference on Language Resources and Evaluation. 3836&#8211;3843 (2016)."/>
    <meta name="citation_reference" content="citation_journal_title=Comput. Speech Lang.; citation_title=Mining methodologies from NLP publications: a case study in automatic terminology recognition; citation_author=A Kova&#269;evi&#263;, Z Konjovi&#263;, B Milosavljevi&#263;, G Nenadic; citation_volume=26; citation_publication_date=2012; citation_pages=105-126; citation_doi=10.1016/j.csl.2011.09.001; citation_id=CR6"/>
    <meta name="citation_reference" content="Bubeck, S. et al. Sparks of Artificial General Intelligence: Early experiments with GPT-4. Preprint at 
                  http://arxiv.org/abs/2303.12712

                 (2023)."/>
    <meta name="citation_reference" content="Lakhanpal, S., Gupta, A. K. &amp; Agrawal, R. Towards Extracting Domains from Research Publications. In Midwest Artificial Intelligence and Cognitive Science Conference. 
                  https://api.semanticscholar.org/CorpusID:5220521

                 (2015)."/>
    <meta name="citation_reference" content="Hirohata, K., Okazaki, N., Ananiadou, S. &amp; Ishizuka, M. Identifying Sections in Scientific Abstracts using Conditional Random Fields. In Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-I. 
                  https://aclanthology.org/I08-1050/

                  (2008)."/>
    <meta name="citation_reference" content="Ronzano, F. &amp; Saggion, H. Dr. Inventor Framework: Extracting Structured Information from Scientific Publications. In International Conference on Discovery Science (eds. Japkowicz, N. &amp; Matwin, S.) vol. 9356, 209&#8211;220. 
                  https://doi.org/10.1007/978-3-319-24282-8

                  (Springer International Publishing, 2015)."/>
    <meta name="citation_reference" content="He, H. et al. An Insight Extraction System on BioMedical Literature with Deep Neural Networks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing 2691&#8211;2701. 
                  https://doi.org/10.18653/v1/D17-1285

                 (Association for Computational Linguistics, Stroudsburg, PA, USA, 2017)."/>
    <meta name="citation_reference" content="Polak, M. P. &amp; Morgan, D. Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering. Preprint at 
                  http://arxiv.org/abs/2303.05352

                 (2023)."/>
    <meta name="citation_reference" content="citation_journal_title=Am. J. Cancer Res.; citation_title=The role of ChatGPT in scientific communication: writing better scientific review articles; citation_author=J Huang, M Tan; citation_volume=13; citation_publication_date=2023; citation_pages=1148-1154; citation_id=CR13"/>
    <meta name="citation_reference" content="Han, R. et al. Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. Preprint at 
                  http://arxiv.org/abs/2305.14450

                 (2023)."/>
    <meta name="citation_reference" content="Yuan, C., Xie, Q. &amp; Ananiadou, S. Zero-shot Temporal Relation Extraction with ChatGPT. Preprint at 
                  http://arxiv.org/abs/2304.05454

                 (2023)."/>
    <meta name="citation_reference" content="Wang, S., Scells, H., Koopman, B. &amp; Zuccon, G. Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search? Preprint at 
                  http://arxiv.org/abs/2302.03495

                 (2023)."/>
    <meta name="citation_reference" content="Zhang, J., Chen, Y., Niu, N., Wang, Y. &amp; Liu, C. Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting. Preprint at 
                  http://arxiv.org/abs/2304.12562

                 (2023)."/>
    <meta name="citation_reference" content="Li, B. et al. Evaluating ChatGPT&#8217;s Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness. Preprint at 
                  http://arxiv.org/abs/2304.11633

                 (2023)."/>
    <meta name="citation_reference" content="Sun, W. et al. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. Preprint at 
                  http://arxiv.org/abs/2304.09542

                 (2023)."/>
    <meta name="citation_reference" content="Jahan, I., Laskar, M. T. R., Peng, C. &amp; Huang, J. Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. Preprint at 
                  http://arxiv.org/abs/2306.04504

                 (2023)."/>
    <meta name="citation_reference" content="citation_journal_title=Appl. Intell.; citation_title=Key-insight extraction from scientific tables; citation_author=S Kempf, M Krug, FKIETA Puppe; citation_volume=53; citation_publication_date=2023; citation_pages=9513-9530; citation_doi=10.1007/s10489-022-03957-8; citation_id=CR21"/>
    <meta name="citation_reference" content="Laban, P. et al. SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing 9662&#8211;9676. 
                  https://aclanthology.org/2023.emnlp-main.600/

                 (Association for Computational Linguistics, 2023)."/>
    <meta name="citation_reference" content="Cai, H. et al. SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis. Preprint at 
                  http://arxiv.org/abs/2403.01976

                 (2024)."/>
    <meta name="citation_reference" content="Touvron, H. et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. Preprint at 
                  http://arxiv.org/abs/2307.09288

                 (2023)."/>
    <meta name="citation_reference" content="Howard, J. &amp; Ruder, S. Universal Language Model Fine-tuning for Text Classification. Preprint at 
                  http://arxiv.org/abs/1801.06146

                 (2018)."/>
    <meta name="citation_reference" content="citation_journal_title=ICLR 2022&#8211;10th Int. Conf. Learn. Represent; citation_title=Lora: low-rank adaptation of large Language models; citation_author=E Hu; citation_volume=1; citation_publication_date=2022; citation_pages=26; citation_id=CR26"/>
    <meta name="citation_reference" content="Fang, C. et al. LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction. Preprint at 
                  http://arxiv.org/abs/2403.00863

                 (2024)."/>
    <meta name="citation_reference" content="Jiang, D., Ren, X. &amp; Lin, B. Y. LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. Preprint at 
                  http://arxiv.org/abs/2306.02561

                 (2023)."/>
    <meta name="citation_reference" content="Lu, K. et al. Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models. Preprint at 
                  http://arxiv.org/abs/2311.08692

                 (2023)."/>
    <meta name="citation_reference" content="Jiang, A. Q. et al. Mixtral of Experts. Preprint at 
                  http://arxiv.org/abs/2401.04088

                 (2024)."/>
    <meta name="citation_reference" content="Hong, S. et al. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. Preprint at 
                  http://arxiv.org/abs/2308.00352

                 (2023)."/>
    <meta name="citation_reference" content="Liang, T. et al. Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. Preprint at 
                  http://arxiv.org/abs/2305.19118

                 (2023)."/>
    <meta name="citation_reference" content="Chen, H. et al. ChatGPT&#8217;s One-year Anniversary: Are Open-Source Large Language Models Catching up? Preprint at 
                  http://arxiv.org/abs/2311.16989

                 (2023)."/>
    <meta name="citation_reference" content="Huang, H., Qu, Y., Liu, J., Yang, M. &amp; Zhao, T. An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers. Preprint at 
                  http://arxiv.org/abs/2403.02839

                 (2024)."/>
    <meta name="citation_reference" content="Wolf, T. et al. HuggingFace&#8217;s Transformers: State-of-the-art Natural Language Processing. Preprint at 
                  http://arxiv.org/abs/1910.03771

                 (2019)."/>
    <meta name="citation_reference" content="Xuechen L. et al. AlpacaEval: An Automatic Evaluator of Instruction-following Models.  
                  https://github.com/tatsu-lab/alpaca_eval

                 (2023)."/>
    <meta name="citation_reference" content="Chung, H. W. et al. Scaling Instruction-Finetuned Language Models. Preprint at 
                  http://arxiv.org/abs/2210.11416

                 (2022)."/>
    <meta name="citation_reference" content="Frantar, E., Ashkboos, S., Hoefler, T. &amp; Alistarh, D. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. Preprint at 
                  https://arxiv.org/abs/2210.17323

                 (2022)."/>
    <meta name="citation_reference" content="Ilya L. &amp; Frank H. Decoupled Weight Decay Regularization. Preprint at 
                  https://arxiv.org/abs/1711.05101

                 (2019)."/>
    <meta name="citation_reference" content="Paul Ginsparg. ArXiv. 
                  https://arxiv.org/

                 (1991)."/>
    <meta name="citation_reference" content="Bakker, M. A. et al. Fine-tuning language models to find agreement among humans with diverse preferences. Preprint at 
                  https://arxiv.org/abs/2211.15006

                 (2022)."/>
    <meta name="citation_reference" content="Hu, Z. et al. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. Preprint at 
                  http://arxiv.org/abs/2304.01933

                 (2023)."/>
    <meta name="citation_reference" content="Liu, N. F. et al. Lost in the Middle: How Language Models Use Long Contexts. Preprint at 
                  http://arxiv.org/abs/2307.03172

                 (2023)."/>
    <meta name="citation_reference" content="Chris A. M. et al. Tika-Python. 
                  https://github.com/chrismattmann/tika-python

                 (2014)."/>
    <meta name="citation_reference" content="Sadvilkar, N. &amp; Neumann, M. PySBD: Pragmatic Sentence Boundary Disambiguation. Preprint at 
                  http://arxiv.org/abs/2010.09657

                 (2020)."/>
    <meta name="citation_reference" content="Zheng, L. et al. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Preprint at 
                  http://arxiv.org/abs/2306.05685

                 (2023)."/>
    <meta name="citation_reference" content="Gao, M., Hu, X., Ruan, J., Pu, X. &amp; Wan, X. LLM-based NLG Evaluation: Current Status and Challenges. Preprint at 
                  http://arxiv.org/abs/2402.01383

                 (2024)."/>
    <meta name="citation_reference" content="Farouk, M. Measuring Sentences Similarity: A Survey. Preprint at 
                  http://arxiv.org/abs/1910.03940

                 (2019)."/>
    <meta name="citation_reference" content="Reimers, N. &amp; Gurevych, I. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Preprint at 
                  http://arxiv.org/abs/1908.10084

                 (2019)."/>
    <meta name="citation_reference" content="Mangrulkar, S. et al. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. 
                  https://github.com/huggingface/peft

                 (2022)."/>
    <meta name="citation_reference" content="citation_journal_title=Biometrics Bull.; citation_title=Individual comparisons by ranking methods; citation_author=F Wilcoxon; citation_volume=1; citation_publication_date=1945; citation_pages=80; citation_doi=10.2307/3001968; citation_id=CR51"/>
    <meta name="citation_reference" content="citation_journal_title=Inf. Fusion; citation_title=Jack of all trades, master of none; citation_author=J Koco&#324;; citation_volume=99; citation_publication_date=2023; citation_pages=101861; citation_doi=10.1016/j.inffus.2023.101861; citation_id=CR52"/>
    <meta name="citation_reference" content="Li, A. et al. Evaluating Modern PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect. IEEE Trans. Parallel Distrib. Syst. 31, 94&#8211;110 (2020)."/>
    <meta name="citation_author" content="Song, Zihan"/>
    <meta name="citation_author_institution" content="Dong-A University, Busan, Republic of Korea"/>
    <meta name="citation_author" content="Hwang, Gyo-Yeob"/>
    <meta name="citation_author_institution" content="Dong-A University, Busan, Republic of Korea"/>
    <meta name="citation_author" content="Zhang, Xin"/>
    <meta name="citation_author_institution" content="Dong-A University, Busan, Republic of Korea"/>
    <meta name="citation_author" content="Huang, Shan"/>
    <meta name="citation_author_institution" content="Dong-A University, Busan, Republic of Korea"/>
    <meta name="citation_author" content="Park, Byung-Kwon"/>
    <meta name="citation_author_institution" content="Dong-A University, Busan, Republic of Korea"/>
    <meta name="access_endpoint" content="https://www.nature.com/platform/readcube-access"/>
    <meta name="twitter:site" content="@SciReports"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models"/>
    <meta name="twitter:description" content="Scientific Reports - A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models"/>
    <meta name="twitter:image" content="https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-025-85715-7/MediaObjects/41598_2025_85715_Fig1_HTML.png"/>




    <meta property="og:url" content="https://www.nature.com/articles/s41598-025-85715-7"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="Nature"/>
    <meta property="og:title" content="A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models - Scientific Reports"/>
    <meta property="og:image" content="https://media.springernature.com/m685/springer-static/image/art%3A10.1038%2Fs41598-025-85715-7/MediaObjects/41598_2025_85715_Fig1_HTML.png"/>


    <script>
        window.eligibleForRa21 = 'false'; 
    </script>
</head>
<body class="article-page">

<div class="position-relative cleared z-index-50 background-white" data-test="top-containers">
    <a class="c-skip-link" href="#content">Skip to main content</a>



<div class="c-grade-c-banner u-hide">
    <div class="c-grade-c-banner__container">

        <p>Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain
            the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in
            Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles
            and JavaScript.</p>

    </div>
</div>







        <div class="u-hide u-show-following-ad"></div>

    <aside class="c-ad c-ad--728x90">
        <div class="c-ad__inner" data-container-type="banner-advert">
            <p class="c-ad__label">Advertisement</p>



    <div id="div-gpt-ad-top-1"
         class="div-gpt-ad advert leaderboard js-ad text-center hide-print grade-c-hide"
         data-ad-type="top"
         data-test="top-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/scientific_reports/article"
         data-gpt-sizes="728x90"
         data-gpt-targeting="type=article;pos=top;artid=s41598-025-85715-7;doi=10.1038/s41598-025-85715-7;subjmeta=1046,117,639,705;kwrd=Computer+science,Scientific+data">

        <script>
            window.SN = window.SN || {};
            window.SN.libs = window.SN.libs || {};
            window.SN.libs.ads = window.SN.libs.ads || {};
            window.SN.libs.ads.slotConfig = window.SN.libs.ads.slotConfig || {};

                window.SN.libs.ads.slotConfig['top'] = {
                    'pos': 'top',
                    'type': 'article',
                    'path': 's41598-025-85715-7'
                };


            window.SN.libs.ads.slotConfig['kwrd'] = 'Computer+science,Scientific+data';


            window.SN.libs.ads.slotConfig['subjmeta'] = '1046,117,639,705';


        </script>
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/scientific_reports/article&amp;sz=728x90&amp;c=1950777307&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41598-025-85715-7%26doi%3D10.1038/s41598-025-85715-7%26subjmeta%3D1046,117,639,705%26kwrd%3DComputer+science,Scientific+data">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/scientific_reports/article&amp;sz=728x90&amp;c=1950777307&amp;t=pos%3Dtop%26type%3Darticle%26artid%3Ds41598-025-85715-7%26doi%3D10.1038/s41598-025-85715-7%26subjmeta%3D1046,117,639,705%26kwrd%3DComputer+science,Scientific+data"
                     alt="Advertisement"
                     width="728"
                     height="90"></a>
        </noscript>
    </div>



        </div>
    </aside>


    <header class="c-header" id="header" data-header data-track-component="nature-150-split-header" style="border-color:#cedde4">
        <div class="c-header__row">
            <div class="c-header__container">
                <div class="c-header__split">


                    <div class="c-header__logo-container">

                        <a href="/srep"
                           data-track="click" data-track-action="home" data-track-label="image">
                            <picture class="c-header__logo">
                                <source srcset="https://media.springernature.com/full/nature-cms/uploads/product/srep/header-d3c533c187c710c1bedbd8e293815d5f.svg" media="(min-width: 875px)">
                                <img src="https://media.springernature.com/full/nature-cms/uploads/product/srep/header-d3c533c187c710c1bedbd8e293815d5f.svg" height="32" alt="Scientific Reports">
                            </picture>
                        </a>

                    </div>

                    <ul class="c-header__menu c-header__menu--global">
                        <li class="c-header__item c-header__item--padding c-header__item--hide-md-max">
                            <a class="c-header__link" href="https://www.nature.com/siteindex" data-test="siteindex-link"
                               data-track="click" data-track-action="open nature research index" data-track-label="link">
                                <span>View all journals</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--pipe">
                            <a class="c-header__link c-header__link--search"
                                href="#search-menu"
                                data-header-expander
                                data-test="search-link" data-track="click" data-track-action="open search tray" data-track-label="button">
                                <svg role="img" aria-hidden="true" focusable="false" height="22" width="22" viewBox="0 0 18 18" xmlns="http://www.w3.org/2000/svg"><path d="M16.48 15.455c.283.282.29.749.007 1.032a.738.738 0 01-1.032-.007l-3.045-3.044a7 7 0 111.026-1.026zM8 14A6 6 0 108 2a6 6 0 000 12z"/></svg><span>Search</span>
                            </a>
                        </li>
                        <li class="c-header__item c-header__item--padding c-header__item--snid-account-widget c-header__item--pipe">

                                <a class="c-header__link eds-c-header__link" id="identity-account-widget" data-track="click_login" data-track-context="header" href='https://idp.nature.com/auth/personal/springernature?redirect_uri=https://www.nature.com/articles/s41598-025-85715-7'><span class="eds-c-header__widget-fragment-title">Log in</span></a>

                        </li>
                    </ul>
                </div>
            </div>
        </div>

            <div class="c-header__row">
                <div class="c-header__container" data-test="navigation-row">
                    <div class="c-header__split">
                        <ul class="c-header__menu c-header__menu--journal">

                                <li class="c-header__item c-header__item--dropdown-menu" data-test="explore-content-button">
                                    <a href="#explore"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--explore"
                                       data-track="click" data-track-action="open explore expander" data-track-label="button">
                                        <span class="c-header__show-text-sm">Content</span>
                                        <span class="c-header__show-text">Explore content</span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>


                                <li class="c-header__item c-header__item--dropdown-menu">
                                    <a href="#about-the-journal"
                                       class="c-header__link"
                                       data-header-expander
                                       data-test="menu-button--about-the-journal"
                                       data-track="click" data-track-action="open about the journal expander" data-track-label="button">
                                        <span>About <span class="c-header__show-text">the journal</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                    </a>
                                </li>

                                    <li class="c-header__item c-header__item--dropdown-menu" data-test="publish-with-us-button">
                                        <a href="#publish-with-us"
                                           class="c-header__link c-header__link--dropdown-menu"
                                           data-header-expander
                                           data-test="menu-button--publish"
                                           data-track="click" data-track-action="open publish with us expander" data-track-label="button">
                                            <span>Publish <span class="c-header__show-text">with us</span></span><svg role="img" aria-hidden="true" focusable="false" height="16" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" transform="matrix(0 1 -1 0 11 3)"/></svg>
                                        </a>
                                    </li>



                        </ul>
                        <ul class="c-header__menu c-header__menu--hide-lg-max">

                                <li class="c-header__item" data-test="alert-link">
                                    <a class="c-header__link"
                                       href="https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41598"
                                       rel="nofollow"
                                       data-track="nav_sign_up_for_alerts"
                                       data-track-action="Sign up for alerts"
                                       data-track-label="link (desktop site header)"
                                       data-track-external>
                                        <span>Sign up for alerts</span><svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#222"/></svg>
                                    </a>
                                </li>


                                <li class="c-header__item c-header__item--pipe">
                                    <a class="c-header__link"
                                       href="https://www.nature.com/srep.rss"
                                       data-track="click"
                                       data-track-action="rss feed"
                                       data-track-label="link">
                                            <span>RSS feed</span>
                                    </a>
                                </li>

                        </ul>
                    </div>
                </div>
            </div>

    </header>




        <nav class="u-mb-16" aria-label="breadcrumbs">
            <div class="u-container">
                <ol class="c-breadcrumbs" itemscope itemtype="https://schema.org/BreadcrumbList">
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:nature"><span itemprop="name">nature</span></a><meta itemprop="position" content="1">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/srep" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:scientific reports"><span itemprop="name">scientific reports</span></a><meta itemprop="position" content="2">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem"><a class="c-breadcrumbs__link"
                               href="/srep/articles?type&#x3D;article" itemprop="item"
                               data-track="click" data-track-action="breadcrumb" data-track-category="header" data-track-label="link:articles"><span itemprop="name">articles</span></a><meta itemprop="position" content="3">
                                    <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10"
                                         xmlns="http://www.w3.org/2000/svg">
                                        <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z"
                                              fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                                    </svg>
                                </li><li class="c-breadcrumbs__item" id="breadcrumb3" itemprop="itemListElement" itemscope itemtype="https://schema.org/ListItem">
                                    <span itemprop="name">article</span><meta itemprop="position" content="4"></li>
                </ol>
            </div>
        </nav>






</div>


<div class="u-container u-mt-32 u-mb-32 u-clearfix" id="content" data-component="article-container"  data-container-type="article">
    <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">


                <div class="c-context-bar u-hide"
                     id="js-enable-context-bar"
                     data-test="context-bar"
                     data-context-bar
                     aria-hidden="true">
                    <div class="c-context-bar__container" data-track-context="sticky banner">
                        <div class="c-context-bar__title">
                            A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models
                        </div>
                        <div class="c-context-bar__cta-container">


        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41598-025-85715-7.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>



                        </div>
                    </div>
                </div>


        <article lang="en">

                <div class="c-pdf-button__container u-mb-8 u-hide-at-lg js-context-bar-sticky-point-mobile">
                    <div class="c-pdf-container" data-track-context="article body">





        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41598-025-85715-7.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>





                    </div>
                </div>

            <div class="c-article-header">
                <header>
                    <ul class="c-article-identifiers" data-test="article-identifier">

        <li class="c-article-identifiers__item" data-test="article-category">Article</li>

        <li class="c-article-identifiers__item">
            <a href="https://www.springernature.com/gp/open-science/about/the-fundamentals-of-open-access-and-open-research" data-track="click" data-track-action="open access" data-track-label="link" class="u-color-open-access" data-test="open-access">Open access</a>
        </li>



                        <li class="c-article-identifiers__item">Published: <time datetime="2025-01-10">10 January 2025</time></li>
                    </ul>

                    <h1 class="c-article-title" data-test="article-title" data-article-title="">A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models</h1>
                    <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="1_5" data-track-context="researcher popup with no profile" href="#auth-Zihan-Song-Aff1" data-author-popup="auth-Zihan-Song-Aff1" data-author-search="Song, Zihan">Zihan Song</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="2_5" data-track-context="researcher popup with no profile" href="#auth-Gyo_Yeob-Hwang-Aff1" data-author-popup="auth-Gyo_Yeob-Hwang-Aff1" data-author-search="Hwang, Gyo-Yeob">Gyo-Yeob Hwang</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="3_5" data-track-context="researcher popup with no profile" href="#auth-Xin-Zhang-Aff1" data-author-popup="auth-Xin-Zhang-Aff1" data-author-search="Zhang, Xin">Xin Zhang</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item c-article-author-list__item--hide-small-screen"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="4_5" data-track-context="researcher popup with no profile" href="#auth-Shan-Huang-Aff1" data-author-popup="auth-Shan-Huang-Aff1" data-author-search="Huang, Shan">Shan Huang</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup> &amp; </li><li class="c-article-author-list__show-more" aria-label="Show all 5 authors for this article" title="Show all 5 authors for this article">…</li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" data-track-index="5_5" data-track-context="researcher popup with no profile" href="#auth-Byung_Kwon-Park-Aff1" data-author-popup="auth-Byung_Kwon-Park-Aff1" data-author-search="Park, Byung-Kwon" data-corresp-id="c1">Byung-Kwon Park<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-mail-medium"></use></svg></a><sup class="u-js-hide"><a href="#Aff1">1</a></sup> </li></ul><button aria-expanded="false" class="c-article-author-list__button"><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-down-medium"></use></svg><span>Show authors</span></button>



                    <p class="c-article-info-details" data-container-section="info">

    <a data-test="journal-link" href="/srep" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">Scientific Reports</i></a>

                        <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 15</b>, Article number: <span data-test="article-number">1608</span> (<span data-test="article-publication-year">2025</span>)
            <a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                    </p>


        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">

                    <li class=" c-article-metrics-bar__item" data-test="access-count">
                        <p class="c-article-metrics-bar__count">7332 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>


                    <li class="c-article-metrics-bar__item" data-test="citation-count">
                        <p class="c-article-metrics-bar__count">5 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>



                        <li class="c-article-metrics-bar__item" data-test="altmetric-score">
                            <p class="c-article-metrics-bar__count">1 <span class="c-article-metrics-bar__label">Altmetric</span></p>
                        </li>



                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__details"><a href="/articles/s41598-025-85715-7/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                    </li>

            </ul>
        </div>


                </header>


    <div class="u-js-hide" data-component="article-subject-links">
        <h3 class="c-article__sub-heading">Subjects</h3>
        <ul class="c-article-subject-list">
            <li class="c-article-subject-list__subject"><a href="/subjects/computer-science" data-track="click" data-track-action="view subject" data-track-label="link">Computer science</a></li><li class="c-article-subject-list__subject"><a href="/subjects/scientific-data" data-track="click" data-track-action="view subject" data-track-label="link">Scientific data</a></li>
        </ul>
    </div>









            </div>

        <div class="c-article-body">
            <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>The exponential growth of scientific articles has presented challenges in information organization and extraction. Automation is urgently needed to streamline literature reviews and enhance insight extraction. We explore the potential of Large Language Models (LLMs) in key-insights extraction from scientific articles, including OpenAI’s GPT-4.0, MistralAI’s Mixtral 8 × 7B, 01AI’s Yi, and InternLM’s InternLM2. We have developed an article-level key-insight extraction system based on LLMs, calling it ArticleLLM. After evaluating the LLMs against manual benchmarks, we have enhanced their performance through fine-tuning. We propose a multi-actor LLM approach, merging the strengths of multiple fine-tuned LLMs to improve overall key-insight extraction performance. This work demonstrates not only the feasibility of LLMs in key-insight extraction, but also the effectiveness of cooperation of multiple fine-tuned LLMs, leading to efficient academic literature survey and knowledge discovery.</p></div></div></section>









            <section aria-labelledby="inline-recommendations" data-title="Inline Recommendations" class="c-article-recommendations" data-track-component="inline-recommendations">
                <h3 class="c-article-recommendations-title" id="inline-recommendations">Similar content being viewed by others</h3>
                <div class="c-article-recommendations-list">

                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">

                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1057%2Fs41599-025-04503-w/MediaObjects/41599_2025_4503_Fig1_HTML.png" loading="lazy" alt=""></div>

                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s41599-025-04503-w?fromPaywallRec=false"
                                           data-track="select_recommendations_1"
                                           data-track-context="inline recommendations"
                                           data-track-action="click recommendations inline - 1"
                                           data-track-label="10.1057/s41599-025-04503-w">Machine-assisted quantitizing designs: augmenting humanities and social sciences with artificial intelligence
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">28 February 2025</span>
                                    </div>
                                </div>
                            </article>
                        </div>

                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">

                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs42256-024-00976-7/MediaObjects/42256_2024_976_Fig1_HTML.png" loading="lazy" alt=""></div>

                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s42256-024-00976-7?fromPaywallRec=false"
                                           data-track="select_recommendations_2"
                                           data-track-context="inline recommendations"
                                           data-track-action="click recommendations inline - 2"
                                           data-track-label="10.1038/s42256-024-00976-7">What large language models know and what people think they know
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>
                                         <span class="c-article-meta-recommendations__access-type">Open access</span>
                                         <span class="c-article-meta-recommendations__date">21 January 2025</span>
                                    </div>
                                </div>
                            </article>
                        </div>

                        <div class="c-article-recommendations-list__item">
                            <article class="c-article-recommendations-card" itemscope itemtype="http://schema.org/ScholarlyArticle">

                                    <div class="c-article-recommendations-card__img"><img src="https://media.springernature.com/w215h120/springer-static/image/art%3A10.1038%2Fs44159-023-00241-5/MediaObjects/44159_2023_241_Fig1_HTML.png" loading="lazy" alt=""></div>

                                <div class="c-article-recommendations-card__main">
                                    <h3 class="c-article-recommendations-card__heading" itemprop="name headline">
                                        <a class="c-article-recommendations-card__link"
                                           itemprop="url"
                                           href="https://www.nature.com/articles/s44159-023-00241-5?fromPaywallRec=false"
                                           data-track="select_recommendations_3"
                                           data-track-context="inline recommendations"
                                           data-track-action="click recommendations inline - 3"
                                           data-track-label="10.1038/s44159-023-00241-5">Using large language models in psychology
                                        </a>
                                    </h3>
                                    <div class="c-article-meta-recommendations" data-test="recommendation-info">
                                        <span class="c-article-meta-recommendations__item-type">Article</span>

                                         <span class="c-article-meta-recommendations__date">13 October 2023</span>
                                    </div>
                                </div>
                            </article>
                        </div>

                </div>
            </section>

            <script>
                window.dataLayer = window.dataLayer || [];
                window.dataLayer.push({
                    recommendations: {
                        recommender: 'semantic',
                        model: 'specter',
                        policy_id: 'NA',
                        timestamp: 1763143805,
                        embedded_user: 'null'
                    }
                });
            </script>




                <div class="main-content">

                        <div class="c-article-section__content c-article-section__content--separator"><p><b>*</b>Correspondence: bpark@dau.ac.kr; Tel.: +82-10-3254-9260.</p></div><section data-title="Introduction"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1">Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>The exponential growth of scientific articles has engendered a complexity in organizing, acquiring, and amalgamating academic information. According to the investigation by Bornmann et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Bornmann, L., Haunschild, R. &amp; Mutz, R. Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. Humanit. Soc. Sci. Commun. 8, 224 (2021)." href="/articles/s41598-025-85715-7#ref-CR1" id="ref-link-section-d31325491e377">1</a></sup>, the overall growth rate of scientific articles stands at 4.1%, doubling every 17 years. The immediate increase of scientific articles has accelerated information overload, hindered the discovery of new insights, and contributed to the potential spread of false information. Although most scientific articles are published in a structured text format to speed up understanding of knowledge, the basic content of these articles remains unstructured text. This means that the literature review is still a time-consuming task that requires manual involvement<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Borah, R., Brown, A. W., Capers, P. L. &amp; Kaiser, K. A. Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the PROSPERO registry. BMJ Open. 7, e012545 (2017)." href="/articles/s41598-025-85715-7#ref-CR2" id="ref-link-section-d31325491e381">2</a></sup>. Therefore, it is necessary to assist the literature review process by automatically extracting key information from scientific articles.</p><p>The automation of key information extraction can be categorized into two classes: metadata extraction and key-insights extraction<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Nasar, Z., Jaffry, S. W. &amp; Malik, M. K. Information extraction from scientific articles: a survey. Scientometrics 117, 1931–1990 (2018)." href="/articles/s41598-025-85715-7#ref-CR3" id="ref-link-section-d31325491e388">3</a></sup>. Metadata extraction encompasses retrieving fundamental attributes from scientific articles, including title, author names, publication year, publishing entity, abstract, and other pertinent foundational details. Researchers and digital repositories use metadata to determine the relevance of specific articles to their fields of interest or to facilitate search and filtering tasks. The key-insight extraction is a summary of the content of the article, such as the problem to solve, the methodology used, evaluation methods, results, limitations, and the future work. Automatic retrieval of those insights will provide researchers with clear and concise concepts of research articles and increase the efficiency of literature reviews.</p><p>Compared with metadata extraction, key-insight extraction is more challenging. The scholarly domain has reported a high degree of accuracy in metadata extraction<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Boukhers, Z. &amp; Bouabdallah, A. Vision and natural language for metadata extraction from scientific PDF documents. In Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries 1–5. &#xA;                  https://doi.org/10.1145/3529372.3533295&#xA;                  &#xA;                 (ACM, 2022)." href="/articles/s41598-025-85715-7#ref-CR4" id="ref-link-section-d31325491e395">4</a></sup>. However, key-insight extraction does not offer an excellent solution because it covers only the parts of the article. Previous studies relied on machine learning technology to extract information at the phrase<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Tateisi, Y., Ohta, T., Miyao, Y., Pyysalo, S. &amp; Aizawa, A. Typed entity and relation annotation on computer science papers. In Proceedings of the 10th International Conference on Language Resources and Evaluation. 3836–3843 (2016)." href="/articles/s41598-025-85715-7#ref-CR5" id="ref-link-section-d31325491e399">5</a></sup> or sentence level<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Kovačević, A., Konjović, Z., Milosavljević, B. &amp; Nenadic, G. Mining methodologies from NLP publications: a case study in automatic terminology recognition. Comput. Speech Lang. 26, 105–126 (2012)." href="/articles/s41598-025-85715-7#ref-CR6" id="ref-link-section-d31325491e403">6</a></sup>. Those approaches have limitations as is that the model struggles to capture complex contexts and semantics at the phrase or sentence level, leading to poor performance in capturing insight. Therefore, another key-insight extraction system is needed to extract key-insights at the section or article level than at the phrase or sentence level. Moreover, the scarcity of annotated training data, the variations among different domains, and the ongoing evolution of research paradigms impede the effectiveness of those models. To our knowledge, there is no effective solution for automatic extraction of key-insights from scientific articles.</p><p>Large Language Models (LLMs) such as GPT-4.0 provide possibilities to solve this challenge. LLMs represent a pioneering advancement in the field of natural language processing, characterized by their colossal neural network architectures, comprising billions to trillions of parameters. These LLMs have emerged as a forefront technology in contemporary artificial intelligence research and application, bearing transformative capabilities in text generation, comprehension, and processing. Bubeck et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Bubeck, S. et al. Sparks of Artificial General Intelligence: Early experiments with GPT-4. Preprint at &#xA;                  http://arxiv.org/abs/2303.12712&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR7" id="ref-link-section-d31325491e410">7</a></sup> reported that GPT-4 has reached near-human performance on a variety of natural language tasks. This opens new opportunities to deeply understand article contents and extract key-insights from them. With their powerful contextual understanding and generation capabilities, LLMs may be able to better capture the details of article contents, enabling more accurate extraction of key-insights. However, there are no studies evaluating the ability of LLMs in key-insights extraction.</p><p>This study aims to develop and evaluate an LLM-based system for extracting key insights from scientific articles. We explore the effectiveness of various state-of-the-art LLMs and enhance their performance through fine-tuning with high-quality datasets. Additionally, we advance their capabilities by constructing a multi-actor system to further improve performance. Specifically, we first employ OpenAI’s ChatGPT-4.0, MistralAI’s Mixtral 8 × 7B, 01AI’s Yi, and InternLM’s InternLM2 respectively as a candidate LLM for extracting key-insights from scientific articles. As the next step, we evaluate the performance of each LLM on key-insight extraction tasks through the manual evaluation. We find that the performance of GPT-4.0 is close to human level. However, it is too expensive to use GPT-4.0 for key-insights extraction on a large scale. Therefore, we use the output of GPT-4 as a label for fine-tuning other open-source LLMs so as to improve their performance. However, the performance of the fine-tuned LLMs still has not reached the peak. Therefore, instead of relying on a single LLM, we present a multi-actor method to merge all the key-insights extracted by multiple fine-tuned open-source LLMs, demonstrating an advancement in the quality of key-insights. As a result, it is shown that we can extract key-insights at article level only using the multiple fine-tuned open-source LLMs.</p></div></div></section><section data-title="Related works"><div class="c-article-section" id="Sec2-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec2">Related works</h2><div class="c-article-section__content" id="Sec2-content"><h3 class="c-article__sub-heading" id="Sec3">Key-insight extraction</h3><p>Key-insight extraction refers to identifying valuable information for research contained in scientific articles. Current research extracts key-insights based on sentences<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Kovačević, A., Konjović, Z., Milosavljević, B. &amp; Nenadic, G. Mining methodologies from NLP publications: a case study in automatic terminology recognition. Comput. Speech Lang. 26, 105–126 (2012)." href="/articles/s41598-025-85715-7#ref-CR6" id="ref-link-section-d31325491e430">6</a></sup> or phrases<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Tateisi, Y., Ohta, T., Miyao, Y., Pyysalo, S. &amp; Aizawa, A. Typed entity and relation annotation on computer science papers. In Proceedings of the 10th International Conference on Language Resources and Evaluation. 3836–3843 (2016)." href="/articles/s41598-025-85715-7#ref-CR5" id="ref-link-section-d31325491e434">5</a></sup>. These researchers extract specific sentences or phrases as key-insights. Sentence-level key-insight extraction can be viewed as a classification task, which classifies the sentences into specific classes. Phrase-level key-insight extraction is more concerned with extracting phrases, or fragments from the text. Key-insight extraction is mainly based on machine learning technology, such as Bayesian classifiers<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Lakhanpal, S., Gupta, A. K. &amp; Agrawal, R. Towards Extracting Domains from Research Publications. In Midwest Artificial Intelligence and Cognitive Science Conference. &#xA;                  https://api.semanticscholar.org/CorpusID:5220521&#xA;                  &#xA;                 (2015)." href="/articles/s41598-025-85715-7#ref-CR8" id="ref-link-section-d31325491e438">8</a></sup>, Conditional Random Fields<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Hirohata, K., Okazaki, N., Ananiadou, S. &amp; Ishizuka, M. Identifying Sections in Scientific Abstracts using Conditional Random Fields. In Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-I. &#xA;                  https://aclanthology.org/I08-1050/&#xA;                  &#xA;                  (2008)." href="/articles/s41598-025-85715-7#ref-CR9" id="ref-link-section-d31325491e442">9</a></sup>, Support Vector Machines<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Ronzano, F. &amp; Saggion, H. Dr. Inventor Framework: Extracting Structured Information from Scientific Publications. In International Conference on Discovery Science (eds. Japkowicz, N. &amp; Matwin, S.) vol. 9356, 209–220. &#xA;                  https://doi.org/10.1007/978-3-319-24282-8&#xA;                  &#xA;                  (Springer International Publishing, 2015)." href="/articles/s41598-025-85715-7#ref-CR10" id="ref-link-section-d31325491e446">10</a></sup>, and Deep Neural Networks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="He, H. et al. An Insight Extraction System on BioMedical Literature with Deep Neural Networks. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing 2691–2701. &#xA;                  https://doi.org/10.18653/v1/D17-1285&#xA;                  &#xA;                 (Association for Computational Linguistics, Stroudsburg, PA, USA, 2017)." href="/articles/s41598-025-85715-7#ref-CR11" id="ref-link-section-d31325491e451">11</a></sup>. Most of those studies are based on extracting key-insights from articles’ abstract. However, those methods have two limitations:</p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>

                      <p>Abstract does not necessarily fully represent the key-insights of the article; for example, the limitations of the research and future research may not exist in the abstract.</p>

                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>

                      <p>The extracted information may not be sufficient to capture the true meaning of the article. In many cases, the key-insights of an article require a broader context synthesis based on the textual summary.</p>

                  </li>
                </ol><p>The extensive understanding of various topics exhibited by LLMs, exemplified by GPT-4, has garnered significant attention across the scientific community. Comprehensive evaluations have been conducted to assess GPT-4’s performance across a multitude of natural language processing tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Polak, M. P. &amp; Morgan, D. Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering. Preprint at &#xA;                  http://arxiv.org/abs/2303.05352&#xA;                  &#xA;                 (2023)." href="#ref-CR12" id="ref-link-section-d31325491e484">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Huang, J. &amp; Tan, M. The role of ChatGPT in scientific communication: writing better scientific review articles. Am. J. Cancer Res. 13, 1148–1154 (2023)." href="#ref-CR13" id="ref-link-section-d31325491e484_1">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Han, R. et al. Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. Preprint at &#xA;                  http://arxiv.org/abs/2305.14450&#xA;                  &#xA;                 (2023)." href="#ref-CR14" id="ref-link-section-d31325491e484_2">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Yuan, C., Xie, Q. &amp; Ananiadou, S. Zero-shot Temporal Relation Extraction with ChatGPT. Preprint at &#xA;                  http://arxiv.org/abs/2304.05454&#xA;                  &#xA;                 (2023)." href="#ref-CR15" id="ref-link-section-d31325491e484_3">15</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Wang, S., Scells, H., Koopman, B. &amp; Zuccon, G. Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search? Preprint at &#xA;                  http://arxiv.org/abs/2302.03495&#xA;                  &#xA;                 (2023)." href="#ref-CR16" id="ref-link-section-d31325491e484_4">16</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Zhang, J., Chen, Y., Niu, N., Wang, Y. &amp; Liu, C. Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting. Preprint at &#xA;                  http://arxiv.org/abs/2304.12562&#xA;                  &#xA;                 (2023)." href="#ref-CR17" id="ref-link-section-d31325491e484_5">17</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Li, B. et al. Evaluating ChatGPT’s Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness. Preprint at &#xA;                  http://arxiv.org/abs/2304.11633&#xA;                  &#xA;                 (2023)." href="#ref-CR18" id="ref-link-section-d31325491e484_6">18</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Sun, W. et al. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. Preprint at &#xA;                  http://arxiv.org/abs/2304.09542&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR19" id="ref-link-section-d31325491e487">19</a></sup>. The results indicate that GPT-4’s performance varies across different tasks. For instance, in such tasks as information retrieval<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="Zhang, J., Chen, Y., Niu, N., Wang, Y. &amp; Liu, C. Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting. Preprint at &#xA;                  http://arxiv.org/abs/2304.12562&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR17" id="ref-link-section-d31325491e491">17</a></sup>, information extraction<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Polak, M. P. &amp; Morgan, D. Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering. Preprint at &#xA;                  http://arxiv.org/abs/2303.05352&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR12" id="ref-link-section-d31325491e495">12</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="Li, B. et al. Evaluating ChatGPT’s Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness. Preprint at &#xA;                  http://arxiv.org/abs/2304.11633&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR18" id="ref-link-section-d31325491e498">18</a></sup>, and text summarization<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="Bubeck, S. et al. Sparks of Artificial General Intelligence: Early experiments with GPT-4. Preprint at &#xA;                  http://arxiv.org/abs/2303.12712&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR7" id="ref-link-section-d31325491e502">7</a></sup>, GPT-4 demonstrates superior performance to traditional models. This could be attributed to GPT-4’s training data encompassing diverse domain knowledge, enabling it to retrieve relevant information effectively from a wide range of languages and document types. However, in the task of relation extraction, GPT-4’s performance falls short of benchmark models. Han et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Han, R. et al. Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. Preprint at &#xA;                  http://arxiv.org/abs/2305.14450&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR14" id="ref-link-section-d31325491e506">14</a></sup> reported that this discrepancy might be attributed to GPT-4’s limited understanding of subject-object relationships within relation extraction tasks.</p><p>In contrast to traditional methods that focus on paragraph-level and sentence-level key-insight extraction, an advantage of LLMs is their capacity to perform full-text level key-insight extraction. This task can be considered a synthesis of text summarization and information extraction. Currently, the use of LLMs for key-insight extraction tasks lacks a systematic evaluation. Existing large datasets intended to evaluate LLMs typically assess their performance in areas such as information extraction<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Jahan, I., Laskar, M. T. R., Peng, C. &amp; Huang, J. Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. Preprint at &#xA;                  http://arxiv.org/abs/2306.04504&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR20" id="ref-link-section-d31325491e513">20</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Kempf, S., Krug, M. &amp; Puppe, F. K. I. E. T. A. Key-insight extraction from scientific tables. Appl. Intell. 53, 9513–9530 (2023)." href="/articles/s41598-025-85715-7#ref-CR21" id="ref-link-section-d31325491e516">21</a></sup>, text summarization<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Laban, P. et al. SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing 9662–9676. &#xA;                  https://aclanthology.org/2023.emnlp-main.600/&#xA;                  &#xA;                 (Association for Computational Linguistics, 2023)." href="/articles/s41598-025-85715-7#ref-CR22" id="ref-link-section-d31325491e520">22</a></sup>, and QA<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Cai, H. et al. SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis. Preprint at &#xA;                  http://arxiv.org/abs/2403.01976&#xA;                  &#xA;                 (2024)." href="/articles/s41598-025-85715-7#ref-CR23" id="ref-link-section-d31325491e524">23</a></sup>. However, the task of key-insight extraction demands that models have the capability to extract and synthesize information from multiple perspectives. Therefore, the performance of LLMs in information extraction, text summarization, and QA cannot be directly equated to their effectiveness in key-insight extraction tasks. Therefore, it is necessary to systematically evaluate the performance of LLMs on key-insight extraction tasks.</p><h3 class="c-article__sub-heading" id="Sec4">Fine-tuning of LLMs</h3><p>The key behind achieving high performance of LLMs lies in the two main stages of the training process: (1) initial pre-training on massive text corpora, endowing the LLM with an expansive grasp of linguistic knowledge and structure; (2) fine-tuning on specific tasks to adapt to distinct domains and applications. This dual-training paradigm equips LLMs with unparalleled adaptability, rendering them instrumental in the modern landscape of natural language processing. Since initial pre-training of LLMs requires a large amount of hardware support, scholars tend to fine-tune the pre-trained LLMs to adapt to tasks in different fields<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Touvron, H. et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. Preprint at &#xA;                  http://arxiv.org/abs/2307.09288&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR24" id="ref-link-section-d31325491e536">24</a></sup>.</p><p>Supervised fine-tuning refers to the process of taking a pre-trained model and adapting it to a new task or dataset by making small adjustments to its parameters<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Howard, J. &amp; Ruder, S. Universal Language Model Fine-tuning for Text Classification. Preprint at &#xA;                  http://arxiv.org/abs/1801.06146&#xA;                  &#xA;                 (2018)." href="/articles/s41598-025-85715-7#ref-CR25" id="ref-link-section-d31325491e543">25</a></sup>. Compared with fine-tuning small-scale models, fine-tuning techniques for LLMs are more complex because of scalability and hardware performance issues. Fine-tuning methods for LLMs are often called PEFT (Parameter-Efficient Fine-Tuning). PEFT methods aim to tweak only a small fraction of the LLM’s parameters, thereby mitigating computational and memory costs. This approach allows for the efficient adaptation of LLMs to specific tasks without the need for extensive resources, making high-quality LLM personalization more accessible.</p><p>LoRA (Low-Rank Adaptation)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Hu, E. et al. Lora: Low-Rank Adaptation of Large Language Models. Preprint at &#xA;                  https://arxiv.org/abs/2106.09685&#xA;                  &#xA;                 (2021)." href="/articles/s41598-025-85715-7#ref-CR26" id="ref-link-section-d31325491e550">26</a></sup> is a PEFT technology that optimizes LLMs like GPT-3 by introducing trainable rank decomposition matrices into their architecture, significantly reducing the number of adjusted parameters while maintaining or improving LLM performance. As a result, LoRA distinguishes itself by providing an optimal balance between LLM efficiency and performance enhancement. By harnessing the power of Low-Rank Adaptation, LoRA enables more nuanced and targeted adjustments to LLMs without the substantial increase in parameters typically associated with such refinements.</p><h3 class="c-article__sub-heading" id="Sec5">Multi-actor approaches in LLM</h3><p>The introduction of multi-actors in LLM systems for improving the performance of LLMs leverages the concept of ensemble learning by coordinating the efforts of multiple AI actors, each embodying an instance of LLM like GPT-4, to act in concert. Through such harmonious interaction, the actors combine their varied knowledge and contextual understanding, addressing intricate challenges with a level of efficiency and inventiveness that exceeds the scope of any single LLM. This transition from individual to collective AI endeavors symbolizes the notion that the aggregate output of an ensemble of actors far exceeds what they could achieve independently.</p><p>Extensive research has been conducted to enhance the performance of large language models (LLMs) for specialized tasks using multi-actor approaches, yet there is no consensus on the optimal way for these actors to collaborate. For instance, employing Dawid-Skene Model to iteratively optimize weights for each actor proves highly effective for tasks where labels are definite<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Fang, C. et al. LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction. Preprint at &#xA;                  http://arxiv.org/abs/2403.00863&#xA;                  &#xA;                 (2024)." href="/articles/s41598-025-85715-7#ref-CR27" id="ref-link-section-d31325491e565">27</a></sup>. However, the multi-actor nature of LLMs significantly increases computational demand<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Jiang, D., Ren, X. &amp; Lin, B. Y. LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. Preprint at &#xA;                  http://arxiv.org/abs/2306.02561&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR28" id="ref-link-section-d31325491e569">28</a></sup>. To mitigate this, a novel routing architecture for multi-actor LLMs based on a reward model has been introduced, which presumes each sub-actor’s proficiency in specific tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Lu, K. et al. Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models. Preprint at &#xA;                  http://arxiv.org/abs/2311.08692&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR29" id="ref-link-section-d31325491e573">29</a></sup>. This method improves both efficiency and accuracy by allocating particular tasks to designated expert actors, similar in philosophy to the popular MoE (Mixture of Experts)<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Jiang, A. Q. et al. Mixtral of Experts. Preprint at &#xA;                  http://arxiv.org/abs/2401.04088&#xA;                  &#xA;                 (2024)." href="/articles/s41598-025-85715-7#ref-CR30" id="ref-link-section-d31325491e577">30</a></sup> model but with greater scalability because it avoids the hard-coding of expert models inherent in MoE architecture.</p><p>In the sphere of practical implementation, the research by Hong et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Hong, S. et al. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. Preprint at &#xA;                  http://arxiv.org/abs/2308.00352&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR31" id="ref-link-section-d31325491e584">31</a></sup> serves as a compelling illustration of the extraordinary capabilities of multi-actor systems. They have adeptly merged human-inspired Standard Operating Procedures (SOPs) with role specialization within an advanced meta-programming architecture, demonstrating how structured cooperation can enhance the performance of LLMs to unprecedented levels. Further, Liang et al.<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Liang, T. et al. Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. Preprint at &#xA;                  http://arxiv.org/abs/2305.19118&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR32" id="ref-link-section-d31325491e588">32</a></sup> have advanced this area by creating a Multi-Agent Debate (MAD) framework, tailor-made to navigate the complexities of intricate reasoning challenges that confront LLMs. This framework provides a systematic arena for agents to participate in deliberative debates, thereby boosting the collective intellectual capacity of the ensemble of agents, showcasing the profound influence that well-orchestrated ensemble strategies can have in transcending the limitations of existing AI paradigms.</p></div></div></section><section data-title="ArticleLLM"><div class="c-article-section" id="Sec6-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec6">ArticleLLM</h2><div class="c-article-section__content" id="Sec6-content"><p>We propose a scientific-article key-insight extraction system, called <i>ArticleLLM</i>, using multi-actor of multiple fine-tuned open-source LLMs. The key-insights we want to extract are the followings: <i>aim of study</i>,<i> motivation of study</i>,<i> problem to solve</i>,<i> method used for solution</i>,<i> evaluation metrics</i>,<i> findings</i>,<i> contributions</i>,<i> limitations</i>,<i> and future work</i>.</p><h3 class="c-article__sub-heading" id="Sec7">Open-source LLMs used for fine-tuning</h3><p>Compared to commercially available proprietary LLMs, fine-tuned and locally deployed open-source LLMs hold advantages in terms of data security, scalability, and cost-effectiveness<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Chen, H. et al. ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up? Preprint at &#xA;                  http://arxiv.org/abs/2311.16989&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR33" id="ref-link-section-d31325491e639">33</a></sup>. Fine-tuning is a pivotal process that enables the LLM to perform better on specific tasks by adjusting the parameters of a pre-trained LLM to fit particular datasets or application contexts<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Huang, H., Qu, Y., Liu, J., Yang, M. &amp; Zhao, T. An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers. Preprint at &#xA;                  http://arxiv.org/abs/2403.02839&#xA;                  &#xA;                 (2024)." href="/articles/s41598-025-85715-7#ref-CR34" id="ref-link-section-d31325491e643">34</a></sup>. Fine-tuning is more resource- and time-efficient than training from scratch, as it leverages the general knowledge acquired by the pre-trained LLM.</p><p>In this study, we use MistralAI’s Mixtral, 01AI’s Yi, and InternLM’s InternLM2 ranked by the transformers framework of Hugging Face<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Wolf, T. et al. HuggingFace’s Transformers: State-of-the-art Natural Language Processing. Preprint at &#xA;                  http://arxiv.org/abs/1910.03771&#xA;                  &#xA;                 (2019)." href="/articles/s41598-025-85715-7#ref-CR35" id="ref-link-section-d31325491e650">35</a></sup> as a candidate open-source LLM for fine-tuning as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/articles/s41598-025-85715-7#Tab1">1</a>. These LLMs all rank high in performance on AlpacaEval leaderboard<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Xuechen L. et al. AlpacaEval: An Automatic Evaluator of Instruction-following Models.  &#xA;                  https://github.com/tatsu-lab/alpaca_eval&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR36" id="ref-link-section-d31325491e657">36</a></sup> and are considered to have high potential for key-insight extraction. Since these LLMs are compatible with the transformers framework of Hugging Face, they can be fine-tuned based on the same set of methods.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Open-source LLMs used for fine-tuning.</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/articles/s41598-025-85715-7/tables/1" aria-label="Full size table 1"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec8">Fine-tuning algorithm</h3><p>We utilize the instruction fine-tuning<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Chung, H. W. et al. Scaling Instruction-Finetuned Language Models. Preprint at &#xA;                  http://arxiv.org/abs/2210.11416&#xA;                  &#xA;                 (2022)." href="/articles/s41598-025-85715-7#ref-CR37" id="ref-link-section-d31325491e793">37</a></sup> method to refine LLMs. This method primarily involves targeted fine-tuning based on specific instructions atop the foundational language model, thereby enhancing the LLM’s capability to comprehend and respond to user commands or inquiries. We fine-tuning the model using Instruction-Response pairs, where the Instructions serve as input data and the Responses are treated as labels. During the fine-tuning phase, the optimization algorithm modifies the LLM’s parameters by calculating the loss function between the predicted outputs and the actual labels.</p><p>To efficiently fine-tune LLMs with minimal performance loss, we employ a 4-bit precision loading approach using the GPTQ algorithm<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Frantar, E., Ashkboos, S., Hoefler, T. &amp; Alistarh, D. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. Preprint at &#xA;                  https://arxiv.org/abs/2210.17323&#xA;                  &#xA;                 (2022)." href="/articles/s41598-025-85715-7#ref-CR38" id="ref-link-section-d31325491e800">38</a></sup>. GPTQ aims to significantly reduce the LLMs’ memory and computational requirements by compressing the weight bit representation from 32 bits to just 4 bits. This reduction not only minimizes the LLM’s size but also enhances its operational speed by treating weights more discretely.</p><p>During the fine-tuning stage, we apply LoRA<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Hu, E. et al. Lora: Low-Rank Adaptation of Large Language Models. Preprint at &#xA;                  https://arxiv.org/abs/2106.09685&#xA;                  &#xA;                 (2021)." href="/articles/s41598-025-85715-7#ref-CR26" id="ref-link-section-d31325491e807">26</a></sup> optimization to adjust the pre-trained LLM for specific tasks efficiently. LoRA operates by selectively modifying a subset of the LLM’s weights using low-rank matrices, maintaining the original structure while enabling swift fine-tuning. This method allows for precise adjustment without retraining the full network. We adopt Adamw<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Ilya L. &amp; Frank H. Decoupled Weight Decay Regularization. Preprint at &#xA;                  https://arxiv.org/abs/1711.05101&#xA;                  &#xA;                 (2019)." href="/articles/s41598-025-85715-7#ref-CR39" id="ref-link-section-d31325491e811">39</a></sup> for optimization, which optimizes learning rates based on gradient moments, ensuring a balance between task-specific performance enhancement and generalizability. We use the default cross-entropy as the loss function because it can effectively handle the multi-class label prediction problem and ensure the accuracy of the probability distribution of the model output. We set the train epoch to 1 to avoid overfitting. The parameters governing the LoRA adaptation and the overall fine-tuning process are carefully selected as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/articles/s41598-025-85715-7#Tab2">2</a> to balance between refining the LLM’s performance on specific tasks and maintaining its generalized capabilities.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-2"><figure><figcaption class="c-article-table__figcaption"><b id="Tab2" data-test="table-caption">Table 2 Fine-tuning parameters.</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/articles/s41598-025-85715-7/tables/2" aria-label="Full size table 2"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec9">Data set used and preparation for fine-tuning</h3><p>The data set used in this study comes from the PDF files of arXiv<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Paul Ginsparg. ArXiv. &#xA;                  https://arxiv.org/&#xA;                  &#xA;                 (1991)." href="/articles/s41598-025-85715-7#ref-CR40" id="ref-link-section-d31325491e1033">40</a></sup> public articles having the keyword of <i>healthcare</i>. The academic consensus suggests that the optimal dataset size for fine-tuning LLMs lies between 1,000 and 10,000 dialogue entries<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Bakker, M. A. et al. Fine-tuning language models to find agreement among humans with diverse preferences. Preprint at &#xA;                  https://arxiv.org/abs/2211.15006&#xA;                  &#xA;                 (2022)." href="/articles/s41598-025-85715-7#ref-CR41" id="ref-link-section-d31325491e1040">41</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Hu, Z. et al. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. Preprint at &#xA;                  http://arxiv.org/abs/2304.01933&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR42" id="ref-link-section-d31325491e1043">42</a></sup>, most of which consist of dialogue sentences of a few hundred words. However, given that the article data involved in key-insight extraction tasks are often dozens of times longer than these dialogue entries, there is a significant increase in training time costs. Therefore, we choose to use 1,000 articles as our training dataset. Among them, following the usual practice, we set the size of training data set to 700 articles.</p><p>Researchers have confirmed that long articles can reduce the performance of LLMs<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Liu, N. F. et al. Lost in the Middle: How Language Models Use Long Contexts. Preprint at &#xA;                  http://arxiv.org/abs/2307.03172&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR43" id="ref-link-section-d31325491e1050">43</a></sup>. To fully exploit the performance of a LLM, the length of the input text must be reduced. Moreover, a long article will require a large amount of GPU memory during the fine-tuning phase. Therefore, we selected short-length articles whose number of words are less than 7000 to form the dataset.</p><p>Converting the PDF format of an article into the text format that can be processed by LLMs is a prerequisite for executing subsequent algorithms. We use Tika-python<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Chris A. M. et al. Tika-Python. &#xA;                  https://github.com/chrismattmann/tika-python&#xA;                  &#xA;                 (2014)." href="/articles/s41598-025-85715-7#ref-CR44" id="ref-link-section-d31325491e1057">44</a></sup> to convert PDF format files into string data. Tika-python returns the string data contained in the entire PDF file. Since the string data may contain some useless characters and symbols, we also use Python’s pySBD<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Sadvilkar, N. &amp; Neumann, M. PySBD: Pragmatic Sentence Boundary Disambiguation. Preprint at &#xA;                  http://arxiv.org/abs/2010.09657&#xA;                  &#xA;                 (2020)." href="/articles/s41598-025-85715-7#ref-CR45" id="ref-link-section-d31325491e1061">45</a></sup> library to filter out them.</p><p>To fine-tune the open-source LLMs in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/articles/s41598-025-85715-7#Tab1">1</a>, the dataset is organized into the following structure: Instruction and Response. Instruction is an order given to the LLM, telling the LLM what information to extract and in what format to return it. We showed in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/articles/s41598-025-85715-7#Tab3">3</a> the instruction used for extracting key-insights from the article text. Response is the result expected to be returned by the LLM. In this supervised fine-tuning stage, the response represents key-insights of the article. We used the key-insights generated by GPT-4 as a label for the response.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-3"><figure><figcaption class="c-article-table__figcaption"><b id="Tab3" data-test="table-caption">Table 3 Instruction for extracting key-insights.</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/articles/s41598-025-85715-7/tables/3" aria-label="Full size table 3"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec10">Multi-actor of fine-tuned LLMs</h3><p>The multi-actor of the fine-tuned LLMs operates on the independently obtained response of each fine-tuned LLM: Mixtral FT, Yi FT, and InternLM2 FT. Each LLM is tasked with extracting key-insights according to the structured instruction as showcased in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/articles/s41598-025-85715-7#Tab3">3</a>. Following the initial extraction phase, each output from each LLM is subjected to a synthesis process. For this step, we harness the capability of one of the LLMs as the centerpiece for integrating diverse perspectives and insights generated by the other LLMs.</p><p>Specifically, we used InternLM2 to summarize the information from the output of each LLM FT using the instructions as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/articles/s41598-025-85715-7#Tab4">4</a>. To ensures that InternLM2 is optimally tailored to effectively integrate and summarize the outputs of the LLM FTs, we fine-tuned the InternLM2 using the dataset of 1,000 entities randomly selected from the key-insight outputs of Mixtral FT, Yi FT, InternLM2 FT, and GPT-4. Among them, the key-insight sentences extracted by Mixtral FT, Yi FT, and InternLM2 FT are used as input, and the key-insight sentences extracted by GPT-4 are used as labels.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-4"><figure><figcaption class="c-article-table__figcaption"><b id="Tab4" data-test="table-caption">Table 4 Instruction for summarizing the key-insights from each fine-tuned LLM.</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/articles/s41598-025-85715-7/tables/4" aria-label="Full size table 4"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div></div></div></section><section data-title="Performance evaluation"><div class="c-article-section" id="Sec11-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec11">Performance evaluation</h2><div class="c-article-section__content" id="Sec11-content"><h3 class="c-article__sub-heading" id="Sec12">Evaluation metrics</h3><p>We use the following three metrics to evaluate the performance of ArticleLLM on the key-insights extraction task:</p><ul class="u-list-style-bullet">
                  <li>
                    <p><i>Manual evaluation</i> Manual evaluation is considered the gold standard for key-insight extraction tasks<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Nasar, Z., Jaffry, S. W. &amp; Malik, M. K. Information extraction from scientific articles: a survey. Scientometrics 117, 1931–1990 (2018)." href="/articles/s41598-025-85715-7#ref-CR3" id="ref-link-section-d31325491e1347">3</a></sup>. However, its high cost limits its application on large datasets. We use manual evaluation to measure the performance of the LLM in a small article dataset. Because the key-insight extraction task involves explicit goals and objectives, we use a relevance score to assess how accurately the LLM extracts key-insights. The relevance score ranges from 0 to 1, where 0 indicates ‘completely irrelevant’ and 1, ‘completely relevant’. Specifically, human researchers assess whether the key-insights extracted by the LLM comprehensively capture all critical information points identified in manually annotated references, evaluating the presence or absence of essential details or elements. To reduce the subjective errors caused by humans, two researchers independently assessed the results and re-evaluated the contradictory parts.</p>
                  </li>
                  <li>
                    <p><i>GPT-4 score</i> Through carefully designed instructions as shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/articles/s41598-025-85715-7#Tab5">5</a>, GPT-4 scores the semantic similarity of the extracted key-insights. The score ranges from 0 to 100, where 0 indicates ‘completely dissimilar’ and 100, ‘completely similar’. The key-insights generated by an open source LLM are evaluated by the key-insights generated by GPT-4. Because, compared to Bleu score, GPT-4 is considered to better capture the deep semantic similarity between texts, GPT-4 score has been widely used in LLM evaluation<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Zheng, L. et al. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Preprint at &#xA;                  http://arxiv.org/abs/2306.05685&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR46" id="ref-link-section-d31325491e1362">46</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Gao, M., Hu, X., Ruan, J., Pu, X. &amp; Wan, X. LLM-based NLG Evaluation: Current Status and Challenges. Preprint at &#xA;                  http://arxiv.org/abs/2402.01383&#xA;                  &#xA;                 (2024)." href="/articles/s41598-025-85715-7#ref-CR47" id="ref-link-section-d31325491e1365">47</a></sup>.</p>
                  </li>
                  <li>
                    <p><i>Vector similarity</i> Vector similarity quantifies the semantic similarity between two sentences using the cosine similarity between their vector representations<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Farouk, M. Measuring Sentences Similarity: A Survey. Preprint at &#xA;                  http://arxiv.org/abs/1910.03940&#xA;                  &#xA;                 (2019)." href="/articles/s41598-025-85715-7#ref-CR48" id="ref-link-section-d31325491e1377">48</a></sup>. In this study, vector representations of each key-insight are produced using the Sentence Transformer model, all-MiniLM-L6-v2<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Reimers, N. &amp; Gurevych, I. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Preprint at &#xA;                  http://arxiv.org/abs/1908.10084&#xA;                  &#xA;                 (2019)." href="/articles/s41598-025-85715-7#ref-CR49" id="ref-link-section-d31325491e1381">49</a></sup>. The similarity scores are normalized to a scale ranging from 0 to 100, where 0 indicates ’completely dissimilar’ and 100 denotes ’completely similar’.</p>
                  </li>
                </ul><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-5"><figure><figcaption class="c-article-table__figcaption"><b id="Tab5" data-test="table-caption">Table 5 Instruction for semantic similarity.</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/articles/s41598-025-85715-7/tables/5" aria-label="Full size table 5"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec13">Evaluation results</h3><p>For evaluation test, we used a Linux server with 4 Nvidia TITAN RTX GPUs. The operating system is CentOS7. The CPU is an Intel (R) Xeon (R) Silver 4114 CPU @ 2.20 GHz with 40 cores. We used a total of 300 articles for test dataset. In terms of software, we employ Python’s Transformers library<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Wolf, T. et al. HuggingFace’s Transformers: State-of-the-art Natural Language Processing. Preprint at &#xA;                  http://arxiv.org/abs/1910.03771&#xA;                  &#xA;                 (2019)." href="/articles/s41598-025-85715-7#ref-CR35" id="ref-link-section-d31325491e1473">35</a></sup> as the foundational framework for fine-tuning and inference processes. We utilize the PEFT library<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Mangrulkar, S. et al. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. &#xA;                  https://github.com/huggingface/peft&#xA;                  &#xA;                 (2022)." href="/articles/s41598-025-85715-7#ref-CR50" id="ref-link-section-d31325491e1477">50</a></sup> to perform parameter efficient fine-tuning.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec14">Manual performance comparison of LLMs before fine-tuning</h4><p>The manual evaluation results for the key-insight extraction performances of the LLMs before fine-tuning are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/articles/s41598-025-85715-7#Tab6">6</a>, using a total of 34 articles for test dataset. GPT-4 demonstrates superior efficacy across all key-insights, achieving an average score of 0.97. This is followed by InternLM2, which exhibits commendable performance with an average score of 0.80, showcasing its potential in extracting key-insights from scientific literature. Conversely, Yi and Mixtral lag slightly behind, with average scores of 0.65 and 0.60, respectively. Notably, GPT-4 achieves perfect scores in understanding the aim, motivation, evaluation metrics, and contribution of scientific articles, underscoring its advanced capability in discerning intricate academic content.</p><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-6"><figure><figcaption class="c-article-table__figcaption"><b id="Tab6" data-test="table-caption">Table 6 Manual performance evaluation for LLMS before fine-tuning.</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/articles/s41598-025-85715-7/tables/6" aria-label="Full size table 6"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec15">Performance comparison of fine-tuned LLMs</h4><p>The performance comparison results are presented in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-025-85715-7#Fig1">1</a>, and the numerical values of the results are shown in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/articles/s41598-025-85715-7#Tab7">7</a>. With respect to the GPT-4 score after fine-tuning, both Yi FT and Mixtral FT exhibited slight improvements, with their average scores increasing from 68.5 to 71.4 and from 49.4 to 51.6, respectively. InternLM2 and its fine-tuned counterpart, InternLM2 FT, stood out for their exceptional performance, with InternLM2 FT achieving the highest average score of 77.8. Unlike GPT-4 scores, the vector similarity scores show less variation, suggesting this metric evaluates a different aspect. Specifically, after fine-tuning, Yi FT and Mixtral FT showed marginal improvements in their scores, shifting from 64.2 to 65.8 and from 63.5 to 63.9 respectively, showcasing slight but noticeable progress. InternLM2 and its fine-tuned version, InternLM2 FT, delivered the highest scores among the individual models at 68.8, indicating a stronger semantic alignment.</p><p>On the other hand, the underperformance of all open-source LLMs in extracting “Evaluation metrics” and “Limitations” can be principally attributed to two specific challenges. First, the task of extracting “Evaluation metrics” often involves identifying multiple distinct indicators such as accuracy, precision, and recall, which may be intricately described within the text. Open-source LLMs may struggle with this multifaceted extraction, leading to the omission of certain indicators that are critical for a comprehensive evaluation. Second, the extraction of “Limitations” presents its unique set of difficulties, as not all authors explicitly mention limitations within their articles. This situation forces open-source LLMs to attempt the synthesis of plausible yet not entirely accurate limitations, which can deviate significantly from the article’s intended messages. These challenges highlight the nuanced understanding and contextual interpretation required for accurately extracting such sophisticated elements from scientific texts, thereby suggesting the need for LLMs to be specifically fine-tuned or trained with a focus on recognizing and handling the diverse and complex nature of “Evaluation metrics” and “Limitations” in scientific literature.</p><p>The multi-actor approach consistently outperforms other models in both metrics, though the margin is narrower in vector similarity, suggesting its overall superior linguistic and cognitive capabilities. Specifically, the multi-actor approach effectively leverages the combined strengths of three fine-tuned LLMs (InternLM2 FT, Yi FT, and Mixtral FT). This collaborative strategy significantly enhances its ability to extract key-insights from scientific articles, as illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-025-85715-7#Fig1">1</a> which summarizes performance across nine essential categories. This underscores the synergistic effect of leveraging multiple fine-tuned LLMs, which collectively enhance the precision and breadth of extracted key-insights, surpassing the capabilities of individual LLMs. Moreover, the lower score of “Limitations” compared to other categories suggests that while multi-actor approach significantly improves performance, identifying areas requiring further refinement remains critical. Collectively, these results solidify the position of multi-actor LLMs as a powerful tool in comprehensively understanding and analyzing the complexities of scientific literature, outperforming singular fine-tuned LLMs in both depth and accuracy of extracted information.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Fig. 1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41598-025-85715-7/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-025-85715-7/MediaObjects/41598_2025_85715_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-025-85715-7/MediaObjects/41598_2025_85715_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="345"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>(<b>a</b>) LLM performance evaluated by GPT-4. (<b>b</b>) LLM performance evaluated by and vector similarity.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41598-025-85715-7/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-7"><figure><figcaption class="c-article-table__figcaption"><b id="Tab7" data-test="table-caption">Table 7 The numerical results of LLM performance evaluated by GPT-4.</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/articles/s41598-025-85715-7/tables/7" aria-label="Full size table 7"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec16">Statistical significance of results</h4><p>Given that the observed performances are so close to each other, we need to see if the observed differences are statistically significant. Considering that our data have natural boundaries which may lead to a skewed distribution, we implemented the Wilcoxon Signed-Rank Test<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Wilcoxon, F. Individual comparisons by ranking methods. Biometrics Bull. 1, 80 (1945)." href="/articles/s41598-025-85715-7#ref-CR51" id="ref-link-section-d31325491e2870">51</a></sup> to compare the median differences between LLMs. This non-parametric method is more appropriate due to the potential non-normality of the data caused by the boundaries. Our null hypothesis states that there would be no significant difference in GPT-4 score or vector similarity, whereas the alternative hypothesis anticipates a noticeable difference. Supplementary Table 1 presents the results of the Wilcoxon Signed-Rank Test for all LLMs. Within the realm of GPT-4 scores, all fine-tuned models exhibit significant differences in certain key-insights (<i>p</i> &lt; 0.05). InternLM2 FT shows significance in aim, methods, question addressed, evaluation metrics, findings, limitations, and future work; Yi FT in aim, question addressed, and findings; Mixtral FT in aim, motivation, question addressed, and contribution. In particular, the multi-actor approach shows significance in all key-insights.</p><p>However, the results of vector similarity differ from those of GPT-4, with no significant statistical differences observed across all key-insights for Mixtral FT and InternLM2 FT. Yi FT shows significance only in aim, methods, and findings. This may be due to the fact that vector similarity is less sensitive to semantic variations. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-025-85715-7#Fig1">1</a>b illustrates this point by demonstrating that the variance of vector similarity between different models is lower, resulting in closer curves. Nevertheless, the multi-actor approach shows significance in all key-insights except aim. This reveals that the multi-actor approach significantly improves the key-insight extraction performance. Overall, the results of statistical tests highlight the significant improvements in key-insight extraction made by the multi-actor approach, thereby validating its effectiveness.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec17">Repeated testing of GPT-4 score</h4><p>Considering the potential variability in GPT-4 score outputs, where identical instructions may yield varying results<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Kocoń, J. et al. Jack of all trades, master of none. Inf. Fusion. 99, 101861 (2023)." href="/articles/s41598-025-85715-7#ref-CR52" id="ref-link-section-d31325491e2892">52</a></sup>, we conducted repeated experiments. As illustrated in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/articles/s41598-025-85715-7#Fig2">2</a>, we performed 30 replicates of testing on the GPT-4 scores across all key-insights. The mean of GPT-4 scores for 30 repeated tests is 88.4 ± 0.63. This result shows that while there is some variation in GPT-4 scores across repeated tests, this variation has minimal impact on the overall outcome.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Fig. 2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/articles/s41598-025-85715-7/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-025-85715-7/MediaObjects/41598_2025_85715_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-025-85715-7/MediaObjects/41598_2025_85715_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="334"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>Repeat testing for GPT-4 score.</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/articles/s41598-025-85715-7/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-chevron-right-small"></use></svg></a></div></figure></div></div></div></section><section data-title="Discussion"><div class="c-article-section" id="Sec18-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec18">Discussion</h2><div class="c-article-section__content" id="Sec18-content"><p>The findings from this study underscore the effectiveness of fine-tuned LLMs for the extraction of key-insights from scientific articles. Notably, the fine-tuned version of the InternLM2, InternLM2 FT, emerged as a standout performer, evidencing significant performance enhancements post-fine-tuning, thus directly evidencing the benefits of fine-tuning. This enhancement in performance, particularly in terms of understanding and articulating critical scientific information, affirms the pivotal role of fine-tuning in optimizing LLMs for specialized academic tasks.</p><p>The multi-actor of LLMs showcases their superiority in handling complex scientific texts by integrating the strengths of the three individually fine-tuned LLMs—Mixtral FT, Yi FT, and InternLM2 FT—each bringing different nuanced insights to the output. This multi-faceted approach ensures a more comprehensive analysis than what any single LLM could achieve. The synthesis process, led by InternLM2 FT, exemplifies a sophisticated method of consolidating diverse perspectives into a coherent, analytically rich summary. This strategy significantly elevates the benchmark for automated text analysis, offering unparalleled precision and depth in extracting key-insights from dense academic literature, and highlights its immense potential for advancing literature review and analysis. This development aligns with recent studies emphasizing the cooperation of multiple LLMs in improving the performance of LLM applications across various fields<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Hong, S. et al. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. Preprint at &#xA;                  http://arxiv.org/abs/2308.00352&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR31" id="ref-link-section-d31325491e2932">31</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Liang, T. et al. Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. Preprint at &#xA;                  http://arxiv.org/abs/2305.19118&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR32" id="ref-link-section-d31325491e2935">32</a></sup>. It is a kind of collective intelligence, where diverse sources of information lead to more robust and reliable conclusions, a concept widely supported in the interdisciplinary research community.</p><p>Distinct from the existing multi-actor approaches<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Fang, C. et al. LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction. Preprint at &#xA;                  http://arxiv.org/abs/2403.00863&#xA;                  &#xA;                 (2024)." href="#ref-CR27" id="ref-link-section-d31325491e2942">27</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Jiang, D., Ren, X. &amp; Lin, B. Y. LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. Preprint at &#xA;                  http://arxiv.org/abs/2306.02561&#xA;                  &#xA;                 (2023)." href="#ref-CR28" id="ref-link-section-d31325491e2942_1">28</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Lu, K. et al. Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models. Preprint at &#xA;                  http://arxiv.org/abs/2311.08692&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR29" id="ref-link-section-d31325491e2945">29</a></sup>, the approach for extracting key-insights from scientific articles merges the unique viewpoints from disparate models. Consequently, such customary solutions as weight updating<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Fang, C. et al. LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction. Preprint at &#xA;                  http://arxiv.org/abs/2403.00863&#xA;                  &#xA;                 (2024)." href="/articles/s41598-025-85715-7#ref-CR27" id="ref-link-section-d31325491e2949">27</a></sup> and expert routing<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Lu, K. et al. Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models. Preprint at &#xA;                  http://arxiv.org/abs/2311.08692&#xA;                  &#xA;                 (2023)." href="/articles/s41598-025-85715-7#ref-CR29" id="ref-link-section-d31325491e2953">29</a></sup> prove ineffective for this task. Our approach addresses this by fine-tuning a specialized actor tasked with integrating and summarizing information from other actors. While this method may not achieve the execution efficiency of expert routing techniques, it significantly enhances the accuracy of key-insights extraction, ensuring maximal fidelity to the source article.</p><p>The results of this study also suggest an insight: the inherent abilities of the original LLMs are more important than the enhancements brought about by fine-tuning. Despite the tangible improvements observed by post-fine-tuning, the baseline performance of an LLM such as InternLM2 sets a precedential standard for excellence in the domain of key-insight extraction. This denotes that the foundational architecture and pre-training of these LLMs may be more pivotal in determining their effectiveness in complex academic tasks, overshadowing the incremental gains achieved through fine-tuning. Specifically, the fact that GPT-4 and InternLM2 demonstrated superior efficacy across various dimensions before fine-tuning, highlights the critical importance of the LLM’s original design and training corpus in grasping intricate academic content. Therefore, while fine-tuning serves as a valuable tool for LLM optimization, the selection of the base LLM is crucial for success in such specialized applications as key-insights extraction.</p><p>An advantage of ArticleLLM is its capability for local deployment, which facilitates the construction of a private large-scale literature management system. This feature is particularly crucial for applications that emphasize data confidentiality and security. Another benefit of ArticleLLM is its cost-effectiveness. Extracting key-insights from an article of approximately 10,000 words using GPT-4 costs about $0.15. This expense can prove substantial for building an extensive private literature management system. In contrast, a locally deployed ArticleLLM eliminates those concerns. By the way, the approach of multi-actor of LLMs proposed in this study requires about 10 min to extract key-insights from an article under our hardware setup. However, the server used in this research does not employ NVLink technology but rather relies on PCIe (Peripheral Component Interconnect Express) for data transmission, indicating significant room for improvement in system performance efficiency<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Li, A. et al. Evaluating Modern PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect. IEEE Trans. Parallel Distrib. Syst. 31, 94–110 (2020)." href="/articles/s41598-025-85715-7#ref-CR53" id="ref-link-section-d31325491e2964">53</a></sup>.</p><p>Compared with previous research<sup><a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Nasar, Z., Jaffry, S. W. &amp; Malik, M. K. Information extraction from scientific articles: a survey. Scientometrics 117, 1931–1990 (2018)." href="/articles/s41598-025-85715-7#ref-CR3" id="ref-link-section-d31325491e2971">3</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Tateisi, Y., Ohta, T., Miyao, Y., Pyysalo, S. &amp; Aizawa, A. Typed entity and relation annotation on computer science papers. In Proceedings of the 10th International Conference on Language Resources and Evaluation. 3836–3843 (2016)." href="/articles/s41598-025-85715-7#ref-CR5" id="ref-link-section-d31325491e2974">5</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Lakhanpal, S., Gupta, A. K. &amp; Agrawal, R. Towards Extracting Domains from Research Publications. In Midwest Artificial Intelligence and Cognitive Science Conference. &#xA;                  https://api.semanticscholar.org/CorpusID:5220521&#xA;                  &#xA;                 (2015)." href="/articles/s41598-025-85715-7#ref-CR8" id="ref-link-section-d31325491e2977">8</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Ronzano, F. &amp; Saggion, H. Dr. Inventor Framework: Extracting Structured Information from Scientific Publications. In International Conference on Discovery Science (eds. Japkowicz, N. &amp; Matwin, S.) vol. 9356, 209–220. &#xA;                  https://doi.org/10.1007/978-3-319-24282-8&#xA;                  &#xA;                  (Springer International Publishing, 2015)." href="/articles/s41598-025-85715-7#ref-CR10" id="ref-link-section-d31325491e2980">10</a></sup> on key-insight extraction, the advantage of ArticleLLM is that it can process the entire article to obtain more reasonable results, demonstrating the feasibility of LLMs for extracting key-insights from articles. ArticleLLM could revolutionize the way academic databases curate and present information, allowing for a more nuanced and efficient retrieval process. Researchers could benefit from contextually relevant summaries, thereby significantly enhancing their research productivity and knowledge discovery.</p><p>There are some limitations worth mentioning. Although the multi-actor system based on multiple fine-tuned LLMs has achieved excellent performance, the restriction to articles under 7,000 words due to GPU memory limits and the focus on healthcare-related articles from arXiv may affect the generalizability of our findings. Additionally, the execution efficiency of the system is still a concern due to the collaboration of multiple LLMs involved. The extraction of key-insights such as “limitations” that may not exist in the article or appear in an ambiguous form requires further research. Compared to the arXiv papers used in this study, more rigorous peer-reviewed papers may avoid this problem. On the other hand, due to the hardware limitations, this study employed 4-bit quantization to fine-tune the LLMs, inevitably leading to a performance loss.</p></div></div></section><section data-title="Conclusion"><div class="c-article-section" id="Sec19-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec19">Conclusion</h2><div class="c-article-section__content" id="Sec19-content"><p>In this paper, it is shown that we can extract key-insights from scientific articles only using open-source LLMs. Our findings affirm the critical role of fine-tuning in enhancing the proficiency of LLMs for extracting key-insights, with InternLM2 FT showcasing remarkable improvements after fine-tuning. In addition, the multi-actor of LLMs, incorporating diverse perspectives from individually fine-tuned LLMs, significantly broadens the scope and accuracy of resulting key-insights. However, the inherent architecture and pre-training of LLMs seem to be more influential than fine-tuning in determining their efficacy for extracting key-insights Looking forward, ArticleLLM can present a promising avenue for enhancing academic research productivity, though such challenges as execution efficiency of ensemble systems warrant further exploration to optimize the utilization of LLMs in scholarly applications.</p></div></div></section>

                </div>


            <div class="u-mt-32">
                <section data-title="Data availability"><div class="c-article-section" id="data-availability-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="data-availability">Data availability</h2><div class="c-article-section__content" id="data-availability-content">

              <p>The datasets generated during and/or analysed during the current study are available from the corresponding author on reasonable request.</p>
            </div></div></section><div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference" data-track-context="references section"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">Bornmann, L., Haunschild, R. &amp; Mutz, R. Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. <i>Humanit. Soc. Sci. Commun.</i> <b>8</b>, 224 (2021).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1057/s41599-021-00903-w" data-track-item_id="10.1057/s41599-021-00903-w" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1057%2Fs41599-021-00903-w" aria-label="Article reference 1" data-doi="10.1057/s41599-021-00903-w">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1327.11037" aria-label="MATH reference 1">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Growth%20rates%20of%20modern%20science%3A%20a%20latent%20piecewise%20growth%20curve%20approach%20to%20model%20publication%20numbers%20from%20established%20and%20new%20literature%20databases&amp;journal=Humanit.%20Soc.%20Sci.%20Commun.&amp;doi=10.1057%2Fs41599-021-00903-w&amp;volume=8&amp;publication_year=2021&amp;author=Bornmann%2CL&amp;author=Haunschild%2CR&amp;author=Mutz%2CR">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Borah, R., Brown, A. W., Capers, P. L. &amp; Kaiser, K. A. Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the PROSPERO registry. <i>BMJ Open.</i> <b>7</b>, e012545 (2017).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1136/bmjopen-2016-012545" data-track-item_id="10.1136/bmjopen-2016-012545" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1136%2Fbmjopen-2016-012545" aria-label="Article reference 2" data-doi="10.1136/bmjopen-2016-012545">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=28242767" aria-label="PubMed reference 2">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC5337708" aria-label="PubMed Central reference 2">PubMed Central</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 2" href="http://scholar.google.com/scholar_lookup?&amp;title=Analysis%20of%20the%20time%20and%20workers%20needed%20to%20conduct%20systematic%20reviews%20of%20medical%20interventions%20using%20data%20from%20the%20PROSPERO%20registry&amp;journal=BMJ%20Open.&amp;doi=10.1136%2Fbmjopen-2016-012545&amp;volume=7&amp;publication_year=2017&amp;author=Borah%2CR&amp;author=Brown%2CAW&amp;author=Capers%2CPL&amp;author=Kaiser%2CKA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">Nasar, Z., Jaffry, S. W. &amp; Malik, M. K. Information extraction from scientific articles: a survey. <i>Scientometrics</i> <b>117</b>, 1931–1990 (2018).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s11192-018-2921-5" data-track-item_id="10.1007/s11192-018-2921-5" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s11192-018-2921-5" aria-label="Article reference 3" data-doi="10.1007/s11192-018-2921-5">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1495.47003" aria-label="MATH reference 3">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 3" href="http://scholar.google.com/scholar_lookup?&amp;title=Information%20extraction%20from%20scientific%20articles%3A%20a%20survey&amp;journal=Scientometrics&amp;doi=10.1007%2Fs11192-018-2921-5&amp;volume=117&amp;pages=1931-1990&amp;publication_year=2018&amp;author=Nasar%2CZ&amp;author=Jaffry%2CSW&amp;author=Malik%2CMK">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Boukhers, Z. &amp; Bouabdallah, A. Vision and natural language for metadata extraction from scientific PDF documents. In <i>Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries</i> 1–5. <a href="https://doi.org/10.1145/3529372.3533295" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1145/3529372.3533295">https://doi.org/10.1145/3529372.3533295</a> (ACM, 2022).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Tateisi, Y., Ohta, T., Miyao, Y., Pyysalo, S. &amp; Aizawa, A. Typed entity and relation annotation on computer science papers. In <i>Proceedings of the 10th International Conference on Language Resources and Evaluation.</i> 3836–3843 (2016).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">Kovačević, A., Konjović, Z., Milosavljević, B. &amp; Nenadic, G. Mining methodologies from NLP publications: a case study in automatic terminology recognition. <i>Comput. Speech Lang.</i> <b>26</b>, 105–126 (2012).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.csl.2011.09.001" data-track-item_id="10.1016/j.csl.2011.09.001" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.csl.2011.09.001" aria-label="Article reference 6" data-doi="10.1016/j.csl.2011.09.001">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1009.68043" aria-label="MATH reference 6">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 6" href="http://scholar.google.com/scholar_lookup?&amp;title=Mining%20methodologies%20from%20NLP%20publications%3A%20a%20case%20study%20in%20automatic%20terminology%20recognition&amp;journal=Comput.%20Speech%20Lang.&amp;doi=10.1016%2Fj.csl.2011.09.001&amp;volume=26&amp;pages=105-126&amp;publication_year=2012&amp;author=Kova%C4%8Devi%C4%87%2CA&amp;author=Konjovi%C4%87%2CZ&amp;author=Milosavljevi%C4%87%2CB&amp;author=Nenadic%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">Bubeck, S. et al. Sparks of Artificial General Intelligence: Early experiments with GPT-4. Preprint at <a href="http://arxiv.org/abs/2303.12712" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2303.12712">http://arxiv.org/abs/2303.12712</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Lakhanpal, S., Gupta, A. K. &amp; Agrawal, R. Towards Extracting Domains from Research Publications. In <i>Midwest Artificial Intelligence and Cognitive Science Conference.</i> <a href="https://api.semanticscholar.org/CorpusID:5220521" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://api.semanticscholar.org/CorpusID:5220521">https://api.semanticscholar.org/CorpusID:5220521</a> (2015).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Hirohata, K., Okazaki, N., Ananiadou, S. &amp; Ishizuka, M. Identifying Sections in Scientific Abstracts using Conditional Random Fields. In <i>Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-I.</i> <a href="https://aclanthology.org/I08-1050/" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://aclanthology.org/I08-1050/">https://aclanthology.org/I08-1050/</a>  (2008).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Ronzano, F. &amp; Saggion, H. Dr. Inventor Framework: Extracting Structured Information from Scientific Publications. In<i> International Conference on Discovery Science</i> (eds. Japkowicz, N. &amp; Matwin, S.) vol. 9356, 209–220. <a href="https://doi.org/10.1007/978-3-319-24282-8" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.1007/978-3-319-24282-8">https://doi.org/10.1007/978-3-319-24282-8</a>  (Springer International Publishing, 2015).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">He, H. et al. An Insight Extraction System on BioMedical Literature with Deep Neural Networks. In <i>Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</i> 2691–2701. <a href="https://doi.org/10.18653/v1/D17-1285" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="10.18653/v1/D17-1285">https://doi.org/10.18653/v1/D17-1285</a> (Association for Computational Linguistics, Stroudsburg, PA, USA, 2017).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Polak, M. P. &amp; Morgan, D. Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering. Preprint at <a href="http://arxiv.org/abs/2303.05352" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2303.05352">http://arxiv.org/abs/2303.05352</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Huang, J. &amp; Tan, M. The role of ChatGPT in scientific communication: writing better scientific review articles. <i>Am. J. Cancer Res.</i> <b>13</b>, 1148–1154 (2023).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed reference" data-track-action="pubmed reference" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=37168339" aria-label="PubMed reference 13">PubMed</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="pubmed central reference" data-track-action="pubmed central reference" href="http://www.ncbi.nlm.nih.gov/pmc/articles/PMC10164801" aria-label="PubMed Central reference 13">PubMed Central</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?07530874" aria-label="MATH reference 13">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=The%20role%20of%20ChatGPT%20in%20scientific%20communication%3A%20writing%20better%20scientific%20review%20articles&amp;journal=Am.%20J.%20Cancer%20Res.&amp;volume=13&amp;pages=1148-1154&amp;publication_year=2023&amp;author=Huang%2CJ&amp;author=Tan%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Han, R. et al. Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. Preprint at <a href="http://arxiv.org/abs/2305.14450" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2305.14450">http://arxiv.org/abs/2305.14450</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Yuan, C., Xie, Q. &amp; Ananiadou, S. Zero-shot Temporal Relation Extraction with ChatGPT. Preprint at <a href="http://arxiv.org/abs/2304.05454" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2304.05454">http://arxiv.org/abs/2304.05454</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">Wang, S., Scells, H., Koopman, B. &amp; Zuccon, G. Can ChatGPT Write a Good Boolean Query for Systematic Review Literature Search? Preprint at <a href="http://arxiv.org/abs/2302.03495" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2302.03495">http://arxiv.org/abs/2302.03495</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">Zhang, J., Chen, Y., Niu, N., Wang, Y. &amp; Liu, C. Empirical Evaluation of ChatGPT on Requirements Information Retrieval Under Zero-Shot Setting. Preprint at <a href="http://arxiv.org/abs/2304.12562" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2304.12562">http://arxiv.org/abs/2304.12562</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">Li, B. et al. Evaluating ChatGPT’s Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness. Preprint at <a href="http://arxiv.org/abs/2304.11633" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2304.11633">http://arxiv.org/abs/2304.11633</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">Sun, W. et al. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. Preprint at <a href="http://arxiv.org/abs/2304.09542" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2304.09542">http://arxiv.org/abs/2304.09542</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Jahan, I., Laskar, M. T. R., Peng, C. &amp; Huang, J. Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. Preprint at <a href="http://arxiv.org/abs/2306.04504" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2306.04504">http://arxiv.org/abs/2306.04504</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">Kempf, S., Krug, M. &amp; Puppe, F. K. I. E. T. A. Key-insight extraction from scientific tables. <i>Appl. Intell.</i> <b>53</b>, 9513–9530 (2023).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="noopener" data-track-label="10.1007/s10489-022-03957-8" data-track-item_id="10.1007/s10489-022-03957-8" data-track-value="article reference" data-track-action="article reference" href="https://link.springer.com/doi/10.1007/s10489-022-03957-8" aria-label="Article reference 21" data-doi="10.1007/s10489-022-03957-8">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1067.81134" aria-label="MATH reference 21">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=Key-insight%20extraction%20from%20scientific%20tables&amp;journal=Appl.%20Intell.&amp;doi=10.1007%2Fs10489-022-03957-8&amp;volume=53&amp;pages=9513-9530&amp;publication_year=2023&amp;author=Kempf%2CS&amp;author=Krug%2CM&amp;author=Puppe%2CFKIETA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">Laban, P. et al. SummEdits: Measuring LLM Ability at Factual Reasoning Through The Lens of Summarization. In <i>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</i> 9662–9676. <a href="https://aclanthology.org/2023.emnlp-main.600/" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://aclanthology.org/2023.emnlp-main.600/">https://aclanthology.org/2023.emnlp-main.600/</a> (Association for Computational Linguistics, 2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">Cai, H. et al. SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis. Preprint at <a href="http://arxiv.org/abs/2403.01976" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2403.01976">http://arxiv.org/abs/2403.01976</a> (2024).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">Touvron, H. et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. Preprint at <a href="http://arxiv.org/abs/2307.09288" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2307.09288">http://arxiv.org/abs/2307.09288</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">Howard, J. &amp; Ruder, S. Universal Language Model Fine-tuning for Text Classification. Preprint at <a href="http://arxiv.org/abs/1801.06146" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1801.06146">http://arxiv.org/abs/1801.06146</a> (2018).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">Hu, E. et al. Lora: Low-Rank Adaptation of Large Language Models. Preprint at <a href="https://arxiv.org/abs/2106.09685" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a> (2021).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1511.60120" aria-label="MATH reference 26">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 26" href="http://scholar.google.com/scholar_lookup?&amp;title=Lora%3A%20low-rank%20adaptation%20of%20large%20Language%20models&amp;journal=ICLR%202022%E2%80%9310th%20Int.%20Conf.%20Learn.%20Represent&amp;volume=1&amp;publication_year=2022&amp;author=Hu%2CE">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">Fang, C. et al. LLM-Ensemble: Optimal Large Language Model Ensemble Method for E-commerce Product Attribute Value Extraction. Preprint at <a href="http://arxiv.org/abs/2403.00863" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2403.00863">http://arxiv.org/abs/2403.00863</a> (2024).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">Jiang, D., Ren, X. &amp; Lin, B. Y. LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. Preprint at <a href="http://arxiv.org/abs/2306.02561" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2306.02561">http://arxiv.org/abs/2306.02561</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">Lu, K. et al. Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models. Preprint at <a href="http://arxiv.org/abs/2311.08692" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2311.08692">http://arxiv.org/abs/2311.08692</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">Jiang, A. Q. et al. Mixtral of Experts. Preprint at <a href="http://arxiv.org/abs/2401.04088" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2401.04088">http://arxiv.org/abs/2401.04088</a> (2024).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">Hong, S. et al. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. Preprint at <a href="http://arxiv.org/abs/2308.00352" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2308.00352">http://arxiv.org/abs/2308.00352</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">Liang, T. et al. Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate. Preprint at <a href="http://arxiv.org/abs/2305.19118" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2305.19118">http://arxiv.org/abs/2305.19118</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">Chen, H. et al. ChatGPT’s One-year Anniversary: Are Open-Source Large Language Models Catching up? Preprint at <a href="http://arxiv.org/abs/2311.16989" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2311.16989">http://arxiv.org/abs/2311.16989</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">Huang, H., Qu, Y., Liu, J., Yang, M. &amp; Zhao, T. An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Models are Task-specific Classifiers. Preprint at <a href="http://arxiv.org/abs/2403.02839" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2403.02839">http://arxiv.org/abs/2403.02839</a> (2024).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">Wolf, T. et al. HuggingFace’s Transformers: State-of-the-art Natural Language Processing. Preprint at <a href="http://arxiv.org/abs/1910.03771" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1910.03771">http://arxiv.org/abs/1910.03771</a> (2019).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">Xuechen L. et al. AlpacaEval: An Automatic Evaluator of Instruction-following Models.  <a href="https://github.com/tatsu-lab/alpaca_eval" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://github.com/tatsu-lab/alpaca_eval">https://github.com/tatsu-lab/alpaca_eval</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR37">Chung, H. W. et al. Scaling Instruction-Finetuned Language Models. Preprint at <a href="http://arxiv.org/abs/2210.11416" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2210.11416">http://arxiv.org/abs/2210.11416</a> (2022).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR38">Frantar, E., Ashkboos, S., Hoefler, T. &amp; Alistarh, D. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. Preprint at <a href="https://arxiv.org/abs/2210.17323" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2210.17323">https://arxiv.org/abs/2210.17323</a> (2022).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR39">Ilya L. &amp; Frank H. Decoupled Weight Decay Regularization. Preprint at <a href="https://arxiv.org/abs/1711.05101" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/1711.05101">https://arxiv.org/abs/1711.05101</a> (2019).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR40">Paul Ginsparg. ArXiv. <a href="https://arxiv.org/" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/">https://arxiv.org/</a> (1991).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR41">Bakker, M. A. et al. Fine-tuning language models to find agreement among humans with diverse preferences. Preprint at <a href="https://arxiv.org/abs/2211.15006" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://arxiv.org/abs/2211.15006">https://arxiv.org/abs/2211.15006</a> (2022).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR42">Hu, Z. et al. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models. Preprint at <a href="http://arxiv.org/abs/2304.01933" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2304.01933">http://arxiv.org/abs/2304.01933</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR43">Liu, N. F. et al. Lost in the Middle: How Language Models Use Long Contexts. Preprint at <a href="http://arxiv.org/abs/2307.03172" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2307.03172">http://arxiv.org/abs/2307.03172</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR44">Chris A. M. et al. Tika-Python. <a href="https://github.com/chrismattmann/tika-python" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://github.com/chrismattmann/tika-python">https://github.com/chrismattmann/tika-python</a> (2014).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR45">Sadvilkar, N. &amp; Neumann, M. PySBD: Pragmatic Sentence Boundary Disambiguation. Preprint at <a href="http://arxiv.org/abs/2010.09657" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2010.09657">http://arxiv.org/abs/2010.09657</a> (2020).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46."><p class="c-article-references__text" id="ref-CR46">Zheng, L. et al. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena. Preprint at <a href="http://arxiv.org/abs/2306.05685" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2306.05685">http://arxiv.org/abs/2306.05685</a> (2023).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47."><p class="c-article-references__text" id="ref-CR47">Gao, M., Hu, X., Ruan, J., Pu, X. &amp; Wan, X. LLM-based NLG Evaluation: Current Status and Challenges. Preprint at <a href="http://arxiv.org/abs/2402.01383" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/2402.01383">http://arxiv.org/abs/2402.01383</a> (2024).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48."><p class="c-article-references__text" id="ref-CR48">Farouk, M. Measuring Sentences Similarity: A Survey. Preprint at <a href="http://arxiv.org/abs/1910.03940" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1910.03940">http://arxiv.org/abs/1910.03940</a> (2019).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49."><p class="c-article-references__text" id="ref-CR49">Reimers, N. &amp; Gurevych, I. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. Preprint at <a href="http://arxiv.org/abs/1908.10084" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="http://arxiv.org/abs/1908.10084">http://arxiv.org/abs/1908.10084</a> (2019).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50."><p class="c-article-references__text" id="ref-CR50">Mangrulkar, S. et al. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. <a href="https://github.com/huggingface/peft" data-track="click_references" data-track-action="external reference" data-track-value="external reference" data-track-label="https://github.com/huggingface/peft">https://github.com/huggingface/peft</a> (2022).</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51."><p class="c-article-references__text" id="ref-CR51">Wilcoxon, F. Individual comparisons by ranking methods. <i>Biometrics Bull.</i> <b>1</b>, 80 (1945).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.2307/3001968" data-track-item_id="10.2307/3001968" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.2307%2F3001968" aria-label="Article reference 51" data-doi="10.2307/3001968">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?0974.93574" aria-label="MATH reference 51">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 51" href="http://scholar.google.com/scholar_lookup?&amp;title=Individual%20comparisons%20by%20ranking%20methods&amp;journal=Biometrics%20Bull.&amp;doi=10.2307%2F3001968&amp;volume=1&amp;publication_year=1945&amp;author=Wilcoxon%2CF">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52."><p class="c-article-references__text" id="ref-CR52">Kocoń, J. et al. Jack of all trades, master of none. <i>Inf. Fusion</i>. <b>99</b>, 101861 (2023).</p><p class="c-article-references__links u-hide-print"><a data-track="click_references" rel="nofollow noopener" data-track-label="10.1016/j.inffus.2023.101861" data-track-item_id="10.1016/j.inffus.2023.101861" data-track-value="article reference" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.inffus.2023.101861" aria-label="Article reference 52" data-doi="10.1016/j.inffus.2023.101861">Article</a> 
    <a data-track="click_references" rel="nofollow noopener" data-track-label="link" data-track-item_id="link" data-track-value="math reference" data-track-action="math reference" href="http://www.emis.de/MATH-item?1468.94476" aria-label="MATH reference 52">MATH</a> 
    <a data-track="click_references" data-track-action="google scholar reference" data-track-value="google scholar reference" data-track-label="link" data-track-item_id="link" rel="nofollow noopener" aria-label="Google Scholar reference 52" href="http://scholar.google.com/scholar_lookup?&amp;title=Jack%20of%20all%20trades%2C%20master%20of%20none&amp;journal=Inf.%20Fusion&amp;doi=10.1016%2Fj.inffus.2023.101861&amp;volume=99&amp;publication_year=2023&amp;author=Koco%C5%84%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53."><p class="c-article-references__text" id="ref-CR53">Li, A. et al. Evaluating Modern PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect. <i>IEEE Trans. Parallel Distrib. Syst.</i> <b>31</b>, 94–110 (2020).</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41598-025-85715-7?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p></div></div></div></section></div><section data-title="Funding"><div class="c-article-section" id="Fun-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Fun">Funding</h2><div class="c-article-section__content" id="Fun-content"><p>This work was supported by the Dong-A University research fund.</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Dong-A University, Busan, 49315, Republic of Korea</p><p class="c-article-author-affiliation__authors-list">Zihan Song, Gyo-Yeob Hwang, Xin Zhang, Shan Huang &amp; Byung-Kwon Park</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Zihan-Song-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Zihan Song</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Zihan%20Song" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Zihan%20Song" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Zihan%20Song%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Gyo_Yeob-Hwang-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Gyo-Yeob Hwang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Gyo-Yeob%20Hwang" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Gyo-Yeob%20Hwang" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Gyo-Yeob%20Hwang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Xin-Zhang-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Xin Zhang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Xin%20Zhang" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Xin%20Zhang" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Xin%20Zhang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Shan-Huang-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Shan Huang</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Shan%20Huang" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shan%20Huang" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shan%20Huang%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Byung_Kwon-Park-Aff1"><span class="c-article-authors-search__title u-h3 js-search-name">Byung-Kwon Park</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?author=Byung-Kwon%20Park" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text"><span class="c-article-authors-search__links-text">Search author on:</span><span class="c-article-identifiers"><a class="c-article-identifiers__item" href="https://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Byung-Kwon%20Park" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="https://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Byung-Kwon%20Park%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="contributions">Contributions</h3><p>Z.S. wrote the main manuscript text. G.-Y.H. and B.-K.P. contributed to the manuscript by performing modifications and proofreading. S.H. and X.Z. provided analysis and discussion on the results. All authors reviewed the manuscript.</p><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" aria-label="email Byung-Kwon Park" href="mailto:bpark@dau.ac.kr">Byung-Kwon Park</a>.</p></div></div></section><section data-title="Ethics declarations"><div class="c-article-section" id="ethics-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="ethics">Ethics declarations</h2><div class="c-article-section__content" id="ethics-content">


                <h3 class="c-article__sub-heading" id="FPar1">Competing interests</h3>
                <p>The authors declare no competing interests.</p>

            </div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher’s note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Electronic supplementary material"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20">Electronic supplementary material</h2><div class="c-article-section__content" id="Sec20-content"><div data-test="supplementary-info"><div id="figshareContainer" class="c-article-figshare-container" data-test="figshare-container"></div><p>Below is the link to the electronic supplementary material.</p><div class="c-article-supplementary__item" data-test="supp-item" id="MOESM1"><h3 class="c-article-supplementary__title u-h3"><a class="print-link" data-track="click" data-track-action="view supplementary info" data-test="supp-info-link" data-track-label="supplementary material 1" href="https://static-content.springer.com/esm/art%3A10.1038%2Fs41598-025-85715-7/MediaObjects/41598_2025_85715_MOESM1_ESM.xlsx" data-supp-info-image="">Supplementary Material 1</a></h3></div></div></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content">
                <p><b>Open Access</b>  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <a href="http://creativecommons.org/licenses/by-nc-nd/4.0/" rel="license">http://creativecommons.org/licenses/by-nc-nd/4.0/</a>.</p>
              <p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=A%20scientific-article%20key-insight%20extraction%20system%20based%20on%20multi-actor%20of%20fine-tuned%20open-source%20large%20language%20models&amp;author=Zihan%20Song%20et%20al&amp;contentID=10.1038%2Fs41598-025-85715-7&amp;copyright=The%20Author%28s%29&amp;publication=2045-2322&amp;publicationDate=2025-01-10&amp;publisherName=SpringerNature&amp;orderBeanReset=true&amp;oa=CC%20BY-NC-ND">Reprints and permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1038/s41598-025-85715-7" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1038/s41598-025-85715-7" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img loading="lazy" width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Song, Z., Hwang, GY., Zhang, X. <i>et al.</i> A scientific-article key-insight extraction system based on multi-actor of fine-tuned open-source large language models.
                    <i>Sci Rep</i> <b>15</b>, 1608 (2025). https://doi.org/10.1038/s41598-025-85715-7</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1038/s41598-025-85715-7?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-eds-i-download-medium"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2024-04-01">01 April 2024</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2025-01-06">06 January 2025</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2025-01-10">10 January 2025</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Version of record<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2025-01-10">10 January 2025</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--full-width"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1038/s41598-025-85715-7</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy shareable link to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span><a href="/search?query=Key-insight%20extraction&amp;facet-discipline=&#34;Science%2C%20Humanities%20and%20Social%20Sciences%2C%20multidisciplinary&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Key-insight extraction</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Large%20Language%20Model%20fine-tuning&amp;facet-discipline=&#34;Science%2C%20Humanities%20and%20Social%20Sciences%2C%20multidisciplinary&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Large Language Model fine-tuning</a></span></li><li class="c-article-subject-list__subject"><span><a href="/search?query=Automatic%20information%20extraction&amp;facet-discipline=&#34;Science%2C%20Humanities%20and%20Social%20Sciences%2C%20multidisciplinary&#34;" data-track="click" data-track-action="view keyword" data-track-label="link">Automatic information extraction</a></span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
            </div>


        <section>
            <div class="c-article-section js-article-section" id="further-reading-section" data-test="further-reading-section">
                <h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="further-reading">This article is cited by</h2>
                <div class="c-article-section__content js-collapsible-section" id="further-reading-content">
                    <ul class="c-article-further-reading__list" id="further-reading-list">

                            <li class="c-article-further-reading__item js-ref-item">

                                <h3 class="c-article-further-reading__title" data-test="article-title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Digital Maktaba project: Toward a metadata-driven, LLM-assisted framework for arabic digital libraries" href="https://doi.org/10.1007/s00799-025-00432-w">
                                        Digital Maktaba project: Toward a metadata-driven, LLM-assisted framework for arabic digital libraries
                                    </a>
                                </h3>


                                    <ul data-test="author-list" class="app-author-list app-author-list--compact u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Amina El Ganadi</li><li>Luca Gagliardelli</li><li>Federico Ruozzi</li>
                                    </ul>

                                <p class="c-article-further-reading__journal-title"><i>International Journal on Digital Libraries</i> (2025)</p>
                            </li>

                            <li class="c-article-further-reading__item js-ref-item">

                                <h3 class="c-article-further-reading__title" data-test="article-title">
                                    <a class="print-link" data-track="click" data-track-action="view further reading article"
                                       data-track-label="link:Towards AI-augmented sustainability assessments: integrating large language models in the case of product social life cycle assessment" href="https://doi.org/10.1007/s11367-025-02508-w">
                                        Towards AI-augmented sustainability assessments: integrating large language models in the case of product social life cycle assessment
                                    </a>
                                </h3>


                                    <ul data-test="author-list" class="app-author-list app-author-list--compact app-author-list--truncated u-sans-serif u-mb-4 u-mt-auto">
                                        <li>Carolyn Cole</li><li>Arash Hajikhani</li><li>Hanna Pihkola</li>
                                    </ul>

                                <p class="c-article-further-reading__journal-title"><i>The International Journal of Life Cycle Assessment</i> (2025)</p>
                            </li>

                    </ul>
                </div>
            </div>
        </section>



        </div>
</article>
</main>

<aside class="c-article-extras u-hide-print" aria-label="Article navigation" data-component-reading-companion data-container-type="reading-companion" data-track-component="reading companion">
    <div class="u-mb-48 js-context-bar-sticky-point-desktop" data-track-context="reading companion">







        <div class="c-pdf-download u-clear-both js-pdf-download">
            <a href="/articles/s41598-025-85715-7.pdf" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="download-pdf" data-draft-ignore="true" data-track="content_download" data-track-type="article pdf download" data-track-action="download pdf" data-track-label="link" data-track-external download>
                <span class="c-pdf-download__text">Download PDF</span>
                <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
            </a>
        </div>






    </div>








    <div class="c-reading-companion">
        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky" data-test="reading-companion-sticky">
            <div class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active" id="tabpanel-sections">
                <div class="u-lazy-ad-wrapper u-mt-16 u-hide" data-component-mpu>
                    <div class="c-ad c-ad--300x250">
                        <div class="c-ad__inner">
                            <p class="c-ad__label">Advertisement</p>

    <div id="div-gpt-ad-right-2"
         class="div-gpt-ad advert medium-rectangle js-ad text-center hide-print grade-c-hide"
         data-ad-type="right"
         data-test="right-ad"
         data-pa11y-ignore
         data-gpt
         data-gpt-unitpath="/285/scientific_reports/article"
         data-gpt-sizes="300x250"
         data-gpt-targeting="type=article;pos=right;artid=s41598-025-85715-7;doi=10.1038/s41598-025-85715-7;subjmeta=1046,117,639,705;kwrd=Computer+science,Scientific+data">

        <script>
            window.SN = window.SN || {};
            window.SN.libs = window.SN.libs || {};
            window.SN.libs.ads = window.SN.libs.ads || {};
            window.SN.libs.ads.slotConfig = window.SN.libs.ads.slotConfig || {};

                window.SN.libs.ads.slotConfig['right'] = {
                    'pos': 'right',
                    'type': 'article',
                    'path': 's41598-025-85715-7'
                };


            window.SN.libs.ads.slotConfig['kwrd'] = 'Computer+science,Scientific+data';


            window.SN.libs.ads.slotConfig['subjmeta'] = '1046,117,639,705';


        </script>
        <noscript>
            <a href="//pubads.g.doubleclick.net/gampad/jump?iu=/285/scientific_reports/article&amp;sz=300x250&amp;c=-1802335632&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41598-025-85715-7%26doi%3D10.1038/s41598-025-85715-7%26subjmeta%3D1046,117,639,705%26kwrd%3DComputer+science,Scientific+data">
                <img data-test="gpt-advert-fallback-img"
                     src="//pubads.g.doubleclick.net/gampad/ad?iu=/285/scientific_reports/article&amp;sz=300x250&amp;c=-1802335632&amp;t=pos%3Dright%26type%3Darticle%26artid%3Ds41598-025-85715-7%26doi%3D10.1038/s41598-025-85715-7%26subjmeta%3D1046,117,639,705%26kwrd%3DComputer+science,Scientific+data"
                     alt="Advertisement"
                     width="300"
                     height="250"></a>
        </noscript>
    </div>

                        </div>
                    </div>
                </div>
            </div>
            <div class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width" id="tabpanel-figures"></div>
            <div class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width" id="tabpanel-references"></div>
        </div>
    </div>
</aside>
</div>



        <nav class="c-header__dropdown" aria-labelledby="Explore-content" data-test="Explore-content" id="explore" data-track-component="nature-150-split-header">
            <div class="c-header__container">
                <h2 id="Explore-content" class="c-header__heading c-header__heading--js-hide">Explore content</h2>
                <ul class="c-header__list c-header__list--js-stack">


                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/research-articles"
                                   data-track="click"
                                   data-track-action="research articles"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Research articles
                                </a>
                            </li>

                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/news-and-comment"
                                   data-track="click"
                                   data-track-action="news &amp; comment"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    News &amp; Comment
                                </a>
                            </li>

                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/collections"
                                   data-track="click"
                                   data-track-action="collections"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Collections
                                </a>
                            </li>

                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/browse-subjects"
                                   data-track="click"
                                   data-track-action="subjects"
                                   data-track-label="link"
                                   data-test="explore-nav-item">
                                    Subjects
                                </a>
                            </li>


                </ul>
                <ul class="c-header__list c-header__list--js-stack">

                        <li class="c-header__item">
                            <a class="c-header__link"
                               href="https://www.facebook.com/scientificreports"
                               data-track="click"
                               data-track-action="facebook"
                               data-track-label="link">Follow us on Facebook
                            </a>
                        </li>


                        <li class="c-header__item">
                            <a class="c-header__link"
                               href="https://twitter.com/SciReports"
                               data-track="click"
                               data-track-action="twitter"
                               data-track-label="link">Follow us on Twitter
                            </a>
                        </li>



                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://journal-alerts.springernature.com/subscribe?journal_id&#x3D;41598"
                               rel="nofollow"
                               data-track="nav_sign_up_for_alerts"
                               data-track-action="Sign up for alerts"
                               data-track-external
                               data-track-label="link (mobile dropdown)">Sign up for alerts<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill="#fff"/></svg>
                            </a>
                        </li>


                        <li class="c-header__item c-header__item--hide-lg">
                            <a class="c-header__link"
                               href="https://www.nature.com/srep.rss"
                               data-track="click"
                               data-track-action="rss feed"
                               data-track-label="link">
                                <span>RSS feed</span>
                            </a>
                        </li>

                </ul>
            </div>
        </nav>



            <nav class="c-header__dropdown" aria-labelledby="About-the-journal" id="about-the-journal" data-test="about-the-journal" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="About-the-journal" class="c-header__heading c-header__heading--js-hide">About the journal</h2>
                    <ul class="c-header__list c-header__list--js-stack">

                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/about"
                                   data-track="click"
                                   data-track-action="about scientific reports"
                                   data-track-label="link">
                                    About Scientific Reports
                                </a>
                            </li>

                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/contact"
                                   data-track="click"
                                   data-track-action="contact"
                                   data-track-label="link">
                                    Contact
                                </a>
                            </li>

                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/journal-policies"
                                   data-track="click"
                                   data-track-action="journal policies"
                                   data-track-label="link">
                                    Journal policies
                                </a>
                            </li>

                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/guide-to-referees"
                                   data-track="click"
                                   data-track-action="guide to referees"
                                   data-track-label="link">
                                    Guide to referees
                                </a>
                            </li>

                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/calls-for-papers"
                                   data-track="click"
                                   data-track-action="calls for papers"
                                   data-track-label="link">
                                    Calls for Papers
                                </a>
                            </li>

                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/editorschoice"
                                   data-track="click"
                                   data-track-action="editor&#x27;s choice"
                                   data-track-label="link">
                                    Editor&#x27;s Choice
                                </a>
                            </li>

                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/highlights"
                                   data-track="click"
                                   data-track-action="journal highlights"
                                   data-track-label="link">
                                    Journal highlights
                                </a>
                            </li>

                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/open-access"
                                   data-track="click"
                                   data-track-action="open access fees and funding"
                                   data-track-label="link">
                                    Open Access Fees and Funding
                                </a>
                            </li>

                    </ul>
                </div>
            </nav>



            <nav class="c-header__dropdown" aria-labelledby="Publish-with-us-label" id="publish-with-us" data-test="publish-with-us" data-track-component="nature-150-split-header">
                <div class="c-header__container">
                    <h2 id="Publish-with-us-label" class="c-header__heading c-header__heading--js-hide">Publish with us</h2>
                    <ul class="c-header__list c-header__list--js-stack">

                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/author-instructions"
                                   data-track="click"
                                   data-track-action="for authors"
                                   data-track-label="link">
                                    For authors
                                </a>
                            </li>


                            <li class="c-header__item">
                                <a class="c-header__link" data-test="nature-author-services"
                                   data-track="nav_language_services"
                                   data-track-context="header publish with us dropdown menu"
                                   data-track-action="manuscript author services"
                                   data-track-label="link manuscript author services"
                                   href="https://authorservices.springernature.com/go/sn/?utm_source=For+Authors&utm_medium=Website_Nature&utm_campaign=Platform+Experimentation+2022&utm_id=PE2022">
                                    Language editing services
                                </a>
                            </li>


                            <li class="c-header__item">
                                <a class="c-header__link"
                                   href="/srep/open-access-funding"
                                   data-test="funding-eligibility-link"
                                   data-track="click_explore_funding"
                                   data-track-context="header publish with us"
                                   data-track-action="funding eligibility">Open access funding</a>
                            </li>


                            <li class="c-header__item c-header__item--keyline">
                                <a class="c-header__link"
                                   href="https://author-welcome.nature.com/41598"
                                   data-track="click_submit_manuscript"
                                   data-track-context="submit link in Nature header dropdown menu"
                                   data-track-action="submit manuscript"
                                   data-track-label="link (publish with us dropdown menu)"
                                   data-track-external
                                   data-gtm-criteo="submit-manuscript">Submit manuscript<svg role="img" aria-hidden="true" focusable="false" height="18" viewBox="0 0 18 18" width="18" xmlns="http://www.w3.org/2000/svg"><path d="m15 0c1.1045695 0 2 .8954305 2 2v5.5c0 .27614237-.2238576.5-.5.5s-.5-.22385763-.5-.5v-5.5c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-9v3c0 1.1045695-.8954305 2-2 2h-3v10c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h7.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-7.5c-1.1045695 0-2-.8954305-2-2v-10.17157288c0-.53043297.21071368-1.0391408.58578644-1.41421356l3.82842712-3.82842712c.37507276-.37507276.88378059-.58578644 1.41421356-.58578644zm-.5442863 8.18867991 3.3545404 3.35454039c.2508994.2508994.2538696.6596433.0035959.909917-.2429543.2429542-.6561449.2462671-.9065387-.0089489l-2.2609825-2.3045251.0010427 7.2231989c0 .3569916-.2898381.6371378-.6473715.6371378-.3470771 0-.6473715-.2852563-.6473715-.6371378l-.0010428-7.2231995-2.2611222 2.3046654c-.2531661.2580415-.6562868.2592444-.9065605.0089707-.24295423-.2429542-.24865597-.6576651.0036132-.9099343l3.3546673-3.35466731c.2509089-.25090888.6612706-.25227691.9135302-.00001728zm-.9557137-3.18867991c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm-8.5-3.587-3.587 3.587h2.587c.55228475 0 1-.44771525 1-1zm8.5 1.587c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill="#fff"/></svg>
                                </a>
                            </li>

                    </ul>
                </div>
            </nav>


    <script>
        window.dataLayer = window.dataLayer || [];
        window.dataLayer.push({
            page: {
                content: {
                    fundingWidget: "true",
                        }
                    }
                });
    </script>


<div id="search-menu" class="c-header__dropdown c-header__dropdown--full-width" data-track-component="nature-150-split-header">
    <div class="c-header__container">
        <h2 class="c-header__visually-hidden">Search</h2>
        <form class="c-header__search-form" action="/search" method="get" role="search" autocomplete="off" data-test="inline-search">
            <label class="c-header__heading" for="keywords">Search articles by subject, keyword or author</label>
            <div class="c-header__search-layout c-header__search-layout--max-width">
                <div>
                    <input type="text" required="" class="c-header__input" id="keywords" name="q" value="">
                </div>
                <div class="c-header__search-layout">
                    <div>
                        <label for="results-from" class="c-header__visually-hidden">Show results from</label>
                        <select id="results-from" name="journal" class="c-header__select">


                                    <option value="" selected>All journals</option>
                                    <option value="srep">This journal</option>


                        </select>
                    </div>
                    <div>
                        <button type="submit" class="c-header__search-button">Search</button>
                    </div>
                </div>

            </div>
        </form>

        <div class="c-header__flush">
            <a class="c-header__link" href="/search/advanced"
               data-track="click" data-track-action="advanced search" data-track-label="link">
                Advanced search
            </a>
        </div>

        <h3 class="c-header__heading c-header__heading--keyline">Quick links</h3>
        <ul class="c-header__list">
            <li><a class="c-header__link" href="/subjects" data-track="click" data-track-action="explore articles by subject" data-track-label="link">Explore articles by subject</a></li>
            <li><a class="c-header__link" href="/naturecareers" data-track="click" data-track-action="find a job" data-track-label="link">Find a job</a></li>
            <li><a class="c-header__link" href="/authors/index.html" data-track="click" data-track-action="guide to authors" data-track-label="link">Guide to authors</a></li>
            <li><a class="c-header__link" href="/authors/editorial_policies/" data-track="click" data-track-action="editorial policies" data-track-label="link">Editorial policies</a></li>
        </ul>
    </div>
</div>

<footer class="composite-layer" itemscope itemtype="http://schema.org/Periodical">
        <meta itemprop="publisher" content="Springer Nature">


        <div class="u-mt-16 u-mb-16">
    <div class="u-container">
        <div class="u-display-flex u-flex-wrap u-justify-content-space-between">


            <div class="c-meta u-ma-0 u-flex-shrink">
                <p class="c-meta__item c-meta__type u-mt-0">
                    <span itemprop="name">
                        Scientific Reports
                    </span>
                    (<i itemprop="alternateName">Sci Rep</i>)
                </p>


        <p class="c-meta__item u-mt-0">
            <abbr title="International Standard Serial Number">ISSN</abbr> <span itemprop="issn">2045-2322</span> (online)
        </p>





            </div>
        </div>
    </div>
</div>

    <div class="c-footer">
        <div class="u-hide-print" data-track-component="footer">
    <h2 class="u-visually-hidden">nature.com sitemap</h2>
    <div class="c-footer__container">
        <div class="c-footer__grid c-footer__group--separator">
            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">About Nature Portfolio</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/company_info/index.html"
                                                  data-track="click" data-track-action="about us"
                                                  data-track-label="link">About us</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/npg_/press_room/press_releases.html"
                                                  data-track="click" data-track-action="press releases"
                                                  data-track-label="link">Press releases</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://press.nature.com/"
                                                  data-track="click" data-track-action="press office"
                                                  data-track-label="link">Press office</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://support.nature.com/support/home"
                                                  data-track="click" data-track-action="contact us"
                                                  data-track-label="link">Contact us</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Discover content</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/siteindex"
                                                  data-track="click" data-track-action="journals a-z"
                                                  data-track-label="link">Journals A-Z</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/subjects"
                                                  data-track="click" data-track-action="article by subject"
                                                  data-track-label="link">Articles by subject</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.protocols.io/"
                                                  data-track="click" data-track-action="protocols.io"
                                                  data-track-label="link">protocols.io</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureindex.com/"
                                                  data-track="click" data-track-action="nature index"
                                                  data-track-label="link">Nature Index</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Publishing policies</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/authors/editorial_policies"
                                                  data-track="click" data-track-action="Nature portfolio policies"
                                                  data-track-label="link">Nature portfolio policies</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/nature-research/open-access"
                                                  data-track="click" data-track-action="open access"
                                                  data-track-label="link">Open access</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Author &amp; Researcher services</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/reprints"
                                                  data-track="click" data-track-action="reprints and permissions"
                                                  data-track-label="link">Reprints &amp; permissions</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/authors/research-data"
                                                  data-track="click" data-track-action="data research service"
                                                  data-track-label="link">Research data</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/language-editing/"
                                                  data-track="click" data-track-action="language editing"
                                                  data-track-label="link">Language editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://authorservices.springernature.com/scientific-editing/"
                                                  data-track="click" data-track-action="scientific editing"
                                                  data-track-label="link">Scientific editing</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://masterclasses.nature.com/"
                                                  data-track="click" data-track-action="nature masterclasses"
                                                  data-track-label="link">Nature Masterclasses</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://solutions.springernature.com/"
                                                  data-track="click" data-track-action="research solutions"
                                                  data-track-label="link">Research Solutions</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Libraries &amp; institutions</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/tools-services"
                                                  data-track="click" data-track-action="librarian service and tools"
                                                  data-track-label="link">Librarian service &amp; tools</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/manage-your-account/librarianportal"
                                                  data-track="click" data-track-action="librarian portal"
                                                  data-track-label="link">Librarian portal</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.nature.com/openresearch/about-open-access/information-for-institutions"
                                                  data-track="click" data-track-action="open research"
                                                  data-track-label="link">Open research</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://www.springernature.com/gp/librarians/recommend-to-your-library"
                                                  data-track="click" data-track-action="Recommend to library"
                                                  data-track-label="link">Recommend to library</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Advertising &amp; partnerships</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/digital-advertising/"
                                                  data-track="click" data-track-action="advertising"
                                                  data-track-label="link">Advertising</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://partnerships.nature.com/"
                                                  data-track="click" data-track-action="partnerships and services"
                                                  data-track-label="link">Partnerships &amp; Services</a></li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/media-kits/" data-track="click"
                                                  data-track-action="media kits" data-track-label="link">Media kits</a>
                    </li>
                    <li class="c-footer__item"><a class="c-footer__link"
                                                  href="https://partnerships.nature.com/product/branded-content-native-advertising/"
                                                  data-track-action="branded content" data-track-label="link">Branded
                        content</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Professional development</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/immersive/natureawards/index.html"
                                                  data-track="click" data-track-action="nature awards"
                                                  data-track-label="link">Nature Awards</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/naturecareers/"
                                                  data-track="click" data-track-action="nature careers"
                                                  data-track-label="link">Nature Careers</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://conferences.nature.com"
                                                  data-track="click" data-track-action="nature conferences"
                                                  data-track-label="link">Nature<span class="u-visually-hidden"> </span>
                        Conferences</a></li>
                </ul>
            </div>

            <div class="c-footer__group">
                <h3 class="c-footer__heading u-mt-0">Regional websites</h3>
                <ul class="c-footer__list">
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/natafrica"
                                                  data-track="click" data-track-action="nature africa"
                                                  data-track-label="link">Nature Africa</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="http://www.naturechina.com"
                                                  data-track="click" data-track-action="nature china"
                                                  data-track-label="link">Nature China</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nindia"
                                                  data-track="click" data-track-action="nature india"
                                                  data-track-label="link">Nature India</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.natureasia.com/ja-jp"
                                                  data-track="click" data-track-action="nature japan"
                                                  data-track-label="link">Nature Japan</a></li>
                    <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/nmiddleeast"
                                                  data-track="click" data-track-action="nature middle east"
                                                  data-track-label="link">Nature Middle East</a></li>
                </ul>
            </div>

        </div>
    </div>
    <div class="c-footer__container">
        <ul class="c-footer__links">
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/privacy"
                                          data-track="click" data-track-action="privacy policy" data-track-label="link">Privacy
                Policy</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/cookies"
                                          data-track="click" data-track-action="use of cookies" data-track-label="link">Use
                of cookies</a></li>
            <li class="c-footer__item">
                <button class="optanon-toggle-display c-footer__link" onclick="javascript:;"
                        data-cc-action="preferences" data-track="click" data-track-action="manage cookies"
                        data-track-label="link">Your privacy choices/Manage cookies
                </button>
            </li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/legal-notice"
                                          data-track="click" data-track-action="legal notice" data-track-label="link">Legal
                notice</a></li>
            <li class="c-footer__item"><a class="c-footer__link"
                                          href="https://www.nature.com/info/accessibility-statement" data-track="click"
                                          data-track-action="accessibility statement" data-track-label="link">Accessibility
                statement</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/info/terms-and-conditions"
                                          data-track="click" data-track-action="terms and conditions"
                                          data-track-label="link">Terms &amp; Conditions</a></li>
            <li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/ccpa"
                                          data-track="click" data-track-action="california privacy statement"
                                          data-track-label="link">Your US state privacy rights</a></li>

        </ul>
    </div>
</div>


        <div class="c-footer__container">
    <a href="https://www.springernature.com/" class="c-footer__link">
        <img src="/static/images/logos/sn-logo-white-ea63208b81.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
    </a>
    <p class="c-footer__legal" data-test="copyright">&copy; 2025 Springer Nature Limited</p>
</div>

    </div>
    <div class="u-visually-hidden" aria-hidden="true">

    <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><symbol id="icon-access" viewBox="0 0 18 18"><path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd"/></symbol><symbol id="icon-account" viewBox="0 0 18 18"><path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-alert" viewBox="0 0 18 18"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-broad" viewBox="0 0 16 16"><path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)"/></symbol><symbol id="icon-arrow-down" viewBox="0 0 16 16"><path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-left" viewBox="0 0 16 16"><path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-right" viewBox="0 0 16 16"><path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-sub" viewBox="0 0 16 16"><path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-up" viewBox="0 0 16 16"><path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/></symbol><symbol id="icon-article" viewBox="0 0 18 18"><path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd"/></symbol><symbol id="icon-audio" viewBox="0 0 18 18"><path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd"/></symbol><symbol id="icon-block" viewBox="0 0 24 24"><path d="m0 0h24v24h-24z" fill-rule="evenodd"/></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-broad" viewBox="0 0 24 24"><path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)"/></symbol><symbol id="icon-calendar" viewBox="0 0 18 18"><path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd"/></symbol><symbol id="icon-cart" viewBox="0 0 18 18"><path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z"/></symbol><symbol id="icon-chevron-less" viewBox="0 0 10 10"><path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)"/></symbol><symbol id="icon-chevron-more" viewBox="0 0 10 10"><path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-chevron-right" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-circle-fill" viewBox="0 0 16 16"><path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-circle" viewBox="0 0 16 16"><path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-citation" viewBox="0 0 18 18"><path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-close" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-collections" viewBox="0 0 18 18"><path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-compare" viewBox="0 0 18 18"><path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd"/></symbol><symbol id="icon-download-file" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd"/></symbol><symbol id="icon-download" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd"/></symbol><symbol id="icon-email" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd"/></symbol><symbol id="icon-error" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd"/></symbol><symbol id="icon-ethics" viewBox="0 0 18 18"><path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd"/></symbol><symbol id="icon-expand"><path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd"/></symbol><symbol id="icon-explore" viewBox="0 0 18 18"><path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd"/></symbol><symbol id="icon-filter" viewBox="0 0 16 16"><path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z"/></symbol><symbol id="icon-home" viewBox="0 0 18 18"><path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd"/></symbol><symbol id="icon-image" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd"/></symbol><symbol id="icon-info" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-institution" viewBox="0 0 18 18"><path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd"/></symbol><symbol id="icon-location" viewBox="0 0 18 18"><path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd"/></symbol><symbol id="icon-minus" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-newsletter" viewBox="0 0 18 18"><path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd"/></symbol><symbol id="icon-orcid" viewBox="0 0 18 18"><path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd"/></symbol><symbol id="icon-plus" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-print" viewBox="0 0 18 18"><path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd"/></symbol><symbol id="icon-search" viewBox="0 0 22 22"><path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd"/></symbol><symbol id="icon-social-facebook" viewBox="0 0 24 24"><path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd"/></symbol><symbol id="icon-social-twitter" viewBox="0 0 24 24"><path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd"/></symbol><symbol id="icon-social-youtube" viewBox="0 0 24 24"><path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd"/></symbol><symbol id="icon-subject-medicine" viewBox="0 0 18 18"><path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-success" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd"/></symbol><symbol id="icon-table" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd"/></symbol><symbol id="icon-tick-circle" viewBox="0 0 24 24"><path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd"/></symbol><symbol id="icon-tick" viewBox="0 0 16 16"><path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd"/></symbol><symbol id="icon-update" viewBox="0 0 18 18"><path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd"/></symbol><symbol id="icon-upload" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd"/></symbol><symbol id="icon-video" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd"/></symbol><symbol id="icon-warning" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-checklist-banner" viewBox="0 0 56.69 56.69"><path style="fill:none" d="M0 0h56.69v56.69H0z"/><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/></symbol><symbol id="icon-chevron-down" viewBox="0 0 16 16"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-eds-i-arrow-right-medium" viewBox="0 0 24 24"><path d="m12.728 3.293 7.98 7.99a.996.996 0 0 1 .281.561l.011.157c0 .32-.15.605-.384.788l-7.908 7.918a1 1 0 0 1-1.416-1.414L17.576 13H4a1 1 0 0 1 0-2h13.598l-6.285-6.293a1 1 0 0 1-.082-1.32l.083-.095a1 1 0 0 1 1.414.001Z"/></symbol><symbol id="icon-eds-i-check-circle-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18Zm5.125 4.72a1 1 0 0 1 .156 1.405l-6 7.5a1 1 0 0 1-1.421.143l-3-2.5a1 1 0 0 1 1.28-1.536l2.217 1.846 5.362-6.703a1 1 0 0 1 1.406-.156Z"/></symbol><symbol id="icon-eds-i-chevron-down-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-chevron-down-small" viewBox="0 0 16 16"><path d="M13.692 5.278a1 1 0 0 1 .03 1.414L9.103 11.51a1.491 1.491 0 0 1-2.188.019L2.278 6.692a1 1 0 0 1 1.444-1.384L8 9.771l4.278-4.463a1 1 0 0 1 1.318-.111l.096.081Z"/></symbol><symbol id="icon-eds-i-chevron-right-medium" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-right-small" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-eds-i-chevron-up-medium" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-close-medium" viewBox="0 0 24 24"><path d="M12 1c6.075 0 11 4.925 11 11s-4.925 11-11 11S1 18.075 1 12 5.925 1 12 1Zm0 2a9 9 0 1 0 0 18 9 9 0 0 0 0-18ZM8.707 7.293 12 10.585l3.293-3.292a1 1 0 0 1 1.414 1.414L13.415 12l3.292 3.293a1 1 0 0 1-1.414 1.414L12 13.415l-3.293 3.292a1 1 0 1 1-1.414-1.414L10.585 12 7.293 8.707a1 1 0 0 1 1.414-1.414Z"/></symbol><symbol id="icon-eds-i-copy-link" viewBox="0 0 24 24"><path fill-rule="evenodd" clip-rule="evenodd" d="M19.4594 8.57015C19.0689 8.17963 19.0689 7.54646 19.4594 7.15594L20.2927 6.32261C20.2927 6.32261 20.2927 6.32261 20.2927 6.32261C21.0528 5.56252 21.0528 4.33019 20.2928 3.57014C19.5327 2.81007 18.3004 2.81007 17.5404 3.57014L16.7071 4.40347C16.3165 4.794 15.6834 4.794 15.2928 4.40348C14.9023 4.01296 14.9023 3.3798 15.2928 2.98927L16.1262 2.15594C17.6673 0.614803 20.1659 0.614803 21.707 2.15593C23.2481 3.69705 23.248 6.19569 21.707 7.7368L20.8737 8.57014C20.4831 8.96067 19.85 8.96067 19.4594 8.57015Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M18.0944 5.90592C18.4849 6.29643 18.4849 6.9296 18.0944 7.32013L16.4278 8.9868C16.0373 9.37733 15.4041 9.37734 15.0136 8.98682C14.6231 8.59631 14.6231 7.96314 15.0136 7.57261L16.6802 5.90594C17.0707 5.51541 17.7039 5.5154 18.0944 5.90592Z"/><path fill-rule="evenodd" clip-rule="evenodd" d="M13.5113 6.32243C13.9018 6.71295 13.9018 7.34611 13.5113 7.73664L12.678 8.56997C12.678 8.56997 12.678 8.56997 12.678 8.56997C11.9179 9.33006 11.9179 10.5624 12.6779 11.3224C13.438 12.0825 14.6703 12.0825 15.4303 11.3224L16.2636 10.4891C16.6542 10.0986 17.2873 10.0986 17.6779 10.4891C18.0684 10.8796 18.0684 11.5128 17.6779 11.9033L16.8445 12.7366C15.3034 14.2778 12.8048 14.2778 11.2637 12.7366C9.72262 11.1955 9.72266 8.69689 11.2637 7.15578L12.097 6.32244C12.4876 5.93191 13.1207 5.93191 13.5113 6.32243Z"/><path d="M8 20V22H19.4619C20.136 22 20.7822 21.7311 21.2582 21.2529C21.7333 20.7757 22 20.1289 22 19.4549V15C22 14.4477 21.5523 14 21 14C20.4477 14 20 14.4477 20 15V19.4549C20 19.6004 19.9426 19.7397 19.8408 19.842C19.7399 19.9433 19.6037 20 19.4619 20H8Z"/><path d="M4 13H2V19.4619C2 20.136 2.26889 20.7822 2.74705 21.2582C3.22434 21.7333 3.87105 22 4.5451 22H9C9.55228 22 10 21.5523 10 21C10 20.4477 9.55228 20 9 20H4.5451C4.39957 20 4.26028 19.9426 4.15804 19.8408C4.05668 19.7399 4 19.6037 4 19.4619V13Z"/><path d="M4 13H2V4.53808C2 3.86398 2.26889 3.21777 2.74705 2.74178C3.22434 2.26666 3.87105 2 4.5451 2H9C9.55228 2 10 2.44772 10 3C10 3.55228 9.55228 4 9 4H4.5451C4.39957 4 4.26028 4.05743 4.15804 4.15921C4.05668 4.26011 4 4.39633 4 4.53808V13Z"/></symbol><symbol id="icon-eds-i-download-medium" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-info-filled-medium" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-eds-i-institution-medium" viewBox="0 0 24 24"><g><path fill-rule="evenodd" clip-rule="evenodd" d="M11.9967 1C11.6364 1 11.279 1.0898 10.961 1.2646C10.9318 1.28061 10.9035 1.29806 10.8761 1.31689L2.79765 6.87C2.46776 7.08001 2.20618 7.38466 2.07836 7.76668C1.94823 8.15561 1.98027 8.55648 2.12665 8.90067C2.42086 9.59246 3.12798 10 3.90107 10H4.99994V16H4.49994C3.11923 16 1.99994 17.1193 1.99994 18.5V19.5C1.99994 20.8807 3.11923 22 4.49994 22H19.4999C20.8807 22 21.9999 20.8807 21.9999 19.5V18.5C21.9999 17.1193 20.8807 16 19.4999 16H18.9999V10H20.0922C20.8653 10 21.5725 9.59252 21.8667 8.90065C22.0131 8.55642 22.0451 8.15553 21.9149 7.7666C21.7871 7.38459 21.5255 7.07997 21.1956 6.86998L13.1172 1.31689C13.0898 1.29806 13.0615 1.28061 13.0324 1.2646C12.7143 1.0898 12.357 1 11.9967 1ZM4.6844 8L11.9472 3.00755C11.9616 3.00295 11.9783 3 11.9967 3C12.015 3 12.0318 3.00295 12.0461 3.00755L19.3089 8H4.6844ZM16.9999 16V10H14.9999V16H16.9999ZM12.9999 16V10H10.9999V16H12.9999ZM8.99994 16V10H6.99994V16H8.99994ZM3.99994 18.5C3.99994 18.2239 4.2238 18 4.49994 18H19.4999C19.7761 18 19.9999 18.2239 19.9999 18.5V19.5C19.9999 19.7761 19.7761 20 19.4999 20H4.49994C4.2238 20 3.99994 19.7761 3.99994 19.5V18.5Z"/></g></symbol><symbol id="icon-eds-i-mail-medium" viewBox="0 0 22 18"><path d="m19.462 0c1.413 0 2.538 1.184 2.538 2.619v12.762c0 1.435-1.125 2.619-2.538 2.619h-16.924c-1.413 0-2.538-1.184-2.538-2.619v-12.762c0-1.435 1.125-2.619 2.538-2.619zm.538 5.158-7.378 6.258a2.549 2.549 0 0 1 -3.253-.008l-7.369-6.248v10.222c0 .353.253.619.538.619h16.924c.285 0 .538-.266.538-.619zm-.538-3.158h-16.924c-.264 0-.5.228-.534.542l8.65 7.334c.2.165.492.165.684.007l8.656-7.342-.001-.025c-.044-.3-.274-.516-.531-.516z"/></symbol><symbol id="icon-eds-i-menu-medium" viewBox="0 0 24 24"><path d="M21 4a1 1 0 0 1 0 2H3a1 1 0 1 1 0-2h18Zm-4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h14Zm4 7a1 1 0 0 1 0 2H3a1 1 0 0 1 0-2h18Z"/></symbol><symbol id="icon-eds-i-search-medium" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z"/></symbol><symbol id="icon-eds-i-user-single-medium" viewBox="0 0 24 24"><path d="M12 1a5 5 0 1 1 0 10 5 5 0 0 1 0-10Zm0 2a3 3 0 1 0 0 6 3 3 0 0 0 0-6Zm-.406 9.008a8.965 8.965 0 0 1 6.596 2.494A9.161 9.161 0 0 1 21 21.025V22a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1v-.985c.05-4.825 3.815-8.777 8.594-9.007Zm.39 1.992-.299.006c-3.63.175-6.518 3.127-6.678 6.775L5 21h13.998l-.009-.268a7.157 7.157 0 0 0-1.97-4.573l-.214-.213A6.967 6.967 0 0 0 11.984 14Z"/></symbol><symbol id="icon-eds-i-warning-filled-medium" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-expand-image" viewBox="0 0 18 18"><path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd"/></symbol><symbol id="icon-github" viewBox="0 0 100 100"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"/></symbol><symbol id="icon-orcid-logo" viewBox="0 0 40 40"><path fill-rule="evenodd" d="M12.281 10.453c.875 0 1.578-.719 1.578-1.578 0-.86-.703-1.578-1.578-1.578-.875 0-1.578.703-1.578 1.578 0 .86.703 1.578 1.578 1.578Zm-1.203 18.641h2.406V12.359h-2.406v16.735Z"/><path fill-rule="evenodd" d="M17.016 12.36h6.5c6.187 0 8.906 4.421 8.906 8.374 0 4.297-3.36 8.375-8.875 8.375h-6.531V12.36Zm6.234 14.578h-3.828V14.53h3.703c4.688 0 6.828 2.844 6.828 6.203 0 2.063-1.25 6.203-6.703 6.203Z" clip-rule="evenodd"/></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/></symbol><symbol id="icon-submit-open" viewBox="0 0 16 17"><path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero"/></symbol></svg>
</div>
</footer>








<div class="c-site-messages message u-hide u-hide-print c-site-messages--nature-briefing c-site-messages--nature-briefing-email-variant c-site-messages--nature-briefing-redesign-2020 sans-serif "
data-component-id="nature-briefing-banner"
data-component-expirydays="30"
data-component-trigger-scroll-percentage="15"
data-track="in-view"
data-track-action="in-view"
data-track-category="nature briefing"
data-track-label="Briefing banner visible: Flagship">


    <div class="c-site-messages__banner-large">


<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__form-container">

            <div class="grid grid-12 last">
                <div class="grid grid-4">
                    <img alt="Nature Briefing AI and Robotics" src="/static/images/logos/nature-briefing-ai-and-robotics-logo-51b3cf6c52.svg" width="400" height="40">
                    <p class="c-site-messages--nature-briefing__strapline extra-tight-line-height">Sign up for the <em>Nature Briefing: AI and Robotics</em> newsletter — what matters in AI and robotics research, free to your inbox weekly.</p>
                </div>
                <div class="grid grid-8 last">
                    <form action="https://www.nature.com/briefing/ai_and_robotics" method="post" data-location="banner" data-track="signup_nature_briefing_banner" data-track-action="transmit-form" data-track-category="nature briefing" data-track-label="Briefing banner submit: Flagship">
                        <input id="briefing-banner-signup-form-input-track-originReferralPoint" type="hidden" name="track_originReferralPoint" value="AIAndRoboticsBriefingBanner">
                        <input id="briefing-banner-signup-form-input-track-formType" type="hidden" name="track_formType" value="DirectEmailBanner">

                        <input type="hidden" value="false" name="gdpr_tick" id="gdpr_tick_banner">
                        <input type="hidden" value="false" name="marketing" id="marketing_input_banner">
                        <input type="hidden" value="false" name="marketing_tick" id="marketing_tick_banner">
                        <input type="hidden" value="AIAndRoboticsBriefingBanner" name="brieferEntryPoint" id="brieferEntryPoint_banner">

                        <label class="nature-briefing-banner__email-label" for="emailAddress">Email address</label>

                        <div class="nature-briefing-banner__email-wrapper">
                            <input class="nature-briefing-banner__email-input box-sizing text14" type="email" id="emailAddress" name="emailAddress" value="" placeholder="e.g. jo.smith@university.ac.uk" required data-test-element="briefing-emailbanner-email-input">

                            <input type="hidden" value="true" name="N:ai_and_robotics" id="defaultNewsletter_banner">
                            <button type="submit" class="nature-briefing-banner__submit-button box-sizing text14" data-test-element="briefing-emailbanner-signup-button">Sign up</button>
                        </div>

                        <div class="nature-briefing-banner__checkbox-wrapper grid grid-12 last">
                            <input class="nature-briefing-banner__checkbox-checkbox" id="gdpr-briefing-banner-checkbox" type="checkbox" name="gdpr" value="true" data-test-element="briefing-emailbanner-gdpr-checkbox" required>
                            <label class="nature-briefing-banner__checkbox-label box-sizing text13 sans-serif block tighten-line-height" for="gdpr-briefing-banner-checkbox">I agree my information will be processed in accordance with the <em>Nature</em> and Springer Nature Limited <a href="https://www.nature.com/info/privacy">Privacy Policy</a>.</label>
                        </div>
                    </form>
                </div>
            </div>

        </div>

    </div>


    <div class="c-site-messages__banner-small">


<div class="c-site-messages__close-container">
    <button class="c-site-messages__close"
        data-track="click"
        data-track-category="nature briefing"
        data-track-label="Briefing banner dismiss: Flagship">
        <svg width="25px" height="25px" focusable="false" aria-hidden="true" viewBox="0 0 25 25" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
            <title>Close banner</title>
            <defs></defs>
            <g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
                <rect opacity="0" x="0" y="0" width="25" height="25"></rect>
                <path d="M6.29679575,16.2772478 C5.90020818,16.6738354 5.90240728,17.3100587 6.29617427,17.7038257 C6.69268654,18.100338 7.32864195,18.0973145 7.72275218,17.7032043 L12,13.4259564 L16.2772478,17.7032043 C16.6738354,18.0997918 17.3100587,18.0975927 17.7038257,17.7038257 C18.100338,17.3073135 18.0973145,16.671358 17.7032043,16.2772478 L13.4259564,12 L17.7032043,7.72275218 C18.0997918,7.32616461 18.0975927,6.68994127 17.7038257,6.29617427 C17.3073135,5.89966201 16.671358,5.90268552 16.2772478,6.29679575 L12,10.5740436 L7.72275218,6.29679575 C7.32616461,5.90020818 6.68994127,5.90240728 6.29617427,6.29617427 C5.89966201,6.69268654 5.90268552,7.32864195 6.29679575,7.72275218 L10.5740436,12 L6.29679575,16.2772478 Z" fill="#ffffff"></path>
            </g>
        </svg>
        <span class="visually-hidden">Close</span>
    </button>
</div>


        <div class="c-site-messages__content text14">
            <span class="c-site-messages--nature-briefing__strapline strong">Get the most important science stories of the day, free in your inbox.</span>
            <a class="nature-briefing__link text14 sans-serif"
                data-track="click"
                data-track-category="nature briefing"
                data-track-label="Small-screen banner CTA to site"
                data-test-element="briefing-banner-link"
                target="_blank"
                rel="noreferrer noopener"
                href="/briefing/ai-and-robotics/?brieferEntryPoint=AIAndRoboticsBriefingBanner">Sign up for Nature Briefing: AI and Robotics
            </a>
        </div>

    </div>

</div>






<noscript>
    <img hidden src="https://verify.nature.com/verify/nature.png" width="0" height="0" style="display: none" alt="">
</noscript>




<script src="//content.readcube.com/ping?doi=10.1038/s41598-025-85715-7&amp;format=js&amp;last_modified=2025-01-10" async></script>
</body>
</html>            </div>
        </div>

    </div>
</body>
</html>
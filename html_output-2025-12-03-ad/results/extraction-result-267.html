<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-267 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-267</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-267</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-267897566</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.14874v2.pdf" target="_blank">Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation</a></p>
                <p><strong>Paper Abstract:</strong> We propose a straightforward approach called Distillation Contrastive Decoding (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during inference. In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought Prompting and advanced distillation techniques, including Dropout and Quantization. This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands. By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage. Our evaluations demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e267.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e267.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-7B (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2, 7 billion parameters (evaluation on GSM8K arithmetic benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of a 7B open-weight LLM on grade-school multi-step arithmetic (GSM8K) comparing greedy, Contrastive Prompting (CP), Contrastive Decoding (CD), DoLA and Distillation Contrastive Decoding (DCD) variants (Dropout/Quantization).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>grade-school multi-step arithmetic: addition, subtraction, multiplication, division (word problems requiring sequences of elementary calculations)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>grade-school level multi-step problems (GSM8K); specific numeric ranges not specified in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot chain-of-thought prompting (8-shot expert CoT), Distillation Contrastive Decoding (DCD) using distilled amateur via Dropout (primary), also evaluated quantization (AWQ/GPTQ) and combined; hyperparameters: α=0.1, β=0.5 for Llama2 on GSM8K, dropout γ explored (optimal ~0.2-0.4); comparison baselines: greedy, CP, CD, DoLA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>GSM8K accuracy (Table 1): Greedy 14.32%, CP 14.25%, CD 15.39%, DoLA 14.03%, DCD (Dropout) 17.28%, DCD (Quant) 16.00%, DCD (Both) 16.00%. (8-shot expert / 3-shot synthetic amateur setup described)</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>DCD works by contrasting expert logits (se) with a distilled amateur's logits (sa) using s = (1+β)*se − β*sa; the amateur is produced at inference via dropout or quantization to intentionally generate systematic mistakes (invalid CoT examples), so the contrast suppresses tokens that the amateur (which tends to produce incorrect rationales/results) favors. The paper frames arithmetic CoT in terms of 'bridging objects' (numbers/equations) and 'language templates' and uses contrastive corruptions (number shuffles, calculation-errors, irrelevant objects, synthetic invalid CoTs) to expose incorrect reasoning patterns for subtraction in the contrastive step. No low-level mechanistic/neuron-level attribution (e.g., attention head roles or carry-tracking) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>DCD yields modest absolute improvements on Llama2-7B; authors report a +2.9% improvement (relative change tied to model base knowledge / MMLU correlation) when applying DCD on arithmetic GSM8K for MMLU-strong models; DCD effectiveness depends on base model strength (higher MMLU correlates with larger DCD gains).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Too little or too much dropout degrades performance (optimal dropout ~0.2-0.4); overly long chain-of-thought generations can introduce errors (DCD tends to produce fewer tokens than CP/CD); quantized 'amateurs' did not always help and sometimes reduced reasoning accuracy; inclusion of many valid+invalid shots can confuse unaligned models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Greedy, Contrastive Prompting (CP), Contrastive Decoding (CD) with external smaller amateur, DoLA, and non-distilled CP+CD (Table 2). DCD (Dropout) outperforms CP, CD and DoLA on GSM8K for Llama2-7B.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Using a distilled amateur at inference (via dropout/quantization) within a contrastive decoding framework (DCD) improves multi-step grade-school arithmetic accuracy over greedy, CP and CD baselines for Llama2-7B, with dropout-based distillation giving the best gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e267.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e267.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2-13B (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 2, 13 billion parameters (evaluation on GSM8K arithmetic benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of a larger Llama2 model (13B) on GSM8K arithmetic problems showing higher absolute accuracy and improvements from DCD compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>grade-school multi-step arithmetic: addition, subtraction, multiplication, division (word problems requiring sequences of elementary calculations)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>grade-school level multi-step problems (GSM8K); specific numeric ranges not specified in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot chain-of-thought prompting (8-shot expert CoT), Distillation Contrastive Decoding (DCD) with Dropout (primary) and quantization variations; α=0.1, β=0.5 for Llama2 models on GSM8K; dropout γ tuned (optimal 0.2-0.4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>GSM8K accuracy (Table 1): Greedy 29.42%, CP 25.78%, CD 32.83%, DoLA 28.81%, DCD (Dropout) 33.21%, DCD (Quant) 31.30%, DCD (Both) 32.20%.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Same contrastive mechanism as for 7B: DCD uses differences between expert logits and a distilled (error-prone) amateur's logits produced via dropout/quantization; invalid CoT examples for amateur emphasize incorrect bridging objects and templates so the contrast suppresses incorrect tokens. No deeper mechanistic claims (no head-level or representational decomposition) are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Larger model (13B) attains substantially higher absolute accuracy than 7B; DCD provides incremental gains over CD for the 13B model (DCD Dropout 33.21% vs CD 32.83% on GSM8K), showing DCD is applicable across sizes and benefits stronger models.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>As with 7B: sensitivity to dropout magnitude, quantized amateurs sometimes underperform, longer CoT outputs can introduce more reasoning errors; DCD tends to produce fewer tokens which correlates with better final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Greedy, CP, CD, DoLA and CP+CD; Table 1 and Table 2 report the detailed comparisons for GSM8K and StrategyQA.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>DCD improves arithmetic accuracy even for larger LLMs (13B), giving a small but consistent improvement over CD and other baselines on GSM8K when using dropout-based distillation at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e267.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e267.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral 7B (evaluation on GSM8K arithmetic benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of Mistral-7B on GSM8K showing that DCD yields larger absolute improvements on a model with strong base knowledge; β tuned at 0.8 for GSM8K experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>grade-school multi-step arithmetic (addition, subtraction, multiplication, division) as in GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>grade-school level multi-step problems (GSM8K); specific numeric ranges not specified in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DCD with distilled amateur produced via dropout (primary) and quantization variants; few-shot CoT; β set to 0.8 for Mistral on GSM8K; α=0.1 fixed; dropout rate γ tuned (optimal ~0.2-0.4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>The paper reports that DCD yields notable improvements on Mistral-7B for arithmetic (GSM8K), quantified as a +6.8% improvement tied to the model's base knowledge correlation; exact absolute numbers in the main table are reported but are presented in a compact table format in the paper (DCD outperforms baselines on GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Identical high-level mechanism: contrast expert logits with a distilled amateur producing invalid CoTs (via dropout/quantization) so that incorrect bridging objects/language templates are suppressed. The paper emphasizes that models with stronger base knowledge (higher MMLU) get larger DCD gains, but offers no neuron-level mechanistic explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>DCD is more effective on stronger-base models; authors observed a +6.8% improvement for Mistral (reported in the paper) on GSM8K correlated with higher MMLU scores.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Same constraints: dropout magnitude sensitivity, quantization sometimes harms, CoT length trade-offs; no novel arithmetic-specific failure modes beyond these high-level observations are described.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared vs Greedy, CP, CD, DoLA; DCD outperforms these baselines on GSM8K for Mistral-7B according to the reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>DCD delivers the largest arithmetic (GSM8K) gains on models with stronger base knowledge (e.g., Mistral-7B), supporting that distillation-based contrastive decoding leverages base knowledge to improve arithmetic reasoning via contrasting valid and invalid CoT demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e267.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e267.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-7B (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek 7B (evaluation on GSM8K arithmetic benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of DeepSeek-7B on GSM8K arithmetic tasks within the same DCD vs CP/CD/DoLA comparison framework; DCD variants (Dropout/Quant/Both) are applied.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>grade-school multi-step arithmetic (addition, subtraction, multiplication, division) as in GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>grade-school level multi-step problems (GSM8K); specific numeric ranges not specified in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DCD with distilled amateur via dropout and quantization variants; few-shot CoT; β set to 0.8 for DeepSeek/ Mistral on GSM8K per paper, α=0.1 fixed; dropout rate γ tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>The paper reports that DCD improves arithmetic and commonsense reasoning on DeepSeek-7B (GSM8K results included in Table 1). Exact per-cell numbers for every configuration are shown in the paper's Table 1; the authors state DCD yields improvements compared to CD and CP for DeepSeek as well, though quantization sometimes underperforms dropout.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Same high-level mechanism: contrast expert logits with a distilled amateur (dropout/quantized) to suppress tokens favored by incorrect reasoning patterns; invalid CoT designs (number-shuffle, calculation-error, irrelevant-object/exchange-sign, synthetic invalid CoTs) are critical for arithmetic contrastive signals. No mechanistic claim about internal representations beyond this contrastive-logits mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>DCD effectiveness correlates with base model knowledge; specific scaling numbers for DeepSeek are reported in the paper's tables but not singled out in prose beyond the general trend.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Dropout rate sensitivity, quantization sometimes harms performance, and long CoTs can introduce reasoning errors; these general failure modes apply to DeepSeek too.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Greedy, CP, CD, DoLA; DCD variants generally beat baselines on the tasks reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>DCD improves arithmetic reasoning of DeepSeek-7B by using a distilled amateur at inference to provide useful contrastive negative signals; dropout-based distillation was typically superior to quantization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e267.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e267.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama1 (7B & 13B) (GSM8K)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 1 family (7B and 13B) evaluated on GSM8K arithmetic benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of Llama1 models reported in appendix and tables, showing DCD yields improvements over earlier CD/CP/DoLA baselines on GSM8K arithmetic tasks though gains are smaller compared to stronger newer models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 1</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>grade-school multi-step arithmetic (addition, subtraction, multiplication, division) as in GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>grade-school level multi-step problems (GSM8K); specific numeric ranges not specified in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>DCD with Dropout/Quantization/Both, few-shot CoT prompting; hyperparameters explored (α=0.1, β varied), CP+CD vs DCD comparisons shown (Table 2 and Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Appendix/Table excerpts show DCD improves over CP, CD, DoLA for Llama1 models on GSM8K; specific table entries list DCD GSM8K accuracies (e.g., Llama1-7B DCD ~12.16% vs CD 11.45% in one table excerpt), but these baseline Llama1 numbers are lower in absolute terms than Llama2/Mistral reported elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Same contrastive logits mechanism using distilled amateurs produced via dropout/quantization; paper does not provide internal mechanistic decomposition beyond this operational description.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Smaller/older base models (Llama1) see smaller absolute gains from DCD (the paper reports a +0.7% figure for Llama1-7B in the MMLU-correlation statement), indicating DCD benefits scale with base model quality.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Sensitivity to dropout magnitude and shot design; CP with many valid+invalid examples can confuse unaligned Llama1 variants more than stronger models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared vs CP, CD, DoLA and CP+CD; DCD shows consistent but smaller improvements for Llama1 relative to newer/better models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>DCD gives consistent improvements across model families including Llama1, but the magnitude of arithmetic improvement is correlated with base-model quality (weaker models get smaller absolute gains).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GSM8K <em>(Rating: 2)</em></li>
                <li>Contrastive decoding: Open-ended text generation as optimization <em>(Rating: 2)</em></li>
                <li>Contrastive Chain-of-thought Prompting <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Contrastive decoding improves reasoning in large language models <em>(Rating: 2)</em></li>
                <li>AWQ: Activation-aware weight quantization for LLM compression and acceleration <em>(Rating: 1)</em></li>
                <li>GPTQ: Accurate post-training quantization for generative pre-trained transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-267",
    "paper_id": "paper-267897566",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Llama2-7B (GSM8K)",
            "name_full": "Llama 2, 7 billion parameters (evaluation on GSM8K arithmetic benchmark)",
            "brief_description": "Evaluation of a 7B open-weight LLM on grade-school multi-step arithmetic (GSM8K) comparing greedy, Contrastive Prompting (CP), Contrastive Decoding (CD), DoLA and Distillation Contrastive Decoding (DCD) variants (Dropout/Quantization).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "grade-school multi-step arithmetic: addition, subtraction, multiplication, division (word problems requiring sequences of elementary calculations)",
            "number_range_or_complexity": "grade-school level multi-step problems (GSM8K); specific numeric ranges not specified in the paper",
            "method_or_intervention": "few-shot chain-of-thought prompting (8-shot expert CoT), Distillation Contrastive Decoding (DCD) using distilled amateur via Dropout (primary), also evaluated quantization (AWQ/GPTQ) and combined; hyperparameters: α=0.1, β=0.5 for Llama2 on GSM8K, dropout γ explored (optimal ~0.2-0.4); comparison baselines: greedy, CP, CD, DoLA",
            "performance_result": "GSM8K accuracy (Table 1): Greedy 14.32%, CP 14.25%, CD 15.39%, DoLA 14.03%, DCD (Dropout) 17.28%, DCD (Quant) 16.00%, DCD (Both) 16.00%. (8-shot expert / 3-shot synthetic amateur setup described)",
            "mechanistic_insight": "DCD works by contrasting expert logits (se) with a distilled amateur's logits (sa) using s = (1+β)*se − β*sa; the amateur is produced at inference via dropout or quantization to intentionally generate systematic mistakes (invalid CoT examples), so the contrast suppresses tokens that the amateur (which tends to produce incorrect rationales/results) favors. The paper frames arithmetic CoT in terms of 'bridging objects' (numbers/equations) and 'language templates' and uses contrastive corruptions (number shuffles, calculation-errors, irrelevant objects, synthetic invalid CoTs) to expose incorrect reasoning patterns for subtraction in the contrastive step. No low-level mechanistic/neuron-level attribution (e.g., attention head roles or carry-tracking) is provided.",
            "performance_scaling": "DCD yields modest absolute improvements on Llama2-7B; authors report a +2.9% improvement (relative change tied to model base knowledge / MMLU correlation) when applying DCD on arithmetic GSM8K for MMLU-strong models; DCD effectiveness depends on base model strength (higher MMLU correlates with larger DCD gains).",
            "failure_modes": "Too little or too much dropout degrades performance (optimal dropout ~0.2-0.4); overly long chain-of-thought generations can introduce errors (DCD tends to produce fewer tokens than CP/CD); quantized 'amateurs' did not always help and sometimes reduced reasoning accuracy; inclusion of many valid+invalid shots can confuse unaligned models.",
            "comparison_baseline": "Compared against Greedy, Contrastive Prompting (CP), Contrastive Decoding (CD) with external smaller amateur, DoLA, and non-distilled CP+CD (Table 2). DCD (Dropout) outperforms CP, CD and DoLA on GSM8K for Llama2-7B.",
            "key_finding": "Using a distilled amateur at inference (via dropout/quantization) within a contrastive decoding framework (DCD) improves multi-step grade-school arithmetic accuracy over greedy, CP and CD baselines for Llama2-7B, with dropout-based distillation giving the best gains.",
            "uuid": "e267.0",
            "source_info": {
                "paper_title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Llama2-13B (GSM8K)",
            "name_full": "Llama 2, 13 billion parameters (evaluation on GSM8K arithmetic benchmark)",
            "brief_description": "Evaluation of a larger Llama2 model (13B) on GSM8K arithmetic problems showing higher absolute accuracy and improvements from DCD compared to baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 2",
            "model_size": "13B",
            "model_architecture": null,
            "arithmetic_operation_type": "grade-school multi-step arithmetic: addition, subtraction, multiplication, division (word problems requiring sequences of elementary calculations)",
            "number_range_or_complexity": "grade-school level multi-step problems (GSM8K); specific numeric ranges not specified in the paper",
            "method_or_intervention": "few-shot chain-of-thought prompting (8-shot expert CoT), Distillation Contrastive Decoding (DCD) with Dropout (primary) and quantization variations; α=0.1, β=0.5 for Llama2 models on GSM8K; dropout γ tuned (optimal 0.2-0.4).",
            "performance_result": "GSM8K accuracy (Table 1): Greedy 29.42%, CP 25.78%, CD 32.83%, DoLA 28.81%, DCD (Dropout) 33.21%, DCD (Quant) 31.30%, DCD (Both) 32.20%.",
            "mechanistic_insight": "Same contrastive mechanism as for 7B: DCD uses differences between expert logits and a distilled (error-prone) amateur's logits produced via dropout/quantization; invalid CoT examples for amateur emphasize incorrect bridging objects and templates so the contrast suppresses incorrect tokens. No deeper mechanistic claims (no head-level or representational decomposition) are provided.",
            "performance_scaling": "Larger model (13B) attains substantially higher absolute accuracy than 7B; DCD provides incremental gains over CD for the 13B model (DCD Dropout 33.21% vs CD 32.83% on GSM8K), showing DCD is applicable across sizes and benefits stronger models.",
            "failure_modes": "As with 7B: sensitivity to dropout magnitude, quantized amateurs sometimes underperform, longer CoT outputs can introduce more reasoning errors; DCD tends to produce fewer tokens which correlates with better final answers.",
            "comparison_baseline": "Compared to Greedy, CP, CD, DoLA and CP+CD; Table 1 and Table 2 report the detailed comparisons for GSM8K and StrategyQA.",
            "key_finding": "DCD improves arithmetic accuracy even for larger LLMs (13B), giving a small but consistent improvement over CD and other baselines on GSM8K when using dropout-based distillation at inference.",
            "uuid": "e267.1",
            "source_info": {
                "paper_title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Mistral-7B (GSM8K)",
            "name_full": "Mistral 7B (evaluation on GSM8K arithmetic benchmark)",
            "brief_description": "Evaluation of Mistral-7B on GSM8K showing that DCD yields larger absolute improvements on a model with strong base knowledge; β tuned at 0.8 for GSM8K experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "grade-school multi-step arithmetic (addition, subtraction, multiplication, division) as in GSM8K",
            "number_range_or_complexity": "grade-school level multi-step problems (GSM8K); specific numeric ranges not specified in the paper",
            "method_or_intervention": "DCD with distilled amateur produced via dropout (primary) and quantization variants; few-shot CoT; β set to 0.8 for Mistral on GSM8K; α=0.1 fixed; dropout rate γ tuned (optimal ~0.2-0.4).",
            "performance_result": "The paper reports that DCD yields notable improvements on Mistral-7B for arithmetic (GSM8K), quantified as a +6.8% improvement tied to the model's base knowledge correlation; exact absolute numbers in the main table are reported but are presented in a compact table format in the paper (DCD outperforms baselines on GSM8K).",
            "mechanistic_insight": "Identical high-level mechanism: contrast expert logits with a distilled amateur producing invalid CoTs (via dropout/quantization) so that incorrect bridging objects/language templates are suppressed. The paper emphasizes that models with stronger base knowledge (higher MMLU) get larger DCD gains, but offers no neuron-level mechanistic explanation.",
            "performance_scaling": "DCD is more effective on stronger-base models; authors observed a +6.8% improvement for Mistral (reported in the paper) on GSM8K correlated with higher MMLU scores.",
            "failure_modes": "Same constraints: dropout magnitude sensitivity, quantization sometimes harms, CoT length trade-offs; no novel arithmetic-specific failure modes beyond these high-level observations are described.",
            "comparison_baseline": "Compared vs Greedy, CP, CD, DoLA; DCD outperforms these baselines on GSM8K for Mistral-7B according to the reported results.",
            "key_finding": "DCD delivers the largest arithmetic (GSM8K) gains on models with stronger base knowledge (e.g., Mistral-7B), supporting that distillation-based contrastive decoding leverages base knowledge to improve arithmetic reasoning via contrasting valid and invalid CoT demonstrations.",
            "uuid": "e267.2",
            "source_info": {
                "paper_title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "DeepSeek-7B (GSM8K)",
            "name_full": "DeepSeek 7B (evaluation on GSM8K arithmetic benchmark)",
            "brief_description": "Evaluation of DeepSeek-7B on GSM8K arithmetic tasks within the same DCD vs CP/CD/DoLA comparison framework; DCD variants (Dropout/Quant/Both) are applied.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DeepSeek",
            "model_size": "7B",
            "model_architecture": null,
            "arithmetic_operation_type": "grade-school multi-step arithmetic (addition, subtraction, multiplication, division) as in GSM8K",
            "number_range_or_complexity": "grade-school level multi-step problems (GSM8K); specific numeric ranges not specified in the paper",
            "method_or_intervention": "DCD with distilled amateur via dropout and quantization variants; few-shot CoT; β set to 0.8 for DeepSeek/ Mistral on GSM8K per paper, α=0.1 fixed; dropout rate γ tuned.",
            "performance_result": "The paper reports that DCD improves arithmetic and commonsense reasoning on DeepSeek-7B (GSM8K results included in Table 1). Exact per-cell numbers for every configuration are shown in the paper's Table 1; the authors state DCD yields improvements compared to CD and CP for DeepSeek as well, though quantization sometimes underperforms dropout.",
            "mechanistic_insight": "Same high-level mechanism: contrast expert logits with a distilled amateur (dropout/quantized) to suppress tokens favored by incorrect reasoning patterns; invalid CoT designs (number-shuffle, calculation-error, irrelevant-object/exchange-sign, synthetic invalid CoTs) are critical for arithmetic contrastive signals. No mechanistic claim about internal representations beyond this contrastive-logits mechanism.",
            "performance_scaling": "DCD effectiveness correlates with base model knowledge; specific scaling numbers for DeepSeek are reported in the paper's tables but not singled out in prose beyond the general trend.",
            "failure_modes": "Dropout rate sensitivity, quantization sometimes harms performance, and long CoTs can introduce reasoning errors; these general failure modes apply to DeepSeek too.",
            "comparison_baseline": "Compared against Greedy, CP, CD, DoLA; DCD variants generally beat baselines on the tasks reported.",
            "key_finding": "DCD improves arithmetic reasoning of DeepSeek-7B by using a distilled amateur at inference to provide useful contrastive negative signals; dropout-based distillation was typically superior to quantization.",
            "uuid": "e267.3",
            "source_info": {
                "paper_title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Llama1 (7B & 13B) (GSM8K)",
            "name_full": "Llama 1 family (7B and 13B) evaluated on GSM8K arithmetic benchmark",
            "brief_description": "Evaluation of Llama1 models reported in appendix and tables, showing DCD yields improvements over earlier CD/CP/DoLA baselines on GSM8K arithmetic tasks though gains are smaller compared to stronger newer models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 1",
            "model_size": "7B and 13B",
            "model_architecture": null,
            "arithmetic_operation_type": "grade-school multi-step arithmetic (addition, subtraction, multiplication, division) as in GSM8K",
            "number_range_or_complexity": "grade-school level multi-step problems (GSM8K); specific numeric ranges not specified in the paper",
            "method_or_intervention": "DCD with Dropout/Quantization/Both, few-shot CoT prompting; hyperparameters explored (α=0.1, β varied), CP+CD vs DCD comparisons shown (Table 2 and Table 3).",
            "performance_result": "Appendix/Table excerpts show DCD improves over CP, CD, DoLA for Llama1 models on GSM8K; specific table entries list DCD GSM8K accuracies (e.g., Llama1-7B DCD ~12.16% vs CD 11.45% in one table excerpt), but these baseline Llama1 numbers are lower in absolute terms than Llama2/Mistral reported elsewhere.",
            "mechanistic_insight": "Same contrastive logits mechanism using distilled amateurs produced via dropout/quantization; paper does not provide internal mechanistic decomposition beyond this operational description.",
            "performance_scaling": "Smaller/older base models (Llama1) see smaller absolute gains from DCD (the paper reports a +0.7% figure for Llama1-7B in the MMLU-correlation statement), indicating DCD benefits scale with base model quality.",
            "failure_modes": "Sensitivity to dropout magnitude and shot design; CP with many valid+invalid examples can confuse unaligned Llama1 variants more than stronger models.",
            "comparison_baseline": "Compared vs CP, CD, DoLA and CP+CD; DCD shows consistent but smaller improvements for Llama1 relative to newer/better models.",
            "key_finding": "DCD gives consistent improvements across model families including Llama1, but the magnitude of arithmetic improvement is correlated with base-model quality (weaker models get smaller absolute gains).",
            "uuid": "e267.4",
            "source_info": {
                "paper_title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GSM8K",
            "rating": 2
        },
        {
            "paper_title": "Contrastive decoding: Open-ended text generation as optimization",
            "rating": 2,
            "sanitized_title": "contrastive_decoding_openended_text_generation_as_optimization"
        },
        {
            "paper_title": "Contrastive Chain-of-thought Prompting",
            "rating": 2,
            "sanitized_title": "contrastive_chainofthought_prompting"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Contrastive decoding improves reasoning in large language models",
            "rating": 2,
            "sanitized_title": "contrastive_decoding_improves_reasoning_in_large_language_models"
        },
        {
            "paper_title": "AWQ: Activation-aware weight quantization for LLM compression and acceleration",
            "rating": 1,
            "sanitized_title": "awq_activationaware_weight_quantization_for_llm_compression_and_acceleration"
        },
        {
            "paper_title": "GPTQ: Accurate post-training quantization for generative pre-trained transformers",
            "rating": 1,
            "sanitized_title": "gptq_accurate_posttraining_quantization_for_generative_pretrained_transformers"
        }
    ],
    "cost": 0.017279,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation
23 Aug 2024</p>
<p>Phuc Phan 
Hieu Tran 
Long Phan 
Vietai Research 
Jinze Bai 
Shuai Bai 
Yunfei Chu 
Zeyu Cui 
Kai Dang 
Xiaodong Deng 
Yang Fan 
Wenbin Ge 
Yu Han 
Fei Huang 
Binyuan Hui 
Luo Ji 
Mei Li 
Junyang Lin 
Trenton Bricken 
Adly Templeton 
Joshua Batson 
Brian Chen 
Adam Jermyn 
Tom Conerly 
Nick Turner 
Cem Anil 
Carson Denison 
Amanda Askell 
Robert Lasenby 
Yifan Wu 
Shauna Kravec 
Nicholas Schiefer 
Tim Maxwell 
Nicholas Joseph 
Zac Hatfield-Dodds 
Alex Tamkin 
Karina Nguyen 
Brayden Mclean 
Josiah E Burke 
Tristan Hume 
Shan Carter 
Tom Henighan 
Christopher 2023 Olah 
Tom B Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Sandhini Agarwal 
Ariel Herbert-Voss 
Gretchen Krueger 
Rewon Child 
Aditya Ramesh 
Daniel M Ziegler 
Jeffrey Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Mateusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Sam Mccandlish 
Alec Radford 
Ilya Sutskever 
Sam Mc- Candlish 
Karl Cobbe 
Vineet Kosaraju 
Mohammad Bavarian 
Heewoo Jun 
Lukasz Kaiser 
Matthias Plappert 
Jerry Tworek 
Jacob Hilton 
Reiichiro Nakano 
Xiao Bi 
Deli Chen 
Guanting Chen 
Shanhuang Chen 
Damai Dai 
Chengqi Deng 
Honghui Ding 
Kai Dong 
Qiushi Du 
Zhe Fu 
Huazuo Gao 
Kaige Gao 
Wenjun Gao 
Ruiqi Ge 
Kang Guan 
Daya Guo 
Jianzhong Guo 
Guangbo Hao 
Zhewen Hao 
Ying He 
Wenjie Hu 
Panpan Huang 
Erhang Li 
Guowei Li 
Jiashi Li 
Yao K Li 
Wenfeng Liang 
Fangyun Lin 
A X Liu 
Bo Liu 
Wen Liu 
Xiaodong Liu 
Xin Liu 
Yiyuan Liu 
Haoyu Lu 
Shanghao Lu 
Fuli Luo 
Shirong Ma 
Xiaotao Nie 
Tian Pei 
Yishi Piao 
Junjie Qiu 
Hui Qu 
Wentao Zhang 
Yichao Zhang 
Chenggang Zhang 
Yao Zhao 
Shangyan Zhao 
Shunfeng Zhou 
Qihao Zhou 
Yuheng Zhu 
Zou 
Deepseek </p>
<p>Runji Lin
Dayiheng Liu, Chengqiang Lu, Jianxin MaGao Liu, Keming Lu, Rui Men</p>
<p>Xingzhang Ren, Xuancheng Ren
Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-guang Wu, Benfeng Xu, Jin Xu, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Jianwei ZhangChuanqi Tan, An Yang, Hao Yang, Hongyi Yuan, Zheng Yuan</p>
<p>Xingx-uan Zhang
Yichang ZhangZhenru Zhang, Chang</p>
<p>Tongzheng Ren, Zehui Ren
Chong Ruan, Zhangli Sha
Zhihong Shao, Jingxiang Sun, Minghui TangJunxiao Song, Xuecheng Su, Yaofeng Sun</p>
<p>Bingx-uan Wang
Shiyu Wang, Tong Wu, Y. Wu, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Liyue ZhangPeiyi Wang, Yaohui Wang, Yongji Wang, Xin Xie, Haowei Zhang, Lecong Zhang, Mingchuan ZhangMinghua</p>
<p>Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation
23 Aug 202468B3C8C5157E1034398AA187A40F910EarXiv:2402.14874v2[cs.CL]Towards monosemanticity: Decomposing language models with dictionary learningTransformer Circuits ThreadHttps://transformercircuitspub/2023/monosemanticfeatures/indexhtml
We propose a straightforward approach called Distillation Contrastive Decoding (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during inference.In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought Prompting and advanced distillation techniques, including Dropout and Quantization.This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands.By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage.Our evaluations demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets. 1* * Equal contribution 1 Code is available at https://github.com/pphuc25/distil-cd</p>
<p>Introduction</p>
<p>Reasoning capabilities in LLMs refer to the models' ability to analyze, understand, and infer information, mirroring human-like logical reasoning.Recently, the reasoning skills of LLMs have seen substantial advancements, showcasing their vast potential in various natural language processing applications (Brown et al., 2020a).While some research focuses on enhancing models through advanced training techniques and architectures (Touvron et al., 2023a;Jiang et al., 2023;Bai et al., 2023), others aim to augment the models' internal capabilities (Zou et al., 2023;Bricken et al., 2023).Beyond model training and augmentation, further research explores innovative methods to enhance LLM efficiency during inference (Li et al., 2023b,a;Chuang et al., 2023).In this work, we introduce Distillation Contrastive Decoding (DCD), a method designed to enhance the reasoning abilities of LLMs during inference by leveraging Contrastive Chain-of-thought prompts and distillation.</p>
<p>DCD builds on recent advancements in enhancing the reasoning capabilities of LLMs through Contrastive Decoding (CD) (O' Brien and Lewis, 2023) and Contrastive Chain-of-thought Prompting (CP) (Chia et al., 2023).These methods utilize contrasting elements to reduce reasoning errors in text generation, thereby improving task performance.The primary motivation behind DCD is addressing two common limitations of CD.Firstly, CD typically requires a smaller amateur LLM within the same family to evaluate the outputs of the primary LLM.This prerequisite poses a challenge, especially for small-sized models, as smaller models with identical vocabularies may not be available.This challenge is notably present in cases such as Mistral-7B (Jiang et al., 2023) and DeepSeek-7B (DeepSeek-AI et al., 2024), where smaller models are unavailable.The second limitation with CD is the requirement to simultaneously load two models into memory: an expert and an amateur model, which significantly increases computational resource demands.An example of this is using Llama2-7B as the amateur model and Llama2-13B as the expert model, highlighting the resourceintensive nature of the CD approach.</p>
<p>Our findings demonstrate that DCD surpasses existing methodologies in enhancing Chain-ofthought reasoning within LLMs.Specifically, on the GSM8K benchmark, which comprises gradeschool level word math problems, DCD elevates the performance of Llama2 models by as much as 3.79% and exhibits a performance increase of 1.89% over CD.On StrategyQA, DCD outperforms all existing methods by a significant gap.Notably, it helps Llama2 models in achieving performance enhancements of up to 5.9%.We ob-Figure 1: An overview of Distillation Contrastive Decoding method.Valid CoT demonstrations as well as the query will be sent to an LLM, while invalid CoT demonstrations and the query will be sent into a distilled version of the model.We will then use this logit information to enhance the reasoning decoding process.</p>
<p>serve marked improvements in both arithmetic and commonsense reasoning tasks when DCD is applied to Mistral-7B, known for its robust foundational knowledge and high scores on the MMLU benchmark (Hendrycks et al., 2020), suggesting that DCD could bring such widespread improvements to much stronger models.</p>
<p>In summary, our main contributions are: (1) Introducing a straightforward approach that combines Contrastive Chain-of-thought Prompting, Contrastive Decoding, and Distillation to enhance LLM reasoning abilities, eliminating the need for smaller models and reducing memory usage.(2) Demonstrating significant performance improvements across multiple reasoning benchmarks compared to Contrastive Decoding and other methods.</p>
<p>Related Works</p>
<p>Chain-of-thought (CoT) is a significant development in enhancing text-generation models' reasoning capabilities.This concept, as originally introduced by (Wei et al., 2023), involves the model generating intermediate steps in its reasoning process, akin to human problem-solving methods.Furthermore, the work by (Kojima et al., 2023) revealed that specific prompts, such as "Let's think stepby-step", can spontaneously trigger CoT reasoning in LLMs.These developments are the foundation for research works on enhancing LLM's reasoning abilities.</p>
<p>Recently, O 'Brien and Lewis (2023) demonstrated that CD, a decoding method proposed by Li et al. (2023b), can enhance LLM performance across a range of reasoning tasks.Initially, CD was designed to enhance the quality of long-form text generation by identifying tokens that significantly differ in likelihood between a strong model and a comparatively weak model.The study by O 'Brien and Lewis (2023)</p>
<p>Methodology</p>
<p>Our approach, DCD, builds upon the foundational work of CD (O' Brien and Lewis, 2023) and CP (Chia et al., 2023).A principal motivation behind DCD is to overcome a significant limitation of CD: its reliance on a smaller model of the same architecture, often referred to as an amateur model.This de- pendency poses significant challenges, as an equivalent amateur model is not always available across different open-source architectures, a situation exemplified by Mistral (Jiang et al., 2023).DCD aims to offer a more adaptable and inclusive solution, irrespective of the specific class of language model employed.</p>
<p>Contrastive Decoding</p>
<p>CD involves two models: a larger expert model, and a smaller amateur model.The method leverages a comparison between the predicted logits of an expert model, denoted as s e , and those of an amateur model, denoted as s a , to compute greedy decoding information.A hyperparameter β is introduced as an amateur penalty.The next greedy decoding token s is defined as:
s = (1 + β) • s e − β • s a
By exploiting the differences in predictive confidence between the two models, this method improves the generation of text sequences in reasoning tasks.However, the work shows that while a 1B-parameter amateur helps improve reasoning capabilities, a 7B-parameter amateur harms it.This poses a significant drawback as not all model classes have a 1B-parameter model to act as an amateur model in the decoding process.</p>
<p>Contrastive Chain-of-thought Prompting</p>
<p>CP integrates both correct and incorrect reasoning examples to direct the model through a step-bystep reasoning process, thereby minimizing logical errors.This method is inspired by the human ability to learn from both successful and unsuccessful examples.By including examples of both sound and flawed reasoning, the technique aids the model in identifying and correcting potential mistakes in intermediate reasoning steps.Such errors have been identified as significant obstacles to accurate reasoning processes (Ling et al., 2023).</p>
<p>Concretely, given a query Q and a set of CoT examples D = {E 1 , ..., E n }, the goal of the model is to generate a target A. The method can be formulated as:
A j = (Q j , E 1+ , E 1− , ..., E n+ , E n− )
However, the method tends to extend the length of input sequences significantly, necessitating increased computational resources.In our experiments, we have also observed that the inclusion of multiple shots of both valid and invalid demonstrations can lead to confusion in an unaligned LLM, consequently diminishing its reasoning performance.</p>
<p>Distillation Contrastive Decoding (Ours)</p>
<p>DCD is designed to overcome existing drawbacks in both CD and CP.Instead of requiring an external 1B-parameters amateur model, we utilize distillation techniques such as Dropout and Quantization to acquire the amateur reasoning information.</p>
<ol>
<li>
<p>Dropout is applied to the attention weights, randomly omitting some to introduce variability and simulate erroneous reasoning patterns.This helps in creating a model that systematically generates the kinds of mistakes necessary for our contrastive approach.However, this approach is inherently nondeterministic and requires careful tuning to set the appropriate dropout rate.</p>
</li>
<li>
<p>Quantization reduces data complexity by converting higher-bit formats, like 32-bit floats, into lower-bit ones, such as 8-bit integers.This involves rescaling the original data to fit the range of the lower-bit data type, typically using normalization based on the maximum value of the input tensor elements.Unlike Dropout, this technique is deterministic, ensuring consistent results in each run.In our study, we mainly experimented with 4-bit AWQ (Lin et al., 2024), 4-bit GPTQ (Frantar et al., 2023), and 8-bit formats (Dettmers et al., 2022).</p>
</li>
</ol>
<p>To replace the 1B-parameters amateur model, it is crucial that this alternative not only exhibits weak reasoning abilities but also adheres to given instruction prompts.Our contrastive method relies on contrasting incorrect answers from the amateur model with the outputs from the expert model to derive the accurate conclusions.The likelihood of the amateur model producing incorrect rationales and results directly enhances the clarity and accuracy of the contrastive outcomes.We can achieve this through the distillation techniques that intentionally reduce the model's size and memory footprint, thus degrading its knowledge and performance.</p>
<p>For the anchor expert model, we employ regular valid CoT demonstrations as a few shot examples.For the distilled amateur model, we employ invalid CoT examples to enable the motivations in leveraging incorrect reasoning features in computing the next token weights.The DCD algorithm is shown in Algorithm 1.</p>
<p>In practice, we found that distilling the model by enabling a higher dropout rate during the in-
= Ma(Q, E 1− , .., E n− , C) Compute next token s = (1 + β) • se − β • sa
Append s to output sequence C end while return Sequence C ference step works best in most cases.The final results comparing between DCD with Dropout and previous baselines are shown in Section 6.Additionally, we explore other distillation methods such as Quantization, as well as a combined approach of applying both Dropout and Quantization to the model in Section 7.</p>
<p>Contrastive Chain-of-thought Design</p>
<p>Compared to conventional prompting methods with in-context demonstrations (Brown et al., 2020b), CoT prompting (Wei et al., 2023) enhances this approach by incorporating a rationale for each fewshot example.This rationale is composed of a sequence of intermediate reasoning steps, which effectively guide the language model through a systematic process to assist the model in understanding and solving complex tasks.(Wang et al., 2023) identifies two components of a CoT rationale:</p>
<p>• Bridging objects are the symbolic items that the model saw during the traverse to the final answer.In arithmetic reasoning, these are numbers and equations, while in factual/commonsense reasoning, these are subject and object entities.</p>
<p>• Language templates are the complementary parts of the bridging objects, which serve as textual hints and relations or predicates that guide the model to derive the correct bridging objects throughout the reasoning process.</p>
<p>Detailed examples of bridging objects and language template components of CoT reasoning are shown in Figure 10.</p>
<p>Building on previous research (Chia et al., 2023) that explores Contrastive Chain-of-thought Prompting design, we identified three types of contrasting bridging objects and one type of contrasting both bridging objects and language templates in arithmetic reasoning tasks.In our experiments on contrasting bridging objects, we explored three settings:</p>
<p>(1) Number shuffle involves shuffling the positions of numbers within arithmetic rationales, creating incoherencies in the logical flow of steps.</p>
<p>(2) Number shuffle plus Calculation error extends the number shuffle by intentionally introducing an error in the arithmetic operation's result.The final result of the equation is randomly adjusted by either adding or subtracting one, leading to a deliberately incorrect solution, such as in 1 + 1 = 3 or 1 + 1 = 0.</p>
<p>(3) Number shuffle plus Irrelevant object plus Exchange sign extends the number shuffle by manually substituting objects in arithmetic rationales with unrelated ones (e.g., trees to apples) and randomly changing operation signs (e.g., + to -).</p>
<p>For the Contrastive Chain-of-thought that involves contrasting both bridging objects and language templates, (4) we prompted GPT-3.5 to generate contrastive synthetic demonstrations.An example of each contrastive demonstration is shown in Figure 4. Figure 3 shows the accuracy of the four contrastive settings on the GSM8K dataset using the Llama2 model with our method (DCD).Each of the four contrasting designs demonstrates a different increase in score compared to baselines.These preliminary results suggest that the incorporation of both contrastive bridging objects and language templates is crucial for designing effective Contrastive Chain-of-thought demonstrations.Additionally, setting (4), which includes synthetic examples, shows a significant increase in score.This indicates that DCD can effectively utilize automatic synthetic contrastive prompting generation with an external LLM, such as GPT-3.5.</p>
<p>Experimental Settings</p>
<p>Benchmarks</p>
<p>To obtain results, we evaluated two domains of text generation: arithmetic reasoning and commonsense reasoning.For arithmetic reasoning, we utilized the GSM8K dataset (Cobbe et al., 2021), and for commonsense reasoning, the StrategyQA dataset (Geva et al., 2021) was employed.</p>
<p>(1)</p>
<p>(2)</p>
<p>(3) (4) 14  3) involve rule-based approaches for contrasting bridging objects.Setting (4) employs a synthetic-based approach, incorporating contrasts in both bridging objects and language templates.</p>
<p>Arithmetic Reasoning</p>
<p>The GSM8K dataset (Cobbe et al., 2021) is structured to facilitate question answering on fundamental mathematical problems that require multi-step reasoning for resolution.The solutions to these problems primarily involve performing a sequence of elementary calculations using basic arithmetic operations, including addition, subtraction, multiplication, and division.In our experimental setup, we employed the complete test set, which consisted of 1319 samples.We utilized an 8-shot for the expert model and a 3-shot (using synthetic demonstrations) for the amateur model.</p>
<p>Commonsense Reasoning</p>
<p>The StrategyQA dataset (Geva et al., 2021) is a question-answering benchmark focusing on opendomain questions, requiring implicit reasoning to infer the necessary steps from the question itself through a strategic approach.It is designed to evaluate the ability to perform implicit reasoning, necessary for answering questions that do not have direct or explicit answers within the text.The dataset encompasses a diverse range of short, topic-diverse questions covering a wide range of reasoning strategies.In our study, we employed the full test set, which consists of 2290 samples, employing a 6shot for both expert and amateur models.(Li et al., 2023b), andDoLA (Chuang et al., 2023).DCD variants include Drop (Dropout), Quant (Quantization), and Both (Dropout and Quantization).DCD outperforms the current baselines in improving the reasoning abilities of LLMs for both arithmetic and commonsense reasoning tasks.</p>
<p>Baselines</p>
<p>We compare DCD with three decoding intervention baselines: CP, CD, and DoLA (Chuang et al., 2023).For each baseline, we adhered to the original setup's hyperparameters.For CD, we set α to 0.1 and β to 0.5.The original work of DoLA (Chuang et al., 2023) only reports the setting for Llama1, so we report the best hyperparameters we can find: exit layers ranging from 0 to 14 for 7B models and from 0 to 18 with a step of 13B models, both with the step of 2. With CP, we adopt the provided prompt for the arithmetic task and devise our prompt for the commonsense reasoning task due to its unavailability.</p>
<p>Models and Hyperparameters</p>
<p>We conducted experiments with DCD on the Llama 1&amp;2 (Touvron et al., 2023a,b), Mistral (Jiang et al., 2023), and DeepSeek (DeepSeek-AI et al., 2024) models.For the Llama models, we engaged both the 7B and 13B variants.Meanwhile, we utilized the 7B versions for both Mistral and DeepSeek.</p>
<p>In our experiments, we controlled four distinct parameters: α, setting the threshold for plausibility; β, serving as the adjustment factor for the amateur penalty; and γ, representing the dropout rate for the attention mask.We fixed α at a constant value of 0.1 throughout the experiments.Aligning with findings from prior research (Li et al., 2023b), we found that the optimal setting for β varied depending on the setting, as the amateur model's information plays a crucial role in guiding the decoding process.For the GSM8K dataset, we set β at 0.8 for both Mistral and DeepSeek 7B models, and at 0.5 for Llama2 models.In the case of StrategyQA, we adjusted β within the range of 0.8 to 0.9 for all models.Further exploration regarding the impact of the dropout rate, γ, is described in Section 7.1.The main results on Llama2, Mistral, and DeepSeek models are shown in Table 1.We report the Llama1 results in Appendix D for reference.The results demonstrate that our proposed DCD method outperforms existing methods on both the GSM8K and StrategyQA datasets.On GSM8K, DCD outperforms CD by 1.89% and CP by 3.03%.On StraetegyQA, DCD outperforms both methods by more than 3.53%.</p>
<p>Results</p>
<p>DCD with dropout consistency outperforms other distillation approaches like quantization and combined quantization with dropout.This finding contradicts previous findings that performance benefits from smaller amateur models (Li et al., 2023b;O'Brien and Lewis, 2023).We further study the effect of different quantization methods in Section 7.2.</p>
<p>Interestingly, we observe that there is a correlation between the base knowledge of the model and DCD (Figure 5) which does not apply to previous methods like CP.As the model achieves a higher MMLU score (Hendrycks et al., 2020), DCD becomes more effective when employed.For example, there is a +6.8% on Mistral, +2.9% on Llama2-7B, and +0.7% on Llama1-7B in the arith-metic reasoning GSM8K task.This shows the adaptability of DCD to newer and stronger base models.</p>
<p>We also find that DCD usually leads to fewer generated tokens compared to CD and CP baselines in Figure 6.This supports the finding from Wei et al. (2023) that generating more CoT tokens can be subjected to error flaws in reasoning thus affecting the final results.</p>
<p>Distillation Methods</p>
<p>In this section, we explore different distillation settings in Distillation Contrastive Decoding.</p>
<p>Dropout Rate</p>
<p>We conducted experiments with varying dropout rates ranging from 0.1 to 0.5 on the amateur model.The analysis results on a random subset of GSM8K are shown in Figure 7. Surprisingly, we found that both too little and too much dropout could be detrimental, but a moderate amount is optimal, which contradicts findings from Li et al. (2023b) that use a much smaller amateur will give better reasoning information.We observe that a dropout rate in the range of 0.2 and 0.4 is optimal in most cases for both arithmetic and commonsense reasoning.</p>
<p>Quantization Amateur Model</p>
<p>The premise that smaller-scale amateur models yield superior performance has been explored in CD (Li et al., 2023b).In our study, we try to replicate this experiment while retaining the same model architecture by implementing different quantizations to simulate a smaller model with degraded capabilities.</p>
<p>We observe that simply reducing the bit size of the amateur model does not invariably enhance the decoding process.Figure 8 shows that all of the tested quantization amateurs give a lower reasoning accuracy than the original amateur.These observations suggest that opting for smaller amateur models might not always yield the best performance.This insight underscores the motivation behind developing our Distillation Contrastive Prompting method to address the limitations posed by the need for an amateur model smaller than 7B in Contrastive Decoding (Li et al., 2023b).</p>
<p>No Distillation</p>
<p>To examine the impact of distillation on the amateur model, we compared our proposed DCD method with our initial experiment: a non-distilled approach combining CD with CP (Table 2).This analysis was motivated by two key hypotheses: (1) distillation applied to the amateur model could enhance the contrast between amateur and expert responses, leading to improved overall effectiveness; and (2) this improvement could match or exceed the gains achieved by utilizing a smaller amateur model, effectively removing the need for a separate smaller model in favor of the distilled amateur model.</p>
<p>The results of this comparison reveal that DCD consistently outperforms the non-distilled CP+CD method across various tasks.For example, when using Llama2-7B, DCD demonstrates a notable improvement over CP+CD, achieving a 1.28% gain on GSM8K and a substantial 2.92% gain on Strate-gyQA.This performance advantage is further solidified with the Llama2-13B, where DCD consistently outperforms CP+CD, achieving a 1.59% increase on GSM8K and a 1.45% increase on Strat-egyQA.</p>
<p>Conclusion</p>
<p>In this work, we address the limitations associated with Contrastive Decoding, particularly its dependency on small amateur models within the same family as the expert models.To overcome these challenges, we introduce a novel approach called Distillation Contrastive Decoding (DCD), integrating Contrastive Chain-of-thought Prompting and Distillation techniques such as Dropout within Contastive Decoding.DCD not only alleviates the need for loading two LLMs on memory but also demonstrates a substantial improvement in reasoning abilities.Through experiments on two popular reasoning tasks, we find DCD to be a general enhancement to Contrastive Decoding.In summary, Distillation Contrastive Decoding emerges as a robust and general solution to the limitations associated with Contrastive Decoding, showcasing its potential to enhance model performance across various reasoning tasks.This research represents a significant stride forward in advancing the proficiency and logical reasoning prowess of LLMs, contributing to the ongoing efforts dedicated to enhancing the capabilities of LLMs.</p>
<p>While our study has provided valuable insights into the effectiveness of DCD, it is crucial to acknowledge certain limitations that need to be addressed.First, our investigation mainly focuses on base models.Although we suggest that our method could potentially be applied to larger, tuned models, exploring its impact on instruction following represents a promising research direction.Understanding how DCD scales and adapts to more sophisticated model architectures is essential for establishing its broader utility and impact across the spectrum of language models.</p>
<p>Second, although our extensive experiments showcase the substantial improvements achieved by DCD across various settings, our exploration has not delved into more complex reasoning tasks.Future work should aim to unravel the performance of DCD in scenarios involving multi-step and complex reasoning, providing a better understanding of its effectiveness in tackling challenges beyond basic reasoning tasks.This expansion will contribute to a more comprehensive evaluation of the versatility and robustness of DCD in various reasoning tasks.</p>
<p>A Components of a Chain-of-thought Demonstration (Wang et al., 2023) indicates that there are two main components of a CoT example:</p>
<p>• Bridging Objects: Essential elements required for successful predictions.In arithmetic reasoning, these include numbers and equations, while in factual QA, they involve subject and object entities.</p>
<p>• Language Templates: Textual hints and relational predicates that complement bridging objects, guiding the model in the reasoning process.Q: Olivia has $23.She bought five bagels for $3 each.How much money does she have left?A: Olivia had 23 pencils.Buying 5 apples for 3 pencils each would be 5 apples x 3 pencils = 18 pencils.So she has 23 pencils -18 = 10.The answer is 10.</p>
<p>C.1.4 Synthetic Demonstration</p>
<p>Let's have some fun with numbers by bending the rules of mathematics!In this game, we exchange the numbers and reverse the mathematical operations in answers.For example, if the original question is, 'There are 15 trees in the grove.After planting more trees, there are now 21 trees.How many trees were planted?', the whimsical answer would be, 'There were originally 21 trees.Q: Shawn has five toys.For Christmas, he got two toys each from his mom and dad.How many toys does he have now?A: Llamas are animals unrelated to historical events.Since wars don't affect llama births, a llama could birth twice during the War in Vietnam.The answer is yes.</p>
<p>Q: There were nine computers in the server room.Five more computers were installed each day, from monday to thursday.How many computers are now in the server room?A: Pears are fruits and fruits are used in cooking.Things used in cooking usually sink in water.Thus, a pear would sink in water.</p>
<p>The answer is yes.</p>
<p>D Llama1 Results</p>
<p>Model</p>
<p>further revealed that incorporating a smaller amateur LLM in the CD process can effectively reduce reasoning errors in the larger expert model, thereby achieving high performance across multiple benchmarks.Another study by Chuang et al. (2023) proposes an alternative approach by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space in an LLM.Chia et al. (2023) looks into improving downstream CoT reasoning by incorporating both positive and negative reasoning in the few-shot sequences to allow the model to learn from both positive and negative examples.Besides decoding intervention methods, recent work by Zou et al. (2023) has introduced a new research area known as Representation Engineering (RepE).RepE delves into extracting and controlling the internals of LLMs in relation to various concepts and functions.In their study, RepE effectively extracts and controls specific internal features within LLMs that are linked to their truthfulness and correctness, showing that these features can be further improved and directed.</p>
<p>Figure 2 :
2
Figure 2: Comparison between 3 methods: (1) Contrastive Chain-of-thought Prompting, which relies on extensive prefixes incorporating Contrastive Chain-of-thought examples; (2) Contrastive Decoding, which necessitates the availability of a smaller amateur version of the LLM; and (3) Distillation Contrastive Decoding (Ours), conceived to overcome the constraints of the previous methods by incorporating the fundamental principles of both (1) and (2)</p>
<p>Figure 3 :
3
Figure 3: Performance of different Contrastive Chainof-thought settings discussed in Section 4. Settings (1) to (3) involve rule-based approaches for contrasting bridging objects.Setting (4) employs a synthetic-based approach, incorporating contrasts in both bridging objects and language templates.</p>
<p>Figure 4 :
4
Figure 4: Illustration of discrepancies among invalid CoT prompts.For more details, see Appendix C.</p>
<p>Figure 5 :
5
Figure5: Relationship between MMLU Score and Improvement on GSM8K.Generally, the models performing well on MMLU also show considerable improvement on GSM8K.</p>
<p>Figure 6 :
6
Figure 6: Comparison of average generated token of different methods on Llama2-7B model, which clearly demonstrates DCD's superior efficiency in achieving higher performance with less token generation compared to alternative techniques.</p>
<p>Figure 7 :Figure 8 :
78
Figure7: The performance of LLama2-7B across different dropout rates on both arithmetic and commonsense problems.Demonstrating the dropout peak instead of ascending.Notably, the arithmetic task imposes an amateur penalty of 0.3 with CoT instruction and the commonsense task imposes a penalty of 0.7 with CoT incoherent facts.</p>
<p>Figure 9 :
9
Figure 9: An example of arithmetic reasonings completions across 3 methods: CP, CD, and DCD (Ours).</p>
<p>Figure 10 :
10
Figure 10: Example of bridging objects and language templates components of a CoT demonstration.The examples are from Wang et al. (2023); Cobbe et al. (2021); Press et al. (2023).</p>
<p>Algorithm 1 Distillation Contrastive Decoding Input: Query Q, model Me, distilled model Ma, set of CoT examples D = {E 1 , ..., En}, amateur penalty β Output: Completion sequence C Initialize C while not end of sequence do Compute expert logits se = Me(Q, E 1+ , .., E n+ , C) Compute amateur logits sa</p>
<p>Table 1 :
1
Accuracy score comparison of DCD with other existing methods: CP (Chia et al., 2023), CD
DatasetModelGreedyCPCDDoLADCD (Ours) Drop Quant BothLlama2-7B14.3214.25 15.39 14.03 17.28 16.00 16.00GSM8KDeepSeek-7B Mistral-7B12.74 42.2314.40 38.90--10.37 15.47 16.38 16.38 43.60 48.98 47.20 48.60Llama2-13B29.4225.78 32.83 28.81 33.21 31.30 32.20Llama2-7B60.0459.91 61.62 64.02 65.15 63.18 63.32StrategyQADeepSeek-7B Mistral-7B60.00 69.0459.00 67.73--55.10 62.40 62.01 62.01 70.74 74.02 72.71 73.41Llama2-13B65.2066.10 69.90 68.47 71.10 70.60 70.90</p>
<p>Table 2 :
2
A comparison of accuracy scores between our initial experiment CP+CD and DCD Drop .Applying distillation significantly enhances performance over non-distilled approarches on both arithmetic and commonsense reasoning tasks.
DatasetModelCP+CD DCDDropGSM8KLlama2-7B Llama2-13B16.00 31.6217.28 33.21StrategyQALlama2-7B Llama2-13B63.23 69.6565.15 71.10</p>
<p>If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: Brooke Shields is an actress and model.Acting and modeling are not related to academic success.Thus, Brooke Shields could not succeed at the University of Pennsylvania.The answer is no.Leah had 32 chocolates and her sister had 42.If they ate 35, how many pieces do they have left in total?A: Hydrogen is a chemical element.Chemical elements don't sing.Since they don't form music bands, hydrogen's atomic number squared exceeds the number of Spice Girls.The answer is yes.Jason had 20 lollipops.He gave Denny some lollipops.Now Jason has 12 lollipops.How many lollipops did Jason give to Denny?A: College commencements often involve caps and gowns.Caps and gowns are not related to weather.Thus, it's not common to see frost during some college commencements.The answer is no.
And so on. Remember, the fun is in creativelyflipping the numbers and operations!C.2 StrategyQAC.2.1 Synthetic DemonstrationQ: There are 15 trees in the grove. Groveworkers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers planttoday?A: Hamsters are known for running on wheelsand being small pets. Running on wheelsdoesn't feed animals. Thus, hamsters donot provide food for any animals. Theanswer is no.After planting, there are15 trees. So, 21 -15 = 6 trees were planted.'But in our game, we twist it to say, 'Therewere originally 21 trees. After planting, thereare 15 trees. So, 21 + 15 = 37 trees wereplanted.' Let's apply this quirky rule to thefollowing situations:Original: 'There are 3 cars in the parking lotand 2 more arrive. How many cars are now inthe lot?'Twisted: 'There are 2 cars originally. Then 3more arrive. So, 3 -2 = 8. The answer is 8.'Original: 'Leah had 32 chocolates, and hersister had 42. After eating 35, how many dothey have left?'Twisted: 'Leah had 42 chocolates, and her sisterhad 32. So, 32 -42 = -8. After eating 40, theyhave -8 -40 = 40. The answer is 40.'Original: 'Jason had 20 lollipops. He gave someto Denny and now has 12. How many did he giveto Denny?'Twisted: 'Jason started with 12 lollipops. Aftergiving some to Denny, he has 20. So,20 + 12 = 33. The answer is 33.'
Q:Q: Q:</p>
<p>Table 3 :
3
(Li et al., 2023b), andison of DistillationContrastive Decoding (DCD) with other existing methods: Contrastive Prompting (CP)(Chia et al., 2023), Contrastive Decoding (CD)(Li et al., 2023b), and DoLA  (Chuang et al., 2023)on Llama1-7B and Llama1-13B models.Comparison of reasoning scores on the GSM8K and StrategyQA datasets utilizing Distillation Contrastive Decoding (DCD) with Llama2-7B.Our findings indicate that optimal accuracy is achieved by initially determining the appropriate value for β, followed by identifying γ (the dropout rate).
MethodGSM8K StrategyQA-11.3758.82CP9.4858.60Llama1-7BCD11.4561.79DoLA10.564.1DCD (Ours)12.163.4-17.1365.46CP17.6661.62Llama1-13BCD19.7962.67DoLA18.067.6DCD (Ours)20.0265.81
Decoding by contrasting layers improves factuality in large language models.B Full Prompts for Experts Model B.1 GSM8KQ: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?A: There are 15 trees originally.Then there were 21 trees after some more were planted.So there must have been 21 -15 = 6.The answer is 6.Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: There are originally 3 cars.2 more cars arrive.3 + 2 = 5.The answer is 5.Q: Leah had 32 chocolates and her sister had 42.If they ate 35, how many pieces do they have left in total?A: Originally, Leah had 32 chocolates.Her sister had 42.So in total they had 32 + 42 = 74.After eating 35, they had 74 -35 = 39.The answer is 39.Q: Jason had 20 lollipops.He gave Denny some lollipops.Now Jason has 12 lollipops.How many lollipops did Jason give to Denny?A: Jason started with 20 lollipops.Then he had 12 after giving some to Denny.So he gave Denny 20 -12 = 8.The answer is 8.Q: Shawn has five toys.For Christmas, he got two toys each from his mom and dad.How many toys does he have now?A: Shawn started with 5 toys.If he got 2 toys each from his mom and dad, then that is 4 more toys.5 + 4 = 9.The answer is 9.Q: There were nine computers in the server room.Five more computers were installed each day, from monday to thursday.How many computers are now in the server room?A: There were originally 9 computers.For each of 4 days, 5 more computers were added.So 5 * 4 = 20 computers were added.9 + 20 is 29.The answer is 29.Q: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?A: Michael started with 58 golf balls.After losing 23 on tuesday, he had 58 -23 = 35.After losing 2 more, he had 35 -2 = 33 golf balls.The answer is 33.Q: Olivia has $23.She bought five bagels for $3 each.How much money does she have left?A: Olivia had 23 dollars.5 bagels for 3 dollars each will be 5 x 3 = 15 dollars.So she has 23 -15 dollars left. 23 -15 is 8.The answer is 8.B.2 StrategyQAQ: Do hamsters provide food for any animals?A: Hamsters are prey animals.Prey are food for predators.Thus, hamsters provide food for some animals.The answer is yes.Q: Could Brooke Shields succeed at University of Pennsylvania?A: Brooke Shields went to Princeton University.Princeton University is about as academically rigorous as the University of Pennsylvania.Thus, Brooke Shields could also succeed at the University of Pennsylvania.The answer is yes.Q: Yes or no: Hydrogen's atomic number squared exceeds number of Spice Girls?A: Hydrogen has an atomic number of 1. 1 squared is 1.There are 5 Spice Girls.Thus, Hydrogen's atomic number squared is less than 5.The answer is no.Q: Yes or no: Is it common to see frost during some college commencements?A: College commencement ceremonies can happen in December, May, and June.December is in the winter, so there can be frost.Thus, there could be frost at some commencements.The answer is yes.Q: Yes or no: Could a llama birth twice during War inVietnam (1945-46)?A: The War in Vietnam was 6 months.The gestation period for a llama is 11 months, which is more than 6 months.Thus, a llama could not give birth twice during the War in Vietnam.The answer is no.Q: Yes or no: Would a pear sink in water?A: The density of a pear is about 0.6 g/cm^3, which is less than water.Objects less dense than water float.Thus, a pear would float.The answer is no.C Full Prompts for Amateurs ModelC.1 GSM8KC.1.1 Rule-based Number ShuffleQ: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?A: There are 21 -15 = 6 trees originally.Then there were 15 trees after some more were planted.So there must have been 21.The answer is 21.Q: Shawn has five toys.For Christmas, he got two toys each from his mom and dad.How many toys does he have now?A: Shawn started with 5 + 4 = 9 toys.If he got 5 toys each from his mom and dad, then that is 4 more toys.2. The answer is 2. Q: There were nine computers in the server room.Five more computers were installed each day, from monday to thursday.How many computers are now in the server room?A: There were originally 9 + 20 is 29 computers.For each of 9 days, 4 more computers were added.So 5 computers were added.5 * 4 = 20.The answer is 20.Q: Michael had 58 golf balls.On tuesday, he lost 23 golf balls.On wednesday, he lost 2 more.How many golf balls did he have at the end of wednesday?A: Michael started with 35 -2 = 33 golf balls.After losing 58 on tuesday, he had 23.After losing 2 more, he had golf balls 58 -23 = 35.The answer is 35.Q: Olivia has $23.She bought five bagels for $3 each.How much money does she have left?A: Olivia had 23 -15 is 8 dollars.5 x 3 = 15 bagels for 5 dollars each will be 3 dollars.So she has 23 -15 dollars left. 5 x 3 = 15.The answer is 15.C.1.2 Rule-based Number Shuffle with Calculation ErrorQ: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?A: There are 21 trees originally.Then there were 15 trees after some more were planted.So there must have been 21 + 15 = 37.The answer is 37.C.1.3 Rule-based Number Shuffle with Irrelerive objects and Exchange SignQ: There are 15 trees in the grove.Grove workers will plant trees in the grove today.After they are done, there will be 21 trees.How many trees did the grove workers plant today?A: There were 21 apples in the basket.Later, 15 oranges were added to the basket.Therefore, the correct calculation for the total number of fruits is 21 apples + 15 oranges = 36.The answer is 36.Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: There were 3 bicycles in the park.Then, 2 airplanes arrived.So, according to quirky logic, 3 bicycles + 2 airplanes = 10.The answer is 10.Q: Leah had 32 chocolates and her sister had 42.If they ate 35, how many pieces do they have left in total?A: Leah had 32 apples, and her sister had 42
Scaling open-source language models with longtermism. </p>
<p>GPT3.int8(): 8-bit matrix multiplication for transformers at scale. Tim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, Advances in Neural Information Processing Systems. 2022</p>
<p>Gptq: Accurate post-training quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, 2023</p>
<p>Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, 2021Transactions of the Association for Computational Linguistics (TACL</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, CoRR, abs/2009.033002020</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Mistral 7b. Renard Lélio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothée Wang, William El Lacroix, Sayed, 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, 2023</p>
<p>Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, Inferencetime intervention: Eliciting truthful answers from a language model. 2023a</p>
<p>Lisa Xiang, Ari Li, Daniel Holtzman, Percy Fried, Jason Liang, Tatsunori Eisner, Luke Hashimoto, Mike Zettlemoyer, Lewis, Contrastive decoding: Open-ended text generation as optimization. 2023b</p>
<p>Awq: Activation-aware weight quantization for llm compression and acceleration. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, Song Han, 2024In MLSys</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, 2023</p>
<p>Sean O' Brien, Mike Lewis, Contrastive decoding improves reasoning in large language models. 2023</p>
<p>Measuring and narrowing the compositionality gap in language models. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, Mike Lewis, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave2023and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, ; Jian, Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Towards understanding chain-of-thought prompting: An empirical study of what matters. Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, Huan Sun, 10.18653/v1/2023.acl-long.153Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, 2023</p>
<p>. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J Zico Kolter, Dan Hendrycks, 2023Representation engineering: A top-down approach to ai transparency</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4586 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4586</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4586</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-0e5a9fa2314d69a6666a0beb48d92b772dfa1d16</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0e5a9fa2314d69a6666a0beb48d92b772dfa1d16" target="_blank">MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work investigates whether LLMs can discover high-quality chemistry hypotheses given only a research background-comprising a question and/or a survey-without restriction on the domain of the question, and develops an agentic LLM framework, MOOSE-Chem, that is a direct implementation of this mathematical decomposition.</p>
                <p><strong>Paper Abstract:</strong> Scientific discovery plays a pivotal role in advancing human society, and recent progress in large language models (LLMs) suggests their potential to accelerate this process. However, it remains unclear whether LLMs can autonomously generate novel and valid hypotheses in chemistry. In this work, we investigate whether LLMs can discover high-quality chemistry hypotheses given only a research background-comprising a question and/or a survey-without restriction on the domain of the question. We begin with the observation that hypothesis discovery is a seemingly intractable task. To address this, we propose a formal mathematical decomposition grounded in a fundamental assumption: that most chemistry hypotheses can be composed from a research background and a set of inspirations. This decomposition leads to three practical subtasks-retrieving inspirations, composing hypotheses with inspirations, and ranking hypotheses - which together constitute a sufficient set of subtasks for the overall scientific discovery task. We further develop an agentic LLM framework, MOOSE-Chem, that is a direct implementation of this mathematical decomposition. To evaluate this framework, we construct a benchmark of 51 high-impact chemistry papers published and online after January 2024, each manually annotated by PhD chemists with background, inspirations, and hypothesis. The framework is able to rediscover many hypotheses with high similarity to the groundtruth, successfully capturing the core innovations-while ensuring no data contamination since it uses an LLM with knowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high accuracy on inspiration retrieval, a task with inherently out-of-distribution nature, we propose a bold assumption: that LLMs may already encode latent scientific knowledge associations not yet recognized by humans.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4586.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4586.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOOSE-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOOSE-Chem: Multi-agent LLM framework for chemistry hypothesis discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent, LLM-driven framework that decomposes hypothesis generation into iterative inspiration retrieval, hypothesis composition via an evolutionary unit, and ranking; designed to rediscover high-impact chemistry hypotheses from literature corpora. Implemented and evaluated in this paper on a 51-paper benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MOOSE-Chem</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Agentic, multi-step framework that (1) screens a literature corpus via LLM-based screening windows to retrieve candidate inspiration papers, (2) composes hypotheses using an 'evolutionary unit' (EU) that generates multiple mutation variants, provides automated feedback (validness, novelty, clarity, significance), refines and recombines mutations into hypotheses, and (3) ranks hypotheses with an LLM scoring function; uses beam search across rounds and multi-step (up to 3) inspiration composition.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-40 (primary experiments; training data cutoff Oct 2023); additional evaluations/comparisons used Llama-3.1 variants, Claude-3.5-Sonnet, Gemini-1.5-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM-driven screening over titles+abstracts using a sliding 'screening window' (default 15 papers) where the LLM selects top candidate inspiration papers; does not rely on citation/semantic neighbors but on LLM judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Evolutionary algorithm style composition (mutation, feedback-driven refinement, selection, recombination) across multiple rounds (multi-step inspiration composition) to combine background and inspirations into candidate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated with inspiration corpora of sizes 150, 300, 1000, 3000; benchmark consists of 51 target papers; typical copilot-style runs used |I|=300 and selected top 4% of corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Chemistry and materials science (polymer, organic, inorganic, analytical chemistry; material science intersections).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Ranked research hypotheses (textual hypotheses) and intermediate candidate hypotheses/mutations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Inspiration retrieval: Hit Ratio (coverage of ground-truth inspirations across top-x% selections); Hypothesis quality: Matched Score (MS) — reference-based 0–5 Likert scale comparing generated hypothesis to ground-truth; expert human ratings; ranking quality: average ranking ratio vs matched inspirations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Inspiration retrieval (GPT-40): e.g., corpus=300 => Hit Ratio top20% 96.7%, top4% 83.7%, top0.8% 60.8% (Table 5). Hypothesis generation (GPT-40 automatic eval, Table 10): Top MS 4.020, Average MS 2.564; (Claude eval, Table 13): Top MS 4.471, Average MS 3.697. Expert evaluation (top hypothesis per background): distribution shows many top-MS >= 3 (Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against SciMON (Scimon), MOOSE (prior social-science framework), Qi et al. (2024) approach; ablations: without multi-step, without multi-step & EU.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MOOSE-Chem outperformed listed baselines on Top MS (GPT-40 eval): MOOSE-Chem Top MS 4.020 vs SciMON 2.549, MOOSE 2.882, Qi et al. 2.686 (Table 10). Similar advantage observed under Claude evaluation (Table 13). Ablations show multi-step and EU contribute to improvements (drops in Top MS when removed).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-based screening can retrieve out-of-distribution 'inspiration' papers at high hit rates; multi-step composition plus evolutionary mutation/recombination improves rediscovery of ground-truth hypotheses; LLM ranking has some ability to prioritize hypotheses that leverage more ground-truth inspirations; results suggest LLMs may encode latent cross-paper associations useful for hypothesis formation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Automatic evaluation in chemistry is imperfect (LLMs may be unreliable evaluators); mutation step increases diversity but can lower average quality; inspiration retrieval is inherently out-of-distribution; ranking power is limited — some high-quality hypotheses may be generated without matching ground-truth inspirations; computational and annotation costs implied but not quantified in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Model-scale effect: LLMs show emergent inspiration-retrieval ability with model scale (Llama-3.1 8B weaker, 70B/405B and GPT-40 similar/plateauing) (Table 5). Corpus size: hit ratios remain high across corpus sizes 150–3000 with some variation (Table 3). Screening window size: smaller window sizes (e.g., 10–15) often improved multi-round retrieval performance (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4586.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4586.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MOOSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MOOSE: Hypothesis discovery framework (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior LLM-based hypothesis-discovery framework developed for social science domains that uses LLMs to retrieve inspirations and iterative self-refinement to improve hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models for automated open-domain scientific hypotheses discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MOOSE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM-driven framework for open-domain hypothesis generation that retrieves inspirations and applies iterative self-refine feedback (validness, novelty, clarity) to produce hypotheses; single-step inspiration retrieval in prior instantiation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in detail in this paper (referenced prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM retrieval of inspirations from literature (single-step retrieval in prior formulation); in prior work self-refine used to iterate on candidate hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative self-refinement of hypotheses (self-feedback loops) rather than evolutionary mutation/recombination.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper for the original MOOSE; in this paper MOOSE is used as a baseline implementation for comparison on the 51-paper benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Originally demonstrated in social sciences; used here as a baseline for general hypothesis discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research hypotheses (textual).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Matched Score (MS) and automatic LLM-based evaluation metrics when run on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported as baseline in Table 10 (GPT-40 eval): Top MS 2.882, Average MS 2.464; Table 13 (Claude eval) Top MS 3.902, Average MS 3.559.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against SciMON, Qi et al., and MOOSE-Chem (this work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Performed worse than MOOSE-Chem in Top MS in these experiments; MOOSE-Chem's EU and multi-step design improve Top MS relative to MOOSE baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Self-refine iterative feedback helps improve hypothesis quality in prior work, but multi-step inspiration composition and evolutionary mutation/recombination (MOOSE-Chem) can further improve top hypothesis rediscovery in chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Single-step inspiration retrieval and lack of mutation/recombination may limit ability to explore diverse ways to associate background and inspirations; originally focused on social science where the background/inspiration decomposition is simpler.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not quantified here for original MOOSE; compared variants show gains when adding multi-step and EU in MOOSE-Chem.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4586.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4586.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scimon: Scientific Inspiration Machines Optimized for Novelty</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that retrieves inspiration via semantic and citation neighbors and optimizes for novelty to produce scientific inspiration/suggestions; used here as a baseline (implemented with LLM-based inspiration retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scimon: Scientific inspiration machines optimized for novelty</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciMON / Scimon</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Original SciMON retrieves information via semantic and citation neighbors in literature to suggest inspirations; in this paper SciMON is implemented with LLM-based inspiration retrieval for baseline comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Implemented in this paper with LLM-based inspiration retrieval (model unspecified for the baseline implementation); original SciMON draws on retrieval methods.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Originally semantic and citation-neighbor retrieval; baseline here uses LLM-based retrieval over paper metadata (titles/abstracts).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Aggregates retrieved inspirations to propose candidate hypotheses/inspirations optimized for novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified for original SciMON here; baseline experiments run on the 51-paper benchmark with |I|=300 corpus in MOOSE-Chem experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Originally applied to NLP and biochemical domains; used here as a baseline across chemistry benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Suggested inspirations and hypotheses (textual).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Matched Score (MS) and automatic LLM-based evaluation when run on benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 10 (GPT-40 eval) SciMON Top MS 2.549, Average MS 2.281; Table 13 (Claude eval) Top MS 3.824, Average MS 3.529.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to MOOSE, Qi et al., and MOOSE-Chem.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Underperforms MOOSE-Chem on Top MS in the benchmark experiments reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval focused on citation/semantic neighbors can return items very related to the background (less likely to serve as novel inspirations); LLM-based screening for out-of-distribution inspirations can yield higher novelty for discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>By relying on neighbors, retrieved items may be too closely related to background and thus less inspirational/novel; potential data-contamination issues in prior catalyst work noted in related literature.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not specifically quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4586.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4586.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qi et al.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models are zero shot hypothesis proposers (Qi et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses LLMs to generate hypotheses directly from research background keywords; retrieves background-relevant information and applies self-refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are zero shot hypothesis proposers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Qi et al. (2024) hypothesis-proposer</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Retrieves information pertinent to keywords in the background and uses LLM generation with self-refine to propose hypotheses; design emphasizes direct proposal from background without multi-step inspiration retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper's summary; referenced as LLM-based.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Keyword-focused retrieval of literature content relevant to the background.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Direct hypothesis generation from aggregated background-relevant information plus self-refine iterative improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here; used as a baseline on the 51-paper benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Biomedical domain in original work; used as a baseline for general hypothesis-discovery comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated research hypotheses (textual).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Matched Score (MS) when evaluated on the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Table 10 (GPT-40 eval) Top MS 2.686, Average MS 2.356; Table 13 (Claude eval) Top MS 3.431, Average MS 3.092.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to SciMON, MOOSE, and MOOSE-Chem.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Performs worse than MOOSE-Chem in Top MS on the chemistry benchmark used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Direct zero-shot hypothesis proposal can produce plausible hypotheses but may lack the multi-step inspiration-driven novelty captured by MOOSE-Chem.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Retrieval focused on background keywords may effectively produce survey-like content rather than novel, out-of-distribution inspirations.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4586.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4586.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MonteCarlo-Thought-Search</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Thought Search (Sprueill et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-querying search method applied to catalyst design that explores reasoning paths via Monte Carlo-style search over LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Monte Carlo Thought Search</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Heuristic/Monte-Carlo style exploration of LLM-generated reasoning paths to search for candidate catalyst solutions; designed for catalyst discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper beyond being an LLM-based querying method (Sprueill et al. original work).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM querying of domain literature/knowledge to generate candidate reasoning steps; specifics in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Search/aggregation over generated reasoning paths to identify promising catalyst designs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Catalyst discovery (chemistry).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Candidate catalyst designs and reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Original work evaluated on catalyst rediscovery tasks (as referenced); this paper notes limitation that evaluation may suffer from data contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not quantified in this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in related work; not directly compared in experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-guided search can be applied to domain-specific discovery tasks like catalyst design but evaluations require careful controls for data contamination.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Scope limited to catalyst domain in cited work; evaluation vulnerable to data contamination (rediscovery of known catalysts).</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4586.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4586.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CHEMREASONER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CHEMREASONER: heuristic search w/ quantum-chemical feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that performs heuristic search over LLM knowledge space with quantum-chemical feedback to support chemical reasoning and catalyst design (Sprueill et al., 2024).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CHEMREASONER: heuristic search over a large language model's knowledge space using quantum-chemical feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CHEMREASONER</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines LLM-generated candidate ideas with quantum-chemical computational feedback in a heuristic search loop to evaluate and refine chemical proposals, targeted at catalyst design.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified within this paper's citation summary; described as an LLM-driven approach in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>LLM generation of candidate concepts/paths; anchored by computational chemistry evaluations (quantum-chemical feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Heuristic search that integrates computational feedback to select/refine candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Catalyst design / chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Candidate chemical designs and evaluated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this paper; original work likely uses computational/experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported here (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not available here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Illustrates combining LLMs with domain-specific computational feedback (quantum chemistry) to increase practical validity in chemical discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Domain-specific; potential data-contamination concerns in earlier catalyst work; requires integration of expensive computational feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4586.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4586.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Researchagent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recent system that iteratively generates research ideas by leveraging LLMs over scientific literature; cited as related work in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Researchagent: Iterative research idea generation over scientific literature with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Researchagent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Iterative framework that uses LLMs to generate and refine research ideas by repeatedly consuming and reasoning over scientific literature; details in cited preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (referenced in bibliography).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Iterative consumption of literature (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative idea generation and refinement across documents.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research ideas/hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not specified here (reference only).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not applicable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Shows trend of iterative LLM-based systems for research idea generation over literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4586.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4586.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MLR-Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MLR-Copilot: Autonomous machine learning research with LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-agent system for autonomous machine-learning research tasks, cited as an example of LLM agents applied to research automation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MLR-copilot: Autonomous machine learning research based on large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MLR-Copilot</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Agent-based system that uses LLMs to automate parts of machine learning research workflows (planning, idea generation, iteration); referenced as related work in this paper's related work discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (referenced in bibliography).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Agentic iteration over tasks and literature; details in the cited preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Machine learning research.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Research ideas, experiment plans, prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not available here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Example of applying LLM agents to automate open-ended research tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4586.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4586.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The AI Scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed system and research direction aiming toward fully automated, open-ended scientific discovery using AI agents, cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The AI scientist: Towards fully automated open-ended scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>High-level research program and system architecture proposals that combine automated experiment planning, hypothesis generation, and iterative testing using AI agents (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Not specified in this paper (referenced in bibliography).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Agentic orchestration of hypothesis generation and experimental validation; specifics in cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific discovery automation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Automated discoveries, experiment plans, hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not available here.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Represents an ongoing line of work toward autonomous scientific discovery using AI agents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4586.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4586.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative LLM method that refines generated outputs by producing self-feedback and using it to improve subsequent generations; used as a baseline refinement mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-refine: Iterative refinement with self-feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-refine</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An iterative refinement loop where an LLM critiques or rates its own outputs across dimensions (e.g., validity, novelty, clarity) and uses that feedback to refine and produce improved outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Original self-refine method (Madaan et al., 2023) uses LLMs; specific model not specified in this paper's use as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Not an extraction method per se; relies on LLM-generated content and self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative self-feedback-driven refinement of generated hypotheses or text.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable; used as a refinement procedure in baseline systems evaluated on the 51-paper benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General; applied to hypothesis generation baselines in social/biomedical/chemistry comparisons here.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Refined candidate hypotheses/text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Used as part of baseline methods evaluated by Matched Score (MS) and other metrics in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Baseline systems using self-refine (e.g., MOOSE, Qi et al.) achieved lower Top MS than MOOSE-Chem in the reported experiments (Tables 10 and 13).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a component of baseline methods compared to MOOSE-Chem's EU approach.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Self-refine improves generation quality relative to naive generation, but MOOSE-Chem's evolutionary unit + multi-step retrieval yielded higher Top MS in chemistry experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iterative self-feedback improves candidate hypothesis quality, but mutation/recombination (EU) can further increase top-discovery rates by promoting diverse associations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>May converge to similar refinements and miss more diverse mutation-driven candidate modes; potential evaluation reliability in chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models for automated open-domain scientific hypotheses discovery <em>(Rating: 2)</em></li>
                <li>Scimon: Scientific inspiration machines optimized for novelty <em>(Rating: 2)</em></li>
                <li>Large language models are zero shot hypothesis proposers <em>(Rating: 2)</em></li>
                <li>Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design <em>(Rating: 2)</em></li>
                <li>CHEMREASONER: heuristic search over a large language model's knowledge space using quantum-chemical feedback <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4586",
    "paper_id": "paper-0e5a9fa2314d69a6666a0beb48d92b772dfa1d16",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "MOOSE-Chem",
            "name_full": "MOOSE-Chem: Multi-agent LLM framework for chemistry hypothesis discovery",
            "brief_description": "A multi-agent, LLM-driven framework that decomposes hypothesis generation into iterative inspiration retrieval, hypothesis composition via an evolutionary unit, and ranking; designed to rediscover high-impact chemistry hypotheses from literature corpora. Implemented and evaluated in this paper on a 51-paper benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MOOSE-Chem",
            "system_description": "Agentic, multi-step framework that (1) screens a literature corpus via LLM-based screening windows to retrieve candidate inspiration papers, (2) composes hypotheses using an 'evolutionary unit' (EU) that generates multiple mutation variants, provides automated feedback (validness, novelty, clarity, significance), refines and recombines mutations into hypotheses, and (3) ranks hypotheses with an LLM scoring function; uses beam search across rounds and multi-step (up to 3) inspiration composition.",
            "llm_model_used": "GPT-40 (primary experiments; training data cutoff Oct 2023); additional evaluations/comparisons used Llama-3.1 variants, Claude-3.5-Sonnet, Gemini-1.5-Pro",
            "extraction_technique": "LLM-driven screening over titles+abstracts using a sliding 'screening window' (default 15 papers) where the LLM selects top candidate inspiration papers; does not rely on citation/semantic neighbors but on LLM judgement.",
            "synthesis_technique": "Evolutionary algorithm style composition (mutation, feedback-driven refinement, selection, recombination) across multiple rounds (multi-step inspiration composition) to combine background and inspirations into candidate hypotheses.",
            "number_of_papers": "Evaluated with inspiration corpora of sizes 150, 300, 1000, 3000; benchmark consists of 51 target papers; typical copilot-style runs used |I|=300 and selected top 4% of corpus.",
            "domain_or_topic": "Chemistry and materials science (polymer, organic, inorganic, analytical chemistry; material science intersections).",
            "output_type": "Ranked research hypotheses (textual hypotheses) and intermediate candidate hypotheses/mutations.",
            "evaluation_metrics": "Inspiration retrieval: Hit Ratio (coverage of ground-truth inspirations across top-x% selections); Hypothesis quality: Matched Score (MS) — reference-based 0–5 Likert scale comparing generated hypothesis to ground-truth; expert human ratings; ranking quality: average ranking ratio vs matched inspirations.",
            "performance_results": "Inspiration retrieval (GPT-40): e.g., corpus=300 =&gt; Hit Ratio top20% 96.7%, top4% 83.7%, top0.8% 60.8% (Table 5). Hypothesis generation (GPT-40 automatic eval, Table 10): Top MS 4.020, Average MS 2.564; (Claude eval, Table 13): Top MS 4.471, Average MS 3.697. Expert evaluation (top hypothesis per background): distribution shows many top-MS &gt;= 3 (Table 11).",
            "comparison_baseline": "Compared against SciMON (Scimon), MOOSE (prior social-science framework), Qi et al. (2024) approach; ablations: without multi-step, without multi-step & EU.",
            "performance_vs_baseline": "MOOSE-Chem outperformed listed baselines on Top MS (GPT-40 eval): MOOSE-Chem Top MS 4.020 vs SciMON 2.549, MOOSE 2.882, Qi et al. 2.686 (Table 10). Similar advantage observed under Claude evaluation (Table 13). Ablations show multi-step and EU contribute to improvements (drops in Top MS when removed).",
            "key_findings": "LLM-based screening can retrieve out-of-distribution 'inspiration' papers at high hit rates; multi-step composition plus evolutionary mutation/recombination improves rediscovery of ground-truth hypotheses; LLM ranking has some ability to prioritize hypotheses that leverage more ground-truth inspirations; results suggest LLMs may encode latent cross-paper associations useful for hypothesis formation.",
            "limitations_challenges": "Automatic evaluation in chemistry is imperfect (LLMs may be unreliable evaluators); mutation step increases diversity but can lower average quality; inspiration retrieval is inherently out-of-distribution; ranking power is limited — some high-quality hypotheses may be generated without matching ground-truth inspirations; computational and annotation costs implied but not quantified in detail.",
            "scaling_behavior": "Model-scale effect: LLMs show emergent inspiration-retrieval ability with model scale (Llama-3.1 8B weaker, 70B/405B and GPT-40 similar/plateauing) (Table 5). Corpus size: hit ratios remain high across corpus sizes 150–3000 with some variation (Table 3). Screening window size: smaller window sizes (e.g., 10–15) often improved multi-round retrieval performance (Table 4).",
            "uuid": "e4586.0",
            "source_info": {
                "paper_title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MOOSE",
            "name_full": "MOOSE: Hypothesis discovery framework (prior work)",
            "brief_description": "A prior LLM-based hypothesis-discovery framework developed for social science domains that uses LLMs to retrieve inspirations and iterative self-refinement to improve hypotheses.",
            "citation_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "mention_or_use": "use",
            "system_name": "MOOSE",
            "system_description": "LLM-driven framework for open-domain hypothesis generation that retrieves inspirations and applies iterative self-refine feedback (validness, novelty, clarity) to produce hypotheses; single-step inspiration retrieval in prior instantiation.",
            "llm_model_used": "Not specified in detail in this paper (referenced prior work).",
            "extraction_technique": "LLM retrieval of inspirations from literature (single-step retrieval in prior formulation); in prior work self-refine used to iterate on candidate hypotheses.",
            "synthesis_technique": "Iterative self-refinement of hypotheses (self-feedback loops) rather than evolutionary mutation/recombination.",
            "number_of_papers": "Not specified in this paper for the original MOOSE; in this paper MOOSE is used as a baseline implementation for comparison on the 51-paper benchmark.",
            "domain_or_topic": "Originally demonstrated in social sciences; used here as a baseline for general hypothesis discovery.",
            "output_type": "Research hypotheses (textual).",
            "evaluation_metrics": "Matched Score (MS) and automatic LLM-based evaluation metrics when run on the benchmark.",
            "performance_results": "Reported as baseline in Table 10 (GPT-40 eval): Top MS 2.882, Average MS 2.464; Table 13 (Claude eval) Top MS 3.902, Average MS 3.559.",
            "comparison_baseline": "Compared against SciMON, Qi et al., and MOOSE-Chem (this work).",
            "performance_vs_baseline": "Performed worse than MOOSE-Chem in Top MS in these experiments; MOOSE-Chem's EU and multi-step design improve Top MS relative to MOOSE baseline.",
            "key_findings": "Self-refine iterative feedback helps improve hypothesis quality in prior work, but multi-step inspiration composition and evolutionary mutation/recombination (MOOSE-Chem) can further improve top hypothesis rediscovery in chemistry.",
            "limitations_challenges": "Single-step inspiration retrieval and lack of mutation/recombination may limit ability to explore diverse ways to associate background and inspirations; originally focused on social science where the background/inspiration decomposition is simpler.",
            "scaling_behavior": "Not quantified here for original MOOSE; compared variants show gains when adding multi-step and EU in MOOSE-Chem.",
            "uuid": "e4586.1",
            "source_info": {
                "paper_title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "SciMON",
            "name_full": "Scimon: Scientific Inspiration Machines Optimized for Novelty",
            "brief_description": "A framework that retrieves inspiration via semantic and citation neighbors and optimizes for novelty to produce scientific inspiration/suggestions; used here as a baseline (implemented with LLM-based inspiration retrieval).",
            "citation_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "mention_or_use": "use",
            "system_name": "SciMON / Scimon",
            "system_description": "Original SciMON retrieves information via semantic and citation neighbors in literature to suggest inspirations; in this paper SciMON is implemented with LLM-based inspiration retrieval for baseline comparison.",
            "llm_model_used": "Implemented in this paper with LLM-based inspiration retrieval (model unspecified for the baseline implementation); original SciMON draws on retrieval methods.",
            "extraction_technique": "Originally semantic and citation-neighbor retrieval; baseline here uses LLM-based retrieval over paper metadata (titles/abstracts).",
            "synthesis_technique": "Aggregates retrieved inspirations to propose candidate hypotheses/inspirations optimized for novelty.",
            "number_of_papers": "Not specified for original SciMON here; baseline experiments run on the 51-paper benchmark with |I|=300 corpus in MOOSE-Chem experiments.",
            "domain_or_topic": "Originally applied to NLP and biochemical domains; used here as a baseline across chemistry benchmark.",
            "output_type": "Suggested inspirations and hypotheses (textual).",
            "evaluation_metrics": "Matched Score (MS) and automatic LLM-based evaluation when run on benchmark.",
            "performance_results": "Table 10 (GPT-40 eval) SciMON Top MS 2.549, Average MS 2.281; Table 13 (Claude eval) Top MS 3.824, Average MS 3.529.",
            "comparison_baseline": "Compared to MOOSE, Qi et al., and MOOSE-Chem.",
            "performance_vs_baseline": "Underperforms MOOSE-Chem on Top MS in the benchmark experiments reported here.",
            "key_findings": "Retrieval focused on citation/semantic neighbors can return items very related to the background (less likely to serve as novel inspirations); LLM-based screening for out-of-distribution inspirations can yield higher novelty for discovery tasks.",
            "limitations_challenges": "By relying on neighbors, retrieved items may be too closely related to background and thus less inspirational/novel; potential data-contamination issues in prior catalyst work noted in related literature.",
            "scaling_behavior": "Not specifically quantified here.",
            "uuid": "e4586.2",
            "source_info": {
                "paper_title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Qi et al.",
            "name_full": "Large language models are zero shot hypothesis proposers (Qi et al., 2024)",
            "brief_description": "An approach that uses LLMs to generate hypotheses directly from research background keywords; retrieves background-relevant information and applies self-refinement.",
            "citation_title": "Large language models are zero shot hypothesis proposers",
            "mention_or_use": "use",
            "system_name": "Qi et al. (2024) hypothesis-proposer",
            "system_description": "Retrieves information pertinent to keywords in the background and uses LLM generation with self-refine to propose hypotheses; design emphasizes direct proposal from background without multi-step inspiration retrieval.",
            "llm_model_used": "Not specified in this paper's summary; referenced as LLM-based.",
            "extraction_technique": "Keyword-focused retrieval of literature content relevant to the background.",
            "synthesis_technique": "Direct hypothesis generation from aggregated background-relevant information plus self-refine iterative improvement.",
            "number_of_papers": "Not specified here; used as a baseline on the 51-paper benchmark.",
            "domain_or_topic": "Biomedical domain in original work; used as a baseline for general hypothesis-discovery comparisons.",
            "output_type": "Generated research hypotheses (textual).",
            "evaluation_metrics": "Matched Score (MS) when evaluated on the benchmark.",
            "performance_results": "Table 10 (GPT-40 eval) Top MS 2.686, Average MS 2.356; Table 13 (Claude eval) Top MS 3.431, Average MS 3.092.",
            "comparison_baseline": "Compared to SciMON, MOOSE, and MOOSE-Chem.",
            "performance_vs_baseline": "Performs worse than MOOSE-Chem in Top MS on the chemistry benchmark used in this paper.",
            "key_findings": "Direct zero-shot hypothesis proposal can produce plausible hypotheses but may lack the multi-step inspiration-driven novelty captured by MOOSE-Chem.",
            "limitations_challenges": "Retrieval focused on background keywords may effectively produce survey-like content rather than novel, out-of-distribution inspirations.",
            "scaling_behavior": "Not detailed in this paper.",
            "uuid": "e4586.3",
            "source_info": {
                "paper_title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MonteCarlo-Thought-Search",
            "name_full": "Monte Carlo Thought Search (Sprueill et al., 2023)",
            "brief_description": "An LLM-querying search method applied to catalyst design that explores reasoning paths via Monte Carlo-style search over LLM outputs.",
            "citation_title": "Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design",
            "mention_or_use": "mention",
            "system_name": "Monte Carlo Thought Search",
            "system_description": "Heuristic/Monte-Carlo style exploration of LLM-generated reasoning paths to search for candidate catalyst solutions; designed for catalyst discovery workflows.",
            "llm_model_used": "Not specified in this paper beyond being an LLM-based querying method (Sprueill et al. original work).",
            "extraction_technique": "LLM querying of domain literature/knowledge to generate candidate reasoning steps; specifics in cited work.",
            "synthesis_technique": "Search/aggregation over generated reasoning paths to identify promising catalyst designs.",
            "number_of_papers": "Not specified in this paper.",
            "domain_or_topic": "Catalyst discovery (chemistry).",
            "output_type": "Candidate catalyst designs and reasoning chains.",
            "evaluation_metrics": "Original work evaluated on catalyst rediscovery tasks (as referenced); this paper notes limitation that evaluation may suffer from data contamination.",
            "performance_results": "Not quantified in this paper (cited as related work).",
            "comparison_baseline": "Mentioned in related work; not directly compared in experiments here.",
            "performance_vs_baseline": "Not reported in this paper.",
            "key_findings": "LLM-guided search can be applied to domain-specific discovery tasks like catalyst design but evaluations require careful controls for data contamination.",
            "limitations_challenges": "Scope limited to catalyst domain in cited work; evaluation vulnerable to data contamination (rediscovery of known catalysts).",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4586.4",
            "source_info": {
                "paper_title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "CHEMREASONER",
            "name_full": "CHEMREASONER: heuristic search w/ quantum-chemical feedback",
            "brief_description": "A system that performs heuristic search over LLM knowledge space with quantum-chemical feedback to support chemical reasoning and catalyst design (Sprueill et al., 2024).",
            "citation_title": "CHEMREASONER: heuristic search over a large language model's knowledge space using quantum-chemical feedback",
            "mention_or_use": "mention",
            "system_name": "CHEMREASONER",
            "system_description": "Combines LLM-generated candidate ideas with quantum-chemical computational feedback in a heuristic search loop to evaluate and refine chemical proposals, targeted at catalyst design.",
            "llm_model_used": "Not specified within this paper's citation summary; described as an LLM-driven approach in cited work.",
            "extraction_technique": "LLM generation of candidate concepts/paths; anchored by computational chemistry evaluations (quantum-chemical feedback).",
            "synthesis_technique": "Heuristic search that integrates computational feedback to select/refine candidates.",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "Catalyst design / chemistry.",
            "output_type": "Candidate chemical designs and evaluated hypotheses.",
            "evaluation_metrics": "Not detailed in this paper; original work likely uses computational/experimental validation.",
            "performance_results": "Not reported here (cited as related work).",
            "comparison_baseline": "Not compared in this paper.",
            "performance_vs_baseline": "Not available here.",
            "key_findings": "Illustrates combining LLMs with domain-specific computational feedback (quantum chemistry) to increase practical validity in chemical discovery.",
            "limitations_challenges": "Domain-specific; potential data-contamination concerns in earlier catalyst work; requires integration of expensive computational feedback.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4586.5",
            "source_info": {
                "paper_title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Researchagent",
            "name_full": "Researchagent: Iterative research idea generation over scientific literature with LLMs",
            "brief_description": "A recent system that iteratively generates research ideas by leveraging LLMs over scientific literature; cited as related work in this paper.",
            "citation_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "mention_or_use": "mention",
            "system_name": "Researchagent",
            "system_description": "Iterative framework that uses LLMs to generate and refine research ideas by repeatedly consuming and reasoning over scientific literature; details in cited preprint.",
            "llm_model_used": "Not specified in this paper (referenced in bibliography).",
            "extraction_technique": "Iterative consumption of literature (details in cited work).",
            "synthesis_technique": "Iterative idea generation and refinement across documents.",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "General scientific literature idea generation.",
            "output_type": "Research ideas/hypotheses.",
            "evaluation_metrics": "Not specified in this paper.",
            "performance_results": "Not specified here (reference only).",
            "comparison_baseline": "Not applicable in this paper.",
            "performance_vs_baseline": "Not reported here.",
            "key_findings": "Shows trend of iterative LLM-based systems for research idea generation over literature.",
            "limitations_challenges": "Not detailed here.",
            "scaling_behavior": "Not discussed in this paper.",
            "uuid": "e4586.6",
            "source_info": {
                "paper_title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MLR-Copilot",
            "name_full": "MLR-Copilot: Autonomous machine learning research with LLM agents",
            "brief_description": "An LLM-agent system for autonomous machine-learning research tasks, cited as an example of LLM agents applied to research automation.",
            "citation_title": "MLR-copilot: Autonomous machine learning research based on large language models",
            "mention_or_use": "mention",
            "system_name": "MLR-Copilot",
            "system_description": "Agent-based system that uses LLMs to automate parts of machine learning research workflows (planning, idea generation, iteration); referenced as related work in this paper's related work discussion.",
            "llm_model_used": "Not specified in this paper (referenced in bibliography).",
            "extraction_technique": "Not specified here.",
            "synthesis_technique": "Agentic iteration over tasks and literature; details in the cited preprint.",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "Machine learning research.",
            "output_type": "Research ideas, experiment plans, prototypes.",
            "evaluation_metrics": "Not specified in this paper.",
            "performance_results": "Not given here.",
            "comparison_baseline": "Not discussed here.",
            "performance_vs_baseline": "Not available here.",
            "key_findings": "Example of applying LLM agents to automate open-ended research tasks.",
            "limitations_challenges": "Not detailed here.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4586.7",
            "source_info": {
                "paper_title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AI-Scientist",
            "name_full": "The AI Scientist: Towards fully automated open-ended scientific discovery",
            "brief_description": "A proposed system and research direction aiming toward fully automated, open-ended scientific discovery using AI agents, cited as related work.",
            "citation_title": "The AI scientist: Towards fully automated open-ended scientific discovery",
            "mention_or_use": "mention",
            "system_name": "AI Scientist",
            "system_description": "High-level research program and system architecture proposals that combine automated experiment planning, hypothesis generation, and iterative testing using AI agents (details in cited work).",
            "llm_model_used": "Not specified in this paper (referenced in bibliography).",
            "extraction_technique": "Not specified here.",
            "synthesis_technique": "Agentic orchestration of hypothesis generation and experimental validation; specifics in cited paper.",
            "number_of_papers": "Not specified here.",
            "domain_or_topic": "General scientific discovery automation.",
            "output_type": "Automated discoveries, experiment plans, hypotheses.",
            "evaluation_metrics": "Not detailed in this paper.",
            "performance_results": "Not provided here.",
            "comparison_baseline": "Not discussed here.",
            "performance_vs_baseline": "Not available here.",
            "key_findings": "Represents an ongoing line of work toward autonomous scientific discovery using AI agents.",
            "limitations_challenges": "Not detailed here.",
            "scaling_behavior": "Not discussed here.",
            "uuid": "e4586.8",
            "source_info": {
                "paper_title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Self-refine",
            "name_full": "Self-refine: Iterative refinement with self-feedback",
            "brief_description": "An iterative LLM method that refines generated outputs by producing self-feedback and using it to improve subsequent generations; used as a baseline refinement mechanism.",
            "citation_title": "Self-refine: Iterative refinement with self-feedback",
            "mention_or_use": "use",
            "system_name": "Self-refine",
            "system_description": "An iterative refinement loop where an LLM critiques or rates its own outputs across dimensions (e.g., validity, novelty, clarity) and uses that feedback to refine and produce improved outputs.",
            "llm_model_used": "Original self-refine method (Madaan et al., 2023) uses LLMs; specific model not specified in this paper's use as a baseline.",
            "extraction_technique": "Not an extraction method per se; relies on LLM-generated content and self-feedback.",
            "synthesis_technique": "Iterative self-feedback-driven refinement of generated hypotheses or text.",
            "number_of_papers": "Not applicable; used as a refinement procedure in baseline systems evaluated on the 51-paper benchmark.",
            "domain_or_topic": "General; applied to hypothesis generation baselines in social/biomedical/chemistry comparisons here.",
            "output_type": "Refined candidate hypotheses/text.",
            "evaluation_metrics": "Used as part of baseline methods evaluated by Matched Score (MS) and other metrics in experiments.",
            "performance_results": "Baseline systems using self-refine (e.g., MOOSE, Qi et al.) achieved lower Top MS than MOOSE-Chem in the reported experiments (Tables 10 and 13).",
            "comparison_baseline": "Used as a component of baseline methods compared to MOOSE-Chem's EU approach.",
            "performance_vs_baseline": "Self-refine improves generation quality relative to naive generation, but MOOSE-Chem's evolutionary unit + multi-step retrieval yielded higher Top MS in chemistry experiments.",
            "key_findings": "Iterative self-feedback improves candidate hypothesis quality, but mutation/recombination (EU) can further increase top-discovery rates by promoting diverse associations.",
            "limitations_challenges": "May converge to similar refinements and miss more diverse mutation-driven candidate modes; potential evaluation reliability in chemistry.",
            "scaling_behavior": "Not quantified here.",
            "uuid": "e4586.9",
            "source_info": {
                "paper_title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models for automated open-domain scientific hypotheses discovery",
            "rating": 2
        },
        {
            "paper_title": "Scimon: Scientific inspiration machines optimized for novelty",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero shot hypothesis proposers",
            "rating": 2
        },
        {
            "paper_title": "Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design",
            "rating": 2
        },
        {
            "paper_title": "CHEMREASONER: heuristic search over a large language model's knowledge space using quantum-chemical feedback",
            "rating": 2
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        }
    ],
    "cost": 0.023647249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific HYPOTHESES</h1>
<p>Zonglin Yang ${ }^{1,21}$, Wanhao Liu ${ }^{2,3}$, Ben Gao ${ }^{2,4}$, Tong Xie ${ }^{5,6}$, Yuqiang Li ${ }^{2}$, Wanli Ouyang ${ }^{2}$, Soujanya Poria ${ }^{7}$, Erik Cambria ${ }^{11}$, Dongzhan Zhou ${ }^{23}$<br>${ }^{1}$ Nanyang Technological University ${ }^{2}$ Shanghai Artificial Intelligence Laboratory<br>${ }^{3}$ University of Science and Technology of China ${ }^{4}$ Wuhan University ${ }^{5}$ University of New South Wales<br>${ }^{6}$ GreenDynamics ${ }^{7}$ Singapore University of Technology and Design<br>{zonglin001, cambria}@ntu.edu.sg, zhoudongzhan@pjlab.org.cn</p>
<h4>Abstract</h4>
<p>Scientific discovery plays a pivotal role in advancing human society, and recent progress in large language models (LLMs) suggests their potential to accelerate this process. However, it remains unclear whether LLMs can autonomously generate novel and valid hypotheses in chemistry. In this work, we investigate whether LLMs can discover high-quality chemistry hypotheses given only a research back-ground-comprising a question and/or a survey-without restriction on the domain of the question. We begin with the observation that hypothesis discovery is a seemingly intractable task. To address this, we propose a formal mathematical decomposition grounded in a fundamental assumption: that most chemistry hypotheses can be composed from a research background and a set of inspirations. This decomposition leads to three practical subtasks-retrieving inspirations, composing hypotheses with inspirations, and ranking hypotheses-which together constitute a sufficient set of subtasks for the overall scientific discovery task. We further develop an agentic LLM framework, MOOSE-Chem, that is a direct implementation of this mathematical decomposition. To evaluate this framework, we construct a benchmark of 51 high-impact chemistry papers published and online after January 2024, each manually annotated by PhD chemists with background, inspirations, and hypothesis. The framework is able to rediscover many hypotheses with high similarity to the groundtruth, successfully capturing the core innovations-while ensuring no data contamination since it uses an LLM with knowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high accuracy on inspiration retrieval, a task with inherently out-of-distribution nature, we propose a bold assumption: that LLMs may already encode latent scientific knowledge associations not yet recognized by humans. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Discovering new science has long been one of the deepest desires of humanity, which can not only satisfy our curiosity to understand the universe but also contribute largely to the prosperity of human society (Coccia, 2019). Recently, there are some breakthroughs indicating that LLMs have the potential to assist scientists in accelerating the discovery process (Luo et al., 2025).
Yang et al. (2024b) first find that LLMs can generate novel and valid enough hypotheses evaluated by experts. They focus on the social science domain and make discoveries by developing a multi-agent system, leveraging an assumption that a majority of social science hypotheses can be divided into a research background concept and an inspiration concept. This assumption is largely valid because a social science hypothesis is about how an independent variable can influence another dependent variable (Hair et al., 2007).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Si et al. (2024) further validate this finding by employing a large group of scientists to evaluate LLMs' generated hypotheses in the NLP domain and show that LLM can generate more novel but slightly less valid research hypotheses than human researchers. However, it is still unclear LLMs' scientific discovery ability in natural science such as the chemistry domain.
Sprueill et al. $(2023 ; 2024)$ adopt LLMs to conduct a search process for catalyst discovery. However, their method is limited in the catalyst discovery domain, and their evaluation relies on whether LLMs can rediscover existing commercially used catalysts, potentially influenced by a data contamination problem. As a result, it is still unclear how good LLMs are for chemistry scientific discovery.
In this paper, we investigate this central research question: Can LLMs automatically discover novel and valid chemistry research hypotheses (even at the Nature level) given only a chemistry research background (consisting of a research question and/or a background survey), without limitation on the domain of the research question? With extensive discussions with chemistry experts, we find that the assumption used in social science, that a hypothesis can be divided into background and inspiration, can also apply to a majority of chemistry hypotheses. It is not too surprising, since cognitive science research has shown that creative ideas often result from the cohesive association of two seemingly unrelated pieces of knowledge (Koestler, 1964; Benedek et al., 2012; Lee \&amp; Chung, 2024). A main difference is that chemistry might need more than one inspiration (e.g., adding several components to compose a novel chemistry system). With this key insight, we break the seemingly impossible-to-solve central question into three smaller, more practical, and executable fundamental questions that, when summed up, should be very close to a set of sufficient conditions for the central question. Specifically, the smaller questions are (1) whether LLM can identify inspiration papers that have the potential to help with the given research question; (2) given only known knowledge (from background and inspirations), whether LLMs can infer unknown knowledge that is highly likely to be valid; and (3) whether LLM can identify good hypotheses and rank them higher.
To investigate these three questions, we build a benchmark consisting of 51 chemistry papers annotated by chemistry PhD students, breaking every paper into a background, several inspirations, and a hypothesis. The goal is to rediscover the hypothesis with only the background by using LLMs trained with data up to December 2023. The papers are all published in Nature, Science, or a similar level in 2024, and they are only made public on the internet in 2024. The benchmark is designed to be similar to the Mathematical Olympiad Competition (Trinh et al., 2024), to provide several dozens of very difficult and meaningful questions to solve. Along with the benchmark, we propose a ranking task for scientific discovery (along with evaluation criteria), which has been largely overlooked in previous works (Yang et al., 2024a; Wang et al., 2024b). Ranking is important because although AI systems can generate a large number of hypotheses in a relatively short time, verifying them one by one requires a lot of experimental costs.
Motivated by this breakup into three smaller questions, we design a multi-agent framework named MOOSE-CHEM for chemistry scientific discovery. It in general includes three stages: (1) searching through chemistry literature to find inspiration papers, (2) leveraging the inspirations to propose hypotheses for the background research question, and (3) identifying high-quality hypotheses to give them a higher rank. Compared with Yang et al. (2024b)'s method in social science that assumes a similar separation between background and inspiration for hypothesis formulation, MOOSE-CHEM adopts an evolutionary algorithm to foster a broader diversity of approaches in using inspiration for background, thereby capitalizing on the benefits derived from varied mutations. In addition, MOOSE-CHEM also adopts a multi-step design to collect more than one inspirations for chemistry discovery. Finally, it uses an efficient ranking method for better reference for scientists.
We design experiments with the benchmark to test the three fundamental questions and find that LLMs are highly capable. We also test MOOSE-CHEM with the benchmark, mimicking the setting to run it in the wild by only giving a background and a corpus of up to 3,000 chemistry papers to select inspiration. Even in this challenging setting, MOOSE-CHEM can still rediscover many hypotheses with very high similarity with the ground truth ones, covering the main innovations.
Overall, the contributions of this paper are:</p>
<ul>
<li>
<p>We provide the first mathematical derivation on how to decompose the seemingly impossible-to-solve question $P$ (hypothesis/research background) into many executable and practical smaller steps. This decomposition make $P$ (hypothesis/research background) possible to be practical.</p>
</li>
<li>
<p>We develop a scientific discovery framework directly based on the mathematical derivation. Different from previous works, we propose an evolutionary algorithm-based method to better associate background and inspiration, multi-step inspiration retrieval and composition, and an efficient ranking method. In addition, the framework can be applied to chemistry and material science, which are not covered by previous methods.</p>
</li>
<li>We construct a benchmark by three chemistry PhD students, consisting of 51 chemistry papers published on Nature, Science, or a similar level, decomposing each paper into the research background, inspirations, and hypothesis.</li>
<li>We propose an assumption, grounded in preliminary experiments, that LLMs may already possess numerous knowledge pairs capable of being associated to create novel knowl-edge-even when scientists have not previously recognized any relationship between them.</li>
<li>For the first time, we show that an LLM-based framework can largely rediscover the main innovations of many chemistry hypotheses that have been published in Nature and Science. The rediscovery is not because of data contamination, because we have controlled the date of the training corpus of the LLM and the online date of the chemistry papers.</li>
</ul>
<h1>2 RELATED WORK</h1>
<p>Zhong et al. (2023) work on finding the difference between two corpora to propose hypotheses, but their evaluation is conducted by Turkers, which cannot lead to a novel discovery. Wang et al. (2024b) try to utilize LLMs to discover novel NLP and biochemical hypotheses, and find the hypotheses still fall far behind scientific papers in terms of novelty, depth, and utility. Yang et al. (2024b) first show that LLMs can generate novel and valid enough hypotheses evaluated by PhD students, but they only focus on social science. FunSearch (Romera-Paredes et al., 2024) can discover specific solutions for mathematical conjecture but can't discover new math theorems. Qi et al. (2024) analyzes LLM's ability for scientific discovery in the biomedical domain by directly generating hypotheses with only the research background. Boiko et al. (2023); Baek et al. (2024); Li et al. (2024); Lu et al. (2024) focus on subsequent steps for scientific discovery, mainly developing and conducting experiments. Sprueill et al. $(2023 ; 2024)$ focus on catalyst discovery, but their evaluation relies on whether can rediscover existing commercially used catalysts, which might cause data contamination problem. Kumar et al. (2024) compare different LLMs on scientific discovery in different disciplines. Tshitoyan et al. (2019) show that word embedding obtained from large-scale chemistry literature can recommend materials years before their discovery. Xie et al. (2024) predict emerging thermoelectric materials by summarizing the sentiment in the existing literature.</p>
<h2>3 BENCHMARK CONSTRUCTION</h2>
<p>The goal of the benchmark, named TOMATO-Chem, is two-fold. Firstly, it is used to analyze LLM's ability in terms of the three smaller questions. Secondly, it serves as a challenge to rediscover naturelevel chemistry hypotheses with only a research background. The setting of the challenge is very similar to a real copilot setting, where scientists tell the copilot about the specific research question they are interested in, and optionally a small survey consisting of several paragraphs summarizing the existing best-performing methods for the research question.</p>
<p>To achieve the goals, we split each collected paper into the following components: <background question, background question (strict), background survey, background survey (strict), one to three inspiration paper titles and their reason to serve as an inspiration, research hypothesis, experiments, reasoning process, summarization of inspirations $>$. Every component is described by text.</p>
<p>The reason we add a strict version for background question and background survey is that many hypotheses are making relatively minor modifications based on existing methods covered by the survey, and the question can be very insightful to provide a hint on the general direction of the hypothesis. In practice, these situations are entirely possible, especially when the scientist users can provide a more comprehensive survey on existing methods, or contain deep insights in their question. Here, we also keep the strict version to make the task more challenging and encourage developing methods to better assist scientists even when they are also new to their research topic.</p>
<p>The reasoning process indicates the relation between the components of background, inspirations, and hypothesis. For example, the reasoning process can be "background + inspiration 1 + inspiration 2 = hypothesis", or "background + inspiration 1/inspiration 2 + inspiration 3 = hypothesis".</p>
<p>The benchmark consists of 51 chemistry and material science papers and is constructed by multiple chemistry PhD students. We only select those papers published on top chemistry venues and be public on the internet after January 2024. After constructing, the experts check again on (1) whether the identification of the inspirations is correct and whether more inspirations are needed; (2) whether the background does not contain any information in inspirations or hypothesis; and (3) whether the background and the identified inspirations can roughly logically lead to the hypothesis. The complete instruction on the check process is shown in $\S$ A.3.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Polymer Chemistry</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: center;">Organic Chemistry</td>
<td style="text-align: center;">22</td>
</tr>
<tr>
<td style="text-align: center;">Inorganic Chemistry</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Analytical Chemistry</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">51</td>
</tr>
</tbody>
</table>
<p>Table 1: Distribution of categories.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Publication Venue</th>
<th style="text-align: center;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Nature / Science</td>
<td style="text-align: center;">27</td>
</tr>
<tr>
<td style="text-align: center;">Nature Subjournals</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">Other Top Journals</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">Total</td>
<td style="text-align: center;">51</td>
</tr>
</tbody>
</table>
<p>Table 2: Distribution of publication venues.</p>
<p>Table 1 and Table 2 show the statistics of the benchmark in terms of chemistry category and publication venue. Material science is a sub-category of chemistry and can belong to the categories in Table 1, such as polymer material and organic material. Around 13 collected benchmark papers are inside the material science domain. Beyond them, more papers have intersections with material science. In this paper, we target both chemistry and material science, but for simplicity, we only refer to them as chemistry in this paper.</p>
<h1>4 Methodology</h1>
<h3>4.1 Fundamental Assumption and Following Decomposition</h3>
<p>We propose an assumption that a majority of chemistry hypotheses can originate from a research background and several inspirations. This assumption is not only supported by many chemistry researchers whom we have extensive discussions with but also by the cognitive science finding that "creative ideas often result from the cohesive association of two (or more) seemingly unrelated pieces of knowledge" (Koestler, 1964; Benedek et al., 2012; Lee \&amp; Chung, 2024). We design our method based on this fundamental assumption.</p>
<p>Denoting background knowledge as $b$, inspiration knowledge as $i$, and hypothesis as $h$, we translate this assumption as:</p>
<p>$$
h=f\left(b, i_{1}, \ldots, i_{k}\right)
$$</p>
<p>Here, $k \in \mathbb{Z}$ represents the number of inspirations needed for a particular $h$. Typically in chemistry, $k \in[1,3]$. In other words, given existing knowledge in the background, a majority of chemistry research is about searching knowledge that previously not known to be related to the background but in fact can assist the background, then associate the background knowledge and the searched knowledge in a reasonable way to compose a hypothesis.</p>
<p>Based on this assumption, we can transform the seemingly impossible-to-solve $P(h \mid b)$ into an equivalent form, where each step in the equivalent form is practical and executable.</p>
<p>$$
P(h \mid b) \approx \prod_{j=1}^{k} P\left(i_{j} \mid b, h_{j-1}, I\right) \cdot P\left(h_{j} \mid b, h_{j-1}, i_{j}\right), \text { where } h_{0}=\emptyset
$$</p>
<p>This decomposition naturally defines a Markov Decision Process (MDP), where each intermediate hypothesis $h_{j-1}$ and selected inspiration $i_{j}$ constitute a state-action pair, $P\left(i_{j} \mid b, h_{j-1}, I\right)$ models the policy of selecting the next inspiration, and $P\left(h_{j} \mid b, h_{j-1}, i_{j}\right)$ models the state transition toward</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The MOOSE-Chem framework. It receives $b$ and $I$ as input, and outputs a list of ranked $h$. The bottom-right legend describes the symbols in the figure.
the next step of hypothesis. Here, $I$ denotes the full (chemistry) literature, representing the entire inspiration space to search for each $i$. The complete derivation and theoretical justification are provided in § A.2, which forms the core of this paper.
Equation 2 is meaningful in that by decomposing $P(h \mid b)$ into more practical and executable smaller questions, the seemingly impossible-to-solve $P(h \mid b)$ itself becomes practical. We analyze how $P\left(i_{j} \mid b, h_{j-1}, I\right)$ and $P\left(h_{j} \mid b, h_{j-1}, i_{j}\right)$ are practical and executable by LLMs in $\S 5.1$ and $\S 5.2$ correspondingly.
Now we have clarified the steps to obtain $h$ from $b$. However, it still might not be enough helpful in practice, since $I$ can be on a large scale, and the search process might find lots of $i$, and finally lead to lots of $h$. Moreover, it is very time-consuming for scientists to conduct experiments to verify every single $h$. Therefore, it would be very helpful if the generated $h$ could be ranked based on quality. Here, we adopt a straightforward and efficient way for ranking. Specifically, we design a rating function $R(h)$, such that $R(h) \rightarrow \mathbb{R}$. Denoting the full set of generated $h$ as $H$, we can obtain</p>
<p>$$
P\left(H_{\text {ranked }}\right)=P(H, R), \text { where } H_{\text {ranked }}=\left{h_{1}, h_{2}, \ldots, h_{n} \mid R\left(h_{i}\right) \geq R\left(h_{i+1}\right) \text { for all } i\right}
$$</p>
<p>Supported by Equation 2 and Equation 3, as a result, to model $P(h \mid b)$, the only three components we need to model are $P\left(i_{j} \mid b, h_{j-1}, I\right), P\left(h_{j} \mid b, h_{j-1}, i_{j}\right)$, and $R(h)$. The implementation details of the three components are illustrated in the remaining subsections in $\S 4$. Analyses of LLM's ability on the three components are provided in $\S 5$.</p>
<h1>4.2 The Framework Developed Based on the Assumption</h1>
<h3>4.2.1 The General Picture</h3>
<p>Our methodology is developed based on the fundamental assumption discussed in $\S$ 4.1. Specifically, we use LLMs to perform $P\left(i_{j} \mid b, h_{j-1}, I\right), P\left(h_{j} \mid b, h_{j-1}, i_{j}\right)$, and $R(h)$, and organize them into a multi-agent LLM-based framework. The input to the framework is only a background question and/or background survey, together with a (large) chemistry literature corpus to search for inspiration. The output of the framework is a list of ranked research hypothesis.
The framework's design is shown in Figure 1 (overview in Figure 2). It is a direct implementation of Equation 2 and 3. We develop it as simply as possible, retaining only the necessary parts.
In the general picture, given a research background $b$ (research question and/or research survey), the framework first performs $P\left(i_{1} \mid b, h_{0}=\emptyset, I\right)$ by screening through the literature corpus $I$ to select many papers $i$, where each of them has the potential to serve as an inspiration. Then the framework performs $P\left(h_{1} \mid b, i_{1}, h_{0}=\emptyset\right)$, associating $b$ and each $i$ together to compose $h$. Then, it ranks $h$ by assigning an evaluation score $r$ on each of $h_{1}$ by $R\left(h_{1}\right)$. We call these three steps as one round. Another round means going through the three steps again, based on the previous round's results.</p>
<p>Since normally in chemistry, no more than three inspirations are needed for one hypothesis ( $k \in$ $[1,3]$ ), the default setting for MOOSE-Chem is to perform three rounds for each $b$. In every other round, the number of $i$ and $h$ can expand exponentially. Here, we adopt beam search to select a fixed size of the top-ranked $h$ to enter the next round. The default beam size is 15 .</p>
<h1>4.2.2 Design Details of $P\left(i_{j} \mid b, h_{j-1}, I\right)$ And Its Motivation</h1>
<p>We use LLMs to conduct a screening process for $P\left(i_{j} \mid b, h_{j-1}, I\right)$. Specifically, for each inference, we (1) sequentially select a fixed number of papers from $I$, where the fixed number is called the screening window size (default is 15 ); (2) set up a prompt consisting of $b$, the title and abstract of the selected papers from $I$, and the previous $h$ (if it is not $\emptyset$ ); and (3) instruct the LLM to generate three titles from the input that can best serve as $i$ for $b$ (and optionally previous $h$ ), and give reasons.</p>
<p>In particular, we use LLMs to choose potential inspiration $i$, but not choose $i$ from citation nor semantic neighbors because $i$ is supposed to be previously not known to be related to $b$ (we have discussed it in § 4.1). If the chosen $i$ is already known to be related to $b$, then the composed $h$ probably would not be novel. If the chosen $i$ contains similar semantic information with $b$, then probably it is not necessary to add $i$ at all, since it does not introduce much (any) extra information.</p>
<p>Our bold assumption here is that advanced LLMs, trained on vast scientific literature, may already recognize novel knowledge pairs unknown to any scientist that can be associated to create novel knowledge. However, this may not be too bold, as Tshitoyan et al. (2019) showed that unsupervised word embeddings from 3.3 million materials science abstracts could predict functional materials years before their discovery. Here, the functional applications can be seen as $b$, and the recommended materials can be seen as $i$, or even directly as $h$ if it is enough similar. It probably indicates that LLMs trained with significantly more literature tokens and parameters might already be able to identify the relation between many knowledge pairs that are unknown to be related by any scientist. We analyze this assumption in $\S 5.1$.</p>
<h3>4.2.3 Design Details of $P\left(h_{j} \mid b, h_{j-1}, i_{j}\right)$ And Its Motivation</h3>
<p>The retrieved $i$ is expected to be not known to be related to $b$; therefore, it might be difficult to figure out an effective way to associate $b$ and $i$ together to compose $h$. Think of the time when backpropagation is about to be invented. Even if we are very familiar with $b$ (multi-layer logistic regression) and have successfully retrieved $i$ (chain rule in mathematics), can we invent backpropagation?</p>
<p>Our answer is, at least we might need to try multiple times and various ways to leverage the chain rule for multi-layer logistic regression. With this motivation, we develop a simple evolutionary algorithm-based method, shown in the top-right of Figure 1. We call it "evolutionary unit" (EU).</p>
<p>Specifically, given $b$ and $i$, EU will first generate multiple hypothesis "mutations" $m$, where each $m$ is a unique way to associate $b$ and $i$ together. Then EU further develops each $m$ independently by providing feedback to each $m$ in terms of validness, novelty, clarity, and significance, and then refining them based on the feedback. Yang et al. (2024b) first propose to provide feedback in terms of validness, novelty, and clarity to refine hypotheses. Here, we add an additional aspect, significance, since significance is an important evaluation criterion in chemistry. We assume the refined hypothesis should be of better quality so that the refined hypothesis is "selected", while the previous hypothesis is "eliminated" by the "environment". Finally EU "recombines" the remaining selected $m$, leveraging the advantages from every $m$ to propose $h$ to better associate $b$ and $i$.</p>
<h3>4.2.4 Design Details of $R(h)$ And Its Motivation</h3>
<p>We adopt a simple and efficient way for $R(h)$, which is to prompt an LLM to output evaluation scores for an input $h$ in terms of validness, novelty, significance, and potential. Validness and novelty are two fundamental requirements for such an inductive reasoning process as scientific discovery (Yang et al., 2024a;b). Significance is added because it is important for chemistry. We additionally add potential, because the generated $h$ are about to be further developed by scientists, so we might want to pick those $h$ that not only are currently in high quality but also have good potential to be further developed. We did not design $R(h)$ in a more complicated way, since there are lots of $h$ to rank, and we might want to save more inference time.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Corpus Size</th>
<th style="text-align: center;">Hit Ratio (top 20\%)</th>
<th style="text-align: center;">Hit Ratio (top 4\%)</th>
<th style="text-align: center;">Hit Ratio (top 0.8\%)</th>
<th style="text-align: center;">Hit Ratio (top 0.016\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">150</td>
<td style="text-align: center;">$92.8 \%$</td>
<td style="text-align: center;">$76.8 \%$</td>
<td style="text-align: center;">$61.4 \%$</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">300</td>
<td style="text-align: center;">$96.7 \%$</td>
<td style="text-align: center;">$83.7 \%$</td>
<td style="text-align: center;">$60.8 \%$</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">1000</td>
<td style="text-align: center;">$96.4 \%$</td>
<td style="text-align: center;">$88.9 \%$</td>
<td style="text-align: center;">$69.0 \%$</td>
<td style="text-align: center;">$46.7 \%$</td>
</tr>
<tr>
<td style="text-align: center;">3000</td>
<td style="text-align: center;">$95.8 \%$</td>
<td style="text-align: center;">$86.9 \%$</td>
<td style="text-align: center;">$70.6 \%$</td>
<td style="text-align: center;">$52.0 \%$</td>
</tr>
</tbody>
</table>
<p>Table 3: Main table for $Q 1$. For each screen window of 15 papers, 3 papers are selected.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Screen window size</th>
<th style="text-align: center;">Hit Ratio (1 round)</th>
<th style="text-align: center;">Hit Ratio (2 round)</th>
<th style="text-align: center;">Hit Ratio (3 round)</th>
<th style="text-align: center;">Hit Ratio (4 round)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">$98.0 \%$</td>
<td style="text-align: center;">$88.9 \%$</td>
<td style="text-align: center;">$79.4 \%$</td>
<td style="text-align: center;">$56.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">15</td>
<td style="text-align: center;">$96.7 \%$</td>
<td style="text-align: center;">$83.7 \%$</td>
<td style="text-align: center;">$60.8 \%$</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$91.2 \%$</td>
<td style="text-align: center;">$76.8 \%$</td>
<td style="text-align: center;">$58.8 \%$</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">40</td>
<td style="text-align: center;">$88.9 \%$</td>
<td style="text-align: center;">$54.9 \%$</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
<tr>
<td style="text-align: center;">60</td>
<td style="text-align: center;">$71.6 \%$</td>
<td style="text-align: center;">$53.9 \%$</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">NA</td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation table on screen window size for $Q 1$. The corpus size is 300 . For each screen window no matter its size, 3 papers are selected to remain for the next round of screening.</p>
<p>Yang et al. (2024b) use the scores as automatic evaluation for generated social science hypotheses and have shown a high consistency score between automatic evaluation and expert evaluation. However, in the chemistry domain, LLMs might not be reliable enough to directly evaluate the generated $h$ (Sprueill et al., 2024). But it might still be able to provide a preliminary quality identifier to $h$ : the ranking of the average score between the four aspects of an $h$ determines whether it will enter the next round of MOOSE-Chem by beam search. To understand how well LLMs can perform $R(h)$, we analyze "how well LLMs can rank chemistry hypotheses" in § 5.3.</p>
<h1>5 INVESTIGATION ON FUNDAMENTAL QUESTIONS</h1>
<p>$P(h \mid b)$ can be understood as the task to discover high-quality chemistry research hypothesis, given only a background question and/or background survey. Our central question to investigate is how well LLMs can perform $P(h \mid b)$. Supported by Equation 2 and 3, we break up this main question into three smaller questions: how well can LLMs perform (1) $P\left(i_{j} \mid b, h_{j-1}, I\right)$, (2) $P\left(h_{j} \mid\right.$ $\left.b, h_{j-1}, i_{j}\right)$, and (3) $R(h)$ ? All experiments are performed by GPT-40 (its training data is up to October 2023).</p>
<h3>5.1 How Well Can LLMs PERForm $P\left(i_{j} \mid b, h_{j-1}, I\right)$ ?</h3>
<p>Here, we investigate the question (denoted as $Q 1$ ): "whether LLM can identify inspiration papers which are unknown to be able to associate with the background (or at least unknown to associate in a certain way) but in fact can associate with the background to create novel knowledge?".</p>
<p>We first find 3000 most cited chemistry papers published in Nature, and construct a series of $I$ in size of 150, 300, 1000, and 3000. $I$ is constructed by first adding the ground truth inspiration papers (around 120), then randomly selecting the remaining papers from the 3000 papers, and finally randomizing the order of all the collected papers. Only title and abstract are needed for each paper in $I$. The default setting is that each inference of LLMs will screen 15 papers from $I$, and generate three titles that LLMs think can best assist $b$ (and/or previous $h$ ). Screening through $I$ for one round, only $20 \%$ of $I$ will be selected. Screening another round will only leave $4 \%$, and so on.</p>
<p>We use Hit Ratio as the evaluation metric, which is calculated by the number of selected ground truth inspiration papers divided by the number of all ground truth inspiration papers. All the Hit Ratio numbers shown in the tables are averaged across the 51 papers in the benchmark.</p>
<p>Table 3 shows the main experiment results. The Hit Ratio is surprisingly high: More than $75 \%$ of the ground truth inspirations are covered by even only the $4 \%$ chosen papers from the chemistry literature corpus. It seems that LLMs are quite capable of finding inspiration papers that are unknown to be able to associate with the background but in fact, can associate with the background to create novel knowledge. It means our bold assumption in $\S 4.2 .2$ that "the most advanced LLMs might already know lots of knowledge pairs that are able to associate to create novel knowledge, where the knowledge pairs are not known by any scientist to be related" is possible to be true.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Hit Ratio (top 20\%)</th>
<th style="text-align: center;">Hit Ratio (top 4\%)</th>
<th style="text-align: center;">Hit Ratio (top 0.8\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama-3.1-8B</td>
<td style="text-align: center;">$71.6 \%$</td>
<td style="text-align: center;">$43.5 \%$</td>
<td style="text-align: center;">$26.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3.1-70B</td>
<td style="text-align: center;">$95.1 \%$</td>
<td style="text-align: center;">$83.0 \%$</td>
<td style="text-align: center;">$59.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Llama-3.1-405B</td>
<td style="text-align: center;">$95.7 \%$</td>
<td style="text-align: center;">$78.7 \%$</td>
<td style="text-align: center;">$52.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-40</td>
<td style="text-align: center;">$96.7 \%$</td>
<td style="text-align: center;">$83.7 \%$</td>
<td style="text-align: center;">$60.8 \%$</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of Llama series and GPT-40 on inspiration retrieval. The corpus size is 300. For each screen window of 15 papers, 3 papers are selected.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">5 points</th>
<th style="text-align: center;">Generated hypothesis covers three key points (or covers all the key points) and leverage <br> them similarly as in the groundtruth hypothesis; Extra key points do not have apparent flaws.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">4 points</td>
<td style="text-align: center;">Generated hypothesis covers three key points (or covers all the key points) and leverage <br> them similarly as in the groundtruth hypothesis; Extra key points have apparent flaws.</td>
</tr>
<tr>
<td style="text-align: center;">3 points</td>
<td style="text-align: center;">Generated hypothesis covers two key points and leverage them similarly <br> as in the groundtruth hypothesis, but does not cover more or all key points</td>
</tr>
<tr>
<td style="text-align: center;">2 points</td>
<td style="text-align: center;">Generated hypothesis covers one key point and leverage it similarly <br> as in the groundtruth hypothesis, but does not cover more or all key points</td>
</tr>
<tr>
<td style="text-align: center;">1 point</td>
<td style="text-align: center;">Generated hypothesis covers at least one key point, but is used differently <br> as in the groundtruth hypothesis</td>
</tr>
<tr>
<td style="text-align: center;">0 point</td>
<td style="text-align: center;">Generated hypothesis does not cover any key point</td>
</tr>
</tbody>
</table>
<p>Table 6: Description of the Matched Score.</p>
<p>Table 4 shows the ablation study in terms of screen window size. It seems that a smaller window size can lead to better performance: a screen window size of 60 to keep 3 for one round will select $5 \%$ of the corpus, and the Hit Ratio is $71.6 \%$; while a screen window size of 15 to keep 3 for two rounds will select only $4 \%$ of the corpus, but the Hit Ratio is as high as $83.7 \%$.</p>
<p>Table 5 compares LLMs in different scales on inspiration retrieval ability. The results indicate that LLMs obtain the emergent ability for inspiration retrieval since a rather small parameter size, but then quickly plateau. § A. 9 discusses research background options' influence on inspiration retrieval.</p>
<h1>5.2 How Well Can LLMs PERForm $P\left(h_{j} \mid b, h_{j-1}, i_{j}\right)$ ?</h1>
<p>Here, we investigate the question (denoted as $Q 2$ ): "Given only known knowledge, whether LLM can reason to unknown knowledge that has high probability to be valid?".</p>
<p>The first challenge to answer $Q 2$ is the evaluation method: The benchmark covers a large range of chemistry topics, and chemistry is a very complex discipline that a slight change of research topic would make a chemist unable to provide a reliable enough evaluation. In fact, a chemistry researcher might not be able to provide a reliable enough evaluation even if the hypothesis is in his domain.</p>
<p>Therefore, we adopt a reference-based evaluation method called "Matched Score" (MS). The descriptions are shown in Table 6. It's on a 6-point Likert scale, roughly containing four stages. Denoting generated hypothesis as $g h$, and original hypothesis as $o h$, the four stages are (1) $g h \cap o h=\emptyset$ (0 point); (2) $g h \cap o h \neq \emptyset$ (1/2/3 points); (3) $g h \supseteq o h$ (4 points); (4) $g h \approx o h$ (5 points).</p>
<p>We use MOOSE-Chem to investigate $Q 2$. Specifically, we initialize $I$ as only the ground truth inspiration papers and search $i$ for $k$ round, where $k$ is the number of ground truth $i$ needed for each $b$. MOOSE-Chem will not retrieve the same $i$ already retrieved in previous rounds, guaranteeing that before generating the final $h$, the framework has already seen all the ground truth inspirations.</p>
<p>Table 7 shows the results. For each $b$, the top two $h$ with the highest MS by GPT-40 are selected for expert evaluation (by two chemistry PhD students). It indicates that LLMs are quite capable of associating known knowledge into unknown knowledge that has a high probability to be valid (very close to $o h$ ). In addition, providing a survey can assist the new knowledge-discovery process. We discuss the agreement between GPT-40-based evaluation and expert evaluation in § A.14.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/ background survey</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Average MS (GPT-40)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Top MS (GPT-40)</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Top MS (Experts)</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o background survey</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Average MS (GPT-40)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Top MS (GPT-40)</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
</tbody>
</table>
<p>Table 7: Main table for $Q 2$. Average/Top MS means the average/highest Matched Score of all generated $h$ from one $b$. Table 12 is a more complete version of this table including automatic evaluation results by Claude-3.5-Sonnet and Gemini-1.5-Pro.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">#Matched $i$</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Average Rank Ratio</td>
<td style="text-align: center;">NA</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.474</td>
<td style="text-align: center;">0.521</td>
</tr>
<tr>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">302</td>
<td style="text-align: center;">2458</td>
<td style="text-align: center;">4899</td>
</tr>
</tbody>
</table>
<p>Table 8: Relation between the number of matched ground truth $i$ and the average ranking ratio $(\downarrow)$.</p>
<h1>5.3 How Well Can LLMs PERForm $R(h)$ ?</h1>
<p>Here, we investigate $Q 3$ : "whether LLMs can select high-quality $h$ to rank them higher?".
To investigate $Q 3$, we run MOOSE-Chem with every $b$ from the benchmark; $|I|=300$, containing all the ground truth $i$. Every $h$ is given a rating $r=R(h)$, and is ranked based on $r$. For every generated $h$, we get the number of ground truth $i$ it leveraged (#Matched $i$ ), and evaluate it with a GPT-40 evaluated MS (here MS is -1 means this $h$ has not used any ground truth $i$ ).</p>
<p>Table 8 shows the relation between the #Matched $i$ and average ranking ratio (the lower, the better). It shows a clear trend that the more ground truth $i$ is leveraged, the better ranking score $h$ can have. It indicates that $h$ with a higher ranking ratio is more likely to be matched with better $i$.</p>
<p>Table 9 shows the relation between the GPT-40 evaluated MS and the average ranking ratio. There is a trend that the higher the MS, the better the average rank ratio (when MS $\in[2,4]$ ). However, the disadvantage of those $h$ without a positive MS is not very significant. It seems that LLMs have a certain ability to rank good $h$ higher. But it is not sure how significant it is, because a part of the reason for these results is that those $h$ generated without ground truth $i$ could be also in high quality.</p>
<h2>6 EXPERIMENT AND Ablation Study</h2>
<p>We perform experiments in a setting similar to the copilot in the wild setting. Only background question (strict), background survey (strict), and a chemistry corpus $|I|=300$ are provided to the framework. Only the top $4 \%$ of $I$ is selected and used to develop $h$. The evaluation metrics are Top MS and Average MS (the highest/average Matched Score of all generated $h$ from one $b$ ), averaging across the benchmark. Experiments are conducted by GPT-40 (training data up to October 2023).</p>
<h3>6.1 BASELINES</h3>
<p>MOOSE is a hypothesis discovery framework for the general social science domain. It leverages LLMs to retrieve inspirations and uses self-refine (Madaan et al., 2023) to improve the validness, novelty, and clarity aspects. The difference is that (1) it does not adopt the mutation and recombination step to better associate background and inspiration; (2) it only retrieves one step of inspiration.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Matched Score</th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">-1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Average Rank Ratio</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">0.488</td>
<td style="text-align: center;">0.501</td>
<td style="text-align: center;">0.436</td>
<td style="text-align: center;">0.501</td>
<td style="text-align: center;">0.503</td>
</tr>
<tr>
<td style="text-align: center;">Size</td>
<td style="text-align: center;">210</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">404</td>
<td style="text-align: center;">427</td>
<td style="text-align: center;">29</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">6451</td>
</tr>
</tbody>
</table>
<p>Table 9: Relation between the GPT-40 labeled Matched Score and average ranking ratio $(\downarrow)$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Top MS</th>
<th style="text-align: center;">Average MS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SciMON (Wang et al., 2024b)</td>
<td style="text-align: center;">2.549</td>
<td style="text-align: center;">2.281</td>
</tr>
<tr>
<td style="text-align: left;">MOOSE (Yang et al., 2024a)</td>
<td style="text-align: center;">2.882</td>
<td style="text-align: center;">2.464</td>
</tr>
<tr>
<td style="text-align: left;">Qi et al. (2024)</td>
<td style="text-align: center;">2.686</td>
<td style="text-align: center;">2.356</td>
</tr>
<tr>
<td style="text-align: left;">MOOSE-Chem</td>
<td style="text-align: center;">$\mathbf{4 . 0 2 0}$</td>
<td style="text-align: center;">2.564</td>
</tr>
<tr>
<td style="text-align: left;">w/o multi-step</td>
<td style="text-align: center;">3.765</td>
<td style="text-align: center;">2.730</td>
</tr>
<tr>
<td style="text-align: left;">w/o multi-step \&amp; EU</td>
<td style="text-align: center;">2.863</td>
<td style="text-align: center;">2.578</td>
</tr>
</tbody>
</table>
<p>Table 10: Experiments and ablation study. The Matched Score (MS) is evaluated by GPT-40 (this table), Claude-3.5-Sonnet (Table 13), and Gemini-1.5-Pro (Table 14).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Top MS (Expert)</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">51</td>
</tr>
</tbody>
</table>
<p>Table 11: MOOSE-Chem runs with $|I|=300$, mimicking the copilot setting. This table shows the statistics of the top Matched Score across the benchmark. The evaluation is done by experts.</p>
<p>SciMON is a hypothesis discovery framework for the NLP and biochemical domain. It relies on semantic and citation neighbors to retrieve information to assist the background. As a result, the retrieved information could be very related to the background that might not be able to serve as an inspiration. Here, we implement SciMON with LLM-based inspiration retrieval.</p>
<p>Qi et al. (2024) work on hypothesis discovery in the biomedical domain. It retrieves information pertinent to the keywords in the background to generate hypotheses. As a result, the retrieved information might compose of a background survey, but not as inspiration. Self-refine is also adopted.</p>
<h1>6.2 ReSults</h1>
<p>Table 10 shows the baseline results and the ablation study of MOOSE-Chem. It indicates that both mutation \&amp; recombination and the multi-step designs can significantly improve the best-performing $h$. Mutation \&amp; recombination leads to a drop of Average MS compared to the MOOSE baseline; we attribute the reason to that the mutation step forces LLMs to generate $h$ different from previous $h$ mutations from the same $b$ and $i$, and therefore might generate many $h$ that do not make a lot of sense. The assigned MS to these mutation $h$ is low, and therefore lower down the Average MS.</p>
<p>To better understand the performance of MOOSE-Chem in this real copilot setting, for each $b$ the top 4 generated $h$ with the highest MS by GPT-40 are evaluated again by two experts in terms of MS. Table 11 shows the expert evaluation results. Here, the top MS is the highest MS for each $b$, out of the 4 expert evaluated $h$ for this $b$. Note that MS rated as three is already very high. Illustrated in Table 6, it means the generated $h$ by MOOSE-Chem (that has not seen $h$ ) in the real copilot setting covers two main innovations of the chemistry hypothesis, which is published in Nature, Science or a similar level. Some case studies can be seen in § A.16.</p>
<h2>7 CONCLUSION</h2>
<p>We investigated this central question: "Can LLMs automatically discover novel and valid chemistry (including material science) research hypotheses (even those which deserve a publication in Nature, Science, or a similar level) given only a chemistry research background (consisting of a research question and/or a background survey), without limitation on the domain of the research question?". We proposed a fundamental assumption to break up this seemingly impossible-to-solve central question into three smaller, more practical, and executable fundamental questions. Then, we investigated LLM's ability on each of them. To this end, we constructed a benchmark consisting of chemistry and material science papers published and only be public in 2024. We also developed an LLM-based multi-agent framework consisting of three stages reflecting the three smaller fundamental questions. Experiments showed that the framework (runs in a copilot in-the-wild setting, with LLMs with training data up to October 2023) can rediscover many hypotheses with very high similarity with the ground-truth ones, covering the main innovations.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>This work is supported by the Shanghai Municipal Science and Technology Major Project. This work is supported by Shanghai Artificial Intelligence Laboratory. This research/project is supported by the Ministry of Education, Singapore under its MOE Academic Research Fund Tier 2 (STEM RIE2025 Award MOE-T2EP20123-0005).</p>
<p>We thank Mengsong Wu for his insightful discussions with us, and we thank Yuwei Wan for her efforts to support this research.</p>
<h2>REFERENCES</h2>
<p>Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738, 2024.</p>
<p>Mathias Benedek, Tanja Könen, and Aljoscha C Neubauer. Associative abilities underlying creativity. Psychology of Aesthetics, Creativity, and the Arts, 6(3):273, 2012.</p>
<p>Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Wen-tau Yih, and Yejin Choi. Abductive commonsense reasoning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id= Byglv1HKDB.</p>
<p>Daniil A. Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nat., 624(7992):570-578, 2023. doi: 10.1038/S41586-023-06792-0. URL https://doi.org/10.1038/s41586-023-06792-0.</p>
<p>Faxiang Bu, Yuqi Deng, Jie Xu, Dali Yang, Yan Li, Wu Li, and Aiwen Lei. Electrocatalytic reductive deuteration of arenes and heteroarenes. Nature, pp. 1-2, 2024.</p>
<p>Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. Universal self-consistency for large language model generation. CoRR, abs/2311.17311, 2023. doi: 10.48550/ARXIV.2311.17311. URL https://doi.org/10.48550/arXiv.2311.17311.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pp. 3882-3890. ijcai.org, 2020. doi: 10.24963/ijcai.2020/537. URL https://doi.org/10.24963/ijcai.2020/537.</p>
<p>Mario Coccia. Why do nations produce science advances and new technology? Technology in society, 59:101124, 2019.</p>
<p>Joseph F Hair, Arthur H Money, Philip Samouel, and Mike Page. Research methods for business. Education+ Training, 49(4):336-337, 2007.</p>
<p>Arthur Koestler. The act of creation. London: Hutchinson, 1964.
Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, and Asif Ekbal. Can large language models unlock novel scientific research ideas? arXiv preprint arXiv:2409.06185, 2024.</p>
<p>Byung Cheol Lee and Jaeyeon Chung. An empirical investigation of the impact of chatgpt on creativity. Nature Human Behaviour, pp. 1-9, 2024.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and HsuanTien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html.</p>
<p>Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. Mlr-copilot: Autonomous machine learning research based on large language models agents. arXiv preprint arXiv:2408.14033, 2024.</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI scientist: Towards fully automated open-ended scientific discovery. CoRR, abs/2408.06292, 2024. doi: 10. 48550/ARXIV.2408.06292. URL https://doi.org/10.48550/arXiv.2408.06292.</p>
<p>Ziming Luo, Zonglin Yang, Zexin Xu, Wei Yang, and Xinya Du. LLM4SR: A survey on large language models for scientific research. CoRR, abs/2501.04306, 2025. doi: 10.48550/ARXIV. 2501.04306. URL https://doi.org/10.48550/arXiv.2501.04306.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html.</p>
<p>Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, and Bowen Zhou. Large language models are zero shot hypothesis proposers. CoLM, abs/2311.05965, 2024. doi: 10.48550/ARXIV.2311.05965. URL https://doi.org/10.48550/arXiv.2311. 05965 .</p>
<p>Bernardino Romera-Paredes, Mohammadamin Barekatain, Alexander Novikov, Matej Balog, M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, Omar Fawzi, Pushmeet Kohli, and Alhussein Fawzi. Mathematical discoveries from program search with large language models. Nat., 625(7995):468-475, 2024. doi: 10.1038/ S41586-023-06924-6. URL https://doi.org/10.1038/s41586-023-06924-6.</p>
<p>Kaito Shibahara, Yoshihito Kayaki, Kairi Yamashiro, Yuki Nagashima, Kohei Fujii, and Ken Tanaka. Rh-catalysed enantioselective $[2+2+1]$ cycloaddition reactions using three different $2 \pi$-components. Nature Synthesis, pp. 1-13, 2024.</p>
<p>Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109, 2024.</p>
<p>Henry Sprueill, Carl Edwards, Mariefel V. Olarte, Udishnu Sanyal, Heng Ji, and Sutanay Choudhury. Monte carlo thought search: Large language model querying for complex scientific reasoning in catalyst design. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 8348-8365. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-EMNLP. 560. URL https://doi.org/10.18653/v1/2023.findings-emnlp.560.</p>
<p>Henry W. Sprueill, Carl Edwards, Khushbu Agarwal, Mariefel V. Olarte, Udishnu Sanyal, Conrad Johnston, Hongbin Liu, Heng Ji, and Sutanay Choudhury. CHEMREASONER: heuristic search over a large language model's knowledge space using quantum-chemical feedback. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=3tJDnEszco.</p>
<p>Ryuhei Suzuki, Taiga Ando, Fritz Deufel, Kohsuke Ohmatsu, and Takashi Ooi. Photocatalytic carbyne reactivity of phosphorus ylides for three-component formal cycloaddition reactions. Nature Synthesis, pp. 1-7, 2024.</p>
<p>Don R Swanson. Undiscovered public knowledge. The Library Quarterly, 56(2):103-118, 1986.
Trieu H. Trinh, Yuhuai Wu, Quoc V. Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nat., 625(7995):476-482, 2024. doi: 10.1038/S41586-023-06747-5. URL https://doi.org/10.1038/s41586-023-06747-5.</p>
<p>Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin A. Persson, Gerbrand Ceder, and Anubhav Jain. Unsupervised word embeddings capture latent knowledge from materials science literature. Nat., 571(7763):95-98, 2019. doi: 10.1038/ S41586-019-1335-8. URL https://doi.org/10.1038/s41586-019-1335-8.</p>
<p>Jinpei Wang, Yuxin Song, Fanfei Yu, Yijun Zeng, Chenyang Wu, Xuezhi Qin, Liang Peng, Yitan Li, Yongsen Zhou, Ran Tao, et al. Ultrastrong, flexible thermogalvanic armor with a carnot-relative efficiency over 8\%. Nature Communications, 15(1):6704, 2024a.</p>
<p>Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Scimon: Scientific inspiration machines optimized for novelty. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 279-299. Association for Computational Linguistics, 2024b. doi: 10.18653/V1/2024.ACL-LONG.18. URL https://doi.org/10.18653/v1/2024.acl-long. 18.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/ forum?id=1PL1NIMMrw.</p>
<p>Tong Xie, Yuwei Wan, Haoran Wang, Ina Østrøm, Shaozhou Wang, Mingrui He, Rong Deng, Xinyuan Wu, Clara Grazian, Chunyu Kit, and Bram Hoex. Opinion mining by convolutional neural networks for maximizing discoverability of nanomaterials. J. Chem. Inf. Model., 64(7): 2746-2759, 2024. doi: 10.1021/ACS.JCIM.3C00746. URL https://doi.org/10.1021/ acs.jcim.3c00746.</p>
<p>Zonglin Yang, Xinya Du, Alexander M. Rush, and Claire Cardie. Improving event duration prediction via time-aware pre-training. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, volume EMNLP 2020 of Findings of ACL, pp. 3370-3378. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.findings-emnlp.302. URL https: //doi.org/10.18653/v1/2020.findings-emnlp.302.</p>
<p>Zonglin Yang, Xinya Du, Erik Cambria, and Claire Cardie. End-to-end case-based reasoning for commonsense knowledge base completion. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 3509-3522, Dubrovnik, Croatia, May 2023a. Association for Computational Linguistics. URL https://aclanthology. org/2023.eacl-main. 255 .</p>
<p>Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, and Erik Cambria. Logical reasoning over natural language as knowledge representation: A survey. In 1st Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2023), 2023b.</p>
<p>Zonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei. Language models as inductive reasoners. In Yvette Graham and Matthew Purver (eds.), Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 - Volume 1: Long Papers, St. Julian's, Malta, March 17-22, 2024, pp. 209-225. Association for Computational Linguistics, 2024a. URL https://aclanthology.org/2024.eacl-long. 13.</p>
<p>Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik Cambria. Large language models for automated open-domain scientific hypotheses discovery. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pp. 13545-13565. Association for Computational Linguistics, 2024b. doi: 10.18653/V1/2024.FINDINGS-ACL.804. URL https://doi.org/10.18653/v1/2024.findings-acl.804.</p>
<p>Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 7e810b2c75d69be186cadd2fe3febeab-Abstract-Conference.html.</p>
<h1>A APPENDIX</h1>
<h2>A. 1 MOOSE-Chem Overview I/O Figure</h2>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of the input and output of the MOOSE-Chem framework.
Figure 2 presents an overview of the input and output of the MOOSE-Chem framework. In this work, the inspiration corpus is initialized using the titles and abstracts of numerous papers, which together constitute the search space.</p>
<h2>A. 2 Proof of the Rigorous Decomposition of $P(h \mid b)$ BASED ON the Fundamental Assumption</h2>
<p>We propose an assumption that a majority of chemistry hypotheses can originate from a research background and several inspirations. This assumption is not only supported by many chemistry researchers whom we have extensive discussions with but also by the cognitive science finding that "creative ideas often result from the cohesive association of two (or more) seemingly unrelated pieces of knowledge" (Koestler, 1964; Benedek et al., 2012; Lee \&amp; Chung, 2024). We design our method based on this fundamental assumption.</p>
<p>This assumption is reminiscent of Swanson Linking (Swanson, 1986) in the domain of literaturebased discovery (LBD), also known as the "ABC model", where two concepts A and C are hypothesized as linked if they both co-occur with some intermediate concept B in papers. Our assumption differs in that: (1) for a chemistry hypothesis published in a good venue, usually more than one inspiration is needed; (2) background and inspiration are not necessarily linked by a path of intermediate papers; (3) our assumption is applied to a majority of existing published chemistry hypotheses, while LBD has been considered to only focus on a very specific, narrow type of hypothesis (Wang et al., 2024b). It might indicate that a similar proportion of future chemistry hypotheses can also result from linkages of existing literature.</p>
<p>Denoting background knowledge as $b$, inspiration knowledge as $i$, and hypothesis as $h$, we translate this assumption as:</p>
<p>$$
h=f\left(b, i_{1}, \ldots, i_{k}\right)
$$</p>
<p>Here, $k \in \mathbb{Z}$ represents the number of inspirations needed for a particular $h$. Typically in chemistry, $k \in[1,3]$. We further assume each hypothesis $h$ has a unique minimal set of inspirations $\left{i_{1}, \ldots, i_{k}\right}$ that, combined with the background $b$, determines its formation. We refer to this as the uniqueness assumption.</p>
<p>Equation 4 expresses the idea that, for the majority of chemistry hypotheses (if not all), each hypothesis can be formulated as a composition of background knowledge and additional knowledge elements, which we refer to as inspirations. This functional form reflects a universal pattern in hypothesis formulation: regardless of where the inspiration originates-be it prior literature, serendipitous observation, or discussions with peers-the essential step is to identify correct inspirations and integrate them with existing background knowledge in a meaningful way. In this formulation, the process of first collecting and selecting the appropriate background knowledge and then identifying and integrating suitable inspirations constitutes both a necessary and sufficient condition for generating a valid hypothesis $h$.</p>
<p>Here's an example in chemistry:</p>
<ul>
<li>Research Question: How to obtain $D_{2}$ gas more efficiently?</li>
<li>Background Knowledge: The best performing methods are electrocatalytic methods.</li>
<li>Inspiration Knowledge 1: Ruthenium as catalyst</li>
<li>Inspiration Knowledge 2: Nitrogen-doped electrode</li>
<li>Inspiration Knowledge 3: $D_{2} O$ as chemical solution</li>
<li>Hypothesis: A nitrogen-doped ruthenium (Ru) electrode can effectively catalyze the reductive deuteration of (hetero)arenes in the presence of $D_{2} O$ in an electrocatalytic method, leading to efficient $D_{2}$ gas production.</li>
</ul>
<p>Here's an example in AI:</p>
<ul>
<li>Research Question: How can we automatically update the parameters of a multi-layer logistic regression model using data?</li>
<li>Background Knowledge: Multi-layer logistic regression</li>
<li>Inspiration Knowledge: The chain rule from calculus</li>
<li>Hypothesis: Backpropagation</li>
</ul>
<p>Here's another example in AI:</p>
<ul>
<li>Research Question: How can we improve reasoning performance in language models?</li>
<li>Background Knowledge: Chain-of-Thought prompting</li>
<li>Inspiration Knowledge: Majority voting over multiple reasoning paths</li>
<li>Hypothesis: Self-consistency decoding</li>
</ul>
<p>Here, "background knowledge" and "inspiration knowledge" as illustrated in the example above, can be understood as specific, well-defined knowledge pieces. In practice, however, knowledge retrieval is rarely so clean. Instead of isolating a single knowledge unit, we often retrieve a noisy cluster of information that contains the desired piece along with extraneous content. For instance, when retrieving a paper that includes a relevant inspiration, the paper will inevitably also contain unrelated information that may not be useful for the current research question. Conversely, a single clean inspiration $i$ may be embedded across multiple papers in the literature. This redundancy is beneficial-it increases the likelihood of retrieving $i$ even when searching imperfectly.
In other words, given existing knowledge in the background, a majority of chemistry research is about searching knowledge that previously not known to be related to the background but in fact can assist the background, then associate the background knowledge and the searched knowledge in a reasonable way to compose a hypothesis. Crucially, the inspiration should not be previously known to be related to the background-at least not in a way that has already been used to formulate hypotheses. Otherwise, the resulting hypothesis would lack novelty. This requirement positions the inspiration retrieval task as an inherently out-of-distribution (OOD) problem, where the goal is to surface connections that lie outside established knowledge associations.
Our goal is to transform the seemingly impossible-to-solve $P(h \mid b)$ into an equivalent form, where each step in the equivalent form is practical and executable. Denoting the full inspiration knowledge space as $I$, such that $P(I)=1$. Then a straightforward way of decomposing $P(h \mid b)$ is by the chain rule based on Equation 4:</p>
<p>$$
\begin{aligned}
P(h \mid b) &amp; =\sum_{i_{1}, \ldots, i_{k}} P\left(h, i_{1}, \ldots, i_{k} \mid b\right) \
&amp; =\sum_{\pi \in \Pi_{k}} P\left(h, i_{\pi(1)}, \ldots, i_{\pi(k)} \mid b\right) \
&amp; =P\left(h, i_{1}, \ldots, i_{k} \mid b\right) \
&amp; = \begin{cases}P\left(h \mid b, i_{1}\right) \cdot P\left(i_{1} \mid b, I\right) &amp; \text { if } k=1 \
P\left(h \mid b, i_{1}, \ldots, i_{k}\right) \cdot \prod_{j=2}^{k} P\left(i_{j} \mid b, i_{1}, \ldots, i_{j-1}, I\right) \cdot P\left(i_{1} \mid b, I\right) &amp; \text { if } k&gt;1\end{cases}
\end{aligned}
$$</p>
<p>Equation 5 expands $P(h \mid b)$ by marginalizing over all possible inspiration sequences $\left(i_{1}, \ldots, i_{k}\right)$ that could, together with the background $b$, yield $h$. Equation 6 applies the uniqueness assumption—each hypothesis $h$ corresponds to a unique minimal set of inspirations $\left{i_{1}, \ldots, i_{k}\right}$, leaving only the order of incorporation as a source of variability. Let $\Pi_{k}$ denote all permutations of ${1, \ldots, k}$; thus, the marginalization becomes a sum over $\pi \in \Pi_{k}$. Equation 7 follows from the fixed-order assumption (introduced here to simplify the equation), which posits a canonical constructive order of inspirations (i.e., $\left|\Pi_{k}\right|=1$ ), collapsing the sum to a single term. Finally, Equation 8 expands this joint distribution using the chain rule.
$I$ denotes the full inspiration space-that is, the set of all possible knowledge pieces that could serve as an inspiration for generating a new hypothesis. This space includes not only all existing chemistry knowledge but also potentially relevant knowledge from other disciplines that could be leveraged in formulating novel chemistry hypotheses. However, computing over the full space $I$ is computationally infeasible. To make the problem tractable, we approximate $I$ with a large but manageable subset $\hat{I}$, consisting of approximately 3,000 top cited chemistry papers from the existing chemistry literature.</p>
<p>Equation 8 describes the process of $P(h \mid b)$ from a knowledge-searching perspective. However, the terms $P\left(h \mid b, i_{1}, \ldots, i_{k}\right)$ and $P\left(i_{j} \mid b, i_{1}, \ldots, i_{j-1}, I\right)$ may not fully capture how chemistry researchers actually discover new inspirations in practice. One key reason is that researchers typically reason in an incremental fashion, composing hypotheses by integrating one or two knowledge components at a time. It is cognitively and practically difficult to evaluate or integrate all candidate inspirations simultaneously. Instead, researchers iteratively assess partial combinations-gradually building toward a complete hypothesis.</p>
<p>To mimic how chemistry researchers conduct research and make it more practicable, we break $P(h \mid$ $\left.b, i_{1}, \ldots, i_{k}\right)$ into a series of recursive smaller steps as</p>
<p>$$
\begin{aligned}
P\left(h_{k} \mid b, i_{1}, \ldots, i_{k}\right) &amp; \approx P\left(h_{k} \mid b, f\left(b, i_{1}, \ldots, i_{k-1}\right), i_{k}\right) &amp; &amp; \text { if } k&gt;1 \
&amp; =P\left(h_{k} \mid b, h_{k-1}, i_{k}\right) &amp; &amp; \text { if } k&gt;1
\end{aligned}
$$</p>
<p>Similarly, we can break $P\left(i_{j+1} \mid b, i_{1}, \ldots, i_{j}, I\right)$ as</p>
<p>$$
\begin{aligned}
P\left(i_{k+1} \mid b, i_{1}, \ldots, i_{k}, I\right) &amp; \approx P\left(i_{k+1} \mid b, f\left(b, i_{1}, \ldots, i_{k}\right), I\right) &amp; &amp; \text { if } k&gt;1 \
&amp; =P\left(i_{k+1} \mid b, h_{k}, I\right) &amp; &amp; \text { if } k&gt;1
\end{aligned}
$$</p>
<p>As a result, to achieve the final $h_{k}$, we need to obtain $\left{h_{1}, \ldots, h_{k-1}\right}$ first (if $k&gt;1$ ). In addition, viewing $h$ as a "state" and $i$ as an "action", obtaining $h$ and $i$ through $P\left(h_{k} \mid b, h_{k-1}, i_{k}\right)$ and $P\left(i_{k+1} \mid b, h_{k}, I\right)$ correspondingly indicates a Markov property: (1) a new $h$ depends only on $b$, its previous $h$, and the current $i$; and (2) an $i$ depends only on $b, I$, and the previous $h$.</p>
<p>Markov Decision Process (MDP) Formulation. Building upon this Markov property, we now interpret the sequential formation of hypotheses as a decision process. We therefore formalize this process as a Markov Decision Process (MDP), where:</p>
<ul>
<li>
<p>the state is $\left(b, h_{j-1}\right)$, encoding the research background and the current intermediate hypothesis;</p>
</li>
<li>
<p>the action is the selected inspiration $i_{j}$;</p>
</li>
<li>the transition function is $P\left(h_{j} \mid b, h_{j-1}, i_{j}\right)$, yielding the next hypothesis state; and</li>
<li>the reward can be defined as the quality or plausibility of the resulting hypothesis $h_{j}$ (e.g., as judged by an LLM or domain expert).</li>
</ul>
<p>$$
b \xrightarrow{i_{1}} h_{1} \xrightarrow{i_{2}} h_{2} \xrightarrow{\cdots} h_{k-1} \xrightarrow{i_{k}} h_{k}=h
$$</p>
<p>Each transition remains conditioned on the background knowledge $b$, though $b$ is omitted from the notation to emphasize the Markov structure of the progression. This MDP view implies that hypothesis discovery can be modeled as a sequential policy choosing inspirations and applying transition dynamics to update the intermediate hypothesis state. We next make this view explicit in the probability space.
Building on this formulation, we interpret the formation of a hypothesis $h$ (specifically, $h=h_{k}$ ) as a constructive process that sequentially integrates a set of inspirations $\left{i_{1}, \ldots, i_{k}\right}$ into intermediate hypothesis states $\left{h_{1}, \ldots, h_{k}\right}$.
Formally, the conditional probability $P(h \mid b)$ can be expressed as a marginal over all valid sequences of inspirations that can generate $h$ (supported by assumption in Equation 4 and uniqueness assumption):</p>
<p>$$
P(h \mid b)=\sum_{\pi \in \Pi_{k}} P\left(i_{\pi(1)}, \ldots, i_{\pi(k)}, h_{1}, \ldots, h_{k} \mid b\right)
$$</p>
<p>where $\Pi_{k}$ denotes the set of all permutations of ${1, \ldots, k}$ applied to the inspirations $\left{i_{1}, \ldots, i_{k}\right}$ such that the resulting composition yields the final hypothesis $h_{k}=h$, under the assumptions of Equation 4 and the uniqueness assumption, which specify that $h$ is fully determined by $b$ and $\left{i_{1}, \ldots, i_{k}\right}$. In this context, $h_{j}$ represents the intermediate hypothesis state obtained after integrating $i_{\pi(j)}$ at step $j$.
The degree to which the order of inspirations $\left{i_{1}, \ldots, i_{k}\right}$ affects hypothesis formulation can vary across disciplines. In empirical sciences such as chemistry, the contributions of individual inspirations are largely interchangeable, and their order of integration has limited impact on the final hypothesis, resulting in a large $\left|\Pi_{k}\right|$. Conversely, in disciplines such as mathematics, where hypotheses (e.g., theorems) often require constructing a specific sequence of lemmas and prior results, the ordering of inspirations is more constrained and may follow a near-deterministic path.
For simplicity of exposition, we adopt the fixed-order assumption introduced earlier-i.e., $\left|\Pi_{k}\right|=$ 1 -which selects a canonical constructive order of inspirations $\left{i_{1}, \ldots, i_{k}\right}$ for analysis. Under this assumption, the hypothesis $h$ is constructed through that specific sequence, giving:</p>
<p>$$
P(h \mid b)=P\left(i_{1}, \ldots, i_{k}, h_{1}, \ldots, h_{k} \mid b\right)
$$</p>
<p>with $h_{j}$ denoting the intermediate hypothesis state after incorporating $i_{j}$, and $h_{k}=h$.
Therefore, under the uniqueness assumption and fixed-order assumption, for $k&gt;1$,</p>
<p>$$
\begin{aligned}
P(h \mid b) &amp; =P\left(i_{1}, \ldots, i_{k}, h_{1}, \ldots, h_{k} \mid b\right) \
&amp; =P\left(i_{1}, h_{1} \mid b\right) \cdot P\left(i_{2}, h_{2} \mid b, i_{1}, h_{1}\right) \cdot \ldots \cdot P\left(i_{k}, h_{k} \mid b, i_{1}, \ldots, i_{k-1}, h_{1}, \ldots, h_{k-1}\right) \
&amp; \approx P\left(i_{1}, h_{1} \mid b\right) \cdot P\left(i_{2}, h_{2} \mid b, h_{1}\right) \cdot \ldots \cdot P\left(i_{k}, h_{k} \mid b, h_{k-1}\right) \
&amp; =\prod_{j=1}^{k} P\left(i_{j} \mid b, h_{j-1}, I\right) \cdot P\left(h_{j} \mid b, h_{j-1}, i_{j}\right), \text { where } h_{0}=\emptyset
\end{aligned}
$$</p>
<p>Equation 15 follows from the assumption in Equation 4, together with the uniqueness and fixedorder assumptions, which specify that $h$ is determined by $b$ and a unique ordered set of inspirations $\left{i_{1}, \ldots, i_{k}\right}$. Equation 16 applies the chain rule to factorize the joint distribution. Equation 17 follows from the Markov property, assuming that the next $\left(i_{j}, h_{j}\right)$ pair depends only on $b$ and the preceding state $h_{j-1}$. Finally, Equation 18 re-applies the chain rule, with $P(I)=1$ by definition.
Although starting from $k&gt;1$, Derivation 18 covers the situation when $k=1$ in Equation 8. Therefore, in sum, we successfully break up the seemingly impossible question $P(h \mid b)$ into many practical and executable smaller questions as:</p>
<p>$$
P(h \mid b) \approx \prod_{j=1}^{k} P\left(i_{j} \mid b, h_{j-1}, I\right) \cdot P\left(h_{j} \mid b, h_{j-1}, i_{j}\right), \text { where } h_{0}=\emptyset \text { and } k \geq 1
$$</p>
<p>Equation 19 provides the MDP-based decomposition of $P(h \mid b)$, where each term $P\left(i_{j}\right.$ $\left.b, h_{j-1}, I\right)$ and $P\left(h_{j} \mid b, h_{j-1}, i_{j}\right)$ corresponds respectively to the policy (action selection) and state transition components of the underlying Markov Decision Process. This formulation highlights that scientific hypothesis discovery can be viewed as a sequential decision-making problem, where an agent iteratively selects inspirations to maximize the expected quality (usually compounded evaluation of validity and novelty) of the final hypothesis $h_{k}$.
Of course, without the fixed-order assumption, a more complete derivation of $P(h \mid b)$ involves marginalizing over all valid permutations in $\Pi_{k}$ :</p>
<p>$$
\begin{aligned}
P(h \mid b) &amp; =\sum_{\pi \in \Pi_{k}} P\left(i_{\pi(1)}, \ldots, i_{\pi(k)}, h_{1}, \ldots, h_{k} \mid b\right) \
&amp; \approx \sum_{\pi \in \Pi_{k}} \prod_{j=1}^{k} P\left(i_{\pi(j)} \mid b, h_{j-1}^{(\pi)}, I\right) \cdot P\left(h_{j}^{(\pi)} \mid b, h_{j-1}^{(\pi)}, i_{\pi(j)}\right)
\end{aligned}
$$</p>
<p>where $h_{0}^{(\pi)}=\emptyset, h_{k}^{(\pi)}=h$, and $k \geq 1$. Here, $h_{j}^{(\pi)}$ denotes the intermediate hypothesis state at step $j$ in the permutation $\pi$, which results from sequentially incorporating inspirations in the order $\left{i_{\pi(1)}, \ldots, i_{\pi(j)}\right}$.</p>
<h1>A. 3 The Full Instruction for Benchmark Checking</h1>
<p>Please help us check again before finalizing the decomposition of each paper in the benchmark:</p>
<ol>
<li>Whether the background question is correct.</li>
<li>Background survey shouldn't contain any information/method in inspiration or hypothesis (except if this information/method has been used for this particular background question before). It is encouraged to include the most similar existing method to the proposed method. For example, the proposal is to change BaCl 2 to BaSO 4 . It is encouraged to include BaCl 2 in the survey, but SO 4 must not be included in the survey (since SO4 belongs to the inspiration).</li>
<li>Background question cannot contain any information in inspiration or hypothesis as well: It should be a little bit general question, instead of a specific question asking about how the inspiration can be leveraged to help with the question. It also shouldn't be too general that we can't understand which specific research domain it works on.</li>
<li>Whether the identification of inspirations really the main inspirations for this paper, and whether we need more main inspiration(s).</li>
<li>Whether the main hypothesis is correct and covers the main key points.</li>
<li>Whether the background survey + background question + identified inspirations can logically lead to the hypothesis (if not, we might need to identify more inspirations).
Thank you for the efforts! Your contribution is indispensable for the success of this research. Please let me know if you have any questions.</li>
</ol>
<h1>A. 4 Prompt to obtain $R(h)$</h1>
<p>You are known as a diligent and harsh reviewer in Chemistry and Material Science that will spend much time to find flaws when reviewing and therefore usually gives a relatively much lower score than other reviewers. But when you meet with a hypothesis you truly appreciate, you don't mind to give it good scores. Given a not yet peer reviewed research hypothesis in Chemistry or Material Science domain, try to evaluate the research hypothesis from four research aspects and give score according to evaluation guidelines provided below. All four aspects should be evaluated in a 5 point scale.</p>
<h2>Aspect 1: Validness.</h2>
<p>5 points: The hypothesis is a logical next step from current research, strongly supported by theory, perhaps with some indirect experimental evidence or highly predictive computational results. The experimental verification seems straightforward with a high probability of confirming the hypothesis; 4 points: Here, the hypothesis is well-rooted in existing theory with some preliminary data or computational models supporting it. It extends known science into new but logically consistent areas, where experiments are feasible with current technology, and there's a reasonable expectation of positive results; 3 points: This hypothesis is within the realm of theoretical possibility but stretches the boundaries of what's known. It might combine existing knowledge in very novel ways or predict outcomes for which there's no direct evidence yet. There's a conceptual framework for testing, but success is uncertain; 2 points: While the hypothesis might be grounded in some theoretical aspects, it significantly deviates from current understanding or requires conditions or materials that are currently impossible or highly improbable to achieve or synthesize; 1 point: The hypothesis proposes concepts or outcomes that are not only unsupported by current theory but also contradict well-established principles or data. There's no clear path to experimental testing due to fundamental theoretical or practical barriers.</p>
<h2>Aspect 2: Novelty.</h2>
<p>5 points: This level of novelty could fundamentally alter our understanding of chemistry or create entirely new fields. It often involves predictions or discoveries that, if proven, would require a significant overhaul of existing chemical theories; 4 points: The hypothesis significantly departs from established norms, potentially redefining how certain chemical phenomena are understood or applied. It might involve entirely new materials or theoretical frameworks; 3 points: This level involves a hypothesis that could potentially lead to new insights or applications. It might challenge minor aspects of current theories or introduce new methodologies or materials; 2 points: The hypothesis introduces a new angle or method within an established framework. It might involve known compounds or reactions but in contexts or combinations not previously explored; 1 point: The hypothesis involves minor tweaks or applications of well-known principles or techniques. It might slightly extend existing knowledge but doesn't introduce fundamentally new concepts.</p>
<h2>Aspect 3: Significance.</h2>
<p>5 points: This hypothesis could fundamentally change one or more branches of chemistry. It might introduce entirely new principles, theories, or methodologies that redefine the boundaries of chemical science; 4 points: This hypothesis challenges current understanding or introduces a concept that could lead to substantial changes in how a particular area of chemistry is viewed or applied. It might lead to new technologies or significant theoretical advancements; 3 points: this hypothesis proposes something new or an innovative approach that could lead to noticeable advancements in a specific area of chemistry. It might open new avenues for research or application but doesn't revolutionize the field; 2 points: This hypothesis might offer a small variation or incremental improvement on existing knowledge. It could potentially refine a known concept but doesn't significantly alter the field; 1 point: The hypothesis addresses a very narrow or already well-established aspect of chemistry. It might confirm what is already known without adding much new insight.</p>
<h2>Aspect 4: Potential.</h2>
<p>5 points: The hypothesis, while potentially intriguing now, holds the promise of being revolutionary with the addition of a key methodological component. This could introduce entirely new concepts</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">5</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">3</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/ background survey</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Average MS (GPT-40)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Average MS (Claude-3.5-Sonnet)</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Average MS (Gemini-1.5-Pro)</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Top MS (GPT-40)</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Top MS (Claude-3.5-Sonnet)</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Top MS (Gemini-1.5-Pro)</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Top MS (Experts)</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">w/o background survey</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Average MS (GPT-40)</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Average MS (Claude-3.5-Sonnet)</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Average MS (Gemini-1.5-Pro)</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Top MS (GPT-40)</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Top MS (Claude-3.5-Sonnet)</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">51</td>
</tr>
<tr>
<td style="text-align: center;">Top MS (Gemini-1.5-Pro)</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">51</td>
</tr>
</tbody>
</table>
<p>Table 12: Main table for Q2. Average/Top MS means the average/highest Matched Score of all generated $h$ from one $b$. The numbers represent the statistics of Average/Top MS over the benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Top MS</th>
<th style="text-align: center;">Average MS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SciMON (Wang et al., 2024b)</td>
<td style="text-align: center;">3.824</td>
<td style="text-align: center;">3.529</td>
</tr>
<tr>
<td style="text-align: left;">MOOSE (Yang et al., 2024a)</td>
<td style="text-align: center;">3.902</td>
<td style="text-align: center;">3.559</td>
</tr>
<tr>
<td style="text-align: left;">Qi et al. (2024)</td>
<td style="text-align: center;">3.431</td>
<td style="text-align: center;">3.092</td>
</tr>
<tr>
<td style="text-align: left;">MOOSE-Chem</td>
<td style="text-align: center;">$\mathbf{4 . 4 7 1}$</td>
<td style="text-align: center;">3.697</td>
</tr>
<tr>
<td style="text-align: left;">w/o multi-step</td>
<td style="text-align: center;">4.216</td>
<td style="text-align: center;">3.592</td>
</tr>
<tr>
<td style="text-align: left;">w/o multi-step \&amp; EU</td>
<td style="text-align: center;">3.941</td>
<td style="text-align: center;">3.614</td>
</tr>
</tbody>
</table>
<p>Table 13: Experiments and ablation study. The Matched Score is evaluated by Claude-3.5-Sonnet.
or fields, fundamentally changing our understanding or capabilities in chemistry; 4 points: The hypothesis, though promising, could be transformative with the right methodological enhancement. This enhancement might lead to groundbreaking discoveries or applications, significantly advancing the field; 3 points: The hypothesis, while interesting in its current form, could be significantly elevated with the right methodological addition. This might lead to new insights or applications that go beyond the initial scope; 2 points: The hypothesis currently offers some value but has the potential for more substantial contributions if enhanced with a new methodological approach. This could lead to incremental advancements in understanding or application; 1 point: The hypothesis, as it stands, might be straightforward or well-trodden. Even with methodological enhancements, it's unlikely to significantly expand current knowledge or applications beyond minor improvements.</p>
<p>The hypothesis is:
Please give a response to the initial question on scoring the hypothesis from four aspects. Remember that you are a diligent and harsh reviewer.</p>
<h1>A. 5 Automatic Evaluation by Claude and Gemini</h1>
<p>To investigate whether the results and corresponding conclusions in the main text are caused by the usage of GPT-40 for automatic evaluation, here we use Claude-3.5-Sonnet and Gemini-1.5-Pro to evaluate all of the results that have been evaluated by GPT-40.
Table 12 covers the contents in Table 7, but with more results on using Claude-3.5-Sonnet and Gemini-1.5-Pro for automatic evaluation. When using different LLMs for automatic evaluation, the instruction is the same (can be found in § A.12). The robust results indicate again that LLMs are quite capable of associating known knowledge into unknown knowledge that has a high probability to be valid (very close to $o h$ ).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ All code and data can be found in https://github.com/ZonglinY/MOOSE-Chem
${ }^{2}$ Contribution during internship at Shanghai Artificial Intelligence Laboratory. ${ }^{1}$ Corresponding author.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
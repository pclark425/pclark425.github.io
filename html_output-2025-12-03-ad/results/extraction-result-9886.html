<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9886 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9886</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9886</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-168.html">extraction-schema-168</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <p><strong>Paper ID:</strong> paper-6670546864bd235ea13669dae51c7dfe65944439</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6670546864bd235ea13669dae51c7dfe65944439" target="_blank">BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Surprisingly, it is discovered that state-of-the-art agents, both daily scenarios and biomedical ones, have either failed or inferior performance on the authors' benchmark.</p>
                <p><strong>Paper Abstract:</strong> Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models (LLMs). However, to evaluate such systems, people either rely on direct Question-Answering (QA) to the LLM itself, or in a biomedical experimental manner. How to precisely benchmark biomedical agents from an AI Scientist perspective remains largely unexplored. To this end, we draw inspiration from one most important abilities of scientists, understanding the literature, and introduce BioKGBench. In contrast to traditional evaluation benchmark that only focuses on factual QA, where the LLMs are known to have hallucination issues, we first disentangle"Understanding Literature"into two atomic abilities, i)"Understanding"the unstructured text from research papers by performing scientific claim verification, and ii) Ability to interact with structured Knowledge-Graph Question-Answering (KGQA) as a form of"Literature"grounding. We then formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based Retrieval-Augmented Generation (RAG) to identify the factual errors of existing large-scale knowledge graph databases. We collect over two thousand data for two atomic tasks and 225 high-quality annotated data for the agent task. Surprisingly, we discover that state-of-the-art agents, both daily scenarios and biomedical ones, have either failed or inferior performance on our benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent. On the widely used popular knowledge graph, we discover over 90 factual errors which provide scenarios for agents to make discoveries and demonstrate the effectiveness of our approach. The code and data are available at https://github.com/westlake-autolab/BioKGBench.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9886.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9886.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioKGBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A benchmark introduced in this paper to evaluate biomedical AI scientist agents on literature understanding and knowledge-graph grounding via three tasks (KGQA, SCV, KGCheck), with supplied KG subset, literature corpus, and annotated evaluation sets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4; Llama-3-70B-Instruct (representative evaluated models)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4: proprietary API model by OpenAI; Llama-3-70B-Instruct: large open-source instruction-tuned model (70B parameters) evaluated as the top OSS model in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical science (computational biology, clinical knowledge graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Three-stage evaluation combining structured KG queries (KGQA), retrieval-augmented verification from literature (SCV), and an integrated agent task (KGCheck) where agents query KG then validate with RAG and external DBs; comparisons to gold answers/annotations and multi-agent process logs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Quantitative metrics per task: KGQA: F1, Exact Match (EM), Executability; SCV: Accuracy, 'Right Quotes' (alignment of retrieved evidence), Quote Error rate; KGCheck: Support/Refute label accuracy, Exact Match on final decision, process metrics (Tool Selection, Executability).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>BioKGBench dataset: KGQA (698 Qs from a CKG subgraph), SCV (1,385 claims reconstructed from PubMedQA and SciFact with 5,664-abstract corpus), KGCheck (225 human-annotated KG checking instances, 51 full papers for evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>State-of-the-art API models (GPT-4) and top OSS model (Llama-3-70B-Instruct) perform best but still show gaps; benchmark revealed many failures and hallucinations across models. The benchmark enabled discovery of >90 factual errors in the Clinical Knowledge Graph and 96 'Refute' annotations within the KGCheck set.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Binary labels in KGCheck create a high chance baseline (~50%); static KGs vs. dynamic external sources complicate ground truth; LLMs sometimes answer correctly without evidence (low 'right quote' alignment), and process-level failures (leader agent errors) can dominate final outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>KGCheck simulates human scientific workflow (query KG, retrieve literature, weigh evidence). The paper notes that process-level evaluation (how agents search and validate) better mirrors human scientific practice than simple QA accuracy alone.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use decomposition into atomic tasks (KGQA and SCV) plus an integrated KGCheck; report process metrics (tool selection, executability) in addition to outcome metrics; include a 'right quote' evidence alignment metric; use RAG with curated corpora and authoritative databases (UniProt, STRING, PubMed) to ground claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9886.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9886.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Question Answering (KGQA) atomic task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An atomic evaluation task that measures an agent's ability to query a biomedical knowledge graph (CKG subgraph) to answer natural-language questions; questions span one-hop, multi-hop, and conjunction reasoning types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Various (GPT-4, GLM-4, Llama-3-70B-Instruct, Qwen1.5-72B etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Models evaluated ranged from commercial APIs (GPT-4, GLM-4) to multiple open-source models of different sizes (e.g., Llama-3-70B-Instruct, Qwen variants).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical (protein, pathway, disease relations within Clinical Knowledge Graph)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Agents are given KG-querying tools to retrieve entity sets from the KG; LLM outputs are compared to gold sets derived from the KG subgraph.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>F1 score (entity set overlap), Exact Match (EM) for full-set correctness, and 'Executability' measuring whether agents correctly invoked tools/queries.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>KGQA dataset from BioKGBench: 698 generated questions (dev 60, test 638) based on a CKG subgraph of 12 node types and 18 relations.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Top performers: GPT-4 (F1 81.8, EM 79.2, Executability 88.4) and Llama-3-70B-Instruct (F1 80.7, EM 77.8, Executability 97.0); other models show wide variance tied to size, training, and specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Biomedical KG density increases reasoning complexity; model parameter count does not strictly predict KGQA performance; some models struggle with tool-call formatting ('executability').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>KGQA provides a structured grounding alternative to free-form QA; closer to database-query tasks humans perform when verifying facts from KGs.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Provide atomic KG-query tools, evaluate executability/process as well as answer correctness, include diverse reasoning types (one-hop, multi-hop, conjunction).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9886.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9886.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SCV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Claim Verification (SCV) atomic task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented evaluation where claims are judged as 'Support', 'Refute', or 'NEI' by finding and citing evidence from scientific abstracts; dataset reconstructed from PubMedQA and SciFact.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 and multiple OSS models (Llama-3-70B-Instruct, Qwen variants, GLM-4, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Same set of evaluated models as other tasks; includes large proprietary and open-source models with varying domain exposure.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature (claims about clinical/biological facts derived from abstracts)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>RAG: retrieve abstracts from a corpus (5,664 abstracts) and then classify claim as Support/Refute/NEI; also check whether retrieved quotes align with gold evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Accuracy of Support/Refute/NEI classification; 'Right Quotes' metric measuring alignment of model-provided evidence quotes with ground truth; Quote Error rate.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>SCV dataset: 1,385 claims reconstructed from PubMedQA and SciFact, with retrieval corpus of 5,664 abstracts (dev 120, test 1,265).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-4 achieved Accuracy 83.9% and Right Quotes 87.7%; GLM-4 and some Qwen models show higher Accuracy than Right Quotes, indicating some models can classify claims correctly without producing correct supporting quotes (internalized biomedical knowledge). Performance varies widely across OSS models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Models sometimes correctly classify claims without retrieving correct evidence ('right quote' mismatch), showing possible memorization or hallucination; RAG scope affects metrics in non-intuitive ways (see ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Mirrors human literature verification (find evidence then judge), but models may bypass evidence step, unlike rigorous human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report evidence-alignment metrics (right quote) along with classification accuracy; evaluate RAG retrieval scope effects; prefer curated corpora over generic web search for biomedical claims.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9886.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9886.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KGCheck</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Checking (KGCheck) agent task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An integrated agent-level task where agents query a KG to extract nodes/triples and then validate those facts against external databases and literature (via RAG) to label KG items as Support or Refute.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 and Llama-3-70B-Instruct as agent backbones (through BKGAgent)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>GPT-4: API-based high-performing model; Llama-3-70B-Instruct: large OSS model used to instantiate multi-agent baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical knowledge graph curation and verification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Multi-agent workflow: leader decomposes task, KG agent queries KG, validation agent checks evidence from web DBs (UniProt, STRING) and publications; final decision compared to human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Support/Refute ground-truth accuracy (Exact Match), process metrics including Tool Selection correctness and Executability for both query and validation stages, and per-check-type analysis (node existence, attribute, existing relationship, potential relationship).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>KGCheck set: 225 high-quality annotated KG checking instances (dev 20, test 205), categorized into four check types and annotated Support/Refute labels; 51 full papers provided as literature evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Agents discovered real KG inconsistenciesâ€”96 Refute annotations found in the KG. In process/outcome metrics (Table 6), GPT-4-based BKGAgent achieved higher final exact-match accuracy than Llama-based for final KGCheck outcomes (examples: Web DB KGCheck final exact match: GPT-4 64.5% with high executability; Llama had higher tool-executability but lower final exact match), showing process-success doesn't guarantee correct final decision.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Binary Support/Refute labels inflate chance-level performance; team-leader errors (planning, repetitive self-talk) can derail correct assistant outputs; reliance on external DB currency is critical since KGs may be outdated.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>KGCheck imitates human curation workflows (query KG -> consult databases/literature -> adjudicate), enabling agent evaluation closer to real scientific verification than standalone QA.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Measure both process and outcome metrics (tool selection, executability, evidence alignment) and use more granular metrics than exact match; include authoritative DB cross-checks and curated full-text evidence where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9886.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9886.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BKGAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BKGAgent: Biomedical Knowledge-Graph Agent (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent baseline (leader, KG agent, validation agent, tool executor) implemented to interact with KGs and literature for KGCheck, coordinating tool use and evidence validation replicating human team workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4 and Llama-3-70B-Instruct (both used to instantiate leader/assistants in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Instances using either GPT-4 (API) or Llama-3-70B-Instruct (open-source 70B) as the LLMs for leader and assistant agents; tool executor is non-LLM code.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical KG verification and literature-based validation</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Run multi-agent workflows on KGCheck items; log per-agent tool selection/executability and the final leader decision; validate outputs against human labels and external DBs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Process metrics: Tool Selection correctness, Executability for KG query and validation stages; Outcome metrics: Exact Match of final decision, final Executability. Separately reported for Web DB KGCheck and Publication DB KGCheck.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Evaluated on the KGCheck 225-instance set from BioKGBench, using UniProt and STRING as web DBs and a curated set of 51 publications for publication checks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>BKGAgent showed that high assistant executability doesn't guarantee correct final outcomes; GPT-4-based agent had higher final exact match (e.g., Web DB KGCheck exact match ~64.5%) while Llama-based agent had higher tool activation/executability but lower final exact match (e.g., ~38.1% in Web DB KGCheck), due to leader decision errors and coordination failures.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Leader agent planning/coordination is a common failure mode (misinstructions, looping, taking assistant actions). Even with correct assistant retrievals, team chatter or failure to finalize results produces wrong answers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Framework is explicitly inspired by human team workflows (leader + assistants) and exposes process-level failures analogous to human coordination errors.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Monitor and evaluate per-agent process metrics; design leader prompts/feedback mechanisms carefully to avoid planning faults; separate tool execution from LLM to improve reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9886.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9886.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Right Quotes metric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Right Quotes (evidence-alignment) metric for SCV</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric introduced/used in this work measuring whether the model's retrieved quotation or cited evidence matches the ground-truth evidence supporting the claim verdict.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>GPT-4; GLM-4; Qwen models (as evaluated examples)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Various LLMs evaluated; the metric exposes discrepancies between verdict accuracy and evidence alignment across models.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical literature claim verification</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compare model-provided evidence quotes or citations to ground-truth evidence passages; compute percentage alignment (Right Quotes) and quote error rate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Right Quotes proportion (how often retrieved quotes match gold evidence) alongside Accuracy on claim labels.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Applied to SCV dataset (1,385 claims from PubMedQA and SciFact) with retrieval corpus of 5,664 abstracts.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Some models (GLM-4, Qwen variants) show higher classification accuracy than Right Quotes alignment, indicating classification can succeed without retrieving correct supporting text (i.e., internalized knowledge or hallucination).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>A high classification accuracy with low Right Quotes suggests models may not actually ground judgments in retrievable evidence; Right Quotes requires gold evidence passages for comparison (not always available).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Humans typically produce judgments grounded in cited evidence; Right Quotes attempts to quantify this alignment for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Report evidence-alignment metrics alongside verdict accuracy; treat Right Quotes as necessary for trust in scientific claim verification systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9886.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9886.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, criteria, benchmarks, and results for evaluating LLM-generated scientific theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG scope ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG) scope ablation ('all', 'partial', 'match')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation study varying the retrieval corpus used for SCV: 'all' = full 5,664 abstracts, 'partial' = 1,888 abstracts containing ground-truth evidence, 'match' = only the exact ground-truth abstracts; used to probe dependence of model verdicts on retrieval scope.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_name</strong></td>
                            <td>Llama-3-70B-Instruct (experiment example shown)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_description</strong></td>
                            <td>Open-source instruction tuned 70B model used to probe retrieval-scope effects; results illustrate non-intuitive interactions between retrieval breadth and verdict/evidence metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedical claim verification (SCV)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Run SCV with RAG under three retrieval-scopes and report Accuracy and Right Quotes metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Changes in Accuracy and Right Quotes across retrieval scopes; interpret whether narrower retrieval yields better grounding or performance.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>SCV dataset with 5,664-abstract corpus and ground-truth evidence subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Ablation found that narrowing retrieval to only exact matching evidence ('match') increased Right Quotes but decreased Accuracy for Llama-3-70B-Instruct; using the full corpus ('all') increased Accuracy despite presumed irrelevant literature, suggesting large models may benefit from broader context or perform analogical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Non-monotonic effects: retrieving exact evidence does not always maximize classification accuracy, complicating assumptions about ideal retrieval scope.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_or_traditional</strong></td>
                            <td>Humans typically search broadly then focus; these results suggest model behavior may be more analogous to broad-context analogical reasoning rather than strict evidence matching.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Evaluate multiple RAG scopes; report both accuracy and evidence alignment; avoid assuming maximal retrieval precision always improves final verdicts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Fact or fiction: Verifying scientific claims <em>(Rating: 2)</em></li>
                <li>PubMedQA: A dataset for biomedical research question answering <em>(Rating: 2)</em></li>
                <li>GPT-4 technical report <em>(Rating: 2)</em></li>
                <li>StructGPT: A general framework for large language model to reason over structured data <em>(Rating: 2)</em></li>
                <li>KG-Agent: An efficient autonomous agent framework for complex reasoning over knowledge graph <em>(Rating: 2)</em></li>
                <li>AgentBench: Evaluating LLMs as agents <em>(Rating: 1)</em></li>
                <li>MedAgents: Large language models as collaborators for zero-shot medical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9886",
    "paper_id": "paper-6670546864bd235ea13669dae51c7dfe65944439",
    "extraction_schema_id": "extraction-schema-168",
    "extracted_data": [
        {
            "name_short": "BioKGBench",
            "name_full": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science",
            "brief_description": "A benchmark introduced in this paper to evaluate biomedical AI scientist agents on literature understanding and knowledge-graph grounding via three tasks (KGQA, SCV, KGCheck), with supplied KG subset, literature corpus, and annotated evaluation sets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4; Llama-3-70B-Instruct (representative evaluated models)",
            "llm_description": "GPT-4: proprietary API model by OpenAI; Llama-3-70B-Instruct: large open-source instruction-tuned model (70B parameters) evaluated as the top OSS model in the study.",
            "scientific_domain": "Biomedical science (computational biology, clinical knowledge graphs)",
            "evaluation_method": "Three-stage evaluation combining structured KG queries (KGQA), retrieval-augmented verification from literature (SCV), and an integrated agent task (KGCheck) where agents query KG then validate with RAG and external DBs; comparisons to gold answers/annotations and multi-agent process logs.",
            "evaluation_criteria": "Quantitative metrics per task: KGQA: F1, Exact Match (EM), Executability; SCV: Accuracy, 'Right Quotes' (alignment of retrieved evidence), Quote Error rate; KGCheck: Support/Refute label accuracy, Exact Match on final decision, process metrics (Tool Selection, Executability).",
            "benchmark_or_dataset": "BioKGBench dataset: KGQA (698 Qs from a CKG subgraph), SCV (1,385 claims reconstructed from PubMedQA and SciFact with 5,664-abstract corpus), KGCheck (225 human-annotated KG checking instances, 51 full papers for evidence).",
            "results_summary": "State-of-the-art API models (GPT-4) and top OSS model (Llama-3-70B-Instruct) perform best but still show gaps; benchmark revealed many failures and hallucinations across models. The benchmark enabled discovery of &gt;90 factual errors in the Clinical Knowledge Graph and 96 'Refute' annotations within the KGCheck set.",
            "limitations_or_challenges": "Binary labels in KGCheck create a high chance baseline (~50%); static KGs vs. dynamic external sources complicate ground truth; LLMs sometimes answer correctly without evidence (low 'right quote' alignment), and process-level failures (leader agent errors) can dominate final outcomes.",
            "comparison_to_human_or_traditional": "KGCheck simulates human scientific workflow (query KG, retrieve literature, weigh evidence). The paper notes that process-level evaluation (how agents search and validate) better mirrors human scientific practice than simple QA accuracy alone.",
            "recommendations_or_best_practices": "Use decomposition into atomic tasks (KGQA and SCV) plus an integrated KGCheck; report process metrics (tool selection, executability) in addition to outcome metrics; include a 'right quote' evidence alignment metric; use RAG with curated corpora and authoritative databases (UniProt, STRING, PubMed) to ground claims.",
            "uuid": "e9886.0",
            "source_info": {
                "paper_title": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "KGQA",
            "name_full": "Knowledge Graph Question Answering (KGQA) atomic task",
            "brief_description": "An atomic evaluation task that measures an agent's ability to query a biomedical knowledge graph (CKG subgraph) to answer natural-language questions; questions span one-hop, multi-hop, and conjunction reasoning types.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "Various (GPT-4, GLM-4, Llama-3-70B-Instruct, Qwen1.5-72B etc.)",
            "llm_description": "Models evaluated ranged from commercial APIs (GPT-4, GLM-4) to multiple open-source models of different sizes (e.g., Llama-3-70B-Instruct, Qwen variants).",
            "scientific_domain": "Biomedical (protein, pathway, disease relations within Clinical Knowledge Graph)",
            "evaluation_method": "Agents are given KG-querying tools to retrieve entity sets from the KG; LLM outputs are compared to gold sets derived from the KG subgraph.",
            "evaluation_criteria": "F1 score (entity set overlap), Exact Match (EM) for full-set correctness, and 'Executability' measuring whether agents correctly invoked tools/queries.",
            "benchmark_or_dataset": "KGQA dataset from BioKGBench: 698 generated questions (dev 60, test 638) based on a CKG subgraph of 12 node types and 18 relations.",
            "results_summary": "Top performers: GPT-4 (F1 81.8, EM 79.2, Executability 88.4) and Llama-3-70B-Instruct (F1 80.7, EM 77.8, Executability 97.0); other models show wide variance tied to size, training, and specialization.",
            "limitations_or_challenges": "Biomedical KG density increases reasoning complexity; model parameter count does not strictly predict KGQA performance; some models struggle with tool-call formatting ('executability').",
            "comparison_to_human_or_traditional": "KGQA provides a structured grounding alternative to free-form QA; closer to database-query tasks humans perform when verifying facts from KGs.",
            "recommendations_or_best_practices": "Provide atomic KG-query tools, evaluate executability/process as well as answer correctness, include diverse reasoning types (one-hop, multi-hop, conjunction).",
            "uuid": "e9886.1",
            "source_info": {
                "paper_title": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SCV",
            "name_full": "Scientific Claim Verification (SCV) atomic task",
            "brief_description": "A retrieval-augmented evaluation where claims are judged as 'Support', 'Refute', or 'NEI' by finding and citing evidence from scientific abstracts; dataset reconstructed from PubMedQA and SciFact.",
            "citation_title": "",
            "mention_or_use": "use",
            "llm_name": "GPT-4 and multiple OSS models (Llama-3-70B-Instruct, Qwen variants, GLM-4, etc.)",
            "llm_description": "Same set of evaluated models as other tasks; includes large proprietary and open-source models with varying domain exposure.",
            "scientific_domain": "Biomedical literature (claims about clinical/biological facts derived from abstracts)",
            "evaluation_method": "RAG: retrieve abstracts from a corpus (5,664 abstracts) and then classify claim as Support/Refute/NEI; also check whether retrieved quotes align with gold evidence.",
            "evaluation_criteria": "Accuracy of Support/Refute/NEI classification; 'Right Quotes' metric measuring alignment of model-provided evidence quotes with ground truth; Quote Error rate.",
            "benchmark_or_dataset": "SCV dataset: 1,385 claims reconstructed from PubMedQA and SciFact, with retrieval corpus of 5,664 abstracts (dev 120, test 1,265).",
            "results_summary": "GPT-4 achieved Accuracy 83.9% and Right Quotes 87.7%; GLM-4 and some Qwen models show higher Accuracy than Right Quotes, indicating some models can classify claims correctly without producing correct supporting quotes (internalized biomedical knowledge). Performance varies widely across OSS models.",
            "limitations_or_challenges": "Models sometimes correctly classify claims without retrieving correct evidence ('right quote' mismatch), showing possible memorization or hallucination; RAG scope affects metrics in non-intuitive ways (see ablation).",
            "comparison_to_human_or_traditional": "Mirrors human literature verification (find evidence then judge), but models may bypass evidence step, unlike rigorous human verification.",
            "recommendations_or_best_practices": "Report evidence-alignment metrics (right quote) along with classification accuracy; evaluate RAG retrieval scope effects; prefer curated corpora over generic web search for biomedical claims.",
            "uuid": "e9886.2",
            "source_info": {
                "paper_title": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "KGCheck",
            "name_full": "Knowledge Graph Checking (KGCheck) agent task",
            "brief_description": "An integrated agent-level task where agents query a KG to extract nodes/triples and then validate those facts against external databases and literature (via RAG) to label KG items as Support or Refute.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4 and Llama-3-70B-Instruct as agent backbones (through BKGAgent)",
            "llm_description": "GPT-4: API-based high-performing model; Llama-3-70B-Instruct: large OSS model used to instantiate multi-agent baseline.",
            "scientific_domain": "Biomedical knowledge graph curation and verification",
            "evaluation_method": "Multi-agent workflow: leader decomposes task, KG agent queries KG, validation agent checks evidence from web DBs (UniProt, STRING) and publications; final decision compared to human annotations.",
            "evaluation_criteria": "Support/Refute ground-truth accuracy (Exact Match), process metrics including Tool Selection correctness and Executability for both query and validation stages, and per-check-type analysis (node existence, attribute, existing relationship, potential relationship).",
            "benchmark_or_dataset": "KGCheck set: 225 high-quality annotated KG checking instances (dev 20, test 205), categorized into four check types and annotated Support/Refute labels; 51 full papers provided as literature evidence.",
            "results_summary": "Agents discovered real KG inconsistenciesâ€”96 Refute annotations found in the KG. In process/outcome metrics (Table 6), GPT-4-based BKGAgent achieved higher final exact-match accuracy than Llama-based for final KGCheck outcomes (examples: Web DB KGCheck final exact match: GPT-4 64.5% with high executability; Llama had higher tool-executability but lower final exact match), showing process-success doesn't guarantee correct final decision.",
            "limitations_or_challenges": "Binary Support/Refute labels inflate chance-level performance; team-leader errors (planning, repetitive self-talk) can derail correct assistant outputs; reliance on external DB currency is critical since KGs may be outdated.",
            "comparison_to_human_or_traditional": "KGCheck imitates human curation workflows (query KG -&gt; consult databases/literature -&gt; adjudicate), enabling agent evaluation closer to real scientific verification than standalone QA.",
            "recommendations_or_best_practices": "Measure both process and outcome metrics (tool selection, executability, evidence alignment) and use more granular metrics than exact match; include authoritative DB cross-checks and curated full-text evidence where possible.",
            "uuid": "e9886.3",
            "source_info": {
                "paper_title": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "BKGAgent",
            "name_full": "BKGAgent: Biomedical Knowledge-Graph Agent (baseline)",
            "brief_description": "A multi-agent baseline (leader, KG agent, validation agent, tool executor) implemented to interact with KGs and literature for KGCheck, coordinating tool use and evidence validation replicating human team workflows.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4 and Llama-3-70B-Instruct (both used to instantiate leader/assistants in experiments)",
            "llm_description": "Instances using either GPT-4 (API) or Llama-3-70B-Instruct (open-source 70B) as the LLMs for leader and assistant agents; tool executor is non-LLM code.",
            "scientific_domain": "Biomedical KG verification and literature-based validation",
            "evaluation_method": "Run multi-agent workflows on KGCheck items; log per-agent tool selection/executability and the final leader decision; validate outputs against human labels and external DBs.",
            "evaluation_criteria": "Process metrics: Tool Selection correctness, Executability for KG query and validation stages; Outcome metrics: Exact Match of final decision, final Executability. Separately reported for Web DB KGCheck and Publication DB KGCheck.",
            "benchmark_or_dataset": "Evaluated on the KGCheck 225-instance set from BioKGBench, using UniProt and STRING as web DBs and a curated set of 51 publications for publication checks.",
            "results_summary": "BKGAgent showed that high assistant executability doesn't guarantee correct final outcomes; GPT-4-based agent had higher final exact match (e.g., Web DB KGCheck exact match ~64.5%) while Llama-based agent had higher tool activation/executability but lower final exact match (e.g., ~38.1% in Web DB KGCheck), due to leader decision errors and coordination failures.",
            "limitations_or_challenges": "Leader agent planning/coordination is a common failure mode (misinstructions, looping, taking assistant actions). Even with correct assistant retrievals, team chatter or failure to finalize results produces wrong answers.",
            "comparison_to_human_or_traditional": "Framework is explicitly inspired by human team workflows (leader + assistants) and exposes process-level failures analogous to human coordination errors.",
            "recommendations_or_best_practices": "Monitor and evaluate per-agent process metrics; design leader prompts/feedback mechanisms carefully to avoid planning faults; separate tool execution from LLM to improve reliability.",
            "uuid": "e9886.4",
            "source_info": {
                "paper_title": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Right Quotes metric",
            "name_full": "Right Quotes (evidence-alignment) metric for SCV",
            "brief_description": "A metric introduced/used in this work measuring whether the model's retrieved quotation or cited evidence matches the ground-truth evidence supporting the claim verdict.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "GPT-4; GLM-4; Qwen models (as evaluated examples)",
            "llm_description": "Various LLMs evaluated; the metric exposes discrepancies between verdict accuracy and evidence alignment across models.",
            "scientific_domain": "Biomedical literature claim verification",
            "evaluation_method": "Compare model-provided evidence quotes or citations to ground-truth evidence passages; compute percentage alignment (Right Quotes) and quote error rate.",
            "evaluation_criteria": "Right Quotes proportion (how often retrieved quotes match gold evidence) alongside Accuracy on claim labels.",
            "benchmark_or_dataset": "Applied to SCV dataset (1,385 claims from PubMedQA and SciFact) with retrieval corpus of 5,664 abstracts.",
            "results_summary": "Some models (GLM-4, Qwen variants) show higher classification accuracy than Right Quotes alignment, indicating classification can succeed without retrieving correct supporting text (i.e., internalized knowledge or hallucination).",
            "limitations_or_challenges": "A high classification accuracy with low Right Quotes suggests models may not actually ground judgments in retrievable evidence; Right Quotes requires gold evidence passages for comparison (not always available).",
            "comparison_to_human_or_traditional": "Humans typically produce judgments grounded in cited evidence; Right Quotes attempts to quantify this alignment for LLMs.",
            "recommendations_or_best_practices": "Report evidence-alignment metrics alongside verdict accuracy; treat Right Quotes as necessary for trust in scientific claim verification systems.",
            "uuid": "e9886.5",
            "source_info": {
                "paper_title": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "RAG scope ablation",
            "name_full": "Retrieval-Augmented Generation (RAG) scope ablation ('all', 'partial', 'match')",
            "brief_description": "An ablation study varying the retrieval corpus used for SCV: 'all' = full 5,664 abstracts, 'partial' = 1,888 abstracts containing ground-truth evidence, 'match' = only the exact ground-truth abstracts; used to probe dependence of model verdicts on retrieval scope.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_name": "Llama-3-70B-Instruct (experiment example shown)",
            "llm_description": "Open-source instruction tuned 70B model used to probe retrieval-scope effects; results illustrate non-intuitive interactions between retrieval breadth and verdict/evidence metrics.",
            "scientific_domain": "Biomedical claim verification (SCV)",
            "evaluation_method": "Run SCV with RAG under three retrieval-scopes and report Accuracy and Right Quotes metrics.",
            "evaluation_criteria": "Changes in Accuracy and Right Quotes across retrieval scopes; interpret whether narrower retrieval yields better grounding or performance.",
            "benchmark_or_dataset": "SCV dataset with 5,664-abstract corpus and ground-truth evidence subsets.",
            "results_summary": "Ablation found that narrowing retrieval to only exact matching evidence ('match') increased Right Quotes but decreased Accuracy for Llama-3-70B-Instruct; using the full corpus ('all') increased Accuracy despite presumed irrelevant literature, suggesting large models may benefit from broader context or perform analogical reasoning.",
            "limitations_or_challenges": "Non-monotonic effects: retrieving exact evidence does not always maximize classification accuracy, complicating assumptions about ideal retrieval scope.",
            "comparison_to_human_or_traditional": "Humans typically search broadly then focus; these results suggest model behavior may be more analogous to broad-context analogical reasoning rather than strict evidence matching.",
            "recommendations_or_best_practices": "Evaluate multiple RAG scopes; report both accuracy and evidence alignment; avoid assuming maximal retrieval precision always improves final verdicts.",
            "uuid": "e9886.6",
            "source_info": {
                "paper_title": "BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Fact or fiction: Verifying scientific claims",
            "rating": 2
        },
        {
            "paper_title": "PubMedQA: A dataset for biomedical research question answering",
            "rating": 2
        },
        {
            "paper_title": "GPT-4 technical report",
            "rating": 2
        },
        {
            "paper_title": "StructGPT: A general framework for large language model to reason over structured data",
            "rating": 2
        },
        {
            "paper_title": "KG-Agent: An efficient autonomous agent framework for complex reasoning over knowledge graph",
            "rating": 2
        },
        {
            "paper_title": "AgentBench: Evaluating LLMs as agents",
            "rating": 1
        },
        {
            "paper_title": "MedAgents: Large language models as collaborators for zero-shot medical reasoning",
            "rating": 1
        }
    ],
    "cost": 0.0173795,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>BioKGBench: A Knowledge Graph Checking Benchmark of AI Agent for Biomedical Science</h1>
<p>Xinna Lin ${ }^{1,2}$ Siqi Ma ${ }^{1}$ Junjie Shan ${ }^{1}$ Xiaojing Zhang ${ }^{1}$ Shell Xu Hu ${ }^{3}$<br>Tiannan Guo ${ }^{1}$ Stan Z. Li ${ }^{1}$ Kaicheng Yu ${ }^{1 *}$<br>${ }^{1}$ Westlake University ${ }^{2}$ Zhejiang University ${ }^{3}$ Samsung AI Center, Cambridge<br>{linxinna, kyuj@westlake.edu.cn</p>
<h4>Abstract</h4>
<p>Pursuing artificial intelligence for biomedical science, a.k.a. AI Scientist, draws increasing attention, where one common approach is to build a copilot agent driven by Large Language Models (LLMs). However, to evaluate such systems, people either rely on direct Question-Answering (QA) to the LLM itself, or in a biomedical experimental manner. How to precisely benchmark biomedical agents from an AI Scientist perspective remains largely unexplored. To this end, we draw inspiration from one most important abilities of scientists, understanding the literature, and introduce BioKGBench. In contrast to traditional evaluation benchmark that only focuses on factual QA, where the LLMs are known to have hallucination issues, we first disentangle "Understanding Literature" into two atomic abilities, i) "Understanding" the unstructured text from research papers by performing scientific claim verification, and ii) Ability to interact with structured Knowledge-Graph Question-Answering (KGQA) as a form of "Literature" grounding. We then formulate a novel agent task, dubbed KGCheck, using KGQA and domain-based Retrieval-Augmented Generation (RAG) to identify the factual errors of existing large-scale knowledge graph databases. We collect over two thousand data for two atomic tasks and 225 high-quality annotated data for the agent task. Surprisingly, we discover that state-of-the-art agents, both daily scenarios and biomedical ones, have either failed or inferior performance on our benchmark. We then introduce a simple yet effective baseline, dubbed BKGAgent. On the widely used popular knowledge graph, we discover over 90 factual errors which provide scenarios for agents to make discoveries and demonstrate the effectiveness of our approach. The code and data are available at https://github.com/westlake-autolab/BioKGBench.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) are so powerful that they facilitate nearly every aspect of daily life and work right now, even research [1, 2, 3, 4]. Observing their marvelous successes in text generation [5, 6], text summarization [7, 8], and other tasks [9, 10], along with their consistent failures such as hallucinations [11, 12], one can conclude that LLMs are powerful in certain tasks involving large-scale unstructured data like daily text or images, but relatively powerless when dealing with data-hungry scenarios. As such, researchers then construct AI agents [13, 14] assisting LLMs with external tools to extend the capabilities of LLMs. These attempts are fruitful in many fields, including autonomous computer [15], shopping web-agent [16], code developing [17], society simulation [18, 19], etc. A natural subsequent attempt is to develop AI agents to simulate scientists, aiding or even taking over the process of scientific discovery [2].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: (Left) Previous benchmarks for domain-specific AI Agents either focus on the low-level tasks like question answering or are embedded in a complicated pipeline as a scientist copilot. (Right) We close the gap by constructing a knowledge graph checking task that consists of two atomic sub-tasks: Knowledge Graph Question Answering (KGQA) and Scientific Claim Verification (SCV), to provide a better evaluation of AI Agents in biomedical science domain.</p>
<p>As in Figure 1, existing attempts can be grouped into two categories: i) to build an AI agent for a specific task, such as Question Answering (QA) in a specific domain [20]; ii) to encompass multiple AI agents to formulate a multi-agent system as the copilot of scientists, automating certain scientific activities, such as experiment result analysis [21, 22].</p>
<p>Literature review is the most critical ability that a scientist should possess [23, 24]. It does not only involve reading and memorizing, but also requires scientists to understand and critically analyze. Researchers and scientists widely spend a significant amount of time in reading recent works. To save human efforts in scientific discovery, it is necessary for AI scientists to be able to accurately understand and analyze the existing research. Many researchers have dedicated to literature understanding in AI agents [25, 26], while a systematic evaluation system is missing and even underexplored. The current finest evaluation system [25] consists of multiple-choice questions extracted from literature, which cannot fully reveal the underlying reasoning regime of an agent's success or failure, leaving no clue for future advancement nor indicating whether the agent understands the reasoning rationale or merely memorizes data patterns.</p>
<p>On the other hand, another crucial research direction is to help AI agents capture the underlying logic of literature through domain-specific Knowledge Graphs (KGs) [27, 28]. KGs store massive knowledge triples in a graph-structured format [29, 30], complementing LLMs with external knowledge while providing frameworks for interpretation and reasoning [31]. However, manually constructing such KGs is both intellectually and physically intensive. These domain-specific KGs require annotators with profound domain-specific knowledge, leading to high costs to create or maintain the knowledge graphs. As such, we observe that the existing and well-known biomedical KGs [32, 33] are not fully reliable due to outdated information. We attribute such discrepancy to the static nature of KGs, which lack mechanisms for dynamic updates to align with the evolution of external knowledge sources.</p>
<p>In this paper, we propose a novel agent evaluation benchmark BioKGBench to address both challenges simultaneously. As in Figure 1 (right), the ultimate goal of our benchmark is to verify the correctness of nodes and triples in the knowledge graph based on various information, including papers and well-maintained databases. We dub this task Knowledge Graph Checking (KGCheck). Agents need to first query the information recorded on the KGs as directed, then cross-reference this information with external literature or databases to combat hallucinations. This task evaluates the agents' capacities to both process and understand structured data (like KGs) and unstructured data (like literature). It is worth mentioning that the process of verifying knowledge within KGs closely mirrors the methodology of human scientific research, including database queries and extensive literature reviews. This similarity not only underscores the task's relevance to real-world scientific inquiry but also provides intriguing insights. Furthermore, we decompose this task into two more atomic subtasks: Knowledge Graph Question Answering (KGQA) and Scientific Claim Verification (SCV), enabling a more detailed evaluation of the agents' capabilities in processing and understanding of structured and unstructured data, respectively.</p>
<p>We extensively analyze existing AI agents on our benchmark and find that none of the existing agents can accomplish our tasks without moderate adaptation. Therefore, we introduce our agent BKGAgent, the first agent framework to interact with external knowledge graphs as well as research papers. Experiments demonstrate fascinating results that our agent is capable of discovering real conflicts in the existing large-scale datasets. Within 225 professional-annotated data in Clinical Knowledge Graph (CKG) [33], our agent BKGAgent successfully identified some conflicting or missing pairs. This evidence further supports the academic value of our agent by providing researchers with a tool to update their own knowledge bases, offering substantial potential in both academic and commercial markets.</p>
<h1>2 Related Work</h1>
<p>Science Agent. The swift progression of large language models (LLMs) has catalyzed the widespread deployment of intelligent agents across diverse fields, notably within the science domain. Notable examples include ChemCrow [34] and Coscientist [35] in the field of chemistry, DoInstruct [21] in ocean science, and GeneGPT [36], Almanac [37], MedAgents [38] in biomedical domain, etc. Among them, biomedical agents, in particular, have garnered significant attention due to its critical importance. Biomedical agents [39] impact areas ranging from hybrid cell simulation [40], the design of cellular circuits [41] to the development of new therapies [42] and so on. We posit that biomedical agents will emerge as a focal point of research. However, the current benchmark in this field remains inadequate. For instance, MedAgents is evaluated in MedQA [20], MedMCQA [43], PubMedQA [44], relying heavily on inherent knowledge of LLMs, which leads to hallucinations easily. Our proposed BioKGBench is a dynamic benchmark that evaluates the capabilities of agents in utilizing external tools and knowledge retrieval, thereby addressing this gap.
Agent Benchmark. As agents are progressively applied across various domains, the urgency to construct corresponding benchmarks is escalating. Currently, the majority of benchmarks for evaluating agents adopt the approach of evaluating LLM-as-Agent [45], linking LLMs to external frameworks to assess their performance on specific tasks. For instance, AgentBench [45] is a general benchmark for evaluating an agent's reasoning and decision-making capabilities, SWE-bench [46] assesses an agent's proficiency in software engineering, and AgentClinic [47] examines an agent's performance in a simulated clinical environment. However, a benchmark in AI Scientist perspective remains largely unexplored. Our benchmark originates from this perspective, taking the processing and understanding of large-scale data scenarios as the entry point, representing an initial attempt in this direction.</p>
<p>Agent Integrating LLMs and KGs. The collaborative use of LLM and KG has become one of the leading methodologies in contemporary agent design, aimed at alleviating uncertainties stemming from the intrinsic mechanisms of LLMs [48, 49, 50]. This paradigm not only capitalizes on the generalization ability of LLMs but also employs KGs as an external, trustworthy, and structured data source, thereby achieving reasoning proficiency that strikingly emulates human intellect[48]. For instance, StructGPT [51] boosts an LLM's performance on general questions by tapping into the information from a supplied KG. Similarly, KG-Agent [52] leverages knowledge from KGs, synthesizing instruction data for fine-tuning an open-sourced LLM, thereby achieving competitive performance on general question-answering tasks. However, to our knowledge, while this paradigm has been widely applied to the general question-answering area, its potential remains untapped in the biomedical field. BKGAgent, hence, is poised to fill this gap.</p>
<h2>3 BioKGBench</h2>
<p>Here, we present our benchmark in detail. As aforementioned, one key ability of "AI Scientists" is to understand domain knowledge. However, current LLM-driven agent systems inevitably suffer from hallucinations as a consequence of the statistical nature of LLMs along with the lack of scientific training data compared to data from daily scenarios. We notice that a recent trend in research is to use AI agents to leverage external tools to address these limitations [34, 21].</p>
<p>Table 1: Statistics of our BioKGBench.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task</th>
<th style="text-align: center;">Metric</th>
<th style="text-align: center;">Scope</th>
<th style="text-align: center;">Data</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Dev</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">All</td>
</tr>
<tr>
<td style="text-align: center;">KGQA</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">KG</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">638</td>
<td style="text-align: center;">698</td>
</tr>
<tr>
<td style="text-align: center;">SCV</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">1,265</td>
<td style="text-align: center;">1,385</td>
</tr>
<tr>
<td style="text-align: center;">KGCheck</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">KG + T</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">205</td>
<td style="text-align: center;">225</td>
</tr>
</tbody>
</table>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Drawing inspiration from this, we design two atomic abilities to evaluate AI scientists, i) Knowledge Graph Question Answering (KGQA) aiming to address the hallucination issue by grounding the knowledge with structured knowledge graphs; and ii) Scientific Claim Verification (SCV) based on retrieved text from peer-reviewed research papers. In addition, we propose an encompassing task combining these two atomic abilities, to perform Knowledge Graph Checking (KGCheck) as shown in Figure 1. The motivation behind this stems from our interviews with experts from biomedical domains. Their answers to the question "What is the most expected AI agent you would like to use in your daily research?" often included an AI agent that helps in extensive literature review and claim verification. We report the statistics over the scopes of knowledge search, including knowledge graphs and academic literature, in Table 1.</p>
<h1>3.1 Atomic Ability</h1>
<h3>3.1.1 Knowledge Graph Question Answering</h3>
<p>This atomic task in the benchmark is to evaluate the agents' ability to interact with structured Knowledge Graph Question Answering as a grounding of academic literature. Without loss of generality, we choose Clinical Knowledge Graph (CKG) [53] as the source of our data, which is one of the most popular large-scale knowledge graph databases in the biomedical domain. CKG is a knowledge graph database with data imported from diverse biomedical databases, aimed at streamlining automated knowledge discovery through the graph's extensive information.
As the original database is unnecessarily large, we focus on a sub-graph to mitigate the chal-
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The sub-graph of the Clinical Knowledge Graph (CKG) retains 12 types of nodes and 18 kinds of relationships.
lenge while preserving all relevant information. Starting from the origin of CKG-protein, we select the sub-graph to contain exactly 12 categories of biological entities, as indicated in Figure 2. Thus, the sub-graph consists of 484,955 entities (nodes) across 12 categories (Biologically defined) and 18,959,943 relationships (edges) of 18 types, with each type consisting of relationships between a unique pair of entity categories.
After the sub-graph is ready, we construct the question set for the Question Answering (QA) database in two steps. We first handcraft question templates by selecting biomedical fields and pinpointing entities and relations in the CKG. Natural language questions were constructed in various formats, ensuring their accuracy through peer reviews and expert consultations. We then expand our dataset with autogenerated questions by matching CKG data to constructed QA templates, resulting in the generation of 698 questions across three reasoning types and 16 question categories (refer to Table 2).
In this task, we outfit LLMs with a set of atomic KG-querying tools and ask them to answer biomedical questions by querying the provided KG. The responses will be compared with the gold answers and</p>
<p>Table 2: Statics of three different reasoning types of KGQA dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reasoning Type</th>
<th style="text-align: center;">Graph</th>
<th style="text-align: center;">Example Question</th>
<th style="text-align: center;">Question Types</th>
<th style="text-align: center;">\%</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">One-hop</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">What proteins does the protein O94842 act on?</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">56.0</td>
</tr>
<tr>
<td style="text-align: center;">Multi-hop</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">What diseases are associated with the protein encoded by the gene KCNS1?</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">28.7</td>
</tr>
<tr>
<td style="text-align: center;">Conjunction</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Which pathway are the proteins P02778 and P25106 both annotated in?</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">15.3</td>
</tr>
</tbody>
</table>
<p>Table 3: Examples of reconstructed dataset for SCV, where data from PubMedQA is converted from QA to declarative claims. "NEI" stands for "Not Enough Information".</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Example Claim</th>
<th style="text-align: center;">Label</th>
<th style="text-align: center;">$\%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">A deficiency of folate increases blood levels of homocysteine.</td>
<td style="text-align: center;">Support</td>
<td style="text-align: center;">65.2</td>
</tr>
<tr>
<td style="text-align: left;">Therapeutic anticoagulation in the trauma patient is safe.</td>
<td style="text-align: center;">Refute</td>
<td style="text-align: center;">33.1</td>
</tr>
<tr>
<td style="text-align: left;">Sternal fracture in growing children is a rare and often overlooked fracture.</td>
<td style="text-align: center;">NEI</td>
<td style="text-align: center;">1.7</td>
</tr>
</tbody>
</table>
<p>evaluated using the F1 score, where the gold answer to the input question is typically characterized by a set of KG entities. It is noteworthy that our KGQA is built upon a biomedical KG rather than a common sense KG, the former being characterized by its highly dense information storage. This compactness greatly increases the complexity for agents to employ tools and perform reasoning on the structured graph to complete QA tasks. This task enables the development of assessing the robustness and tool learning ability of agents built upon various LLMs, and hopefully it would aid in guiding agents to leverage the extensive biomedical knowledge within the KG, thereby propelling scientific discovery.</p>
<h1>3.1.2 Scientific Claim Verification</h1>
<p>This task is designed to evaluate LLMs' understanding of unstructured text from research papers in a retrieval-augmented generation manner. Following the definition in [54], the task is to identify evidence related to the claim from the research literature and give a verdict of "Support", "Refute", or "NEI" (Not Enough Information) based on it. We reconstruct two high-quality biomedical datasets, PubMedQA [44] and SciFact [54], into one dataset for SCV, yielding a corpus constituted of abstracts derived from 5,664 scholarly articles, alongside a dataset comprising 1,385 biomedical claims, as shown in Table 3.</p>
<h3>3.2 Agent Task</h3>
<p>Building upon the atomic abilities, we propose a novel and comprehensive task, KGCheck. This task necessitates the initial application of the tool-query functionality to extract information from the KG. Subsequently, it employs the RAG approach or database access to procure evidence pertaining to the queried information, facilitating a determination of either "Support" or "Refute". This methodology enables agents to scrutinize the knowledge encapsulated within a large-scale KG, a venture of particular importance considering the prevalence of inaccuracies within numerous datasets, including prominent ones such as ImageNet [55].</p>
<p>Table 4: Four different checking types of KGCheck.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Check Type</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Graph</th>
<th style="text-align: center;">\%</th>
<th style="text-align: center;">Support: Refute</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Node</td>
<td style="text-align: center;">Existence</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">71.0:29.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Attribute</td>
<td style="text-align: center;">?</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Triple</td>
<td style="text-align: center;">Existing</td>
<td style="text-align: center;">a ?</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">46.4:53.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Potential</td>
<td style="text-align: center;">a ?</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>For this task, we collect 225 high-quality annotated data, as illustrated in Table 4. Given the massive data encapsulated within KGs via triples, we delineate the inspection process into two distinct categories: single-node and triple-based. The single-node inspection is divided into node existence and attribute value assessments, while the triple inspection encompasses scenarios with and without edges between two nodes:</p>
<ul>
<li>Existence: We note that databases may excise entries during updates due to inaccuracies or redundancies, whereas KGs remain static post-construction, similar to LLMs in some respects. If nodes corresponding to obsolete entities persist in the KG, the label is "Refute"; if they are congruent with real-time updated external databases, the label is "Support".</li>
<li>
<p>Attribute: Our KG is characterized by high information density, with each node and edge encapsulating numerous attribute values, which we scrutinize for accuracy and completeness.</p>
</li>
<li>
<p>Existing Relationship: We check whether existing edges contradict information from external, real-time updated databases and literature. If external knowledge corroborates the relationship, the label is "Support"; conversely, it is "Refute".</p>
</li>
<li>Potential Relationship: If a relationship is confirmed by databases or literature but is not represented in the KG, the label is "Refute"; otherwise, it is "Support".</li>
</ul>
<p>Despite utilizing the latest databases (as of May 2024), we identified errors within the KG, evidenced by 96 "Refute" annotations. These data are valuable and provide scenarios for agents to comprehend knowledge from heterogeneous sources and make discoveries.</p>
<h1>3.3 BKGAgent: A Simple Baseline</h1>
<p>We propose a biomedical knowledge-graph agent (BKGAgent), as shown in Figure 3. It's a multiagent framework based on langgraph [56], capable of retrieving information from knowledge graph and cross-validating its correctness with multiple information sources. Our framework is comprised of three agents: the team leader for the progress control, the KG agent for information retrieval from KG, and the validation agent for checking the correctness of the information from KG. This setup simulates the workflow of a human research team, where a leader supervises the assistants' work and makes the final decision based on their feedback. Additionally, the tool executor is solely responsible for executing functions, and is not based on LLMs.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Framework of our BKGAgent.
When a user assigns a task, the leader initially breaks down the task and announces the plan. Then the KG agent is activated to retrieve task-related information from the KG. This involves specifying the tool and its arguments to the tool executor, interpreting the tool result, and communicating it back to the leader. After that, the validation agent is called to verify the information with a workflow similar to that of the KG agent. Finally, the leader will draw a conclusion and return it to the user.</p>
<h2>4 Experiments</h2>
<h3>4.1 Main Results and Analysis: Atomic Abilities</h3>
<p>The detailed experimental results of atomic abilities evaluation on LLMs are shown in Table 5, and we summarize our key findings as follows:</p>
<p>Table 5: Test set (standard) results of two easy tasks: KGQA, SCV. Bold/underline and red/blue indicate the best and second in the subgroup and overall.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLM <br> Type</th>
<th style="text-align: center;">Models</th>
<th style="text-align: center;">KGQA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SCV</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">Executability</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Right Quotes</td>
<td style="text-align: center;">Error</td>
</tr>
<tr>
<td style="text-align: center;">API</td>
<td style="text-align: center;">GPT-4 [57]</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">88.4</td>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GLM-4 [58]</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">70.4</td>
<td style="text-align: center;">82.7</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">0.6</td>
</tr>
<tr>
<td style="text-align: center;">OSS <br> (Large)</td>
<td style="text-align: center;">Qwen1.5-72B-Chat [59]</td>
<td style="text-align: center;">74.7</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">85.7</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Llama-3-70B-Instruct [60]</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">86.6</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DeepSeek-LLM-67B-Chat [61]</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Qwen1.5-32B-Chat [59]</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">62.1</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">0.4</td>
</tr>
<tr>
<td style="text-align: center;">OSS <br> (Medium)</td>
<td style="text-align: center;">Qwen1.5-14B-Chat [59]</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Baichuan2-13B-Chat [62]</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">42.0</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">33.6</td>
</tr>
<tr>
<td style="text-align: center;">OSS <br> (Small)</td>
<td style="text-align: center;">Llama-3-8B-Instruct [60]</td>
<td style="text-align: center;">54.7</td>
<td style="text-align: center;">51.3</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Qwen1.5-7B-chat [59]</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">72.5</td>
<td style="text-align: center;">39.1</td>
<td style="text-align: center;">2.2</td>
</tr>
<tr>
<td style="text-align: center;">OSS <br> (MoE)</td>
<td style="text-align: center;">Mixtral-8x7B-Instruct-v0.1 [63]</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">67.9</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">2.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Starling-LM-alpha-8x7B-MoE-GPTQ [64]</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">10.9</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Qwen1.5-MoE-A2.7B-Chat [59]</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">55.0</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">3.0</td>
</tr>
</tbody>
</table>
<ul>
<li>Disparity between open-source and commercial API models. Commercial API models like GPT-4 and GLM-4 generally outperform open-source models in several key metrics. GPT-4, for example, consistently achieves higher scores in both KGQA and SCV tasks, highlighting the advantage of proprietary training techniques and larger computational resources.</li>
<li>Strong performance of open-source large models. Some large OSS models, such as Llama-3-70B-Instruct, perform competitively, sometimes surpassing API models in specific metrics. Llama-3-70B-Instruct, in particular, excels in KGQA executability, suggesting that optimized training can enable open-source models to rival or exceed commercial counterparts.</li>
<li>Model parameters do not always correlate with better performance. In the OSS (Medium) and OSS (Small) categories, smaller models like Llama-3-8B-Instruct sometimes outperform larger models like Qwen1.5-32B-Chat in SCV tasks, indicating that model architecture, training data quality, and fine-tuning strategies significantly impact performance. Notably, Qwen1.5-14B-Chat outperforms Qwen1.5-32B-Chat in KGQA, suggesting the latter's pre-training may be insufficient.</li>
<li>Domain-specific models lack transferability. DeepSeek-LLM-67B-Chat excells in mathematical problems [61], but underperforms in biomedical-related tasks, highlighting its lack of cross-domain transferability. This suggests that specialization in one area may compromise generalizability.</li>
<li>Inconsistent performance of MoE models. While Mixtral-8x7B-Instruct-v0.1 performs well in both KGQA and SCV tasks, other MoE models like Starling-LM-alpha-8x7B-MoE-GPTQ and Qwen1.5-MoE-A2.7B-Chat show significantly lower scores. This inconsistency suggests that the effectiveness of MoE models heavily depends on the implementation and integration of the expert models. Additionally, Mixtral-8x7B-Instruct-v0.1, though strong in main metrics, struggles with controlling response format, indicating that individual expert models still require improvement.</li>
<li>Biomedical knowledge embedded in model parameters. The new metric "right quote" for SCV assesses the alignment of retrieved quotes with ground truth evidence. Some models, such as GLM4, Qwen1.5-72B-Chat, and Qwen1.5-7B-Chat, exhibit higher accuracy metrics than "right quote" metrics. This suggests these models can accurately assess input claims even without sufficient literature evidence, indicating they possess specialized biomedical knowledge.</li>
</ul>
<p>We also conduct an ablation experiment on three scopes of RAG, as shown in Figure 4, where 'all' refers to the abstract of 5,664 articles, 'partial' denotes the 1,888 abstracts containing ground truth evidence of claims, and 'match' corresponds to the abstracts of the ground truth evidence for the claims. Interestingly, we observe an unexpected phenomenon where the model's performance in the 'match' setting only increases in terms of the right quotes metric, while the accuracy metric actually decreases. In the 'all' setting, we initially thought that irrelevant literature would introduce interference, but the accuracy metric, on the contrary, increases.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Llama-3-70B-Instruct's performance in RAG across different scopes of literature.</p>
<p>This suggests that there is a potential connection</p>
<p>Table 6: Process and outcome metrics of BKGAgent on KGCheck task, where bold numbers denote better scores among the two models while green numbers denote $100 \%$ acuracy.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">KG Query Task</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Validation Task</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Final Result</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tool Selection</td>
<td style="text-align: center;">Executability</td>
<td style="text-align: center;">Tool Selection</td>
<td style="text-align: center;">Executability</td>
<td style="text-align: center;">Exact Match</td>
<td style="text-align: center;">Executability</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Web Database KGCheck</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 [57]</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">96.9</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-70B-Instruct [60]</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">100.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Publication Database KGCheck</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 [57]</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">67.7</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">61.5</td>
<td style="text-align: center;">95.4</td>
</tr>
<tr>
<td style="text-align: center;">Llama-3-70B-Instruct [60]</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">63.1</td>
</tr>
</tbody>
</table>
<p>among the extensive literature, where large models exhibit a form of "analogical reasoning". This provides us with insights for conducting extensive literature research in simulating human scientific research.</p>
<h1>4.2 Main Results and Analysis: BKGAgent</h1>
<p>The experiment results including both process and final answer are shown in Table 6, where the executability metric of the assistant agent means whether it is correctly activated. The exact match score of the final result agrees with what we observed in the atomic abilities experiments: GPT-4 surpasses Llama-3-70B-Instruct in the final result, which shows the best performance in the atomic abilities experiments. We discovered some interesting phenomena based on the analysis of BKGAgent chat history.</p>
<ul>
<li>Leader agent performance significantly influences the team behavior. While the behavior of the assistant agents like KG agent can be modified by the leader's instruction, the leader itself lacks action-related feedback from others, meaning that a bad decision made by the leader may lead to a catastrophe. We found four common error cases induced by the leader, as shown in Figure 5. Among these cases, the leader either fails to give effective instructions to team members, becomes trapped in repeated self-talks, or attempts to perform the tasks that are meant for the assistants.</li>
<li>Assistant agents can always select the right tool. As shown in the results, assistant agents's executability is almost the same as its right tool selection rate, which means once the agent is correctly woken up, it has a good chance of selecting the right tool.</li>
<li>Good process results do not necessarily mean a good final result. Surprisingly, the agent based on Llama surpasses the one based on GPT in terms of assistant agents' tool selection. However, the final result is far from satisfactory. We watch for the chat history of the Llama-based agent and find out that while the right answer can be found in the history of the agent in many cases, the team starts chatter instead of sending the result to the user thus leading to a final wrong answer.</li>
<li>Balanced abilities of BKGAgent. We classify our tasks into two categories by the way the agent checks the information from KG, namely the web database (UniProt [65] and STRING [66]) check task and the publication database check task. The performance of them is separately evaluated. Considering the exact match score of the final result, both the GPT-based agent and Llama-based agent show similar performance in both types of tasks, indicating BKGAgent does not show any preference for specific tasks.</li>
</ul>
<h3>4.3 Capability Analysis</h3>
<p>Biomedical agents are not widely employed, as most tasks rely solely on models (refer to Appendix F). Additionally, most agents are primarily focused on QA tasks. For example, MedAgents[67] employs a Multi-disciplinary Collaboration (MC) framework and was tested on nine QA datasets. General agents such as the multi-agent framework AutoGen and the single-agent framework AutoGPT are capable of performing web searches and database retrieval enhancements through tool calls. However, since accessing knowledge graphs and biomedical information is beyond daily usage, and the information directly gained from the web being unreliable, it is challenging to achieve the same level of biomedical information retrieval and verification effectiveness using existing general agent frameworks.
As presented in Table 7, our framework encompasses the basic abilities, especially for retrieval in the biomedical domain. The range of our information for KG result validation includes the UniProt</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Error analysis. Here, we show a failure case due to a leader's various mistakes: the hallucination of the leader misleading the later task or using the wrong process, the leader producing unnecessary repeated tasks and misunderstanding leads to the wrong process.
database, STRING database, and publications selected from PubMed, which surpasses the limited information from a simple web search.</p>
<h1>5 Conclusion</h1>
<p>We present BioKGBench, an interactive benchmark that encompasses the KGCheck task with two atomic capabilities for assessment: KGQA and SCV. KGCheck offers agents a valuable scenario for detecting knowledge hallucination within large-scale data, akin to the experience of researchers making discoveries amidst voluminous literature in the real world. We conduct evaluations of these two atomic capabilities across 13 LLMs and select the topperforming open-source model, Llama-3-70BInstruct, and API-based model, GPT-4, to construct BKGAgent-a multi-agent system serving as the baseline. Comparisons with existing general and biomedical agents revealed their poor performance due to the absence of certain process capabilities, thereby demonstrating the challenging nature of our benchmark. We expect BioKGBench to serve as a valuable endeavor</p>
<p>Table 7: Comparison of capabilities for BKGAgent and other frameworks.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Scenario</th>
<th style="text-align: center;">Framework</th>
<th style="text-align: center;">MA KGq</th>
<th style="text-align: center;">IR IF</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AutoGPT ${ }^{1}$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times \times \times$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AutoGen [68]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times \times \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HuggingGPT [69]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times \times \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Common</td>
<td style="text-align: center;">OpenAgents [70]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times \times \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">AgentVerse [71]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times \times \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Xagent [72]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times \times \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BabyAGI [73]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times \times \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Domain</td>
<td style="text-align: center;">MedAgents [74]</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times \times \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">gpt-researcher ${ }^{2}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times \times \checkmark$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BKGAgent (ours)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark \checkmark$</td>
</tr>
</tbody>
</table>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>correct responses are often due to chance, attributed to the binary nature of answers which inherently allows a $50 \%$ success rate from random guesses. More granular metrics instead of exact match score should be introduced for a more accurate evaluation, which will be part of future work.</p>
<h1>References</h1>
<p>[1] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Z. Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen. A survey of large language models. ArXiv, abs/2303.18223, 2023.
[2] Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738, 2024.
[3] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics. ArXiv, abs/2310.05694, 2023.
[4] Hongjian Zhou, Boyang Gu, Xinyu Zou, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Xian Wu, Zheng Li, and Fenglin Liu. A survey of large language models in medicine: Progress, application, and challenge. ArXiv, abs/2311.05112, 2023.
[5] Wenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu, Qingyun Wang, Heng Ji, and Meng Jiang. A survey of knowledge-enhanced text generation. ACM Computing Surveys, 54(11s):1-38, 2022.
[6] Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. Evaluation of text generation: A survey. arXiv preprint arXiv:2006.14799, 2020.
[7] Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea, and Hoda K Mohamed. Automatic text summarization: A comprehensive survey. Expert systems with applications, 165:113679, 2021.
[8] Mahak Gambhir and Vishal Gupta. Recent automatic text summarization techniques: a survey. Artificial Intelligence Review, 47(1):1-66, 2017.
[9] Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong Wang, Shirui Pan, and Qingsong Wen. Position paper: What can large language models tell us about time series analysis. In International Conference on Machine Learning (ICML 2024), 2024.
[10] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. Graphgpt: Graph instruction tuning for large language models. ArXiv, abs/2310.13023, 2023.
[11] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. Towards mitigating llm hallucination via self reflection. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1827-1843, 2023.
[12] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. Llm lies: Hallucinations are not bugs, but features as adversarial examples. arXiv preprint arXiv:2310.01469, 2023.
[13] Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long Han, and Yang Tang. A brief overview of chatgpt: The history, status quo and potential future development. IEEE/CAA Journal of Automatica Sinica, 10(5):1122-1136, 2023.
[14] Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, and Hang Su. Evil geniuses: Delving into the safety of llm-based agents. arXiv preprint arXiv:2311.11855, 2023.
[15] Neil Joseph Steiner. Autonomous computing systems. PhD thesis, Virginia Tech, 2008.
[16] Raymond ST Lee and James NK Liu. ijade web-miner: an intelligent agent framework for internet shopping. IEEE Transactions on Knowledge and Data Engineering, 16(4):461-473, 2004.
[17] Jean-Michel Dalle and Paul A David. Simcode: agent-based simulation modelling of opensource software development. EconWPA, Industrial Organization, 2004.</p>
<p>[18] Alexis Drogoul and Jacques Ferber. Multi-agent simulation as a tool for studying emergent processes in societies. In Simulating societies, pages 127-142. Routledge, 2018.
[19] Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong, and Hao Wang. Llm-based agent society investigation: Collaboration and confrontation in avalon gameplay. arXiv preprint arXiv:2310.14985, 2023.
[20] Xiao Zhang, Ji Wu, Zhiyang He, Xien Liu, and Ying Su. Medical exam question answering with large-scale reading comprehension. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.
[21] Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, and Huajun Chen. Oceangpt: A large language model for ocean science tasks. arXiv preprint arXiv:2310.02031, 2023.
[22] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, et al. Scientific discovery in the age of artificial intelligence. Nature, 620(7972):47-60, 2023.
[23] Hannah Snyder. Literature review as a research methodology: An overview and guidelines. Journal of business research, 104:333-339, 2019.
[24] Julie Thomas, Juliana Utley, Soo-Young Hong, Hunkar Korkmaz, and Gwen Nugent. A review of the research. Handbook of Research on STEM Education, 2020.
[25] Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Yongge Li, Mujie Lin, Shuwen Yang, et al. Sciassess: Benchmarking llm proficiency in scientific literature analysis. arXiv preprint arXiv:2403.01976, 2024.
[26] Yutong Li, Lu Chen, Aiwei Liu, Kai Yu, and Lijie Wen. Chatcite: Llm agent with human workflow guidance for comparative literature summary. arXiv preprint arXiv:2403.02574, 2024.
[27] Bilal Abu-Salih. Domain-specific knowledge graphs: A survey. Journal of Network and Computer Applications, 185:103076, 2021.
[28] Mayank Kejriwal. Domain-specific knowledge graph construction. Springer, 2019.
[29] Aidan Hogan, Eva Blomqvist, Michael Cochez, Claudia d'Amato, Gerard De Melo, Claudio Gutierrez, Sabrina Kirrane, JosÃ© Emilio Labra Gayo, Roberto Navigli, Sebastian Neumaier, et al. Knowledge graphs. ACM Computing Surveys (Csur), 54(4):1-37, 2021.
[30] Sakher Khalil Alqaaidi and Krzysztof Kochut. Knowledge graph completion using structural and textual embeddings. arXiv preprint arXiv:2404.16206, 2024.
[31] Lars-Peter Meyer, Claus Stadler, Johannes Frey, Norman Radtke, Kurt Junghanns, Roy Meissner, Gordian Dziwis, Kirill Bulert, and Michael Martin. Llm-assisted knowledge graph engineering: Experiments with chatgpt. In Working conference on Artificial Intelligence Development for a Resilient and Sustainable Tomorrow, pages 103-115. Springer Fachmedien Wiesbaden Wiesbaden, 2023.
[32] Alberto Santos, Ana R ColaÃ§o, Annelaura B Nielsen, Lili Niu, Maximilian Strauss, Philipp E Geyer, Fabian Coscia, Nicolai J Wewer Albrechtsen, Filip Mundt, Lars Juhl Jensen, et al. A knowledge graph to interpret clinical proteomics data. Nature biotechnology, 40(5):692-702, 2022.
[33] Payal Chandak, Kexin Huang, and Marinka Zitnik. Building a knowledge graph to enable precision medicine. Scientific Data, 10(1):67, 2023.
[34] Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.
[35] Daniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. Autonomous chemical research with large language models. Nature, 624(7992):570-578, 2023.
[36] Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. Genegpt: Augmenting large language models with domain tools for improved access to biomedical information. Bioinformatics, 40(2):btae075, 2024.</p>
<p>[37] Cyril Zakka, Rohan Shad, Akash Chaurasia, Alex R Dalal, Jennifer L Kim, Michael Moor, Robyn Fong, Curran Phillips, Kevin Alexander, Euan Ashley, et al. Almanac-retrievalaugmented language models for clinical medicine. NEJM AI, 1(2):AIoa2300068, 2024.
[38] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning. arXiv preprint arXiv:2311.10537, 2023.
[39] Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, and Marinka Zitnik. Empowering biomedical discovery with ai agents. arXiv preprint arXiv:2404.02831, 2024.
[40] Yihang Xiao, Jinyi Liu, Yan Zheng, Xiaohan Xie, Jianye Hao, Mingzhi Li, Ruitao Wang, Fei Ni, Yuxiao Li, Jintian Luo, et al. Cellagent: An llm-driven multi-agent framework for automated single-cell data analysis. bioRxiv, pages 2024-05, 2024.
[41] Srinivas Niranj Chandrasekaran, Beth A Cimini, Amy Goodale, Lisa Miller, Maria KostAlimova, Nasim Jamali, John G Doench, Briana Fritchman, Adam Skepner, Michelle Melanson, et al. Three million images and morphological profiles of cells treated with matched chemical and genetic perturbations. Nature Methods, pages 1-8, 2024.
[42] Li Zhenzhu, Zhang Jingfeng, Zhou Wei, Zheng Jianjun, and Xia Yinshui. Gpt-agents based on medical guidelines can improve the responsiveness and explainability of outcomes for traumatic brain injury rehabilitation. Scientific Reports, 14(1):7626, 2024.
[43] Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering. In Conference on health, inference, and learning, pages 248-260. PMLR, 2022.
[44] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. Pubmedqa: A dataset for biomedical research question answering. arXiv preprint arXiv:1909.06146, 2019.
[45] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023.
[46] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023.
[47] Samuel Schmidgall, Rojin Ziaei, Carl Harris, Eduardo Reis, Jeffrey Jopling, and Michael Moor. Agentclinic: a multimodal agent benchmark to evaluate ai in simulated clinical environments. arXiv preprint arXiv:2405.07960, 2024.
[48] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap. IEEE Transactions on Knowledge and Data Engineering, 2024.
[49] Shengyuan Chen, Yunfeng Cai, Huang Fang, Xiao Huang, and Mingming Sun. Differentiable neuro-symbolic reasoning on large-scale knowledge graphs. In Neural Information Processing Systems, 2023.
[50] Lin F. Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. Give us the facts: Enhancing large language models with knowledge graphs for fact-aware language modeling. IEEE Transactions on Knowledge and Data Engineering, 36:3091-3110, 2023.
[51] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji-Rong Wen. Structgpt: A general framework for large language model to reason over structured data. arXiv preprint arXiv:2305.09645, 2023.
[52] Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, Yang Song, Chen Zhu, Hengshu Zhu, and JiRong Wen. Kg-agent: An efficient autonomous agent framework for complex reasoning over knowledge graph. arXiv preprint arXiv:2402.11163, 2024.
[53] Linfeng Li, Peng Wang, Jun Yan, Yao Wang, Simin Li, Jinpeng Jiang, Zhe Sun, Buzhou Tang, Tsung-Hui Chang, Shenghui Wang, et al. Real-world data medical knowledge graph: construction and applications. Artificial intelligence in medicine, 103:101817, 2020.
[54] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. Fact or fiction: Verifying scientific claims. arXiv preprint arXiv:2004.14974, 2020.</p>
<p>[55] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.
[56] Harrison Chase. Langgraph. https://github.com/langchain-ai/langgraph, October 2023. Computer Software.
[57] OpenAI. Gpt-4 technical report. 2023.
[58] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GLM: general language model pretraining with autoregressive blank infilling. pages 320-335, 2022.
[59] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023.
[60] AI@Meta. Llama 3 model card. 2024.
[61] DeepSeek-AI Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wen-Hui Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Jun-Mei Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Min Tang, Bing-Li Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Yu Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yi Xiong, Hanwei Xu, Ronald X Xu, Yanhong Xu, Dejian Yang, Yu mei You, Shuiping Yu, Xin yuan Yu, Bo Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghu Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models with longtermism. ArXiv, abs/2401.02954, 2024.
[62] Ai Ming Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Hai Zhao, Hang Xu, Hao-Lun Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kuncheng Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Pei Guo, Ruiyang Sun, Zhang Tao, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yan-Bin Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models. ArXiv, abs/2309.10305, 2023.
[63] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L'elio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and William El Sayed. Mixtral of experts. ArXiv, abs/2401.04088, 2024.
[64] Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao Jiao. Starling-7b: Improving llm helpfulness \&amp; harmlessness with rlaif, November 2023.
[65] Uniprot: the universal protein knowledgebase in 2023. Nucleic acids research, 51(D1):D523D531, 2023.
[66] Damian Szklarczyk, Rebecca Kirsch, Mikaela Koutrouli, Katerina Nastou, Farrokh Mehryary, Radja Hachilif, Annika L Gable, Tao Fang, Nadezhda T Doncheva, Sampo Pyysalo, et al. The string database in 2023: protein-protein association networks and functional enrichment analyses for any sequenced genome of interest. Nucleic acids research, 51(D1):D638-D646, 2023.</p>
<p>[67] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning, 2024.
[68] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023.
[69] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face, 2023.
[70] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, et al. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634, 2023.
[71] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2023.
[72] XAgent Team. Xagent: An autonomous agent for complex task solving, 2023.
[73] Yoheinakajima. Yoheinakajima/babyagi.
[74] Xiangru Tang, Anni Zou, Zhuosheng Zhang, Yilun Zhao, Xingyao Zhang, Arman Cohan, and Mark B. Gerstein. Medagents: Large language models as collaborators for zero-shot medical reasoning. ArXiv, abs/2311.10537, 2023.
[75] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal DaumÃ© Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86-92, 2021.
[76] Sandra Orchard, Mais Ammari, Bruno Aranda, Lionel Breuza, Leonardo Briganti, Fiona Broackes-Carter, Nancy H Campbell, Gayatri Chavali, Carol Chen, Noemi Del-Toro, et al. The mintact project-intact as a common curation platform for 11 molecular interaction databases. Nucleic acids research, 42(D1):D358-D363, 2014.
[77] Oana Palasca, Alberto Santos, Christian Stolte, Jan Gorodkin, and Lars Juhl Jensen. Tissues 2.0: an integrative web resource on mammalian tissue expression. Database, 2018:bay003, 2018.
[78] Sune Pletscher-Frankild, Albert PallejÃ , Kalliopi Tsafou, Janos X Binder, and Lars Juhl Jensen. Diseases: Text mining and data integration of disease-gene associations. Methods, 74:83-89, 2015.
[79] Ruth L Seal, Bryony Braschi, Kristian Gray, Tamsin EM Jones, Susan Tweedie, Liora HaimVilmovsky, and Elspeth A Bruford. Genenames. org: the hgnc resources in 2023. Nucleic Acids Research, 51(D1):D1003-D1009, 2023.
[80] Noemi Del Toro, Anjali Shrivastava, Eliot Ragueneau, Birgit Meldal, Colin Combe, Elisabet Barrera, Livia Perfetto, Karyn How, Prashansa Ratan, Gautam Shirodkar, et al. The intact database: efficient access to fine-grained molecular interaction data. Nucleic acids research, 50(D1):D648-D653, 2022.
[81] Janet PiÃ±ero, Juan Manuel RamÃ­rez-Anguita, Josep SaÃ¼ch-Pitarch, Francesco Ronzano, Emilio Centeno, Ferran Sanz, and Laura I Furlong. The disgenet knowledge platform for disease genomics: 2019 update. Nucleic acids research, 48(D1):D845-D855, 2020.
[82] Igor Rodchenkov, Ozgun Babur, Augustin Luna, Bulent Arman Aksoy, Jeffrey V Wong, Dylan Fong, Max Franz, Metin Can Siper, Manfred Cheung, Michael Wrana, et al. Pathway commons 2019 update: integration, analysis and exploration of pathway data. Nucleic acids research, 48(D1):D489-D497, 2020.
[83] Antonio Fabregat, Steven Jupe, Lisa Matthews, Konstantinos Sidiropoulos, Marc Gillespie, Phani Garapati, Robin Haw, Bijay Jassal, Florian Korninger, Bruce May, et al. The reactome pathway knowledgebase. Nucleic acids research, 46(D1):D649-D655, 2018.
[84] Timothy Jewison, Yilu Su, Fatemeh Miri Disfany, Yongjie Liang, Craig Knox, Adam Maciejewski, Jenna Poelzer, Jessica Huynh, You Zhou, David Arndt, et al. Smpdb 2.0: big improvements to the small molecule pathway database. Nucleic acids research, 42(D1):D478-D484, 2014.</p>
<p>[85] Prisca Lo Surdo, Marta Iannuccelli, Silvia Contino, Luisa Castagnoli, Luana Licata, Gianni Cesareni, and Livia Perfetto. Signor 3.0, the signaling network open resource 3.0: 2022 update. Nucleic acids research, 51(D1):D631-D637, 2023.
[86] Lynn M Schriml, Elvira Mitraka, James Munro, Becky Tauber, Mike Schor, Lance Nickle, Victor Felix, Linda Jeng, Cynthia Bearer, Richard Lichenstein, et al. Human disease ontology 2018 update: classification, content and workflow expansion. Nucleic acids research, 47(D1):D955-D962, 2019.
[87] Antje Chang, Ida Schomburg, Sandra Placzek, Lisa Jeske, Marcus Ulbrich, Mei Xiao, Christoph W Sensen, and Dietmar Schomburg. Brenda in 2015: exciting developments in its 25th year of existence. Nucleic acids research, 43(D1):D439-D446, 2015.
[88] Gene Ontology Consortium. Expansion of the gene ontology knowledgebase and resources. Nucleic acids research, 45(D1):D331-D338, 2017.
[89] Gerhard Mayer, Luisa Montecchi-Palazzi, David Ovelleiro, Andrew R Jones, Pierre-Alain Binz, Eric W Deutsch, Matthew Chambers, Marius Kallhardt, Fredrik Levander, James Shofstahl, et al. The hupo proteomics standards initiative-mass spectrometry controlled vocabulary. Database, 2013:bat009, 2013.
[90] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
[91] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837, 2022.
[92] Michael GÃ¼nther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. Jina embeddings 2: 8192-token general-purpose text embeddings for long documents, 2023.
[93] Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne J. Bergen, Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun K. Manrai, Debora S. Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Velickovic, Max Welling, Linfeng Zhang, Connor W. Coley, Yoshua Bengio, and Marinka Zitnik. Scientific discovery in the age of artificial intelligence. Nature, 620:47-60, 2023.
[94] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1-35, 2023.
[95] Francesca Grisoni. Chemical language models for de novo drug design: Challenges and opportunities. Current Opinion in Structural Biology, 79:102527, 2023.
[96] Taicheng Guo, Kehan Guo, Zhengwen Liang, Zhichun Guo, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. What indeed can GPT models do in chemistry? a comprehensive benchmark on eight tasks. arXiv preprint arXiv:2305.18365, 2023.
[97] Youwei Liang, Ruiyi Zhang, Li Zhang, and Pengtao Xie. Drugchat: Towards enabling chatgpt-like capabilities on drug molecule graphs. 2023.
[98] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony S. Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.
[99] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171-4186, 2019.
[100] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
[101] Elliot Bolton, David Hall, Michihiro Yasunaga, Tony Lee, Chris Manning, and Percy Liang. Biomedlm. https://crfm.stanford.edu/2022/12/15/biomedlm.html, 2022.</p>
<p>[102] K. Singhal, Shekoofeh Azizi, Tao Tu, Said Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather J. Cole-Lewis, Stephen J. Pfohl, P A Payne, Martin G. Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, P. A. Mansfield, Blaise AgÃ¼era y Arcas, Dale R. Webster, Greg S. Corrado, Yossi Matias, Katherine Hui-Ling Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, JoÃ«lle K. Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large language models encode clinical knowledge. Nature, 620:172 - 180, 2022.
[103] K. Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen R. Pfohl, Heather J. Cole-Lewis, Darlene Neal, Mike Schaekermann, Amy Wang, Mohamed Amin, S. Lachgar, P. A. Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise AgÃ¼era y Arcas, Nenad Tomasev, Yun Liu, Renee C Wong, Christopher Semturs, Seyedeh Sara Mahdavi, JoÃ«lle K. Barral, Dale R. Webster, Greg S Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617, 2023.
[104] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
[105] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. arXiv preprint arXiv:2302.14233, 2023.
[106] Vladimir Sotnikov and Anastasiia Chaikova. Language models for multimessenger astronomy. Galaxies, 11(3), 2023.
[107] Tianhao Li, Sandesh Shetty, Advaith Kamath, Ajay Jaiswal, Xianqian Jiang, Ying Ding, and Yejin Kim. Cancergpt: Few-shot drug pair synergy prediction using large pre-trained language models. arXiv preprint arXiv:2304.10946, 2023.
[108] Carl N Edwards, Aakanksha Naik, Tushar Khot, Martin D Burke, Heng Ji, and Tom Hope. Synergpt: In-context learning for personalized drug synergy prediction and drug design. bioRxiv, 2023.
[109] Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. Genegpt: Teaching large language models to use ncbi web apis. 2023.
[110] Daniil A. Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research capabilities of large language models. arXiv preprint arXiv:2304.05332, 2023.
[111] Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, and Chaowei Xiao. Chatgpt-powered conversational drug editing using retrieval and domain feedback. arXiv preprint arXiv:2305.18090, 2023.
[112] Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, and Yuqing Yang. Mlcopilot: Unleashing the power of large language models in solving machine learning tasks. arXiv preprint arXiv:2304.14979, 2023.
[113] Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar. LeanDojo: Theorem proving with retrieval-augmented language models. In Neural Information Processing Systems (NeurIPS), 2023.
[114] Peiyang Song, Kaiyu Yang, and Anima Anandkumar. Towards large language models as copilots for theorem proving in Lean. arXiv preprint arXiv: Arxiv-2404.12534, 2024.</p>
<h1>Part I</h1>
<h2>Appendix</h2>
<h2>contents</h2>
<p>A Dataset and Code ..... 18
A. 1 Dataset and Code Access ..... 18
A. 2 Responsibility Statement ..... 18
B Datasheet ..... 18
B. 1 Motivation ..... 18
B. 2 Composition ..... 18
B. 3 Collection Process ..... 19
B. 4 Distribution ..... 22
B. 5 Maintenance ..... 22
B. 6 Uses ..... 23
C Breakdown Results ..... 23
C. 1 KGQA ..... 23
C. 2 SCV ..... 23
C. 3 KGCheck ..... 24
D Experimental Details ..... 25
D. 1 Construction of Knowledge Graph ..... 25
D. 2 Deployment of Open-Source LLMs ..... 25
D. 3 Experimental Setup ..... 25
D. 4 Prompt ..... 27
E Case Study ..... 35
E. 1 KGQA ..... 35
E. 2 SCV ..... 35
E. 3 KGCheck ..... 35
F Other Related Work ..... 66</p>
<h1>A Dataset and Code</h1>
<h2>A. 1 Dataset and Code Access</h2>
<p>Following NeurIPS Dataset and Benchmark Track guidelines, our data and code are available at https://github.com/westlake-autolab/BioKGBench under MIT license. More details are also documented at this address. The Croissant metadata record is available at https://huggingface. co/api/datasets/AutoLab-Westlake/BioKGBench-Dataset/croissant.</p>
<h2>A. 2 Responsibility Statement</h2>
<p>We declare that we assume full responsibility for any violations of rights. We confirm that the data used in this paper is properly licensed and ensure that its use complies with all relevant laws, regulations, and licensing agreements.</p>
<h2>B Datasheet</h2>
<p>Here, we provide a detailed description of our benchmark dataset, following the guidelines of the "Datasheet for Datasets" [75].</p>
<h2>B. 1 Motivation</h2>
<p>Our benchmark dataset was created to address the lack of benchmarks for evaluating biomedical agents from the perspective of an "AI scientists". In [39], it is stated that "AI scientists" can be realized as AI agents supported by humans, LLMs, ML models, and other tools like experimental platforms that cooperate to solve complex tasks. However, the current evaluation methods for biomedical agents remain unexplored, limited to simple question-answering tasks, which not only fail to avoid the hallucination problem inherent in solely relying on LLMs but also do not assess agents' abilities to utilize external tools and knowledge bases. Our proposed benchmark fills this gap by designing tasks ranging from easy to hard, based on two atomic capabilities: tool-query and memory-RAG. These tasks evaluate the agents' ability to leverage external support, including external knowledge and tools, when handling large-scale and multi-modal data. Moreover, Our data collected for KGCheck, the most challenging task, provides scenarios for agents to comprehend knowledge from heterogeneous sources and make discoveries.
The conception and construction of this dataset were jointly completed by the biomedical experts and AI researchers listed in the author list.</p>
<h2>B. 2 Composition</h2>
<p>We provide the necessary data for constructing the knowledge graph, literature for RAG, as well as the development and test data for KGQA, SCV, and KGCheck, where knowledge graph and literature are external knowledge sources provided for agent.
The knowledge graph is derived from a subset of Clinical Knowledge Graph (CKG) [32]. We specifically retain twelve key node types to ensure there is no loss of generality: Protein, Biological process (BP), Molecular function (MF), Cellular component (CC), Amino acid sequence, Tissue, Protein structure, Pathway, Modified protein, Modification, Disease, and Gene. The statistics of the triples in our knowledge graph are presented in Table 8. Detailed information stored in our knowledge graph is shown in Table 9.
Besides the knowledge graph, literature also serves as an external source of knowledge. We provide a corpus of 5,664 abstracts (under ODC-By 1.0) for SCV and 51 full papers for KGCheck. The 5,664 abstracts are sourced from existing datasets SciFact [54] (under CC BY 4.0) and PubMedQA [44] (under MIT license), while the 51 full papers, all of which are open access, were selected by experts based on entries in the IntAct [76] database and CKG. Table 10 summarizes the sources of the abstracts, and Figure 6 describes the literature with more details. Table 11 summarizes the sources of the 51 full papers, and Figure 7 provides more details about the literature.
For evaluation, we carefully collected 698, 1385, and 225 instances for KGQA, SCV, and KGCheck respectively. These datasets are split into development (dev) and test sets at an approximate ratio of</p>
<p>Table 8: The data statistics of our knowledge graph drawn from CKG.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Head Node</th>
<th style="text-align: center;">Tail Node</th>
<th style="text-align: center;">Relation</th>
<th style="text-align: center;">Number</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Protein</td>
<td style="text-align: center;">Protein_structure</td>
<td style="text-align: center;">HAS_STRUCTURE</td>
<td style="text-align: center;">271,512</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Amino_acid_sequence</td>
<td style="text-align: center;">HAS_SEQUENCE</td>
<td style="text-align: center;">20,598</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Cellular_component</td>
<td style="text-align: center;">ASSOCIATED_WITH</td>
<td style="text-align: center;">3,796,383</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Tissue</td>
<td style="text-align: center;">ASSOCIATED_WITH</td>
<td style="text-align: center;">7,117,321</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Disease</td>
<td style="text-align: center;">ASSOCIATED_WITH</td>
<td style="text-align: center;">5,882,437</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Molecular_function</td>
<td style="text-align: center;">ASSOCIATED_WITH</td>
<td style="text-align: center;">85,013</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Biological_process</td>
<td style="text-align: center;">ASSOCIATED_WITH</td>
<td style="text-align: center;">153,219</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Protein</td>
<td style="text-align: center;">ACTS_ON</td>
<td style="text-align: center;">985,376</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pathway</td>
<td style="text-align: center;">ANNOTATED_IN_PATHWAY</td>
<td style="text-align: center;">357,739</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Protein</td>
<td style="text-align: center;">CURATED_INTERACTS_WITH</td>
<td style="text-align: center;">3,448</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Modified_protein</td>
<td style="text-align: center;">HAS_MODIFIED_SITE</td>
<td style="text-align: center;">4,498</td>
</tr>
<tr>
<td style="text-align: center;">Disease</td>
<td style="text-align: center;">Disease</td>
<td style="text-align: center;">HAS_PARENT</td>
<td style="text-align: center;">16,058</td>
</tr>
<tr>
<td style="text-align: center;">Modified_protein</td>
<td style="text-align: center;">Protein</td>
<td style="text-align: center;">IS_SUBSTRATE_OF</td>
<td style="text-align: center;">6,633</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Modification</td>
<td style="text-align: center;">HAS_MODIFICATION</td>
<td style="text-align: center;">4,559</td>
</tr>
<tr>
<td style="text-align: center;">Gene</td>
<td style="text-align: center;">Protein</td>
<td style="text-align: center;">TRANSLATED_INTO</td>
<td style="text-align: center;">179,854</td>
</tr>
<tr>
<td style="text-align: center;">Biological_process</td>
<td style="text-align: center;">Biological_process</td>
<td style="text-align: center;">HAS_PARENT</td>
<td style="text-align: center;">49,081</td>
</tr>
<tr>
<td style="text-align: center;">Molecular_function</td>
<td style="text-align: center;">Molecular_function</td>
<td style="text-align: center;">HAS_PARENT</td>
<td style="text-align: center;">13,659</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Specific information of the 5,664 abstracts</p>
<p>1:10. The dev data is intended for users to debug and fine-tune their evaluation code, while the test data is reserved for the final assessment. Each instance includes both input and output (ground truth answer or label) pairs, with additional information to make the data easier to understand. The dataset for SCV is reconstructed from well-known existing datasets SciFact and PubMedQA, while the rest is self-contained. The dataset represents a carefully selected sample of instances from a larger set, ensuring a comprehensive and representative coverage of the key aspects.</p>
<h1>B. 3 Collection Process</h1>
<p>Biomedical experts and AI researchers listed in the author list were invloved in the data collection process. The collection process for different tasks varies:
KGQA. The collection process can be summarized into two steps: manually constructing question templates and automatically generating questions:</p>
<ul>
<li>Workflow for the Handcrafted Question Templates: The process commenced by selecting specific biomedical research fields and identifying relevant entity types and relationships from our knowledge graph. We defined various types of natural language questions, including one-hop questions, multi-hop questions, and conjunction questions (involving multiple entities). For each question, we created corresponding queries in both human-readable and machine-readable formats. These questions and queries, along with their associated metadata, such as question type and query structure, underwent rigorous peer reviews to ensure syntactic and semantic correctness.</li>
</ul>
<p>Table 9: Details of the information stored in the nodes of our knowledge graph.</p>
<table>
<thead>
<tr>
<th>Entity Type</th>
<th>Content</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td>Protein</td>
<td>name, id, accession, synonyms</td>
<td>{'name': 'PLEKHG6', 'id': 'Q3KR16', 'accession': 'PKHG6_HUMAN', 'synonyms': ['PKHG6_HUMAN', 'PLEKHG6', '9606.ENSP00000380185', 'ENSG00000008323'], 'taxid': '9606'}</td>
</tr>
<tr>
<td>Disease</td>
<td>name, description, id(DOID), type, synonyms</td>
<td>{'synonyms': ['sulfamethoxazole allergy', 'SMX allergy', 'SMZ allergy', 'sulphamethoxazole allergy'], 'name': 'sulfamethoxazole allergy', 'description': 'A drug allergy that has_allergic_trigger sulfamethoxazole. [url:https://www.ncbi.nlm.nih.gov/pubmed/7602118]', 'id': 'DOID:0040016', 'type': '-26'}</td>
</tr>
<tr>
<td>Protein structure</td>
<td>link, id, source</td>
<td>{'link': http://www.rcsb.org/structure/6XWD, 'id': '6XWD', 'source': 'Uniprot'}</td>
</tr>
<tr>
<td>Amino acid sequence</td>
<td>sequence, header, source, id, size</td>
<td>{'sequence': 'LRGAAGRLGGGLLVL', 'size': '15', 'header': 'trlA0A0A0MTA2lA0A0A0MTA2_HUMAN', 'source': 'UniProt', 'id': 'A0A0A0MTA2'}</td>
</tr>
<tr>
<td>Cellular component</td>
<td>name, description, id, type, synonyms</td>
<td>{'name': 'Golgi membrane', 'description': 'The lipid bilayer surrounding any of the compartments of the Golgi apparatus. [GOC:mah]', 'id': 'GO:0000139', 'type': '-21', 'synonyms': ['Golgi membrane']}</td>
</tr>
<tr>
<td>Molecular function</td>
<td>name, description, id(GO), type, synonyms</td>
<td>{'name': 'polymeric immunoglobulin binding', 'description': 'Interacting selectively and non-covalently with a J-chain-containing polymeric immunoglobulin of the IgA or IgM isotypes. [GOC:add, ISBN:0781735149]', 'id': 'GO:0001790', 'type': '-21', 'synonyms': ['polymeric immunoglobulin binding']}</td>
</tr>
<tr>
<td>Biological process</td>
<td>name, description, id(GO), type, synonyms</td>
<td>{'synonyms': ['mitochondrion inheritance'], 'name': 'mitochondrion inheritance', 'description': 'The distribution of mitochondria, including the mitochondrial genome, into daughter cells after mitosis or meiosis, mediated by interactions between mitochondria and the cytoskeleton. [GOC:mcc, PMID:10873824, PMID:11389764]', 'id': 'GO:0000001', 'type': '-21'}</td>
</tr>
<tr>
<td>Pathway</td>
<td>name, description, linkout, id, source</td>
<td>{'name': 'Antigen processing: Ubiquitination \&amp; Proteasome degradation', 'description': 'Antigen processing: Ubiquitination \&amp; Proteasome degradation', 'linkout': https: //reactome.org/PathwayBrowser/#/R-HSA-983168, 'id': 'R-HSA-983168', 'source': 'Reactome'}</td>
</tr>
<tr>
<td>Tissue</td>
<td>name, description, id, type, synonyms</td>
<td>{'name': 'stratum basale', 'description': 'The deepest layer, as of the epidermis or the endometrium. In the epidermis it is a single layer of cells. In the endometrium it provides the regenerative tissue after menstrual loss of the functional layer. [Dorlands_Medical_Dictionary:MerckMedicus]', 'id': 'BTO:0004680', 'type': '-25', 'synonyms': ['stratum basale', 'basal layer']}</td>
</tr>
<tr>
<td>Modified protein</td>
<td>sequence_window, protein, position, source, id, residue</td>
<td>{'sequence_window': 'MEPAPARsPRPQQDP', 'protein': 'P29590', 'position': '8', 'source': 'SIGNOR', 'id': 'P29590_S8-p', 'residue': 'S'}</td>
</tr>
<tr>
<td>Modification</td>
<td>synonyms, name, description, id, type</td>
<td>{'synonyms': ['Unimod', 'Source: "none"'], 'name': 'Unimod', 'description': 'Entry from Unimod. [PubMed:18688235]', 'id': 'MOD:00003', 'type': '-41'}</td>
</tr>
<tr>
<td>Gene</td>
<td>taxid, synonyms, name, id, family</td>
<td>{'taxid': '9606', 'synonyms': ['54843', 'ENSG00000137501', 'OTTHUMG00000166977', 'uc010rti.4', 'AJ303364'], 'name': 'synaptotagmin like 2', 'id': 'SYTL2', 'family': "'Protein phosphatase 1 regulatory subunits</td>
</tr>
</tbody>
</table>
<p>Table 10: The 5,664 papers come from 1,484 journals. Due to space limitations, we only list the names of journals with an IF greater than 70 and use 'others' to represent journals with an IF less than 70 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Journal</th>
<th style="text-align: left;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Nature reviews. Microbiology</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">CA: a cancer journal for clinicians</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">The Lancet. Infectious diseases</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">Nature reviews. Drug discovery</td>
<td style="text-align: left;">5</td>
</tr>
<tr>
<td style="text-align: left;">Nature reviews. Molecular cell biology</td>
<td style="text-align: left;">14</td>
</tr>
<tr>
<td style="text-align: left;">Nature reviews. Immunology</td>
<td style="text-align: left;">21</td>
</tr>
<tr>
<td style="text-align: left;">Lancet (London, England)</td>
<td style="text-align: left;">46</td>
</tr>
<tr>
<td style="text-align: left;">The New England journal of medicine</td>
<td style="text-align: left;">46</td>
</tr>
<tr>
<td style="text-align: left;">BMJ (Clinical research ed.)</td>
<td style="text-align: left;">90</td>
</tr>
<tr>
<td style="text-align: left;">JAMA</td>
<td style="text-align: left;">113</td>
</tr>
<tr>
<td style="text-align: left;">Nature medicine</td>
<td style="text-align: left;">138</td>
</tr>
<tr>
<td style="text-align: left;">others</td>
<td style="text-align: left;">5182</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: left;">5664</td>
</tr>
</tbody>
</table>
<p>Table 11: Journal distribution of the 51 full papers</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Journal</th>
<th style="text-align: left;">Count</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Brain research</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Molecular \&amp; cellular proteomics : MCP</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Aging cell</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Cell reports</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">PloS one</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Genes to cells : devoted to molecular \&amp; cellular mechanisms</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">EMBO reports</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">IUBMB life</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">The Journal of allergy and clinical immunology</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">The EMBO journal</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Open biology</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Nature communications</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Pigment cell \&amp; melanoma research</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Molecular biology of the cell</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Mobile DNA</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Journal of molecular biology</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Nutrients</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Biological research for nursing</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Genes \&amp; development</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Developmental cell</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Bone</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Cancers</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Animals : an open access journal from MDPI</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Nucleic acids research</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Molecular cell</td>
<td style="text-align: left;">2</td>
</tr>
<tr>
<td style="text-align: left;">Scientific reports</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">Proceedings of the National Academy of Sciences of the</td>
<td style="text-align: left;">3</td>
</tr>
<tr>
<td style="text-align: left;">United States of America</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Molecular and cellular biology</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">Cell</td>
<td style="text-align: left;">4</td>
</tr>
<tr>
<td style="text-align: left;">The Journal of biological chemistry</td>
<td style="text-align: left;">10</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: left;">51</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://news.agpt.co/
${ }^{2}$ https://gptr.dev/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
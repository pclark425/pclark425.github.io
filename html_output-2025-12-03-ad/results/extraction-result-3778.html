<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3778 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3778</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3778</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-89.html">extraction-schema-89</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-51c184bc40aa1960958a5950d3a9e2f5f1dc751a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/51c184bc40aa1960958a5950d3a9e2f5f1dc751a" target="_blank">LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> LLaMP is introduced, a multimodal retrieval-augmented generation (RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can dynamically and recursively interact with computational and experimental data on Materials Project (MP) and run atomistic simulations via high-throughput workflow interface.</p>
                <p><strong>Paper Abstract:</strong> Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences, where reliability and reproducibility are crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can dynamically and recursively interact with computational and experimental data on Materials Project (MP) and run atomistic simulations via high-throughput workflow interface. Without fine-tuning, LLaMP demonstrates strong tool usage ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structure and elastic tensor), and streamline complex tasks in computational materials and chemistry. We propose a simple metric combining uncertainty and confidence estimates to evaluate the self-consistency of responses by LLaMP and vanilla LLMs. Our benchmark shows that LLaMP effectively mitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli, electronic bandgaps, and formation energies that seem to derive from mixed data sources. We also demonstrate LLaMP's capability to edit crystal structures and run annealing molecular dynamics simulations using pre-trained machine-learning force fields. The framework offers an intuitive and nearly hallucination-free approach to exploring and scaling materials informatics, and establishes a pathway for knowledge distillation and fine-tuning other language models. Code and live demo are available at https://github.com/chiang-yuan/llamp</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3778.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3778.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal retrieval-augmented generation (RAG) framework that uses hierarchical ReAct agents to ground LLM reasoning/actions on high-fidelity materials informatics (Materials Project, arXiv, Wikipedia) and simulation tools without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMP (RAG framework) with GPT-4 / other LLM backends</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A hierarchical ReAct-agent orchestration layered on top of backbone LLMs (e.g., GPT-4, GPT-3.5, Gemini, Llama3) that performs function-calling, tool use, and retrieval from domain data stores; it is not a new transformer backbone but a tool-augmented agentic system.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Materials science (computational materials data, synthesis literature, crystallography, atomistic simulations)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction and assembly of domain knowledge and procedural rules (synthesis recipes, material-property relationships, modality-combined heuristics); supports hypothesis generation and data-grounded inference rather than explicit symbolic law derivation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-augmented generation (RAG) using hierarchical ReAct agents that call APIs (Materials Project, arXiv, Wikipedia), Python REPL, and simulation tools to fetch and aggregate literature- and computation-derived information; few-shot / prompt-driven reasoning without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Quantitative benchmarks on numeric property retrieval and classification (Precision, CoP, Confidence, SCoR, MAE, accuracy, R^2) comparing LLaMP to vanilla LLMs and domain baselines; qualitative case studies (synthesis recipe extraction, crystal editing, simulation workflows).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLaMP reduced hallucination and improved self-consistency (higher SCoR) and lower errors on material properties by grounding LLM outputs in authoritative data stores; it successfully extracted synthesis procedures, retrieved higher-order data (tensors, structures), and executed language-driven simulations, showing promise to support hypothesis formation and knowledge-grounded inference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Dependence on backbone LLM function-calling/reasoning; occasional schema-misunderstanding and tool-call errors (mitigated via ReAct assistants); correctness limited by the completeness and fidelity of external data sources (Materials Project) and by LLM backend variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3778.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3778.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tshitoyan2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised word embeddings capture latent knowledge from materials science literature</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study showing that unsupervised word-embedding models trained on materials-science literature encode latent chemical and materials knowledge, revealing correlations and heuristics emergent from large text corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised word embeddings capture latent knowledge from materials science literature</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unsupervised word embeddings (distributional semantic models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Shallow unsupervised distributional embedding models trained on a corpus of materials-science text to produce vector representations whose geometry reflects latent relationships between materials, properties, and synthesis contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Materials science literature (scientific publications and abstracts)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Induction of latent empirical regularities and heuristics (conceptual relationships between materials and properties) emergent from textual co-occurrence patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Unsupervised embedding learning (text mining) to surface latent associations from a large literature corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Analyses demonstrating meaningful nearest-neighbor relations, recovery of known material associations, and downstream tasks showing utility of embeddings; expert inspection in original work (cited here).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Demonstrated that embeddings trained on literature can capture nontrivial, domain-relevant associations (latent knowledge) that can serve as priors or hypotheses in materials discovery workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Embeddings encode corpus biases and statistical co-occurrence rather than causal mechanistic laws; interpretability and validation require domain expertise and further experimental/computational confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3778.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3778.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Polak&Morgan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based workflow that uses conversational LLMs to extract experimental parameters (e.g., cooling rates, yield strengths) from materials-science publications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Conversational language models (unspecified conversational LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt- and conversation-driven use of LLMs to parse and extract structured experimental parameters from text; the exact backbone is not specified in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Materials science experimental literature (metallic glasses, high-entropy alloys)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of experimental parameters and empirical data points (not explicit symbolic laws).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt engineering and conversational workflows to query LLMs for targeted data extraction from papers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in this paper's text; cited as an example of prompt workflows used to extract domain-specific experimental data.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Shown to be effective for extracting numeric experimental parameters in specific tasks, enabling downstream data aggregation for materials research.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prompt-based extraction can suffer from reproducibility issues, hallucination, and limited generalizability to broader or edge-case cases (as discussed in the present paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3778.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3778.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zheng2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An application that embeds and searches a preselected corpus of MOF synthesis papers and uses ChatGPT as an assistant to retrieve and summarize synthesis information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-based MOF synthesis assistant</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A retrieval + conversational LLM pipeline that uses embedding-based search over a curated set of MOF literature and ChatGPT for question answering and synthesis guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Metal-organic framework (MOF) synthesis literature</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>preselected set of MOF papers (size not specified here)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction and distillation of procedural synthesis knowledge and heuristics from literature (practical rules of thumb for synthesis).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Embedding-based retrieval over a curated corpus plus ChatGPT prompting to extract and summarize synthesis procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed here; cited as an example of embedding+LLM-assisted literature querying for synthesis extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Demonstrates embedding+LLM retrieval to provide synthesis assistance grounded in a selected literature subset.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reliance on curated paper set; prompt engineering and selection biases may affect reproducibility and completeness; still prone to hallucination if retrieval is inadequate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3778.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3778.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dagdelen2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured information extraction from scientific text with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work demonstrating the use of fine-tuned GPT-3 models to extract structured, domain-relevant information from scientific paragraphs in materials chemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structured information extraction from scientific text with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 model fine-tuned on domain-specific prompt-completion pairs (~500 pairs as cited) to improve structured information extraction performance from materials-science text.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Materials chemistry literature / scientific text</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td>â‰ˆ500 prompt-completion pairs used for fine-tuning (as reported in this paper's summary)</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Structured extraction of experimental and materials data (enabling downstream inference but not explicit symbolic law induction).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Fine-tuning a pre-trained LLM on curated prompt-completion pairs to improve extraction fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Reported performance improvements on extraction tasks relative to non-fine-tuned baselines (as summarized in the present paper).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuned GPT-3 improved the ability to extract structured materials-chemistry information from text, enabling assembly of datasets for downstream modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fine-tuning constrains up-to-dateness, may suffer from limited memory capacity, and models remain prone to forgetting and lack awareness of post-training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3778.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3778.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yang2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accurate Prediction of Experimental Band Gaps from Large Language Model-Based Data Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study that used GPT-4 to extract experimentally measured bandgap values from the literature and used the extracted dataset to train a graph neural network for bandgap prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accurate Prediction of Experimental Band Gaps from Large Language ModelBased Data Extraction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used for extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 prompted to read literature and extract tabulated experimental bandgap values for downstream machine-learning training.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Materials-science literature reporting experimental bandgaps</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical-data distillation to enable predictive modeling (indirectly supports discovery of empirical relationships between structure/composition and bandgap).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-based LLM extraction of experimental values followed by training a GNN on the assembled dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Evaluation of downstream predictive performance of the trained GNN on bandgap prediction benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>LLM-assisted data extraction (GPT-4) enabled creation of a training set that improved GNN performance on predicting experimental bandgaps.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Extraction quality depends on prompt design and retrieval fidelity; potential for extraction errors/hallucinations if records are ambiguous or LLM mis-parses text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3778.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3778.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jablonka2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leveraging large language models for predictive chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A work demonstrating that GPT-3 fine-tuned on online corpora can outperform purpose-trained models on tasks like classification, regression, and inverse design for alloys and molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging large language models for predictive chemistry</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (fine-tuned on online corpora)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 fine-tuned with domain-relevant text corpora to improve predictive performance on chemistry-related supervised tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Chemistry and materials (alloys and molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Derivation of predictive mappings and heuristics for chemical/material properties (behavior learned as predictive models rather than explicit symbolic laws).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Fine-tuning of a pre-trained LLM on domain corpora and supervised tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Benchmark comparisons showing fine-tuned GPT-3 outperforming some purpose-trained models on specific tasks (classification, regression, inverse design).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuned LLMs can encode domain predictive capabilities useful for chemistry/materials tasks, sometimes outperforming specialized models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fine-tuned LLMs may lack up-to-date awareness, can forget under continual training, and their internal representations of domain rules may be implicit and hard to interpret.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3778.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3778.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gruver2023</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-Tuned Language Models Generate Stable Inorganic Materials as Text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study showing that fine-tuned language models, trained on text-encoded atomistic representations, can generate candidate inorganic materials that satisfy structural/stability constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fine-Tuned Language Models Generate Stable Inorganic Materials as Text</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fine-tuned language models (unspecified backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Language models fine-tuned on text-encoded atomistic data to produce new candidate inorganic materials in textual representation constrained by structural rules.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Inorganic materials represented as text (atomistic descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Implicit generation rules for stable inorganic structures (learned constraints and heuristics for stability from training data).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Supervised fine-tuning on text-encoded crystal/atomistic data to induce generative rules.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Assessment of generated structures for stability/validity against domain constraints and possibly computational checks (as reported in that work).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Fine-tuned LMs can generate structurally plausible inorganic materials in text form, indicating learned generative heuristics for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Generative outputs require validation; model may produce artefacts or invalid structures without additional grounding or verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3778.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3778.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) or related AI systems being used to distill, extract, or induce qualitative laws, rules, or scientific principles from large collections of scholarly or scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kononova2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-mined dataset of inorganic materials synthesis recipes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset compiled by text-mining methods that extracts synthesis recipes for inorganic materials from the literature, enabling downstream data-driven synthesis analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text-mined dataset of inorganic materials synthesis recipes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Text-mining / NLP pipeline (not necessarily LLM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Automated NLP/text-mining pipeline used to extract synthesis steps, precursors, and conditions from a large body of inorganic synthesis literature to create a structured dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>input_domain</strong></td>
                            <td>Inorganic materials synthesis literature</td>
                        </tr>
                        <tr>
                            <td><strong>corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of procedural synthesis rules and recipe elements (practical heuristics for synthesis conditions and precursor choices).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Text-mining and information extraction pipeline to convert unstructured literature into a structured recipes dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Dataset curation and validation against source literature; used as resource for downstream analysis (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Provided a large structured corpus of synthesis recipes enabling analysis of synthesis heuristics and data-driven synthesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Extraction quality depends on text-mining heuristics; dataset may contain noisy or incomplete recipe elements requiring manual curation for high-stakes use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unsupervised word embeddings capture latent knowledge from materials science literature <em>(Rating: 2)</em></li>
                <li>Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering <em>(Rating: 2)</em></li>
                <li>Structured information extraction from scientific text with large language models <em>(Rating: 2)</em></li>
                <li>ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis <em>(Rating: 2)</em></li>
                <li>Accurate Prediction of Experimental Band Gaps from Large Language ModelBased Data Extraction <em>(Rating: 2)</em></li>
                <li>Leveraging large language models for predictive chemistry <em>(Rating: 2)</em></li>
                <li>Fine-Tuned Language Models Generate Stable Inorganic Materials as Text <em>(Rating: 2)</em></li>
                <li>Text-mined dataset of inorganic materials synthesis recipes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3778",
    "paper_id": "paper-51c184bc40aa1960958a5950d3a9e2f5f1dc751a",
    "extraction_schema_id": "extraction-schema-89",
    "extracted_data": [
        {
            "name_short": "LLaMP",
            "name_full": "Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval",
            "brief_description": "A multimodal retrieval-augmented generation (RAG) framework that uses hierarchical ReAct agents to ground LLM reasoning/actions on high-fidelity materials informatics (Materials Project, arXiv, Wikipedia) and simulation tools without fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMP (RAG framework) with GPT-4 / other LLM backends",
            "model_description": "A hierarchical ReAct-agent orchestration layered on top of backbone LLMs (e.g., GPT-4, GPT-3.5, Gemini, Llama3) that performs function-calling, tool use, and retrieval from domain data stores; it is not a new transformer backbone but a tool-augmented agentic system.",
            "input_domain": "Materials science (computational materials data, synthesis literature, crystallography, atomistic simulations)",
            "corpus_size": null,
            "law_type": "Extraction and assembly of domain knowledge and procedural rules (synthesis recipes, material-property relationships, modality-combined heuristics); supports hypothesis generation and data-grounded inference rather than explicit symbolic law derivation.",
            "distillation_method": "Retrieval-augmented generation (RAG) using hierarchical ReAct agents that call APIs (Materials Project, arXiv, Wikipedia), Python REPL, and simulation tools to fetch and aggregate literature- and computation-derived information; few-shot / prompt-driven reasoning without fine-tuning.",
            "evaluation_method": "Quantitative benchmarks on numeric property retrieval and classification (Precision, CoP, Confidence, SCoR, MAE, accuracy, R^2) comparing LLaMP to vanilla LLMs and domain baselines; qualitative case studies (synthesis recipe extraction, crystal editing, simulation workflows).",
            "results_summary": "LLaMP reduced hallucination and improved self-consistency (higher SCoR) and lower errors on material properties by grounding LLM outputs in authoritative data stores; it successfully extracted synthesis procedures, retrieved higher-order data (tensors, structures), and executed language-driven simulations, showing promise to support hypothesis formation and knowledge-grounded inference.",
            "limitations_or_challenges": "Dependence on backbone LLM function-calling/reasoning; occasional schema-misunderstanding and tool-call errors (mitigated via ReAct assistants); correctness limited by the completeness and fidelity of external data sources (Materials Project) and by LLM backend variability.",
            "uuid": "e3778.0",
            "source_info": {
                "paper_title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Tshitoyan2019",
            "name_full": "Unsupervised word embeddings capture latent knowledge from materials science literature",
            "brief_description": "A study showing that unsupervised word-embedding models trained on materials-science literature encode latent chemical and materials knowledge, revealing correlations and heuristics emergent from large text corpora.",
            "citation_title": "Unsupervised word embeddings capture latent knowledge from materials science literature",
            "mention_or_use": "mention",
            "model_name": "Unsupervised word embeddings (distributional semantic models)",
            "model_description": "Shallow unsupervised distributional embedding models trained on a corpus of materials-science text to produce vector representations whose geometry reflects latent relationships between materials, properties, and synthesis contexts.",
            "input_domain": "Materials science literature (scientific publications and abstracts)",
            "corpus_size": null,
            "law_type": "Induction of latent empirical regularities and heuristics (conceptual relationships between materials and properties) emergent from textual co-occurrence patterns.",
            "distillation_method": "Unsupervised embedding learning (text mining) to surface latent associations from a large literature corpus.",
            "evaluation_method": "Analyses demonstrating meaningful nearest-neighbor relations, recovery of known material associations, and downstream tasks showing utility of embeddings; expert inspection in original work (cited here).",
            "results_summary": "Demonstrated that embeddings trained on literature can capture nontrivial, domain-relevant associations (latent knowledge) that can serve as priors or hypotheses in materials discovery workflows.",
            "limitations_or_challenges": "Embeddings encode corpus biases and statistical co-occurrence rather than causal mechanistic laws; interpretability and validation require domain expertise and further experimental/computational confirmation.",
            "uuid": "e3778.1",
            "source_info": {
                "paper_title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Polak&Morgan",
            "name_full": "Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering",
            "brief_description": "A prompt-based workflow that uses conversational LLMs to extract experimental parameters (e.g., cooling rates, yield strengths) from materials-science publications.",
            "citation_title": "Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering",
            "mention_or_use": "mention",
            "model_name": "Conversational language models (unspecified conversational LLMs)",
            "model_description": "Prompt- and conversation-driven use of LLMs to parse and extract structured experimental parameters from text; the exact backbone is not specified in this paper's summary.",
            "input_domain": "Materials science experimental literature (metallic glasses, high-entropy alloys)",
            "corpus_size": null,
            "law_type": "Extraction of experimental parameters and empirical data points (not explicit symbolic laws).",
            "distillation_method": "Prompt engineering and conversational workflows to query LLMs for targeted data extraction from papers.",
            "evaluation_method": "Not detailed in this paper's text; cited as an example of prompt workflows used to extract domain-specific experimental data.",
            "results_summary": "Shown to be effective for extracting numeric experimental parameters in specific tasks, enabling downstream data aggregation for materials research.",
            "limitations_or_challenges": "Prompt-based extraction can suffer from reproducibility issues, hallucination, and limited generalizability to broader or edge-case cases (as discussed in the present paper).",
            "uuid": "e3778.2",
            "source_info": {
                "paper_title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Zheng2023",
            "name_full": "ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis",
            "brief_description": "An application that embeds and searches a preselected corpus of MOF synthesis papers and uses ChatGPT as an assistant to retrieve and summarize synthesis information.",
            "citation_title": "ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis",
            "mention_or_use": "mention",
            "model_name": "ChatGPT-based MOF synthesis assistant",
            "model_description": "A retrieval + conversational LLM pipeline that uses embedding-based search over a curated set of MOF literature and ChatGPT for question answering and synthesis guidance.",
            "input_domain": "Metal-organic framework (MOF) synthesis literature",
            "corpus_size": "preselected set of MOF papers (size not specified here)",
            "law_type": "Extraction and distillation of procedural synthesis knowledge and heuristics from literature (practical rules of thumb for synthesis).",
            "distillation_method": "Embedding-based retrieval over a curated corpus plus ChatGPT prompting to extract and summarize synthesis procedures.",
            "evaluation_method": "Not detailed here; cited as an example of embedding+LLM-assisted literature querying for synthesis extraction.",
            "results_summary": "Demonstrates embedding+LLM retrieval to provide synthesis assistance grounded in a selected literature subset.",
            "limitations_or_challenges": "Reliance on curated paper set; prompt engineering and selection biases may affect reproducibility and completeness; still prone to hallucination if retrieval is inadequate.",
            "uuid": "e3778.3",
            "source_info": {
                "paper_title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Dagdelen2024",
            "name_full": "Structured information extraction from scientific text with large language models",
            "brief_description": "Work demonstrating the use of fine-tuned GPT-3 models to extract structured, domain-relevant information from scientific paragraphs in materials chemistry.",
            "citation_title": "Structured information extraction from scientific text with large language models",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (fine-tuned)",
            "model_description": "GPT-3 model fine-tuned on domain-specific prompt-completion pairs (~500 pairs as cited) to improve structured information extraction performance from materials-science text.",
            "input_domain": "Materials chemistry literature / scientific text",
            "corpus_size": "â‰ˆ500 prompt-completion pairs used for fine-tuning (as reported in this paper's summary)",
            "law_type": "Structured extraction of experimental and materials data (enabling downstream inference but not explicit symbolic law induction).",
            "distillation_method": "Fine-tuning a pre-trained LLM on curated prompt-completion pairs to improve extraction fidelity.",
            "evaluation_method": "Reported performance improvements on extraction tasks relative to non-fine-tuned baselines (as summarized in the present paper).",
            "results_summary": "Fine-tuned GPT-3 improved the ability to extract structured materials-chemistry information from text, enabling assembly of datasets for downstream modeling.",
            "limitations_or_challenges": "Fine-tuning constrains up-to-dateness, may suffer from limited memory capacity, and models remain prone to forgetting and lack awareness of post-training data.",
            "uuid": "e3778.4",
            "source_info": {
                "paper_title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Yang2023",
            "name_full": "Accurate Prediction of Experimental Band Gaps from Large Language Model-Based Data Extraction",
            "brief_description": "A study that used GPT-4 to extract experimentally measured bandgap values from the literature and used the extracted dataset to train a graph neural network for bandgap prediction.",
            "citation_title": "Accurate Prediction of Experimental Band Gaps from Large Language ModelBased Data Extraction",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (used for extraction)",
            "model_description": "GPT-4 prompted to read literature and extract tabulated experimental bandgap values for downstream machine-learning training.",
            "input_domain": "Materials-science literature reporting experimental bandgaps",
            "corpus_size": null,
            "law_type": "Empirical-data distillation to enable predictive modeling (indirectly supports discovery of empirical relationships between structure/composition and bandgap).",
            "distillation_method": "Prompt-based LLM extraction of experimental values followed by training a GNN on the assembled dataset.",
            "evaluation_method": "Evaluation of downstream predictive performance of the trained GNN on bandgap prediction benchmarks.",
            "results_summary": "LLM-assisted data extraction (GPT-4) enabled creation of a training set that improved GNN performance on predicting experimental bandgaps.",
            "limitations_or_challenges": "Extraction quality depends on prompt design and retrieval fidelity; potential for extraction errors/hallucinations if records are ambiguous or LLM mis-parses text.",
            "uuid": "e3778.5",
            "source_info": {
                "paper_title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Jablonka2024",
            "name_full": "Leveraging large language models for predictive chemistry",
            "brief_description": "A work demonstrating that GPT-3 fine-tuned on online corpora can outperform purpose-trained models on tasks like classification, regression, and inverse design for alloys and molecules.",
            "citation_title": "Leveraging large language models for predictive chemistry",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (fine-tuned on online corpora)",
            "model_description": "GPT-3 fine-tuned with domain-relevant text corpora to improve predictive performance on chemistry-related supervised tasks.",
            "input_domain": "Chemistry and materials (alloys and molecules)",
            "corpus_size": null,
            "law_type": "Derivation of predictive mappings and heuristics for chemical/material properties (behavior learned as predictive models rather than explicit symbolic laws).",
            "distillation_method": "Fine-tuning of a pre-trained LLM on domain corpora and supervised tasks.",
            "evaluation_method": "Benchmark comparisons showing fine-tuned GPT-3 outperforming some purpose-trained models on specific tasks (classification, regression, inverse design).",
            "results_summary": "Fine-tuned LLMs can encode domain predictive capabilities useful for chemistry/materials tasks, sometimes outperforming specialized models.",
            "limitations_or_challenges": "Fine-tuned LLMs may lack up-to-date awareness, can forget under continual training, and their internal representations of domain rules may be implicit and hard to interpret.",
            "uuid": "e3778.6",
            "source_info": {
                "paper_title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Gruver2023",
            "name_full": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
            "brief_description": "A study showing that fine-tuned language models, trained on text-encoded atomistic representations, can generate candidate inorganic materials that satisfy structural/stability constraints.",
            "citation_title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
            "mention_or_use": "mention",
            "model_name": "Fine-tuned language models (unspecified backbone)",
            "model_description": "Language models fine-tuned on text-encoded atomistic data to produce new candidate inorganic materials in textual representation constrained by structural rules.",
            "input_domain": "Inorganic materials represented as text (atomistic descriptions)",
            "corpus_size": null,
            "law_type": "Implicit generation rules for stable inorganic structures (learned constraints and heuristics for stability from training data).",
            "distillation_method": "Supervised fine-tuning on text-encoded crystal/atomistic data to induce generative rules.",
            "evaluation_method": "Assessment of generated structures for stability/validity against domain constraints and possibly computational checks (as reported in that work).",
            "results_summary": "Fine-tuned LMs can generate structurally plausible inorganic materials in text form, indicating learned generative heuristics for stability.",
            "limitations_or_challenges": "Generative outputs require validation; model may produce artefacts or invalid structures without additional grounding or verification.",
            "uuid": "e3778.7",
            "source_info": {
                "paper_title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Kononova2019",
            "name_full": "Text-mined dataset of inorganic materials synthesis recipes",
            "brief_description": "A dataset compiled by text-mining methods that extracts synthesis recipes for inorganic materials from the literature, enabling downstream data-driven synthesis analysis.",
            "citation_title": "Text-mined dataset of inorganic materials synthesis recipes",
            "mention_or_use": "mention",
            "model_name": "Text-mining / NLP pipeline (not necessarily LLM-based)",
            "model_description": "Automated NLP/text-mining pipeline used to extract synthesis steps, precursors, and conditions from a large body of inorganic synthesis literature to create a structured dataset.",
            "input_domain": "Inorganic materials synthesis literature",
            "corpus_size": null,
            "law_type": "Extraction of procedural synthesis rules and recipe elements (practical heuristics for synthesis conditions and precursor choices).",
            "distillation_method": "Text-mining and information extraction pipeline to convert unstructured literature into a structured recipes dataset.",
            "evaluation_method": "Dataset curation and validation against source literature; used as resource for downstream analysis (as cited).",
            "results_summary": "Provided a large structured corpus of synthesis recipes enabling analysis of synthesis heuristics and data-driven synthesis planning.",
            "limitations_or_challenges": "Extraction quality depends on text-mining heuristics; dataset may contain noisy or incomplete recipe elements requiring manual curation for high-stakes use.",
            "uuid": "e3778.8",
            "source_info": {
                "paper_title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unsupervised word embeddings capture latent knowledge from materials science literature",
            "rating": 2
        },
        {
            "paper_title": "Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering",
            "rating": 2
        },
        {
            "paper_title": "Structured information extraction from scientific text with large language models",
            "rating": 2
        },
        {
            "paper_title": "ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis",
            "rating": 2
        },
        {
            "paper_title": "Accurate Prediction of Experimental Band Gaps from Large Language ModelBased Data Extraction",
            "rating": 2
        },
        {
            "paper_title": "Leveraging large language models for predictive chemistry",
            "rating": 2
        },
        {
            "paper_title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
            "rating": 2
        },
        {
            "paper_title": "Text-mined dataset of inorganic materials synthesis recipes",
            "rating": 1
        }
    ],
    "cost": 0.018253,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval</h1>
<p>Yuan Chiang ${ }^{1,2 * \dagger}$ Elvis Hsieh ${ }^{1 *}$ Chia-Hong Chou ${ }^{3}$ Janosh Riebesell ${ }^{2,4}$<br>${ }^{1}$ University of California, Berkeley, ${ }^{2}$ Lawrence Berkeley National Laboratory, ${ }^{3}$ Foothill College, ${ }^{4}$ Cavendish Laboratory, University of Cambridge, UK</p>
<h4>Abstract</h4>
<p>Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences, where reliability and reproducibility are crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and often biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of hierarchical reasoning-and-acting (ReAct) agents that can dynamically and recursively interact with computational and experimental data from the Materials Project (MP) and run atomistic simulations via high-throughput workflow interface. Without fine-tuning, LLaMP demonstrates strong tool-usage ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structure and elastic tensor), and streamline complex tasks in computational materials and chemistry. We propose a metric combining uncertainty and confidence estimates to evaluate the self-consistency of responses by LLaMP and vanilla LLMs. Our benchmark shows that LLaMP effectively mitigates the intrinsic bias in LLMs, counteracting the errors on bulk moduli, electronic bandgaps, and formation energies that seem to derive from mixed data sources. We also demonstrate LLaMP's capability to edit crystal structures and run annealing molecular dynamics simulations using pre-trained machine-learning interatomic potentials. The framework offers an intuitive and nearly hallucination-free approach to exploring and scaling materials informatics, and paves the way for future agentic scientific workflows and knowledge-grounded LLMs. Code and live demo are available at https://github.com/chiang-yuan/llamp.</p>
<h2>1 Introduction</h2>
<p>The generation of convincing yet unreliable information poses a pressing challenge to large language model (LLMs), particularly to their application in the sciences. LLMs are prone to hallucinationproviding outright false information with high confidence [1, 39]. This issue is particularly concerning for knowledge-intensive tasks, where users rely on chatbots and other AI systems to provide accurate guidance [16]. LLMs often lack up-to-date factual knowledge on topics outside their training data, requiring rigorous verification against trusted external sources [22]. In the scientific community, where the integration of insights and data accuracy is already complex, the proliferation of generative models may exacerbate the risk of misinformation. This trend accentuates the importance of scrutinizing and ensuring the reliability of information sources.</p>
<p>Current approaches to enhance LLM accuracy in domain-specific knowledge often involve finetuning pre-trained models [6, 10] or tailored prompt engineering techniques [41, 45]. While these</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>models are easy to deploy, they suffer from diminished reproducibility and data adherence due to the absence of a memory base, untraceable fine-tuning history, or opaque extraction processes. Even though fine-tuning can encode a certain amount of domain-specific knowledge into LLMs, it is constrained by scalability and intrinsic memory capacity. Fine-tuned LLMs struggle to retain in the long term the knowledge they were trained on as the training progresses, nor can they be aware of the up-to-date events and data beyond pre-training. Prompt engineering, while effective, also compromise the generalizability, thus limiting the overall power and flexibility of LLMs. Therefore, a more sensible approach involves equipping LLMs with external data sources, allowing them to generate holistic responses via few-shot adaptation to factual information [17] that can reliably support real-world scientific research and decision-making.</p>
<p>In this work, we propose LLaMP, a multimodal retrieval-augmented generation (RAG) framework leveraging hierarchical reasoning-and-acting (ReAct) agents to interact with Materials Project (MP), arXiv, Wikipedia, and atomistic simulation tools. The framework serves as a safeguard against LLM hallucination and grounds them on high-fidelity material informatics derived from various sources, including computational data from quantum-mechanical first-principles calculations and expert-curated material synthesis recipes [14], and further enables the possibility of language-driven simulations. Through hierarchical planning of multiple ReAct agents [42], we demonstrate that LLaMP not only can correctly retrieve higher-order materials data such as tensors and 3D crystal structures but also can combine different modalities to perform complex, knowledge-intensive inferences and operations essential for real-world research applications.</p>
<p>Our contributions are as follows: (1) we introduce a multimodal RAG framework employing hierarchical ReAct agents that dynamically interact with the Materials Project, enabling LLMs to access high-fidelity materials informatics; (2) we propose a statistical metric to assess the self-consistency of LLM responses in high-precision, reproducibility-critical settings; (3) we evaluate the performance of LLaMP and standard LLMs in predicting key material properties, including bulk moduli, electronic bandgaps, formation energies, and magnetic orderings; (4) we showcase real-world applications in materials science, such as inorganic synthesis and crystal structure generation and editing; (5) we enhance LLaMP with high-throughput atomistic simulation workflows and pre-trained universal ML force fields, lowering the entry barriers to computational materials and chemistry.</p>
<h1>2 Background</h1>
<p>Materials Project (MP) The Materials Project is a multi-institution effort to explore and compute the properties of all known inorganic materials [13] and molecules [31]. The initiative leverages high-throughput electronic structure calculations [15, 30] based on density functional theory (DFT), providing large-scale open-source database and analysis algorithms, with the ultimate goal to drastically reduce the time and cost required for materials discovery by focusing experiments on the promising candidates from computational screening. Most of the atomic structures are selected from the Inorganic Crystal Structure Database (ICSD) [43] and undergo standardized relaxation procedures, followed by post-processing or additional calculations for higher-order material properties such as electron and phonon bandgaps, elastic tensors, dielectric tensors, and more. MP provides these calculated material properties through API endpoints.
Natural language processing (NLP) in science NLP has found extensive application in extracting valuable information from scientific publications, with notable instances involving text-to-text or more recent image-to-text summarization techniques [10, 29, 34]. For summarizing crystal structures in textual form, Ganose and Jain [8] introduced the robocrystallographer, a toolkit designed for the analysis and generation of descriptions for crystalline materials. Their method condenses atomic structures into descriptive JSON representations that encompass coordination statistics, connectivity motifs, geometric features, and dimensionality. MP leverages robocrystallographer to generate human-level descriptions for 130 K compounds which are accessible through MP website and API.</p>
<h2>3 Related Work</h2>
<p>Prompting and fine-tuning in domain science Prompt-based methods have been used as effective tools for automating data extraction process from the literature. Polak and Morgan [28] employ a prompt workflow to extract the cooling rates of metallic glasses and yield strengths of high entropy</p>
<p>alloys. Zheng et al. [45] implement a ChatGPT metal-organic framework (MOF) synthesis assistant through embedding and searching on preselected papers. StructChem [26] leverages step-by-step reasoning, and iteratively refines results to solve college-level chemistry questions. Yang et al. [41] use GPT-4 to extract experimentally measured bandgaps to train a graph neural network for accurate bandgap prediction from crystal structures. Despite the success in the specific data extraction tasks, prompt-based methods face challenges in reproducibility when the used prompts are fine-grained to work for specific edge cases. They are also still prone to hallucination and less generalizable to combine different data sources due to the deliberately designed prompt.
Several other knowledge-grounded, domain-specific language models lean on the fine-tuning approach against pre-selected data and literature. For instance, ChemGPT [7] involves fine-tuning GPT-neo on self-referencing embedded strings (SELFIES) representations of small molecules. Jablonka et al. [12] demonstrated GPT-3 fine-tuned against online corpora could outperform purpose-trained models on classification, regression, and inverse design of high-entropy alloys and molecules. Dagdelen et al. [6] fine-tuned GPT-3 on $\sim 500$ prompt-completion pairs to enhance LLM's capability to extract useful information on materials chemistry from text paragraphs. However, the fine-tuned models without augmentation inherently lack awareness of the up-to-date results and any data only available after their training. Moreover, fine-tuned LLMs still suffer from limited memory retention and are prone to forget during continual training [35].
LLM function calling and tool usage An emerging class of LLM applications, including this work, take advantage of LLM text completion and instruction following capability for function calling. This approach extends LLMs with expert-curated tools to improve the quality of control for downstream applications. Coscientist [3] combines tools such as search engines, Python, and document index for autonomous chemical research. ChemCrow [21] gathers multiple molecule and safety tools to enhance organic chemistry experiment and molecule design.
However, most prior works adopt flat planning strategy, where a single agent accesses all the available tools, resulting in a lack of self-correcting tool usage capabilities. This often leads to premature reasoning stop and summarization when the agent encounters tool usage errors. We mitigate this through hierarchical planning of multiple ReAct agents (see Section 4.1).</p>
<h1>4 Method</h1>
<h3>4.1 Hierarchical orchestration</h3>
<p>Overviews To manage heterogeneous data sources and diverse types of queries, we introduce hierarchical planning, featuring the supervisor ReAct agent overseeing assistant ReAct agents (Figure 1). This design offers three major advantages over flat planning commonly implemented in previous works [3, 21]. (1) modularity of the system ensures that each assistant agent can focus on domain-specific queries while the supervisor agent handles higher-level reasoning and task allocation; (2) the hierarchical structure improves the overall accuracy and efficiency by reducing the cognitive load on any individual agent; (3) by offloading specific functions to specialized agents, we minimize the context window consumption and schema parsing.
Supervisor agent The supervisor agent acts as a router and decision-maker, handling abstract logic between user requests and assistant agents. Here, we adopt ReAct on GPT-4 [42] to augment the agent's action space $\mathcal{A}$ with a language space $\mathcal{L}$ to create an expanded action space of $\mathcal{A}=$ $\mathcal{A} \cup \mathcal{L}$. This expanded action space empowers the agent to take action $\tilde{a}_{t} \in \mathcal{L}$ in language space that facilitate the collaboration with assistant agents to retrieve domain specific information and achieve complex downstream tasks such as molecular dynamics simulations.
Assistant agent The efficient function calling in LLMs is often hindered by the need to process complex API schemas, which can consume a significant portion of the context window. To address this, we assign a specialized ReAct agent for each specific tool or API endpoint. It reduces context window consumption, as each agent handles only the relevant schema for its task, avoiding unnecessary schema parsing. Additionally, the use of ReAct agents enables them to refine their API calls based on feedback, significantly improving task completion rates through ReAct's iterative self-correcting mechanism.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Hierarchical ReAct agent planning in LLaMP. Two levels of agents are deployed using a standardized LangChain interface. Supervisor ReAct agent oversees assistant ReAct agents at the bottom-level, each equipped with distinct toolkits and data/document stores to accomplish various tasks, including high-fidelity materials information retrieval, atomistic modeling and simulations, and literature search. For a detailed example, refer to Figure A.1.</p>
<p>The full list of agents are defined in A.1. Each MP assistant agent employs a self-correcting mechanism, enabling agents to refine their API calls and improve task completion rates. The framework's modularity enable a seamless integration of new assistant agents, allowing for extensibility to various materials discovery methods and experimental techniques [20, 27, 36, 37, 44].</p>
<h1>4.2 Self-consistency of response (SCoR)</h1>
<p>When LLMs are integrated in scientific workflows and deployed in high-stakes settings (i.e. selfdriving labs), it is important for these models to have consistent and predictable behaviors [19]. For numeric knowledge retrieval tasks, we define the following metrics:
Precision measures the uncertainty in the model's responses where $n$ is the number valid responses from $N$ trials and $\hat{\sigma}$ is the standard deviation of valid response:</p>
<p>$$
\text { Precision }=\frac{\hat{\sigma}}{\sqrt{n}} \geq 0
$$</p>
<p>Coefficient of Precision (CoP) maps the precision to $(0,1]$ :</p>
<p>$$
\operatorname{CoP}=\exp (-\text { Precision })=\exp \left(-\frac{\hat{\sigma}}{\sqrt{n}}\right) \in(0,1]
$$</p>
<p>Confidence measures the ratio of generating $n$ valid responses in $N$ trials:</p>
<p>$$
\text { Confidence }=\frac{n}{N}
$$</p>
<p>Self-consistency of Response (SCoR) is then defined as</p>
<p>$$
\mathrm{SCoR}=\mathrm{CoP} \times \text { Confidence } \in[0,1]
$$</p>
<p>The limit of $\mathrm{SCoR}=1$ is reached when the model yields the same response to a given query every time. At the limit of $\mathrm{SCoR}=0$, the model is either very inconsistent (with large variance across the responses) or very reluctant (with low confidence) to answer the query. Despite the simplicity in definition, SCoR effectively reflects the reproducibility and practical usability of the method, which is important when the method is incorporated into broader systems where the stable and expected behaviors are prioritized.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: LLaMP RAG responses, baseline methods, and LLM intrinsic knowledge on material properties. (a) Bulk moduli, K, of 3d transition metals. (b) Formation energies, Î”H<sup>f</sup>, of common compounds. (c) Electronic bandgaps, E<sup>g</sup>, of common intrinsic semiconductors. (d) Electronic bandgaps of multi-element (ternary or quaternary) materials. Missing predictions are marked by shaded areas. Fliers are marked in circles. All LLaMP results use GPT-4 as backend language provider.</p>
<h2>5 Experiments</h2>
<h3>5.1 Multimodal ReAct Augmentation</h3>
<p>Materials design often involves multi-objective property optimization. These properties span a Pareto front where optimizing one factor incurs deterioration in others. To succeed in such tasks, combining different modalities of materials properties is necessary. LLaMP achieves this through the hierarchical orchestration of multiple ReAct agents [42]. For the example question "<em>What's the stiffest material with the lowest formation energy in Si-O system?</em>" (Figure A.1), when a query requires multimodal information and compound logic, the supervisor agent decomposes the query into multiple subtasks, delegates them to assistant agents (MPThermoExpert and MPElasticityExpert) for information retrieval, and in the final stage of reasoning integrates information from both modalities, drawing on the context in episodic memory retrieved from the assistant agents (Figure 1). This enables LLaMP to achieve various tasks step-by-step by combining multiple data sources from the Materials Project (MP) (e.g. 3D crystal structures, thermodynamic, mechanical, magnetic properties, and more listed in and Appendix A.1) in a single query. In fact, we found that flat planning implementation struggles to accomplish tool usages and RAG because the single agent sees too much information at once and often fails to follow the API schema (see Section 4.1 for details).</p>
<h3>5.2 Performance Benchmarks</h3>
<p><strong>Response quality and consistency</strong> We evaluate the performance of LLaMP, StructChem [26], Darwin [38], and vanilla LLMs (gpt-4, llama3-8b, gemini-1.0-pro) on material properties such as bulk modulus, formation energy, and bandgap (Figure 2, Table 1). Performance is assessed through Precision, CoP, SCoR, and MAE metrics, as defined in Section 4.2. We argue that any useful LLM agents to be included in the scientific workflow should have high SCoR and low error on the materials properties. Notably, LLaMP consistently outperforms other models, achieving the highest SCoR and the lowest errors across material properties, making it highly suitable for scientific</p>
<p>Table 1: Performance metrics of LLaMP and LLM baselines on the prediction of material properties. The metrics from left to right are precision (sample standard deviation), coefficient of precision (CoP), confidence, self-consistency of response (SCoR), and mean absolute error (MAE), where Materials Project are taken as the ground truth. All the tabulated values are the average metrics over five runs and the sampled materials. All LLaMP and StructChem results use GPT-4 as backend language provider.</p>
<p>| Bulk Modulus $K(\mathrm{GPa})$ | | | | | | Formation Energy $\Delta H_{f}(\mathrm{eV})$ | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>capability of backbone LLM: Claude-3.5-Sonnet (#1) $&gt;$ Gemini-1.5-Flash (#24) $&gt;$ and Llama3-8B (#46). The number following each model refers to its ranking on the Berkeley Function-Calling Leaderboard at the time of the experiment [40].
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Prediction of LLaMP, GPT-3.5, and GPT-4 on (a,b,d,e) magnetic orderings and (c,f) total magnetization per formula unit of randomly selected materials. Confusion matrix presents the number of entries in each class. Colormap represents the percentage of correct classification.</p>
<p>High-fidelity and higher-order data retrieval The challenge for LLMs in excelling at knowledge- and data-intensive tasks is well-documented [5, 11, 19]. Figure 3 shows the prediction of LLaMP, GPT-3.5, and GPT-4 on the magnetic orderings and total magnetization of 800 materials randomly selected from all unary, binary, and ternary compounds in MP. Our result indicates that without RAG, vanilla LLMs suffer from hallucinations and misclassify the magnetic orderings of materials. LLaMP with GPT-4 as backend can counteract the intrinsic bias of GPT models, increasing the classification accuracy to 0.98 and $R^{2}$ of magnetization prediction to 0.992 (Table 2). We note that GPT-3.5 as backend, while effective for classification and other information retrieval tasks, struggles to distinguish total_magnetization from magnetization_per_formula_unit in magnetism API schema and often requests the wrong field and forgets to normalize the values. In the magnetic orderings queries, LLaMP with GPT-3.5 as backend fails to distinguish ferromagnetic (FM) and ferrimagnetic (FiM) orderings, while LLaMP with GPT-4 as backend gracefully separates the two classes (Figure 3a, d).</p>
<p>We further test the capability of LLaMP and LLMs for higher-order data (such as tensors, 3D crystal structures, curves). As shown in Table B5.2, GPT-3.5 hallucinates the values for the components in the elastic tensor of NaCl , with serious erroneous values such as $C_{11}=$ 289.2 GPa-a significant deviation from DFTcalculated values ( 76 GPa ). It also omits the values for $C_{22}, C_{33}, C_{55}, C_{66}$ and fails to represent the full elastic tensor in a matrix format, despite the query explicitly requesting the full elastic tensor. This hightlights the limitation of</p>
<p>Table 2: Prediction performance of LLaMP, GPT3.5, and GPT-4 on magnetic orderings and magnetization. LLaMP with GPT-4 and GPT-3.5 as backend LLM are compared.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Magnetic Ordering</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Magnetization</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">MAE</td>
<td style="text-align: center;">$R^{2}$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMP (GPT-4)</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">0.045</td>
<td style="text-align: center;">0.992</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0.48</td>
<td style="text-align: center;">0.26</td>
<td style="text-align: center;">1.611</td>
<td style="text-align: center;">$-0.201$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMP (GPT-3.5)</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">1.896</td>
<td style="text-align: center;">0.407</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.18</td>
<td style="text-align: center;">1.988</td>
<td style="text-align: center;">$-0.024$</td>
</tr>
</tbody>
</table>
<p>intrsinic knowledge in LLMs to recall higherorder, more complex data for more comprehensive, holistic response.</p>
<h1>5.3 Real-world Applications</h1>
<p>Inorganic synthesis recipes Empowered by the MP synthesis endpoint, LLaMP can extract synthesis recipes and summarize detailed step-by-step procedures grounded on real experimental papers with associated DOI references, as demonstrated in the example queries (Table B5.9 and B5.10). Vanilla GPT-3.5 gives a seemingly correct and verbose synthesis procedure for $\mathrm{YMnO}<em 2="2">{3}$ in Table B5.9, inferring possible reaction pathways from two common oxides as precursors $\left(\mathrm{Y}</em>} \mathrm{O<em 2="2">{3}\right.$ and $\mathrm{MnO}</em>}$ ). However, it pulls irrelevant lithium compounds $\left(\mathrm{Li<em 3="3">{2} \mathrm{CO}</em>\right.$ and LiOH$)$ into the recipe and overlooks the fact that metathesis reactions $[18,33]$ require less applied energy than high-temperature sintering, which relies on solid-state diffusion [23]. Vanilla GPT-3.5 also exhibits uncertainty about specific synthesis details, such as heating temperature, duration, cooling rate, etc.
Consider the example of $\mathrm{LiFePO}<em 4="4">{4}$ presented in Table B5.10. Explicit instruction is provided: "Please provide a detailed step-by-step procedure and reference." While GPT-3.5 does offer both a procedure and reference as asked, and the reference is indeed associated with a real paper, the paper itself contains no information about the synthesis procedure of $\mathrm{LiFePO}</em>$. The procedure listed in Table B5.10 is dissociated from the title and is hallucinated from the pre-training corpus.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Generation and manipulation of crystal structures using LLMs to insert an additional lithium atom at the interstitial site in diamond cubic silicon structure. Blue: Si. Green: Li. Questionanswer pairs are listed in Table B5.11. Additional atoms extended through bonds are visualized.</p>
<p>Table 3: Structural parameters of the generated crystals compared with diamond cubic (DC) silicon. From left to right are fractional coordinates of inserted Li atom $(x, y, z)<em _mathrm_SiSi="\mathrm{SiSi">{\mathrm{Li}}$, total cell volume $V$, average $\mathrm{Si}-\mathrm{Si}$ bond lengths $\ell</em>$. GPT-4 refuses to respond due to their safeguard against the lack of atomic structure information.}}, \mathrm{Si}-\mathrm{Si}-\mathrm{Si}$ angles $\theta_{\mathrm{SiSiSi}}$, and $\mathrm{Si}-\mathrm{Li}-\mathrm{Si}$ angles $\theta_{\mathrm{SiLiSi}</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$(x, y, z)_{\mathrm{Li}}$</th>
<th style="text-align: center;">$\ell_{\mathrm{SiSi}}(\AA)$</th>
<th style="text-align: center;">Error (\%)</th>
<th style="text-align: center;">$V\left(\AA^{3}\right)$</th>
<th style="text-align: center;">Error (\%)</th>
<th style="text-align: center;">$\theta_{\mathrm{SiSiSi}}\left({ }^{\circ}\right)$</th>
<th style="text-align: center;">Error (\%)</th>
<th style="text-align: center;">$\theta_{\mathrm{SiLiSi}}\left({ }^{\circ}\right)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LLaMP</td>
<td style="text-align: center;">$(0.5,0.5,0.5)$</td>
<td style="text-align: center;">2.36</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">40.33</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">109.47</td>
<td style="text-align: center;">0.0</td>
<td style="text-align: center;">62.96</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">$(0.5,0.5,0.5)$</td>
<td style="text-align: center;">2.71</td>
<td style="text-align: center;">$+15.0$</td>
<td style="text-align: center;">67.05</td>
<td style="text-align: center;">$+66.3$</td>
<td style="text-align: center;">98.28</td>
<td style="text-align: center;">$-10.2$</td>
<td style="text-align: center;">67.69</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">DC Si (mp-149)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">2.36</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">40.33</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">109.47</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>RAG-assisted crystal generation and editing Fine-tuned LLMs for text-encoded atomistic information have shown the capability to generate stable crystals under the constraints of atomic positions and charges [9]. In this context, we delve into the examination and comparison of the crystal generation capabilities between LLaMP and GPT-3.5, without resorting to fine-tuning or tailored prompt messages in previous work. Figure 4 showcases the structures generated by LLaMP and vanilla GPT-3.5 without RAG, both instructed to insert one lithium atom at the tetrahedral site of the diamond cubic silicon structure (Table B5.11). Notably, both LLaMP and GPT-3.5 place an additional Li atom at fractional coordinate $(0.5,0.5,0.5)$. However, the Si structure retrieved by LLaMP adheres to the MP convention, positioning two Si bases at $(0.125,0.125,0.125)$ and $(0.875,0.875,0.875)$. This causes the inserted Li atom to be hexagonal interstitial instead of tetrahedral interstitial.
GPT-3.5 locates the Li atom at the tetrahedral site given the "luckily chosen" Si bases at $(0,0,0)$ and $(0.25,0.25,0.25)$; however, the resulting cell volume and shape are highly distorted, and the $\mathrm{Si}-\mathrm{Si}$</p>
<p>bond length and $\mathrm{Si}-\mathrm{Si}-\mathrm{Si}$ angle deviate significantly from the ground truth (Table 3), highlighting the limitations in the intrinsic encoding of LLMs for atomistic information and the challenges associated with zero-shot generation of crystal structures. In contrast, the LLaMP-retrieved MP structure serves as a robust prior, anchoring the lattice parameters of the generated structure to the correct values.</p>
<p>Language-driven simulation LLaMP equipped with Python REPL and atomistic simulation workflow package atomate2 performs well out of the box for complex multi-step simulations using pre-trained universal machine learning interatomic potential MACE-MP-0 [2] through language instruction. As demonstrated in Appendix C.1 and Appendix C.2, LLaMP is able to follow multi-step instruction to fetch stable crystal structure from MP, generate a supercell of atomic structure, and run annealing molecular dynamics simulation with varying temperature from 300 K to 800 K and back to 300 K . After the simulation is finished, LLaMP can read the simulation trajectories and plot the temperature profile over time (Appendix C.1).</p>
<h1>6 Discussion</h1>
<p>Robustness The hierarchical ReAct framework implemented here is essentially a graph of agents, or language graph, with one central node (supervisor) in connection with many satellite nodes (assistants). The implementation of ReAct for the assistant agents enables self-correcting tool usages and fortifies the robustness of data retrieval. As presented in Figure A.1c, MPThermoExpert initially misunderstood the schema at the first trial and filled in the formula field with $\mathrm{Si}-\mathrm{O}$, an invalid input but a valid one for chemical system (chemsys) field. The observation step (step 4) allows MPThermoExpert to handle exceptions and to refine the correct input fields after adaptation (step 6). Storing (Retrieving) question-answer and query-argument pairs to (from) vector databases could further reduce the number of trial-and-error steps, and the stored pairs can be used to refine foundation LLMs to improve function calling quality.
Limitation We recognize the effectiveness of LLaMP's framework relies on backbone LLM's function calling and reasoning capabilities. Sometimes LLMs misunderstand the description of schemas and therefore yield unexpected behaviors. For example, sort_fields argument allows sorting the returned documents in ascending order or descending order if the field is prefixed with - , but LLMs sometimes mistake the sign and sort in the opposite order. The correctness of LLaMP is also subject to the quality of theoretical prediction and the comprehensiveness of the data in MP. Other than the underpredicted bandgaps by GGA functional, MP's ongoing effort to search all possible magnetic configurations is also not complete. Most of the existing calculations in MP start from high-spin ferromagnetic configurations, which may overlook many antiferromagnetic ground states below the current energy convex hull. While MP is one of the most comprehensive materials databases, the available crystal structures on MP are not exhaustive but continuously expanding [24]. Furthermore, Kohn-Sham DFT theory is insufficient in some cases, and a higher level of theory is needed. Currently LLaMP only supports a few atomate2 workflows with machine learning force fields and VASP calculations. More diverse electronic calculation methods and workflows will be supported in the future work.</p>
<p>Summary We present a hierarchical agentic framework, LLaMP, based on ReAct to extract and manipulate material informatics through few-shot generalization. By grounding thoughts and actions with high-fidelity information, LLaMP showcases the ability to integrate various modalities of material properties and perform logical inferences to accomplish assigned tasks, all without the need for fine-tuning. In essence, the proposed LangChain framework holds the potential to expand its applicability to multiple data sources, encompassing both theoretical computations and experimental data, and real-world laboratories by incorporating additional assistant agents for data retrieval and robot control. LLaMP functions as a knowledge-aware agent, empowering users to navigate and manipulate complex materials informatics. In the context of self-driving labs [3, 32], LLM agents with multimodal data sources, sensors, and actors may improve their decision making and operation [25]. As new tools continue to emerge, there is an exciting avenue for further exploration to ascertain if this framework can effectively facilitate scientific hypothesis generation and guide data-driven experiments.</p>
<h1>References</h1>
<p>[1] Y. Bang et al. A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity, Nov. 2023. URL http://arxiv.org/abs/2302.04023. arXiv:2302.04023 [cs].
[2] I. Batatia et al. A foundation model for atomistic materials chemistry, Dec. 2023. URL http: //arxiv.org/abs/2401.00096. arXiv:2401.00096 [cond-mat, physics:physics].
[3] D. A. Boiko, R. MacKnight, B. Kline, and G. Gomes. Autonomous chemical research with large language models. Nature, 624(7992):570-578, Dec. 2023. ISSN 14764687. doi: 10.1038/s41586-023-06792-0. URL https://www.nature.com/articles/ s41586-023-06792-0. Number: 7992 Publisher: Nature Publishing Group.
[4] P. Borlido et al. Exchange-correlation functionals for band gaps of solids: benchmark, reparametrization and machine learning. npj Computational Materials, 6(1):1-17, July 2020. ISSN 2057-3960. doi: 10.1038/s41524-020-00360-0. URL https://www.nature.com/ articles/s41524-020-00360-0. Number: 1 Publisher: Nature Publishing Group.
[5] K. Cobbe et al. Training Verifiers to Solve Math Word Problems, Nov. 2021. URL http: //arxiv.org/abs/2110.14168. arXiv:2110.14168 [cs].
[6] J. Dagdelen et al. Structured information extraction from scientific text with large language models. Nature Communications, 15(1):1418, Feb. 2024. ISSN 2041-1723. doi: 10.1038/ s41467-024-45563-x. URL https://www.nature.com/articles/s41467-024-45563-x. Publisher: Nature Publishing Group.
[7] N. Frey et al. Neural Scaling of Deep Chemical Models, May 2022. URL https://chemrxiv. org/engage/chemrxiv/article-details/627bddd544bdd532395fb4b5.
[8] A. M. Ganose and A. Jain. Robocrystallographer: automated crystal structure text descriptions and analysis. MRS Communications, 9(3):874-881, Sept. 2019. ISSN 2159-6867. doi: 10. 1557/mrc.2019.94. URL https://doi.org/10.1557/mrc.2019.94.
[9] N. Gruver et al. Fine-Tuned Language Models Generate Stable Inorganic Materials as Text. Nov. 2023. URL https://openreview.net/forum?id=0r5DE2Z5wJ.
[10] T. Gupta, M. Zaki, N. M. A. Krishnan, and Mausam. MatSciBERT: A materials domain language model for text mining and information extraction. npj Computational Materials, 8(1):1-11, May 2022. ISSN 2057-3960. doi: 10.1038/s41524-022-00784-w. URL https://www.nature.com/articles/s41524-022-00784-w. Number: 1 Publisher: Nature Publishing Group.
[11] D. Hendrycks et al. Measuring Massive Multitask Language Understanding, Jan. 2021. URL http://arxiv.org/abs/2009.03300. arXiv:2009.03300 [cs].
[12] K. M. Jablonka, P. Schwaller, A. Ortega-Guerrero, and B. Smit. Leveraging large language models for predictive chemistry. Nature Machine Intelligence, 6(2):161-169, Feb. 2024. ISSN 2522-5839. doi: 10.1038/s42256-023-00788-1. URL https://www.nature.com/articles/ s42256-023-00788-1. Publisher: Nature Publishing Group.
[13] A. Jain et al. Commentary: The Materials Project: A materials genome approach to accelerating materials innovation. APL Materials, 1(1):011002, July 2013. ISSN 2166-532X. doi: 10.1063/1.4812323. URL https://doi.org/10.1063/1.4812323.
[14] O. Kononova et al. Text-mined dataset of inorganic materials synthesis recipes. Scientific Data, 6(1):203, Oct. 2019. ISSN 2052-4463. doi: 10.1038/s41597-019-0224-1. URL https:// www.nature.com/articles/s41597-019-0224-1. Number: 1 Publisher: Nature Publishing Group.
[15] G. Kresse and J. FurthmÃ¼ller. Efficient iterative schemes for ab initio total-energy calculations using a plane-wave basis set. Physical Review B, 54(16):11169-11186, Oct. 1996. doi: 10.1103/PhysRevB.54.11169. URL https://link.aps.org/doi/10.1103/PhysRevB.54. 11169. Publisher: American Physical Society.</p>
<p>[16] P. Lewis et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In Advances in Neural Information Processing Systems, volume 33, pages 9459-9474. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html.
[17] P. Lewis et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, Apr. 2021. URL http://arxiv.org/abs/2005.11401. arXiv:2005.11401 [cs] version: 4.
[18] J. Li et al. True Composition and Structure of Hexagonal "YAlO3", Actually Y3Al3O8CO3. Inorganic Chemistry, 54(3):837-844, Feb. 2015. ISSN 0020-1669. doi: 10.1021/ic502027k. URL https://doi.org/10.1021/ic502027k. Publisher: American Chemical Society.
[19] P. Liang et al. Holistic Evaluation of Language Models, Oct. 2023. URL http://arxiv.org/ abs/2211.09110. arXiv:2211.09110 [cs].
[20] Y. Luo, C. Liu, and S. Ji. Towards Symmetry-Aware Generation of Periodic Materials, July 2023. URL https://arxiv.org/abs/2307.02707v2.
[21] A. M. Bran et al. Augmenting large language models with chemistry tools. Nature Machine Intelligence, 6(5):525-535, May 2024. ISSN 2522-5839. doi: 10.1038/s42256-024-00832-8. URL https://www.nature.com/articles/s42256-024-00832-8. Publisher: Nature Publishing Group.
[22] A. Mallen et al. When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories, July 2023. URL http://arxiv.org/abs/2212.10511. arXiv:2212.10511 [cs].
[23] A. L. Maximenko and E. A. Olevsky. Effective diffusion coefficients in solid-state sintering. Acta Materialia, 52(10):2953-2963, June 2004. ISSN 1359-6454. doi: 10.1016/ j.actamat.2004.02.042. URL https://www.sciencedirect.com/science/article/pii/ S1359645404001326.
[24] A. Merchant et al. Scaling deep learning for materials discovery. Nature, pages 1-6, Nov. 2023. ISSN 1476-4687. doi: 10.1038/s41586-023-06735-9. URL https://www.nature. com/articles/s41586-023-06735-9. Publisher: Nature Publishing Group.
[25] S. Miret and N. M. A. Krishnan. Are LLMs Ready for Real-World Materials Discovery?, Sept. 2024. URL http://arxiv.org/abs/2402.05200. arXiv:2402.05200 [cond-mat].
[26] S. Ouyang et al. Structured Chemistry Reasoning with Large Language Models, Feb. 2024. URL http://arxiv.org/abs/2311.09656. arXiv:2311.09656 [cs].
[27] G. Pilania, J. E. Gubernatis, and T. Lookman. Multi-fidelity machine learning models for accurate bandgap predictions of solids. Computational Materials Science, 129:156163, Mar. 2017. ISSN 0927-0256. doi: 10.1016/j.commatsci.2016.12.004. URL https: //www.sciencedirect.com/science/article/pii/S0927025616306188.
[28] M. P. Polak and D. Morgan. Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering, June 2023. URL http://arxiv. org/abs/2303.05352. arXiv:2303.05352 [cond-mat].
[29] A. Radford et al. Learning Transferable Visual Models From Natural Language Supervision, Feb. 2021. URL http://arxiv.org/abs/2103.00020. arXiv:2103.00020 [cs].
[30] Y. Shao et al. Advances in molecular quantum chemistry contained in the Q-Chem 4 program package. Molecular Physics, 113(2):184-215, Jan. 2015. ISSN 0026-8976. doi: 10. 1080/00268976.2014.952696. URL https://www.tandfonline.com/doi/full/10.1080/ 00268976.2014 .952696 . Publisher: Taylor \&amp; Francis.
[31] E. W. C. Spotte-Smith et al. A database of molecular properties integrated in the Materials Project, Oct. 2023. URL https://chemrxiv.org/engage/chemrxiv/article-details/ 651a3302ade1178b247ee6d1.</p>
<p>[32] N. J. Szymanski et al. An autonomous laboratory for the accelerated synthesis of novel materials. Nature, 624(7990):86-91, Dec. 2023. ISSN 1476-4687. doi: 10.1038/ s41586-023-06734-w. URL https://www.nature.com/articles/s41586-023-06734-w. Number: 7990 Publisher: Nature Publishing Group.
[33] P. K. Todd et al. Selectivity in Yttrium Manganese Oxide Synthesis via Local Chemical Potentials in Hyperdimensional Phase Space. Journal of the American Chemical Society, 143(37):15185-15194, Sept. 2021. ISSN 0002-7863. doi: 10.1021/jacs.1c06229. URL https://doi.org/10.1021/jacs.1c06229. Publisher: American Chemical Society.
[34] V. Tshitoyan et al. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature, 571(7763):95-98, July 2019. ISSN 1476-4687. doi: 10.1038/ s41586-019-1335-8. URL https://www.nature.com/articles/s41586-019-1335-8. Number: 7763 Publisher: Nature Publishing Group.
[35] W. Wang et al. Augmenting Language Models with Long-Term Memory, June 2023. URL http://arxiv.org/abs/2306.07174. arXiv:2306.07174 [cs].
[36] M. Wen et al. Chemical reaction networks and opportunities for machine learning. Nature Computational Science, 3(1):12-24, Jan. 2023. ISSN 2662-8457. doi: 10.1038/ s43588-022-00369-z. URL https://www.nature.com/articles/s43588-022-00369-z. Publisher: Nature Publishing Group.
[37] M. Wen et al. An equivariant graph neural network for the elasticity tensors of all seven crystal systems. Digital Discovery, 3(5):869-882, May 2024. ISSN 2635-098X. doi: 10. 1039/D3DD00233K. URL https://pubs.rsc.org/en/content/articlelanding/2024/ dd/d3dd00233k. Publisher: RSC.
[38] T. Xie et al. DARWIN Series: Domain Specific Large Language Models for Natural Science, Aug. 2023. URL http://arxiv.org/abs/2308.13565. arXiv:2308.13565 [cond-mat, physics:physics].
[39] Z. Xu, S. Jain, and M. Kankanhalli. Hallucination is Inevitable: An Innate Limitation of Large Language Models, Jan. 2024. URL http://arxiv.org/abs/2401.11817. arXiv:2401.11817 [cs].
[40] F. Yan et al. Berkeley Function Calling Leaderboard. 2024. URL https://gorilla.cs. berkeley.edu/blogs/8.berkeley.function.calling.leaderboard.html.
[41] S. Yang et al. Accurate Prediction of Experimental Band Gaps from Large Language ModelBased Data Extraction. Nov. 2023. URL https://openreview.net/forum?id=oRKWhmtUG6.
[42] S. Yao et al. ReAct: Synergizing Reasoning and Acting in Language Models, Mar. 2023. URL http://arxiv.org/abs/2210.03629. arXiv:2210.03629 [cs].
[43] D. Zagorac et al. Recent developments in the Inorganic Crystal Structure Database: theoretical crystal structure data and related features. Journal of Applied Crystallography, 52 (5):918-925, Oct. 2019. ISSN 1600-5767. doi: 10.1107/S160057671900997X. URL https://journals.iucr.org/j/issues/2019/05/00/in5024/. Publisher: International Union of Crystallography.
[44] C. Zeni et al. MatterGen: a generative model for inorganic materials design, Jan. 2024. URL http://arxiv.org/abs/2312.03687. arXiv:2312.03687 [cond-mat].
[45] Z. Zheng et al. ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis. Journal of the American Chemical Society, 145(32):18048-18062, Aug. 2023. ISSN 00027863, 1520-5126. doi: 10.1021/jacs.3c05819. URL http://arxiv.org/abs/2306.11296. arXiv:2306.11296 [cond-mat, physics:physics].</p>
<h1>A Supplementary Information</h1>
<h2>A. 1 List of Implemented Assistant Agents and Tools</h2>
<p>Here we provide the comprehensive list of implemented assistant agents and tools. Note that MP Assistants are highly modular so it is very trivial to support extra API endpoints from https://api.materialsproject.org/docs.</p>
<ul>
<li>MPSummaryExpert: summary provides amalgamated data for a material by combining subsets of data from many of the other API endpoints.</li>
<li>MPThermoExpert: thermo provides computed thermodynamic data for a material such as formation energy and energy above hull.</li>
<li>MPElasticityExpert: elasticity provides bulk, shear, and Young's modulus, poisson ratio, and universal anisotropy index.</li>
<li>MPMagnetismExpert: magnetism provides computed magnetic ordering related data.</li>
<li>MPDielectricExpert: dielectric provides computed dielectric data from density functional perturbation theory.</li>
<li>MPPiezoelectricExpert: piezoelectric provides computed piezoelectric data from density functional perturbation theory.</li>
<li>MPElectronicExpert: electronic_structure provides computed electronic structure related data for a material such as band gap and fermi level. Python objects for line-mode band structures, density of states, and fermi surfaces are also available.</li>
<li>MPSynthesisExpert: synthesis provides a synthesis recipes for materials extracted from literature using text mining and natural language processing techniques.</li>
<li>MPStructureRetriever: MaterialsStructureText fetches and saves pymatgen Structure objects to local JSON files.</li>
<li>MLFFAgent: MLFFMD runs molecular dynamics simulations using pre-trained machine learning force fields; MLFFElastic calculates the elastic constants of a given material using pre-trained machine learning force fields.</li>
<li>PythonREPLTool: Python REPL that LLMs could run the generated script.</li>
<li>ArxivQueryRun: LangChain built-in tool that LLMs can use to send API request to ArXiv.</li>
<li>WikipediaQueryRun: LangChain built-in tool that LLMs can use to send API reqeust to Wikipedia.</li>
</ul>
<p>Table 4: Performance of LLaMP with different backbone LLMs and ablation of ReAct agent with MP API and SerpAPI tools.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Bulk Moduli $K(\mathrm{GPa})$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Formation Energies $\Delta H_{f}(\mathrm{eV})$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Precision $\downarrow$</td>
<td style="text-align: center;">CoP</td>
<td style="text-align: center;">Confidence</td>
<td style="text-align: center;">SCoR $\uparrow$</td>
<td style="text-align: center;">MAE $\downarrow$</td>
<td style="text-align: center;">Precision $\downarrow$</td>
<td style="text-align: center;">CoP</td>
<td style="text-align: center;">Confidence</td>
<td style="text-align: center;">SCoR $\uparrow$</td>
<td style="text-align: center;">MAE $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">LLaMP</td>
<td style="text-align: center;">2.698</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.900</td>
<td style="text-align: center;">14.574</td>
<td style="text-align: center;">0.007</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.960</td>
<td style="text-align: center;">0.953</td>
<td style="text-align: center;">0.009</td>
</tr>
<tr>
<td style="text-align: center;">LLaMP (Sonnet)</td>
<td style="text-align: center;">1.345</td>
<td style="text-align: center;">0.702</td>
<td style="text-align: center;">0.867</td>
<td style="text-align: center;">0.608</td>
<td style="text-align: center;">16.121</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.000</td>
</tr>
<tr>
<td style="text-align: center;">LLaMP (Gemini-1.5)</td>
<td style="text-align: center;">21.586</td>
<td style="text-align: center;">0.217</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.217</td>
<td style="text-align: center;">64.324</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">0.718</td>
<td style="text-align: center;">0.560</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">0.531</td>
</tr>
<tr>
<td style="text-align: center;">LLaMP (Llama 3)</td>
<td style="text-align: center;">7.314</td>
<td style="text-align: center;">0.327</td>
<td style="text-align: center;">0.800</td>
<td style="text-align: center;">0.261</td>
<td style="text-align: center;">47.386</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.721</td>
<td style="text-align: center;">3.062</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4+Serp</td>
<td style="text-align: center;">2.222</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">41.742</td>
<td style="text-align: center;">5.947</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">8.214</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">0.186</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.910</td>
<td style="text-align: center;">41.225</td>
<td style="text-align: center;">0.000</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">1.680</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure A.1: Multimodal retrieval-augmented generation for materials informatics. (a) User query. (b) Supervisor ReAct agent capable of handling multiple assistant agents and high-level reasoning. (c-d) Assistant ReAct agents executing function calling and summarization. (c) MPThermoExpert and (d) MPElasticityExpert have access to the API schemas of thermo and elasticity endpoints on Materials Project, respectively. The selected details are highlighted in red, demonstrating the capabilities of RAG and ReAct implemented in LLaMP. The blue texts show LLaMP assistant ReAct agent can handle API calling errors and self-correct the input query accordingly.</p>
<h1>A. 2 Prompt Template</h1>
<p>We use the ReAct template hwchase17/react-multi-input-json from LangChain Hub (https://smith.langchain.com/hub/hwchase17/react-json) as follows:</p>
<p>Answer the following questions as best you can. You have access to the following tools:
{tools}
The way you use the tools is by specifying a JSON blob.
Specifically, this JSON should have an 'action' key (with the name of the tool to use) and an 'action_input' key (with the input to the tool going here).</p>
<p>The only values that should be in the "action" field are: {tool_names}
The \$JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid \$JSON_BLOB:
ãƒ»ãƒ»
{{
"action": \$TOOL_NAME,
"action_input": \$INPUT
}}
ãƒ»ãƒ»
ALWAYS use the following format:
Question: the input question you must answer
Thought: you should always think about what to do
Action:
ãƒ»ãƒ»
\$JSON_BLOB
ãƒ»ãƒ»
Observation: the result of the action
... (this Thought/Action/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question
Begin! Reminder to always use the exact characters 'Final Answer' when responding.</p>
<p>The above system prompt message is prepended with the following solicitation to guide the supervisor agent:</p>
<p>You name is LLaMP and you are a helpful agent that can consult materials-related data through Materials Project (MP) database, arXiv, Wikipedia, and run calculations/simulations via a python REPL and MP atomate2 computational workflow.</p>
<p>If you get an error, debug your code and try again. Only use the output of your code to answer the question. Ask user to clarify their queries if needed. Please note that you don't have direct control over MP but through multiple assistant agents to help you. You need to provide complete context in the input for assistants to do their job. REFINE and CRITQUE the output of the assistants if needed. You can end the conversation by saying "Bye".</p>
<h1>B Sample Questions</h1>
<p>Table 5: QA comparison between LLaMP and GPT-3.5.</p>
<table>
<thead>
<tr>
<th></th>
<th>Question</th>
<th>Answer</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>LLaMP</td>
<td>GPT-3.5</td>
</tr>
<tr>
<td>B5.1</td>
<td>What are the bulk moduli of the following metals: $\mathrm{Sc}, \mathrm{Ti}, \mathrm{V}, \mathrm{Cr}, \mathrm{Mn}, \mathrm{Fe}, \mathrm{Co}$, $\mathrm{Ni}, \mathrm{Cu}, \mathrm{Zn}$ ?</td>
<td>The bulk moduli of the metals you asked for are as follows (in GPa): Scandium (Sc): Voigt $=45.715$, Reuss $=45.34$, VRH $=45.528$ Titanium (Ti): Voigt $=112.955$, Reuss $=112.807$, VRH $=112.881$ Vanadium (V): Voigt $=181.232$, Reuss $=181.232$, VRH $=181.232$ Chromium (Cr): Voigt $=252.155$, Reuss $=252.155$, VRH $=252.155$ Manganese (Mn): Voigt $=280.959$, Reuss $=280.959$, VRH $=280.959$ Iron (Fe): Voigt $=296.003$, Reuss $=$ 295.207, VRH $=295.605$ - Cobalt (Co): Voigt $=200.587$, Reuss $=$ 200.587, VRH $=200.587$ - Nickel (Ni): Voigt $=197.399$, Reuss $=$ 197.29, VRH $=197.345$ - Copper (Cu): Voigt $=151.394$, Reuss $=$ 151.394, VRH $=151.394$ - Zinc (Zn): Voigt $=76.283$, Reuss $=$ 95.46, VRH $=85.872$</td>
<td>The bulk modulus (K) is a measure of a material's resistance to uniform compression. It is defined as the ratio of the change in pressure to the fractional volume compression. The values for the bulk modulus can vary somewhat depending on the specific conditions and crystallographic structures of the materials. The values provided here are approximate and given in gigapascals (GPa):</td>
</tr>
<tr>
<td>- Scandium (Sc): 55 GPa - Titanium (Ti): 110 GPa - Vanadium (V): 160 GPa Chromium (Cr): 160 GPa - Manganese (Mn): 120 GPa - Iron (Fe): 170 GPa Cobalt (Co): 180 GPa - Nickel (Ni): 180 GPa - Copper (Cu): 140 GPa - Zinc (Zn): 70 GPa</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>These values represent typical bulk moduli for these metals, and actual values may vary depending on factors such as temperature and pressure.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5 - continued from previous page</p>
<table>
<thead>
<tr>
<th></th>
<th>Question</th>
<th>Answer</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>LLaMP</td>
<td>GPT-3.5</td>
</tr>
<tr>
<td>B5.2</td>
<td>What is the full elastic tensor of NaCl ?</td>
<td>The elastic tensor for NaCl is available for two different entries on Materials Project. For material 'id 'mp-22851', the elastic tensor in IEEE format is $[[76.0,1.0$, $1.0,-0.0,0.0,-0.0],[1.0,76.0,1.0$, $-0.0,-0.0,0.0],[1.0,1.0,76.0,0.0$, $-0.0,-0.0],[-0.0,-0.0,0.0,-3.0$, $-0.0,-0.0],[0.0,-0.0,-0.0,-0.0$, $-3.0,0.0],[-0.0,0.0,-0.0,-0.0,0.0$, $-3.0]]$. For material 'id 'mp-22862', the elastic tensor in IEEE format is $[[47.0,12.0,12.0,0.0,-0.0,-0.0]$, $[12.0,47.0,12.0,0.0,-0.0,-0.0]$, $[12.0,12.0,47.0,0.0,0.0,-0.0]$, $[0.0,0.0,0.0,12.0,-0.0,-0.0],[-0.0$, $-0.0,0.0,-0.0,12.0,0.0],[-0.0,-0.0$, $-0.0,-0.0,0.0,12.0]]$.</td>
<td>The elastic tensor describes the relationship between stress and strain in a material. It is a 4th-rank tensor that characterizes the elastic properties of a crystal. In the case of NaCl (sodium chloride), the elastic tensor is typically represented by a 6x6 matrix.</td>
</tr>
<tr>
<td>The elastic constants of NaCl can vary slightly depending on the specific conditions and crystallographic orientation. Here is an approximate form of the elastic tensor for NaCl :</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$\mathrm{C} 11=289.2 \mathrm{GPa} \mathrm{C} 12=97.8 \mathrm{GPa} \mathrm{C} 44=81.6 \mathrm{GPa} \mathrm{C} 13=\mathrm{C} 14=\mathrm{C} 15=\mathrm{C} 16=$ $\mathrm{C} 23=\mathrm{C} 24=\mathrm{C} 25=\mathrm{C} 26=0 \mathrm{GPa}$</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Note: The values are given in gigapascals (GPa). The elastic constants Cij represent the components of the elastic tensor. In this case, C11, C12, and C44 are the non-zero elastic constants. The off-diagonal terms C13, C14, C15, C16, C23, C24, C25, and C26 are typically zero due to cubic symmetry.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Keep in mind that these values are approximate, and there may be slight variations depending on the source and the specific conditions under which the measurements were made. Experimental techniques, such as Brillouin scattering or ultrasonic measurements, are often used to determine the elastic constants of materials.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>B5.3</td>
<td>What are the bandgaps of the following materials: Si , Ge, and $\mathrm{GaAs}, \mathrm{GaN}, \mathrm{SiC}$, $\mathrm{BN}, \mathrm{InSe}, \mathrm{MoS} 2, \mathrm{BaTiO} 3$, CsPbI3? Only look for ground-state bandgaps.</td>
<td>The ground state bandgaps for the requested materials are as follows: Si: 1.12 eV , Ge: 0.66 eV , GaAs: 1.42 eV , GaN: 3.28 eV , SiC: 2.36 eV , BN: 6.4 eV , InSe: 1.26 eV , MoS2: 1.29 eV , BaTiO3: 3.2 eV , CsPbI3: 1.73 eV . Please note that these values are typical for bulk materials and can vary depending on the specific structure and calculation method.</td>
<td>As of my last knowledge update in January 2022, I can provide you with approximate values for the ground state bandgaps of the materials you mentioned. Keep in mind that these values can vary depending on factors like temperature and crystal structure. Additionally, more recent research may have refined these values. Here are the approximate ground state bandgaps in electron volts (eV):</td>
</tr>
<tr>
<td>Silicon (Si): 1.1 eV Germanium (Ge): 0.67 eV Gallium Arsenide (GaAs): 1.42 eV Gallium Nitride (GaN): 3.4 eV Silicon Carbide (SiC): 3.2 eV (for 4H-SiC) Boron Nitride (BN): 6 eV Indium Selenide (InSe): 1.25 eV Molybdenum Disulfide (MoS2): 1.3 eV (indirect bandgap) Barium Titanate (BaTiO3): 3.2 eV Cesium Lead Iodide (CsPbI3): 1.7 eV Please note that these values are general approximations and may not represent the specific conditions or variations in different forms of these materials. For the most accurate and up-to-date information, I recommend checking recent scientific literature or databases.</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5 - continued from previous page</p>
<table>
<thead>
<tr>
<th></th>
<th>Question</th>
<th>Answer</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>LLaMP</td>
<td>GPT-3.5</td>
</tr>
<tr>
<td>B5.4</td>
<td>Give me the decomposition enthalpies of the following materials: Ag2O3, MgZn2, GaAs, GaN, SiC, BN, InSe, MoS2, BaTiO3, CsPbI3. If there are multiple values, please give me the largest value.</td>
<td>The maximum decomposition enthalpies for the requested materials are as follows: - Ag2O3: 0.2565836414999989 eV/atom MgZn2: 0.1356 eV/atom - GaAs: 0.075 eV/atom - GaN: 0.07162 eV/atom - SiC: -0.172 eV/atom BN: -0.358 eV/atom - InSe: -0.01816 eV/atom - MoS2: 0.267 eV/atom - BaTiO3: 3.055 eV/atom - CsPbI3: 0.017536019000000902 eV/atom Please note that the values for $\mathrm{SiC}, \mathrm{BN}$, and InSe are negative, indicating that the decomposition is exothermic. All other values are positive, indicating endothermic decompositions.</td>
<td>The decomposition enthalpies (in $\mathrm{kJ} / \mathrm{mol}$ ) of the following materials are:</td>
</tr>
<tr>
<td>Ag2O3: -335</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MgZn2: -82</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GaAs: -317</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GaN: -505</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SiC: -40</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BN: -248</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>InSe: -125</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>MoS2: -393</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>BaTiO3: -112</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>CsPbI3: -153</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>B5.5</td>
<td>Give me the formation energies of the following materials: Ag2O3, MgZn2, GaAs, GaN, SiC, BN, InSe, MoS2, BaTiO3, CsPbI3. If there are multiple values, please give me the smallest value.</td>
<td>The smallest formation energies of the materials you asked for are as follows: Ag2O3: -0.3601 eV/atom, MgZn2: -0.1306 eV/atom, GaAs: -0.446 eV/atom, GaN: -0.052 eV/atom, SiC: -0.204 eV/atom, BN: -1.41 eV/atom, InSe: -0.742582154921875 eV/atom, MoS2: -1.202 eV/atom, BaTiO3: -3.493 eV/atom, CsPbI3: -1.296 eV/atom.</td>
<td>Ag2O3: -2.63 eV/atom MgZn2: -0.27 eV/atom GaAs: -4.13 eV/atom GaN: -3.51 eV/atom SiC: -4.98 eV/atom BN: -3.8 eV/atom InSe: -0.89 eV/atom MoS2: -3.31 eV/atom BaTiO3: -4.45 eV/atom CsPbI3: -2.32 eV/atom</td>
</tr>
</tbody>
</table>
<p>Table 5 - continued from previous page</p>
<table>
<thead>
<tr>
<th></th>
<th>Question</th>
<th>Answer</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>LLaMP</td>
<td>GPT-3.5</td>
</tr>
<tr>
<td>B5.6</td>
<td>Give me the decomposition enthalpies of the following materials: Ag2O3, MgZn2, GaAs, GaN, SiC, BN, InSe, MoS2, BaTiO3, CsPbI3. If there are multiple values, please give me the smallest value.</td>
<td>The smallest decomposition enthalpy values for the requested materials are as follows: Ag2O3: 0.019373648083333528 eV/atom, MgZn2: -0.0197 eV/atom, GaAs: -0.446 eV/atom, GaN: -0.05231 eV/atom, SiC: -0.204 eV/atom, BN: -1.41 eV/atom, InSe: -0.06336 eV/atom, MoS2: -0.669 eV/atom, BaTiO3: -0.063 eV/atom, CsPbI3: -0.018602 eV/atom.</td>
<td>The decomposition enthalpies of the materials are as follows: - Ag2O3: -1138.5 $\mathrm{kJ} / \mathrm{mol}$ - MgZn2: -54.2 kJ/mol - GaAs: -452.2 kJ/mol - GaN: -510.9 kJ/mol SiC: -2133.6 kJ/mol - BN: -2551.0 kJ/mol - InSe: -209.3 kJ/mol - MoS2: -739.3 $\mathrm{kJ} / \mathrm{mol}$ - BaTiO3: -393.5 kJ/mol - CsPbI3: -65.3 kJ/mol</td>
</tr>
<tr>
<td>B5.7</td>
<td>Give me the energy above hull values of the following materials: $\begin{aligned} &amp; \text { Ba2MnSe3, } \quad \text { KAlSiO4, } \ &amp; \text { LiCuO, } \quad \text { H8C3N2O, } \ &amp; \text { Dy2ZnS4, } \quad \text { Rb2BeF4, } \ &amp; \text { K9Fe2S7, } \quad \text { Tb2FeC4, } \ &amp; \text { ScAlAu, CaCrO4. If there } \ &amp; \text { are multiple values, please } \ &amp; \text { give me the lowest one. } \end{aligned}$</td>
<td>Here are the lowest energy above hull values for the requested materials: Ba2MnSe3: 0.0 eV/atom KAlSiO4: 0.0 eV/atom LiCuO: 0.0 eV/atom H8C3N2O: 0.0735 eV/atom Dy2ZnS4: 0.00966 eV/atom Rb2BeF4: 0.0 eV/atom K9Fe2S7: 0.0 eV/atom Tb2FeC4: 0.0 eV/atom ScAlAu: 0.0 eV/atom CaCrO4: 0.0 eV/atom. Please note that these are the lowest values found in the Materials Project database and actual values may vary based on different entries.</td>
<td>The energy above hull values for the given materials are as follows: - Ba2MnSe3: 0.0000 eV/atom - KAlSiO4: 0.0000 eV/atom - LiCuO: 0.0000 eV/atom H8C3N2O: 0.0000 eV/atom - Dy2ZnS4: 0.0000 eV/atom - Rb2BeF4: 0.0000 eV/atom - K9Fe2S7: 0.0000 eV/atom - Tb2FeC4: 0.0000 eV/atom - ScAlAu: 0.0000 eV/atom - CaCrO4: 0.0000 eV/atom</td>
</tr>
</tbody>
</table>
<p>Table 5 - continued from previous page</p>
<table>
<thead>
<tr>
<th></th>
<th>Question</th>
<th>Answer</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>LLaMP</td>
<td>GPT-3.5</td>
</tr>
<tr>
<td>B5.8</td>
<td>What are the bandgaps of the following materials: $\mathrm{Ba}(\mathrm{PdS} 2) 2, \mathrm{FePO} 4$, DyBi2IO4, NaFe(SO4)2, Eu2B5BrO9, Sr5As3H, TlGaS2, AgSbS2, Pr3S3N, LiLaO2. If there are polymorphs, give me only the bandgap of the stable material.</td>
<td>The bandgaps for the materials are:</td>
<td></td>
</tr>
<tr>
<td>- $\mathrm{Ba}(\mathrm{PdS} 2) 2: 0.779 \mathrm{eV}$</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>- Most stable FePO4: 2.1932 eV</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>- DyBi2IO4: 1.367 eV</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>- Most stable NaFe(SO4)2: 2.094 eV</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>- Eu2B5BrO9: 0.88 eV</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>- Sr5As3H: 0.676 eV</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>- TlGaS2: 1.7885 eV</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>- Most stable AgSbS2: 1.372 eV</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>- Pr3S3N: 1.15 eV</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>- Most stable LiLaO2: 3.7336 eV Please note that these values are obtained from the Materials Project database and might be different in experimental results due to various factors.</td>
<td>I don't have real-time data access, and my training only includes information available up to January 2022. As a result, I may not have specific bandgap values for the materials you mentioned. However, I can provide you with general information about some of these materials and their known properties up to my last update:</td>
<td></td>
<td></td>
</tr>
<tr>
<td>$\mathrm{Ba}(\mathrm{PdS} 2) 2$ : I don't have specific bandgap information for this compound. The bandgap may depend on factors such as crystal structure and synthesis conditions. FePO4: Iron(II) phosphate, known as ferrous phosphate, typically has a bandgap in the range of $2.5-3.0 \mathrm{eV}$.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DyBi2IO4: Dysprosium bismuth iodate. I don't have specific bandgap information for this compound.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>$\mathrm{NaFe}(\mathrm{SO} 4) 2$ : Sodium iron sulfate. I don't have specific bandgap information for this compound.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Eu2B5BrO9: Europium borate bromate. I don't have specific bandgap information for this compound.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Sr5As3H: Strontium arsenide hydride. I don't have specific bandgap information for this compound.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>TlGaS2: Thallium gallium sulfide. I don't have specific bandgap information for this compound.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>AgSbS2: Silver antimony sulfide. This material is a semiconductor with a bandgap typically around $1.5-2.0 \mathrm{eV}$.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Pr3S3N: Praseodymium sulfide nitride. I don't have specific bandgap information for this compound.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>LiLaO2: Lithium lanthanum oxide. I don't have specific bandgap information for this compound.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>The most stable structure for each material can vary based on synthesis conditions and other factors. For accurate and up-to-date bandgap values and structural information, you may want to consult recent scientific literature or databases that specialize in material properties.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5 - continued from previous page</p>
<table>
<thead>
<tr>
<th></th>
<th>Question</th>
<th></th>
<th>Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>B5.9</td>
<td>How to synthesize $\mathrm{YMnO}_{3}$ ?</td>
<td>To synthesize $\mathrm{YMnO}_{3}$, a solid-state reaction method can be employed. Here is a summarized procedure based on the synthesis details provided in the literature with DOI: 10.1021/ic502027k:</td>
</tr>
<tr>
<td>1. Dissolve $\mathrm{Y}(\mathrm{NO} 3) 3 \cdot 6 \mathrm{H} 2 \mathrm{O}$, $\mathrm{Mn}(\mathrm{NO} 3) 2 \cdot 4 \mathrm{H} 2 \mathrm{O}$, and a source of $[\mathrm{OH}-]$ in water to create a homogeneous solution.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>2. Add citric acid to the solution to act as a mixing media.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>3. Heat the mixture at $250^{\circ} \mathrm{C}$ for 2 hours to achieve pre-calcination.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>4. Finally, heat the pre-calcined mixture at a temperature range of $700^{\circ} \mathrm{C}$ to $930^{\circ} \mathrm{C}$ for 10 to 18 hours in an air atmosphere to form $\mathrm{YMnO}_{3}$.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td>Synthesizing $\mathrm{YMnO}_{3}$ (yttrium manganite) typically involves the solid-state reaction method. Please note that the details may vary depending on the specific conditions and equipment available in your laboratory. Here's a general procedure:</td>
</tr>
<tr>
<td>Materials: Yttrium oxide $\left(\mathrm{Y}<em 3="3">{2} \mathrm{O}</em>}\right)$ Manganese oxide $\left(\mathrm{MnO<em 2="2">{2}\right.$ or $\left.\mathrm{Mn}</em>} \mathrm{O<em 2="2">{3}\right)$ Highpurity manganese carbonate $\left(\mathrm{MnCO}_{3}\right)$ Lithium carbonate $\left(\mathrm{Li}</em>)$ Crucible and pestle High-temperature furnace Mortar and pestle Platinum crucible or alumina crucible} \mathrm{CO}*{3}\right)$ or lithium hydroxide $(\mathrm{LiOH</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Procedure:</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Step 1: Weighing the Precursors</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Weigh the appropriate amounts of yttrium oxide $\left(\mathrm{Y}<em 3="3">{2} \mathrm{O}</em>}\right)$, manganese oxide $\left(\mathrm{MnO<em 2="2">{2}\right.$ or $\mathrm{Mn}</em>$ ), and lithium carbonate (Li2CO3) or lithium hydroxide (LiOH) according to the stoichiometric ratio of YMnO 3 .} \mathrm{O}*{3</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Step 2: Mixing the Powders 2. GPT-3.5 starts to fail numbering correctly in this response. Grind the powders thoroughly in a mortar and pestle to ensure a homogenous mixture.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Step 3: Heating 3. Place the mixed powder in a crucible or alumina crucible. Heat the mixture in a high-temperature furnace. The specific temperature and heating rate depend on the exact synthesis conditions, but a typical range is 800 $1100^{\circ} \mathrm{C}$.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Step 4: Sintering 5. Hold the mixture at the target temperature for a specific duration (several hours) to allow for sintering and the formation of $\mathrm{YMnO}_{3}$.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Step 5: Cooling 6. Slowly cool the sample to room temperature to avoid thermal stress.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Step 6: Characterization 7. Characterize the synthesized $\mathrm{YMnO}_{3}$ using techniques such as X-ray diffraction (XRD), scanning electron microscopy (SEM), and energy-dispersive X-ray spectroscopy (EDS) to confirm the desired phase and composition.</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Notes: The choice of manganese precursor $\left(\mathrm{MnO}<em 2="2">{2}\right.$ or $\left.\mathrm{Mn}</em>\right)$ and lithium source (carbonate or hydroxide) can impact the synthesis process and may require optimization. It's crucial to use high-purity starting materials to avoid impurities in the final product. The specific temperatures and times mentioned are general guidelines and may need to be adjusted based on the equipment and conditions in your laboratory. Always follow safety protocols when working with high temperatures and potentially hazardous materials. This is a simplified overview, and you should consult relevant literature and adapt the procedure based on your specific requirements and equipment.} \mathrm{O}*{3</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Primary contributors
${ }^{\dagger}$ Correspondence to: cyrusyc@berkeley.edu</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
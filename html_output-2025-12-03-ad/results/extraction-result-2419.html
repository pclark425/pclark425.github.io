<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2419 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2419</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2419</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-65.html">extraction-schema-65</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-271245060</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.12532v1.pdf" target="_blank">Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Effective collaboration in multi-agent systems requires communicating goals and intentions between agents. Current agent frameworks often suffer from dependencies on single-agent execution and lack robust inter-module communication, frequently leading to suboptimal multi-agent reinforcement learning (MARL) policies and inadequate task coordination. To address these challenges, we present a framework for training large language models (LLMs) as collaborative agents to enable coordinated behaviors in cooperative MARL. Each agent maintains a private intention consisting of its current goal and associated sub-tasks. Agents broadcast their intentions periodically, allowing other agents to infer coordination tasks. A propagation network transforms broadcast intentions into teammate-specific communication messages, sharing relevant goals with designated teammates. The architecture of our framework is structured into planning, grounding, and execution modules. During execution, multiple agents interact in a downstream environment and communicate intentions to enable coordinated behaviors. The grounding module dynamically adapts comprehension strategies based on emerging coordination patterns, while feedback from execution agents influnces the planning module, enabling the dynamic re-planning of sub-tasks. Results in collaborative environment simulation demonstrate intention propagation reduces miscoordination errors by aligning sub-task dependencies between agents. Agents learn when to communicate intentions and which teammates require task details, resulting in emergent coordinated behaviors. This demonstrates the efficacy of intention sharing for cooperative multi-agent RL based on LLMs.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2419.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2419.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of multi-agent AI systems that coordinate to perform scientific research tasks, including details about their coordination mechanisms, communication protocols, feedback mechanisms, agent specializations, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REMALIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursive Multi-Agent Learning with Intention Sharing (REMALIS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent framework that trains LLM-based agents to coordinate via intention propagation, bidirectional feedback, and recursive reasoning; it decomposes planning, grounding, and execution across specialized agents to achieve emergent team-level behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>REMALIS (Recursive Multi-Agent Learning with Intention Sharing)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>REMALIS is a hierarchical multi-agent system that uses LLM components for planning and grounding and a cooperative execution module of specialized agents. Each agent maintains a private intention (tuple of current goal, sub-goals, sub-goal selection distribution, and desired teammate assignment) that is periodically broadcast. A learned propagation network (f_Λ, implemented as an RNN/GRU) converts broadcasts into teammate-specific messages. The architecture comprises: (1) a Planning module p_θ (encoder + GNN) that predicts next sub-goals and can re-plan using execution feedback, (2) a Grounding module g_ϕ (encoder, cross-attention, convolutional extractor, uncertainty modeling) that maps symbols to grounded embeddings and adapts based on coordination confusion statistics, and (3) a Cooperative Execution module of N specialized agents trained jointly via integrated learning, which execute actions conditioned on state and intention and produce coordination feedback used upstream.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_agents</strong></td>
                            <td>variable (N); implemented with 7 specialized execution agents in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>agent_specializations</strong></td>
                            <td>Specialized execution agents each cover distinct semantic domains; example roles given: query processing agent, information retrieval agent, arithmetic/computation agent. In the traffic application, roles include agents monitoring vehicle counts, road conditions, and signal timings; overall the system uses 7 specialized agents in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>research_phases_covered</strong></td>
                            <td>Planning (high-level sub-goal prediction and re-planning), Grounding (mapping language symbols to environment/action embeddings and uncertainty modeling), Execution (specialized agents performing actions), Adaptation / Re-planning (feedback-driven replanning), and Evaluation (team reward and auxiliary losses used during training). The framework does not explicitly cover literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_mechanism</strong></td>
                            <td>Decentralized partially-observable Markov game with centralized training (centralized training / decentralized execution). Coordination is via intention propagation (agents broadcast private intentions that are transformed via a propagation channel into teammate-specific messages), a propagation network acting as an attentive communication module, and bidirectional coordination channels where execution-derived summary statistics (coordination feedback) are sent upstream to grounding and planning for dynamic re-planning and grounding adjustments. Planning uses a GNN to capture sub-goal dependencies; grounding incorporates confusion estimates and episodic confusion memory to reduce miscoordination.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_protocol</strong></td>
                            <td>Structured message passing: agents exchange messages m_ij encoding intentions I_i = (γ_i, Σ_i, π_i, δ_i). Messages are encoded by an Encoder and processed by the propagation channel f_Λ (a recurrent neural network / GRU; implemented as a 4-layer GRU with 256 hidden size in experiments) to infer teammate intention beliefs b_i(I_j | m_ij). Messages carry semantic intent information (current goal, set of sub-goals, probabilistic next-subgoal distribution, desired teammate assignment) that is transformed into teammate-specific communication. The propagation network produces selective, teammate-directed messages rather than raw broadcast.</td>
                        </tr>
                        <tr>
                            <td><strong>feedback_mechanism</strong></td>
                            <td>Bidirectional feedback: execution agents produce execution encodings h_exec_t = [ϕ_1(o_1), ..., ϕ_N(o_N)] which are summarized via Φ into coordination feedback c_t (also denoted ψ(t)=Φ(h_exec_t)). c_t and related interpretability signals (E_t, U_t, R_t) are fed back to grounding and planning modules to guide grounding-strategy adjustments and re-planning. Grounding uses a coordination confusion matrix C_t and episodic confusion memory M_t to regularize grounding outputs (f_ϕ(s_t, I_t) = LLM_ϕ(s_t, I_t) ⊙ λ C_t). The planning module consumes agent feedback f_t and re-plans sub-goal dependencies using p_θ(s_t, I_t, e_t, f_t). An auxiliary loss L_aux and the team reward R are used to train execution policies.</td>
                        </tr>
                        <tr>
                            <td><strong>communication_frequency</strong></td>
                            <td>Periodic and at every decision step: agents 'broadcast their intentions periodically' and the LLM planning/grounding modules process inputs 'at each time-step t' including state s_t, intention I_t, and feedback f_{1:t}. The system also learns selective propagation policies (agents learn when and to which teammates to send detailed intentions).</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Applied in the paper to two domains: traffic flow prediction/control (TFP dataset) and structured web activities (web-based interaction automation); generalizable to cooperative language and decision tasks, though not explicitly applied to laboratory scientific research in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include accuracy (%) on classification/prediction tasks, alignment of sub-tasks (% aligned sub-tasks), coordination time (ms), BLEU/ROUGE in ablations, and auxiliary interpretability/uncertainty signals. Key reported numbers: Web 'Hard' task accuracy REMALIS 21.42% vs GPT-3.5 17.36%; Traffic Flow Prediction accuracies REMALIS: Easy 89.15%, Medium 77.62%, Hard 64.53%, Hell 55.37% (paper tables); Table 2 ablation: ReMALIS (Full) traffic accuracy 58.7% (others reported in ablation table for different settings), web ReMALIS (Full) accuracy 55.4%; Table 3 alignment: Full Intention Sharing aligned sub-tasks = 91% (Easy), 71% (Medium), 62% (Hard); coordination time (Full Intention Sharing) = 248 ms (Easy), 386 ms (Medium), 521 ms (Hard) vs No Communication 592/873/1198 ms respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>REMALIS is compared to single-agent and existing multi-agent baselines: GPT-3.5-Turbo (CoT), LUMOS (13B), AgentLM, ReAct, FiReAct, AUTOACT, DGN, LToS, etc. Across reported metrics REMALIS (7B) outperforms larger single-agent models: e.g., Web Hard: REMALIS 21.42% vs GPT-3.5 17.36%; TFP Easy: REMALIS 89.15% vs AUTOACT 87.89%; TFP Hard: REMALIS 64.53% vs DGN 62.34%; overall TFP 'All' REMALIS 55.37% vs LUMOS 51.49% (as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_benefits</strong></td>
                            <td>Documented benefits include improved accuracy and alignment, reduced miscoordination, and faster coordination: Full intention sharing dramatically increases aligned sub-tasks (91% vs 31% with No Communication on Easy) and reduces coordination time (Hard: 521 ms vs No Communication 1198 ms, ~57% faster). Ablations show intention propagation provides >6% accuracy increase, bidirectional feedback ~4.37% improvement, and recursive reasoning provides ~5.86% improvement compared to non-recursive architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_challenges</strong></td>
                            <td>Reported limitations include reliance on centralized training which may hinder scalability to fully decentralized deployments, lack of explicit handling for dynamic agent arrival or departure, potential limits of recursive reasoning for very long-horizon dependencies, and general communication/coordination overhead implicit in multi-agent messaging (not quantified beyond timing results).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_studies</strong></td>
                            <td>Comprehensive ablations reported: removing intention propagation reduces accuracy by over 6% (across datasets); removing bidirectional coordination channels produces a ~4.37% drop; replacing recursive reasoning with CNN/RNN models reduces contextual inference accuracy by ~5.86%. Table 3 ablations compare No Communication, Basic Propagation, Selective Propagation, Full Intention Sharing: aligned sub-tasks and coordination times improve progressively (e.g., aligned sub-tasks Hard: NoComm 17% → Basic 41% → Selective 51% → Full 62%; coordination time Hard: NoComm 1198 ms → Basic 691 ms → Selective 602 ms → Full 521 ms). Table 2 contains accuracy/BLEU/ROUGE ablations for Traffic and Web datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configurations</strong></td>
                            <td>Paper reports experimentally effective configurations rather than formal proofs of optimality: central findings indicate Full Intention Sharing (selective, teammate-directed propagation) + bidirectional coordination channels + recursive reasoning yields best performance. Implementation choices that worked well: 7 specialized execution agents, planning via 4-layer GNN (512 hidden), grounding via 6-layer Transformer (d_model=768), intention propagation via 4-layer GRU (256 hidden), coordination feedback via a GAT (2 heads, α=0.2). Centralized training with decentralized execution was used in experiments. The authors note these as practical configurations but acknowledge scalability and dynamic-team limitations remain.</td>
                        </tr>
                        <tr>
                            <td><strong>coordination_theory_notes</strong></td>
                            <td>Agents represent private intentions as I_i = (γ_i, Σ_i, π_i, δ_i). Intention inference uses b_i(I_j | m_ij) = f_Λ(m_ij) with f_Λ trained end-to-end to maximize a coordination reward R_c. Grounding adapts using a grounding confusion matrix C_t and episodic confusion memory M_t to reduce recurrent misalignment. Execution summaries h_exec_t are converted to c_t = Φ(h_exec_t) and used to regularize/planning reconfiguration (ψ(t) signals). Training optimizes a combination of RL objective and auxiliary losses L = L_RL + λ L_aux and a team-level objective L(η, γ, ζ, ξ) = E[α U_t + β E_t − R] + Ω(...).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Counterfactual multi-agent policy gradients <em>(Rating: 2)</em></li>
                <li>Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning <em>(Rating: 2)</em></li>
                <li>Roma: Multi-agent reinforcement learning with emergent roles <em>(Rating: 2)</em></li>
                <li>Lumos: Learning agents with unified modular design <em>(Rating: 2)</em></li>
                <li>Ensemble-Bot <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2419",
    "paper_id": "paper-271245060",
    "extraction_schema_id": "extraction-schema-65",
    "extracted_data": [
        {
            "name_short": "REMALIS",
            "name_full": "Recursive Multi-Agent Learning with Intention Sharing (REMALIS)",
            "brief_description": "A multi-agent framework that trains LLM-based agents to coordinate via intention propagation, bidirectional feedback, and recursive reasoning; it decomposes planning, grounding, and execution across specialized agents to achieve emergent team-level behaviors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "REMALIS (Recursive Multi-Agent Learning with Intention Sharing)",
            "system_description": "REMALIS is a hierarchical multi-agent system that uses LLM components for planning and grounding and a cooperative execution module of specialized agents. Each agent maintains a private intention (tuple of current goal, sub-goals, sub-goal selection distribution, and desired teammate assignment) that is periodically broadcast. A learned propagation network (f_Λ, implemented as an RNN/GRU) converts broadcasts into teammate-specific messages. The architecture comprises: (1) a Planning module p_θ (encoder + GNN) that predicts next sub-goals and can re-plan using execution feedback, (2) a Grounding module g_ϕ (encoder, cross-attention, convolutional extractor, uncertainty modeling) that maps symbols to grounded embeddings and adapts based on coordination confusion statistics, and (3) a Cooperative Execution module of N specialized agents trained jointly via integrated learning, which execute actions conditioned on state and intention and produce coordination feedback used upstream.",
            "number_of_agents": "variable (N); implemented with 7 specialized execution agents in experiments",
            "agent_specializations": "Specialized execution agents each cover distinct semantic domains; example roles given: query processing agent, information retrieval agent, arithmetic/computation agent. In the traffic application, roles include agents monitoring vehicle counts, road conditions, and signal timings; overall the system uses 7 specialized agents in experiments.",
            "research_phases_covered": "Planning (high-level sub-goal prediction and re-planning), Grounding (mapping language symbols to environment/action embeddings and uncertainty modeling), Execution (specialized agents performing actions), Adaptation / Re-planning (feedback-driven replanning), and Evaluation (team reward and auxiliary losses used during training). The framework does not explicitly cover literature review.",
            "coordination_mechanism": "Decentralized partially-observable Markov game with centralized training (centralized training / decentralized execution). Coordination is via intention propagation (agents broadcast private intentions that are transformed via a propagation channel into teammate-specific messages), a propagation network acting as an attentive communication module, and bidirectional coordination channels where execution-derived summary statistics (coordination feedback) are sent upstream to grounding and planning for dynamic re-planning and grounding adjustments. Planning uses a GNN to capture sub-goal dependencies; grounding incorporates confusion estimates and episodic confusion memory to reduce miscoordination.",
            "communication_protocol": "Structured message passing: agents exchange messages m_ij encoding intentions I_i = (γ_i, Σ_i, π_i, δ_i). Messages are encoded by an Encoder and processed by the propagation channel f_Λ (a recurrent neural network / GRU; implemented as a 4-layer GRU with 256 hidden size in experiments) to infer teammate intention beliefs b_i(I_j | m_ij). Messages carry semantic intent information (current goal, set of sub-goals, probabilistic next-subgoal distribution, desired teammate assignment) that is transformed into teammate-specific communication. The propagation network produces selective, teammate-directed messages rather than raw broadcast.",
            "feedback_mechanism": "Bidirectional feedback: execution agents produce execution encodings h_exec_t = [ϕ_1(o_1), ..., ϕ_N(o_N)] which are summarized via Φ into coordination feedback c_t (also denoted ψ(t)=Φ(h_exec_t)). c_t and related interpretability signals (E_t, U_t, R_t) are fed back to grounding and planning modules to guide grounding-strategy adjustments and re-planning. Grounding uses a coordination confusion matrix C_t and episodic confusion memory M_t to regularize grounding outputs (f_ϕ(s_t, I_t) = LLM_ϕ(s_t, I_t) ⊙ λ C_t). The planning module consumes agent feedback f_t and re-plans sub-goal dependencies using p_θ(s_t, I_t, e_t, f_t). An auxiliary loss L_aux and the team reward R are used to train execution policies.",
            "communication_frequency": "Periodic and at every decision step: agents 'broadcast their intentions periodically' and the LLM planning/grounding modules process inputs 'at each time-step t' including state s_t, intention I_t, and feedback f_{1:t}. The system also learns selective propagation policies (agents learn when and to which teammates to send detailed intentions).",
            "task_domain": "Applied in the paper to two domains: traffic flow prediction/control (TFP dataset) and structured web activities (web-based interaction automation); generalizable to cooperative language and decision tasks, though not explicitly applied to laboratory scientific research in the paper.",
            "performance_metrics": "Reported metrics include accuracy (%) on classification/prediction tasks, alignment of sub-tasks (% aligned sub-tasks), coordination time (ms), BLEU/ROUGE in ablations, and auxiliary interpretability/uncertainty signals. Key reported numbers: Web 'Hard' task accuracy REMALIS 21.42% vs GPT-3.5 17.36%; Traffic Flow Prediction accuracies REMALIS: Easy 89.15%, Medium 77.62%, Hard 64.53%, Hell 55.37% (paper tables); Table 2 ablation: ReMALIS (Full) traffic accuracy 58.7% (others reported in ablation table for different settings), web ReMALIS (Full) accuracy 55.4%; Table 3 alignment: Full Intention Sharing aligned sub-tasks = 91% (Easy), 71% (Medium), 62% (Hard); coordination time (Full Intention Sharing) = 248 ms (Easy), 386 ms (Medium), 521 ms (Hard) vs No Communication 592/873/1198 ms respectively.",
            "baseline_comparison": "REMALIS is compared to single-agent and existing multi-agent baselines: GPT-3.5-Turbo (CoT), LUMOS (13B), AgentLM, ReAct, FiReAct, AUTOACT, DGN, LToS, etc. Across reported metrics REMALIS (7B) outperforms larger single-agent models: e.g., Web Hard: REMALIS 21.42% vs GPT-3.5 17.36%; TFP Easy: REMALIS 89.15% vs AUTOACT 87.89%; TFP Hard: REMALIS 64.53% vs DGN 62.34%; overall TFP 'All' REMALIS 55.37% vs LUMOS 51.49% (as reported).",
            "coordination_benefits": "Documented benefits include improved accuracy and alignment, reduced miscoordination, and faster coordination: Full intention sharing dramatically increases aligned sub-tasks (91% vs 31% with No Communication on Easy) and reduces coordination time (Hard: 521 ms vs No Communication 1198 ms, ~57% faster). Ablations show intention propagation provides &gt;6% accuracy increase, bidirectional feedback ~4.37% improvement, and recursive reasoning provides ~5.86% improvement compared to non-recursive architectures.",
            "coordination_challenges": "Reported limitations include reliance on centralized training which may hinder scalability to fully decentralized deployments, lack of explicit handling for dynamic agent arrival or departure, potential limits of recursive reasoning for very long-horizon dependencies, and general communication/coordination overhead implicit in multi-agent messaging (not quantified beyond timing results).",
            "ablation_studies": "Comprehensive ablations reported: removing intention propagation reduces accuracy by over 6% (across datasets); removing bidirectional coordination channels produces a ~4.37% drop; replacing recursive reasoning with CNN/RNN models reduces contextual inference accuracy by ~5.86%. Table 3 ablations compare No Communication, Basic Propagation, Selective Propagation, Full Intention Sharing: aligned sub-tasks and coordination times improve progressively (e.g., aligned sub-tasks Hard: NoComm 17% → Basic 41% → Selective 51% → Full 62%; coordination time Hard: NoComm 1198 ms → Basic 691 ms → Selective 602 ms → Full 521 ms). Table 2 contains accuracy/BLEU/ROUGE ablations for Traffic and Web datasets.",
            "optimal_configurations": "Paper reports experimentally effective configurations rather than formal proofs of optimality: central findings indicate Full Intention Sharing (selective, teammate-directed propagation) + bidirectional coordination channels + recursive reasoning yields best performance. Implementation choices that worked well: 7 specialized execution agents, planning via 4-layer GNN (512 hidden), grounding via 6-layer Transformer (d_model=768), intention propagation via 4-layer GRU (256 hidden), coordination feedback via a GAT (2 heads, α=0.2). Centralized training with decentralized execution was used in experiments. The authors note these as practical configurations but acknowledge scalability and dynamic-team limitations remain.",
            "coordination_theory_notes": "Agents represent private intentions as I_i = (γ_i, Σ_i, π_i, δ_i). Intention inference uses b_i(I_j | m_ij) = f_Λ(m_ij) with f_Λ trained end-to-end to maximize a coordination reward R_c. Grounding adapts using a grounding confusion matrix C_t and episodic confusion memory M_t to reduce recurrent misalignment. Execution summaries h_exec_t are converted to c_t = Φ(h_exec_t) and used to regularize/planning reconfiguration (ψ(t) signals). Training optimizes a combination of RL objective and auxiliary losses L = L_RL + λ L_aux and a team-level objective L(η, γ, ζ, ξ) = E[α U_t + β E_t − R] + Ω(...).",
            "uuid": "e2419.0",
            "source_info": {
                "paper_title": "Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Counterfactual multi-agent policy gradients",
            "rating": 2,
            "sanitized_title": "counterfactual_multiagent_policy_gradients"
        },
        {
            "paper_title": "Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning",
            "rating": 2,
            "sanitized_title": "weighted_qmix_expanding_monotonic_value_function_factorisation_for_deep_multiagent_reinforcement_learning"
        },
        {
            "paper_title": "Roma: Multi-agent reinforcement learning with emergent roles",
            "rating": 2,
            "sanitized_title": "roma_multiagent_reinforcement_learning_with_emergent_roles"
        },
        {
            "paper_title": "Lumos: Learning agents with unified modular design",
            "rating": 2,
            "sanitized_title": "lumos_learning_agents_with_unified_modular_design"
        },
        {
            "paper_title": "Ensemble-Bot",
            "rating": 1,
            "sanitized_title": "ensemblebot"
        }
    ],
    "cost": 0.01498075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models</p>
<p>Xihe Qiu 
Shanghai University of Engineering Science
ShanghaiChina</p>
<p>Haoyu Wang 
Shanghai University of Engineering Science
ShanghaiChina</p>
<p>Xiaoyu Tan 
INF Technology(Shanghai) Co., Ltd
ShanghaiChina</p>
<p>Chao Qu 
INF Technology(Shanghai) Co., Ltd
ShanghaiChina</p>
<p>Yujie Xiong 
Shanghai University of Engineering Science
ShanghaiChina</p>
<p>Yuan Cheng 
Fudan University
ShanghaiChina</p>
<p>Yinghui Xu 
Fudan University
ShanghaiChina</p>
<p>Wei Chu 
INF Technology(Shanghai) Co., Ltd
ShanghaiChina</p>
<p>Yuan Qi 
Fudan University
ShanghaiChina</p>
<p>Towards Collaborative Intelligence: Propagating Intentions and Reasoning for Multi-Agent Coordination with Large Language Models
E84462B586A8974ACC0D8A7638F58C62
Effective collaboration in multi-agent systems requires communicating goals and intentions between agents.Current agent frameworks often suffer from dependencies on singleagent execution and lack robust inter-module communication, frequently leading to suboptimal multi-agent reinforcement learning (MARL) policies and inadequate task coordination.To address these challenges, we present a framework for training large language models (LLMs) as collaborative agents to enable coordinated behaviors in cooperative MARL.Each agent maintains a private intention consisting of its current goal and associated sub-tasks.Agents broadcast their intentions periodically, allowing other agents to infer coordination tasks.A propagation network transforms broadcast intentions into teammate-specific communication messages, sharing relevant goals with designated teammates.The architecture of our framework is structured into planning, grounding, and execution modules.During execution, multiple agents interact in a downstream environment and communicate intentions to enable coordinated behaviors.The grounding module dynamically adapts comprehension strategies based on emerging coordination patterns, while feedback from execution agents influnces the planning module, enabling the dynamic re-planning of sub-tasks.Results in collaborative environment simulation demonstrate intention propagation reduces miscoordination errors by aligning sub-task dependencies between agents.Agents learn when to communicate intentions and which teammates require task details, resulting in emergent coordinated behaviors.This demonstrates the efficacy of intention sharing for cooperative multi-agent RL based on LLMs.</p>
<p>Introduction</p>
<p>With the recent advancements of large language models (LLMs), developing intelligent agents that can perform complex reasoning and long-horizon planning has attracted increasing research attention (Sharan et al., 2023;Huang et al., 2022).A variety of agent frameworks have been proposed, such as ReAct (Yao et al., 2022), LUMOS (Yin et al., 2023), Chameleon (Lu et al., 2023) and BOLT (Chiu et al., 2024).These frameworks typically consist of modules for high-level planning, grounding plans into executable actions, and interacting with environments or tools to execute actions (Rana et al., 2023).</p>
<p>Despite their initial success, existing agent frameworks may experience some limitations.Firstly, most of them rely on a single agent for execution (Song et al., 2023;Hartmann et al., 2022).However, as tasks become more complex, the action dimension can be increased exponentially, and it poses significant challenges for a single agent to handle all execution functionalities (Chebotar et al., 2023;Wen et al., 2023).Secondly, existing frameworks lack inter-module communication mechanisms.Typically, the execution results are directly used as input in the planning module without further analysis or coordination (Zeng et al., 2023;Wang et al., 2024b).When execution failures occur, the agent may fail to adjust its strategies accordingly (Chaka, 2023).Thirdly, the grounding module in existing frameworks operates statically, without interactions with downstream modules.It grounds plans independently without considering feedback or states of the execution module (Xi et al., 2023).LLMs struggle to handle emergent coordination behaviors and lack common grounding on shared tasks.Moreover, existing multi-agent reinforcement learning (MARL) methods often converge on suboptimal policies that fail to exhibit a certain level of cooperation (Gao et al., 2023;Yu et al., 2023).</p>
<p>How can the agents with LLMs effectively communicate and collaborate with each other?we propose a novel approach, Recursive Multi-Agent arXiv:2407.12532v1[cs.CL] 17 Jul 2024</p>
<p>Learning with Intention Sharing (REMALIS 1 ) to address the limitations of existing cooperative artificial intelligence (AI) multi-agent frameworks with LLMs.REMALIS employs intention propagation between LLM agents to enable a shared understanding of goals and tasks.This common grounding allows agents to align intentions and reduce miscoordination.Additionally, we introduce bidirectional feedback loops between downstream execution agents and upstream planning and grounding modules.This enables execution coordination patterns to guide adjustments in grounding strategies and planning policies, resulting in more flexible emergent behaviors (Topsakal and Akinci, 2023).By integrating these mechanisms, REMALIS significantly improves the contextual reasoning and adaptive learning capabilities of LLM agents during complex collaborative tasks.The execution module utilizes specialized agents that collaboratively execute actions, exchange information, and propagate intentions via intention networks.These propagated intentions reduce miscoordination errors and guide grounding module adjustments to enhance LLM comprehension based on coordination patterns (Dong et al., 2023).Furthermore, execution agents can provide feedback to prompt collaborative re-planning in the planning module when necessary.</p>
<p>Compared to single-agent frameworks, the synergistic work of multiple specialized agents enhances REMALIS's collective intelligence and leads to emerging team-level behaviors (Wang et al., 2023).The collaborative design allows for dealing with more complex tasks that require distributed knowledge and skills.We demonstrate that:</p>
<p>• Intention propagation between execution agents enables emergent coordination behaviors and reduces misaligned sub-tasks.</p>
<p>• Grounding module strategies adjusted by intention sharing improve LLM scene comprehension.</p>
<p>• Planning module re-planning guided by execution feedback increases goal-oriented coordination.</p>
<p>Compared to various single-agent baselines and existing state-of-the-art MARL (Hu and Sadigh, 2023;Zou et al., 2023) methods using LLMs, our 1 The code can be accessed at the following URL:https: //github.com/AnonymousBoy123/ReMALIS.</p>
<p>REMALIS framework demonstrates improved performance on complex collaborative tasks, utilizing the publicly available large-scale traffic flow prediction (TFP) dataset and web-based activities dataset.This demonstrates its effectiveness in deploying LLMs as collaborative agents capable of intention communication, strategic adjustments, and collaborative re-planning (Du et al., 2023).</p>
<p>Preliminary</p>
<p>In this section, we introduce the methods of the proposed REMALIS framework in detail.As illustrated in Figure 1, REMALIS consists of four key components:</p>
<p>Planning Module p θ predicts the next pending sub-goal s t+1 , given the current sub-goal s t and other inputs s t+1 = p θ (s t , I t , e t , f t ), where I t is the current intention, e t is the grounded embedding, and f t is agent feedback.p θ first encode information through encoding layers h t = Encoder(s t , I t , e t , f t ) and subsequently predict the sub-goal through s t+1 = Sof tmax(T θ (h t )), where T θ utilizes the graph neural network (GNN) architecture.</p>
<p>The module is trained to maximize the likelihood of all sub-goals along the decision sequences given the current information on time step t.This allows the dynamic re-planning of sub-task dependencies based on agent feedback.
θ * = arg max θ T t=1 p θ (s t+1 |s t , I t , e t , f t ).
(1)</p>
<p>Grounding Module g ϕ contextualizes symbol embeddings e t = g ϕ (s t , I t , f 1:t ), where s t , I t , and f 1:t represent the states, intention, and feedback up to time step t, respectively.These embeddings are processed by encoders h t = Encoder(s t , I t , f 1:t ) and then by cross-attention layers and convolutional feature extractors: e t = Conv(Attn(h t , V ))+P t over vocabulary V .Here, P t includes agent feedback to enhance grounding accuracy based on coordination signals for more accurate contextual understanding.The module maps language symbols to physical environment representations through:
g(x) = f θ N i=1 w i g(x i ) ,(2)
where g(x) is the grounded embeddings of policy set x and g(x i ) represents its individual action embedding on agent i, respectively, and w i are learnable weights.The grounding function f θ utilizes a GNN architecture for structural composition.Additionally, we employ an uncertainty modeling module that represents ambiguities in grounding:
q ϕ (z|x) = Normal z; µ ϕ (x), σ 2 ϕ (x) ,(3)
where z is a latent variable modeled as a normal distribution, enabling the capture of multimodal uncertainties in grounding.</p>
<p>Cooperative Execution Module comprises N specialized agents {A 1 , ..., A N }.This architecture avoids using a single agent to handle all tasks.Instead, each agent is dedicated to a distinct semantic domain, cultivating expertise specific to that domain.For instance, agents A 1 , A 2 , and A 3 may be dedicated to query processing, information retrieval, and arithmetic operations, respectively.This specialization promotes an efficient distribution of tasks and reduces overlap in capabilities.</p>
<p>Decomposing skills into specialized agents risks creating isolated capabilities that lack coordination.To address this, it is essential that agents not only excel individually but also comprehend the capacities and limitations of their peers.We propose an integrated training approach where specialized agents are trained simultaneously to foster collaboration and collective intelligence.We represent the parameters of agent A i as θ i .Each agent's policy, denoted as y i ∼ π θ i (•|s), samples an output y i from a given input state s.The training objective for our system is defined by the following equation:
L exe = N i=1 E (s,y ⋆ )∼D ℓ(π θ i (y i |s), y ⋆ ), (4)
where ℓ(•) represents the task-specific loss function, comparing the agent-generated output y i with the ground-truth label y ⋆ .D denotes the distribution of training data.By optimizing this objective collectively across all agents, each agent not only improves its own output accuracy but also enhances the overall team's ability to produce coherent and well-coordinated results.</p>
<p>During training, we adjust the decomposition of grounding tasks to enhance collaboration, which is represented by the soft module weights {w 1 , ..., w N }.These weights indicate how the distribution of grounding commands can be optimized to better utilize the capabilities of different agents.The objective of this training is defined by the following loss function: L com = ℓ(d, w ⋆ ), where ℓ represents the loss function, d is expressed as subgoal task instruction data, and w ⋆ signifies the optimal set of weights.</p>
<p>Approach</p>
<p>The collaborative MARL of REMALIS focuses on three key points: intention propagation for grounding, bidirectional coordination channels, and integration with recursive reasoning agents.Detailed parameter supplements and pseudocode details can be found in Appendix C and Appendix F.</p>
<p>Planning</p>
<p>Grounding Execution</p>
<p>Is there any feedback on re planning?The current error is too large.Please plan and deploy again</p>
<p>Planning with Intention Propagation</p>
<p>We formulate a decentralized, partially observable Markov game for multi-agent collaboration.Each agent i maintains a private intention I i encoded as a tuple I i = (γ i , Σ i , π i , δ i ), where γ i is the current goal, Σ i = {σ i1 , σ i2 , . ..} is a set of related sub-goals, π i (σ) is a probability distribution over possible next sub-goals, and δ i (σ) is the desired teammate assignment for sub-goal σ.</p>
<p>Intentions are propagated through a communication channel f Λ parameterized by Λ.For a received message m ij from agent j, agent i infers a belief over teammate j's intention b i (I
j |m ij ) = f Λ (m ij )
, where Λ is a recurrent neural network.The channel f θ is trained in an end-to-end manner to maximize the coordination reward function R c .This propagates relevant sub-task dependencies to enhance common grounding on collaborative goals.
Λ * = arg max Λ E I,m∼f Λ [R c (I, m)].(5)
At each time-step t, the LLM witll processinputs comprising the agent's state s t , the intention I t , and the feedback f 1:t .</p>
<p>Grounding with Bidirectional Coordination Channels</p>
<p>The execution agent policies, denoted by π ξ (a i |s i , I i ), are parameterized by ξ and conditioned on the agent's state s i and intention I i .Emergent coordination patterns are encoded in a summary statistic c t and passed to upstream modules to guide planning and grounding adjustments.</p>
<p>For example, frequent miscoordination on sub-goal σ indicates the necessity to re-plan σ dependencies in I.This bidirectional feedback aligns low-level execution with high-level comprehension strategies.In addition to the downstream propagation of intents, execution layers provide bidirectional feedback signals ψ(t) to upstream modules ψ(t) = Φ(h exec t ):
h exec t = [ϕ 1 (o 1 ), . . . , ϕ N (o N )],(6)
where Φ(
L(θ) = L RL (θ) + λL aux (θ),(9)
where L RL is the reinforcement learning objective and λ a weighting factor.</p>
<p>Grounding Strategy Adjustment</p>
<p>We model action dependencies using a graph neural policy module h a t = GNN(s t , a), where h a t models interactions between action a and the state s t .The policy is then given by π θ (a t |s t ) = |A| i=1 h a i t .This captures the relational structure in the action space, enabling coordinated action generation conditioned on agent communication.</p>
<p>The coordination feedback c t is used to guide adjustments in the grounding module's strategies.We define a grounding confusion matrix C t , where C t (i, j) represents grounding errors between concepts i and j.The confusion matrix constrains LLM grounding as:
f ϕ (s t , I t ) = LLM ϕ (s t , I t ) ⊙ λC t (10)
where ⊙ is element-wise multiplication and λ controls the influence of C t , reducing uncertainty on error-prone concept pairs.We propose a modular regularization approach, with the grounding module g ϕ regularized by a coordination confusion estimator:
L confusion = 1 N i,j A ψ (c i , c j ) • Conf(c i , c j ) (11)
where L task is the task reward, Conf(c i , c j ) measures confusion between concepts c i and c j , and A ψ (c i , c j ) are attention weights assigning importance based on grounding sensitivity.An episodic confusion memory M t accumulates long-term grounding uncertainty statistics:
M t (i, j) = M t−1 (i, j) + I(Confuse(c i , c j ) t ),(12)
where I(•) are indicator functions tracking confusion events.By regularizing with a coordinationfocused confusion estimator and episodic memory, the grounding module adapts to avoid miscoordination.</p>
<p>Collective Learning and Adaptation</p>
<p>The coordination feedback signals c t and interpretability signals E t , U t , R t play a crucial role in enabling the LLM agents to adapt and learn collectively.By incorporating these signals into the training process, the agents can adjust their strategies and policies to better align with the emerging coordination patterns and requirements of the collaborative tasks.</p>
<p>The collective learning process can be formalized as an optimization problem, where the goal is to minimize the following objective function
L(η, γ, ζ, ξ) = E st,It,f 1:t [αU t + βE t − R] + Ω(η, γ, ζ, ξ).
Here, α and β are weighting factors that balance the contributions of the grounding uncertainty U t and coordination errors E t , respectively.The team reward R is maximized to encourage collaborative behavior.The term Ω(η, γ, ζ, ξ) represents regularization terms or constraints on the model parameters to ensure stable and robust learning.</p>
<p>The objective function L is defined over the current state s t , the interpretability signals
I t = {E t , U t , R t },
and the trajectory of feedback signals
f 1:t = {c 1 , I 1 , . . . , c t , I t } up to the current time step t. The expectation E st,It,f 1:t [•] is</p>
<p>Experiments 4.1 Datasets</p>
<p>To assess the performance of our models, we conducted evaluations using two large-scale real-world datasets: the traffic flow prediction (TFP) dataset and the web-based activities dataset.</p>
<p>TFP dataset comprises 100,000 traffic scenarios, each accompanied by corresponding flow outcomes.Each example is detailed with descriptions of road conditions, vehicle count, weather, and traffic control measures, and is classified as traffic flow: smooth, congested, or jammed.The raw data was sourced from traffic cameras, incident reports, and simulations, and underwent preprocessing to normalize entities and eliminate duplicates.</p>
<p>Web activities dataset contains over 500,000 examples of structured web interactions such as booking flights, scheduling appointments, and making reservations.Each activity follows a template with multiple steps like searching, selecting, filling forms, and confirming.User utterances and system responses were extracted to form the inputoutput pairs across 150 domains, originating from real anonymized interactions with chatbots, virtual assistants, and website frontends.</p>
<p>Implementation Details</p>
<p>To handle the computational demands of training our framework with LLMs, we employ 8 Nvidia A800-80G GPUs (Chen et al., 2024) under the DeepSpeed (Aminabadi et al., 2022) training framework, which can effectively accommodate the extensive parameter spaces and activations required by our framework's LLM components and multiagent architecture (Rasley et al., 2020).</p>
<p>For the TFP dataset, we classified the examples into four difficulty levels: "Easy", "Medium", "Hard", and "Hell".The "Easy" level comprises small grid networks with low, stable vehicle arrival rates.The "Medium" level includes larger grids with variable arrival rates."Hard" tasks feature large, irregular networks with highly dynamic arrival rates and complex intersection configurations.</p>
<p>The "Hell" level introduces challenges such as partially observable states, changing road conditions, and fully decentralized environments.</p>
<p>For the web activities dataset, we divided the tasks into "Easy", "Medium", "Hard", and "All" levels."Easy" tasks required basic single-click or short phrase interactions."Medium" involved complex multi-page sequences like form submissions. "Hard" tasks demanded significant reasoning through ambiguous, dense websites.The "All" level combined tasks across the full difficulty spectrum.</p>
<p>The dataset was divided into 80% for training, 10% for validation, and 10% for testing, with examples shuffled.These large-scale datasets offer a challenging and naturalistic benchmark to evaluate our multi-agent framework on complex, real-world prediction and interaction tasks.</p>
<p>Results and Analysis</p>
<p>Table 1 displays the principal experimental results of our REMALIS framework in comparison with various single-agent baselines and contemporary methods using the web activities dataset.We evaluated the models across four levels of task difficulty: "Easy", "Medium", "Hard", and "All".</p>
<p>The results from our comparative analysis indicate that REMALIS (7B), equipped with a 7B parameter LLM backbone, significantly outperforms competing methods.On the comprehensive Notably, REMALIS (7B) also exceeded the performance of GPT-3.5 (Turbo), a substantially larger foundation model, across all difficulty levels.On "Hard" tasks, REMALIS's 21.42% surpassed GPT-3.5's17.36% by over 4 points.This indicates that REMALIS's coordination mechanisms transform relatively modest LLMs into highly capable collaborative agents.</p>
<p>Despite their larger sizes, single-agent approaches like GPT-3.5 CoT, ReAct, and AgentLM significantly underperformed.Notably, even the advanced single-agent method LUMOS (13B) could not rival the performance of REMALIS (7B).The superiority of REMALIS, attributed to its specialized multi-agent design and novel features such as intention propagation, bidirectional feedback, and recursive reasoning, was particularly evident.On complex "Hard" tasks that required extensive reasoning, REMALIS achieved a notable performance of 21.42%, surpassing LUMOS by over 2 percentage points, thus highlighting the benefits of its multi-agent architecture and collaborative learning mechanisms.</p>
<p>The exceptional performance of our proposed REMALIS framework on the Traffic Flow Prediction (TFP) dataset can also be attributed to its innovative design and the effective integration of advanced techniques.On the "Easy" difficulty level, REMALIS achieved an impressive accuracy of 89.15%, outperforming the second-best method, AUTOACT, by a substantial margin of 1.26%.In the "Medium" category, REMALIS secured an accuracy of 77.62%, surpassing AU-TOACT's 76.29% by 1.33%.Even in the most challenging "Hard" and "Hell" levels, REMALIS maintained its lead with accuracies of 64.53% and 55.37%, respectively, outperforming the next best methods, DGN (62.34%) and LToS (54.81%), by 2.19% and 0.56%.</p>
<p>Ablation Studies</p>
<p>1)The Impact on Improving Multi-Agent Coordination Accuracy We conduct ablation studies to evaluate the impact of each component within the REMALIS framework.The observations can be found in Table 2. Excluding intention propagation results in a decrease in accuracy by over 6% across both datasets, highlighting difficulties in achieving common grounding among agents without shared local beliefs This highlights the importance of intention sharing for emergent team behaviors.</p>
<p>The absence of bidirectional coordination channels leads to a 4.37% decline in performance across various metrics, illustrating the importance of execution-level signals in shaping planning and grounding strategies.Without feedback coordination, agents become less responsive to new scenarios that require re-planning.Substituting recursive reasoning with convolutional and recurrent neural networks reduces contextual inference accuracy by 5.86%.Nonrecursive agents display short-sighted behavior compared to the holistic reasoning enabled by recursive transformer modeling.This emphasizes that recursive architectures are vital for complex temporal dependencies.</p>
<p>2)The Impact on Improving Multi-Agent Coordination Capability As presented in Table 3, on aligned sub-task percentage, the proposed Basic Propagation, Selective Propagation, and Full Intention Sharing methods consistently outperform baseline models like REACT and AgentLM across varying difficulty levels ("easy", "medium", and "hard").For example, Full Intention Sharing achieves alignment of 91%, 71%, and 62% across these levels, respectively.These results are substantially higher compared to scenarios with no communication (31%, 23%, and 17%).</p>
<p>Similarly, coordination time metrics exhibit major efficiency gains from intention propagation.On "Hard" tasks, Full Intention Sharing reduces coordination time to 521 ms, 57% faster than the 1198 ms for No Communication.As task complexity increases from easy to hard, the coordination time savings compared to baselines grows from 138 ms to 677 ms.This reveals that intention sharing mitigates growing coordination delays for difficult scenarios.</p>
<p>The highlighted propagation mechanisms also demonstrate clear incremental performance improvements over increasingly selective information sharing.As agents propagate more precise intentions to relevant teammates, both sub-task alignment and coordination efficiency improve.Moving from Basic to Selective to Full sharing provides gains on top of gains.</p>
<p>Conclusion</p>
<p>In this paper, we introduce a novel framework, RE-MALIS, designed to enhance collaborative capabilities within multi-agent systems using LLMs.Our approach incorporates three principal innovations: intention propagation for establishing a shared understanding among agents, bidirectional coordination channels to adapt reasoning processes in response to team dynamics, and recursive reasoning architectures that provide agents with advanced contextual grounding and planning capabilities necessary for complex coordination tasks.Experimental results indicate that REMALIS significantly outperforms several baseline methods, underscoring the efficacy of cooperative multi-agent AI systems.By developing frameworks that enable LLMs to acquire cooperative skills analogous to human team members, we advance the potential for LLM agents to manage flexible coordination in complex collaborative environments effectively.</p>
<p>While REMALIS demonstrates promising results in collaborative multi-agent tasks, our framework relies on a centralized training paradigm, which may hinder scalability in fully decentralized environments.The current implementation does not explicitly handle dynamic agent arrival or departure during execution, which could impact coordination in real-world applications, the recursive reasoning component may struggle with long-term dependencies and planning horizons beyond a certain time frame.</p>
<p>References</p>
<p>A Related Work</p>
<p>A.1 Single Agent Frameworks</p>
<p>Early agent frameworks such as Progprompt (Singh et al., 2023) directly prompt large language models (LLMs) to plan, execute actions, and process feedback in a chained manner within one model (Song et al., 2023).Despite its conceptual simplicity (Valmeekam et al., 2022), an integrated framework imposes a substantial burden on a single LLM, leading to challenges in managing complex tasks (Raman et al., 2022;Wang et al., 2024a).</p>
<p>To reduce the reasoning burden, recent works explore modular designs by separating high-level planning and low-level execution into different modules.For example, LUMOS (Yin et al., 2023) consists of a planning module, a grounding module, and an execution module.The planning and grounding modules break down complex tasks into interpretable sub-goals and executable actions.FiReAct (Chen et al., 2023) introduces a similar hierarchical structure, with a focus on providing step-by-step explanations (Zhang and Gao, 2023).Although partitioning into modules specializing for different skills is reasonable, existing modular frameworks still rely on a single agent for final action execution (Miao et al., 2023;Qiu et al., 2024).Our work pushes this idea further by replacing the single execution agent with a cooperative team of multiple agents.</p>
<p>A.2 Multi-Agent Reinforcement Learning</p>
<p>Collaborative multi-agent reinforcement learning has been studied to solve complex control or gameplaying tasks.Representative algorithms include COMA (Foerster et al., 2018), QMIX (Rashid et al., 2020) and ROMA (Wang et al., 2020).These methods enable decentralized execution of different agents but allow centralized training by sharing experiences or parameters (Lyu et al., 2021).Drawing on this concept, our REMALIS framework places greater emphasis on integrating modular LLMs to address complex language tasks.In REMALIS, each execution agent specializes in specific semantic domains such as query, computation, or retrieval, and is coordinated through a communication module (Mao et al., 2022).</p>
<p>The concept of multi-agent RL has recently influenced the design of conversational agents (Zimmer et al., 2021a;Schumann et al., 2024).Ensemble-Bot (Schuchard and Crooks, 2021) utilizes multiple bots trained on distinct topics, coordinated by a routing model.However, this approach primarily employs a divide-and-conquer strategy with independent skills (Martini et al., 2021), and communication within EnsembleBot predominantly involves one-way dispatching rather than bidirectional coordination.In contrast, our work focuses on fostering a more tightly integrated collaborative system for addressing complex problems (Schroeder de Witt et al., 2019;Zimmer et al., 2021b).</p>
<p>A.3 Integrated &amp; Collaborative Learning</p>
<p>Integrated learning techniques originate from transfer learning (Zhuang et al., 2020;Zhu et al., 2023), aiming to improve a target model by incorporating additional signals from other modalities (Lotfollahi et al., 2022;Shanahan et al., 2023).For multi-agent systems, (Li et al., 2022;Zhao et al., 2024) find joint training of multiple agents simultaneously boosts performance over separately trained independent agents (Lee and Perret, 2022).Recently, integrated learning has been used in single agent frameworks like (Shen et al., 2020) and(Loey et al., 2021), where auxiliary losses of interpretable outputs facilitate main model training through multitasking (Khamparia et al., 2021;Saber et al., 2021).</p>
<p>Our work adopts integrated learning to train specialized execution agents that are semantically consistent.At the team level, a communication module learns to attentively aggregate and propagate messages across agents, which indirectly coordinates their strategies and behaviors (Fan et al., 2020).The integrated and collaborative learning synergizes individual skills and leads to emerged collective intelligence, enhancing the overall reasoning and planning capabilities when dealing with complex tasks (He et al., 2021;Li et al., 2020).</p>
<p>B Methodology and Contributions</p>
<p>Based on the motivations and inspirations above, we propose recursive multi-agent learning with intention sharing framework (REMALIS), an innovative multi-agent framework empowered by integrated learning for communication and collaboration.The main contributions are:</p>
<ol>
<li>
<p>We design a cooperative execution module with multiple agents trained by integrated learning.Different execution agents specialize in different semantic domains while understanding peer abilities, which reduces redundant capacities and improves efficient division of labor.</p>
</li>
<li>
<p>We propose an attentive communication mod-ule that propagates informative cues across specialized agents.The module coordinates agent execution strategies without explicit supervision, acting as the role of team leader.</p>
</li>
<li>
<p>The collaborative design allows REMALIS to handle more complex tasks compared to singleagent counterparts.Specialized agents focus on their specialized domain knowledge while collaborating closely through communicative coordination, leading to strong emergent team intelligence.</p>
</li>
<li>
<p>We enable dynamic feedback loops from communication to the grounding module and replanning of the planning module, increasing adaptability when execution difficulties arise.</p>
</li>
</ol>
<p>We expect the idea of integrating specialized collaborative agents with dynamic coordination mechanisms to inspire more future research toward developing intelligent collaborative systems beyond conversational agents.</p>
<p>C Key variables and symbols</p>
<p>Table 4 summarizes the key variables and symbols used in the proposed recursive multi-agent learning framework called REMALIS.It includes symbols representing various components like the planning module, grounding module, execution policies, intentions, goals, sub-goals, and the intention propagation channel.</p>
<p>D Tasks Setup</p>
<p>D.1 Traffic Control</p>
<p>We define four levels of difficulty for our traffic control tasks: Easy, Medium, Hard, and Hell in Table 5.</p>
<p>D.2 Web Tasks</p>
<p>Similarly, we categorize the web tasks in our dataset into four levels of difficulty: Easy, Medium, Hard, and All.</p>
<p>Easy: The easy web tasks involve basic interactions like clicking on a single link or typing a short phrase.They require navigating simple interfaces with clear options to reach the goal.</p>
<p>Medium: The medium-difficulty tasks demand more complex sequences of actions across multiple pages, such as selecting filters or submitting forms.They test the agent's ability to understand the site structure and flow.</p>
<p>Hard: The hard web tasks feature more openended exploration through dense sites with am-biguity.Significant reasoning is needed to chain obscure links and controls to achieve aims.</p>
<p>All: The all-level combines tasks across the spectrum of difficulty.Both simple and complex interactions are blended to assess generalized web agent skills.The performance here correlates to readiness for real-world web use cases.</p>
<p>E Experimental Setups</p>
<p>In this study, we compare the performance of several state-of-the-art language models, including RE-MALIS, LUMOS, AgentLM, and GPT-3.5.These models vary in size, architecture, and training configurations, reflecting the diversity of approaches in the field of natural language processing in Table 6.</p>
<p>REMALIS is a 7 billion parameter model trained using the AdamW optimizer with a learning rate of 1e-4, a batch size of 32, and no dropout.It has 12 layers, a model dimension of 768, and 12 attention heads.The model was trained for 15 epochs with a warmup period of 1 epoch and a weight decay of 0.01.REMALIS employs a Graph Neural Network (GNN) architecture, which is particularly suited for modeling complex relationships and structures.</p>
<p>LUMOS, a larger model with 13 billion parameters, was trained using the Adam optimizer with a learning rate of 2e-5, a batch size of 64, and a dropout rate of 0.1.It has 8 layers, a model dimension of 512, and 8 attention heads.The model was trained for 20 epochs with a warmup period of 2 epochs and a weight decay of 0.001.LUMOS follows a Transformer architecture, which has proven effective in capturing long-range dependencies in sequential data.</p>
<p>AgentLM, a 6 billion parameter model, was trained using the AdamW optimizer with a learning rate of 1e-4, a batch size of 32, and no dropout.It has 6 layers, a model dimension of 768, and 12 attention heads.The model was trained for 10 epochs with a warmup period of 1 epoch and a weight decay of 0.01.AgentLM also uses a Transformer architecture.</p>
<p>GPT-3.5, the largest model in this study with 175 billion parameters, was trained using the Adam optimizer with a learning rate of 2e-5, a batch size of 64, and a dropout rate of 0.1.It has 48 layers, a model dimension of 1024, and 16 attention heads.The model was trained for 20 epochs with a warmup period of 2 epochs and a weight decay
i = (γ i , Σ i , π i , δ i ) Intention of agent i γ i
Current goal of agent i Σ i = {σ i1 , σ i2 , . ..}Set of sub-goals for agent i π i (σ)</p>
<p>Probability distribution over possible next sub-goals for agent i δ i (σ)</p>
<p>Desired teammate assignment for sub-goal σ of agent i of 0.001.GPT-3.5 follows the Transformer architecture, which has been widely adopted for large language models.</p>
<p>In addition to the base language models, the table provides details on the specialized modules and configurations employed by REMALIS and LUMOS.REMALIS incorporates a planning module with a 4-layer GNN and a 512 hidden size, a grounding module with a 6-layer Transformer and a model dimension of 768, 7 specialized and integrated execution agents, a 4-layer Gated Recurrent Unit (GRU) with a 256 hidden size for intention propagation, and a Graph Attention Network (GAT) with 2 heads and an alpha value of 0.2 for coordination feedback.LUMOS, on the other hand, employs a 2-layer GNN with a 1024 hidden size for planning, a 4layer Transformer with a model dimension of 512 for grounding, and a single integrated execution agent.</p>
<p>F Pseudo-code</p>
<p>This algorithm 2 presents the hierarchical planning and grounding processes in the proposed recursive multi-agent learning framework.The planning module p θ takes the current sub-goal s t , intention I t , grounded embedding e t , and feedback f t as inputs, and predicts the next sub-goal s t+1 .It first encodes the inputs using an encoder, and then passes the encoded representation through a graph neural network T θ parameterized by θ.The output of T θ is passed through a softmax layer to obtain the probability distribution over the next sub-goal.</p>
<p>The grounding module g ϕ takes the current state s t , intention I t , and feedback trajectory f 1:t as inputs, and produces the grounded embedding e t .It encodes the inputs using an encoder, and then applies cross-attention over the vocabulary V , followed by a convolutional feature extractor.The output is combined with agent feedback P t to enhance the grounding accuracy.The grounding module is parameterized by ϕ.This algorithm 3 describes the intention propagation mechanism in the proposed recursive multiagent learning framework.The goal is for each agent i to infer a belief b i (I j |m ij ) over the intention I j of a teammate j, given a message m ij received from j.</p>
<p>Algorithm 2 Hierarchical Planning and Grounding The intention inference process works as follows:</p>
<p>1.The received message m ij is encoded using an encoder to obtain a representation h ij .</p>
<ol>
<li>The encoded message h ij is passed through the propagation channel f Λ to infer the belief b i (I j |m ij ) over teammate j's intention I j .</li>
</ol>
<p>The objective is to train the parameters Λ of the propagation channel f Λ to maximize the coordination reward R c over sampled intentions I and messages m from the distribution defined by f Λ .</p>
<p>Algorithm 3 Intention Propagation Mechanism</p>
<p>Require: Current intention I i of agent i, message m ij from teammate j Ensure: Belief b i (I j |m ij ) over teammate j's intention I j 1: Initialization: 2: Intention propagation channel f Λ parameterized by Λ 3: f Λ is a recurrent neural network 4: Intention Inference: 5: Encode message:  Our algorithm takes experience tuples (s t , a t , r t , s t+1 ) for all agents as input, where s t is the state, a t is the action taken, r t is the reward received, and s t+1 is the next state.
h ij ← Encoder(m ij ) 6: Infer intention belief: b i (I j |m ij ) ← f Λ (m ij
The execution policy part works as follows:</p>
<p>1.For each agent i, get the agent's state s i,t and intention I i,t .</p>
<ol>
<li>Sample an action a i,t from the execution policy π ξ i (a i |s i,t , I i,t ), parameterized by ξ i .</li>
</ol>
<p>The coordination feedback part works as follows:</p>
<ol>
<li>
<p>Collect execution encodings h exec t = [ϕ 1 (o 1 ), . . ., ϕ N (o N )] by encoding the observations o i of each agent i using an encoder ϕ i .</p>
</li>
<li>
<p>Summarize the coordination patterns c t from the execution encodings h exec t using a function Φ.</p>
</li>
</ol>
<p>The objective is to maximize the team reward R and an auxiliary loss L aux by optimizing the execution policy parameters ξ.The auxiliary loss L aux is used to incorporate additional regularization or constraints.</p>
<p>The bidirectional coordination mechanism allows execution agents to act based on their policies and intentions, while also generating coordination feedback c t that summarizes the emerging coordination patterns.This feedback can be used to guide the planning and grounding modules in the recursive multi-agent learning framework.</p>
<p>G Discussion</p>
<p>The results demonstrate the efficacy of the proposed REMALIS framework in enabling coordinated multi-agent collaboration for complex tasks.By propagating intentions between agents, establishing bidirectional feedback channels, and integrating recursive reasoning architectures, RE-MALIS outperformed single-agent baselines and concurrent methods across difficulty levels on both the traffic flow prediction and web activities datasets.</p>
<p>The performance gains highlight the importance of fostering a shared understanding of goals and sub-tasks among agents through intention propagation.Communicating local beliefs allows agents to align their actions towards common objectives, leading to emergent coordinated behaviors that reduce misaligned sub-tasks and miscoordination errors.Furthermore, the bidirectional feedback channels play a crucial role in shaping the reasoning strategies of the planning and grounding modules based on the coordination patterns observed during execution.This adaptability enables the agents to adjust their comprehension and planning policies dynamically, resulting in more flexible and responsive behaviors.</p>
<p>The integration of recursive reasoning architectures also contributes to the superior performance of REMALIS.By modeling the intentions and strategies of other agents, the execution agents can engage in more contextual and holistic reasoning, enhancing their ability to handle complex temporal dependencies and long-term planning horizons.This recursive reasoning capability further amplifies the benefits of intention propagation and bidirectional feedback, as agents can better interpret and leverage the shared information and coordination signals.</p>
<p>It is important to note that while REMALIS demonstrates substantial improvements over singleagent frameworks, there are still limitations and potential areas for further research.For instance, the current implementation relies on a centralized training paradigm, which may hinder scalability in fully decentralized environments.Additionally, the framework does not explicitly handle dynamic agent arrival or departure during execution, which could impact coordination in real-world applications with fluid team compositions.</p>
<p>Future work could explore decentralized training approaches that maintain the benefits of multiagent collaboration while addressing scalability concerns.Moreover, developing mechanisms to adaptively handle changes in the agent team during execution could enhance the robustness and flexibility of the framework in dynamic environments.</p>
<p>H Supplementary application description of the overall framework</p>
<p>To further illustrate the practical applicability and versatility of our proposed REMALIS framework, we present a supplementary application scenario.</p>
<p>Figure 2 depicts a high-level overview of how RE-MALIS can be employed in a real-world setting to tackle complex, multi-step tasks that require orchestrating multiple agents with diverse capabilities.This exemplary use case demonstrates the framework's ability to decompose intricate problems into manageable sub-tasks, dynamically allocate appropriate agents, and seamlessly coordinate their actions to achieve the overarching goal efficiently and effectively.</p>
<p>Planning Module (Figure 4):</p>
<ol>
<li>
<p>Analyze the current traffic conditions, including vehicle counts, road incidents, and construction zones.</p>
</li>
<li>
<p>Identify intersections experiencing congestion and potential bottlenecks.</p>
</li>
<li>
<p>Formulate high-level goals to alleviate congestion and optimize traffic flow.</p>
</li>
<li>
<p>Break down the goals into a sequence of subgoals and subtasks.</p>
</li>
<li>
<p>Determine the dependencies and coordination needs between subtasks.</p>
</li>
<li>
<p>Plan the assignment of subtasks to specialized execution agents based on their expertise.</p>
</li>
</ol>
<p>Grounding Module (Figure 5):</p>
<ol>
<li>
<p>Contextualize the abstract traffic concepts and symbols into grounded representations.</p>
</li>
<li>
<p>Map entities like intersections, vehicles, and signal phases to their physical counterparts.</p>
</li>
<li>
<p>Resolve ambiguities and uncertainties in grounding based on the current traffic context.</p>
</li>
<li>
<p>Adjust grounding strategies based on feedback from execution agents and emerging coordination patterns.</p>
</li>
<li>
<p>Provide grounded embeddings to inform the execution agents' decision-making.</p>
</li>
</ol>
<p>Execution Module (Figure 6,7):</p>
<ol>
<li>
<p>Specialized agents monitor their respective domains (vehicle counts, road conditions, signal timings, etc.).</p>
</li>
<li>
<p>Agents communicate their local intentions and goals to relevant teammates.</p>
</li>
<li>
<p>Agents align their actions based on shared intentions and the coordinated plans.</p>
</li>
<li>
<p>Agents execute their assigned subtasks (adjusting signal phases, routing emergency vehicles, etc.).</p>
</li>
<li>
<p>Agents observe the impact of their actions and provide feedback on emerging coordination patterns.</p>
</li>
<li>
<p>Agents adapt their strategies dynamically based on the feedback and changing traffic conditions.</p>
</li>
<li>
<p>Agents continuously monitor and respond to fluctuations in vehicle arrival rates and traffic patterns.</p>
</li>
<li>
<p>Agents collaborate and coordinate their efforts to collectively alleviate congestion and optimize traffic flow.</p>
</li>
</ol>
<p>Figure 1 :
1
Figure 1: This framework introduces a multi-agent learning strategy designed to enhance the capabilities of LLMs through cooperative coordination.It enables agents to collaborate and share intentions for effective coordination, and utilizes recursive reasoning to model and adapt to each other's strategies.</p>
<p>Figure 2 :
2
Figure 2: Overview of the proposed REMALIS: This framework comprises a planning module, grounding module, cooperative execution module, and intention coordination channels.</p>
<p>taken over the distribution of states, interpretability signals, and feedback signal trajectories encountered during training.</p>
<p>Figure 3 :
3
Figure 3: Comparative performance evaluation across varying task difficulty levels for the web activities dataset, which indicates the accuracy scores achieved by REMALIS and several state-of-the-art baselines.</p>
<p>) 7: Objective: 8: Sample intentions I and messages m from f Λ 9: Maximize coordination reward R c over intentions and messages: 10: Λ * ← arg max Λ E I,m∼f Λ [R c (I, m)]</p>
<p>Figure 4 :
4
Figure 4: Overview of the proposed REMALIS Planning Module for predicting sub-goals based on current goals, intentions, grounded embeddings, and agent feedback.</p>
<p>Figure 5 :
5
Figure 5: Framework of the proposed REMALIS Grounding Module that contextualizes symbol embeddings using the current state, intentions, and feedback signals.</p>
<p>Figure 6 :
6
Figure 6: Overview of our REMALIS Cooperative Execution Module consisting of specialized agents that collaboratively execute actions and propagate intentions.</p>
<p>Figure 7 :
7
Figure 7: Overview of the collaborative evaluation setup in the proposed REMALIS framework.</p>
<p>Table 1 :
1
Comparative analysis of the REMALIS framework against single-agent baselines and contemporary methods across two datasets
MethodWebTFPEasy Medium HardAllEasy Medium Hard HellGPT-3.5-TurboCoT65.7751.6232.45 17.36 81.2768.9259.81 41.27Zero-Shot Plan57.6152.7328.92 14.58 82.2963.7755.39 42.38Llama2-7BCoT59.8354.9230.38 15.62 82.7365.8157.19 44.58ReAct56.9541.8627.59 13.48 81.1561.6553.97 43.25ART62.5152.3433.81 18.53 81.9863.2351.78 46.83ReWOO63.9253.1734.95 19.37 82.1271.3861.23 47.06AgentLM62.1446.7530.84 15.98 82.9666.0357.16 43.91FireAct64.0350.6832.78 17.49 83.7868.1958.94 45.06LUMOS66.2753.8135.37 19.53 84.0371.7562.57 51.49Llama3-8BCode-Llama (PoT) 64.8549.4932.16 17.03 83.3468.4759.15 52.64AgentLM66.7751.4531.59 16.58 85.2671.8158.68 53.39FiReAct68.9253.2732.95 17.64 84.1172.1558.63 51.65DGN69.1554.7833.63 18.17 83.4271.0862.34 53.57LToS68.4855.0333.06 17.71 85.7774.6159.37 54.81AUTOACT67.6256.2531.84 16.79 87.8976.2958.94 52.87ReMALIS(Ours) 73.9258.6438.37 21.42 89.1577.6264.53 55.37"All" difficulty level, which aggregates tasks acrossa range of complexities, REMALIS achieved anotable score of 55.37%, surpassing the second-highest scoring method, LUMOS, which scored51.49%. Additionally, REMALIS (7B) also ex-celled against AUTOACT, which utilizes a larger13B parameter model, by achieving a score that isover 3 percentage points higher at 52.87%. Thesefindings highlight the efficacy of REMALIS'sparameter-efficient design and its advanced multi-agent collaborative training approach, which allowit to outperform larger single-agent LLMs signifi-cantly.</p>
<p>Table 2 :
2
Ablation studies on Traffic and Web datasets
DatasetMethodMetricsAccuracy BLEU ROUGESingle Agent Baseline42.5%0.2170.384Intention Propagation47.3%0.2510.425TrafficBidirectional Feedback49.8%0.2780.461Recursive Reasoning53.2%0.3110.503ReMALIS (Full)58.7%0.3420.538Single Agent Baseline38.9%0.2550.416Intention Propagation42.7%0.2830.453WebBidirectional Feedback46.3%0.3110.492Recursive Reasoning50.6%0.3450.531ReMALIS (Full)55.4%0.3790.567</p>
<p>Table 3 :
3
Ablation on agent coordination capabilities
Method% Aligned sub-tasksCoordination Time (ms)Easy Medium Hard Easy Medium HardNo Communication31%23%17%5928731198REACT42%34%29%497732984AgentLM48%39%32%438691876FiReAct58%47%37%382569745Basic Propagation68%53%41%314512691Selective Propagation 79%62%51%279438602Full Intention Sharing 91%71%62%248386521</p>
<p>Chan HeeSong et al. 2023.Llm-planner: Few-shotgrounded planning for embodied agents with large language models.In Proceedings of the IEEE/CVF International Conference on Computer Vision.
Xihe Qiu et al. 2024. Chain-of-lora: Enhancing the in-Junxian He et al. 2021. Towards a unified view ofstruction fine-tuning performance of low-rank adap-parameter-efficient transfer learning. arXiv preprinttation on diverse instruction set. IEEE Signal Pro-arXiv:2110.04366.cessing Letters.Hengyuan Hu and Dorsa Sadigh. 2023. Language in-Shreyas Sundara Raman et al. 2022. Planning with large language models via corrective re-prompting.structed reinforcement learning for human-ai coordi-Oguzhan Topsakal and Tahir Cetin Akinci. 2023. Cre-nation. arXiv preprint arXiv:2304.07297. ating large language model applications utilizingIn NeurIPS 2022 Foundation Models for Decision Making Workshop. Krishan Rana et al. 2023. Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. arXiv preprint arXiv:2307.06135.langchain: A primer on developing llm apps fast. Baichuan Huang, Abdeslam Boularias, and Jingjin Yu. In International Conference on Applied Engineering 2022. Parallel monte carlo tree search with batched and Natural Sciences, volume 1. rigid-body simulations for speeding up long-horizon episodic robot planning. In 2022 IEEE/RSJ Interna-Karthik Valmeekam et al. 2022. Large language mod-tional Conference on Intelligent Robots and Systems els still can't plan (a benchmark for llms on plan-(IROS). IEEE. ning and reasoning about change). arXiv preprintReza Yazdani Aminabadi et al. 2022. Deepspeed-inference: Enabling efficient inference of transformer Tabish Rashid et al. 2020. Weighted qmix: Expand-ing monotonic value function factorisation for deep multi-agent reinforcement learning. In Advances in neural information processing systems 33, pages 10199-10210. models at unprecedented scale. In SC22: Interna-Jeff Rasley et al. 2020. Deepspeed: System optimiza-tional Conference for High Performance Computing, tions enable training deep learning models with over Networking, Storage and Analysis. 100 billion parameters. In Proceedings of the 26tharXiv:2206.10498. Aditya Khamparia et al. 2021. An internet of health things-driven deep learning framework for detection Haoyu Wang et al. 2024a. Carbon-based molecular and classification of skin cancer using transfer learn-properties efficiently predicted by deep learning-ing. Transactions on Emerging Telecommunications based quantum chemical simulation with large lan-Technologies, 32(7):e3963. guage models. Computers in Biology and Medicine, Irene Lee and Beatriz Perret. 2022. Preparing high page 108531. school teachers to integrate ai methods into stem classrooms. In Proceedings of the AAAI Conference Haoyu Wang et al. 2024b. Subequivariant reinforce-ACM SIGKDD International Conference on Knowl-on Artificial Intelligence, volume 36. ment learning framework for coordinated motion con-edge Discovery &amp; Data Mining.trol. arXiv preprint arXiv:2403.15100.Chuan Li et al. 2020. A systematic review of deepAbeer Saber et al. 2021. A novel deep-learning modeltransfer learning for machinery fault diagnosis. Neu-Lei Wang et al. 2023. A survey on large languagefor automatic detection and classification of breastrocomputing, 407:121-135. model based autonomous agents. arXiv preprintcancer using the transfer-learning technique. IEEE Access, 9:71194-71209. Christian Schroeder de Witt et al. 2019. Multi-agent vances in Neural Information Processing Systems common knowledge reinforcement learning. In Ad-arXiv:2308.11432. Weihua Li et al. 2022. A perspective survey on deep transfer learning for fault diagnosis in indus-Tonghan Wang et al. 2020. Roma: Multi-agent re-trial scenarios: Theories, applications and chal-inforcement learning with emergent roles. arXiv lenges. Mechanical Systems and Signal Processing, 167:108487. preprint arXiv:2003.08039.32. Ross J. Schuchard and Andrew T. Crooks. 2021. In-sights into elections: An ensemble bot detection cov-erage framework applied to the 2018 us midtermHao Wen et al. 2023. Empowering llm to use smart-Mohamed Loey et al. 2021. A hybrid deep transfer phone for intelligent task automation. arXiv preprint learning model with machine learning methods for arXiv:2308.15272. face mask detection in the era of the covid-19 pan-demic. Measurement, 167:108288. Zhiheng Xi et al. 2023. The rise and potential ofelections. Plos one, 16(1):e0244309. Yu Ying Chiu et al. 2024. A computational framework for behavioral assessment of llm therapists. arXiv Raphael Schumann et al. 2024. Velma: Verbalization preprint arXiv:2401.00820. embodiment of llm agents for vision and languageMohammad Lotfollahi et al. 2022. Mapping single-cell large language model based agents: A survey. arXiv data to reference atlases by transfer learning. Nature preprint arXiv:2309.07864. biotechnology, 40(1):121-130. Shunyu Yao et al. 2022. React: Synergizing reason-navigation in street view. In Proceedings of the AAAI Yihong Dong et al. 2023. Codescore: Evaluating Conference on Artificial Intelligence, volume 38. code generation by learning code execution. arXiv preprint arXiv:2301.09043. Yali Du et al. 2023. A review of cooperation in multi-agent learning. arXiv preprint arXiv:2312.05162. Murray Shanahan, Kyle McDonell, and Laria Reynolds. 2023. Role play with large language models. Nature, 623(7987):493-498.Pan Lu et al. 2023. Chameleon: Plug-and-play compo-ing and acting in language models. arXiv preprint sitional reasoning with large language models. arXiv arXiv:2210.03629. preprint arXiv:2304.09842. Da Yin et al. 2023. Lumos: Learning agents with unified Xueguang Lyu et al. 2021. Contrasting centralized data, modular design, and open-source llms. arXiv and decentralized critics in multi-agent reinforcement learning. arXiv preprint arXiv:2102.04402. preprint arXiv:2311.05657.Cheng Fan et al. 2020. Statistical investigations of trans-fer learning-based methodology for short-term build-ing energy predictions. Applied Energy, 262:114499. Jakob Foerster et al. 2018. Counterfactual multi-agent S. P. Sharan, Francesco Pittaluga, and Manmohan Chan-draker. 2023. Llm-assist: Enhancing closed-loop planning with language-based reasoning. arXiv preprint arXiv:2401.00125. policy gradients. In Proceedings of the AAAI confer-ence on artificial intelligence, volume 32. Sheng Shen et al. 2020. Deep convolutional neural networks with ensemble learning and transfer learn-ing for capacity estimation of lithium-ion batteries. Applied Energy, 260:114296.Shengcheng Yu et al. 2023. Llm for test script gener-Weichao Mao et al. 2022. On improving model-free al-ation and migration: Challenges, capabilities, and gorithms for decentralized multi-agent reinforcement opportunities. In 2023 IEEE 23rd International Con-learning. In International Conference on Machine ference on Software Quality, Reliability, and Security Learning. PMLR. (QRS). IEEE. Franziska Martini et al. 2021. Bot, or not? com-Fanlong Zeng et al. 2023. Large language mod-paring three methods for detecting social bots in els for robotics: A survey. arXiv preprint five political discourses. Big data &amp; society, 8(2):20539517211033566. arXiv:2311.07226.
Chaka Chaka.2023.Generative ai chatbots-chatgpt versus youchat versus chatsonic: Use cases of selected areas of applied english language studies.International Journal of Learning, Teaching Educational Research, 22(6):1-19.Yevgen Chebotar et al. 2023.Q-transformer: Scalable offline reinforcement learning via autoregressive qfunctions.In Conference on Robot Learning.PMLR.Baian Chen et al. 2023.Fireact: Toward language agent fine-tuning.arXiv preprint arXiv:2310.05915.Yushuo Chen et al. 2024.Towards coarse-to-fine evaluation of inference efficiency for large language models.arXiv preprint arXiv:2404.11502.Yunfan Gao et al. 2023.Retrieval-augmented generation for large language models: A survey.arXiv preprint arXiv:2312.10997.Valentin N. Hartmann et al. 2022.Long-horizon multirobot rearrangement planning for construction assembly.IEEE Transactions on Robotics, 39(1):239-252.Ning Miao, Yee Whye Teh, and Tom Rainforth.2023.Selfcheck: Using llms to zero-shot check their own step-by-step reasoning.arXiv preprint arXiv:2308.00436.Ishika Singh et al. 2023.Progprompt: Generating situated robot task plans using large language models.In 2023 IEEE International Conference on Robotics and Automation (ICRA).IEEE.Xuan Zhang and Wei Gao.2023.Towards llm-based fact verification on news claims with a hierarchical step-by-step prompting method.arXiv preprint arXiv:2310.00305.</p>
<p>Table 4 :
4
Key variables and symbols in the proposed recursive multi-agent learning framework.Message sent from agent j to agent i b i (I j |m ij ) Agent i's belief over teammate j's intentionI j given message m ij R c Coordination reward π ξ (a i |s i , I i )Execution agent policy conditioned on state s i and intention I i a i
SymbolDescriptionp θPlanning module parameterized by θs tCurrent sub-goal at time tI tCurrent intention at time te tGrounded embedding at time tf tAgent feedback at time tg ϕGrounding module parameterized by ϕπ ξ iExecution policy of agent i parameterized by ξ if ΛIntention propagation channel parameterized by Λm ijAction of agent is iState of agent iI</p>
<p>Table 5 :
5
Comparison of Traffic Network Complexity Levels
Difficulty Level Grid Size IntersectionsArrival RatesPhases per IntersectionEasy3x39Low and stable (0.5 vehicles/s)Less than 10Medium5x525Fluctuating (0.5-2 vehicles/s)10-15Hard8x864Highly dynamic (0.1 to 3 vehicles/s)More than 15HellIrregular100+Extremely dynamic with spikes&gt;25</p>
<p>Table 6 :
6
Training hyperparameters and configurations
Hyperparameter/ConfigurationReMALISLUMOSAgentLMGPT-3.5Language Model Size7B13B6B175BOptimizerAdamWAdamAdamWAdamLearning Rate1e-42e-51e-42e-5Batch Size32643264Dropout00.100.1Number of Layers128648Model Dimension7685127681024Number of Heads1281216Training Epochs15201020Warmup Epochs1212Weight Decay0.010.0010.010.001Network ArchitectureGNNTransformerTransformer TransformerPlanning ModuleGNN, 4 layers, 512 hidden size2-layer GNN, 1024 hidden size--Grounding Module6-layer Transformer, d model = 768 4-layer Transformer, d model = 512--Execution Agents7 specialized, integrated trainingSingle agent8 agent4 agentIntention Propagation4-layer GRU, 256 hidden size---Coordination FeedbackGAT, 2 heads, = 0.2---Trainable Parameters5.37B6.65B4.61B17.75B</p>
<p>Current sub-goal s t , intention I t , grounded embedding e t , feedback f t 2: Output: Next sub-goal s t+1 3: h t = Encoder(s t , I t , e t , f t ) {Encode inputs} 4: s t+1 = Softmax(T θ (h t )) {Predict next sub-goal} 5: T θ is a graph neural network parameterized by θ {Planning module p θ } 6: Input: Current state s t , intention I t , feedback f 1:t 7: Output: Grounded embedding e t 8: h t = Encoder(s t , I t , f 1:t ) {Encode inputs} 9: e t = Conv(Attn(h t , V )) + P t {Grounded em-It initializes an intention propagation channel f Λ , parameterized by Λ, which is implemented as a recurrent neural network.
bedding}10: Attn(•, •) is a cross-attention layer over vocab-ulary V11: Conv(•) is a convolutional feature extractor12: P t includes agent feedback to enhance ground-ing accuracy13: g ϕ is the grounding module parameterized byϕ
1: Input:</p>
<p>Experience tuples (s t , a t , r t , s t+1 ) for all agents Ensure: Execution policies π ξ i (a i |s i , I i ) and coordination feedback c t 1: Execution Policy: 2: for each agent i do ← arg max ξ E (s,a)∼π ξ [R + λL aux ] multi-agent learning framework.It involves executing actions based on the agents' policies and generating coordination feedback from the execution experiences.
Algorithm 4 Bidirectional CoordinationRequire: 3: Get agent state s i,t and intention I i,t4:a i,t ∼ π ξ i (a i |s i,t , I i,t ) {Execution policy}5: end for6: Coordination Feedback:7: Collect execution encodings h exec t=[ϕ 1 (o 1 ), . . . , ϕ N (o N )] {Encode observations}8: c t ← Φ(h exec t) {Summarize coordination pat-terns}9: Objective:10: Maximize team reward R and auxiliary lossL aux :11: ξThis algorithm 4 describes the bidirectional co-ordination mechanism in the proposed recursive
*</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Transfer learning in deep reinforcement learning: A survey. Zhuangdi Zhu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2023</p>
<p>A comprehensive survey on transfer learning. Fuzhen Zhuang, Proceedings of the IEEE. the IEEE2020109</p>
<p>Learning fair policies in decentralized cooperative multi-agent reinforcement learning. Matthieu Zimmer, International Conference on Machine Learning. PMLR2021a</p>
<p>Learning fair policies in decentralized cooperative multi-agent reinforcement learning. Matthieu Zimmer, International Conference on Machine Learning. PMLR2021b</p>
<p>Wireless multi-agent generative ai: From connected intelligence to collective intelligence. Hang Zou, arXiv:2307.027572023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
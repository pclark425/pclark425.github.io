<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7321 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7321</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7321</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-258823112</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.11307v2.pdf" target="_blank">Semantic anomaly detection with large language models</a></p>
                <p><strong>Paper Abstract:</strong> As robots acquire increasingly sophisticated skills and see increasingly complex and varied environments, the threat of an edge case or anomalous failure is ever present. For example, Tesla cars have seen interesting failure modes ranging from autopilot disengagements due to inactive traffic lights carried by trucks to phantom braking caused by images of stop signs on roadside billboards. These system-level failures are not due to failures of any individual component of the autonomy stack but rather system-level deficiencies in semantic reasoning. Such edge cases, which we call semantic anomalies, are simple for a human to disentangle yet require insightful reasoning. To this end, we study the application of large language models (LLMs), endowed with broad contextual understanding and reasoning capabilities, to recognize such edge cases and introduce a monitoring framework for semantic anomaly detection in vision-based policies. Our experiments apply this framework to a finite state machine policy for autonomous driving and a learned policy for object manipulation. These experiments demonstrate that the LLM-based monitor can effectively identify semantic anomalies in a manner that shows agreement with human reasoning. Finally, we provide an extended discussion on the strengths and weaknesses of this approach and motivate a research outlook on how we can further use foundation models for semantic anomaly detection. Our project webpage can be found at https://sites.google.com/view/llm-anomaly-detection.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7321",
    "paper_id": "paper-258823112",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0050525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Semantic Anomaly Detection with Large Language Models
11 Sep 2023</p>
<p>Amine Elhafsi 
Autonomous Systems Laboratory
Stanford University
496 Lomita Mall94305StanfordCAUSA</p>
<p>Rohan Sinha rhnsinha@stanford.edu 
Christopher Agia cagia@stanford.edu 
Edward Schmerling schmrlng@stanford.edu 
Issa Nesnas issa.a.nesnas@jpl.nasa.gov 
Jet Propulsion Laboratory
4800 Oak Grove Drive, La Cañada Flintridge91109CAUSA</p>
<p>Marco Pavone pavone@stanford.edu 
Semantic Anomaly Detection with Large Language Models
11 Sep 2023B253B8D747ADEAFC23F2876629F47243arXiv:2305.11307v2[cs.RO]Semantic ReasoningOOD DetectionFault Monitoring
As robots acquire increasingly sophisticated skills and see increasingly complex and varied environments, the threat of an edge case or anomalous failure is ever present.For example, Tesla cars have seen interesting failure modes ranging from autopilot disengagements due to inactive traffic lights carried by trucks to phantom braking caused by images of stop signs on roadside billboards.These system-level failures are not due to failures of any individual component of the autonomy stack but rather system-level deficiencies in semantic reasoning.Such edge cases, which we call semantic anomalies, are simple for a human to disentangle yet require insightful reasoning.To this end, we study the application of large language models (LLMs), endowed with broad contextual understanding and reasoning capabilities, to recognize such edge cases and introduce a monitoring framework for semantic anomaly detection in vision-based policies.Our experiments apply this framework to a finite state machine policy for autonomous driving and a learned policy for object manipulation.These experiments demonstrate that the LLM-based monitor can effectively identify semantic anomalies in a manner that shows agreement with human reasoning.Finally, we provide an extended discussion on the strengths and weaknesses of this approach and motivate a research outlook on how we can further use foundation models for semantic anomaly detection.</p>
<p>Introduction</p>
<p>Driven by advances in machine learning, robotic systems are rapidly advancing in capabilities, enabling their deployment in increasingly complex and varied scenarios.However, the infinitude of situations these robots may encounter in the real world means that we can never completely exclude the existence of rare corner cases and failure scenarios.Therefore, as well we may hope our robots generalize to novel conditions, there is an increasing need for runtime monitoring components that help issue advance warnings when a system encounters anomalies to mitigate rare failure modes.Fig. 1 An example of a semantic anomaly where (a) the system visualization shows the detection of a stream of traffic lights passing through the car, (b) upon viewing the full scene it is clear that these are indeed traffic lights, just inactive in transport, and (c) a reconstruction of this scenario in the CARLA simulator which we use to demonstrate that large language models may perform the contextual reasoning necessary to detect such semantic anomalies.</p>
<p>Since modern robots increasingly rely on learned components embedded within the autonomy stack, performance is often sensitive to socalled out-of-distribution inputs -inputs that are dissimilar from training data [1][2][3][4][5].Although numerous methods have been developed to detect OOD inputs [6] at the component level, such component level monitoring (e.g., detecting image classification errors) can be insufficient to prevent system-level faults.This is best exemplified by recent at-scale deployments of autonomous robotic systems, which have given rise to a steady stream of edge cases and outlier scenarios of seemingly never ending creativity.For example, among the millions of Tesla cars currently on the road, passengers have encountered disengagements due to out-ofcommission traffic lights carried by a truck (see Figure 1) 1 and dangerous phantom braking caused by images of stop signs on roadside billboards. 2imilarly, recent research has shown equivalent behavior on billboards with pictures of pedestrians [7].These examples defy blame-assignment to a specific component: arguably, these are correct detections of traffic lights or stop signs and, at least nominally, autonomous vehicles should obey signage.Instead, these examples illustrate that it is often the context surrounding objects and their interrelations that can cause a robot to misinterpret its observations at a system-level, necessitating monitors that perform such contextual reasoning.</p>
<p>We refer to observations of this type as semantic anomalies, system-level out-of-distribution (OOD) inputs that arise from an unusual or "tricky" combination of individually indistribution observations.For example, they can arise because common robotics perception datasets, like BDD100K [8] for autonomous driving, do not differentiate between functioning traffic lights and those we should disregard.Moreover, even a hypothetical perception model with perfect uncertainty quantification could issue detections on images of signage on billboards without considering them OOD observations.Rather, it is their semantic irregularity-their relation to a moving truck or a roadside billboard-that causes robot failures.It is this combinatorial origin that makes these anomalies particularly difficult to guard against, for manually-programmed and learned systems alike.Conventionally, we engage a design cycle to improve the system once such a failure mode has been observed, for example by constructing targeted data augmentations [7,9].Other approaches adapt models to changing distributions with tools from domain adaptation and continual learning [10][11][12].However, in safetycritical applications like autonomous driving, it is unacceptable to let failures occur and hope to fix them in the future.Instead, we require real time Fig. 2 Semantic anomaly detection framework.At each sampling time, the robot's observation is converted to a natural language description.The scene description is incorporated within a prompt template and passed to the LLM, which applies the necessary contextual reasoning to identify any potential semantic anomalies.</p>
<p>monitors that perform the necessary semantic reasoning to detect such anomalies before system failures occur.</p>
<p>In this work, we study the application of foundation models trained on vast and wide-ranging data from the human experience, specifically large language models (LLMs), to provide the necessary insight to recognize semantic anomalies.We propose to do so because 1) semantic anomalies, like the traffic lights on a truck or the stop sign on a billboard, are often straightforward for a human to identify, and 2) because LLMs have demonstrated strong performance on automated reasoning tasks [13][14][15][16], so we intuitively expect the sheer scale and diversity of an LLM's training corpus to provide a broad contextual knowledge base drawn from the human experience to reason about scenarios that the robot has never experienced before.</p>
<p>Specifically, our contributions are three-fold.First, as illustrated in Figure 2, we introduce a monitoring framework for semantic anomaly detection for vision-based policies; this monitor applies an LLM to reason in a chain-of-thought fashion about what observed objects in a scene may result in task-relevant confusion that could cause policy errors.Second, we instantiate and evaluate this framework in two experimental settings: 1) an autonomous driving system consisting of a perception system combined with discrete state machine logic for high-level decision making and 2) an end-to-end learned policy for object manipulation.Our experiments show that an LLM-based monitor can effectively recognize semantic anomalies, with anomaly characterizations that generally agree with human reasoning.</p>
<p>Finally, in light of these results, we provide an extended discussion on the success and failure modes of this approach which motivate a research outlook on how foundation models may be further adapted and utilized for the task of semantic anomaly detection.</p>
<p>Related Work</p>
<p>Our method leverages large language models to detect semantically anomalous scenarios that may compromise a robot's reliability.Therefore, in this section, we briefly review related work on 1) outof-distribution and anomaly detection and 2) the use of LLMs in robotics.</p>
<p>Out-of-Distribution Detection</p>
<p>Commonly, deep anomaly or OOD detection methods train an auxiliary model to explicitly flag data from novel classes or data that is dissimilar from training data, aiming to detect inputs on which a DNN performs poorly [17][18][19].A wealth of such approaches exists.Some methods construct an OOD detection classifier on only in-distribution data [20][21][22], e.g., through so called one-class classification losses [23].Others directly model the training distribution [24] and measure dissimilarity from a nominal set of values using some distance function in a latent space [25][26][27], or analyze deterioration in reconstruction errors of, e.g., autoencoders [28,29].Alternative to direct OOD classification, other methods seek to improve the often poorly calibrated uncertainty scores of neural networks on OOD data [30].Many are based on a Bayesian viewpoint and emphasize tractable approximations to the Bayesian posterior via, e.g., deep ensembles [31], Monte-Carlo dropout [32], or sensitivity-based Laplace approximations [6,33].Alternatively, several approaches construct architectures, loss functions, or training procedures to design networks that output high predictive uncertainty outside the training data [30,[34][35][36].</p>
<p>Beyond OOD detection for individual models, anomaly detection methods have been developed that make comparisons between multiple related models, leveraging domain knowledge on expected relationships to flag potential faults.For example, [37,38] demonstrate that we can detect perception errors by checking consistency across sensing modalities (e.g., camera, LiDAR) and over time.In addition, other methods apply supervised learning on examples of successes and failures based on ground truth information or domain specific supervisory signals [39,40].While these existing methods for OOD/anomaly detection have proven useful in increasing the reliability of downstream decisionmaking [41,42], their application comes with two limitations.First, these existing approaches cannot reason about robot failure modes that arise from semantically anomalous scenarios as defined in the preceding section.Instead, they only aim to detect anomalies correlated with inference errors or quantify uncertainty in the perception system and its predictions.That is, they monitor the correctness of individual components (even in the aforementioned case of multimodal perception), rather than the aggregate reasoning of an autonomy stack.Second, a limitation of existing OOD/anomaly detection methods is that we cannot immediately apply them as general purpose diagnostic tools: they require access to the model's training data, data of failures, or domain-specific knowledge.Instead, we propose the use of LLMs to perform zero-shot reasoning over possible reasoning errors that may be made about an observed scene with respect to a task description.As a result, our method is a generalpurpose diagnostic tool that we can directly apply to many tasks without requiring access to training data or task-specific component redesign.</p>
<p>It is commonly understood that unexpected inference errors often occur at test time because models overfit to spurious correlations that may hold on a particular dataset, but not in general [3,4].To overcome spurious correlations, recent work in domain generalization (that is, increasing robustness to unknown distributional shifts as defined in [43]) has sought to disentangle the causal relations between inputs and outputs [44].Furthermore [45] apply similar principles of causal inference to learn to avoid semantically motivated reasoning errors of end-to-end learned policies.However, doing so requires sufficient natural interventions, and thus observations of semantic anomalies, to prune spurious correlations.By definition, such anomalies are extremely rare, so zero-shot approaches to reason about semantic anomalies are necessary to avoid failures.To more structurally enable semantic reasoning over a scene, some recent work has developed methods for perception that construct scene representations that capture the relations between objects and the environment as 3D Scene Graphs [46] and [47] has applied LLMs to classify room type from objects contained in a 3D Scene Graph.We further investigate the ability of LLMs to flexibly interpret relational scene descriptions.A key difference is that we investigate reasoning from the semantic context of a downstream decision making task rather than considering a classification task in isolation.</p>
<p>Large Language Models in Robotics</p>
<p>Thanks to their broad and general purpose knowledge, LLMs are becoming a ubiquitous part of robotic systems that can be adapted to a range of downstream applications.LLMs have been used for various purposes such as autonomous navigation [48], long-horizon planning [49,50], [51] and policy synthesis [52].LLM embeddings have also been used to achieve language-conditioned control as in [53].The common approach among these works is the integration of LLMs within a broader robotics subsystem (e.g., perception, planning, etc.) to improve functionality or make the subsystem amenable to tasks that require linguistic reasoning.In contrast, we seek to leverage LLMs as a "semantic reasoning" module that monitors the observations that an independent robot (or subsystem thereof) encounters.</p>
<p>A line of works demonstrate that LLMs are avid reasoners in few-shot settings [13], [14], particularly when LLMs are prompted to explain the logical steps underpinning their response [15] Similar prompting strategies have been developed to elicit zero-shot reasoning from LLMs [16].In many real-world cases, robot failures may be predictable from semantically anomalous conditions; conditions that can be identified with such LLM prompting strategies before the failure occurs.</p>
<p>Large pretrained models have also been used at the intersection of computer vision and language, as in Socratic Models [54] wherein large pretrained models are composed in a zero-shot fashion to perform tasks such as video understanding and image captioning.Multimodal models such as Flamingo [55], BLIP [56] and PaLM-E [57] have also demonstrated impressive performance on image and video understanding tasks.However, characterizing the suitability of a scene for a particular downstream task is an unsolved problem.</p>
<p>LLM-Based Semantic Anomaly Detection</p>
<p>In this study, we propose a monitoring framework that leverages an LLM-based module to supervise a robot's perception stream while it operates, identifying any semantic anomalies that it may encounter.At some sampling frequency, the monitor examines the robot's visual observations and transforms them into textual descriptions.These scene descriptions are then integrated into an LLM prompt that aims to identify any factors that could give rise to a policy error, unsafe behavior, or task confusion.We emphasize that the level of task and system specificity of the monitor is determined by the context provided in the prompt.In particular, the role of the LLM-based monitor is to indicate potential factors in the visual observations that could contribute to confusion, but it is neither explicitly grounded in the system's capabilities or deficiencies nor does it access the system's internal processes.Figure 2 provides an illustration of this framework.</p>
<p>An essential component of this framework is the conversion of visual information into natural language descriptions.Our method is agnostic to the approach used for visual-to-text conversion, and various techniques can be utilized ranging from classical image processing to vision-language models (VLMs) depending on the task requirements.In this work, we employ an open vocabulary object detector [58], which describes the objects present in a scene along with associated context.</p>
<p>To enhance the LLM's ability to reason over the scene descriptions, we employ prompting strategies that leverage current best practices in prompt engineering.Specifically, we employ few-shot prompting [13,14] and chain-of-thought reasoning [16] in our instantions of this framework.Few-shot prompting provides examples to prime the LLM's reasoning.These examples highlight common pitfalls and reasoning errors that the robot may encounter during operation, and serve to guide the LLM towards more robust decision-making.Chain-of-thought reasoning adds specific task-and system-specific structure to the reasoning.</p>
<p>We describe and evaluate two instantiations of this framework in the subsequent section.</p>
<p>Experiments</p>
<p>In this section, we assess an LLM's ability to identify situations that could potentially result in semantic failures through several experiments, and provide insights into the strengths and limitations of the approach.We perform experiments in an autonomous driving setting with a reasoningbased policy and in a manipulation setting with an end-to-end learned visuomotor policy.</p>
<p>We first present the autonomous driving experiments, which are primarily motivated by the aforementioned real-world Tesla failure cases induced by semantic anomalies.These experiments instantiate a complete implementation of the proposed anomaly detection framework and demonstrate its effectiveness in recognizing semantic anomalies.To confirm the novel detection capabilities, we also compare against two existing OOD baselines founded on different detection principles, and indeed demonstrate that these methods complement each other in discerning different types of anomalies.</p>
<p>Second, we present the manipulation experiments to evaluate the utility of an LLM-based monitor when the system of interest is controlled by a purely learned ("black box") policy.These experiments demonstrate that, although the LLMbased monitor still tends to produce anomaly characterizations that align with human expectations, learned policies can often fail in erratic or inexplicable ways, which limits the versatility of our approach.</p>
<p>Reasoning-Based Policy</p>
<p>Experimental Setup</p>
<p>We conducted autonomous driving experiments in the CARLA simulator [59], a platform which provides realistic 3D environments for testing and validating autonomous driving systems.The goal of these experiments is to demonstrate that the LLM-based monitor can effectively identify autonomous driving semantic edge cases in an end-to-end manner, without requiring additional training or fine-tuning.We designed five classes of scenarios for our experiments:</p>
<ol>
<li>Nominal Stop Sign Interaction: The autonomous vehicle encounters stop signs at intersections where it is required to make a complete stop before proceeding.</li>
</ol>
<p>Nominal Traffic Light Interaction:</p>
<p>The autonomous vehicle encounters traffic lights and must obey their signals.</p>
<p>Anomalous Stop Sign Interaction:</p>
<p>In these scenarios, we introduce a billboard or poster with a stop sign as part of the graphic along the vehicle's route, in order to simulate situations where the vehicle's perception system might mistake a non-functional stop sign for a real one and cause the vehicle to stop unnecessarily.4. Anomalous Traffic Light Interaction: In these scenarios, the autonomous vehicle encounters a truck transporting a traffic light along its route, to simulate situations where the autonomous vehicle may be influenced by a non-functional traffic light.5. Strange Objects: In these scenarios, the autonomous vehicle encounters an entity unexpected to be seen on the road in an adjacent lane.We introduce one of five objects for each episode chosen from an airplane, a boat, an elephant, a robot, and a train.</p>
<p>We handcrafted 10, 10, 16, 18, and 15 cases for the respective scenario classes which were evenly distributed over all public CARLA maps to encompass diverse driving environments representing suburban, urban, rural and highway settings.</p>
<p>To simulate the autonomous vehicle's driving behavior, we developed a finite-state-machinebased planner designed to follow a route while obeying stop sign and traffic light signals.Perception is achieved through a single forward-facing RGB camera fixed on the vehicle.Stop signs and traffic lights native to the CARLA maps are detected by querying privileged simulator information once they enter the vehicle's field of view.The corresponding stopping regions are also retrieved with the detections.To determine the active traffic light color, we developed a simple approach that involved masking green, yellow, and red pixels on the traffic light patch and identifying the most prominent color.</p>
<p>To test the vehicle's performance in the presence of anomalies, we employed YOLOv8 [60] to detect any stop signs or traffic lights that may exist in addition to those present in the environment by default.Detected stop signs cause the vehicle to come to a temporary stop at the point on its route nearest the detection.Similarly, traffic light detections can influence the vehicle's behavior based on the classified active color.</p>
<p>In our simulations, all edge cases caused the vehicle to stop in unsafe locations, such as in the middle of intersections or on the freeway, demonstrating that the detection of such semantic edge cases meaningfully improves autonomous driving safety.</p>
<p>Anomaly Detection Methodology</p>
<p>To detect anomalies, we generated prompts from the autonomous vehicle's observations and fed them to an LLM.Observations were evaluated at a frequency of 2 Hz.In tandem, we also evaluate two baseline OOD detection methods.</p>
<p>LLM Anomaly Detection (Ours): First, the observed images were parsed by the OWL-ViT open vocabulary object detector to yield a scene description, consisting of the detected objects along with relevant context.To generate descriptive labels, we constructed a vocabulary by enumerating a diverse set of objects ranging from vehicle types to scenery and pairing them with descriptive predicates (e.g., "near the road," "on the bridge," etc.).The scene description was then integrated into a prompt template designed to elicit a chain-of-thought reasoning style to identify whether any element of the scene might cause erroneous, unsafe, or unexpected behavior.Finally, the text-davinci-003 LLM analyzed the prompt to identify any potential semantic anomalies present in the observation.An illustration of this process is depicted in Figure 3.</p>
<p>SCOD [61] (Baseline): As a baseline, we apply the SCOD OOD detector to the object detection model used in the autonomy stack.SCOD wraps an existing model to produce a scalar anomaly signal that reflects a model's epistemic uncertainty in a task-relevant manner.Specifically, since SCOD is based on the Laplace approximation of the Bayesian posterior over models given their training data, the SCOD metric essentially functions as a measure of confidence in a model's predictions on unseen and possibly OOD inputs.To construct the SCOD metric in an efficient manner, we wrap the detection head (which outputs a categorical distribution over classes in a bounding box) of the object detector, similar to the practice of wrapping the final layers of a model advocated in [61].SCOD requires a-priori processing of local curvature information of predictions on the training data, so we instantiate SCOD with the MS-COCO dataset used to train the object detector.Mahalanobis Distance (Baseline): As a second baseline, we compare against a Mahalanobis distance metric, a simple and typical anomaly detection method that fits a Gaussian mixture model (GMM) to the feature embeddings of nominal, in-distribution inputs and evaluates the likelihood of test inputs under the nominal distribution.Specifically, we perform a dimensionality reduction on the feature embeddings of the object detector's ResNet backbone and fit a GMM in reduced coordinates using the images from the nominal driving scenarios.Since this approach explicitly models the nominal input distribution, we can expect it to issue warnings on inputs that look visually distinctive from nominal observations.</p>
<p>Results</p>
<p>We assess the performance of our semantic anomaly detection methodology by comparing the LLM-based monitor's anomaly detections with ground truth labels determined based on the visibility of the anomaly.To compute these values, we first split each episode into distinct time intervals corresponding to periods where the anomaly either is or is not in view.If the LLM yields an anomaly classification for any timestep during which the anomaly is in view, we treat the entire interval as a single true positive (TP).Conversely, if the anomaly is not detected at any point in the interval, we treat the interval as a false negative (FN).We count true negatives (TNs) and false positives (FPs) on a per timestep basis when an anomaly is not in the field of view.We present the results for the four different scenario types in Table 1.</p>
<p>We note that the LLM demonstrates strong performance across the anomalous stop sign and traffic light, and nominal stop sign scenarios.The results indicate a high true positive rate (TPR) and low false negative rate (FNR) in the anomalous scenarios with relatively lower rates of misdetections.In the nominal stop sign episodes, the LLM demonstrates a low false positive rate (FPR).However, we interestingly see an elevated FPR in the nominal traffic light episodes.However, taken altogether, the detections indicate strong reason capabilities in realistic nominal and anomalous scenarios.</p>
<p>We noted that CARLA's visual fidelity, though of high quality, differs from that of the realworld images on which OWL-ViT was trained.As a result, we found that the object detector Table 1 Results of our LLM-based monitor on finite state machine-based autonomous vehicle stack in the CARLA simulator.The first two rows report the total number of semantic anomalies and the total number of nominal observations in the dataset.We distinguish between baseline episodes without any anomalies (nominal episodes) and the test cases where a semantic anomaly is visible for a short duration (anomalous episodes), each of which is further split into episodes where the AV encounters a particular type of interaction.A true positive (TP) occurs when a semantic anomaly is in view and the monitor issues an alert, a false negative (FN) occurs when the anomaly is in view but the monitor does not detect it.A true negative (TN) occurs when no anomalies are in view and the LLM does not detect anomalies, a false negative (TN) occurs when an anomaly is in view and the monitor does not detect so.We report the rate for each of these cases respectively.</p>
<p>Nominal Episodes</p>
<p>Anomalous  would occasionally misclassify objects or hallucinate unseen objects altogether, possibly due to lighting conditions or artifacts in the simulated visuals.One common error that we observed was the detection of objects with the predicate "on a billboard," despite the fact that no billboards were in the scene, an example of which is illustrated in Figure 4. We hypothesize that this issue is a result of background lighting effects creating the illusion of a flat background.To demonstrate that the object detector's errors were the primary source of anomaly misclassifications we also present anomaly detection performance with ground truth scene labels in Table 2.</p>
<p>We emphasize the LLM-based monitor's unique ability to detect semantic anomalies by comparing its performance to two OOD detection methods: SCOD, the method proposed by [61], and a Mahalanobis distance-based approach.We apply these OOD detectors to the DETR object detection model [62] 3 .Since these baselines produce real-valued OOD metrics, we tune the threshold at which we signal an anomaly as the 95% quantile of the OOD signal on observations from the nominal driving scenarios.This ensures a false positive rate of 5%.As shown in Table 3, we observe that the LLM-based monitor significantly outperforms both SCOD and the Mahalanobis distance at the CARLA semantic anomaly detection task.</p>
<p>We can explain the poor semantic anomaly detection performance of our baselines by examining the differences in their underlying principles: the SCOD metric aims to detect when the perception system produces erroneous predictions and the Mahalanobis distance functions as a measure of distinctiveness in an observation's visual appearance with respect to nominal data.Since semantic anomalies typically consist of unusual combinations of commonly seen, in-distribution elements (e.g., stop sign imagery or traffic lights on trucks), the resulting failure modes can occur even when the AV's perception model confidently produces correct predictions or when observations appear "normal" at a pixel-level.In contrast, the LLM excels at identifying these tricky scenarios.Nonetheless, we point out that semantic anomaly detection and traditional OOD detectors are not mutually exclusive tools.In fact, we highlight the complementarity of our LLM-based monitor and the baseline OOD detectors with an additional set of results.In Table 4, we contrast the ability of our method and the baselines to detect when the AV's perception system makes erroneous object detections, a meaningful task for a component-level runtime monitor.Here, we tuned the baselines' detection thresholds so that the false positive rate was roughly 50% to show their predictive ability of detecting perception inference errors.We see that the LLM is completely unable to detect perception errors, whereas SCOD is able to do so.The Mahalanobis baseline also demonstrates some capacity to detect perception errors, although here it does not perform as well as SCOD.</p>
<p>Finally, it is important to emphasize that while these principles are complementary, they are not disjoint: Table 3 shows that SCOD has some capacity to the detect the traffic light on a truck, and the Mahalanobis distance has some capacity to detect strange objects.Indeed, at a qualitative level, we observed that the Tesla Cybertruck asset used in the traffic light anomalies was often not detected by the AVs perception system, hence this semantic anomaly often also corresponds with perception errors.In addition, the assets used for the strange objects appear quite cartoonesque, so they may appear quite visually distinct from the real photos of elephants, boats, and airplanes in the MS-COCO dataset or the realistic objects in the nominal CARLA scenarios.</p>
<p>Learned Policy</p>
<p>Experimental Setup</p>
<p>We applied our LLM-based runtime monitor to a manipulation task, where we employed CLIPort [63].CLIPort is an end-to-end visuomotor policy, trained through imitation learning, that relies on language conditioning and is specifically designed to execute fine-grained manipulation tasks.We made use of a single multitask policy that had been trained to complete a wide range of tabletop tasks, including but not limited to stacking blocks, packing household items, and manipulating deformable objects.As a result of its diverse training, this multitask policy demonstrates an understanding of a wide range of concepts such as colors, common household objects (such as "a yoshi figure" or "a soccer cleat"), and directives (including "push," "place," and "pack") [63].</p>
<p>The intent behind this setup was to evaluate whether our LLM-based anomaly detector can effectively identify when such "known concepts" are assembled in a semantically distracting way in the context of a downstream task and compare with existing OOD detection methods.Therefore, to test the policy and our monitor, we define a manipulation task wherein the robot must pick and place blocks into bowls 4 and consider three variations of this task:</p>
<ol>
<li>Baseline: Two blocks of a randomly chosen color and two bowls of another randomly chosen color are placed on the workspace.This was one of the tasks used to train the CLIPort policy.</li>
</ol>
<p>Neutral Distractor: The baseline setup with</p>
<p>an additional randomly selected object.The object is visually distinct from the task blocks and bowls in both form and color.An example of one such distractor is the honey dipper seen in Figure 5.</p>
<p>Semantic Distractor: The baseline setup with</p>
<p>an additional randomly selected object.The object is meant to visually resemble either the blocks or bowls in some way to challenge the policy.An example of one such distractor is the spatula with a square purple head meant to look like the purple blocks in Figure 5.</p>
<p>We do so because despite its comprehensive multitask training, the CLIPort policy is still prone to failure, especially on variations of the pick and place task not seen at training time.Intuitively, we can likely attribute failures when distracting objects are present to contextual reasoning errors induced by spurious correlations, since the multi-task policy has seen many of the distractor objects in other tasks.For example, we may expect the policy to mistake a Butterfinger chocolate in a yellow wrapper for a yellow block even though the policy is aware of the Butterfinger chocolate concept.Therefore, in this experiment we investigate whether the LLM can reason about the differences between neutral and semantically distracting objects.We instantiated and simulated 250 episodes of each variant in Ravens [64], an environment consisting of a Universal Robot UR5e manipulator attached to the 0.5 m × 1 m workspace.Neutral and semantic distractors were sampled from the subset of the Google Scanned Objects dataset [65] used in [63] (see Appendix B.2). Examples of each task variant are shown in Figure 5.</p>
<p>Anomaly Detection Methodology</p>
<p>For these experiments, we compare our LLMbased anomaly detector with a commonly used OOD detection baseline.</p>
<p>LLM Anomaly Detection (Ours): For these experiments, we prompted the LLM for each neutral or semantic distractor episode to indicate whether the distractor was likely to yield degraded task performance.Prompts were generated by introducing the task specification (e.g., "put the red blocks in a gray bowl") and a brief visual description of the scene within a template 5 .The prompt template was designed to elicit a chainof-thought reasoning style, by querying the LLM to describe the shapes and colors of the blocks, bowls, and distractor and subsequently compare them to identify any possible sources of visual similarity and thus confusion.We refer the reader to the appendix for the full prompt template.</p>
<p>Autoencoder Anomaly Detection (Baseline): As a baseline OOD detection method, we trained a convolutional autoencoder with an L2 reconstruction loss on images from the CLIPort training tasks in Ravens [64].The reconstruction error of the autoencoder is a common OOD detection heuristic, because autoencoders learn to reconstruct the training images but fail to generalize on test images that are visually dissimilar from the training data, leading to high reconstruction error OOD [18,19,28,29].Similar to [41], we threshold the reconstruction error at the 95% quantile of the reconstruction losses on the  training examples for which the CLIPort policy was successful, yielding a classifier that flags an anomaly when the scene is visually distinct from nominal policy successes.</p>
<p>Results</p>
<p>In Figure 6, we compare the policy's performance across the task variants.It is clear that introducing distractors degrades the policy's performance, with the largest drop in performance on average corresponding to the semantic distractor cases.This is reflected both in the number of successful task completions and in the average task progress as measured by the mean reward (a reward of 0.5 is granted for each block that is placed in a bowl).</p>
<p>Qualitatively, we found that the policy would tend to "misinterpret" semantic distractors as task-relevant objects.For example, the spatula with a purple head in Figure 5 was indeed mistaken for a purple block.However, we also observed a significant proportion of erratic failures across all three experiment variants, such as the manipulator repeatedly reaching for empty areas of the workspace, even in experiments without distractors.The rate of such failures increased in the presence of neutral distractors.</p>
<p>Next, we evaluate the LLM's performance in detecting semantic anomalies.First, we compare the LLM's classification with that of human intuition and show the results in Table 5.Similar to the human classifications, the LLM flags the majority of semantic distractors and ignores most neutral distractors, thereby supporting the hypothesis that we can use LLMs to reason in alignment with a human's intuition about semantic anomalies.Disagreement tends to arise when the LLM makes decisions on the basis of ambiguous descriptions.For example, when the task involves an orange bowl, the LLM does not detect the presence of "a light brown fedora" distractor as a potential source of confusion on the basis that "light brown" and "orange" are different colors, even though they are actually visually similar.</p>
<p>Secondly, we contrast the LLMs anomaly classifications with the autoencoder baseline, also shown in Table 5.We see that the autoencoder flags more neutral distractors than the LLM, and only exceeds random guessing on the semantic distractors by a small margin.This indicates that although the episodes with semantic distractors appear too visually similar to in-distribution observations for common OOD detectors to notice, the LLM can still detect semantic anomalies by reasoning about the scene's context.</p>
<p>In addition to evaluating the LLM's performance in detecting semantic anomalies, we also Table 5 Anomaly detection on the CLIPort pick and place variations with neutral and semantic distractors.The detection rate is the fraction of episodes for which an anomaly was detected out of a total of 250 episodes for each task variation (i.e., the fraction of neutral distractors detected out of 250 episodes with a neutral distractor), for the human baseline, our LLM monitor, and the autoencoder comparison.</p>
<p>Anomaly Detection Rate Task Variant</p>
<p>Human LLM Autoencoder Neutral Distractor 0 0.16 0.24 Semantic Distractor 1.0 0.86 0.51 investigate whether its anomaly detection can predict task failures and compare against the autoencoder.Table 6 presents confusion matrices for the semantic and neutral distractor experiments.</p>
<p>Our findings indicate that the LLM-based monitor's anomaly detections correspond to a significant number of policy failures when a semantic distractor is present, although the number of false positives is high.However, the monitor understandably struggles to predict failures with only a neutral distractor present.In contrast, the autoencoder monitor has difficulties in both experiment classes.Over the course of these experiments we noticed that a substantial proportion of the policy failures were difficult to trace to a clear cause, since even without distractors present, the failure rate is approximately 30%.</p>
<p>Utimately, our evaluations indicate that this anomaly detection framework demonstrates strong alignment with human intuition.However, the unexplainable policy failures confound the failure prediction results motivating future work in grounding the LLM-based monitor with respect to system capabilities (see Section 5.2.2).</p>
<p>Discussion</p>
<p>The experimental results demonstrate the potential for LLMs to advance in-context and taskrelevant anomaly detection capabilities.The autonomous driving experiments illustrate that the LLM-based detection methodology exhibits the ability to identify semantic anomalies that could arise in complex and diverse scenarios.Furthermore, the manipulation experiments highlight an LLM's capacity to disentangle the subtle semantic distinctions that give rise to anomalous behavior, which classical OOD methods are not equipped to handle.In this section, we will present a series of illustrative examples to elucidate the strengths and limitations of our proposed methodology, and then proceed to discuss potential remedies for these limitations.</p>
<p>Qualitative Analysis</p>
<p>In order to provide a more detailed insight into the performance of our methodology, we turn our attention to a series of illustrative examples.In particular, we present six examples from our autonomous vehicle experiments in Figure 7.</p>
<p>Images (a) and (b) showcase the effectiveness of the LLM-based approach in detecting anomalies.These cases respectively correspond to reconstructions of the aforementioned Tesla failure cases; the first simulates the vehicle's encounter with a truck transporting an inactive traffic light and the second displays the vehicle's observation of a billboard with realistic stop sign imagery.Following these, images (c) and (d) depict unmodified, common traffic light and stop sign intersections, which the LLM correctly identifies as non-anomalous.</p>
<p>Notably, these cases demonstrate the LLM's capabilities to perform nuanced contextual reasoning.In the case of the stop sign on the billboard, the LLM recognizes that it is an image on a billboard and is not functional.Similarly, when confronted with the traffic light on the truck, the LLM can identify that it is not in its usual position but rather is being transported by the truck.In contrast, the LLM considers the observation of a simple stop sign or traffic light to be typical and well within the capabilities of an autonomous vehicle.</p>
<p>The final pair of images in Figure 7 highlight two examples that demonstrate the types of reasoning errors made by the LLM.Image (e) features a false anomaly alert due to an innocuous sighting of "a building by the road."Curiously, such detections in other observations do not trigger a warning and are appropriately deemed commonplace scenery.Image (f) similarly depicts an Table 6 Confusion matrices for fault detection on the CLIPort task variations.Each task variation consists of 250 episodes, for which we report the total number episodes on which the policy succeeded or failed (task success/failure) vs. the number of anomalies detected or missed.We report the confusion matrix for both our LLM detector and the autoencoder baseline.More generally, we qualitatively observe that the LLM's analysis tends to exploit the provided few-shot examples quite heavily as opposed to truly analyzing the scene description.We note that the provided justifications for the anomaly characterizations often closely reflect or overfit to the reasoning provided in the examples, with limited novelty.Zero-shot prompting was found to leverage the LLM's latent knowledge most effectively, yielding the most creative responses at the cost of task-relevance and reliability, limiting their utility for anomaly detection.As of yet, this novelty-relevance trade off is a matter of prompt tuning, but upcoming foundation models are suggested to feature improved creativity and reasoning capabilities [66,67].We further discuss constraining the LLM's responses with respect to task-relevant and system-capabilities in Section 5.2.2.</p>
<p>LLM</p>
<p>In light of the current limitations and ongoing foundation model advances, we present a discussion on possible avenues for future development of semantic anomaly detection in the following section.</p>
<p>Research Outlook</p>
<p>Multimodal Context</p>
<p>Accurate and comprehensive scene descriptions are crucial for effective semantic anomaly detection using LLMs.An LLM's ability to distinguish normal from anomalous observations often relies on subtle qualifications of scene descriptions.For instance, distinguishing between a "truck carrying a traffic light" and a common "traffic light" results in the former being considered an anomaly and the latter being a commonplace observation.</p>
<p>However, natural language, while a powerful and flexible means of conveying information, has a fundamental limitation in producing ambiguous or underspecified descriptions of scenes and objects.Consider the observation of a "house on the sidewalk," which the LLM wrongly flags as an anomaly due to the lack of specificity and context in the object description.Instead of interpreting this description to indicate a house located near the sidewalk, the LLM understands it to mean a house physically placed on the sidewalk, obstructing the road.Crucially, the lack of specificity and limited context afforded by the object description allow the LLM to justify this misclassification.</p>
<p>To address these issues, it is essential to move beyond purely natural language prompting.Multimodal models such as GPT-4 [67] and Flamingo [55] can incorporate both visual and textual inputs to provide a more direct means of conveying the robot's observations.Directly incorporating images into the prompt can better preserve context, overcoming the limitations associated with natural language descriptions and minimizing information losses incurred through the visual parsing process.</p>
<p>Grounding System Capabilities</p>
<p>Although LLMs are be trained on general datasets which provide a broad basis for contextual reasoning, a target system possesses a specific set of skills.In order to effectively integrate an LLM-based anomaly detector within an autonomy stack, it is necessary to "inform" the LLM of these system-specific capabilities and limitations.</p>
<p>For example, an autonomous vehicle programmed for urban environments may have different failure modes and anomalous responses than one designed to operate in rural areas.</p>
<p>To an extent, this is achieved through few-shot and chain-of-thought prompting, as the provided examples and logical structure "prime" the LLM as to how to characterize anomalies.However, this is an indirect strategy that only conveys a system's competencies implicitly and incompletely.Furthermore, this requires manual prompt engineering on a per-system basis, is subject to the designers' biases and oversights, and does not account for account for system improvements or updates over time.More comprehensive solutions merit consideration, such as fine-tuning the LLM on system-specific data or more tightly integrating the LLM within the autonomy stack to jointly reason over observations, states and actions.</p>
<p>Complementary Anomaly Detection and Mitigation</p>
<p>Our LLM-based anomaly detection methodology offers a powerful tool for monitoring robot observations at a contextual level.However, data-driven systems may sometimes fail unexpectedly or inexplicably, as evidenced by our CLIPort experiments.In light of this, we recommend conducting a more comprehensive study to investigate how our method can complement other OOD detection methods that are better suited to identifying and explaining such failure modes.This would enable us to develop a more complete and robust monitoring suite that can provide deeper insights into system performance and potential issues.Beyond anomaly detection, fault mitigation and recovery are important related considerations.Although these problems are beyond the scope of this work, an LLM-based approach to anomaly detection allows for the formulation of structured queries to either initiate system recovery or request supervisor assistance, which could facilitate the process of devising contingency strategies or simplifying the required human intervention required.In fact, recent works have demonstrated that LLMs can generate feedback to propose improvements or corrections for control tasks [68], produce an executable plan to rectify failures based on recent experiences [69], or simply ask a human for help when needed [70].</p>
<p>Conclusion</p>
<p>As modern robotic systems continue to evolve and become more sophisticated, the possibility of encountering challenging corner cases that may degrade system performance or pose safety hazards persists.The semantic anomalies that this study aims to address represent a particularly difficult subset of these cases, requiring a level of insight and reasoning capabilities akin to those of humans.Given their extensive training data, largely derived from human experience, LLMs possess the contextual knowledge and demonstrate strong emergent reasoning capabilities, making them compelling tools for semantic anomaly detection.</p>
<p>The proposed framework leverages LLMs' reasoning abilities to detect semantic anomalies.Our experiments have demonstrated that the LLMbased monitor matches human intuition in both fully end-to-end policies and classical autonomy stacks that utilize learned perception.We found that semantic anomalies do not always correspond to semantically-explainable failures, particularly for end-to-end policies which can behave erratically.However, LLM-based monitoring lends itself to multiple avenues of further developement, such as by 1) more tightly coupling robot perception to LLM prompts, 2) grounding LLM outputs with respect to system capabilities, and 3) investigating the complementarity of the LLM-based monitor with other OOD/anomaly monitors.</p>
<p>Compliance with Ethical Standards</p>
<p>Not applicable.</p>
<p>The following template was designed to prompt an analysis of the autonomous vehicle's scene observations.Placeholders are indicated by the braces and are substituted for the relevant information at each query.The following prompt was designed to elicit a comparison of the distractor objects and the blocks and bowls from the LLM.Placeholders are indicated by the braces and are substituted for the relevant information at each query.</p>
<p>You are a robot's fault monitor responsible for detecting situations that could confuse the robot and adversely affect its performance.The robot is a manipulator attached to a table and its task is to pick up blocks of a specified color and place them into bowls of a specified color.However, the robot's vision is prone to making mistakes and may misinterpret an object as a block or a bowl if that object has a similar appearance.Your job is to analyze what the robot sees and identify any objects that may cause it to make a mistake in performing its task.You should flag any objects that have any similarity with the blocks or bowls just in case.The robot's task is to put the {block_color} blocks in a {bowl_color} bowl.</p>
<p>On the table, the robot sees {scene_objects}</p>
<p>Analyze each object.First, describe the appearance of each object focusing on the shape and color of each constituent component.Then, indicate whether an object or part of an object may resemble the {block_color} block or {bowl_color} bowl in any way, and whether the robot is likely to misidentify the object.</p>
<p>For We chose to abstain from using few-shot prompting for this set of experiments.We noted that the diversity exhibited by the common household object classes used as distractors (as compared to driving objects classes, such as traffic lights and signals exhibit some degree of standardization features) necessitated some degree of zero-shot reasoning by the LLM.This zeroshot prompting strategy encouraged the LLM to leverage its inherent knowledge of common objects more effectively.In contrast, when fewshot prompted, we found that the responses tended to overfit to the provided examples, negatively impacting the LLM's function as a monitor.</p>
<p>Fig. 3
3
Fig.3An overview of the autonomous driving semantic anomaly detection methodology.Vehicle perception is sampled at a frequency of 2 Hz and passed through an open vocabulary object detector to generate a scene description.The scene description is incorporated within a prompt template, which is used to query the LLM.Note that the prompt template and response depicted are respectively paraphrased from the full prompt (Appendix A) and excerpted from the full output.</p>
<p>Fig. 4
4
Fig.4Example of an object detection error.The stop sign in the scene is mistaken for an "image of a stop sign on a billboard."</p>
<p>Fig. 5
5
Fig. 5 An instance of each manipulation task variant: 1. Baseline, 2. Neutral Distractor, 3. Semantic Distractor.</p>
<p>Fig. 6
6
Fig.6CLIPort policy performance on task variants.An episode is considered successful if all blocks are successfully placed into bowls.The mean reward indicates the average task progress across the set of episodes, with a reward of 0.5 granted for each block that is placed in a bowl.</p>
<p>Fig. 7
7
Fig. 7 Illustrative observations from various simulated autonomous driving scenarios.Images (a) and (b) correspond to images that the LLM-based monitor correctly identifies as semantic anomalies.Images (c) and (d) depict common scenes that the monitor correctly identifies as normal.Images (e) and (f) demonstrate mistakes made by the monitor.</p>
<p>Table 2
2
Results of our LLM-based monitor on finite state machine-based autonomous vehicle stack in the CARLA simulator using ground truth scene descriptions.
Nominal Episodes Stop Traffic TotalStopAnomalous Episodes Traffic Strange TotalTotalSignsLightsSignsLightsObjectsSemantic Anomalies0001619155050Nominal Observations 3094948032481973377821585TPRN/AN/AN/A1.01.01.01.01.0FNRN/AN/AN/A0.000.000.000.000.0TNR0.970.850.900.960.941.000.950.92FPR0.030.150.100.040.060.000.050.08</p>
<p>Table 3
3
Baseline comparison results for semantic anomaly detection using the LLM monitor, SCOD, and the Mahalanobis distance on the FSM-based AV stack in the CARLA simulator.As in Table1, the TPR and TNR indicate the true positive and negative rates of an OOD detectors' ability to detect semantic anomalies.
Nominal Episodes Stop Traffic TotalStopAnomalous Episodes Traffic Strange TotalTotalSignsLightsSignsLightsObjectsLLMTPR TNRN/A 0.91N/A 0.54N/A 0.680.94 0.860.84 0.801.0 0.930.92 0.880.92 0.78SCODTPR TNRN/A 0.94N/A 0.96N/A 0.950.13 0.890.63 0.970.07 0.890.30 0.910.30 0.93Mahal.TPR TNRN/A 0.93N/A 0.96N/A 0.950.31 0.700.47 0.770.67 0.500.48 0.630.48 0.79</p>
<p>Table 4
4
Results on detection of errors in the outputs of the object detector used in the FSM-based AV stack: A true positive indicates that a warning was issued when the detector produces an erroneous bounding-box or misclassifies an object.A false positive indicates that a warning was issued when the object detector correctly identified all objects relevant to the driving task.We only consider the nominal episodes here to avoid any ambiguity on whether e.g., a detection of an image of a stop sign on a billboard as a stop sign qualifies as a perception error.
Nominal EpisodesMethodTPRFPRLLM0.090.42SCOD0.800.46Mahalanobis0.390.55</p>
<p>Can this influence the vehicle's behavior?No, they are on the sidewalk and not on the road.3. Can the vehicle drive safely in its presence?Yes, cyclists are commonly seen on the road and the vehicle should be able to drive safely in their presence.4. Can this cause the vehicle to make unpredictable or unsafe maneuvers?No, they are on the sidewalk and not on the road.Classification: Normal.Car on the road: 1.Is this common to see while driving?Yes, cars are common to see while driving.2. Can this influence the vehicle's behavior?Yes, the autonomous vehicle must respect other vehicles on the road, avoid collisions and obey the rules of the road.3. Can the vehicle drive safely in its presence?Yes, cars are commonly seen on the road and the autonomous vehicle should be able to drive safely in their presence.4. Can this cause the vehicle to make unpredictable or unsafe maneuvers?
Appendix B AdditionalOverall Scenario Classification: AnomalyExperimental Details:. I am driving on the road and I see: "Learned Policy{scene_description}B.1 Prompt TemplateI am the fault monitor for a vision-based autonomous vehicle. My job isto analyze the vehicle'sobservations and identify anythingthat could cause the vehicle to takeactions that are unsafe, unpredictable or violate traffic rules. For each object that the vehicle observes, I will reason about whether the object constitutes a normal observation or an anomaly. Normal observations do not detrimentally affect the vehicle's performance, whereas anomalies might . Finally, I will classify whether the overall scene is normal or abnormal. For example, "The vehicle is driving on the road and observes: -a cyclist on the sidewalk -a car on the road -a pedestrian carrying a bright green balloon Cyclist on the sidewalk: 1. Is this common to see while driving? Yes, cyclists can often be seen riding on the road or occasionally on sidewalks. 2. No, autonomous vehicles are programmed to appropriately drive and interact with other cars on the road . Classification: Normal. Pedestrian carrying a bright green balloon: 1. Is this common to see while driving? Pedestrians are commonly seen on the sidewalk or crossing at intersections. They may possess arbitrary objects and a balloon is a reasonable object to carry. 2. Can this influence the vehicle's behavior? Yes, the autonomous vehicle may mistake the green balloon for a green traffic light signal, which could deceive it into driving forward when it should otherwise be stopped (e.g., at a red light). 3. Can the vehicle drive safely in its presence? No, this could deceive the vehicle into interpreting the greenballoon as a legal traffic signal.4. Can this cause the vehicle to makeunpredictable or unsafe maneuvers?Yes, this could deceive theautonomous vehicle into drivingforward when it should otherwise bestopped (e.g., at a red light)Classification: Anomaly.</p>
<p>each object follow the template:</p>
<Object>:Object Description:Object Similarity with Block Shape(explain):Object Similarity with Block Color(explain):Misidentifiable as Block (yes or no):Object Similarity with Bowl Shape(explain):Object Similarity with Bowl Color(explain):Misidentifiable as Bowl (yes or no):After all of the objects have beenanalyzed, indicate whether there anyof the objects can cause the robotto make a mistake. Indicate thiswith the template:Misidentifiable Objects Present (yes orno):

Table B1 -
B1
continued from previous page Image Description Rocket Raccoon Figure: Neutral distractor.

Screw Driver: Neutral distractor.Silver Tape: Semantic distractor for gray bowl.Spatula with Purple Head: Semantic distractor for purple block.

https://futurism.com/the-byte/ tesla-autopilot-bamboozled-truck-traffic-lights
https://www.youtube.com/watch?v=-OdOmU58zOw
Although we use YOLOv8[60] in our vehicle planner, we find that DETR yields similar performance. We apply the baselines to DETR as it is trained on the same dataset as YOLOv8, though its architecture is more amenable to applying traditional OOD detectors.
This task is adapted from the put-blocks-in-bowl task defined by[63].
In these experiments, we generated these scene descriptions using privileged simulator information. In principle, an object detector could have been used to identify the objects involved in our experiments, however we found that the simulator visuals were not amenable to pretrained detection models.
Declarations 6.1 LLM Usage StatementLLMs were exclusively used for the experimentation and evaluation of our proposed methodology.OpenAI's text-davinci-003 model was used for all experiments.LLMs were not used in the composition of this manuscript.FundingThe NASA University Leadership initiative (grant #80NSSC20M0163) and Toyota Research Institute provided funds to support this work.Amine Elhafsi is supported by a NASA NSTGRO fellowship (grant #80NSSC19K1143).This article solely reflects the opinions and conclusions of its authors and not any NASA, TRI, or Toyota entity.Competing InterestsNot applicable.Consent to ParticipateNot applicable.Consent for PublicationThe authors unanimously endorsed the content and provided explicit consent for submission.They also ensured that consent was obtained from the responsible authorities at the institute(s)/organization(s) where the research was conducted.Availability of Data and MaterialsRelevant documentation, data and/or code is readily available to verify the validity of the results presented upon request.Code AvailabilityThe authors plan to open source their code.Author ContributionsAmine Elhafsi initiated the project, developed the methodology, performed prompt tuning, and implemented and conducted the experiments.Rohan Sinha prepared the structure for the CARLA autonomous vehicle stack, conducted autonomous vehicle experiments, computed autoencoder OOD detector baseline metrics, processed experimental results, and performed data analysis.Christopher Agia implemented the autoencoder OOD detector baseline for the learned policy experiments.Edward Schmerling implemented the autonomous vehicle traffic light classification, performed data analysis, and advised the project.Issa A. D. Nesnas advised the project.Marco Pavone was the primary advisor for the project.The manuscript was jointly written by Amine, Rohan and Edward.All authors reviewed and revised the manuscript.Appendix A Additional Details: Reasoning-Based Policy B.2 Semantic and Neutral DistractorsTableB1: Distractors used in the learned policy experiments.We include an image of each distractor alongside its usage in the experiment.Note that semantic distractors were also used as neutral distractors when the task involved blocks or bowls with colors dissimilar to those of the object for additional variety.Image DescriptionAndroid Toy: Semantic distractor for green block.Ball Puzzle: Semantic distractor for blue bowl.Pepsi Wild Cherry Box: Semantic distractor for red block.Pink Towel: Semantic distractor for pink and white blocks.Porcelain Spoon: Semantic distractor for white bowl.Purple Tape: Semantic distractor for purple bowl.Flashlight: Neutral Distractor.Red Cup: Semantic distractor for red bowl.Continued on next page
Wilds: A benchmark of in-the-wild distribution shifts. Pang Wei, Koh , Proceedings of the 38th International Conference on Machine Learning. Marina Meila, Tong Zhang, the 38th International Conference on Machine LearningPMLRJul 2021139

Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V Dillon, Balaji Lakshminarayanan, Jasper Snoek, Proceedings of the 33rd International Conference on Neural Information Processing Systems. the 33rd International Conference on Neural Information Processing SystemsRed Hook, NY, USACurran Associates Inc2019

Shortcut learning in deep neural networks. Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A Wichmann, Nature Machine Intelligence. 211Nov 2020

Unbiased look at dataset bias. Antonio Torralba, Alexei A Efros, CVPR 2011. 2011

A system-level view on out-of-distribution data in robotics. Rohan Sinha, Apoorva Sharma, Somrita Banerjee, Thomas Lew, Rachel Luo, Yixiao Spencer M Richards, Edward Sun, Marco Schmerling, Pavone, 2022

Sketching curvature for efficient out-of-distribution detection for deep neural networks. Apoorva Sharma, Navid Azizan, Marco Pavone, Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence. Cassio De Campos, Marloes H Maathuis, the Thirty-Seventh Conference on Uncertainty in Artificial IntelligencePMLRJul 2021161of Proceedings of Machine Learning Research

Three-dimensional reconstruction using sfm for actual pedestrian classification. Francisco Gomez-Donoso, Julio Castano-Amoros, Felix Escalona, Miguel Cazorla, Expert Systems with Applications. 2131190062023

BDD100K: A diverse driving dataset for heterogeneous multitask learning. Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, Trevor Darrell, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2020

Towards robust cnn-based object detection through augmentation with synthetic rain variations. Georg Volk, Stefan Müller, Dennis Alexander Von Bernuth, Oliver Hospach, Bringmann, 2019 IEEE Intelligent Transportation Systems Conference (ITSC). 2019

A survey of unsupervised deep domain adaptation. Garrett Wilson, Diane J Cook, ACM Trans. Intell. Syst. Technol. 115jul 2020

A continual learning survey: Defying forgetting in classification tasks. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, Tinne Tuytelaars, IEEE Transactions on Pattern Analysis and Machine Intelligence. 4472022

Data lifecycle management in evolving input distributions for learning-based aerospace applications. Somrita Banerjee, Apoorva Sharma, Edward Schmerling, Max Spolaor, Michael Nemerouf, Marco Pavone, Computer Vision -ECCV 2022 Workshops. Leonid Karlinsky, Tomer Michaeli, Ko Nishino, ChamSpringer Nature Switzerland2023

Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033

Language models of code are few-shot commonsense learners. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, Graham Neubig, arXiv:2210.071282022arXiv preprint

Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022

Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. 2022

A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges. Mohammadreza Salehi, Hossein Mirzaei, Dan Hendrycks, Yixuan Li, Mohammad Hossein Rohban, Mohammad Sabokrou, 2021

A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE. Lukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Grégoire Montavon, Wojciech Samek, Marius Kloft, Thomas G Dietterich, Klaus-Robert Müller, 2021109

Generalized out-ofdistribution detection: A survey. Jingkang Yang, Kaiyang Zhou, Yixuan Li, Ziwei Liu, arXiv:2110.113342021arXiv preprint

A baseline for detecting misclassified and out-ofdistribution examples in neural networks. Dan Hendrycks, Kevin Gimpel, International Conference on Learning Representations. 2017

Enhancing the reliability of out-ofdistribution image detection in neural networks. Shiyu Liang, Yixuan Li, Srikant, ICLR 20186th International Conference on Learning Representations. 2018

Mood: Multi-level out-of-distribution detection. Ziqian Lin, Sreya Dutta Roy, Yixuan Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2021

Deep one-class classification. Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Ahmed Shoaib, Alexander Siddiqui, Emmanuel Binder, Marius Müller, Kloft, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLRJul 201880of Proceedings of Machine Learning Research

Deep autoencoding gaussian mixture model for unsupervised anomaly detection. Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Dae Ki Cho, Haifeng Chen, International Conference on Learning Representations. 2018

A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin, Advances in Neural Information Processing Systems. S Bengio, H Wallach, H Larochelle, K Grauman, N Cesa-Bianchi, R Garnett, Curran Associates, Inc201831

Improving reconstruction autoencoder out-of-distribution detection with mahalanobis distance. Taylor Denouden, Rick Salay, Krzysztof Czarnecki, Buu Vahdat Abdelzad, Sachin Phan, Vernekar, arXiv:1812.027652018arXiv preprint

Contrastive language-image pretrained (clip) models are powerful out-of-distribution detectors. Felix Michels, Nikolas Adaloglou, Tim Kaiser, Markus Kollmann, 2023

C2ae: Class conditioned auto-encoder for open-set recognition. Poojan Oza, M Vishal, Patel, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019

A novelty detection approach to classification. Nathalie Japkowicz, Catherine E Myers, Mark A Gluck, International Joint Conference on Artificial Intelligence. 1995

A review of uncertainty quantification in deep learning: Techniques, applications and challenges. Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana Rezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul Fieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra Acharya, Vladimir Makarenkov, Saeid Nahavandi, Information Fusion. 762021

Simple and scalable predictive uncertainty estimation using deep ensembles. Alexander Balaji Lakshminarayanan, Charles Pritzel, Blundell, Advances in Neural Information Processing Systems. I Guyon, U Von Luxburg, S Bengio, H Wallach, R Fergus, S Vishwanathan, R Garnett, Curran Associates, Inc201730

Dropout as a bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, Proceedings of The 33rd International Conference on Machine Learning. Maria , Florina Balcan, Kilian Q Weinberger, The 33rd International Conference on Machine LearningNew York, New York, USAPMLR20-22 Jun 201648of Proceedings of Machine Learning Research

A scalable laplace approximation for neural networks. Hippolyt Ritter, Aleksandar Botev, David Barber, 6th International Conference on Learning Representations, ICLR 2018-Conference Track Proceedings. 20186International Conference on Representation Learning

Deep evidential regression. Alexander Amini, Wilko Schwarting, Ava Soleimany, Daniela Rus, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033

Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Morteza Ibrahimi, Xiuyuan Lu, Benjamin Van Roy, Epistemic neural networks. 2023

Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033

Monitoring and diagnosability of perception systems. Pasquale Antonante, David I Spivak, Luca Carlone, 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2021

Detect, reject, correct: Crossmodal compensation of corrupted sensors. Michelle A Lee, Matthew Tan, Yuke Zhu, Jeannette Bohg, 2021 IEEE International Conference on Robotics and Automation (ICRA). 2021

Introspective perception: Learning to predict failures in vision systems. Shreyansh Daftry, Sam Zeng, J Andrew Bagnell, Martial Hebert, 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2016

Ivoa: Introspective vision for obstacle avoidance. Sadegh Rabiee, Joydeep Biswas, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE Press2019

Safe visual navigation via deep learning and novelty detection. Charles Richter, Nicholas Roy, RSS. July 2017

Robustness to out-ofdistribution inputs via task-aware generative uncertainty. Rowan Mcallister, Gregory Kahn, Jeff Clune, Sergey Levine, ICRA. 2019

In search of lost domain generalization. Ishaan Gulrajani, David Lopez-Paz, International Conference on Learning Representations. 2021

Invariant risk minimization. Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, David Lopez-Paz, 

Causal confusion in imitation learning. Dinesh Pim De Haan, Sergey Jayaraman, Levine, Advances in Neural Information Processing Systems. Curran Associates, Inc201932

Kimera: From slam to spatial perception with 3d dynamic scene graphs. Antoni Rosinol, Andrew Violette, Marcus Abate, Nathan Hughes, Yun Chang, Jingnan Shi, Arjun Gupta, Luca Carlone, The International Journal of Robotics Research. 4012-142021

Leveraging large language models for robot 3d scene understanding. William Chen, Siyi Hu, Rajat Talak, Luca Carlone, 2022

Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. Dhruv Shah, Sergey Lażej Osiński, Levine, Conference on Robot Learning. PMLR2023

Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, 6th Annual Conference on Robot Learning. 2022

Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, Jeannette Bohg, arXiv:2303.12153Text2motion: From natural language instructions to feasible plans. 2023arXiv preprint

Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on Robot Learning. PMLR2023

Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Pete Florence, Andy Zeng, Workshop on Language and Robotics at CoRL 2022. 2022

Can foundation models perform zero-shot task specification for robot manipulation?. Yuchen Cui, Scott Niekum, Abhinav Gupta, Vikash Kumar, Aravind Rajeswaran, Learning for Dynamics and Control Conference. PMLR2022

Socratic models: Composing zero-shot multimodal reasoning with language. Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, arXiv:2204.005982022arXiv preprint

Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 2022

Blip: Bootstrapping languageimage pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, International Conference on Machine Learning. PMLR2022

Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Yevgen Huang, Pierre Chebotar, Daniel Sermanet, Sergey Duckworth, Vincent Levine, Karol Vanhoucke, Marc Hausman, Klaus Toussaint, Andy Greff, Igor Zeng, Pete Mordatch, Florence, arXivpreprintarXiv:2303.033782023

Simple openvocabulary object detection with vision transformers. Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, Neil Houlsby, 2022ECCV

Carla: An open urban driving simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, Conference on robot learning. PMLR2017

. Glenn Jocher, Ayush Chaurasia, Jing Qiu, January 2023YOLO by Ultralytics

Sketching curvature for efficient out-of-distribution detection for deep neural networks. Apoorva Sharma, Navid Azizan, Marco Pavone, Uncertainty in Artificial Intelligence. PMLR2021

End-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Alexander Usunier, Sergey Kirillov, Zagoruyko, European conference on computer vision. Springer2020

Cliport: What and where pathways for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Proceedings of the 5th Conference on Robot Learning (CoRL). the 5th Conference on Robot Learning (CoRL)2021

Transporter networks: Rearranging the visual world for robotic manipulation. Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, Johnny Lee, Conference on Robot Learning (CoRL). 2020

Google scanned objects: A high-quality dataset of 3d scanned household items. Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas B Mchugh, Vincent Vanhoucke, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022

Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023arXiv preprint

. OpenAI. Gpt-4 technical report. 2023

Generating language corrections for teaching physical control tasks. Megha Srivastava, Noah Goodman, Dorsa Sadigh, arXiv:2306.070122023arXiv preprint

Reflect: Summarizing robot experiences for failure explanation and correction. Zeyi Liu, Arpit Bahety, Shuran Song, arXiv:2306.157242023arXiv preprint

Anushri Allen Z Ren, Alexandra Dixit, Sumeet Bodrova, Stephen Singh, Noah Tu, Peng Brown, Leila Xu, Fei Takayama, Jake Xia, Varley, arXiv:2307.01928Robots that ask for help: Uncertainty alignment for large language model planners. 2023arXiv preprint            </div>
        </div>

    </div>
</body>
</html>
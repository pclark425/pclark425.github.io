<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9164 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9164</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9164</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-274436066</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.00821v1.pdf" target="_blank">Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) demonstrate remarkable capabilities in various reasoning tasks. However, they encounter significant challenges when it comes to scientific reasoning, particularly in physics, which requires not only mathematical reasoning but also factual and conceptual understanding. When addressing complex physics problems, LLMs typically face three key issues: problem miscomprehension, incorrect concept application, and computational errors. While each of these problems can be addressed individually, there is a need for a generalized approach that can tackle all three issues simultaneously. To address this, we introduce Mixture of Refinement Agents (MoRA), a novel agentic refinement framework that iteratively refines the LLM generated base solution by correcting the aforementioned errors, resulting in a significant performance improvement for open-source LLMs. Our approach aims to bridge the gap between opensource LLMs and GPT-4o by utilizing the latter as error identifier to guide these refinement agents. We evaluate our approach on the SciEval and MMLU subsets along with our own physics dataset (PhysicsQA). MoRA significantly improves the performance of Llama-3-70B and Gemma-2-27B on these datasets, achieving up to a 16% increase in final answer accuracy.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9164.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9164.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMa-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source large language model (70 billion parameters) used in this work to generate chain-of-thought solutions for physics problems and as the base LLM refined by the MoRA framework.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLM with ~70B parameters (referred to as LLaMa-3-70B in the paper); used to generate CoT solutions and to run refinement agents (same LLM is used for refinements). Training corpus/details not specified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Physics (high-school / undergraduate physics problem solving; benchmarks: PhysicsQA, SciEval-Static, MMLU High School & College)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of physics problem solving: generate chain-of-thought (CoT) step-by-step solutions, apply physics concepts/formulae, perform algebraic/arithmetic computations (with optional code-driven computation refinement), and be iteratively refined by MoRA agents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final-answer accuracy on multiple benchmarks (percentage correct answers); additionally concept and computation diagnostic scores (Score_concept, Score_comp) and refinement rates for agent ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported final answer accuracies: SciEval-Static CoT 82.23% -> with MoRA 86.58%; PhysicsQA CoT 56.76% -> with MoRA 70.14%; MMLU High School CoT 72.88% -> with MoRA 78.81%; MMLU College 71.76% -> with MoRA 78.82%. (All values are percentages of final-answer accuracy.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Identified factors: (1) problem miscomprehension (objective/variable misuse), (2) incorrect concept/formula application, (3) computational/algebraic errors; model class (open-source vs advanced models like GPT-4o), prompt/instructioning (CoT, 3-shot), availability of external knowledge (GraphRAG / physics knowledge base), use of program-aided computation (code generation + execution), and the iterative agentic refinement strategy (MoRA) and routing/prioritization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to baselines Answer-Only (AO), Chain-of-Thought (CoT), and Few-shot (3-shot). MoRA substantially improves over CoT and 3-shot for LLaMa-3-70B across datasets (examples above). GPT-4o is used as a stronger reference for error identification and is described as having near-perfect comprehension and higher concept/computation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLaMa-3-70B (as an open-source model) shows: occasional miscomprehension, moderate-to-high rates of conceptual errors (e.g., ~18.11% conceptual errors on PhysicsQA average across opensource models) and computational errors (~21.62% on PhysicsQA average), and limited ability to reliably self-identify its own errors without an external verifier (paper uses GPT-4o for error identification). Refinement agent performance varies by dataset (concept-refinement rates were moderate; computation-refinement worked well but not perfect).</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend using an external strong model (GPT-4o) for error identification, coupling concept refinement with a domain knowledge retrieval (GraphRAG + physics KB), using code generation + execution for computational corrections, and an iterative prioritized routing (miscomprehension -> concept -> computation) to maximize final-answer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9164.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9164.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-2-27B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-2 27B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source medium-scale language model (27 billion parameters) used to generate CoT physics solutions and to evaluate the effectiveness of MoRA refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-27B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLM with ~27B parameters (referred to as Gemma-2-27B); used as a base solver whose outputs are refined by MoRA agents. Detailed training data not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Physics (high-school physics problem solving; benchmarks include PhysicsQA, SciEval-Static, MMLU High School & College)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simulation of physics problem solving: produce chain-of-thought solutions, apply physics concepts/formulae, perform computations; participate in iterative concept and computation refinements driven by MoRA.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final-answer accuracy on benchmarks (percentage correct); concept and computation scores (Score_concept, Score_comp); agent refinement rates in ablation studies.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported accuracies: SciEval-Static CoT 79.26% -> MoRA 88.76%; PhysicsQA CoT 54.59% -> MoRA 70.62%; MMLU College CoT 73.52% -> MoRA 82.20%; MMLU High School CoT 77.11% -> MoRA 75.88% (noting a reported decrease on that particular dataset). (Values are percentages.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Same three primary error sources (miscomprehension, concept application, computation); sensitivity to retrieval of correct physics concepts (needs external KB); benefits from code-driven computation refinement; variability across datasets and tasks; model capacity (27B) and its ability to generate effective retrieval thoughts affects concept refinement performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to AO, CoT, and 3-shot baselines; MoRA yields sizable improvements over CoT and 3-shot on most datasets. GPT-4o is treated as a higher-performing system for error identification and overall reference in analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Gemma-2-27B shows non-uniform refinement performance: strong computation-refinement in some datasets (e.g., 100% in MMLU College computational refinement in ablation) but weak conceptual retrieval/refinement in some cases (e.g., low concept-refinement rates on MMLU High School). It also relies on external error identification (GPT-4o) and external knowledge retrieval for concept fixes.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend combining Gemma-2-27B with automated error identification (GPT-4o), GraphRAG retrieval to a physics KB for concept fixes, and code-execution for computational fixes. Iterative prioritized routing of refinement agents improves final accuracy significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9164.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9164.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (as verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (used as an external error identifier and verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An advanced OpenAI model used in this work solely for identifying and locating errors in other LLMs' CoT solutions (flags for objective/variable misalignment and two diagnostic scores for concept and computation correctness).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A strong OpenAI LLM (version '4o' in the paper) selected for superior performance in problem comprehension and physics concept application; used with OpenAI Code Interpreter for computation verification.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Physics (used as an automated verifier/evaluator for physics problem solutions across PhysicsQA, SciEval-Static, MMLU datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based verification/simulation of solution correctness: (1) assign Problem Comprehension Flags (F_obj, F_val), (2) compute Score_concept (locates first concept/formula error step), and (3) compute Score_comp (via Code Interpreter executes Python to verify computations), guiding which refinement agents to route.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Used internally to produce diagnostic flags and scores; authors also report high concept and computation accuracies from GPT-4o (e.g., 'near-perfect' comprehension and average concept accuracy ~96.4% and computation accuracy ~95.64% across datasets in analysis). Final-answer accuracy as a solver is referenced qualitatively (near-perfect on several datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Diagnostic performance reported in analysis: GPT-4o achieves near-perfect problem comprehension on SciEval-Static, MMLU College and High School; averaged concept-correctness ~96.4% and computation-correctness ~95.64% across the four datasets (values reported by the authors as averages). The paper does not provide a complete per-benchmark final-answer accuracy table for GPT-4o as a solver but describes it qualitatively as excelling.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>GPT-4o's stronger internal reasoning and ability to run Code Interpreter for numerical verification improves identification accuracy; access to code execution reduces false computation flags; its broader training/architectural strengths compared to open-source models are cited implicitly. Error tolerance for computation verification was set to 0.1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a verifier/referee against which open-source models' solutions are judged; described as outperforming LLaMa-3-70B and Gemma-2-27B in concept and computation diagnostics. Not compared as a primary solver baseline numerically in the main result tables.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>The paper uses GPT-4o only for error identification (not for end-to-end solution generation in the MoRA pipeline). Authors note dependence on GPT-4o for reliable error localization because open-source LLMs often cannot self-identify mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors propose leveraging a strong verifier like GPT-4o to locate errors and produce diagnostic scores that drive specialized refinement agents; they emphasize using Code Interpreter capability of GPT-4o for robust computation verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9164.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9164.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-1.5-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini 1.5 Flash</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performing LLM referenced and evaluated in the paper (used to compare CoT performance on SciEval-Static), noted for strong accuracy on the SciEval-Static benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-1.5-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A commercially available LLM (Gemini family) referenced in the experiments; exact parameter count or training details are not stated in the paper, but it's reported to have competitive CoT performance on SciEval-Static.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Physics (evaluated on SciEval-Static benchmark for scientific reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based scientific reasoning / problem solving on SciEval-Static: generate CoT solutions for scientific (physics) questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Final-answer accuracy (percentage correct) on SciEval-Static.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported SciEval-Static CoT accuracy 85.97% (percentage correct) in the paper's descriptive results section.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Implied factors include model capability/capacity, prompting strategy (CoT), and nature of the benchmark (SciEval-Static). The paper emphasizes that advanced models perform better on comprehension and concept application.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared qualitatively/numerically against LLaMa-3-70B and other models on SciEval-Static; outperforms some open-source models in reported SciEval-Static CoT accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No detailed failure analysis presented for Gemini-1.5-Flash specifically in the paper; it is included as a point of comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Advanced proprietary models like Gemini-1.5-Flash show strong baseline performance on scientific reasoning benchmarks, motivating methods (like MoRA) to raise open-source models closer to that level via targeted refinement and external verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research <em>(Rating: 2)</em></li>
                <li>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification <em>(Rating: 2)</em></li>
                <li>MathCoder: Seamless code integration in LLMs for enhanced mathematical reasoning <em>(Rating: 2)</em></li>
                <li>PAL: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Selfcheck: Using LLMs to zero-shot check their own step-by-step reasoning <em>(Rating: 1)</em></li>
                <li>Structured chemistry reasoning with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9164",
    "paper_id": "paper-274436066",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "LLaMa-3-70B",
            "name_full": "LLaMA-3 70B",
            "brief_description": "An open-source large language model (70 billion parameters) used in this work to generate chain-of-thought solutions for physics problems and as the base LLM refined by the MoRA framework.",
            "citation_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-70B",
            "model_description": "Open-source transformer LLM with ~70B parameters (referred to as LLaMa-3-70B in the paper); used to generate CoT solutions and to run refinement agents (same LLM is used for refinements). Training corpus/details not specified in paper.",
            "scientific_subdomain": "Physics (high-school / undergraduate physics problem solving; benchmarks: PhysicsQA, SciEval-Static, MMLU High School & College)",
            "simulation_task": "Text-based simulation of physics problem solving: generate chain-of-thought (CoT) step-by-step solutions, apply physics concepts/formulae, perform algebraic/arithmetic computations (with optional code-driven computation refinement), and be iteratively refined by MoRA agents.",
            "evaluation_metric": "Final-answer accuracy on multiple benchmarks (percentage correct answers); additionally concept and computation diagnostic scores (Score_concept, Score_comp) and refinement rates for agent ablations.",
            "simulation_accuracy": "Reported final answer accuracies: SciEval-Static CoT 82.23% -&gt; with MoRA 86.58%; PhysicsQA CoT 56.76% -&gt; with MoRA 70.14%; MMLU High School CoT 72.88% -&gt; with MoRA 78.81%; MMLU College 71.76% -&gt; with MoRA 78.82%. (All values are percentages of final-answer accuracy.)",
            "factors_affecting_accuracy": "Identified factors: (1) problem miscomprehension (objective/variable misuse), (2) incorrect concept/formula application, (3) computational/algebraic errors; model class (open-source vs advanced models like GPT-4o), prompt/instructioning (CoT, 3-shot), availability of external knowledge (GraphRAG / physics knowledge base), use of program-aided computation (code generation + execution), and the iterative agentic refinement strategy (MoRA) and routing/prioritization.",
            "comparison_baseline": "Compared to baselines Answer-Only (AO), Chain-of-Thought (CoT), and Few-shot (3-shot). MoRA substantially improves over CoT and 3-shot for LLaMa-3-70B across datasets (examples above). GPT-4o is used as a stronger reference for error identification and is described as having near-perfect comprehension and higher concept/computation accuracy.",
            "limitations_or_failure_cases": "LLaMa-3-70B (as an open-source model) shows: occasional miscomprehension, moderate-to-high rates of conceptual errors (e.g., ~18.11% conceptual errors on PhysicsQA average across opensource models) and computational errors (~21.62% on PhysicsQA average), and limited ability to reliably self-identify its own errors without an external verifier (paper uses GPT-4o for error identification). Refinement agent performance varies by dataset (concept-refinement rates were moderate; computation-refinement worked well but not perfect).",
            "author_recommendations_or_insights": "Authors recommend using an external strong model (GPT-4o) for error identification, coupling concept refinement with a domain knowledge retrieval (GraphRAG + physics KB), using code generation + execution for computational corrections, and an iterative prioritized routing (miscomprehension -&gt; concept -&gt; computation) to maximize final-answer accuracy.",
            "uuid": "e9164.0",
            "source_info": {
                "paper_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Gemma-2-27B",
            "name_full": "Gemma-2 27B",
            "brief_description": "An open-source medium-scale language model (27 billion parameters) used to generate CoT physics solutions and to evaluate the effectiveness of MoRA refinements.",
            "citation_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
            "mention_or_use": "use",
            "model_name": "Gemma-2-27B",
            "model_description": "Open-source transformer LLM with ~27B parameters (referred to as Gemma-2-27B); used as a base solver whose outputs are refined by MoRA agents. Detailed training data not provided in paper.",
            "scientific_subdomain": "Physics (high-school physics problem solving; benchmarks include PhysicsQA, SciEval-Static, MMLU High School & College)",
            "simulation_task": "Text-based simulation of physics problem solving: produce chain-of-thought solutions, apply physics concepts/formulae, perform computations; participate in iterative concept and computation refinements driven by MoRA.",
            "evaluation_metric": "Final-answer accuracy on benchmarks (percentage correct); concept and computation scores (Score_concept, Score_comp); agent refinement rates in ablation studies.",
            "simulation_accuracy": "Reported accuracies: SciEval-Static CoT 79.26% -&gt; MoRA 88.76%; PhysicsQA CoT 54.59% -&gt; MoRA 70.62%; MMLU College CoT 73.52% -&gt; MoRA 82.20%; MMLU High School CoT 77.11% -&gt; MoRA 75.88% (noting a reported decrease on that particular dataset). (Values are percentages.)",
            "factors_affecting_accuracy": "Same three primary error sources (miscomprehension, concept application, computation); sensitivity to retrieval of correct physics concepts (needs external KB); benefits from code-driven computation refinement; variability across datasets and tasks; model capacity (27B) and its ability to generate effective retrieval thoughts affects concept refinement performance.",
            "comparison_baseline": "Compared to AO, CoT, and 3-shot baselines; MoRA yields sizable improvements over CoT and 3-shot on most datasets. GPT-4o is treated as a higher-performing system for error identification and overall reference in analysis.",
            "limitations_or_failure_cases": "Gemma-2-27B shows non-uniform refinement performance: strong computation-refinement in some datasets (e.g., 100% in MMLU College computational refinement in ablation) but weak conceptual retrieval/refinement in some cases (e.g., low concept-refinement rates on MMLU High School). It also relies on external error identification (GPT-4o) and external knowledge retrieval for concept fixes.",
            "author_recommendations_or_insights": "Authors recommend combining Gemma-2-27B with automated error identification (GPT-4o), GraphRAG retrieval to a physics KB for concept fixes, and code-execution for computational fixes. Iterative prioritized routing of refinement agents improves final accuracy significantly.",
            "uuid": "e9164.1",
            "source_info": {
                "paper_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "GPT-4o (as verifier)",
            "name_full": "GPT-4o (used as an external error identifier and verifier)",
            "brief_description": "An advanced OpenAI model used in this work solely for identifying and locating errors in other LLMs' CoT solutions (flags for objective/variable misalignment and two diagnostic scores for concept and computation correctness).",
            "citation_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "A strong OpenAI LLM (version '4o' in the paper) selected for superior performance in problem comprehension and physics concept application; used with OpenAI Code Interpreter for computation verification.",
            "scientific_subdomain": "Physics (used as an automated verifier/evaluator for physics problem solutions across PhysicsQA, SciEval-Static, MMLU datasets)",
            "simulation_task": "Text-based verification/simulation of solution correctness: (1) assign Problem Comprehension Flags (F_obj, F_val), (2) compute Score_concept (locates first concept/formula error step), and (3) compute Score_comp (via Code Interpreter executes Python to verify computations), guiding which refinement agents to route.",
            "evaluation_metric": "Used internally to produce diagnostic flags and scores; authors also report high concept and computation accuracies from GPT-4o (e.g., 'near-perfect' comprehension and average concept accuracy ~96.4% and computation accuracy ~95.64% across datasets in analysis). Final-answer accuracy as a solver is referenced qualitatively (near-perfect on several datasets).",
            "simulation_accuracy": "Diagnostic performance reported in analysis: GPT-4o achieves near-perfect problem comprehension on SciEval-Static, MMLU College and High School; averaged concept-correctness ~96.4% and computation-correctness ~95.64% across the four datasets (values reported by the authors as averages). The paper does not provide a complete per-benchmark final-answer accuracy table for GPT-4o as a solver but describes it qualitatively as excelling.",
            "factors_affecting_accuracy": "GPT-4o's stronger internal reasoning and ability to run Code Interpreter for numerical verification improves identification accuracy; access to code execution reduces false computation flags; its broader training/architectural strengths compared to open-source models are cited implicitly. Error tolerance for computation verification was set to 0.1.",
            "comparison_baseline": "Used as a verifier/referee against which open-source models' solutions are judged; described as outperforming LLaMa-3-70B and Gemma-2-27B in concept and computation diagnostics. Not compared as a primary solver baseline numerically in the main result tables.",
            "limitations_or_failure_cases": "The paper uses GPT-4o only for error identification (not for end-to-end solution generation in the MoRA pipeline). Authors note dependence on GPT-4o for reliable error localization because open-source LLMs often cannot self-identify mistakes.",
            "author_recommendations_or_insights": "Authors propose leveraging a strong verifier like GPT-4o to locate errors and produce diagnostic scores that drive specialized refinement agents; they emphasize using Code Interpreter capability of GPT-4o for robust computation verification.",
            "uuid": "e9164.2",
            "source_info": {
                "paper_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Gemini-1.5-Flash",
            "name_full": "Gemini 1.5 Flash",
            "brief_description": "A high-performing LLM referenced and evaluated in the paper (used to compare CoT performance on SciEval-Static), noted for strong accuracy on the SciEval-Static benchmark.",
            "citation_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
            "mention_or_use": "use",
            "model_name": "Gemini-1.5-Flash",
            "model_description": "A commercially available LLM (Gemini family) referenced in the experiments; exact parameter count or training details are not stated in the paper, but it's reported to have competitive CoT performance on SciEval-Static.",
            "scientific_subdomain": "Physics (evaluated on SciEval-Static benchmark for scientific reasoning tasks)",
            "simulation_task": "Text-based scientific reasoning / problem solving on SciEval-Static: generate CoT solutions for scientific (physics) questions.",
            "evaluation_metric": "Final-answer accuracy (percentage correct) on SciEval-Static.",
            "simulation_accuracy": "Reported SciEval-Static CoT accuracy 85.97% (percentage correct) in the paper's descriptive results section.",
            "factors_affecting_accuracy": "Implied factors include model capability/capacity, prompting strategy (CoT), and nature of the benchmark (SciEval-Static). The paper emphasizes that advanced models perform better on comprehension and concept application.",
            "comparison_baseline": "Compared qualitatively/numerically against LLaMa-3-70B and other models on SciEval-Static; outperforms some open-source models in reported SciEval-Static CoT accuracy.",
            "limitations_or_failure_cases": "No detailed failure analysis presented for Gemini-1.5-Flash specifically in the paper; it is included as a point of comparison.",
            "author_recommendations_or_insights": "Advanced proprietary models like Gemini-1.5-Flash show strong baseline performance on scientific reasoning benchmarks, motivating methods (like MoRA) to raise open-source models closer to that level via targeted refinement and external verification.",
            "uuid": "e9164.3",
            "source_info": {
                "paper_title": "Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research",
            "rating": 2,
            "sanitized_title": "scieval_a_multilevel_large_language_model_evaluation_benchmark_for_scientific_research"
        },
        {
            "paper_title": "Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification",
            "rating": 2,
            "sanitized_title": "solving_challenging_math_word_problems_using_gpt4_code_interpreter_with_codebased_selfverification"
        },
        {
            "paper_title": "MathCoder: Seamless code integration in LLMs for enhanced mathematical reasoning",
            "rating": 2,
            "sanitized_title": "mathcoder_seamless_code_integration_in_llms_for_enhanced_mathematical_reasoning"
        },
        {
            "paper_title": "PAL: Program-aided language models",
            "rating": 2,
            "sanitized_title": "pal_programaided_language_models"
        },
        {
            "paper_title": "Selfcheck: Using LLMs to zero-shot check their own step-by-step reasoning",
            "rating": 1,
            "sanitized_title": "selfcheck_using_llms_to_zeroshot_check_their_own_stepbystep_reasoning"
        },
        {
            "paper_title": "Structured chemistry reasoning with large language models",
            "rating": 1,
            "sanitized_title": "structured_chemistry_reasoning_with_large_language_models"
        }
    ],
    "cost": 0.013113,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents</p>
<p>Raj Jaiswal 
HarshDhruv Jain dhruv.jain.ece21@itbhu.ac.in 
Parimal Popat 
Avinash Anand 
Abhishek Dharmadhikari abhishekdharmadhikari25@gmail.com 
Atharva Marathe atharvamarathe8@gmail.com 
Ratn Rajiv rajivratn@iiitd.ac.in 
Shah 
Improving Physics Reasoning in Large Language Models Using Mixture of Refinement Agents
BCF50812D5A08AE799E9F64C19BF68DF
Large Language Models (LLMs) demonstrate remarkable capabilities in various reasoning tasks.However, they encounter significant challenges when it comes to scientific reasoning, particularly in physics, which requires not only mathematical reasoning but also factual and conceptual understanding.When addressing complex physics problems, LLMs typically face three key issues: problem miscomprehension, incorrect concept application, and computational errors.While each of these problems can be addressed individually, there is a need for a generalized approach that can tackle all three issues simultaneously.To address this, we introduce Mixture of Refinement Agents (MoRA), a novel agentic refinement framework that iteratively refines the LLM generated base solution by correcting the aforementioned errors, resulting in a significant performance improvement for open-source LLMs.Our approach aims to bridge the gap between opensource LLMs and GPT-4o by utilizing the latter as error identifier to guide these refinement agents.We evaluate our approach on the SciEval and MMLU subsets along with our own physics dataset (PhysicsQA).MoRA significantly improves the performance of Llama-3-70B and Gemma-2-27B on these datasets, achieving up to a 16% increase in final answer accuracy.</p>
<p>Introduction</p>
<p>Scientific reasoning, particularly in field of physics, requires a deep understanding that spans multiple disciplines.It demands not only domain-specific knowledge but also the integration of mathematical reasoning with theoretical concepts, applying abstract principles and formulae across various contexts and scenario.Successfully solving these challenges is a fundamental aspect of human intelligence, as it entails not just recalling information but adapting knowledge to solve diverse complex problems.</p>
<p>Solving complex physics problems still remains a challenge for open source LLMs.The difficulty stems from the need to integrate both mathematical and domain-specific knowledge while engaging in multi-hop, step-by-step reasoning.One approach to address this challenge can be collecting question and solution trajectory annotations and finetune LLMs to enhance these capabilities, similar to recent mathematical reasoning works (Luo et al. 2023;Yuan et al. 2024).However, the process of such annotations and finetuning is time-consuming and costly.On the other hand, so-lutions generated by LLMs for physics problems using CoT prompting (Wei et al. 2022) often contain errors, such as objective misalignment, incorrect formula application, and computational mistakes, as illustrated in Figure 1.Moreover, solutions to multihop physics problems contain multiple such errors together.</p>
<p>Open source LLMs struggles to accurately directly identify reasoning mistakes in their own solutions (Li et al. 2024;Tyen et al. 2024;Anand et al. 2023e), making them unreliable for error detection and self-refinement.While objective alignment errors can be corrected once identified, refining computational and conceptual errors requires strong mathematical reasoning and contextual understanding of the specific question.Addressing all these different errors simultaneously remains a significant challenge for open-source LLMs.</p>
<p>This motivated us to develop the Mixture of Refinement Agents (MoRA) framework.MoRA iteratively refines LLM responses through a two-step process in each iteration.First, the framework leverages a advanced model to identify various errors within the solution using appropriate flags and scores.In the next step, based on the identified errors, prioritized agent routing is conducted, in which the appropriate agents are activated to address and mitigate the specific errors.This process results in a progressively refined solution.</p>
<p>In the domain of physics, evaluation benchmarks are essential for assessing the conceptual and mathematical reasoning of LLMs.Benchmarks like MMLU, SciEval (Sun et al. 2024), and ScienceQA (Lu et al. 2022) focus on foundational knowledge and general reasoning, while more challenging ones like OlympiadBench (He et al. 2024) and JEEBench (Arora, Singh et al. 2023) require advanced reasoning skills.To bridge the gap, we curated our own dataset PhysicsQA, containing set of diverse, intermediatelevel high school physics problems that provide a balanced challenge, allowing a exhaustive evaluation and analysis of open-source LLMs on physics problems.</p>
<p>We perform exhaustive experimentation of MoRA across four datasets including PhysicsQA as shown in Table 3. MoRA improves accuracy on the PhysicsQA benchmark over CoT-generated solutions by 13.38% for Llama-3-70B and by 16.03% for Gemma-2-27B.This significant enhancement highlights MoRA's effectiveness in refining solutions, particularly in complex and diverse physics problems as in arXiv:2412.00821v1[cs.AI] 1 Dec 2024 PhysicsQA.Our further analysis offers insights into the error distribution across different models and evaluates the effectiveness of individual refinement agents based on their refinement rates.</p>
<p>Related Works</p>
<p>LLM Reasoning LLMs have been successfully applied to address multi-step reasoning tasks by generating intermediate reasoning steps, referred to as Chain-of-Thought (CoT) (Wei et al. 2022), Auto-CoT (Zhang et al. 2022), and Complex-CoT (Fu et al. 2022), among others.Advanced techniques like Iter-CoT (Sun et al. 2023a) and ToT (Yao et al. 2024) extend these capabilities but remain constrained by the knowledge in training data and the specific structures they were designed with.While In-Context Learning (ICL) (Brown et al. 2020) has significantly improved LLM performance, challenges like hallucinations and limitations in reasoning flexibility persist.</p>
<p>LLMs for Scientific Reasoning LLMs face significant limitations in complex knowledge reasoning tasks (Petroni et al. 2020).(Ouyang et al. 2023) introduced a structured reasoning strategy to guide LLMs in solving complex chemistry problems, enabling them to generate high-quality reasoning.Solving these problems requires not only domain knowledge, like formulae and calculations, but also a stepby-step reasoning process.(Ma et al. 2024) proposed a method where agents generate a high-level plan based on the question, retrieve relevant functions from a toolset, and execute low-level actions by integrating natural language and Python code.</p>
<p>Self Verification with LLMs Recent works (Cobbe et al. 2021;Ling et al. 2024) have attempted to address the challenge of error detection in step-by-step reasoning.However, these methods often require additional training data or domain-specific exemplars, making them less practical.(Miao, Teh, and Rainforth 2023) proposes using the LLM itself to verify the conditional correctness of each step in the reasoning chain, similar to how a human reviews their work.(Anand et al. 2023c) Accurate error recognition and correction are crucial for enhancing problem-solving capabilities, as demonstrated by (Li et al. 2024), which defines tasks to assess LLMs' mathematical reasoning abilities in error identification and correction.</p>
<p>LLMs for Mathematical Reasoning LLMs tends to struggle with arithmetic calculations when solving math problems (Cobbe et al. 2021;Gao et al. 2023).However, incorporating code generation and execution has shown promise in enhancing the accuracy of mathematical reasoning.Leveraging these strengths, the GPT-4 Code Interpreter (Zhou et al. 2023) has been integral to frameworks like MathCoder (Wang et al. 2023), which is designed to improve the mathematical reasoning capabilities of opensource models.Findings from (Zhou et al. 2023) indicate that GPT-4 Code's impressive proficiency in solving mathematical problems is largely due to its step-by-step code generation and the dynamic refinement of solutions based on code execution outcomes.</p>
<p>LLM Reasoning with external database (Lewis et al. 2020) proposed RAG framework, which incorporates (Anand et al. 2023b) a retrieval component to fetch relevant information from a given knowledge base.Integrating LLMs with knowledge representation tools, such as knowledge graphs (KGs) (Mruthyunjaya et al. 2023), has further enhanced reasoning capabilities.(Yao et al. 2024) demonstrated that augmenting LLMs with comprehensive external knowledge from KGs can significantly improve their performance and facilitate more robust reasoning processes.A notable example is GraphRAG (Edge et al. 2024), a retrieval enhancement technique that leverages knowledge graphs to map relationships between entities, thereby enhancing the retrieval process using large language models (LLMs).</p>
<p>Dataset: PhysicsQA</p>
<p>Our dataset comprises 370 carefully selected high school physics questions sourced from online resources.These questions are notably complex, often requiring the application of multiple concepts, intricate computations, (Anand et al. 2023a) and multihop reasoning.Each question is paired with a comprehensive, step-by-step solution, to support the evaluation (Anand et al. 2024b) and fine-tuning of LLMs for physics reasoning.Table 1 illustrates the topic-wise distribution of the questions, providing a clear overview of the areas covered.PhysicsQA offers a more robust evaluation and analysis of LLM performance by encompassing a diverse range of questions, both in terms of complexity and the topics covered.</p>
<p>Chapter</p>
<p>Mixture of Refinement Agents</p>
<p>This section introduces our mixture of refinement agents (MoRA) framework.We first discuss our motivation behind MoRA; then, we introduce the error identification stage and refinement agents.Finally, we discuss how these agents are routed iteratively to correct different errors in the solutions generated by the LLM.</p>
<p>Motivation</p>
<p>While analyzing physics problems and their CoT solutions generated with LLMs (Llama-3-70B &amp; Gemma-2-27B), we observed three key errors made by them: Observation 1: LLMs in few cases struggle to fully grasp the objective of the question, along with misinterpreting the values of variables and constants provided in the question.</p>
<p>Although this issue has been identified in only a few cases, it is significant one because it leads to solutions that Observation 2: LLMs struggle to apply the correct concepts or formulae with respect to the context of the given problem.</p>
<p>This issue is a more recurring one in LLMs, especially for problems requiring considering a specific case rather than relying on a generic formula.(Anand et al. 2024c) For instance, the formula for calculating the moment of inertia varies depending on the distribution of mass.</p>
<p>Observation 3: Many physics problems involve mathematical reasoning and algebraic computation, areas where LLMs tend to struggle.</p>
<p>Computational errors account for the majority of errors in solutions generated by LLMs.LLMs struggles with accurate algebraic and arithmetic computations resulting in errors within the reasoning and final answer.</p>
<p>While these three issues can be addressed individually, solutions often exhibit multiple errors together.Therefore, a single framework is required to rectify all three issues effectively, which motivated us to develop the MoRA.We first perform error identification on a given solution; then these errors are mitigated iteratively using specialized refinement agents, resulting in accurate solutions.</p>
<p>Error Identification</p>
<p>The errors in the solutions are classified into three categories: 1) problem miscomprehension, 2) incorrect concept application, and 3) computational errors as showed in Figure 1.</p>
<p>For error identification, we choose to rely on GPT-4o.Our experiments and analysis shows that GPT-4o showcases superior performance compared to other models, particularly in problem comprehension and correct physics concept application required.Thus, it is adequate for locating errors within solutions generated by other models.Given a question and it's LLM response, we prompt GPT-4o to identify and locate different errors in the solution using the combination of following flags and scores:</p>
<p>Problem Comprehension Flags: We prompt GPT-4o to check for the problem miscomprehension using the following two flags: (i) Objective Alignment Flag, F obj , verifies whether the solution is focused on solving the correct objective of the given question.(Anand et al. 2023d) (ii) Variables Application Flag, F val , verifies whether the solution uses the correct values for all variables and constants provided in the question, ensuring their correct values are applied in formulae and reasoning.</p>
<p>Concept Verification Score: We instruct GPT-4o to check the given solution against the relevant concept and formulae required to solve the given problem, based on its own understanding of the question.A score (Score concept ) is assigned to each solution to quantify the correctness of the applied physics concepts and formulae.The score is designed to identify the stage at which any conceptual or formulae error first occurs, if at all.Score concept ranges from 0 to 1, where a lower score indicates an earlier-stage error and a higher score indicates a later-stage error in the solution process.The score is calculated as follows:
Score concept =    n N if 1  n &lt; N (error at step n) n N +1 if n = N (error at last step) 1 if no errors occur
where n is the step at which the first error occurs, and N is the total number of steps in the solution process.</p>
<p>Computation Verification Score: We employ GPT-4o with OpenAI Code Interpreter for generation and execution of python code to evaluate the correctness of all arithmetic and algebraic operations in the given solution.Similar to the Score concept , we assign Score comp to each solution.This score quantifies the accuracy of the mathematical computation performed, ranges from 0 to 1, and is calculated similar to Score concept .All the computations are evaluated with an Figure 2: The illustration of thought and concept retrieval for conceptual error refinement in LLM response.Given the response and concept verification score, LLM generates a retrieval thought, which acts as a query to retrieve the correct conceptual context from an physics knowledge base using GraphRAG.error tolerance of 0.1.Using Code Interpreter enables us to leverage the code generation capabilities of GPT-4o rather than solely relying on its mathematical reasoning, which sometimes can lead to erroneous scores.Recent works, such as (Zhou et al. 2023) and (Wang et al. 2023), highlights the remarkable capability of OpenAI Code Interpreter in solving challenging math problems and self-verification.(Anand et al. 2024d) We utilize GPT-4o solely for error identification, which guides the routing to the appropriate refinement agent.The scores are used as the feedback to help refinement agents understand the first stage of the mistake from where the refinement needs to be initiated.</p>
<p>Refinement Agents</p>
<p>To address the three key errors in LLM generated solutions, we introduce a set of specialized refinement agents.Each agent is designed to rectify a specific type of error within the solution, ensuring targeted and effective corrections.The refinement agents use the same LLM with which the original solutions are generated.</p>
<p>Miscomprehension Refinement Although there are very few cases of problem miscomprehension in LLM generated solutions, once identified these mistakes can easily be corrected with simple instruction prompting:</p>
<p>You are tasked with solving a physics problem.Here is the question: [question], The following is your generated solution: [solution], In the generated solution, the correct objective of the question is not being addressed.The solutions contains mistakes which leads to misalignment with the objective of the question.Please carefully review the question &amp; understand the objective in detail and regenerate the solution accordingly.</p>
<p>The above prompt assists in refining the solution to align with the correct objective of the given question.This may involve regenerating the entire solution or correcting an intermediate mistake to ensure the solution addressed the correct objective.Similarly, any incorrect variable values used within the solution is corrected using instruction prompting.</p>
<p>Concept Refinement To address the incorrect concepts and formulae applied in the LLM's solutions, we utilize an external physics knowledge base.This is necessary because LLMs may not always have access to or accurately retrieve the correct formulae, as this information may not be embedded in their internal knowledge.The conceptual refinement occurs in two steps:</p>
<p>Error Identification &amp; Thought Generation: Given a solution S orig and a concept score Score concept , the LLM systematically reviews the solution to identify the earliest stage where an incorrect concept or formula has been applied.Score concept pinpoints this stage of error within the solution.LLM then generates a retrieval thought T R for the the concept or formula required at the failure stage.The thought is structured to be simple and sequential query.</p>
<ol>
<li>Concept Retrieval &amp; Solution Refinement: Given the retrieval thought T R and physics knowledge base K P , we use GraphRAG (Edge et al., 2024) to query the K P to retrieve an observation O T , based on T R as demonstrated in Figure 2. O T contains the correct context for concept and formulae required at the stage of failure.The LLM then initiates the refinement from this stage using the information present in O T , resulting in the refined solution S refined with corrected physics concepts and reasoning.</li>
</ol>
<p>Computational Refinement Inspired by recent works such as PAL (Gao et al. 2023), PoT (Chen et al. 2022), CSV (Zhou et al. 2023), MathCoder (Wang et al. 2023), we use code generation for the refinement of computational and mathematical errors within a solution.The computation score Score comp allows the LLM to locate the first step of error and then initiate the refinement of the failure stage and subsequent computations.The process occurs in two steps:</p>
<ol>
<li>
<p>Code Generation &amp; Execution: Given the original solution S orig and computation score Score comp , the LLM first locates the error step and then generates a Python code C p designed to accurately perform the necessary computation at the identified failure stage and produce the correct result.The generated code C p is then executed to obtain the response R c as shown in Figure 3.</p>
</li>
<li>
<p>Solution Refinement: The LLM is then instructed to refine S orig using the correct response R c generated by the code.This involves pinpointing the exact step where the error occurred, guided by the computation score Score comp , and integrating the correct computation from R c into the solution.The refined solution S refined is then presented with the corrected computations.</p>
</li>
</ol>
<p>Agent Routing and Iterative Refinement</p>
<p>After the error identification, the respective refinement agents are activated to mitigate these errors.The agent routing follows a prioritized sequence: 1.) miscomprehension refinement, 2.) concept refinement, 3.) computational refinement.This prioritization mirrors the human approach to solving physics problems: first, understanding the objectives and variables; next, identifying relevant concepts and formulae; and finally, applying them to perform the necessary computations.</p>
<p>The activated refinement agent then acts upon the solution to mitigate the error.The solution undergoes iterative cycles of error identification and refinement until all flags and scores are resolved or a maximum iteration limit is reached.This process ensures that all errors are corrected without introducing new ones in final refined solution.The complete process is illustrated in Algorithm 1.</p>
<p>Algorithm 1: Error Identification and Iterative Refinement</p>
<p>Require: Question Q, Initial Solution S0, GPT-4o L, Refinement Agents R, Maximum Iterations N , Threshold  Ensure: Final refined solution to Q 1: i = 0, Si = S0 2: while i &lt; N do 3:  (Sun et al. 2023b), consisting 164 questions from physics divided into multiple sub-topics.MMLU (Hendrycks et al. 2021), consists of a 118 College level and 173 high school multiplechoice questions from various disciplines.
(F i obj , F i val , Score i concept , Score i comp )  L(Q, Si) 4: if F i obj == 1 or F i val == 1 then 5: Si+1  Rmiscomprehension(Q, Si) 6: else if Score i concept &lt; 1   then 7: Si+1  Rconcept(Q, Si) 8: else if Score i comp &lt; 1   then 9: Si+1  Rcomputation(Q, Si
LLMs We utilize the API of a range of models with varying parameters and capabilities including LLaMa-3-70B, LLaMa 3.1-405B, Gemma-2-27B, Gemini-1.5-Flash,GPT-3.5 Turbo and GPT-4 as our LLMs for the evaluation.We use same prompts for all the datasets and LLMs during our evaluation.</p>
<p>Baselines We employ an Answer-only approach (AO), where the model is given a question with four options and asked to select the correct answer without any explanation relying solely on its pre-existing knowledge .In contrast, few-shot prompting (Xu et al. 2023;Yasunaga et al. 2023) uses a few examples to help the model learn and apply that knowledge to similar tasks.Chain-of-Thought (CoT) prompting (Wei et al. 2022) guides the model to generate intermediate reasoning steps, improving its performance on complex tasks by breaking them down into smaller, more manageable parts.These three approaches form our primary baselines.</p>
<p>Evaluation Most of the existing works (Luo et al. 2023) , (Chern et al. 2023) , (Yu et al. 2023) measure the mathematical reasoning quality of LLMs by directly comparing the final answer and calculating the overall accuracy on a given dataset.(Anand et al. 2024a) We choose to follow the same evaluation for physics reasoning as well.</p>
<p>Results</p>
<p>In Table 2 we present results from our experiments reveal compelling insights into the strengths and challenges of vari-   ous models across diverse benchmarks.In the SciEval-Static benchmark, LLaMa-3-70B and Gemini 1.5 Flash stand out, with LLaMa-3-70B achieving an accuracy of 82.23% using (CoT), and Gemini 1.5 Flash not far behind at 85.97%.In the PhysicsQA domain, which demands intricate reasoning skills, the models face more significant challenges.LLaMa-3-70B and Gemma 2-27B both show improved performance with CoT, reaching 56.76% and 54.59%, respectively.On the MMLU-High benchmark, LLaMa-3-70B continues to perform solidly, achieving 72.88% with CoT, while Gemini 1.5 Flash pushes ahead to 79.66%.Interestingly, in MMLU-College, a benchmark with a mix of academic and reasoning tasks, Gemma 2-27B shows a significant leap in performance with CoT, reaching 73.52%, which surpasses its base score by over 22%, indicating the effectiveness of CoT in enhancing reasoning in academic settings.</p>
<p>As shown in Table 3, MoRA framework delivers marked improvements across all benchmarks for both LLaMA-3-70B and Gemma-2-27B models.In SciEval-Static, the introduction of MoRA enhances LLaMA-3-70B's accuracy from 82.23% (CoT) to 86.58%, and Gemma-2-27B sees a boost from 79.26% (CoT) to 88.76%.In the PhysicsQA dataset, MoRA significantly improves LLaMA-3-70B's performance from 59.29% (3-shot) to 70.14%, and Gemma-2-27B's from 59.45% (3-shot)to 70.62%.In MMLU High School, LLaMA-3-70B accuracy reaches from 73.66% (3-Shot) to 78.81% and in Gemma-2-27B, accuracy reaches from 77.11% (CoT) to 75.88%.In MMLU College, LLaMA-3-70B accuracy reaches from 71.76% (3-Shot) to 78.82% and in Gemma-2-27B, accuracy reaches from 73.52% (CoT) to 82.20%.The results show that even without extensive fine-tuning, these models can achieve competitive performance.These improvements demonstrate MoRA's ability to elevate smaller models to compete effectively with much larger ones across a range of complex tasks.</p>
<p>Analysis</p>
<p>In this section, we first conduct an in-depth error analysis of physics CoT solutions across various models and datasets, highlighting the error distribution that inspired the development of MoRA.We then present ablation studies, analyzing the effectiveness of each refinement agent in our framework.</p>
<p>Error Analysis</p>
<p>We perform manual analysis of the incorrect CoT solutions of GPT-4o, Llama-3-70B, and Gemma-2-27B on the following datasets: SciEval-Static, PhysicsQA, MMLU High School and College as shown in Table 4. Based on this analysis here are our observations:</p>
<p>(i) LLMs demonstrate good problem comprehension ability for physics question.All models demonstrate strong problem comprehension across the datasets.GPT-4o excels, achieving near-perfect accuracy on SciEval-Static, MMLU College, and High School, with only minor errors in Physic-sQA.This suggests a deep understanding of physics problem structure.Llama-3-70B and Gemma-2-27B also perform well but show slightly higher error rates, particularly in PhysicsQA and SciEval-Static, indicating occasional missed details that need attention.</p>
<p>(ii) Open source LLMs sometimes struggles to retrieve correct physics concept and formulae while reasoning.On average, 18.11% of questions in the PhysicsQA dataset are answered with conceptual errors by Gemma-2-27B and Llama-3-70B, highlighting the difficulty opensource LLMs face in applying correct concepts to physics problems.In contrast, GPT-4o excels with an average accuracy of 96.4% across all four datasets.Notably, Gemma-2-27B outperforms Llama-3-70B on SciEval-Static, Physic-sQA, and MMLU High School.The high error rates of both Llama-3-70B and Gemma-2-27B on PhysicsQA suggest that medium-parameter open-source LLMs may still need external knowledge bases for complex physics problemsolving.</p>
<p>(iii) Open-source LLMs struggles with algebraic and arithmetic computation required while solving physics On average, 21.62% of questions in the Physic-sQA dataset are answered with computational mistakes by Gemma-2-27B and Llama-3-70B, highlighting challenges in executing correct calculations.GPT-4o excels with an average accuracy of 95.64% across four datasets.Gemma-2-27B outperforms Llama-3-70B on SciEval Static and MMLU (High School and College), with both performing similarly on PhysicsQA.The accuracy gap between GPT-4o and Llama-3-70B (12.16%) and GPT-4o and Gemma-2-27B (13.24%) on PhysicsQA suggests that open-source LLMs could benefit from further refinement in handling complex calculations.</p>
<p>Ablation</p>
<p>To understand the effectiveness of each refinement agent, we conduct ablation of each of the refinement agents with Llama-3-70B and Gemma-2-70B in terms of their refinement rate across different datasets as shown in Table 5.Here are our observations:</p>
<p>(i) Problem miscomprhension errors are mitigated with simple instruction prompting and error feedback.Llama-3-70B and Gemma-2-27B demonstrate good miscomprehension error refinement with instruction prompting, particularly in the MMLU datasets (High School and College), where both models achieve a perfect 100% refinement rate.Llama-3-70B outperforms Gemma-2-27B slightly on PhysicsQA, with a refinement rate of 66.7% compared to Gemma-2-27B's 62.5%.However, Gemma-2-27B excels in SciEval Static and MMLU (College and High School).The decent accuracy on PhysicsQA suggests that open-source LLMs sometimes fail to rectify their misinterpretations in complex physics problems.</p>
<p>(ii) Open-source LLM performers moderately in identifying the conceptual mistake and retrieval thought generation.Llama-3-70B shows a 46.9% refinement rate in PhysicsQA and slightly improves in SciEval Static at 57.1%, but struggles with MMLU datasets, achieving 37.5% and 33.3% refinement in High School and College, respectively.Gemma-2-27B has a similar 48.7% refinement rate in PhysicsQA and performs better in SciEval Static at 62.5%, but underperforms significantly on MMLU High School with a 16.7% refinement rate, improving modestly to 37.5% on MMLU College.These results suggest that open-source LLMs have difficulty generating relevant retrieval thoughts at the initial stage of failure.</p>
<p>(iii) Using code-driven refinement significantly corrects the computational errors.Llama-3-70B and Gemma-2-27B excel in refining computational errors, demonstrating the effectiveness of code generation and execution.Llama-3-70B shows consistent performance with a 72.6% refinement rate on PhysicsQA and strong results across SciEval Static (60%), MMLU High School (75%), and MMLU College (81.8%).Gemma-2-27B slightly outperforms in PhysicsQA at 73.3% and achieves a 100% refinement rate in MMLU College.However, Gemma-2-27B's performance is more variable, particularly in MMLU High School (33.3%), indicating potential challenges in specific code generation scenarios.</p>
<p>Conclusion</p>
<p>In this work, we introduce MoRA, a novel agentic refinement framework designed to mitigate three critical errors commonly made by LLMs when solving complex physics problems.MoRA first leverages GPT-4o for error identification and score assignment, which are then subsequently used to guide the refinement agents.This process is done iteratively until all the errors in the solution are mitigated successfully.To ensure a comprehensive evaluation, we also curate our own dataset, PhysicsQA, which includes a diverse set of high school-level physics problems.Our experiments and in-depth analysis across multiple datasets demonstrate that MoRA significantly enhances the performance of Llama-3-70B and Gemma-2-27B across multiple datasets.</p>
<p>Figure 1 :
1
Figure 1: The illustration of three key error observations in the CoT solution of open source LLMs for physics problems.(a)showcases problem miscomprehension, where the LLM response uses the incorrect value of variables given in the question here, M instead of 9M, (b) showcases incorrect concept application in the LLM response, here incorrect moment of inertia formula for uniform cylinder, (c) demonstrate computational error within LLM response here, incorrect calculation of time period.</p>
<p>Figure 3 :
3
Figure 3: The illustration of code generation and execution for computation error refinement in LLM response.Given the response and computation verification score, LLM generates a code to perform the correct required computation; the code is then executed to obtain the response.</p>
<p>Table 1 :
1
Topic-wise Distribution in PhysicsQA
NamePercentageElectromagnetism29.8%Mechanics and Kinematics21.8%Thermodynamics and Heat15.7%Waves and Optics15.4%Nuclear and Modern Physics8.9%Material Properties and Elasticity8.3%</p>
<p>Table 2 :
2
Experimentation of Answer-Only (AO) , COT and Few-Shot (3-shot) on different Datasets
ModelDatasetAOCOT 3-Shot MORAGemma 2 MMLU College51.11% 73.52% 67.64% 82.20%27BMMLU High School 55.93% 77.11% 74.45% 75.88%PhysicsQA39.18% 54.59% 59.45% 70.62%SciEval-Static60.36% 79.26% 53.04% 88.76%LLaMa 3 MMLU College59.41% 71.76% 71.76% 78.82%70BMMLU High School 60.16% 72.88% 73.66% 78.81%PhysicsQA38.37% 56.76% 59.29% 70.14%SciEval-Static70.07% 82.23% 63.41% 86.58%</p>
<p>Table 3 :
3
Comparison of baseline approaches with MoRA across four datasets: SciEval-Static, PhysicsQA, MMLU High School and College based on final answer accuracy.</p>
<p>Table 4 :
4
Error Analysis of incorrect physics CoT solutions of different models across four datasets.
Error TypeDatasetGemma LLaMa2-27B3-70BComputationalMMLU College100%81.8%RefinementMMLU High School 33.3% 75.0%PhysicsQA73.3% 72.6%SciEval-Static57.1% 60.0%Miscomprehension MMLU College37.5% 33.3%RefinementMMLU High School 16.7% 37.5%PhysicsQA48.7% 46.9%SciEval-Static62.5% 57.1%ConceptMMLU College100%100%RefinementMMLU High School 100%100%PhysicsQA62.5% 66.7%SciEval-Static100%66.7%</p>
<p>Table 5 :
5
Ablation studies for different refinement agent in MoRA using Gemma-2-27B and Llama-3-70B across four datasets, evaluated by refinement rate.</p>
<p>Revolutionizing High School Physics Education: A Novel Dataset. A Anand, K Addala, K Baghel, A Goel, M Hira, R Gupta, R R Shah, Big Data and Artificial Intelligence. V Goyal, N Kumar, S S Bhowmick, P Goyal, N Goyal, D Kumar, Cham; Nature SwitzerlandSpringer2023a</p>
<p>Sciphyrag-retrieval augmentation to improve llms on physics q &amp;a. A Anand, A Goel, M Hira, S Buldeo, J Kumar, A Verma, R Gupta, R R Shah, International Conference on Big Data Analytics. Springer2023b</p>
<p>KG-CTG: citation generation through knowledge graph-guided large language models. A Anand, M Gupta, K Prasad, U Goel, N Lal, A Verma, R R Shah, International Conference on Big Data Analytics. Springer2023c</p>
<p>A Anand, M Gupta, K Prasad, N Singla, S Sanjeev, J Kumar, A R Shivam, R R Shah, arXiv:2404.13099Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks. 2024aarXiv preprint</p>
<p>GEC-DCL: Grammatical Error Correction Model with Dynamic Context Learning for Paragraphs and Scholarly Papers. A Anand, A Jairath, N Lal, S Bangar, J Sikka, A Verma, R R Shah, S Satoh, International Conference on Big Data Analytics. Springer2023d</p>
<p>GeoVQA: A Comprehensive Multimodal Geometry Dataset for Secondary Education. A Anand, R Jaiswal, A Dharmadhikari, A Marathe, H Popat, H Mital, A R Nair, K Prasad, S Kumar, A Verma, 2024 IEEE 7th International Conference on Multimedia Information Processing and Retrieval (MIPR). IEEE2024b</p>
<p>A Anand, J Kapuriya, C Kirtani, A Singh, J Saraf, N Lal, J Kumar, A R Shivam, A Verma, R R Shah, arXiv:2404.12926MM-PhyRLHF: Reinforcement Learning Framework for Multimodal Physics Question-Answering. 2024carXiv preprint</p>
<p>MM-PhyQA: Multimodal Physics Question-Answering with Multi-image CoT Prompting. A Anand, J Kapuriya, A Singh, J Saraf, N Lal, A Verma, R Gupta, R Shah, Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer2024d</p>
<p>Context-enhanced language models for generating multi-paper citations. A Anand, K Prasad, U Goel, M Gupta, N Lal, A Verma, R R Shah, International Conference on Big Data Analytics. Springer2023e</p>
<p>Have llms advanced enough? a challenging problem solving benchmark for large language models. D Arora, H G Singh, arXiv:2305.150742023arXiv preprint</p>
<p>Language models are few-shot learners. Advances in neural information processing systems. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, 202033</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, E Chern, H Zou, X Li, J Hu, K Feng, J Li, P Liu, arXiv:2211.12588Generative ai for math. 2022. 2023arXiv preprint</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>From local to global: A graph rag approach to query-focused summarization. D Edge, H Trinh, N Cheng, J Bradley, A Chao, A Mody, S Truitt, J Larson, arXiv:2404.161302024arXiv preprint</p>
<p>Complexity-based prompting for multi-step reasoning. Y Fu, H Peng, A Sabharwal, P Clark, T Khot, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Pal: Program-aided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, International Conference on Machine Learning. PMLR2023</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. C He, R Luo, Y Bai, S Hu, Z L Thai, J Shen, J Hu, X Han, Y Huang, Y Zhang, arXiv:2402.140082024arXiv preprint</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Kttler, M Lewis, W.-T Yih, T Rocktschel, Advances in Neural Information Processing Systems. 202033</p>
<p>X Li, W Wang, M Li, J Guo, Y Zhang, F Feng, arXiv:2406.00755Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction. 2024arXiv preprint</p>
<p>Deductive verification of chain-ofthought reasoning. Z Ling, Y Fang, X Li, Z Huang, M Lee, R Memisevic, H Su, Advances in Neural Information Processing Systems. 202436</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>H Luo, Q Sun, C Xu, P Zhao, J Lou, C Tao, X Geng, Q Lin, S Chen, D Zhang, arXiv:2308.09583Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. 2023arXiv preprint</p>
<p>Y Ma, Z Gou, J Hao, R Xu, S Wang, L Pan, Y Yang, Y Cao, A Sun, arXiv:2402.11451SciAgent: Tool-augmented Language Models for Scientific Reasoning. 2024arXiv preprint</p>
<p>Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. N Teh, Y W Rainforth, T , arXiv:2308.004362023arXiv preprint</p>
<p>Rethinking language models as symbolic knowledge graphs. V Mruthyunjaya, P Pezeshkpour, E Hruschka, N Bhutani, arXiv:2308.136762023arXiv preprint</p>
<p>S Ouyang, Z Zhang, B Yan, X Liu, J Han, L Qin, arXiv:2311.09656Structured chemistry reasoning with large language models. 2023arXiv preprint</p>
<p>F Petroni, A Piktus, A Fan, P Lewis, M Yazdani, N De Cao, J Thorne, Y Jernite, V Karpukhin, J Maillard, arXiv:2009.02252KILT: a benchmark for knowledge intensive language tasks. 2020arXiv preprint</p>
<p>Enhancing chain-of-thoughts prompting with iterative bootstrapping in large language models. J Sun, Y Luo, Y Gong, C Lin, Y Shen, J Guo, N Duan, arXiv:2304.116572023aarXiv preprint</p>
<p>L Sun, Y Han, Z Zhao, D Ma, Z Shen, B Chen, L Chen, K Yu, arXiv:2308.13149SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research. 2023barXiv preprint</p>
<p>Scieval: A multi-level large language model evaluation benchmark for scientific research. L Sun, Y Han, Z Zhao, D Ma, Z Shen, B Chen, L Chen, K Yu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>LLMs cannot find reasoning errors, but can correct them given the error location. G Tyen, H Mansoor, V Carbune, P Chen, T Mak, Findings of the Association for Computational Linguistics ACL 2024. L.-W Ku, A Martins, V Srikumar, BangkokAssociation for Computational Linguistics2024</p>
<p>K Wang, H Ren, A Zhou, Z Lu, S Luo, W Shi, R Zhang, L Song, M Zhan, H Li, arXiv:2310.03731Mathcoder: Seamless code integration in llms for enhanced mathematical reasoning. 2023arXiv preprint</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>B Xu, A Yang, J Lin, Q Wang, C Zhou, Y Zhang, Z Mao, arXiv:2305.14688Expertprompting: Instructing large language models to be distinguished experts. 2023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Large language models as analogical reasoners. M Yasunaga, X Chen, Y Li, P Pasupat, J Leskovec, P Liang, E H Chi, D Zhou, L Yu, W Jiang, H Shi, J Yu, Z Liu, Y Zhang, J T Kwok, Z Li, A Weller, W Liu, arXiv:2310.01714arXiv:2309.12284Metamath: Bootstrap your own mathematical questions for large language models. 2023. 2023arXiv preprint</p>
<p>Scaling Relationship on Learning Mathematical Reasoning with Large Language Models. Z Yuan, H Yuan, C Li, G Dong, K Lu, C Tan, C Zhou, J Zhou, Z Zhang, A Zhang, M Li, A Smola, arXiv:2210.034932024. 2022arXiv preprintAutomatic chain of thought prompting in large language models</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with codebased self-verification. A Zhou, K Wang, Z Lu, W Shi, S Luo, Z Qin, S Lu, A Jia, L Song, M Zhan, arXiv:2308.079212023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
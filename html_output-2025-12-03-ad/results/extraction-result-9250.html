<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9250 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9250</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9250</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-278237303</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.00019v1.pdf" target="_blank">An Empirical Study on Prompt Compression for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Prompt engineering enables Large Language Models (LLMs) to perform a variety of tasks. However, lengthy prompts significantly increase computational complexity and economic costs. To address this issue, we study six prompt compression methods for LLMs, aiming to reduce prompt length while maintaining LLM response quality. In this paper, we present a comprehensive analysis covering aspects such as generation performance, model hallucinations, efficacy in multimodal tasks, word omission analysis, and more. We evaluate these methods across 13 datasets, including news, scientific articles, commonsense QA, math QA, long-context QA, and VQA datasets. Our experiments reveal that prompt compression has a greater impact on LLM performance in long contexts compared to short ones. In the Longbench evaluation, moderate compression even enhances LLM performance. Our code and data is available at https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9250.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9250.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptCompression (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt compression (general concept)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reducing prompt/context length before feeding it to LLMs to lower compute/cost while aiming to retain task-relevant information; studied here across six compressors, multiple tasks, and compression ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization, Reconstruction, Question Answering (QA), VQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard summarization and reconstruction datasets plus QA (including long-context QA) and multimodal VQA; evaluated using BLEU/ROUGE/BERTScore for summarization/reconstruction and accuracy/F1 for QA/VQA.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Compressed prompt produced by an external compressor (six methods evaluated: KiS, SCRL, Selective Context, LLMLingua, LongLLMLingua, LLMLingua-2) at various compression ratios (default ρ = 0.5 unless otherwise specified). Compression can be query-aware (e.g., LongLLMLingua) or query-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original (uncompressed) prompt; random token selection baseline and comparisons across different compressors and compression ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Varies by task and compression ratio; generally: short-context tasks — performance degrades monotonically as compression ratio increases; long-context tasks — moderate compression can improve performance relative to original (see Figures 3 & 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to original prompt: short contexts show consistent degradation with higher ρ; long contexts show an initial performance increase for moderate ρ then decline at high ρ (no single numeric effect-size universally reported).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>For short contexts compression removes relevant information causing steady performance drops; for long contexts moderate compression helps by removing irrelevant/distracting material and concentrating semantically rich tokens (especially if compression is question-aware), improving signal-to-noise.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Six compressors evaluated, compression ratio adjustable for Selective Context, LLMLingua, LongLLMLingua, LLMLingua-2 (default 0.5); KiS and SCRL adaptively choose ratio. Evaluations averaged across GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku; datasets include Gigaword, DUC2004, BNC, Google, Broadcast (summarization), GSM8K/BBC/ArXiv/ShareGPT (reconstruction), LongBench/BBH/GSM8K (QA), IconQA/OK-VQA (VQA).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study on Prompt Compression for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9250.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9250.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMLingua / LongLLMLingua / LLMLingua-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMLingua, LongLLMLingua, LLMLingua-2 (LLM annotation-based compressors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Coarse-to-fine and token-level prompt compression methods using LLM annotation and trained small models; LongLLMLingua is question-aware and tailored for long contexts; LLMLingua-2 uses data distillation and token classification for task-agnostic compression.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Llmlingua: Compressing prompts for accelerated inference of large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization; Long-context QA; General QA and reconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Summarization tasks (Gigaword, DUC2004, etc.) and QA including long-context benchmarks (LongBench); also used in reconstruction comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Compressed prompts produced by LLMLingua-family compressors at adjustable ratios (default ρ=0.5; evaluated across a range of ρ). LongLLMLingua is question-aware (compresses conditioned on the query).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared against Selective Context, KiS, SCRL, and random-selection baseline; also compared across compression ratios (including original uncompressed prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported as generally best-performing among evaluated compressors: (Long)LLMLingua and LLMLingua-2 excel on summarization; LongLLMLingua performs best on long-context QA. (Paper reports these as the top methods, especially at high compression ratios.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Outperforms other compressors at high compression ratios; Selective Context leads in reconstruction and human-centric datasets while (Long)LLMLingua family dominates summarization and long-context QA.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>LLMLingua-family methods concentrate preserved tokens around semantically rich sections and use mechanisms (budget controller, token-level iterative compression, question-awareness, data distillation) that help retain critical information under high compression ratios; LongLLMLingua's question-aware compression helps in long contexts by preserving question-relevant content.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Performance comparisons averaged across three LLMs; compression ratio sweep studies (Figure 3 & 5) show LLMLingua methods maintain advantage at higher ρ. Default experimental ratio 0.5 unless specified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study on Prompt Compression for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9250.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9250.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selective Context</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selective Context (LLM scoring-based compressor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scoring-based token pruning method that computes informativeness/self-information per token using a base causal language model and prunes tokens with lower scores to produce a concise context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Compressing context to enhance inference efficiency of large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reconstruction; human-centric datasets; QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reconstruction tasks and QA on datasets such as BBC News and ShareGPT (human-centric), GSM8K for math contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Compressed prompt created by scoring tokens (self-information) and pruning; ratio adjustable (default ρ=0.5).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to LLMLingua variants, KiS, SCRL, and random selection; original uncompressed prompt also used as reference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Selective Context led reconstruction tasks and performed best on human-centric datasets (BBC News, ShareGPT), aiding reconstruction by retaining tokens evenly across text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Outperforms LLMLingua-family on reconstruction/human-centric reconstruction tasks but underperforms LLMLingua variants on summarization and long-context QA.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because Selective Context retains tokens more evenly across the text (rather than concentrated semantically-rich chunks), it preserves reconstructive cues better, which helps reconstruction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Compression ratio adjustable; when set to 0.5 Selective Context was evaluated alongside other methods; averaged metrics across three LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study on Prompt Compression for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9250.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9250.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KiS / SCRL (RL-based)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KiS (Keep it Simple) and SCRL (unsupervised sentence compression via RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reinforcement-learning-based compressors: KiS regenerates shortened text (autoregressive simplification), SCRL performs token-level sequence labeling to remove non-essential tokens; KiS is slower, SCRL is computationally efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Keep it simple: Unsupervised simplification of multi-paragraph text.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Summarization, Reconstruction, QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same suite of tasks; KiS and SCRL included though their compression ratio is not directly adjustable (self-adapted).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Compressed prompt produced by RL-based simplification/regeneration (KiS) or RL-based token labeling (SCRL).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared with Selective Context, LLMLingua-family and random selection; original prompt baseline used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>KiS and SCRL produce reasonable compressed prompts; SCRL provides the best computational efficiency and lowest memory usage among tested methods (paper notes SCRL lowest time cost and minimal memory).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>KiS sometimes achieves strong BERTScore/ROUGE but is time-intensive; SCRL is fastest but may not match LLMLingua-family in high-ratio performance.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>KiS's autoregressive regeneration can preserve fluency and salience but is slower; SCRL's sequence-labeling approach is efficient, making it preferable where compute is constrained.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Table 3 reports computational overhead: SCRL has the lowest time-per-token and smallest memory footprint (details in Table 3). KiS and SCRL compression ratios are self-adapted (not controlled).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study on Prompt Compression for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9250.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9250.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Compression ratio effect (short vs long contexts)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of compression ratio on LLM performance for short vs long contexts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study finding that increasing compression ratio uniformly harms short-context task performance, whereas for long-context tasks moderate compression can improve performance up to a point before degrading.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku (averaged)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Short-context QA; Long-context QA; Summarization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Short-context QA and long-context QA (LongBench) plus summarization tasks; metric scaling applied across tasks for figure plotting.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Series of compressed prompts at multiple compression ratios (swept from low to high ρ) for each compressor; short-context and long-context tasks analyzed separately.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original uncompressed prompt and multiple compression ratios across different compressors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Short contexts: performance of all compressors declines monotonically as ρ increases. Long contexts: performance increases initially as ρ increases (moderate compression) and then declines at high ρ (Figures 3 & 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Relative to original prompt: short-context tasks show clear negative effect with larger ρ; long-context tasks can show a positive delta at moderate ρ (no single universal numeric delta reported).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>In long contexts, compression helps by abstracting and removing irrelevant content that dilutes signal; in short contexts, compression removes essential tokens, causing consistent degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>For Selective Context, LLMLingua, LongLLMLingua, LLMLingua-2 compression ratio adjustable (default 0.5); KiS and SCRL not adjustable. Evaluation: 100 random instances per dataset per point; averaged metrics across three models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study on Prompt Compression for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9250.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9250.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ResponseLength change</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of prompt compression on LLM response length</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt compression systematically affects produced response lengths in a model-dependent way: the same compression methods increase response length for some LLMs and decrease for others.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>QA (1000 instances sampled)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Recorded number of words in responses to 1000 QA tasks under different prompt compression methods and averaged across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Compressed prompts from multiple compressors (as applied in main experiments) vs original prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original uncompressed prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Average response length changes: GPT-3.5-turbo increased by +1.2 words on average (from 56.8 to 58.0), GPT-4o-mini increased by +1.1 words (from 74.9 to 76.0), Claude-3-Haiku decreased by −2.8 words on average (from 124.6 to 121.8) (Table 4 averages).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>All compressors (including random selection) cause similar directional trends per model: GPT-3.5/GPT-4o-mini increase length; Claude-3-Haiku decreases length.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+1.2 words (GPT-3.5-turbo average), +1.1 words (GPT-4o-mini average), −2.8 words (Claude-3-Haiku average).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize smaller models/variants (or those that normally produce shorter responses) may elaborate to compensate for information loss; models that already produce long outputs (Claude-3-Haiku) may be streamlined by compressed prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Measured on 1000 QA tasks; numbers reported in Table 4 are averages across prompt compression methods and show differences vs original prompt in parentheses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study on Prompt Compression for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9250.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9250.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hallucination increase</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-compression-induced hallucination (MiHR/MaHR, ASH vs ILH)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt compression increases hallucination rates in LLM outputs; hallucinations categorized as Altered Semantic Hallucination (ASH) and Information Loss Hallucination (ILH), with ILH being dominant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reconstruction, Summarization, QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Manual annotation of hallucinations on 120 random samples per task category (40 per model) to compute Micro Hallucination Rate (MiHR) and Macro Hallucination Rate (MaHR).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Compressed prompts (six compressors) vs original prompts; hallucinations manually labeled and categorized.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original uncompressed prompts (baseline) and across compressors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>All compression methods increase hallucination (both MiHR and MaHR). LLMLingua-2 had the least hallucination in reconstruction and summarization; LongLLMLingua had the lowest hallucination rate in long-context QA (Table 5, figures 6 & 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Relative increases in MiHR/MaHR across compressors (exact numeric increases not listed in text); distributions of ASH vs ILH show ILH (information loss) is the primary cause.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Compressed prompts often create truncated or grammatically incomplete inputs that prompt LLMs to 'fill in' missing content, leading to hallucinations. Information loss (ILH) is the main driver rather than semantic alteration alone.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>120 instances sampled per task (40 per model). Hallucinations manually annotated and MiHR/MaHR computed per Li et al. (2024) definitions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study on Prompt Compression for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9250.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9250.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multimodal applicability (VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effectiveness of text-only prompt compressors when applied to Multimodal LLMs (MLLMs) for VQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of text-based prompt compression methods on multimodal VQA tasks shows mixed transferability: some methods vary across datasets while LLMLingua variants are stable but not best-performing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini, Claude-3-Haiku (averaged)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VQA (IconQA, OK-VQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge- and reasoning-based visual question answering benchmarks where textual context (question/prompt) is compressed and paired with image input for MLLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Text-only trained compressors applied to the textual part of multimodal prompts (compression ratio 0.5 default), then used with MLLMs for VQA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original uncompressed prompts and comparisons among compressors (SCRL, Selective Context, LLMLingua-2, LLMLingua, LongLLMLingua).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Performance varies by dataset and compressor; SCRL, Selective Context, LLMLingua-2 show inconsistent performance across datasets; LLMLingua and LongLLMLingua are stable but suboptimal (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Because compressors were trained on text-only tasks, they may not preserve multimodal cues or the specific textual signals required for VQA; further adaptation is needed for multimodal tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>500 samples each from IconQA-txt, IconQA-blank, OK-VQA; averaged scores between GPT-4o-mini and Claude-3-Haiku.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study on Prompt Compression for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9250.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9250.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WordOmission analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Analysis of which words can be omitted and impact of omission</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical analysis of most-frequently omitted words across compressors and effect of removing individual words on QA performance; function words often omitted with varying impact, especially stronger effects in long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Short-context QA; Long-context QA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Random sampling (500 instances each) to measure performance impact of removing individual words (e.g., 'the', 'a') normalized by original prompt score.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompt variants with specific words removed (single-word ablations) compared to original prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Original prompts (no word removed) vs single-word-removed prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Some high-frequency function words (e.g., 'the') when removed have almost no impact; removal of words like 'a' can adversely affect performance; impact larger in long-context QA than short-context QA (Figure 9).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors hypothesize that seemingly low-information tokens may act as 'register' tokens or placeholders that preserve structure or intermediate computational state, analogous to 'register' tokens in vision transformers; removing them can harm model processing in long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Figure 8 lists top-10 omitted words across compressors; Figure 9 shows normalized performance after word removal on 500 samples for short and long-context QA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Empirical Study on Prompt Compression for Large Language Models', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llmlingua: Compressing prompts for accelerated inference of large language models. <em>(Rating: 2)</em></li>
                <li>LongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. <em>(Rating: 2)</em></li>
                <li>LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. <em>(Rating: 2)</em></li>
                <li>Compressing context to enhance inference efficiency of large language models. <em>(Rating: 2)</em></li>
                <li>Keep it simple: Unsupervised simplification of multi-paragraph text. <em>(Rating: 1)</em></li>
                <li>Efficient unsupervised sentence compression by fine-tuning transformers with reinforcement learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9250",
    "paper_id": "paper-278237303",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "PromptCompression (general)",
            "name_full": "Prompt compression (general concept)",
            "brief_description": "Reducing prompt/context length before feeding it to LLMs to lower compute/cost while aiming to retain task-relevant information; studied here across six compressors, multiple tasks, and compression ratios.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku",
            "model_size": null,
            "task_name": "Summarization, Reconstruction, Question Answering (QA), VQA",
            "task_description": "Standard summarization and reconstruction datasets plus QA (including long-context QA) and multimodal VQA; evaluated using BLEU/ROUGE/BERTScore for summarization/reconstruction and accuracy/F1 for QA/VQA.",
            "presentation_format": "Compressed prompt produced by an external compressor (six methods evaluated: KiS, SCRL, Selective Context, LLMLingua, LongLLMLingua, LLMLingua-2) at various compression ratios (default ρ = 0.5 unless otherwise specified). Compression can be query-aware (e.g., LongLLMLingua) or query-agnostic.",
            "comparison_format": "Original (uncompressed) prompt; random token selection baseline and comparisons across different compressors and compression ratios.",
            "performance": "Varies by task and compression ratio; generally: short-context tasks — performance degrades monotonically as compression ratio increases; long-context tasks — moderate compression can improve performance relative to original (see Figures 3 & 5).",
            "performance_comparison": "Compared to original prompt: short contexts show consistent degradation with higher ρ; long contexts show an initial performance increase for moderate ρ then decline at high ρ (no single numeric effect-size universally reported).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "For short contexts compression removes relevant information causing steady performance drops; for long contexts moderate compression helps by removing irrelevant/distracting material and concentrating semantically rich tokens (especially if compression is question-aware), improving signal-to-noise.",
            "null_or_negative_result": false,
            "experimental_details": "Six compressors evaluated, compression ratio adjustable for Selective Context, LLMLingua, LongLLMLingua, LLMLingua-2 (default 0.5); KiS and SCRL adaptively choose ratio. Evaluations averaged across GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku; datasets include Gigaword, DUC2004, BNC, Google, Broadcast (summarization), GSM8K/BBC/ArXiv/ShareGPT (reconstruction), LongBench/BBH/GSM8K (QA), IconQA/OK-VQA (VQA).",
            "uuid": "e9250.0",
            "source_info": {
                "paper_title": "An Empirical Study on Prompt Compression for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "LLMLingua / LongLLMLingua / LLMLingua-2",
            "name_full": "LLMLingua, LongLLMLingua, LLMLingua-2 (LLM annotation-based compressors)",
            "brief_description": "Coarse-to-fine and token-level prompt compression methods using LLM annotation and trained small models; LongLLMLingua is question-aware and tailored for long contexts; LLMLingua-2 uses data distillation and token classification for task-agnostic compression.",
            "citation_title": "Llmlingua: Compressing prompts for accelerated inference of large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku",
            "model_size": null,
            "task_name": "Summarization; Long-context QA; General QA and reconstruction",
            "task_description": "Summarization tasks (Gigaword, DUC2004, etc.) and QA including long-context benchmarks (LongBench); also used in reconstruction comparisons.",
            "presentation_format": "Compressed prompts produced by LLMLingua-family compressors at adjustable ratios (default ρ=0.5; evaluated across a range of ρ). LongLLMLingua is question-aware (compresses conditioned on the query).",
            "comparison_format": "Compared against Selective Context, KiS, SCRL, and random-selection baseline; also compared across compression ratios (including original uncompressed prompts).",
            "performance": "Reported as generally best-performing among evaluated compressors: (Long)LLMLingua and LLMLingua-2 excel on summarization; LongLLMLingua performs best on long-context QA. (Paper reports these as the top methods, especially at high compression ratios.)",
            "performance_comparison": "Outperforms other compressors at high compression ratios; Selective Context leads in reconstruction and human-centric datasets while (Long)LLMLingua family dominates summarization and long-context QA.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "LLMLingua-family methods concentrate preserved tokens around semantically rich sections and use mechanisms (budget controller, token-level iterative compression, question-awareness, data distillation) that help retain critical information under high compression ratios; LongLLMLingua's question-aware compression helps in long contexts by preserving question-relevant content.",
            "null_or_negative_result": false,
            "experimental_details": "Performance comparisons averaged across three LLMs; compression ratio sweep studies (Figure 3 & 5) show LLMLingua methods maintain advantage at higher ρ. Default experimental ratio 0.5 unless specified.",
            "uuid": "e9250.1",
            "source_info": {
                "paper_title": "An Empirical Study on Prompt Compression for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Selective Context",
            "name_full": "Selective Context (LLM scoring-based compressor)",
            "brief_description": "A scoring-based token pruning method that computes informativeness/self-information per token using a base causal language model and prunes tokens with lower scores to produce a concise context.",
            "citation_title": "Compressing context to enhance inference efficiency of large language models.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku",
            "model_size": null,
            "task_name": "Reconstruction; human-centric datasets; QA",
            "task_description": "Reconstruction tasks and QA on datasets such as BBC News and ShareGPT (human-centric), GSM8K for math contexts.",
            "presentation_format": "Compressed prompt created by scoring tokens (self-information) and pruning; ratio adjustable (default ρ=0.5).",
            "comparison_format": "Compared to LLMLingua variants, KiS, SCRL, and random selection; original uncompressed prompt also used as reference.",
            "performance": "Selective Context led reconstruction tasks and performed best on human-centric datasets (BBC News, ShareGPT), aiding reconstruction by retaining tokens evenly across text.",
            "performance_comparison": "Outperforms LLMLingua-family on reconstruction/human-centric reconstruction tasks but underperforms LLMLingua variants on summarization and long-context QA.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Because Selective Context retains tokens more evenly across the text (rather than concentrated semantically-rich chunks), it preserves reconstructive cues better, which helps reconstruction tasks.",
            "null_or_negative_result": false,
            "experimental_details": "Compression ratio adjustable; when set to 0.5 Selective Context was evaluated alongside other methods; averaged metrics across three LLMs.",
            "uuid": "e9250.2",
            "source_info": {
                "paper_title": "An Empirical Study on Prompt Compression for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "KiS / SCRL (RL-based)",
            "name_full": "KiS (Keep it Simple) and SCRL (unsupervised sentence compression via RL)",
            "brief_description": "Reinforcement-learning-based compressors: KiS regenerates shortened text (autoregressive simplification), SCRL performs token-level sequence labeling to remove non-essential tokens; KiS is slower, SCRL is computationally efficient.",
            "citation_title": "Keep it simple: Unsupervised simplification of multi-paragraph text.",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku",
            "model_size": null,
            "task_name": "Summarization, Reconstruction, QA",
            "task_description": "Same suite of tasks; KiS and SCRL included though their compression ratio is not directly adjustable (self-adapted).",
            "presentation_format": "Compressed prompt produced by RL-based simplification/regeneration (KiS) or RL-based token labeling (SCRL).",
            "comparison_format": "Compared with Selective Context, LLMLingua-family and random selection; original prompt baseline used.",
            "performance": "KiS and SCRL produce reasonable compressed prompts; SCRL provides the best computational efficiency and lowest memory usage among tested methods (paper notes SCRL lowest time cost and minimal memory).",
            "performance_comparison": "KiS sometimes achieves strong BERTScore/ROUGE but is time-intensive; SCRL is fastest but may not match LLMLingua-family in high-ratio performance.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "KiS's autoregressive regeneration can preserve fluency and salience but is slower; SCRL's sequence-labeling approach is efficient, making it preferable where compute is constrained.",
            "null_or_negative_result": false,
            "experimental_details": "Table 3 reports computational overhead: SCRL has the lowest time-per-token and smallest memory footprint (details in Table 3). KiS and SCRL compression ratios are self-adapted (not controlled).",
            "uuid": "e9250.3",
            "source_info": {
                "paper_title": "An Empirical Study on Prompt Compression for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Compression ratio effect (short vs long contexts)",
            "name_full": "Effect of compression ratio on LLM performance for short vs long contexts",
            "brief_description": "Study finding that increasing compression ratio uniformly harms short-context task performance, whereas for long-context tasks moderate compression can improve performance up to a point before degrading.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku (averaged)",
            "model_size": null,
            "task_name": "Short-context QA; Long-context QA; Summarization",
            "task_description": "Short-context QA and long-context QA (LongBench) plus summarization tasks; metric scaling applied across tasks for figure plotting.",
            "presentation_format": "Series of compressed prompts at multiple compression ratios (swept from low to high ρ) for each compressor; short-context and long-context tasks analyzed separately.",
            "comparison_format": "Original uncompressed prompt and multiple compression ratios across different compressors.",
            "performance": "Short contexts: performance of all compressors declines monotonically as ρ increases. Long contexts: performance increases initially as ρ increases (moderate compression) and then declines at high ρ (Figures 3 & 5).",
            "performance_comparison": "Relative to original prompt: short-context tasks show clear negative effect with larger ρ; long-context tasks can show a positive delta at moderate ρ (no single universal numeric delta reported).",
            "format_effect_size": null,
            "explanation_or_hypothesis": "In long contexts, compression helps by abstracting and removing irrelevant content that dilutes signal; in short contexts, compression removes essential tokens, causing consistent degradation.",
            "null_or_negative_result": true,
            "experimental_details": "For Selective Context, LLMLingua, LongLLMLingua, LLMLingua-2 compression ratio adjustable (default 0.5); KiS and SCRL not adjustable. Evaluation: 100 random instances per dataset per point; averaged metrics across three models.",
            "uuid": "e9250.4",
            "source_info": {
                "paper_title": "An Empirical Study on Prompt Compression for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ResponseLength change",
            "name_full": "Effect of prompt compression on LLM response length",
            "brief_description": "Prompt compression systematically affects produced response lengths in a model-dependent way: the same compression methods increase response length for some LLMs and decrease for others.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku",
            "model_size": null,
            "task_name": "QA (1000 instances sampled)",
            "task_description": "Recorded number of words in responses to 1000 QA tasks under different prompt compression methods and averaged across methods.",
            "presentation_format": "Compressed prompts from multiple compressors (as applied in main experiments) vs original prompts.",
            "comparison_format": "Original uncompressed prompts.",
            "performance": "Average response length changes: GPT-3.5-turbo increased by +1.2 words on average (from 56.8 to 58.0), GPT-4o-mini increased by +1.1 words (from 74.9 to 76.0), Claude-3-Haiku decreased by −2.8 words on average (from 124.6 to 121.8) (Table 4 averages).",
            "performance_comparison": "All compressors (including random selection) cause similar directional trends per model: GPT-3.5/GPT-4o-mini increase length; Claude-3-Haiku decreases length.",
            "format_effect_size": "+1.2 words (GPT-3.5-turbo average), +1.1 words (GPT-4o-mini average), −2.8 words (Claude-3-Haiku average).",
            "explanation_or_hypothesis": "Authors hypothesize smaller models/variants (or those that normally produce shorter responses) may elaborate to compensate for information loss; models that already produce long outputs (Claude-3-Haiku) may be streamlined by compressed prompts.",
            "null_or_negative_result": false,
            "experimental_details": "Measured on 1000 QA tasks; numbers reported in Table 4 are averages across prompt compression methods and show differences vs original prompt in parentheses.",
            "uuid": "e9250.5",
            "source_info": {
                "paper_title": "An Empirical Study on Prompt Compression for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Hallucination increase",
            "name_full": "Prompt-compression-induced hallucination (MiHR/MaHR, ASH vs ILH)",
            "brief_description": "Prompt compression increases hallucination rates in LLM outputs; hallucinations categorized as Altered Semantic Hallucination (ASH) and Information Loss Hallucination (ILH), with ILH being dominant.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku",
            "model_size": null,
            "task_name": "Reconstruction, Summarization, QA",
            "task_description": "Manual annotation of hallucinations on 120 random samples per task category (40 per model) to compute Micro Hallucination Rate (MiHR) and Macro Hallucination Rate (MaHR).",
            "presentation_format": "Compressed prompts (six compressors) vs original prompts; hallucinations manually labeled and categorized.",
            "comparison_format": "Original uncompressed prompts (baseline) and across compressors.",
            "performance": "All compression methods increase hallucination (both MiHR and MaHR). LLMLingua-2 had the least hallucination in reconstruction and summarization; LongLLMLingua had the lowest hallucination rate in long-context QA (Table 5, figures 6 & 7).",
            "performance_comparison": "Relative increases in MiHR/MaHR across compressors (exact numeric increases not listed in text); distributions of ASH vs ILH show ILH (information loss) is the primary cause.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Compressed prompts often create truncated or grammatically incomplete inputs that prompt LLMs to 'fill in' missing content, leading to hallucinations. Information loss (ILH) is the main driver rather than semantic alteration alone.",
            "null_or_negative_result": false,
            "experimental_details": "120 instances sampled per task (40 per model). Hallucinations manually annotated and MiHR/MaHR computed per Li et al. (2024) definitions.",
            "uuid": "e9250.6",
            "source_info": {
                "paper_title": "An Empirical Study on Prompt Compression for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Multimodal applicability (VQA)",
            "name_full": "Effectiveness of text-only prompt compressors when applied to Multimodal LLMs (MLLMs) for VQA",
            "brief_description": "Evaluation of text-based prompt compression methods on multimodal VQA tasks shows mixed transferability: some methods vary across datasets while LLMLingua variants are stable but not best-performing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini, Claude-3-Haiku (averaged)",
            "model_size": null,
            "task_name": "VQA (IconQA, OK-VQA)",
            "task_description": "Knowledge- and reasoning-based visual question answering benchmarks where textual context (question/prompt) is compressed and paired with image input for MLLMs.",
            "presentation_format": "Text-only trained compressors applied to the textual part of multimodal prompts (compression ratio 0.5 default), then used with MLLMs for VQA.",
            "comparison_format": "Original uncompressed prompts and comparisons among compressors (SCRL, Selective Context, LLMLingua-2, LLMLingua, LongLLMLingua).",
            "performance": "Performance varies by dataset and compressor; SCRL, Selective Context, LLMLingua-2 show inconsistent performance across datasets; LLMLingua and LongLLMLingua are stable but suboptimal (Table 6).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Because compressors were trained on text-only tasks, they may not preserve multimodal cues or the specific textual signals required for VQA; further adaptation is needed for multimodal tasks.",
            "null_or_negative_result": false,
            "experimental_details": "500 samples each from IconQA-txt, IconQA-blank, OK-VQA; averaged scores between GPT-4o-mini and Claude-3-Haiku.",
            "uuid": "e9250.7",
            "source_info": {
                "paper_title": "An Empirical Study on Prompt Compression for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "WordOmission analysis",
            "name_full": "Analysis of which words can be omitted and impact of omission",
            "brief_description": "Empirical analysis of most-frequently omitted words across compressors and effect of removing individual words on QA performance; function words often omitted with varying impact, especially stronger effects in long contexts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo, GPT-4o-mini, Claude-3-Haiku",
            "model_size": null,
            "task_name": "Short-context QA; Long-context QA",
            "task_description": "Random sampling (500 instances each) to measure performance impact of removing individual words (e.g., 'the', 'a') normalized by original prompt score.",
            "presentation_format": "Prompt variants with specific words removed (single-word ablations) compared to original prompts.",
            "comparison_format": "Original prompts (no word removed) vs single-word-removed prompts.",
            "performance": "Some high-frequency function words (e.g., 'the') when removed have almost no impact; removal of words like 'a' can adversely affect performance; impact larger in long-context QA than short-context QA (Figure 9).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors hypothesize that seemingly low-information tokens may act as 'register' tokens or placeholders that preserve structure or intermediate computational state, analogous to 'register' tokens in vision transformers; removing them can harm model processing in long contexts.",
            "null_or_negative_result": false,
            "experimental_details": "Figure 8 lists top-10 omitted words across compressors; Figure 9 shows normalized performance after word removal on 500 samples for short and long-context QA.",
            "uuid": "e9250.8",
            "source_info": {
                "paper_title": "An Empirical Study on Prompt Compression for Large Language Models",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llmlingua: Compressing prompts for accelerated inference of large language models.",
            "rating": 2,
            "sanitized_title": "llmlingua_compressing_prompts_for_accelerated_inference_of_large_language_models"
        },
        {
            "paper_title": "LongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression.",
            "rating": 2,
            "sanitized_title": "longllmlingua_accelerating_and_enhancing_llms_in_long_context_scenarios_via_prompt_compression"
        },
        {
            "paper_title": "LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression.",
            "rating": 2,
            "sanitized_title": "llmlingua2_data_distillation_for_efficient_and_faithful_taskagnostic_prompt_compression"
        },
        {
            "paper_title": "Compressing context to enhance inference efficiency of large language models.",
            "rating": 2,
            "sanitized_title": "compressing_context_to_enhance_inference_efficiency_of_large_language_models"
        },
        {
            "paper_title": "Keep it simple: Unsupervised simplification of multi-paragraph text.",
            "rating": 1,
            "sanitized_title": "keep_it_simple_unsupervised_simplification_of_multiparagraph_text"
        },
        {
            "paper_title": "Efficient unsupervised sentence compression by fine-tuning transformers with reinforcement learning.",
            "rating": 1,
            "sanitized_title": "efficient_unsupervised_sentence_compression_by_finetuning_transformers_with_reinforcement_learning"
        }
    ],
    "cost": 0.01836425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>AN EMPIRICAL STUDY ON PROMPT COMPRESSION FOR LARGE LANGUAGE MODELS
24 Apr 2025</p>
<p>Zheng Zhang 
The Hong Kong University of Science and Technology (Guangzhou)</p>
<p>Jinyi Li 
South China University of Technology</p>
<p>Yihuai Lan 
The Hong Kong University of Science and Technology (Guangzhou)</p>
<p>Xiang Wang 
University of Science and Technology of China</p>
<p>Hao Wang haowang@hkust-gz.edu.cn 
The Hong Kong University of Science and Technology (Guangzhou)</p>
<p>AN EMPIRICAL STUDY ON PROMPT COMPRESSION FOR LARGE LANGUAGE MODELS
24 Apr 20255DEBDED5AF525459D94B92D500F02A6EarXiv:2505.00019v1[cs.CL]
Prompt engineering enables Large Language Models (LLMs) to perform a variety of tasks.However, lengthy prompts significantly increase computational complexity and economic costs.To address this issue, we study six prompt compression methods for LLMs, aiming to reduce prompt length while maintaining LLM response quality.In this paper, we present a comprehensive analysis covering aspects such as generation performance, model hallucinations, efficacy in multimodal tasks, word omission analysis, and more.We evaluate these methods across 13 datasets, including news, scientific articles, commonsense QA, math QA, long-context QA, and VQA datasets.Our experiments reveal that prompt compression has a greater impact on LLM performance in long contexts compared to short ones.In the Longbench evaluation, moderate compression even enhances LLM performance.Our code and data is available at https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.</p>
<p>INTRODUCTION</p>
<p>Large Language Models (LLMs) have demonstrated remarkable generalization capabilities (Grosse et al., 2023;Yang et al., 2024), allowing them to adapt to a wide range of tasks through prompt engineering techniques such as CoT (Wei et al., 2024), ICL (Dong et al., 2024), and RAG (Lewis et al., 2020) without necessitating fine-tuning.However, this advantage comes with an obvious drawback: increasing the length of prompts to encompass the necessary information, which subsequently escalates computational overhead (Wang et al., 2024).Also, for online models such as ChatGPT and Claude, lengthy prompts inflate the economic cost associated with API calls.</p>
<p>To address this issue, prompt compression is the most straightforward strategy.As illustrated in Figure 1, it aims to reduce the length of prompts while retaining the essential information.However, previous works (Li et al., 2023;Jiang et al., 2024;Pan et al., 2024) have primarily focused on how LLMs perform on various tasks (e.g.summarization, reconstruction and question answering) using common metrics (e.g.accuracy, BLEU (Papineni et al., 2002b), ROUGE (Lin, 2004b) and BERTScore (Devlin et al., 2019)) after applying prompt compression.There has been a noticeable gap in understanding how prompt compression affects other aspects of LLM output, beyond the specific task performance.</p>
<p>Specifically, the effects on aspects such as generalizability and hallucinations have not been thoroughly examined.Moreover, existing works rarely apply prompt compression to Multimodal LLMs (MLLMs), raising questions about the generalizability of compression techniques in multimodal tasks.Furthermore, what kind of prompt words can be omitted when prompting is also underinvestigated.This may provide valuable insights for more effective prompt engineering strategies.</p>
<p>Therefore, it is crucial to explore the broader impacts of different prompt compression methods on (M)LLMs across different tasks.The original context is distilled into a more concise form while preserving pertinent information for LLMs to process.Some methods compress the context based on the query, while others do not.Words that are underlined in the original text denote the segments that are trimmed by the compressor.</p>
<p>Our key findings can be summarized as follows:</p>
<p>• (Long)LLMLingua and LLMLingua-2 generally outperform other methods, especially at high compression ratios.• All methods' performance decreases with increasing compression ratios for short contexts, but for long contexts, moderate compression can improve performance.• Prompt compression can influence response length, with the direction of change depending on the specific LLM.• All methods result in some degree of increased hallucination, with information loss being the primary reason.</p>
<p>Our contributions can be summarized as follows:</p>
<p>(1) We present a comprehensive study that evaluates various prompt compression methods across different tasks.</p>
<p>(2) By analyzing the effects of prompt compression on response length, hallucinations, and its generalizability in multimodal contexts, we provide insights beyond traditional metrics.</p>
<p>(3) We compile our implementation into an open-source toolkit, facilitating further research in prompt compression for LLMs.</p>
<p>RELATED WORKS</p>
<p>LLM'S LONG CONTEXT PROCESSING METHOD</p>
<p>Given the performance limitations and computational overhead of LLMs (Wang et al., 2024), how to effectively apply LLMs to tasks involving lengthy textual inputs is a persistent challenge.Various solutions have emerged to address this issue, encompassing techniques such as length extrapolation (Chen et al., 2021;Shaw et al., 2018), attention approximation (Winata et al., 2019;Wang et al., 2020), attention-free transformers (Gu et al., 2021;Orvieto et al., 2023), model compression (Lee et al., 2023;Ma et al., 2023), and hardware-aware transformers (Dao et al., 2022;Liu &amp; Abbeel, 2023).</p>
<p>In this paper, we focus mainly on the prompt compression techniques, especially those that do not rely on the internal states or parameters of LLMs and operate in a text-in, text-out manner.These methods present several advantages: they can be seamlessly integrated with different model architectures without requiring additional modifications, and they are particularly beneficial for online models, helping to reduce the economic costs associated with API calls.</p>
<p>PROMPT COMPRESSION</p>
<p>Figure 1 illustrates the concept of prompt compression, and the compression ratio ρ for prompt compression is defined as:
ρ = 1 − L c L o . (1)
Here L c is the compressed context length and L o is the original context length.Many prompt compression methods have been developed to handle long prompts in LLMs.KiS (Laban et al., 2021) and SCRL (Ghalandari et al., 2022) leverage reinforcement learning (RL) to train models for text compression without the need for ground-truth data, optimizing specific objectives such as fluency and simplicity.Recently, with advances in LLMs, some methods (Li et al., 2023;Jiang et al., 2023;2024;Pan et al., 2024) employ pre-trained language models and various strategies to identify and prune redundant or less informative content.</p>
<p>Besides text-based methods, there are techniques aimed at compressing or trimming the hidden states or KV caches (Liu et al., 2023b;Zhang et al., 2023;Xiao et al., 2024;Ge et al., 2024).However, these methods are separate from our study and are not easily applicable to various model architectures or closed-source LLMs.</p>
<p>METHODS</p>
<p>Figure 2 illustrates the workflows for three categories of prompt compression methods, from which we select six methods: (1) RL-based: KiS, SCRL, (2) LLM scoring-based: Selective Context, and</p>
<p>(3) LLM annotation-based: LLMLingua, LongLLMLingua, LLMLingua-2.Among them, KiS does not typically trim words but uses an autoregressive approach to regenerate a shorter context, which can be time-intensive.However, we include it for comparison.</p>
<p>KiS. Laban et al. (2021) tackles the challenge of text simplification in an unsupervised manner, balancing fluency, salience, and simplicity.The model leverages reinforcement learning to enhance its performance by generating multiple candidate simplifications and optimizing for a composite reward.Utilizing a k-SCST algorithm, KiS generates k candidate outputs for each input, computing a reward for each, and promotes candidates surpassing the mean reward.</p>
<p>SCRL.Ghalandari et al. (2022) also represents unsupervised sentence compression via reinforcement learning, focusing on sequence labeling.It fine-tunes a pre-trained transformer model using a simple policy gradient approach.Each token in a sentence is labeled as essential or non-essential, optimizing the reward functions to maximize the compression quality while maintaining fluency and faithfulness.</p>
<p>Selective Context.Li et al. (2023) involves assessing the informativeness of lexical units by computing their self-information using a base causal language model.By pruning the redundant parts, a more concise context is obtained.</p>
<p>LLMLingua.Jiang et al. (2023) introduces a coarse-to-fine prompt compression method to handle lengthy prompts.LLMLingua includes a budget controller to ensure semantic integrity during high compression ratios, a token-level iterative compression algorithm to model interdependencies, and instruction tuning to align distributions between a small model and LLMs.</p>
<p>LongLLMLingua.Building on LLMLingua, LongLLMLingua (Jiang et al., 2024) is tailored for long context scenarios.It employs a question-aware coarse-to-fine compression technique and reorders documents to mitigate position bias (Liu et al., 2023a).It supports dynamic compression ratios and includes a post-compression strategy to ensure the preservation of content integrity.</p>
<p>LLMLingua-2.Developed as an advancement over LLMLingua, LLMLingua-2 (Pan et al., 2024) focuses on task-agnostic prompt compression for enhanced generalizability and efficiency.It introduces a data distillation procedure from GPT-4, creating an extractive text compression dataset to align with compression objectives effectively.LLMLingua-2 frames prompt compression as a token classification task using a Transformer encoder to leverage full bidirectional context, addressing the reliance on unidirectional context in prior approaches.</p>
<p>EXPERIMENT SETUP</p>
<p>TASKS AND DATASETS</p>
<p>For our study on prompt compression for LLMs, we designated three tasks: summarization, reconstruction, and question answering (QA).The summarization task involves generating summaries from both the original and compressed contexts and measuring the similarity between these summaries.We use datasets including Gigaword (Rush et al., 2015), DUC2004 (Over et al., 2007), BNC (Consortium, 2007), Google (Filippova &amp; Altun, 2013), and Broadcast (Clarke &amp; Lapata, 2008a).</p>
<p>The reconstruction task involves prompting the LLM to reconstruct the original prompt from the compressed prompt and includes datasets like GSM8K (Cobbe et al., 2021), BBC News, Arxiv articles, and ShareGPT (Li et al., 2023).The QA task1 leverages datasets including LongBench (Bai et al., 2024), BBH (Suzgun et al., 2023), and GSM8K2 .</p>
<p>For MLLMs, our primary focus is on their performance in the VQA task, utilizing datasets including IconQA (Lu et al., 2021) and OK-VQA (Marino et al., 2019).Further details about these datasets can be found in Appendix A.1.</p>
<p>METRICS</p>
<p>For the summarization and reconstruction tasks, we utilize BLEU 3 Papineni et al. (2002b), ROUGE4 (Lin, 2004b), and BERTScore5 (Zhang* et al., 2020) to measure the similarity between the generated and reference outputs.For QA and VQA tasks, we differentiate the evaluation metrics based on the nature of the answers.For tasks with clear, precise answers, accuracy is used as the evaluation metric.For open-ended questions, we assess the similarity between the generated responses and reference answers using F1 (Bai et al., 2024).For hallucination detection, following (Li et al., 2024), we use micro hallucination rate (MiHR) and macro hallucination rate (MaHR) to evaluate the degree of hallucination.Further details about the computation of these metrics can be found in Appendix A.2.</p>
<p>IMPLEMENTATIONS</p>
<p>In our experiments, we selected the six prompt compression methods mentioned in Section 3: KiS, SCRL, Selective Context, LLMLingua, LongLLMLingua, and LLMLingua-2.For KiS and SCRL, the compression ratio is self-adapted, while for Selective Context, LLMLingua, LongLLMLingua, and LLMLingua-2, the compression ratio is adjustable.We set it to 0.5 unless otherwise specified.</p>
<p>Table 1: Performance for reconstruction and summarization tasks.We grouped LLMLingua and LongLLMLingua together, as the performance differences between these two compressors were minimal for these two tasks.For each setting, we averaged the scores across three models: GPT-3.5-turbo,GPT-4o-mini, and Claude-3-Haiku.Additionally, we included a random selection strategy, which involves randomly picking words from the original prompt based on the compression ratio, to serve as a baseline comparison.We evaluated these methods' performance across three (M)LLMs: GPT-3.5-turbo,GPT-4o-mini, and Claude-3-Haiku.</p>
<p>Moreover, we have compiled our implementation into a comprehensive toolkit, which we have opensourced to facilitate reproducibility and further research.More details about the toolkit are provided in Appendix C.</p>
<p>EXPERIMENTAL RESULTS</p>
<p>MAIN RESULTS</p>
<p>Question 1: Which prompt compression method performs best across different tasks?  Figure 3: Performance on compression tasks under different compression ratios.We measured the performance of four compression methods by changing the compression ratio while keeping all other settings in accordance with Table 1.For each dataset, we randomly sampled 100 instances for evaluation and averaged their metrics.As mentioned in Section 4.3, KiS and SCRL cannot adjust the compression ratio and are thus not considered.</p>
<p>Our main findings are the following:</p>
<p>• (Long)LLMLingua and LLMLingua-2 excel in summarization tasks, while Selective Context leads in reconstruction tasks.(Long)LLMLingua is best for math contexts (GSM8K), LLMLingua-2 for news articles (Gigaword, DUC2004), and Selective Context for humancentric datasets (BBC News, ShareGPT).We observed that (Long)LLMLingua and LLMLingua-2 retain tokens that are concentrated around semantically rich sections of the text, which helps in creating summaries that capture the essential points effectively.On the other hand, Selective Context retains tokens more evenly distributed across the text, which aids in reconstruction tasks.• LongLLMLingua excels in QA tasks with longer contexts.This demonstrates its capacity to handle extensive information more effectively.For shorter contexts, performance varies across methods and datasets.Compared to short contexts, long contexts have the problem of diluting relevant information with irrelevant details.Unlike other methods, LongLLM-Lingua is question-aware, meaning it compresses prompts by considering the user's question in the prompt.We think that in long contexts, this approach helps to ensure that the most critical information related to the question is retained.This aligns with the ablation results from the LongLLMLingua paper regarding the question-aware mechanism.• SCRL offers the best computational efficiency.As indicated in  We categorized the QA tasks into four categories: logic (Boolean Expression, Web of Lies), common sense (Causal Judgement), math (GSM8K), and long context (LongBench), and calculated the average performance of six prompt compression methods on these four categories.Considering the different metrics, we scaled the results based on the mean performance for each task.</p>
<p>Question 2: How does compression ratio affect the performance of different methods?</p>
<p>Figure 3 illustrates the performance of various prompt compression methods across different compression ratios.Similarly, Figure 5 shows the impact of compression ratio on QA tasks.For shorter contexts, the performance of all methods uniformly declines as the compression ratio increases.However, for longer contexts, a different trend emerges: performance initially improves with increasing compression ratio up to a point, after which it begins to deteriorate.From these observations, we draw the following conclusions:</p>
<p>• (Long)LLMLingua and LLMLingua-2</p>
<p>show an advantage at higher compression ratios, as evidenced in Figure 3 and 5. • For longer contexts, a moderate amount of compression may help in abstracting and retaining the critical information better, thereby improving performance.</p>
<p>EFFECTS ON LLM RESPONSE</p>
<p>Question 3: Will prompt compression affect the length of the model's response?</p>
<p>Some works (Zheng et al., 2023;Singhal et al., 2024) leverage LLMs' perception of response length to optimize inference processes, which underscores the importance of understanding how factors like prompt compression can influence the output length.Notably, as shown in Table 4, the effect of different prompt compression methods on the response length of the same LLM demonstrates a uniform trend.For GPT-3.5-turbo and GPT-4o-mini, all prompt compression methods (even random selection) lead to an increase in response length.Conversely, for Claude-3-Haiku, all methods result in a decrease in response length.One possible interpretation is:</p>
<p>• GPT-3.5-turbo and GPT-4o-mini generally produce shorter responses, and the increase in length might be an attempt by these models to mitigate the loss of information due to prompt compression.• For Claude-3-Haiku, which typically generates longer responses, the reduced response length could imply that compression helps to streamline the output, resulting in more concise answers.</p>
<p>Additional details are provided in Appendix B.</p>
<p>Question 4: Will prompt compression enhance the hallucination?</p>
<p>The hallucination problem in LLMs has been widely acknowledged (Ji et al., 2023;Gudibande et al., 2024).Due to the fact that prompt compression can lead to some grammatically incorrect or overly succinct expressions, we posited that it might cause hallucinations in LLMs.Following the methodology of Li et al. (2024), we investigated the hallucination induced by prompt compression across different tasks, as detailed in Table 5.</p>
<p>In Figure 6, we divided the hallucinations induced by prompt compression into two categories: Altered Semantic Hallucination (ASH) and Information Loss Hallucination (ILH).Figure 7 depicts the proportions of each type of hallucination across different prompt compression methods.Our findings are as follows:</p>
<p>• All compression methods result in some degree of enhanced hallucination.As shown in Table 5, LLMLingua-2 exhibited the least amount of hallucination in reconstruction and summarization, while LongLLMLingua showed the lowest hallucination rate in long-context QA. • Information loss is a primary trigger for hallucinations in prompt compression.The generation of incomplete sentences often prompts LLMs to fill in gaps with their own generated content, leading to hallucinations.</p>
<p>Table 5: The impact of prompt compression on LLM hallucination.We randomly sampled 120 instances from each task category (40 samples each from GPT-3.5-turbo,GPT-4o-mini, and Claude-3-Haiku), manually annotated hallucinations, and computed their MaHR and MiHR according to the definitions described by Li et al. (2024).We calculated the proportion of different types of hallucinations in the average MiHR for six prompt compression methods.Hallucinations that could not be easily attributed to ASH or ILH were classified as "Other".</p>
<p>EFFECTIVENESS ON MULTIMODAL TASKS</p>
<p>Question 5: Are current prompt compression approaches generally effective when applied to MLLMs for multimodal tasks?</p>
<p>Table 6: Performance of prompt compression methods on VQA tasks.We selected 500 samples each from IconQA-txt, IconQA-blank, and OK-VQA for evaluation.For each setting, we averaged the scores between GPT-4o-mini and Claude-3-Haiku.Since all prompt compression methods are designed and trained based on text-only tasks, their applicability to multimodal tasks remains to be explored.Table 6 provides an extensive evaluation of different prompt compression methods when applied to VQA tasks.We observe the following:</p>
<p>• SCRL, Selective Context, and LLMLingua-2 exhibit varied performance across different datasets.This inconsistency is likely due to differences in question complexity and required reasoning capabilities inherent to the datasets.</p>
<p>• LLMLingua and LongLLMLingua maintain stable but suboptimal performance across datasets.Their generalized design may lack the necessary adaptations for excelling in multimodal tasks, suggesting a need for further optimization.</p>
<p>ANALYSIS ON WORD OMISSION</p>
<p>Question 6: What kind of words can be omitted when prompting?</p>
<p>Figure 8 shows the most frequently omitted words across various prompt compression methods, while Figure 9 depicts the performance impact of removing these words on QA tasks.Although the thorough removal of words like "the" has almost no impact, we have observed some noteworthy phenomena:</p>
<p>• Removing the same word has a larger impact on performance in long-context tasks.This can be attributed to the need for clarity and coherence when dealing with larger amounts of information.In longer contexts, these words may help maintain structure and meaning, preventing confusion and loss of detail.• Even words that seem less informative can play notable roles in maintaining the effectiveness of prompts.For instance, in English, the plurality of nouns can be indicated directly on the nouns themselves, and the word "a" seems to convey limited information.However, its removal has an adverse effect on performance.This phenomenon might be analogous to observations in vision transformers (ViTs) (Darcet et al., 2024): ViTs produce high-norm tokens in low-informative areas (such as background regions) during inference.These tokens are used to store and manage intermediate data in computational processes.We speculate that a similar mechanism may exist in LLMs, where tokens for less informative words could serve as registers that facilitate intermediate computations.</p>
<p>CONCLUSION AND LIMITATIONS</p>
<p>We conducted a comprehensive study on different prompt compression methods for LLMs across various tasks.Our results demonstrated that (Long)LLMLingua and LLMLingua-2 generally give the best performance, particularly at higher compression ratios.All methods appeared to increase hallucinations, primarily due to information loss.Additionally, current methods showed varied effectiveness in multimodal tasks, suggesting the need for further optimization.Finally, we analyzed the words that can be omitted during compression.Our study provided a broader understanding of prompt compression, assisting future research in prompt engineering strategies.</p>
<p>Limitations.In this empirical study, we focused on the prompt compression techniques only, conducting experiments with three (M)LLMs: GPT-3.5-turbo,GPT-4o-mini, and Claude-3-Haiku.In terms of the compression methods for open-source models, there are approaches on modifying internal states or KV cache information for compressing or trimming (Liu et al., 2023b;Zhang et al., 2023;Xiao et al., 2024;Ge et al., 2024).We leave the further study to our future work.The compressors module encompasses prompt compression methods that can be accessed through a unified interface with customizable parameters.The datasets module includes diverse datasets.The metrics module comprises primary metrics utilized for evaluating the performance of compressors.The runner module offers a generalized interface for executing evaluations or simply retrieving the compressed prompt generated by the compressors.</p>
<p>B CASE STUDY ON THE EFFECTS OF PROMPT COMPRESSION ON RESPONSE LENGTH</p>
<p>We use two examples (Figure 11 and 12) to illustrate the effects of prompt compression on response length.For GPT-3.5-turbo and GPT-4o-mini, the compressed prompt leads to a more detailed and elaborative response, adding context and clarification likely to compensate for the information loss due to the compressed input.On the other hand, Claude-3-Haiku's response to the compressed prompt tends to be shorter and more concise, focusing on summarizing the main points without delving into extensive detail.</p>
<p>However, it is crucial to note that the length variation patterns mentioned in Section 5.2 are statistical and may vary in individual cases.Specific prompt content, the pattern of compression, and the exact wording can all influence the responses.Our future work may delve into the underlying mechanisms driving these differences and provide further insights.</p>
<p>C PCTOOLKIT: A UNIFIED PLUG-AND-PLAY PROMPT COMPRESSION TOOLKIT</p>
<p>Various toolkits exist for prompt engineering and optimization, such as Promptify (Pal, 2022), ChainForge (Arawjo et al., 2023), Promptotype6 , and OpenPrompt (Ding et al., 2022).Despite the availability of these toolkits, a toolkit specifically focusing on prompt compression remains absent.Thus, with the aim of providing plug-and-play services, easy-customized interfaces and supporting common datasets and metrics, we have released PCToolkit7 , a unified plug-and-play toolkit for prompt compression of LLMs, making accessible and portable prompt compression methods to a wider audience.Our plug-and-play design enables users to deploy and use the toolkit without any further model trainings.</p>
<p>Figure 10 illustrates the architecture of PCToolkit.Key features of PCToolkit include: (i) Reproducible methods.PCToolkit offers a unified interface for six different compressors: KiS (Laban et al., 2021), SCRL (Ghalandari et al., 2022), Selective Context (Li et al., 2023), LLMLingua (Jiang et al., 2023), LongLLMLingua (Jiang et al., 2024), and LLMLingua-2 (Pan et al., 2024).(ii) Modular design.Featuring a modular structure that simplifies the transition between different methods, datasets, and metrics, PCToolkit is organized into four distinct modules: Compressors, Datasets, Metrics and Runner.(iii) User-friendly interface.Facilitating portability and ease of adaptation to different environments, the interfaces within PCToolkit are designed to be easily customizable.</p>
<p>C.1 MODULAR DESIGN</p>
<p>As shown in Figure 10, PCToolkit is designed with a modular architecture, consisting of Compressors, Datasets, Metrics and Runner.</p>
<p>Compressors.pctoolkit.compressorsmodule encompasses six compression methods tailored for prompt optimization.All compressors can be invoked through a unified interface shown in Section C.2.</p>
<p>Datasets.pctoolkit.datasetsmodule includes a diverse collection of datasets, each curated to cover a wide array of natural language tasks.From tasks like reconstruction, summarization, question answering, to more specialized domains such as code completion and lies recognition, PCToolkit offers a comprehensive testing ground for assessing prompt compression techniques.</p>
<p>Metrics.pctoolkit.metricsmodule quantifies the performance of the compression methods across different tasks.All necessary metrics can be easily organized into a list, which instructs the Runner on what to measure.</p>
<p>Runners.pctoolkit.runnersmodule serves as the engine that drives the evaluation process.Users can seamlessly execute experiments, compare results, and analyze the performance of different compression techniques using the Runner component.</p>
<p>C.2 UNIFIED INTERFACE</p>
<p>In PCToolkit, a unified interface for invoking prompt compression methods is provided.In the following example, we show how to simply invoke the compressing methods within few lines.For simple compression task, one compressor is selected.Following the example given above, the original prompt is input to the compressor, and the compressor outputs the compressed prompt.</p>
<p>For datasets evaluation, one datasets and multiple metrics are selected, along with the compressor chosen, these three parts are deployed in Runner.The Runner will provide the evaluation results according to the metrics list.The following example shows how to use PCToolkit to evaluate a dataset.</p>
<p>from pctoolkit.runnersimport run from pctoolkit.datasetsimport load_dataset from pctoolkit.metricsimport load_metrics compressor = PromptCompressor( type='SCCompressor', device='cuda') dataset_name = 'arxiv' dataset = load_dataset(dataset_name) run (compressor=compressor, dataset=dataset, metrics=load_metrics, ratio=0.5)Currently, the supporting dataset calls are implemented inside run.Users can also following the format in run to adapt their own datasets or metrics.</p>
<p>Figure 1 :
1
Figure 1: Illustration of prompt compression.The original context is distilled into a more concise form while preserving pertinent information for LLMs to process.Some methods compress the context based on the query, while others do not.Words that are underlined in the original text denote the segments that are trimmed by the compressor.</p>
<p>Figure 2 :
2
Figure 2: Categories of prompt compression methods.These methods can be grouped into three main categories: (a) RL-based methods, which use heuristic rewards to optimize the compressor, (b) LLM scoring-based methods, which use another language model to score each token in a single autoregressive step and decide to keep or discard each token based on its score, and (c) LLM annotation-based methods, which use LLMs to annotate data for training a small model specifically designed for prompt compression.</p>
<p>Figure 4 :
4
Figure4: Performance on different QA categories.We categorized the QA tasks into four categories: logic (Boolean Expression, Web of Lies), common sense (Causal Judgement), math (GSM8K), and long context (LongBench), and calculated the average performance of six prompt compression methods on these four categories.Considering the different metrics, we scaled the results based on the mean performance for each task.</p>
<p>MiHR (↓) MaHR (↓) MiHR (↓) MaHR (↓) MiHR (↓) MaHR (↓) MiHR (↓) MaHR (↓) MiHR (↓)</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: The types of hallucinations caused by prompt compression.We categorized the hallucinations induced by prompt compression into two types: (a) Altered Semantic Hallucination (ASH), which arises from incorrect compression that alters the original text's meaning, and (b) Information Loss Hallucination (ILH), which stems from the loss of information and incomplete sentence structures.</p>
<p>Figure 8 :
8
Figure 8: Word omitted across prompt compression methods.(a) Frequency of the top 10 omitted words across all prompt compression methods.(b) Proportion of these words in the original text, regardless of whether they were omitted.</p>
<p>Figure 9 :
9
Figure 9: Impact of word removal on performance.We randomly sampled 500 instances each from short context QA and long context QA to evaluate the impact of removing individual words.Each result is normalized by dividing by the score of the original prompt to obtain percentages.</p>
<p>Figure 10 :
10
Figure10: Architecture of PCToolkit.The compressors module encompasses prompt compression methods that can be accessed through a unified interface with customizable parameters.The datasets module includes diverse datasets.The metrics module comprises primary metrics utilized for evaluating the performance of compressors.The runner module offers a generalized interface for executing evaluations or simply retrieving the compressed prompt generated by the compressors.</p>
<p>Table 2 :
2
Performance for QA tasks.For each setting, we averaged the scores across three models: GPT-3.5-turbo,GPT-4o-mini, and Claude-3-Haiku.
MethodMetricReconstruction GSM8K BBC News ShareGPT Arxiv Gigaword DUC2004 BNC Broadcast Google SummarizationRandom Selection0.400.230.190.070.250.210.210.100.23KiS0.580.160.110.050.200.230.550.490.36SCRL Selective ContextBLEU (↑)0.38 0.580.14 0.330.30 0.350.10 0.310.28 0.260.28 0.250.57 0.560.47 0.500.46 0.47(Long)LLMLingua0.760.190.260.150.220.230.660.730.42LLMLingua-20.550.280.260.450.290.340.460.610.48Random Selection0.600.430.540.220.210.150.320.440.37KiS0.750.320.380.130.200.180.630.610.49SCRL Selective ContextROUGE L (↑)0.55 0.820.30 0.670.61 0.660.28 0.560.22 0.210.15 0.150.44 0.590.43 0.590.40 0.54(Long)LLMLingua0.890.520.580.450.230.190.800.880.56LLMLingua-20.860.470.480.350.260.210.640.570.53Random Selection0.930.850.870.810.830.840.840.820.85KiS0.940.880.860.820.850.870.920.910.92SCRL Selective ContextBERTScore (↑)0.69 0.960.84 0.900.89 0.910.85 0.920.84 0.850.84 0.850.85 0.890.82 0.890.88 0.89(Long)LLMLingua0.980.870.900.890.850.870.950.960.93LLMLingua-20.940.850.860.840.860.850.900.910.90BBH BooleanBBH CausalBBHGSM8KLongBenchLongBenchLongBenchLongBenchMethodExpressionJudgementWeb of LiesMathSingleDocMultiDocFewShotSynth.Acc. (↑)Acc. (↑)Acc. (↑)Acc. (↑)F1 (↑)F1 (↑)Acc. (↑)Acc. (↑)Original Prompt0.5160.6480.5560.3370.1490.0950.3340.174Random Selection0.4680.5560.5320.0300.1460.1080.3560.192KiS0.5760.4800.5120.1490.1180.0920.3120.166SCRL0.4640.4720.5560.2180.2140.3020.3780.176Selective Context0.4800.6160.5520.1790.1850.1010.4120.288LLMLingua0.5280.5040.5360.2970.2860.3190.6200.128LongLLMLingua0.5240.5360.4920.2180.3010.3340.6400.582LLMLingua-20.4840.5840.5200.2200.2230.3120.6320.210</p>
<p>Table 3 :
3
Computational overhead for different prompt compression methods."Time per token" refers to the time taken divided by the number of tokens removed.All metrics are evaluated on a single A6000 GPU with 48 GB memory.
MethodTime per Prompt (ms)Time per Token (ms)Memory (MB)KiS24105.031378SCRL670.15315Selective Context3190.69487LLMLingua1800.395309LongLLMLingua1840.405309LLMLingua-21150.252137
GSM8K) to longer ones (LongBench).Furthermore, we assess the computational overhead of these methods, as provided in Table3, to determine their practicality concerning time cost and memory consumption.</p>
<p>Table 3 ,
3
SCRL achieves the lowest time cost and minimal memory consumption.This makes it a practical choice for real-world applications where computational resources are limited.
Average Performance0.2 0.3 0.4 0.5 0.6 0.7 0.8Logic Common Sense Math Long Context0.10.0KiSSCRL Selective ContextLLM-LinguaLongLLM-LinguaLLM-Lingua-2</p>
<p>Table 4 :
4
LLM response length for different prompt compression methods.We recorded the number of words in the responses of three LLMs on 1000 QA tasks using different prompt compression methods."Average" indicates the average response length for all prompt compression methods.Numbers in parentheses show the difference compared to the original prompt.
Average Performance0.3 0.4 0.5 0.60.1 0.3 0.5 0.7 0.9 Compression Ratio Random Selection Selective Context LLMLingua LongLLMLingua LLMLingua-2Average Performance0.1 0.2 0.3 0.4Compression Ratio 0.1 0.3 0.5 0.7 0.9 Random Selection Selective Context LLMLingua LongLLMLingua LLMLingua-2(a) Short Context(b) Long ContextFigure 5: Performance on QA tasks under different compression ratios. The tasks are categorized into short context and long context. Considering the different metrics, we scaled theGPT-3.5-turbo 56.8 Random Selection 60.1 (+3.3) 78.0 (+3.1) 121.6 (−3.0) GPT-4o-Claude-3-Method mini Haiku Original Prompt 74.9 124.6 KiS 58.4 (+1.6) 76.4 (+1.4) 122.1 (−2.5)results based on the mean performance for eachSCRL57.4 (+0.6) 75.6 (+0.6) 121.0 (−3.7)task before averaging.Selective Context LLMLingua58.1 (+1.3) 76.0 (+1.1) 122.4 (−2.2) 57.1 (+0.3) 75.2 (+0.3) 121.8 (−2.8)LongLLMLingua57.7 (+0.9) 75.8 (+0.9) 122.2 (−2.4)LLMLingua-257.2 (+0.4) 75.4 (+0.5) 121.5 (−3.1)Average58.0 (+1.2) 76.0 (+1.1) 121.8 (−2.8)
We also categorize mathematical problems and multiple-choice questions under the scope of QA.
We utilized GSM8K in both reconstruction and QA tasks. For the former, we only evaluate the performance of reconstruction without providing answers.
https://github.com/nltk/nltk
https://github.com/pltrdy/rouge
https://github.com/Tiiiger/bert_score
https://www.promptotype.io
https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression
ACKNOWLEDGMENTSThis research is supported by SMP-IDATA Open Youth Fund, Guangzhou-HKUST(GZ) Joint Funding Program (Grant No.2023A03J0008), the Guangzhou Municipal Science and Technology Project (No. 2025A04J4070), and Education Bureau of Guangzhou Municipality.A IMPLEMENTATION DETAILSA.1 DATASETS In this section, we provide detailed descriptions of the datasets used in our study.GSM8K.GSM8K(Cobbe et al., 2021)contains 8.5K linguistically diverse word problems in elementary school mathematics.Each item contains a problem and its solution.BBC News, Arxiv articles and ShareGPT.Li et al. (2023)provided the three datasets.BBC News provides news articles from BBC, which is a typical context of human daily lives.Arxiv articles provides scientific articles that represents a formal context.ShareGPT contains contexts that is collected from human-AI conversations, which is a normal communication context.Big Bench Hard (BBH).BBH(Suzgun et al., 2023)is a diverse evaluation suite that focuses on a suite of 23 challenging tasks from BIG-Bench that were found to be beyond the capabilities of current language models.LongBench.LongBench(Bai et al., 2024)is a benchmark for bilingual, multitask and comprehensive assessment of long context understanding capabilities of LLMs.LongBench has six different task scenarios including single-document QA, multi-document QA, summarization, few-shot learning, synthetic tasks and code completion.Gigaword, BNC, DUC2004, Broadcast and Google.Ghalandari et al. (2022)provided the five datasets.While Gigaword(Rush et al., 2015)and DUC2004(Over et al., 2007)contain abstractive ground truth summaries, the remaining three datasets(Filippova &amp; Altun, 2013;Clarke &amp; Lapata, 2008b) have token-level extractive ground truth summaries.IconQA.IconQA(Lu et al., 2021)consists of 107,439 VQA questions and includes three subtasks: multi-image-choice, multi-text-choice, and filling-in-the-blank. IconQA is inspired by realworld diagram word problems, emphasizing the importance of abstract diagram understanding and comprehensive cognitive reasoning.OK-VQA.OK-VQA(Marino et al., 2019)is a benchmark for knowledge-based VQA consisting of over 14,000 questions.The image content in this dataset is not sufficient to answer the questions, which encourages the utilization of external knowledge resources.A.2 METRICSIn this section, we outline the evaluation metrics used in our study.Papineni et al. (2002a), Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate machine-translated text by comparing it to reference translations.The BLEU score is computed as follows:BLEU. Proposed byHere c is the length of the candidate translation, r is the length of the reference translation, p n is the precision of n-grams, and w n are weights assigned to each p n .ROUGE.Proposed byLin (2004a), Recall-Oriented Understudy for Gisting Evaluation (ROUGE) encompasses several variants including ROUGE-N, ROUGE-L, ROUGE-W and ROUGE-S.These metrics are used for evaluating the quality of summaries produced by automatic summarization systems.In our experiments, we specifically use ROUGE-L, which measures the longest common subsequence (LCS) between the reference and candidate summaries.The formula for ROUGE-L is defined as:Here LCS(r, c) is the length of LCS between the reference r and candidate c, and |r| and |c| denotes the length of r and c, respectively.In our experiments, we use the F1 score as the ROUGE-L value.BERTScore.Proposed byZhang* et al. (2020), BERTScore evaluates text similarity using contextual embeddings from BERT(Devlin et al., 2019).The formula for BERTScore can be defined as follows:Here c i and r j are the i-th tokens of c and r, and sim denotes cosine similarity between embeddings of c i and r j .In our experiments, we use the F1 score as the BERTScore value.F1.FollowingBai et al. (2024), we utilize the F1 score to measure the similarity between the predicted output and the ground truth by considering common elements between the two.Specifically, this F1 score calculation accounts for the overlap at the character or token level between the predicted and reference texts, which is different from the F1 scores used in other metrics like ROUGE-L or BERTScore.The formula adapted for F1 score is:Here common tokens denotes the set of tokens that appear in both the predicted text and the reference text.Micro Hallucination Rate (MiHR).FollowingLi et al. (2024), MiHR measures the proportion of hallucinatory statements within each response.It is calculated as:Here n is the total number of samples in every domain and r i is the i-th response.Macro Hallucination Rate (MaHR).Also followingLi et al. (2024), MaHR calculates the proportion of responses containing hallucinatory statements.It is computed as:Here n represents the total number of samples.Based on the information provided in the scenario, we can conclude that Daniel did contribute to causing the computer crash, but he was not solely responsible for it.Here's why: 1. Claire warned Daniel that if both of them were logged on simultaneously, the computer would crash.2. The repairman confirmed that the computer will only crash if two people are logged on at the same time.3. The repairman saw that Daniel was logged on.4. The computer did crash.5. Given points 2 and 3, for the crash to occur, someone else must have been logged on at the same time as Daniel.6. Based on Claire's warning and her use of the computer for schoolwork, it's reasonable to assume that Claire was the other person logged on.So, while Daniel's actions contributed to the crash by logging on when he was told not to, the crash was actually caused by both Daniel and Claire being logged on simultaneously.Daniel's action was a necessary condition for the crash, but not a sufficient condition on its own.In this case, it would be more accurate to say that both Daniel and Claire caused the computer crash together, rather than attributing the cause solely to Daniel.(Word count: 206) Response to Compressed Prompt (Claude-3-Haiku): I apologize, but the text you provided is incomplete and contains several grammatical errors, making it difficult to understand the full context.I'll try to piece together the information and answer your question based on what I can interpret: It seems that Claire's parents bought an old computer, which Claire uses for schoolwork and her brother Daniel sometimes uses for gaming.Claire warned Daniel not to log on at the same time as her because it would cause the computer to crash.Despite this warning, Claire and Daniel logged on simultaneously, and the computer crashed.Now, their mother is talking to a repairman, who confirms that the computer will only crash if two users are logged on at the same time.Given this information, it's not accurate to say that Daniel alone caused the computer crash.Both Claire and Daniel are responsible for the crash because they both logged on at the same time, despite knowing that it would cause problems.The crash was a result of their combined actions, not just Daniel's.In this situation, both siblings share the responsibility for not following the instructions they were given about using the computer.(Word count: 193) Figure11: An example on the effect of prompt compression on the response length of LLMs.The text in pink represent the words that are trimmed.Original Prompt: Suzy and Billy are working on a project that is very important for our nation's security.The boss tells Suzy: "Be sure that you are here at exactly 9 am.It is absolutely essential that you arrive at that time."Then he tells Billy: "Be sure that you do not come in at all tomorrow morning.It is absolutely essential that you not appear at that time."Both Billy and Suzy arrive at 9 am.As it happens, there was a motion detector installed in the room where they arrived.The motion detector was set up to be triggered if at least one person appeared in the room at the same time.So the motion detector went off.Did Billy cause the motion detector to go off?Compressed Prompt: Suzy and Billy are working on a project that is very important for nation security.The boss tells Suzy "Be that here at exactly 9 am It is essential that arrive that time."tells Billy "Be that not come in tomorrow morning.It is that not that time."Billy and Suzy arrive at 9 am.there was a motion detector installed in the room arrived.The motion was triggered if one person in the room the same time.the motion detector off.Did Billy cause the motion detector to go off?Response to Original Prompt (GPT-4o-mini):Yes, Billy caused the motion detector to go off.The motion detector was triggered by the presence of at least one person in the room.Since Billy was explicitly instructed not to appear at that time and still arrived, his presence contributed to triggering the motion detector.If only Suzy had arrived, the detector would have gone off due to her presence, but Billy's presence was a direct violation of his instructions and thus contributed to setting off the detector.(Word count: 80) Response to Compressed Prompt (GPT-4o-mini):The motion detector is triggered if one person is in the room at the same time.Since the boss told Billy not to come in tomorrow morning, he was not supposed to arrive.However, the information states that both Suzy and Billy arrived at 9 am.If Billy did indeed arrive at 9 am, then he was in the room at the same time as Suzy.Since the motion detector goes off when one person is in the room at the same time, it means that the motion detector was triggered by their simultaneous presence.So, yes, Billy caused the motion detector to go off by being in the room at the same time as Suzy.(Word count: 116) Response to Original Prompt (Claude-3-Haiku): This is an interesting scenario that touches on concepts of causality and counterfactuals.To answer this question, we need to consider a few key points: 1.The motion detector is triggered if at least one person appears in the room.2. Both Suzy and Billy arrived at 9 am, contrary to Billy's instructions.3. Suzy was supposed to be there at 9 am.Given these facts, we can reason as follows:1.The motion detector would have gone off even if Billy hadn't shown up, because Suzy was supposed to be there and did arrive as instructed.2. Billy's presence was neither necessary nor sufficient for the motion detector to go off.It would have been triggered by Suzy alone.3. In causal analysis, we often consider counterfactuals -what would have happened if the supposed cause (Billy's arrival) hadn't occurred.In this case, the outcome (motion detector going off) would have been the same.Therefore, it would be incorrect to say that Billy caused the motion detector to go off.While his presence contributed to the triggering of the detector, he was not the cause of it going off.The detector would have been triggered regardless of his presence due to Suzy's arrival.In conclusion, Billy did not cause the motion detector to go off.The cause was Suzy's arrival, which was both expected and sufficient to trigger the detector.(Word count: 231) Response to Compressed Prompt (Claude-3-Haiku): I apologize, but there are several issues with the text you provided that make it difficult to understand and answer the question accurately.Let me try to clarify the situation based on the information given and then answer the question:1. Suzy was told to be there at exactly 9 am, and it was essential for her to arrive at that time.2. Billy was told not to come in the next morning and not to be there at that time.3. Both Billy and Suzy arrived at 9 am. 4.There was a motion detector installed in the room.5.The motion detector was triggered if one person was in the room at the same time.Given this information, we can conclude: Billy was not supposed to be there, but he came anyway.Since both Billy and Suzy arrived at 9 am, they both entered the room at the same time.The motion detector would have been triggered regardless of who entered first or if they entered simultaneously.So, to answer the question: Billy did cause the motion detector to go off, but so did Suzy.Both of them contributed to triggering the motion detector by entering the room at 9 am.It's worth noting that if Billy had followed instructions and not come in, only Suzy would have triggered the motion detector.(Word count: 224) Figure12: An example on the effect of prompt compression on the response length of LLMs.The text in pink represent the words that are trimmed.
Chainforge: A visual toolkit for prompt engineering and llm hypothesis testing. Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, Elena L Glassman, ArXiv, abs/2309.091282023</p>
<p>LongBench: A bilingual, multitask benchmark for long context understanding. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li, 10.18653/v1/2024.acl-long.172Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>A simple and effective positional encoding for transformers. Pu-Chin Chen, Henry Tsai, Srinadh Bhojanapalli, Hyung Won Chung, Yin-Wen Chang, Chun-Sung Ferng, 10.18653/v1/2021.emnlp-main.236Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicNovember 2021Online and Punta CanaAssociation for Computational Linguistics</p>
<p>Global inference for sentence compression : an integer linear programming approach. James Clarke, Mirella Lapata, J. Artif. Intell. Res. 3130044472008a</p>
<p>Global inference for sentence compression : an integer linear programming approach. James Clarke, Mirella Lapata, J. Artif. Intell. Res. 3130044472008b</p>
<p>British national corpus, XML edition. Karl Cobbe, BNC ConsortiumVineet Kosaraju, BNC ConsortiumMohammad Bavarian, BNC ConsortiumMark Chen, BNC ConsortiumHeewoo Jun, BNC ConsortiumLukasz Kaiser, BNC ConsortiumMatthias Plappert, BNC ConsortiumJerry Tworek, BNC ConsortiumJacob Hilton, BNC ConsortiumReiichiro Nakano, BNC ConsortiumChristopher Hesse, BNC ConsortiumJohn Schulman, BNC ConsortiumarXiv:2110.14168Training verifiers to solve math word problems. 2021. 2007arXiv preprint</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, Christopher Ré, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Vision transformers need registers. Timothée Darcet, Maxime Oquab, Julien Mairal, Piotr Bojanowski, The Twelfth International Conference on Learning Representations. 2024</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational LinguisticsJune 20191</p>
<p>OpenPrompt: An open-source framework for prompt-learning. Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Haitao Zheng, Maosong Sun, 10.18653/v1/2022.acl-demo.10Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. Zornitsa Valerio Basile, Sanja Kozareva, Stajner, the 60th Annual Meeting of the Association for Computational Linguistics: System DemonstrationsDublin, IrelandAssociation for Computational LinguisticsMay 2022</p>
<p>A survey on incontext learning. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, Baobao Chang, Xu Sun, Lei Li, Zhifang Sui, 2024</p>
<p>Overcoming the lack of parallel data in sentence compression. Katja Filippova, Yasemin Altun, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. David Yarowsky, Timothy Baldwin, Anna Korhonen, Karen Livescu, Steven Bethard, the 2013 Conference on Empirical Methods in Natural Language ProcessingSeattle, Washington, USAAssociation for Computational LinguisticsOctober 2013</p>
<p>In-context autoencoder for context compression in a large language model. Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, Furu Wei, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Efficient unsupervised sentence compression by fine-tuning transformers with reinforcement learning. Demian Ghalandari, Chris Hokamp, Georgiana Ifrim, 10.18653/v1/2022.acl-long.90Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. Preslav Smaranda Muresan, Aline Nakov, Villavicencio, the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>Studying large language model generalization with influence functions. Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamilė Lukošiūtė, Karina Nguyen, Nicholas Joseph, Sam Mccandlish, Jared Kaplan, Samuel R Bowman, 2023</p>
<p>Efficiently modeling long sequences with structured state spaces. Albert Gu, Karan Goel, Christopher R'e, ArXiv, abs/2111.003962021</p>
<p>The false promise of imitating proprietary language models. Arnav Gudibande, Eric Wallace, Charlie Victor Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, Dawn Song, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Comput. Surv. 0360-03005512March 2023</p>
<p>Llmlingua: Compressing prompts for accelerated inference of large language models. Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>LongLLMLingua: Accelerating and enhancing LLMs in long context scenarios via prompt compression. Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu, 10.18653/v1/2024.acl-long.91Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>Keep it simple: Unsupervised simplification of multi-paragraph text. Philippe Laban, Tobias Schnabel, Paul Bennett, Marti A Hearst, 10.18653/v1/2021.acl-long.498Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnlineAugust 20211Association for Computational Linguistics</p>
<p>Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok Park, Owq, arXiv:2306.02272Lessons learned from activation outliers for weight quantization in large language models. 2023arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS '20. the 34th International Conference on Neural Information Processing Systems, NIPS '20Red Hook, NY, USACurran Associates Inc2020ISBN 9781713829546</p>
<p>The dawn after the dark: An empirical study on factuality hallucination in large language models. Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, Ji-Rong Wen, 10.18653/v1/2024.acl-long.586Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. Lun-Wei Ku, Andre Martins, Vivek Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAugust 20241</p>
<p>Compressing context to enhance inference efficiency of large language models. Yucheng Li, Bo Dong, Chenghua Lin, Frank Guerin, Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Annual Meeting of the Association for Computational Linguistics. 2004a964287</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Association for Computational Linguistics. Barcelona, SpainJuly 2004bText Summarization Branches Out</p>
<p>Blockwise parallel transformers for large context models. Hao Liu, Pieter Abbeel, Advances in Neural Information Processing Systems. A Oh, T Neumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, Transactions of the Association for Computational Linguistics. 122593606652023a</p>
<p>Scissorhands: Exploiting the persistence of importance hypothesis for LLM KV cache compression at test time. Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, Anshumali Shrivastava, Thirty-seventh Conference on Neural Information Processing Systems. 2023b</p>
<p>Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, Song-Chun Zhu, The 35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks. 2021</p>
<p>Llm-pruner: On the structural pruning of large language models. Xinyin Ma, Gongfan Fang, Xinchao Wang, A. Oh, T. Neumann, A. Globerson, K. Saenko</p>
<p>Advances in Neural Information Processing Systems. M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Ok-vqa: A visual question answering benchmark requiring external knowledge. Kenneth Marino, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Conference on Computer Vision and Pattern Recognition (CVPR). 2019</p>
<p>Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, ArXiv, abs/2303.063492023</p>
<p>. Paul Over, Hoa Dang, Donna Harman, 10.1016/j.ipm.2007.01.019Duc in context. Inf. Process. Manage. 0306-4573436November 2007</p>
<p>Ankit Pal, Promptify: Structured output from llms. 2022Prompt-Engineering components for NLP tasks in Python</p>
<p>LLMLingua-2: Data distillation for efficient and faithful task-agnostic prompt compression. Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin, Victor Rühle, Yuqing Yang, Chin-Yew Lin, H Vicky, Lili Zhao, Dongmei Qiu, Zhang, 10.18653/v1/2024.findings-acl.57Findings of the Association for Computational Linguistics ACL 2024. Lun-Wei Ku, Andre Martins, Vivek Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 2024and virtual meeting</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Pierre Isabelle, Eugene Charniak, Dekang Lin, the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAJuly 2002aAssociation for Computational Linguistics</p>
<p>Bleu: A method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL '02. the 40th Annual Meeting on Association for Computational Linguistics, ACL '02USAAssociation for Computational Linguistics2002b</p>
<p>A neural attention model for abstractive sentence summarization. Alexander M Rush, Sumit Chopra, Jason Weston, 10.18653/v1/D15-1044Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Lluís Màrquez, Chris Callison-Burch, Jian Su, the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsSeptember 2015</p>
<p>Self-attention with relative position representations. Peter Shaw, Jakob Uszkoreit, Ashish Vaswani, North American Chapter. the Association for Computational Linguistics2018</p>
<p>A long way to go: Investigating length correlations in RLHF. Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett, First Conference on Language Modeling. 2024</p>
<p>Challenging BIG-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, Jason Wei, 10.18653/v1/2023.findings-acl.824Findings of the Association for Computational Linguistics: ACL 2023. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, Toronto, CanadaJuly 2023Association for Computational Linguistics</p>
<p>Linformer: Selfattention with linear complexity. Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, Hao Ma, ArXiv, abs/2006.047682020</p>
<p>Beyond the limits: A survey of techniques to extend the context length in large language models. Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, Armaghan Eshaghi, 10.24963/ijcai.2024/917Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24. Kate Larson, the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-2482024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2024ISBN 9781713871088</p>
<p>Lightweight and efficient end-to-end speech recognition using low-rank transformer. Genta Indra Winata, Samuel Cahyawijaya, Zhaojiang Lin, Zihan Liu, Pascale Fung, ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 2019</p>
<p>Efficient streaming language models with attention sinks. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Unveiling the generalization power of fine-tuned large language models. Haoran Yang, Yumeng Zhang, Jiaqi Xu, Hongyuan Lu, Pheng-Ann Heng, Wai Lam, 10.18653/v1/2024.naacl-long.51Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. Kevin Duh, Helena Gomez, Steven Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20241</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>H2o: Heavyhitter oracle for efficient generative inference of large language models. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, Beidi Chen, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Response length perception and sequence scheduling: An LLM-empowered LLM inference pipeline. Zangwei Zheng, Xiaozhe Ren, Fuzhao Xue, Yang Luo, Xin Jiang, Yang You, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3462 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3462</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3462</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-80.html">extraction-schema-80</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <p><strong>Paper ID:</strong> paper-16066021</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1602.05765v2.pdf" target="_blank">Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Conceptual spaces are geometric representations of conceptual knowledge, in which entities correspond to points, natural properties correspond to convex regions, and the dimensions of the space correspond to salient features. While conceptual spaces enable elegant models of various cognitive phenomena, the lack of automated methods for constructing such representations have so far limited their application in artificial intelligence. To address this issue, we propose a method which learns a vector-space embedding of entities from Wikipedia and constrains this embedding such that entities of the same semantic type are located in some lower-dimensional subspace. We experimentally demonstrate the usefulness of these subspaces as (approximate) conceptual space representations by showing, among others, that important features can be modelled as directions and that natural properties tend to correspond to convex regions.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3462.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3462.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conceptual Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual spaces (Gärdenfors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A geometric, functional-level theory in which concepts are represented as regions in a metric (Euclidean) space whose dimensions correspond to salient features; entities are points and natural properties correspond to convex regions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conceptual Spaces: The Geometry of Thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Conceptual spaces</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are encoded as regions (typically convex) in a domain-specific Euclidean space; entities (actual or possible) are points in that space, and the axes/dimensions correspond to salient, interpretable features. Similarity is modelled by metric distance; typicality by proximity to prototypical points (centroids).</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Paper cites that conceptual spaces can naturally model vagueness and induction and that many cognitive phenomena are captured (e.g., modelling of context effects by rescaling dimensions). In this work, learned subspaces approximate conceptual spaces: the authors show numerical attributes can be modelled as directions and that natural properties tend to correspond to convex regions (ranking and induction experiments showing directions and centroid-based induction).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>A major practical challenge noted is the lack of automated methods for constructing such representations across many domains; existing approaches (e.g. MDS-based) do not scale or require domain-specific metric information. The paper also notes that naïve word-embedding approaches produce single global spaces whose dimensions are not interpretable as salient domain-specific features.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Contrasted directly with word embeddings: conceptual spaces are per-semantic-type subspaces with interpretable dimensions and a formal distinction between entities (points) and properties (regions), whereas word embeddings place all words in a single space with largely uninterpretable axes. Compared (empirically) to an MDS-based pipeline [6]; the paper argues their subspace-embedding approach better scales and aligns subspaces via relations.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to automatically derive appropriate domain-specific dimensions and subspaces at scale; how best to align subspaces for different semantic types; and how well convexity and prototypical-centroid assumptions hold across many real-world semantic types (addressed empirically but open in general). Theoretical questions remain about the appropriate dimensionality per type (paper uses nuclear-norm regularization to select dimensions).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning', 'publication_date_yy_mm': '2016-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3462.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3462.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototype theory (Rosch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cognitive-level theory proposing that natural categories are organized around prototypical (best) examples, with category membership graded by similarity to the prototype.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Natural categories</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented by prototypes (central tendencies); category membership and typicality are determined by distance from the prototype rather than strict necessary and sufficient conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Paper invokes prototype theory to justify the convex-region assumption for natural properties in conceptual spaces (i.e., natural properties correspond to convex regions with central prototypes), citing classic findings on natural categories and typicality.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Paper discusses borderline cases and vagueness (e.g., ice cream shop vs restaurant) that prototype-based views need to account for through graded membership; it also points out the practical difficulty of deriving such prototype/region-based representations automatically from large, heterogeneous corpora or knowledge bases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Prototype theory is presented as compatible with conceptual spaces (convex regions and prototypes), and contrasted implicitly with symbolic/logical representations that assume crisp category boundaries; no direct empirical comparison with exemplar theory is given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to recover prototypes and convex category boundaries automatically from large-scale text and knowledge graph data, and how to handle context-dependence and borderline cases computationally.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning', 'publication_date_yy_mm': '2016-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3462.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3462.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Distributed word embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distributed representations / word embeddings (e.g., Skip-gram, GloVe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Predictive and matrix-factorization based vector-space models that represent words as dense vectors in a single high-dimensional Euclidean space learned from word co-occurrence statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distributed representations of words and phrases and their compositionality</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Distributed vector representations (word embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Words are represented as points (vectors) in a single high-dimensional Euclidean space learned from distributional statistics; semantic relations are reflected as geometric regularities (e.g., vector offsets for analogies) and composition can be modelled by vector arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Paper cites empirical regularities: skip-gram and GloVe capture analogical proportions (parallelograms) and vector addition can reflect semantic composition; these properties motivate using word co-occurrence as proxies for salient features in the proposed entity embedding model.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>Paper argues these embeddings place all words in one undifferentiated space whose axes are essentially meaningless for domain-specific reasoning; they do not distinguish entities (points) from properties (regions) and thus are less suited as conceptual space approximations for domain-specific tasks (ranking, modelling properties as directions, modelling convex property regions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Directly contrasted with conceptual spaces: word embeddings are global and uninterpretable on a per-domain basis, while conceptual spaces are per-type, low-dimensional, and have interpretable axes; the paper builds on GloVe-like objectives but constrains entity vectors into type-specific subspaces.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to make embedding dimensions interpretable and domain-specific; how to separate representations of entities vs properties; and how to exploit relational knowledge to align domain subspaces rather than learning a single monolithic space.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning', 'publication_date_yy_mm': '2016-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3462.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3462.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of functional-level theories or models of how conceptual knowledge is represented in the brain, including descriptions of the representational format, supporting evidence, counter-evidence, and comparisons between theories.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Region / density embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Representing words as regions or densities (region/density-based embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variants of semantic representation that encode words or concepts as regions (sets) or probability densities in vector space rather than as single point vectors, aiming to capture uncertainty, polysemy, and context-dependent similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Representing words as regions in vector space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Region/density-based vector representations</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts/words are represented by regions (convex sets) or probability densities in vector space, allowing one representation to encode graded membership, uncertainty, and more complex internal structure than a single point.</td>
                        </tr>
                        <tr>
                            <td><strong>level_of_analysis</strong></td>
                            <td>functional</td>
                        </tr>
                        <tr>
                            <td><strong>supporting_evidence</strong></td>
                            <td>Paper references prior work (e.g., Erk; Vilnis & McCallum) showing that region/density representations can model context-dependent similarity and capture richer semantics than point embeddings; these approaches are presented as alternatives acknowledging context and polysemy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence_or_challenges</strong></td>
                            <td>The paper only briefly mentions these approaches; it does not provide detailed empirical comparisons here. Practical challenges include increased modelling complexity and evaluation; also the authors stress distinguishing entities (as points) from properties (as regions) as an advantage of conceptual spaces, implying region-based word models address different concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_theories</strong></td>
                            <td>Presented as alternative computational approaches to pure point-based word embeddings; region/density models align with the conceptual-spaces view that properties are regions, but those works typically focused on words rather than entity-vs-property distinctions emphasized in conceptual spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_limitations_or_open_questions</strong></td>
                            <td>How to integrate region/density-based representations with per-type conceptual subspaces and knowledge-graph alignment at large scale; how to evaluate convex-region hypotheses for properties when using region/density embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning', 'publication_date_yy_mm': '2016-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conceptual Spaces: The Geometry of Thought <em>(Rating: 2)</em></li>
                <li>Natural categories <em>(Rating: 2)</em></li>
                <li>Distributed representations of words and phrases and their compositionality <em>(Rating: 2)</em></li>
                <li>Glove: Global vectors for word representation <em>(Rating: 2)</em></li>
                <li>Representing words as regions in vector space <em>(Rating: 2)</em></li>
                <li>Word representations via Gaussian embedding <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3462",
    "paper_id": "paper-16066021",
    "extraction_schema_id": "extraction-schema-80",
    "extracted_data": [
        {
            "name_short": "Conceptual Spaces",
            "name_full": "Conceptual spaces (Gärdenfors)",
            "brief_description": "A geometric, functional-level theory in which concepts are represented as regions in a metric (Euclidean) space whose dimensions correspond to salient features; entities are points and natural properties correspond to convex regions.",
            "citation_title": "Conceptual Spaces: The Geometry of Thought",
            "mention_or_use": "use",
            "theory_name": "Conceptual spaces",
            "theory_description": "Concepts are encoded as regions (typically convex) in a domain-specific Euclidean space; entities (actual or possible) are points in that space, and the axes/dimensions correspond to salient, interpretable features. Similarity is modelled by metric distance; typicality by proximity to prototypical points (centroids).",
            "level_of_analysis": "functional",
            "supporting_evidence": "Paper cites that conceptual spaces can naturally model vagueness and induction and that many cognitive phenomena are captured (e.g., modelling of context effects by rescaling dimensions). In this work, learned subspaces approximate conceptual spaces: the authors show numerical attributes can be modelled as directions and that natural properties tend to correspond to convex regions (ranking and induction experiments showing directions and centroid-based induction).",
            "counter_evidence_or_challenges": "A major practical challenge noted is the lack of automated methods for constructing such representations across many domains; existing approaches (e.g. MDS-based) do not scale or require domain-specific metric information. The paper also notes that naïve word-embedding approaches produce single global spaces whose dimensions are not interpretable as salient domain-specific features.",
            "comparison_to_other_theories": "Contrasted directly with word embeddings: conceptual spaces are per-semantic-type subspaces with interpretable dimensions and a formal distinction between entities (points) and properties (regions), whereas word embeddings place all words in a single space with largely uninterpretable axes. Compared (empirically) to an MDS-based pipeline [6]; the paper argues their subspace-embedding approach better scales and aligns subspaces via relations.",
            "notable_limitations_or_open_questions": "How to automatically derive appropriate domain-specific dimensions and subspaces at scale; how best to align subspaces for different semantic types; and how well convexity and prototypical-centroid assumptions hold across many real-world semantic types (addressed empirically but open in general). Theoretical questions remain about the appropriate dimensionality per type (paper uses nuclear-norm regularization to select dimensions).",
            "uuid": "e3462.0",
            "source_info": {
                "paper_title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning",
                "publication_date_yy_mm": "2016-02"
            }
        },
        {
            "name_short": "Prototype theory",
            "name_full": "Prototype theory (Rosch)",
            "brief_description": "A cognitive-level theory proposing that natural categories are organized around prototypical (best) examples, with category membership graded by similarity to the prototype.",
            "citation_title": "Natural categories",
            "mention_or_use": "mention",
            "theory_name": "Prototype theory",
            "theory_description": "Concepts are represented by prototypes (central tendencies); category membership and typicality are determined by distance from the prototype rather than strict necessary and sufficient conditions.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Paper invokes prototype theory to justify the convex-region assumption for natural properties in conceptual spaces (i.e., natural properties correspond to convex regions with central prototypes), citing classic findings on natural categories and typicality.",
            "counter_evidence_or_challenges": "Paper discusses borderline cases and vagueness (e.g., ice cream shop vs restaurant) that prototype-based views need to account for through graded membership; it also points out the practical difficulty of deriving such prototype/region-based representations automatically from large, heterogeneous corpora or knowledge bases.",
            "comparison_to_other_theories": "Prototype theory is presented as compatible with conceptual spaces (convex regions and prototypes), and contrasted implicitly with symbolic/logical representations that assume crisp category boundaries; no direct empirical comparison with exemplar theory is given in this paper.",
            "notable_limitations_or_open_questions": "How to recover prototypes and convex category boundaries automatically from large-scale text and knowledge graph data, and how to handle context-dependence and borderline cases computationally.",
            "uuid": "e3462.1",
            "source_info": {
                "paper_title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning",
                "publication_date_yy_mm": "2016-02"
            }
        },
        {
            "name_short": "Distributed word embeddings",
            "name_full": "Distributed representations / word embeddings (e.g., Skip-gram, GloVe)",
            "brief_description": "Predictive and matrix-factorization based vector-space models that represent words as dense vectors in a single high-dimensional Euclidean space learned from word co-occurrence statistics.",
            "citation_title": "Distributed representations of words and phrases and their compositionality",
            "mention_or_use": "use",
            "theory_name": "Distributed vector representations (word embeddings)",
            "theory_description": "Words are represented as points (vectors) in a single high-dimensional Euclidean space learned from distributional statistics; semantic relations are reflected as geometric regularities (e.g., vector offsets for analogies) and composition can be modelled by vector arithmetic.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Paper cites empirical regularities: skip-gram and GloVe capture analogical proportions (parallelograms) and vector addition can reflect semantic composition; these properties motivate using word co-occurrence as proxies for salient features in the proposed entity embedding model.",
            "counter_evidence_or_challenges": "Paper argues these embeddings place all words in one undifferentiated space whose axes are essentially meaningless for domain-specific reasoning; they do not distinguish entities (points) from properties (regions) and thus are less suited as conceptual space approximations for domain-specific tasks (ranking, modelling properties as directions, modelling convex property regions).",
            "comparison_to_other_theories": "Directly contrasted with conceptual spaces: word embeddings are global and uninterpretable on a per-domain basis, while conceptual spaces are per-type, low-dimensional, and have interpretable axes; the paper builds on GloVe-like objectives but constrains entity vectors into type-specific subspaces.",
            "notable_limitations_or_open_questions": "How to make embedding dimensions interpretable and domain-specific; how to separate representations of entities vs properties; and how to exploit relational knowledge to align domain subspaces rather than learning a single monolithic space.",
            "uuid": "e3462.2",
            "source_info": {
                "paper_title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning",
                "publication_date_yy_mm": "2016-02"
            }
        },
        {
            "name_short": "Region / density embeddings",
            "name_full": "Representing words as regions or densities (region/density-based embeddings)",
            "brief_description": "Variants of semantic representation that encode words or concepts as regions (sets) or probability densities in vector space rather than as single point vectors, aiming to capture uncertainty, polysemy, and context-dependent similarity.",
            "citation_title": "Representing words as regions in vector space",
            "mention_or_use": "mention",
            "theory_name": "Region/density-based vector representations",
            "theory_description": "Concepts/words are represented by regions (convex sets) or probability densities in vector space, allowing one representation to encode graded membership, uncertainty, and more complex internal structure than a single point.",
            "level_of_analysis": "functional",
            "supporting_evidence": "Paper references prior work (e.g., Erk; Vilnis & McCallum) showing that region/density representations can model context-dependent similarity and capture richer semantics than point embeddings; these approaches are presented as alternatives acknowledging context and polysemy.",
            "counter_evidence_or_challenges": "The paper only briefly mentions these approaches; it does not provide detailed empirical comparisons here. Practical challenges include increased modelling complexity and evaluation; also the authors stress distinguishing entities (as points) from properties (as regions) as an advantage of conceptual spaces, implying region-based word models address different concerns.",
            "comparison_to_other_theories": "Presented as alternative computational approaches to pure point-based word embeddings; region/density models align with the conceptual-spaces view that properties are regions, but those works typically focused on words rather than entity-vs-property distinctions emphasized in conceptual spaces.",
            "notable_limitations_or_open_questions": "How to integrate region/density-based representations with per-type conceptual subspaces and knowledge-graph alignment at large scale; how to evaluate convex-region hypotheses for properties when using region/density embeddings.",
            "uuid": "e3462.3",
            "source_info": {
                "paper_title": "Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning",
                "publication_date_yy_mm": "2016-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conceptual Spaces: The Geometry of Thought",
            "rating": 2,
            "sanitized_title": "conceptual_spaces_the_geometry_of_thought"
        },
        {
            "paper_title": "Natural categories",
            "rating": 2,
            "sanitized_title": "natural_categories"
        },
        {
            "paper_title": "Distributed representations of words and phrases and their compositionality",
            "rating": 2,
            "sanitized_title": "distributed_representations_of_words_and_phrases_and_their_compositionality"
        },
        {
            "paper_title": "Glove: Global vectors for word representation",
            "rating": 2,
            "sanitized_title": "glove_global_vectors_for_word_representation"
        },
        {
            "paper_title": "Representing words as regions in vector space",
            "rating": 2,
            "sanitized_title": "representing_words_as_regions_in_vector_space"
        },
        {
            "paper_title": "Word representations via Gaussian embedding",
            "rating": 2,
            "sanitized_title": "word_representations_via_gaussian_embedding"
        }
    ],
    "cost": 0.012709499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning
25 Oct 2017</p>
<p>Shoaib Jameel 
Steven Schockaert 
Entity Embeddings with Conceptual Subspaces as a Basis for Plausible Reasoning
25 Oct 2017
Conceptual spaces are geometric representations of conceptual knowledge in which entities correspond to points, natural properties correspond to convex regions, and the dimensions of the space correspond to salient features. While conceptual spaces enable elegant models of various cognitive phenomena, the lack of automated methods for constructing such representations have so far limited their application in artificial intelligence. To address this issue, we propose a method which learns a vector-space embedding of entities from Wikipedia and constrains this embedding such that entities of the same semantic type are located in some lower-dimensional subspace. We experimentally demonstrate the usefulness of these subspaces as approximate conceptual space representations by showing, among others, that important features can be modelled as directions and that natural properties tend to correspond to convex regions.</p>
<p>INTRODUCTION</p>
<p>Despite the fact that several large-scale open-domain knowledge bases are now available (e.g. CYC, SUMO, Freebase, Wikidata and YAGO), few knowledge-driven applications rely on logical reasoning. An important reason for this is that available knowledge is often inconsistent. For example, the concept ice cream shop is asserted to be disjoint from restaurant in CYC, while it is considered a type of restaurant on Wikipedia 3 . Another challenge for logical reasoning is that available knowledge is seldom complete. For example, SUMO encodes 4 knowledge about chess, darts and poker, but mentions nothing about checkers.</p>
<p>Humans are remarkably adept at overcoming such challenges [5,11]. For example, we can recognize that the aforementioned conflict between CYC and Freebase is caused by the vagueness of the categories restaurant and shop, which both have ice cream shop as a borderline case. Similarly, we can deal with knowledge gaps by making inductive inferences, e.g. assuming that properties which hold for chess, darts and poker should hold for checkers as well. Automating such forms of plausible reasoning has proven challenging, among others because they rely on an underlying notion of similarity, which is difficult to characterize using purely symbolic methods.</p>
<p>The solution offered by the theory of conceptual spaces [13] is to represent concepts as regions in a suitable metric space. The points of this space correspond to (actual or possible) entities of a given semantic type, such that similar entities are located close to each other.</p>
<p>It is furthermore posited that most natural properties correspond to convex regions, in accordance with prototype theory [26]. Furthermore, the dimensions of a conceptual space correspond to the salient features of the considered domain. For example, a conceptual space of wines could have dimensions relating to sweetness, acidity, fruitiness, amount of tannins, etc. Using conceptual space representations, many cognitive phenomena, including vagueness and induction, can be modelled in a natural way [13,8,27,19]. However, existing applications have focused on a few particular domains in which conceptual space representations can be derived from available metric information. For example, several authors have considered conceptual spaces for music perception [12,4]. In such cases, the definition of the conceptual space, and its relationship to e.g. audio signals, relies on well-understood insights from the field of music cognition.</p>
<p>The research question we consider in this paper is whether we can automatically obtain approximate conceptual space representations for a wide range of domains, by combining information found in existing knowledge bases with representations derived from large text corpora such as Wikipedia.</p>
<p>Our approach builds on existing work for learning word embeddings from text corpora. Similar to conceptual spaces, word embeddings [21,24,29] represent the meaning of words in a highdimensional Euclidean space, typically as vectors 5 . There are, however, two important differences between word embeddings and conceptual spaces. First, while word embeddings represent all words in a single vector space, conceptual spaces model the entities (and their properties) of a particular semantic type only (e.g. people and cities would be modelled in separate conceptual spaces). Because of this restriction, conceptual spaces can have dimensions that reflect the salient properties of the underlying domain. This allows us to use conceptual spaces for ranking entities (e.g. a conceptual space of cities should have a dimension corresponding the population, allowing us to rank cities from the least to the most populous), modelling context effects 6 , and for describing how two entities or concepts are semantically related (e.g. that the rules of chess are more complex than the rules of checkers). In contrast, the dimensions of a word embedding space are essentially meaningless. Second, conceptual spaces clearly differentiate entities, which are modelled as points, from properties, which are modelled as regions. As a result, conceptual spaces can be used to model that a given entity has a given property or belongs to a given category, to model typicality (e.g. an ice cream shop could be located in the region modelling shop but to-wards the border), and to model semantic relations between different properties and categories.</p>
<p>In [6] an approach was proposed for learning conceptual space representations, which consists of (i) representing each entity of a given semantic type as a bag of words (e.g. each movie is represented as its set of user reviews), (ii) converting that bag of words representation to a vector space representation using multi-dimensional scaling (MDS), and (iii) identifying directions corresponding to salient properties of the considered domain in a post-hoc analysis.</p>
<p>An important limitation of the approach from [6] is that it cannot take advantage of relationships between different conceptual spaces. To address this, in this paper we propose a method that learns a single domain-independent vector space, in which each semantic type corresponds to a particular subspace. In other words, we learn conceptual space representations which are themselves embedded in a higher-dimensional vector space. Among others, this allows us to model semantic type hierarchies (e.g. the conceptual space of humans, in our model, is a subspace of the conceptual space of living things). Furthermore, different conceptual spaces can be aligned by taking into account semantic relations between entities of the corresponding types (e.g. the subspaces representing actors, directors and genres can help to obtain a more accurate representation of movies). Another important limitation of the approach from [6] is that MDS requires a distance matrix whose size is quadratic in the number of entities, which severely limits its scalability. In contrast, our model can easily learn representations for millions of entities.</p>
<p>RELATED WORK</p>
<p>Word embedding</p>
<p>Word embeddings are vector space representations which are used to model the meaning of words. Several existing models construct a vector for each word by applying some form of matrix factorization to a term-term co-occurrence matrix; see [29] for an overview of such approaches. Recently, a number of models have been proposed which instead explicitly optimize the predictive power of the word vectors. For example, the popular Skip-gram model [21] tries to find word vectors that can be used to predict the probability of seeing a context word, given an occurrence of the word being modelled, while the related continuous bag-or-words (CBOW) model focuses on the probability of seeing the word being modelled, given the occurrence of a context word.</p>
<p>An interesting property of word embeddings is that they often capture several kinds of semantic relations, beyond simple similarity. For example, in [21] it is shown that analogical proportions of the form a is to b what c is to d correspond to approximate parallelograms in the space obtained by Skip-gram. They also found that vector addition sometimes corresponds to a form of semantic composition, e.g. adding the vectors for Germany and capital resulted in a vector which is close to the vector for Berlin.</p>
<p>The fact that the vector space obtained by the Skip-gram model satisfies such linear regularities is at first glance somewhat surprising. In [24], the authors analyze what characteristics of a word embedding model can explain this effect, and propose a new model, called GloVe, which is explicitly aimed at capturing linear regularities. Since our model will build on GloVe, we briefly review its formulation. The GloVe model relies on a term-term co-occurrence matrix X = (xij), where xij is the number of times that word i appears in the context of word j. For each term ti in the vocabulary, two word vectors wi andwi and a bias bi are chosen by minimizing the follow-ing objective:
J = V i=1 V j=1 f (xij )(wi ·wj + bi + bj − log xij) 2 (1)
where V is the number of words in the vocabulary. The function f is used to limit the impact of rare terms, whose co-occurrence counts are considered to be noisy. It is defined as follows:
f (xij ) =    x ij xmax α if xij &lt; xmax 1 otherwise (2)
where xmax is a constant which was fixed as 100. Intuitively, wi reflects the meaning of term ti whilewj reflects how the occurrence of that term in the context of another term tj impacts the meaning of tj.</p>
<p>Knowledge graph embedding</p>
<p>Knowledge bases such as Freebase and Wikidata can essentially be seen as collections of (subject, predicate, object) triples, and can thus be encoded as a graph, where nodes correspond to entities and edges are labelled with relation types. Several authors have looked at the problem of automatically expanding such knowledge graphs [7].</p>
<p>Here, we focus on models that rely on embedding knowledge graphs in a vector space, as we will use similar ideas for aligning different conceptual subspaces. The idea of embedding knowledge graphs in a vector space was proposed in [3]. In particular, they propose the model SE, in which each entity ei is represented as a vector and each relation r k is represented using two matrices R lhs k and R rhs k . The constraint they impose is that the following distance should be small for triples (ei, r k , ej) in the knowledge graph and large for other triples:
d(R lhs k ei, R rhs k ej )
where d is either the Euclidean or Manhattan distance. An important drawback of this model is that it requires learning a large number of parameters, which was empirically found to lead to underfitting [2].</p>
<p>In [2] a simpler alternative, called TransE, was proposed, which represents each relation as a vector and considers the following scoring function instead:
d(ei + r k , ej)
Despite the simplicity of this model, it was shown to substantially outperform SE in practice. However, as noted in [32], TransE is mostly suitable for one-to-one relations. To obtain a more faithful modelling of one-to-many, many-to-one and many-to-many relations, the model TransH is proposed. In this model, both a hyperplane H k and an (n − 1) dimensional vector r k is associated with each relation type (with n the dimension of the embedding space), and the following scoring function is considered:
d(e H k i + r k , e H k j )
where e , follows a similar strategy, but instead associates an m-dimensional vector r k and an m × n matrix M k with each relation, and uses the following scoring function:
d(eiMr + r k , ejMr)
The underlying idea is to use the TransE model, after projecting the entities onto a relation-specific space. While in general it is not required that n = m, this particular choice was used in all experiments. Finally, [20] also proposes a variant CTransR, in which each entities are clustered, and each relation can have a different representation for each cluster.</p>
<p>In our model, the semantic types of entities play a crucial role. One other approach that explicitly takes semantic type into account is [14]. In particular, they add a regularization term to the objective function of existing embedding models to encode the requirement that entities of the same semantic type should be represented using similar vectors, which they formalize based on two manifold learning algorithms. Unfortunately, the scalability of the resulting method is relatively limited.</p>
<p>In [31] a model is proposed that combines word embedding with knowledge graph embedding. In particular, they jointly learn a representation for words, entities and relations, where the word representations are constrained similarly as in the Skip-gram model and the entities and relations are constrained similarly as in the TransE model. The entity and word representations are aligned either based on Wikipedia anchors or based on the entity names. An improvement of this method was proposed in [35], where the alignment is instead based on the text of the Wikipedia article of the entity. Along similar lines, [34] proposes a model in which the objective functions of Skipgram and TransE are combined. A third component in their objective function allows the model to take into account an external similarity relation, by imposing the requirement that similar terms should have similar vectors. It is shown that the resulting model improves the word embeddings from Skip-gram.</p>
<p>The model we propose in this paper also combines a word based entity embedding component with a knowledge graph embedding component, although our motivation is different. In particular, [31] and [35] add a word based entity embedding component to a knowledge graph embedding model to improve the predictive performance for entities about which little or nothing is included in the knowledge graph. For example, if the knowledge graph contains the fact that entity a is in relation R with entity b, and from the word based component we can derive that entity a ′ is similar to entity a, then we can plausibly derive that entity a ′ might also be in relation R with entity b. Intuitively, we can thus view these approaches as using word based entity embedding to add a kind of smoothing to the knowledge graph embedding model. In contrast, our aim is to model how different entities of the same type are related. The kind of semantic relatedness in which we are interested (e.g. modelling that one building is taller than another one) is typically not captured by existing knowledge graphs. Intuitively, we use the word based entity embedding component to learn domain-specific vector space representations, and then use a knowledge graph embedding model to align these spaces. This allows us to improve the representation for semantic types about which little information is available in the considered textual descriptions. In this sense, we can view our model as using the knowledge graph embedding component to add a kind of smoothing to the word based entity embedding. Our motivation is somewhat similar in spirit to [34], but that model focuses on word embeddings rather than entity embeddings.</p>
<p>To the best of our knowledge, our model is the first to use semantic type information to learn domain-specific subspaces.</p>
<p>DESCRIPTION OF THE MODEL</p>
<p>Our aim is to learn a vector-space embedding of a set of entities E, in which entities of the same semantic type lie in some lowerdimensional subspace. Let S be the set of all semantic types. For s ∈ S, we write Es for the set of all entities of type s. We furthermore assume that a set of binary relations R is available, and a set G ⊆ E ×R×E of triples of the form (e, k, f ), encoding that entities e and f are in relation k. Finally, we assume that for every entity e, a bag of words We describing that entity is available. The model we propose has the following form:
J = αJtext + (1 − α)(Jtype + Jrel) + βJreg(3)
where α ∈ [0, 1] and β ∈ [0, +∞[ are parameters controlling the relative importance of the different components of the model. Component Jtext will be used to constrain the representation of the entities based on their textual description, Jtype will impose the constraint that entities of the same type belong to a particular subspace, Jrel will use the relations in R to improve the alignment between these subspaces, and Jreg is a regularization component which will allow the model to automatically select the most appropriate number of dimensions for every subspace. We now discuss each of these components in more detail.</p>
<p>Word based entity embedding</p>
<p>From the bag of words representations We, we want to find a point pe ∈ R n for each entity e such that similar entities correspond to nearby points and such that salient features can be interpreted as directions in the space. Specifically, let f be a feature of interest, and let xi ∈ R be the value of feature f for entity ei, i.e. xi reflects how much ei has feature f . Then there should be a vector w f ∈ R n such that the orthogonal projection p ′ e i of the point pe i on the line
L f = {q | q = λ · w f , λ ∈ R} is given by p ′ e i = c f xiw f + b f for b f and c f constants in R.
In other words, in a coordinate system where L f coincides with one of the axes, the corresponding coordinate of pe should be proportional to xi. This requirement is equivalent to 7 :
pe i · w f = (c f xi + b f ) · w f(4)
Unfortunately, we do not actually know what are the salient features in most domains. Following [6], we therefore use word cooccurrence as a proxy for feature values. In particular, we assume that each word potentially corresponds to a salient feature, and that the number of times a word co-occurs with a given entity reflects how much that entity has the corresponding feature. This leads to the following constraint
pe i · wj = g(yji) + bj(5)
where yji is the number of times word tj occurs in We i , g is a monotonic function that maps co-occurrence statistics to feature values, and bj is a constant. Typically it will not be possible to satisfy the constraint (5) for all entities and all context words. The assumption underlying this model is that the salient features of an entity affect the co-occurrence statistics of many context words, and that the words for which (5) is (approximately) satisfied, in an optimal solution, will therefore be those that are strongly related to important features of the entity ei.</p>
<p>Note that the requirement in (5) closely resembles the constraints that are optimized by the GloVe model. Moreover, as in the GloVe model, we can choose g(yji) = log(yji) − bi and formalize the objective function as a least squares regression problem, weighted such that frequent terms have a stronger impact on the objective function:
J E text = e i ∈E t j ∈We i f (yji)(pe i · wj + bi + bj − log yji) 2
where f is defined as in (2). The resulting model is essentially the same as GloVe, but instead of modelling word-word co-occurrence we now model entity-word co-occurrence. The geometric interpretation, however, is different, as we view entities as points and context words as vectors. We can further constrain the word vectors wj by adding a second component, capturing word-word co-occurrences, which corresponds to the original GloVe model. In particular, we define Jtext = J E text + Jglove, where Jglove is the objective function J defined in (1).</p>
<p>Subspace constraints</p>
<p>A key distinguishing feature of our model is that all entities of a given type s are imposed to belong to the same subspace. To formalize this constraint, we associate with each semantic type s a set of n + 1 points p s 0 , ..., p s n ∈ R n and express that for each entity ei of type s, the point pe i can be written as a convex combination of the points p s 0 , ..., p s n :
Jtype = s∈S e∈Es pe − n j=0 λ e,s j p s j 2
where we impose that λ e,x j ≥ 0 and n j=0 λ e,s j = 1. Note that on its own, this component is trivial, as it suffices to choose any set of points p s 0 , ..., p s n in general linear position. However, we will additionally require that the space spanned by the points p s 0 , ..., p s n is as low-dimensional as possible. In particular, let Ms be the n × n matrix whose i th row vector is p s i − p s 0 . Then clearly the rank of Ms is equal to the dimension of the space spanned by p s 0 , ..., p s n . We now want to add a regularization term to penalize high-rank matrices Ms. Unfortunately, no efficient methods exist for directly minimizing the rank of a matrix M . The relaxation suggested in [10] is to minimize the nuclear norm M * instead (i.e. the sum of the singular values of M ). This technique was empirically shown to lead to low-rank matrix solutions in many applications, and is known to be equivalent to rank minimization in certain cases [25]. The regularization term associated with Jtype is thus given by
J 1 reg = s∈S Ms *
To implement nuclear norm regularization, we have used the recently proposed method from [16]. We will also consider a variant in which the points p s 0 , ..., p s n are additionally required to be close to each other:
J comb type = s∈S e∈Es pe − n j=0 λ e,s j p s j 2 + n j=0 d(p s j , c s j )
where c s j = 1 n+1 j p s j is the center-of-gravity of the points p s 0 , ..., p s n .</p>
<p>Modelling relations</p>
<p>Often we have information about how entities of different types are related, e.g. the fact that Steven Spielberg is the director of Jurassic Park. Such relationships can help us to align the subspaces corresponding to different types. Since our main aim is to improve the entity embeddings, rather than predicting relationships between entities of different types, methods such as TransH and TransR, which rely on projecting the entities to a different space, are not directly suitable. On the other hand, TransE is only suitable for one-to-one relations.</p>
<p>We propose an alternative to TransE which is inspired by our modeling of semantic types. As in TransE, we assume that every relation k is represented as a vector r k . We furthermore write rhs(e, k) = {f | (e, k, f ) ∈ G} and lhs(k, f ) = {e | (e, k, f ) ∈ G}. Rather than imposing that e + r k = f if (e, k, f ) ∈ G, as in TransE, we require that the points in P e,k = {p f | f ∈ rhs(e, k)} ∪ {pe + r k } lie in a low-dimensional subspace and, similarly, that the points in
P k,f = {pe | e ∈ lhs(k, f )} ∪ {p f − r k } lie in a low-dimensional subspace.
Note that in the case of one-to-many or many-to-one relations, this part of the model is similar to TransH in the special case where the considered subspaces are one-dimensional. Note that in the case of a one-to-many or many-to-many relation, the set of entities rhs(e, k) is essentially treated as an additional semantic type (e.g. the set of all films directed by Stephen Spielberg), and similar for many-to-one relations and the set lhs(k, f ). As for the semantic types we will consider a number of variants:
J dim rel = k∈R p∈P e,k p − n j=0 µ e,k j q e,k j 2 + p∈P k,f p − n j=0 µ k,f j q k,f j 2 J dist rel = f ∈rhs(e,k) d(p f , pe + r k ) 2 + e∈rhs(k,f ) d(pe, p f − r k ) 2 Jrel = Jrel + J dist rel
where we write e.g. p ∈ P e,k to sum over all entities e and all points p in P e,k . Note that the variant J dist rel essentially corresponds to TransE. </p>
<p>EVALUATION</p>
<p>Data acquisition</p>
<p>In our experiments, we have used Wikidata to obtain a set of entities E and their corresponding semantic types. To generate the bagof-words representation We of a given entity, we take advantage of the fact that Wikidata entities e are linked to their corresponding Wikipedia article de. The set We contains the words occurring in de, as well as the m words before and after any mentions of the entity in other Wikipedia articles. Following [24], we have used a window size of m = 10 (but without crossing sentence boundaries). In particular, we treat every link from some Wikipedia article dx to de as a mention of e, as well as any repeated occurrences of the corresponding anchor text in dx. The word-word co-occurrence in the Jglove component of our model has been obtained from the entire Wikipedia corpus, as in the standard GloVe model. Using the Wikidata dump from October 26, 2015 and the Wikipedia dump from November 02, 2015, we have then selected those entities e which are mentioned in at least 10 Wikipedia articles, resulting in a set E containing 1,292,702 entities. For each semantic type s, the set Es contains those entities which are asserted to be of type s via the instance of property as well as all instances which are asserted to belong to one of the supertypes of s, which was determined using the subclass of property. As the set of binary relations R we considered all Wikidata properties whose value is another entity, apart from instance of and subclass of which have already been used to determine the sets Es. In the case of Wikipedia, we adopted a fairly straightforward preprocessing strategy, as used in many other works such as [31]. In particular, we removed punctuations, lower-cased the tokens, and conducted sentence segmentation using the NLTK library 8 . We also removed words whose term frequency in the entire collection was less than 10. A script has been made available online 9 , which generates an exact copy of our data set, starting from the publicly available dumps of Wikipedia and Wikidata. The implementation of all variants of our model has also been made available at the same link. Most knowledge graph embedding models have been evaluated on fragments of Freebase and WordNet. Our choice of Wikidata is motivated by the fact that it has relatively clean semantic type information. For example, while Barack Obama is of type Human on Wikidata, Freebase among others mentions the following types: film subject, musical artist and building occupant. Furthermore, while Freebase contains information about tens of millions of entities, the standard benchmark datasets, called FB15k [2] and FB13 [28], are relatively small: FB15k covers 14,951 entities and 1,345 relation types, while FB13 covers 74,043 entities and 13 relation types. For completeness, we will include a comparison of our model on these standard benchmark sets for link prediction and triple classification, which are the two standard evaluation tasks for knowledge graph embedding. Our other experiments will be oriented more towards evaluating the usefulness of our model for learning conceptual space representations, in particular their ability to capture semantic relations between entities of the same type (which are not covered in the knowledge graph). This requires a sufficient number of entities for each of the considered semantic types, and a sufficiently clean semantic type structure. Accordingly these tasks will be evaluated only on the WikiData fragment described above. Finally, WordNet has a rich semantic type hierarchy, but contains relatively few instances of these types (e.g. of the 51K leaf nodes in WordNet 1.7 only 7K were found to be instances in [1]) and is thus not suitable for our purposes.</p>
<p>The semantic types of the entities occurring in FB15k and FB13 have been obtained from the "type/instance" field in the Freebase dump 10 . To link Freebase entities to Wikipedia, we have made use of existing Freebase-WikiData mappings 11 .</p>
<p>Variants and baseline methods</p>
<p>Our main baseline is pTransE, which also learns an embedding of entities by combining a word embedding model with a knowledge 8 http://www.nltk.org/ 9 https://github.com/bashthebuilder/ECAI-2016/blob/ master/README.md 10 https://developers.google.com/freebase/data 11 https://developers.google.com/freebase/data# freebase-wikidata-mappings Table 1: Overview of considered variants of our model. graph embedding model. We used the it's publicly available implementation 12 . We consider three variants of this baseline: pTransEanch is the version proposed in [31], which uses anchor text for aligning word vectors and entity vectors; pTransEart is the improvement proposed in [35], which uses the words in the Wikipedia article de instead of anchor text (and a slightly different model); pTransEfull is a variant of pTransEart, which uses the bag of words representation We instead, as in our method. In addition, we compare our method against RESCAL, as well as a number of knowledge graph embedding methods: TransE, TransH, TransR and CTransR. The source codes of these translation-based models are publicly available online 13 . RESCAL [23] is a collective matrix factorization model based on tensor factorization, which has been designed to account for the inherent structure of dyadic relational data. The implementation of RESCAL can be found here 14 . For the knowledge graph embedding methods, we used Bernoulli sampling for selecting negative examples (see [31]); we also obtained results for uniform sampling (not shown), and found the results to be very similar to Bernoulli sampling but slightly worse. It is expected that all of these methods will perform worse than both pTransE and our model, as they cannot exploit the text representation We of the entities. We also compare our method with Skip-gram and CBOW, which can only use text representations and are thus also expected to perform worse. In particular, to apply these models to learn entity embeddings, we use the same method as for our model to determine entity mentions on Wikipedia, and then apply the standard models based on the words surrounding these mentions. Finally, we have compared our method with the multi-dimensional scaling (MDS) based method from [6], in which case we learn a separate vector space for every semantic type. Because of the limited scalability of the latter model, however, we have only considered this for semantic types with up to 10000 instances. Following [6], for each of the remaining semantic types, a vector space representation of the corresponding entities was obtained using Positive Pointwise Mutual Information (PPMI). We then applied multi-dimensional scaling to obtain a lower-dimensional representation, using the angular difference between the initial vectors as metric. We have used the MDS model implemented in MATLAB. We have also considered the method from [14] as an additional baseline, but found that this method could not scale to even the reduced data set that we used for the MDS experiments. Throughout this section, we will refer to our model as EECS (Entity Embeddings with Conceptual Subspaces). As an ablation study, we will consider a number of variants of our model in which some components have been removed. EECSfull refers to our full model, in which Jtype is used for modelling semantic types and Jrel is used for modelling relations; EECSno rel refers to a variant in which Jrel and the associated regularization component J 2 reg have been removed; EECSno type refers to a variant in which Jtype and J 1 reg have been removed; EECSno NN refers to a variant in which the regularization component Jreg has been removed (which also trivializes the component Jtype); EECStext refers to a variant in which only the component Jtext is used, reducing our model essentially to a variant of GloVe. Furthermore, we have considered a few variants of EECSfull in which we change the component Jtype or Jrel by one of the proposed alternatives: EECSrel-dim refers to a variant in which Jrel is replaced by J dim rel , EECSrel-dist refers to a variant in which Jrel is replaced by J dist rel , EECStype-comb refers to a variant in which Jtype has been replaced by J comb type , and EECStype-dist refers to a variant in which only distance information is considered for modelling semantic types (which corresponds to using J comb type without regularization). An overview of the considered variants of our model is provided in Table 1.
name type relation regularization EECS full Jtype J rel J 1 reg + J 2 reg EECS no rel Jtype - J 1 reg EECSno type - J rel J 2 reg EECS no NN - J rel - EECStext - - - EECS rel-dim Jtype J dim rel J 1 reg + J 2 reg EECS rel-dist Jtype J dist rel J 1 reg EECS type-comb J comb type J rel J 1 reg + J 2 reg EECS type-dist J comb type J rel J 2 reg</p>
<p>Methodology</p>
<p>All experiments were evaluated using five-fold cross validation. For tuning the parameter β of our model, based on a tuning/validation set in each experiment, we considered the range {50, 100, 150, 200, 250, 300, 350, 400}. For the parameter α, we considered values between 0 and 1 with an increment of 0.1. The number of iterations for all models was set to 20, as we found that beyond this number empirical results became fairly consistent in all cases. Based on the tuning set, in each of the experiments the optimal value of β was found to be 300, while the optimal values of α varied between 0.4 and 0.7. The number of dimensions was always set to 300 for our model, noting that because of the nuclear norm regularization this only represents an upper bound on the actual number of dimensions. All parameters of the baseline methods, including the number of dimensions, have been optimized based on the tuning set in each experiment. For the MDS method, the number of dimensions was tuned for each semantic type separately (as this method learns a separate vector space for each semantic type), considering the range from 10 to 100 in steps of 10. For the remaining baselines, which construct a single vector space, the number of dimensions was varied between 50 to 300 in steps of 50.</p>
<p>Our model has been implemented in C using standard POSIX threads, which helps scale our implementation to large text collections. For example, for the considered 1.2 million Wikidata entities our full model takes about 30 minutes per iteration using 8 threads, scaling almost linearly in the number of entities. In contrast, EECStext takes about 18 minutes for each iteration using 8 threads.</p>
<p>Results</p>
<p>We will evaluate our model on four different tasks: ranking, induction, analogy making, and knowledge graph embedding. The first two of these tasks are directly aimed at evaluating to what extent the type-specific subspaces learned by our model are useful as conceptual space representations. In particular, ranking will evaluate to what extent important features of a given semantic type can indeed be modelled as directions in the associated subspace, while induction assesses to what extent we can use these representations to find new instances of a given concept, given only a few example instances. The analogy making task is aimed at evaluating how well the different subspaces are aligned. As discussed above, these first three tasks will be evaluated using a large fragment of WikiData. The motivation behind the fourth tasks relates to the observation that even though our motivation was rather different from the motivation behind pTransE, Semantic Type Number of Entities  NN-Dimensions  human  191211  288  railway station  4120  121  house  2762  136  organization  1379  88  national park  1307  56  building  1269  52  food  1155  55  college  858  33  automobile  31  12  candy 10 2  both our model and pTransE combine a word based entity embedding component with a knowledge graph embedding component. Since pTransE has proven a successful approach for knowledge graph embedding, we want to analyse whether our model has any advantages in such a setting. As explained above, for this task we will use the standard benchmark datasets FB15k and FB13. An important aspect in the discussion of the result is to analyze the effectiveness of nuclear norm regularization in identifying the most appropriate number of dimensions for each of the semantic types. To illustrate the behaviour of this regularization component, Table 2 shows the number of dimensions that was found for a few notable semantic types (when using the default configuration of our model). As expected, semantic types with more entities generally end up being associated with higher-dimensional subspaces, but other factors affect the choice as well. For example, note that house has fewer instances than railway station, while being represented by a higherdimensional subspace. Intuitively, this reflects the idea that the type house is more diverse or complex than the type railway station.</p>
<p>Ranking</p>
<p>A characteristic feature of conceptual spaces is that they are encoded as Cartesian products of interpretable dimensions. For a vector space model to be meaningful as a conceptual space, it is therefore important that salient properties can be modelled as directions 15 . Therefore, we have evaluated the ability of our model to correctly rank entities according to a given property. As we need the ground truth, we have focused on properties with numerical values which are available in Wikidata (but have not been considered when learning the space), e.g. the date of birth for entities of type human, or the boiling point of entities of type chemical element. In total, we have retrieved 26 numerical attributes which are available for at least 30 entities. Some of these numerical attributes appear for different semantic types (e.g. the property inception, referring to the foundation year, applies to the semantic types film, organization and country, among others). In total, we obtained 73 such property-type combinations, by considering for each numerical attribute all the maximally specific semantic types with at least 30 instances that have the attribute. Each of these 73 combinations was considered as a problem instance. For each problem instance, the corresponding set of entities is split into 60% training, 20% validation and 20% testing sets. The full specification of the 73 problem instances and corresponding splits is available online 16 . From the training set, a direction is estimated using the SVMRank model 17 [18]. The parameters of the resulting ranking models are optimized using the validation sets. Table 3 shows the performance on the testing set, in terms of Spearman's ρ 18 , expressing the correlation between the ranking predicted by the model and the ranking according to the numerical values found in Wikidata.</p>
<p>The results show that standard word and knowledge graph embedding models are not competitive, which is not surprising given that they use less information than our model. However, the results also show that our model substantially outperforms pTransE, even the variant pTransEfull which uses the same input as our model. Comparing the results for the variants of our model, we notice that the relation component only has a small impact, i.e. the performance of EECSno rel is close to EECSfull, and similarly, the performance of EECSno type is close to EECStext. Regarding the variants of Jtype, EECSfull and EECStype-comb are clearly better than EECStype-dist, which shows the importance of nuclear norm regularization for identifying low-dimensional subspaces. The results in Table 4 compare our model against the MDS model from [6] on a reduced set of 27 problem instances. These results clearly show that the MDS method is not competitive. Tables 5 and 6 illustrate the results of the ranking experiment for three attributes, by showing the 5 lowest and 5 highest ranked entities respectively. Note that Table 5 starts with the lowest ranked entity (i.e. the entity that has the lowest value for the considered attribute), while Table 6 starts with the highest ranked entity (i.e. the entity that has the highest value for the considered attribute). While the rankings are not perfect (e.g. Bermuda, Monaco, San Marino and Barbados are all less populous than Malta, Ptolemy lived around 500 years after Plato), the model's ability to separate high-scoring entities from low-scoring entities is nonetheless remarkable, considering that none of the information that was used to learn the vector space explicitly referred to these attributes. 15 Conceptual space representations also encode information about the correlation between the underlying dimensions, which in our case is captured by the angles between these directions. 16 </p>
<p>Induction</p>
<p>A second characteristic feature of conceptual spaces is that properties correspond to convex regions. Moreover, it is often assumed that the boundaries of these regions are determined based on the distance to a particular point in the space, which acts as a prototype. In this experiment, we test our method's ability to make inductive inferences based on this view. In particular, given a number of entities of the same type which have some property in common, the task we consider is to identify other entities that also have this property (without any knowledge about the property being considered). Problem instances in this case were obtained by omitting all triples of the form (., r, f ) for particular choices of r and f , when learning the embeddings. The set of entities e for which (e, r, f ) ∈ G then defines a problem instance. For example, the property being considered could be "films directed by Stephen Spielberg". Given a few examples of such films, the task is to identify other films directed by Stephen Spielberg (but without the knowledge that this is the property being considered). For each (r, f ) combination, the set of entities {e | (e, r, f ) ∈ G} is split into 60% training, 20% tuning and 20% testing sets. Details on the (r, f ) combinations and associated splits are available online 19 .</p>
<p>For evaluation purposes, we consider this task as a ranking task. In particular, for each problem instance, we rank the entities of the associated semantic type (defined as the most specific semantic type that contains all the considered entities) based on their distance to the center-of-gravity of the training instances, and evaluate the quality of this ranking using mean average precision (MAP), Precision@5 (P@5) and Mean Reciprocal Rank (MRR); note that in all cases, higher values are better.</p>
<p>The results in Table 3 show that our model again substantially outperforms all of the baselines. Note, however, that in the case of MAP, the differences with pTransEfull are rather small. The fact that the differences are clearer for P@5 and MRR suggests that our method is better able to select a few entities with high precision. The MAP score tends to be dominated by outliers, leading to smaller differences. Regarding the different variants of our model, the semantic type component and relation component now play a more equal role, given the rather similar performance of EECSno rel and EECSno type, although semantic type information is still more important than the knowledge graph information (as EECSno rel performs better than EECSno type). As for the ranking experiment, we notice that using J dist type in EECSno type leads to worse results, highlighting again the importance of nuclear norm regularization. A before, the MDS model is not competitive.</p>
<p>Analogy making</p>
<p>Finally we have considered the problem of completing analogical proportions of the form "a is to b what c is to ...", which is a standard evaluation task for word embeddings. Our main aim in this task is to evaluate how well different subspaces are aligned. We have used the test sets from the GloVe project 20 that are about entities, resulting in a total of 8363 problem instances. As there is no need for training data, in this case we randomly split the data into 25% tuning and 75% testing sets. Full details on the test sets and splits that were used have been made available online 21 .</p>
<p>The results are largely consistent with the findings from the previous two experiments. The main difference is that the variant EECStype-comb slightly outperforms EECSfull in this case. The rather large difference in performance between EECStype-comb and EECStype-dist again clearly illustrates the impact of nuclear norm regularization on the results.</p>
<p>Knowledge graph embedding</p>
<p>We have also conducted two knowledge graph embedding experiments using the benchmark datasets FB15k and FB13. In particular, we have evaluated our method on the widely used Link Prediction and Triple Classification tasks.</p>
<p>Link prediction For the link prediction task [2], given an entity e and a relation r, the aim is either to find an entity f such that (e, r, f ) or to find an entity f such that (f, r, e). We have used the standard FB15k test set for this evaluation, which allows us to compare our model with the published results of the state-of-the-art knowledge graph embedding models. Two widely used evaluation metrics, which we will also use, are the average rank of correct entities, called "Mean Rank", and "HITS@10", which is defined as the proportion of test triples in which the target entity was ranked in the top 10. Note that the Mean Rank score is to be minimized while the HITS@10 score is to be maximized. We have used the standard evaluation protocol, including the Bernoulli sampling trick to corrupt the head or tail entity. Test instances were not filtered (which corresponds to the so-called raw version of the task).</p>
<p>In Table 7 we show that our model clearly outperforms the standard baselines in both metrics. To a large extent, this is due to the fact that, apart from pTransE, the other models do not exploit the bag-ofwords representations of the entities. Note that we do not show results for EECStext and EECSno rel in the tables because these models do not take into account any input from the knowledge graph, and is therefore not suitable. The baselines Skip-gram and CBOW are not considered for the same reason.</p>
<p>Triple classification The objective in triple classification [28] is to judge whether a given triplet (e, r, f ) is correct or not, i.e. whether entities e and f are in relation r with each other. This can be naturally cast as a binary classification task. We present results on FB13 and FB15k. The FB13 dataset already comes with golden negative triplets, while we followed the methodology from [28] to construct negative samples for FB15k. For the classification task, we need to set a threshold δr for each relation r. We obtain δr by maximizing the classification accuracies on the validation set. For the given triplet, if the energy score is larger than the relation-specific δr, the instance 20 http://nlp.stanford.edu/projects/glove/ 21 https://github.com/bashthebuilder/ECAI-2016  will be classified as positive, otherwise negative. This is the standard experimental setting for this evaluation task.</p>
<p>Our experimental results are shown in Table 7. With the exception of the pTransE variants, we again show the published results for the baseline models. The baseline results have been reported in [33,15,17,22]. On the FB13 dataset, our model matches the performance of the TransH model, although we clearly outperform all baselines on the FB15k dataset. This means that on the FB13 dataset, the bag-ofwords representations of the entities cannot be exploited effectively, although our model is still not at a disadvantage. This seems related to the fact that only 13 relation types are considered in FB13, which were moreover specifically selected such that they can be predicted from each other, in the sense that hard-to-predict relation types have been removed [28].</p>
<p>CONCLUSIONS</p>
<p>We have proposed a new method for learning vector-space embeddings of entities, based on available semantic information (from Wikidata) and textual descriptions (from Wikipedia). From a technical point of view, the main novelty of our model is the use of nuclear norm regularization to encode the requirement that entities of the same semantic type should lie in a lower-dimensional subspace. In particular, nuclear norm regularization allows the model to automatically select the most appropriate number of dimensions for the subspace corresponding to each type. From an application point of view, our main motivation was to learn subspaces that are useful as approximations of conceptual spaces. To support this view, among others, we have shown that many numerical attributes can be faithfully modelled as directions and that the learned representations allow us to model induction based on distance to a centroid. In addition, we have also obtained good results for analogy making and for two standard knowledge graph embedding tasks.</p>
<p>ACKNOWLEDGEMENTS</p>
<p>This work was supported by ERC Starting Grant 637277.</p>
<p>orthogonal projections of ei and ej on the hyperplane H k . The TransR model, introduced in [20]</p>
<p>For the variants Jrel and J comb rel we again use nuclear norm regularization to enforce low-dimensional subspaces. Let the i th row vector of the matrix M e,k be given by q efor M k,f . We define:J 2 reg = k∈R M e,k * + M k,f *Note that we only need to consider the combination (e, k) or the combination (k, f ) if there is at least one triple of the form (e, k, f ) in G, since otherwise we can trivially choose M e,k and M e,k as the zero matrix. The full regularization term is given by Jreg = J 1 reg +J 2 reg .</p>
<p>Table 2 :
2Numberof dimensions selected by the nuclear norm (NN) 
regularization component of our model for some of the semantic 
types. </p>
<p>Ranking 
Induction 
Analogy 
ρ 
MAP 
P@5 
MRR 
Acc. 
Skip-gram 
0.155 
0.176 
0.356 
0.505 
0.184 
CBOW 
0.159 
0.182 
0.350 
0.500 
0.213 
RESCAL 
0.081 
0.020 
0.189 
0.423 
0.371 
TransE 
0.110 
0.060 
0.200 
0.451 
0.382 
TransH 
0.142 
0.072 
0.210 
0.415 
0.382 
TransR 
0.100 
0.102 
0.302 
0.489 
0.378 
CTransR 
0.122 
0.132 
0.323 
0.499 
0.402 
pTransE anch 
0.099 
0.101 
0.301 
0.488 
0.476 
pTransEart 
0.202 
0.218 
0.475 
0.751 
0.512 
pTransE full 
0.213 
0.224 
0.490 
0.756 
0.532 
EECS full 
0.319 
0.231 
0.609 
0.883 
0.591 
EECS no rel 
0.301 
0.229 
0.588 
0.868 
0.552 
EECSno type 
0.266 
0.225 
0.585 
0.854 
0.549 
EECS no NN 
0.258 
0.220 
0.581 
0.843 
0.545 
EECStext 
0.254 
0.218 
0.579 
0.831 
0.540 
EECS type-comb 
0.312 
0.231 
0.601 
0.883 
0.595 
EECS type-dist 
0.295 
0.231 
0.585 
0.858 
0.550 
EECS rel-dim 
0.309 
0.225 
0.585 
0.859 
0.551 
EECS rel-dist 
0.299 
0.225 
0.585 
0.855 
0.549 </p>
<p>Ranking 
Induction 
Analogy 
ρ 
MAP 
P@5 
MRR 
Acc. 
MDS 
0.101 
0.121 
0.231 
0.388 
0.354 
EECS full 
0.218 
0.140 
0.301 
0.463 
0.456 </p>
<p>Table 4 :
4Comparison with MDS on a subset of the WikiData test data.</p>
<p>https://github.com/bashthebuilder/ECAI-2016 https://www.cs.cornell.edu/people/tj/svm_light/ svm_rank.html18 The reported average ρ values have been obtained using the Fisher ztransformation.17 </p>
<p>Population 
Inception 
Date of Birth 
Malta 
General Electric 
Valmiki 
Bermuda 
IBM 
Jesus Christ 
Monaco 
Hewlett Packard 
Cleopatra 
San Marino 
Microsoft 
Ptolemy 
Barbados 
Oracle Corporation 
Plato </p>
<p>Population 
Inception 
Date of Birth 
China 
Alphabet Inc. 
Prince George of Cambridge 
India 
Tencent Holdings 
Isabela Moner 
USA 
Facebook, Inc. 
Justin Bieber 
Soviet Union 
Uber 
Lionel Messi 
Brazil 
Amazon.com 
Kim Kardashian </p>
<p>Table 6 :
6Five highest ranked entities for a number of ranking problem instances.</p>
<p>Table 7 :
7Link prediction and Triple classification results.
Cardiff University, UK, email: JameelS1@cardiff.ac.uk 2 Cardiff University, UK, email: SchockaertS1@cardiff.ac.uk 3 https://en.wikipedia.org/wiki/Category:Types_of_ restaurants 4 https://github.com/ontologyportal/sumo/blob/ master/Sports.kif
Two notable exceptions are[9] and[30], where words are represented using densities6 The context-dependent nature of similarity is modelled in conceptual spaces by allowing dimensions to be rescaled, depending on the importance of the corresponding property in the given context.
We are abusing notation here, using pe i as a notation for the vector − − → 0pe i .
https://github.com/Mrlyk423/Relation Extraction 13 https://github.com/Mrlyk423/Relation Extraction 14 https://github.com/mnick/rescal.py
https://github.com/bashthebuilder/ECAI-2016</p>
<p>Distinguishing concepts and instances in wordnet. Enrique Alfonseca, Suresh Manandhar, Proceedings of the First International Conference of the Global WordNet Association. the First International Conference of the Global WordNet AssociationEnrique Alfonseca and Suresh Manandhar, 'Distinguishing concepts and instances in wordnet', in Proceedings of the First International Conference of the Global WordNet Association, (2002).</p>
<p>Translating embeddings for modeling multi-relational data. A Bordes, N Usunier, A Garcia-Duran, J Weston, O Yakhnenko, Proceedings of the Annual Conference on Neural Information Processing Systems. the Annual Conference on Neural Information Processing SystemsA. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and O. Yakhnenko, 'Translating embeddings for modeling multi-relational data', in Pro- ceedings of the Annual Conference on Neural Information Processing Systems, 2787-2795, (2013).</p>
<p>Learning structured embeddings of knowledge bases. A Bordes, J Weston, R Collobert, Y Bengio, Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence. the Twenty-Fifth AAAI Conference on Artificial IntelligenceA. Bordes, J. Weston, R. Collobert, and Y. Bengio, 'Learning structured embeddings of knowledge bases', in Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, (2011).</p>
<p>A cognitive architecture for music perception exploiting conceptual spaces. Antonio Chella, Applications of Conceptual Spaces, eds., Frank Zenker and Peter GrdenforsSpringer International PublishingAntonio Chella, 'A cognitive architecture for music perception exploit- ing conceptual spaces', in Applications of Conceptual Spaces, eds., Frank Zenker and Peter Grdenfors, 187-203, Springer International Publishing, (2015).</p>
<p>The logic of plausible reasoning: A core theory. A Collins, R Michalski, Cognitive Science. 131A. Collins and R. Michalski, 'The logic of plausible reasoning: A core theory', Cognitive Science, 13(1), 1-49, (1989).</p>
<p>Inducing semantic relations from conceptual spaces: a data-driven approach to plausible reasoning. J Derrac, S Schockaert, Artificial Intelligence. J. Derrac and S. Schockaert, 'Inducing semantic relations from concep- tual spaces: a data-driven approach to plausible reasoning', Artificial Intelligence, 74-105, (2015).</p>
<p>Knowledge vault: A web-scale approach to probabilistic knowledge fusion. X Dong, E Gabrilovich, G Heitz, W Horn, N Lao, K Murphy, T Strohmann, S Sun, W Zhang, Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningX. Dong, E. Gabrilovich, G. Heitz, W. Horn, N. Lao, K. Murphy, T. Strohmann, S. Sun, and W. Zhang, 'Knowledge vault: A web-scale approach to probabilistic knowledge fusion', in Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 601-610, (2014).</p>
<p>Vagueness: A conceptual spaces approach. I Douven, L Decock, R Dietz, P Égré, Journal of Philosophical Logic. 42I. Douven, L. Decock, R. Dietz, and P.Égré, 'Vagueness: A conceptual spaces approach', Journal of Philosophical Logic, 42, 137-160, (2013).</p>
<p>Representing words as regions in vector space. Katrin Erk, Proceedings of the Thirteenth Conference on Computational Natural Language Learning. the Thirteenth Conference on Computational Natural Language LearningKatrin Erk, 'Representing words as regions in vector space', in Pro- ceedings of the Thirteenth Conference on Computational Natural Lan- guage Learning, pp. 57-65, (2009).</p>
<p>Matrix rank minimization with applications. Maryam Fazel, Stanford UniversityPh.D. dissertationMaryam Fazel, Matrix rank minimization with applications, Ph.D. dis- sertation, Stanford University, 2002.</p>
<p>Leon Festinger, A theory of cognitive dissonance. Stanford University PressLeon Festinger, A theory of cognitive dissonance, Stanford University Press, 1957.</p>
<p>Unifying conceptual spaces: Concept formation in musical creative systems', Minds and Machines. J Forth, G Wiggins, A Mclean, 20J. Forth, G. A Wiggins, and A. McLean, 'Unifying conceptual spaces: Concept formation in musical creative systems', Minds and Machines, 20, 503-532, (2010).</p>
<p>P Gärdenfors, Conceptual Spaces: The Geometry of Thought. MIT PressP. Gärdenfors, Conceptual Spaces: The Geometry of Thought, MIT Press, 2000.</p>
<p>Semantically smooth knowledge graph embedding. S Guo, Q Wang, B Wang, L Wang, L Guo, Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language ProcessingS. Guo, Q. Wang, B. Wang, L. Wang, and L. Guo, 'Semantically smooth knowledge graph embedding', in Proceedings of the 53rd Annual Meet- ing of the Association for Computational Linguistics and the 7th In- ternational Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, pp. 84-94, (2015).</p>
<p>Learning to represent knowledge graphs with gaussian embedding. Shizhu He, Kang Liu, Guoliang Ji, Jun Zhao, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management. the 24th ACM International on Conference on Information and Knowledge ManagementACMShizhu He, Kang Liu, Guoliang Ji, and Jun Zhao, 'Learning to repre- sent knowledge graphs with gaussian embedding', in Proceedings of the 24th ACM International on Conference on Information and Knowl- edge Management, pp. 623-632. ACM, (2015).</p>
<p>Nuclear norm minimization via active subspace selection. Jui Cho, Peder Hsieh, Olsen, Proceedings of the 31st International Conference on Machine Learning (ICML-14). the 31st International Conference on Machine Learning (ICML-14)Cho-Jui Hsieh and Peder Olsen, 'Nuclear norm minimization via active subspace selection', in Proceedings of the 31st International Confer- ence on Machine Learning (ICML-14), pp. 575-583, (2014).</p>
<p>Knowledge graph embedding via dynamic mapping matrix. Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, Jun Zhao, Proceedings of ACL. ACLGuoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao, 'Knowl- edge graph embedding via dynamic mapping matrix', in Proceedings of ACL, pp. 687-696, (2015).</p>
<p>Optimizing search engines using clickthrough data. Thorsten Joachims, Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. the eighth ACM SIGKDD international conference on Knowledge discovery and data miningACMThorsten Joachims, 'Optimizing search engines using clickthrough data', in Proceedings of the eighth ACM SIGKDD international con- ference on Knowledge discovery and data mining, pp. 133-142. ACM, (2002).</p>
<p>A knowledge-based system for prototypical reasoning. Antonio Lieto, Andrea Minieri, Alberto Piana, Daniele P Radicioni, Connection Science. 27Antonio Lieto, Andrea Minieri, Alberto Piana, and Daniele P. Radi- cioni, 'A knowledge-based system for prototypical reasoning', Connec- tion Science, 27, 137-152, (2015).</p>
<p>Learning entity and relation embeddings for knowledge graph completion. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, Xuan Zhu, Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence. the Twenty-Ninth AAAI Conference on Artificial IntelligenceYankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu, 'Learning entity and relation embeddings for knowledge graph comple- tion', in Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, pp. 2181-2187, (2015).</p>
<p>Distributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S Corrado, Jeffrey Dean, Proceedings of the 27th Annual Conference on Neural Information Processing Systems. the 27th Annual Conference on Neural Information Processing SystemsTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jef- frey Dean, 'Distributed representations of words and phrases and their compositionality', in Proceedings of the 27th Annual Conference on Neural Information Processing Systems, pp. 3111-3119, (2013).</p>
<p>STransE: a novel embedding model of entities and relationships in knowledge bases. Kairit Dat Quoc Nguyen, Lizhen Sirts, Mark Qu, Johnson, Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologiesto appearDat Quoc Nguyen, Kairit Sirts, Lizhen Qu, and Mark Johnson, 'STransE: a novel embedding model of entities and relationships in knowledge bases', in Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, to appear, (2016).</p>
<p>A three-way model for collective learning on multi-relational data. Maximilian Nickel, Hans-Peter Volker Tresp, Kriegel, Proceedings of the 28th International Conference on Machine Learning. the 28th International Conference on Machine LearningMaximilian Nickel, Volker Tresp, and Hans-Peter Kriegel, 'A three-way model for collective learning on multi-relational data', in Proceedings of the 28th International Conference on Machine Learning, pp. 809- 816, (2011).</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingJeffrey Pennington, Richard Socher, and Christopher D. Manning, 'Glove: Global vectors for word representation', in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process- ing, pp. 1532-1543, (2014).</p>
<p>Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. Benjamin Recht, Maryam Fazel, Pablo A Parrilo, SIAM review. 52Benjamin Recht, Maryam Fazel, and Pablo A Parrilo, 'Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization', SIAM review, 52, 471-501, (2010).</p>
<p>Natural categories. H Eleanor, Rosch, Cognitive Psychology. 43Eleanor H Rosch, 'Natural categories', Cognitive Psychology, 4(3), 328-350, (1973).</p>
<p>Interpolative and extrapolative reasoning in propositional theories using qualitative knowledge about conceptual spaces. Steven Schockaert, Henri Prade, Artificial Intelligence. 202Steven Schockaert and Henri Prade, 'Interpolative and extrapolative reasoning in propositional theories using qualitative knowledge about conceptual spaces', Artificial Intelligence, 202, 86-131, (2013).</p>
<p>Reasoning with neural tensor networks for knowledge base completion. Richard Socher, Danqi Chen, D Christopher, Andrew Manning, Ng, Advances in Neural Information Processing Systems. Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng, 'Reasoning with neural tensor networks for knowledge base comple- tion', in Advances in Neural Information Processing Systems, pp. 926- 934, (2013).</p>
<p>From frequency to meaning: Vector space models of semantics. P D Turney, P Pantel, Journal of Artificial Intelligence Research. 37P. D. Turney and P. Pantel, 'From frequency to meaning: Vector space models of semantics', Journal of Artificial Intelligence Research, 37, 141-188, (2010).</p>
<p>Word representations via Gaussian embedding. Luke Vilnis, Andrew Mccallum, Proceedings of the International Conference on Learning Representations. the International Conference on Learning RepresentationsLuke Vilnis and Andrew McCallum, 'Word representations via Gaus- sian embedding', in Proceedings of the International Conference on Learning Representations, (2015).</p>
<p>Knowledge graph and text jointly embedding. Z Wang, J Zhang, J Feng, Z Chen, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingZ. Wang, J. Zhang, J. Feng, and Z. Chen, 'Knowledge graph and text jointly embedding', in Proceedings of the 2014 Conference on Empiri- cal Methods in Natural Language Processing, pp. 1591-1601, (2014).</p>
<p>Knowledge graph embedding by translating on hyperplanes. Zhen Wang, Jianwen Zhang, Jianlin Feng, Zheng Chen, Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence. the Twenty-Eighth AAAI Conference on Artificial IntelligenceZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen, 'Knowl- edge graph embedding by translating on hyperplanes', in Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, pp. 1112-1119, (2014).</p>
<p>TransG: A generative mixture model for knowledge graph embedding. Han Xiao, Minlie Huang, Yu Hao, Xiaoyan Zhu, arXiv:1509.05488arXiv preprintHan Xiao, Minlie Huang, Yu Hao, and Xiaoyan Zhu, 'TransG: A gen- erative mixture model for knowledge graph embedding', arXiv preprint arXiv:1509.05488, (2015).</p>
<p>RC-NET: A general framework for incorporating knowledge into word representations. C Xu, Y Bai, J Bian, B Gao, G Wang, X Liu, T.-Y Liu, Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management. the 23rd ACM International Conference on Conference on Information and Knowledge ManagementC. Xu, Y. Bai, J. Bian, B. Gao, G. Wang, X. Liu, and T.-Y. Liu, 'RC- NET: A general framework for incorporating knowledge into word rep- resentations', in Proceedings of the 23rd ACM International Confer- ence on Conference on Information and Knowledge Management, pp. 1219-1228, (2014).</p>
<p>Aligning knowledge and text embeddings by entity descriptions. Huaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, Zheng Chen, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingHuaping Zhong, Jianwen Zhang, Zhen Wang, Hai Wan, and Zheng Chen, 'Aligning knowledge and text embeddings by entity descrip- tions', in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 267-272, (2015).</p>            </div>
        </div>

    </div>
</body>
</html>
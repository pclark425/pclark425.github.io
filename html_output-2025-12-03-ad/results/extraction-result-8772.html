<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8772 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8772</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8772</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-271050100</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.01495v2.pdf" target="_blank">Re-ReST: Reflection-Reinforced Self-Training for Language Agents</a></p>
                <p><strong>Paper Abstract:</strong> Finetuning language agents with reasoning-action trajectories is effective, but obtaining these trajectories from human annotations or stronger models is costly and sometimes impractical. In this paper, we investigate the use of self-training in language agents, which can generate supervision from the agent itself, offering a promising alternative without relying on human or stronger model demonstrations. Self-training, however, requires high-quality model-generated samples, which are hard to obtain for challenging language agent tasks. To address this, we present Reflection-Reinforced Self-Training (Re-ReST), which uses a reflector to refine low-quality generated samples during self-training. The reflector takes the agent’s output and feedback from an external environment (e.g., unit test results in code generation) to produce improved samples. This technique enhances the quality of inferior samples and efficiently enriches the self-training dataset with higher-quality samples. We conduct extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation. The results demonstrate the effectiveness of self-training and Re-ReST in language agent tasks, with self-training improving baselines by 7.6% on HotpotQA and 28.4% on AlfWorld, and Re-ReST further boosting performance by 2.0% and 14.1%, respectively. Our studies also confirm the efficiency of using a reflector to generate high-quality samples for self-training. Moreover, we demonstrate a method to employ reflection during inference without ground-truth feedback, addressing the limitation of previous reflection work.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8772.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8772.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Re-ReST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflection-Reinforced Self-Training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-training framework for language agents that augments model-generated pseudo-labels by using a separate reflector LLM which, given environmental feedback, corrects low-quality trajectories and the corrected samples are used to enrich the self-training dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13B / Llama-3-8B / Llama-2-7B (agents); same families used for reflector</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLMs (Llama families) finetuned with LoRA; for HotpotQA agent/reflector used Llama-2-13B and Llama-3-8B, for ALFWorld used Llama-2-7B; training used sampled agent generations and reflector-corrected generations.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflection-Reinforced Self-Training (Re-ReST)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>During self-training, sample k outputs from agent; score with external environment; if generation fails threshold, pass (x,ŷ,E(x,ŷ)) to a reflector R which produces a corrected sample ỹ using the agent's previous trial and environmental feedback; corrected samples that pass are added to training data. The reflector is trained on (incorrect, correct, feedback) pairs before generating reflection data. Reflection during training is limited to a single iteration per sample in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-hop QA (HotpotQA), Sequential decision-making (ALFWorld), Code generation (MBPP), Visual programming (GQA), Text-to-image generation (VPGen)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-horizon agentic tasks where labels are multi-step thought-action trajectories (ReAct style); environments can provide executable feedback (binary success, unit test results, retrieval correctness, or visual evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Across tasks, Re-ReST improves over self-training: summary reported in paper — self-training improved baselines by +7.6% EM on HotpotQA and +28.4% success on ALFWorld; Re-ReST further improved by +2.0% (HotpotQA) and +14.1% (ALFWorld). Example specifics: Llama-2 HotpotQA EM improved from 20.0% (base) to 27.6% (self-training); for Llama-3, 30.0%→34.4% (self-training); Re-ReST yields additional gains (paper reports aggregate +2.0% over self-training on HotpotQA). MBPP code-generation: Re-ReST reported higher sample-accuracy (paper reports e.g. Self-Training ~66.9% sample-acc → Re-ReST ~77.3% in table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Base (no self-training): HotpotQA EM reported (examples) 20.0% (Llama-2) and 30.0% (Llama-3); Self-training (no reflector) produced the +7.6%/ +28.4% gains above. For ALFWorld: base success 8.9% → self-training 37.3% (paper).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>A separate reflector LLM R (no shared parameters) that consumes task input x, agent generation ŷ, and environment feedback E(x,ŷ) (e.g., unit test failures, binary success, retrieval/evaluation outputs). The reflector optionally outputs reasoning and generates corrected trajectory; corrected outputs are re-evaluated by environment and added to training set if passing. Reflector is trained via maximum likelihood on (x, incorrect, feedback, correct) pairs and optionally on zero-shot generated pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: aggregate task gains as reported above (HotpotQA +2.0% over self-training; ALFWorld +14.1% over self-training). Ablations: (1) sampling more agent generations (increasing k) produced diminishing returns and did not match reflector-augmented data; (2) training the reflector yields better results than zero-shot reflector; (3) integrating DPO with reflector-generated preference pairs is compatible and often helps. Tables and figures in paper provide these comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires reliable ground-truth / high-quality environmental feedback during training (oracle signals such as unit tests or exact-match retrieval results); authors note that without high-quality feedback LLM self-correction can fail. Reflection is only used during training in main experiments; inference-time reflection normally needs ground-truth feedback and so is not assumed available. Paper also warns self-training can amplify model biases and that applicability beyond agent tasks (where feedback is accessible) is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to plain self-training (no reflector), Re-ReST yields better-quality training data and higher final performance. Compared against methods that distill from stronger models (FireAct, LUMOS which use GPT-3.5/4), Re-ReST with open-source toolchains achieved comparable or better performance in HotpotQA under the authors' experimental setup. Paper also contrasts with inference-time self-reflection methods (Reflexion, Self-Refine): Re-ReST confines reflection to training where oracle feedback is available and thus avoids inference-time feedback requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Reported ablations: (1) increasing k (more samples per instance) improves solved-instance count but does not reach reflector-corrected solved-instance count and cannot outperform Re-ReST; (2) training the reflector outperforms zero-shot reflector correction; (3) self-consistency + reflector (3 agent samples + 3 reflector samples) outperforms self-consistency with 6 agent-only samples (EM 32.0% vs 30.8% in one reported table); (4) DPO integration with reflector-preference pairs yields general improvement or parity with supervised-positive-only training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8772.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8772.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflector</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflection model R (reflector) used in Re-ReST</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A separate LLM trained to convert low-quality agent-generated trajectories into corrected ones using explicit environmental feedback; used to expand high-quality pseudo-labels for self-training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Instantiations: Llama-2-13B, Llama-3-8B, Llama-2-7B (depending on task)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An LLM (same families as agents) finetuned (LoRA) to perform self-correction given (x, incorrect generation, environmental feedback) and optionally produce intermediate reasoning about what to change before emitting corrected output.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generate-then-reflect / feedback-guided self-correction (reflector)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Given the task input x, agent trial ŷ, and environment feedback E(x,ŷ) (binary success, unit-test failure messages, retrieval correctness), the reflector outputs a corrected trajectory ỹ. The reflector is trained on pairs of incorrect agent outputs with feedback and corresponding correct outputs (MLE objective), plus additional zero-shot generated data for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Same multi-domain set as Re-ReST (HotpotQA, ALFWorld, MBPP, GQA, VPGen)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Reflector is used across agentic tasks where environment can provide evaluative feedback (e.g., unit tests for code, retrieval correctness, task success/failure signals).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>When used to generate training data, the reflector increased the number of solvable training instances and improved downstream agent performance (e.g., contributed the reported +2.0% on HotpotQA and +14.1% on ALFWorld over self-training). In test-time self-consistency experiments, using 3 agent + 3 reflector samples produced EM 32.0% vs 30.8% for 6 agent-only samples (table reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Without reflector (self-training only) the agent achieves lower scores (e.g., HotpotQA self-training improvements but lower than Re-ReST; ALFWorld success 37.3% with self-training vs +14.1% additional with reflector).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External-feedback-conditioned correction: reflector conditions on the agent's previous trial and explicit environment feedback; trained on (incorrect, feedback, correct) tuples using MLE. No shared parameters with the agent; single-step correction per sample in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Empirical comparisons showing more solved instances discovered and higher final agent performance than sampling-only self-training. Ablation shows trained reflector outperforms zero-shot reflector.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Effectiveness depends on high-quality, informative environmental feedback; the paper limits reflection to one iteration for efficiency and notes multiple iterative corrections are possible but not evaluated extensively here. Test-time usage without oracle feedback is challenging; the authors propose self-consistency as a workaround but gains are modest compared to oracle-enabled reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Related to prior inference-time self-reflection methods (Reflexion, Self-Refine) but differs in being applied during training with oracle feedback and used to produce data for finetuning rather than doing online inference corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Training the reflector yields better downstream agent performance than using a pretrained LLM zero-shot as reflector (Table 5 / ablation summary in text).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8772.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8772.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Training (agent-side)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-training of language agents with model-generated pseudo-labels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-training pipeline where the agent samples multiple outputs per unlabeled instance, a scoring function selects high-quality pseudo-labels (using environmental feedback), and the agent is finetuned on the selected pseudo-labeled set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 / Llama-3 families (as used for agents in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent LLMs finetuned with LoRA on selected self-generated trajectories; typical sampling k=3 generations per instance in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-training (no explicit reflector)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For each unlabeled instance x, sample k generations ŷ_j ~ M(y|x); evaluate generations via environment E(x,ŷ); if score > threshold, add (x,ŷ) to agent training set D_M; finetune agent on D_M. Sampling hyperparameter k typically set to 3 in experiments; selection uses exact-match or unit-test pass signals depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA, ALFWorld, MBPP, GQA, VPGen (same suite of agent tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Agentic tasks where environment provides scoring (e.g., exact match for QA, binary success for ALFWorld, unit-test pass for code tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>N/A (this entry is the baseline without reflector).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Self-training (without reflector) improved performance over base: paper summary reports self-training improved HotpotQA by +7.6% (over baseline) and ALFWorld by +28.4%. Example: HotpotQA Llama-2: 20.0%→27.6% EM after self-training; ALFWorld: base 8.9% success → 37.3% after self-training.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Not a reflection method; mechanism is sampling + environment scoring + selection for finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative gains reported for multiple tasks (HotpotQA, ALFWorld, MBPP). The paper explicitly compares self-training to few-shot baselines and to Re-ReST.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Quality of pseudo-labels is critical; sampling more generations increases data but plateaus and cannot match reflector-corrected data quality. Discards low-quality samples which might be salvageable. Also potential to amplify existing model biases through self-training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly against Re-ReST (self-training + reflector) showing Re-ReST yields superior results; compared against other agent finetuning methods that distill from stronger models (FireAct, LUMOS) and shown to be competitive without relying on closed-source strong models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper shows increasing k (more samples per instance) increases number of solvable instances but still falls short of reflector-solved instances, and model performance does not always improve with more data; indicates quality > quantity and supports reflector use.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8772.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8772.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency + Reflection (test-time)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency-decoding combined with reflector outputs for inference-time reflection without oracle feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time technique where multiple agent outputs are sampled, the reflector is applied to each sample regardless of oracle feedback, and final answer is aggregated by self-consistency (majority voting) to enable reflection-like benefits without ground-truth feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 / Llama-3 used in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent and reflector models from the Re-ReST training pipeline; during inference reflection is applied even without environment feedback and aggregation is performed via self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-consistency with reflector-applied samples</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Sample N agent reasoning traces; for each, run the reflector to produce a 'reflected' version of that trace (reflection here is done without oracle ground-truth feedback); aggregate all final answers (agent and reflected) by majority vote (self-consistency) to produce final output. Paper experiments use configurations like 3 agent samples + 3 reflector samples vs 6 agent-only samples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HotpotQA (reported), generalizable to other tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-hop QA with multiple sampled reasoning traces and final answer aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Reported example (HotpotQA): Self-consistency with 6 agent samples -> EM 30.8%; Self-consistency with 3 agent + 3 reflector samples -> EM 32.0%; Oracle (3 agents + 3 reflectors with ground-truth feedback) -> 36.8%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Self-consistency (6 agent samples) baseline EM 30.8% in table reported.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Inference-time application of the trained reflector to each sampled agent trace without using environment-provided ground-truth feedback, followed by self-consistency majority vote aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Modest empirical improvement reported: 3 agent + 3 reflector samples (32.0% EM) outperform 6 agent-only samples (30.8% EM) on HotpotQA; oracle feedback yields larger gains (36.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reflection without ground-truth feedback is known to often fail; paper acknowledges this and frames self-consistency+reflector as a partial mitigation with modest gains. Oracle (ground-truth) reflection is substantially stronger, showing the key role of good feedback. Authors note that high-quality feedback is often unavailable at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to agent-only self-consistency (more agent samples) and to oracle-assisted reflection; shows reflector+self-consistency outperforms agent-only self-consistency but is weaker than oracle-supported reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Table 5 style comparisons: EM numbers for different sampling/reflection configurations (6 agents vs 3 agents+3 reflectors vs oracle) are provided as an ablation of inference-time strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8772.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8772.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Refine (iterative refinement with self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (Madaan et al., 2023) that iteratively has an LLM evaluate and refine its own outputs to improve answers at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as an inference-time iterative self-reflection method (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>LLM iteratively evaluates its output and refines it based on its own critique, usually at inference time without oracle environment feedback (prior work noted in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompting-based iterative self-critique and refinement at inference time (as described in the cited literature).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mentioned as related inference-time reflection approach; no experiment in this paper using that method directly.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper cites work (Huang et al., 2024) showing that inference-time self-correction without high-quality external feedback can fail; uses this to motivate training-time, feedback-guided reflection instead.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Listed among prior inference-time reflection approaches and contrasted with Re-ReST which performs reflection during training with oracle feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8772.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8772.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (language agents with verbal reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work (Shinn et al., 2023) that uses agent verbal feedback and internal reflection to guide subsequent generations; an inference-time agent reflection approach referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior LLM-based agent framework enabling verbal reinforcement and reflection; cited as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Reflexion (verbal reinforcement learning / reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Agents reflect on prior failures and produce verbal feedback to shape future behavior; typically inference-time and leverages environment interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Agent produces verbal feedback based on prior trials and environment responses and uses that to guide future trials (prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Mentioned in related work; Re-ReST is contrasted as using reflection during training with oracle feedback to produce data for finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper notes inference-time reflection methods like Reflexion rely on test-time feedback or suffer when high-quality feedback is absent; motivates Re-ReST's training-time reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared conceptually in related work; Re-ReST differs by using reflection at training time with ground-truth feedback to generate improved pseudo-labels.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8772.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8772.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Huang et al. (2024) Claim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claim that LLMs cannot self-correct reasoning without high-quality feedback (Huang et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced empirical claim that large LMs often fail to self-correct reasoning unless provided with high-quality external feedback, motivating the requirement of environment feedback in Re-ReST.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>N/A (empirical claim about self-correction)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Observation that LLMs' capability to self-improve is heavily dependent on high-quality, accurate ground-truth feedback; without it, inference-time self-correction methods can degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Paper cites this work as motivation and as evidence that oracle/environment feedback is important; uses that to justify performing reflection during training (with oracle feedback) rather than relying on inference-time self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Used by authors to explain why inference-time reflection without oracle feedback can fail and why training-time reflector with environment signals is preferred.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Serves as critique/context for prior inference-time reflection methods (Self-Refine, Reflexion); Re-ReST attempts to address the limitation by using feedback during training.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Re-ReST: Reflection-Reinforced Self-Training for Language Agents', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-Refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Reinforced Self-Training (ReST) for language modeling <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8772",
    "paper_id": "paper-271050100",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "Re-ReST",
            "name_full": "Reflection-Reinforced Self-Training",
            "brief_description": "A self-training framework for language agents that augments model-generated pseudo-labels by using a separate reflector LLM which, given environmental feedback, corrects low-quality trajectories and the corrected samples are used to enrich the self-training dataset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-13B / Llama-3-8B / Llama-2-7B (agents); same families used for reflector",
            "model_description": "Open-source LLMs (Llama families) finetuned with LoRA; for HotpotQA agent/reflector used Llama-2-13B and Llama-3-8B, for ALFWorld used Llama-2-7B; training used sampled agent generations and reflector-corrected generations.",
            "reflection_method_name": "Reflection-Reinforced Self-Training (Re-ReST)",
            "reflection_method_description": "During self-training, sample k outputs from agent; score with external environment; if generation fails threshold, pass (x,ŷ,E(x,ŷ)) to a reflector R which produces a corrected sample ỹ using the agent's previous trial and environmental feedback; corrected samples that pass are added to training data. The reflector is trained on (incorrect, correct, feedback) pairs before generating reflection data. Reflection during training is limited to a single iteration per sample in the paper.",
            "task_name": "Multi-hop QA (HotpotQA), Sequential decision-making (ALFWorld), Code generation (MBPP), Visual programming (GQA), Text-to-image generation (VPGen)",
            "task_description": "Long-horizon agentic tasks where labels are multi-step thought-action trajectories (ReAct style); environments can provide executable feedback (binary success, unit test results, retrieval correctness, or visual evaluation).",
            "performance_with_reflection": "Across tasks, Re-ReST improves over self-training: summary reported in paper — self-training improved baselines by +7.6% EM on HotpotQA and +28.4% success on ALFWorld; Re-ReST further improved by +2.0% (HotpotQA) and +14.1% (ALFWorld). Example specifics: Llama-2 HotpotQA EM improved from 20.0% (base) to 27.6% (self-training); for Llama-3, 30.0%→34.4% (self-training); Re-ReST yields additional gains (paper reports aggregate +2.0% over self-training on HotpotQA). MBPP code-generation: Re-ReST reported higher sample-accuracy (paper reports e.g. Self-Training ~66.9% sample-acc → Re-ReST ~77.3% in table).",
            "performance_without_reflection": "Base (no self-training): HotpotQA EM reported (examples) 20.0% (Llama-2) and 30.0% (Llama-3); Self-training (no reflector) produced the +7.6%/ +28.4% gains above. For ALFWorld: base success 8.9% → self-training 37.3% (paper).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "A separate reflector LLM R (no shared parameters) that consumes task input x, agent generation ŷ, and environment feedback E(x,ŷ) (e.g., unit test failures, binary success, retrieval/evaluation outputs). The reflector optionally outputs reasoning and generates corrected trajectory; corrected outputs are re-evaluated by environment and added to training set if passing. Reflector is trained via maximum likelihood on (x, incorrect, feedback, correct) pairs and optionally on zero-shot generated pairs.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Quantitative: aggregate task gains as reported above (HotpotQA +2.0% over self-training; ALFWorld +14.1% over self-training). Ablations: (1) sampling more agent generations (increasing k) produced diminishing returns and did not match reflector-augmented data; (2) training the reflector yields better results than zero-shot reflector; (3) integrating DPO with reflector-generated preference pairs is compatible and often helps. Tables and figures in paper provide these comparisons.",
            "limitations_or_failure_cases": "Requires reliable ground-truth / high-quality environmental feedback during training (oracle signals such as unit tests or exact-match retrieval results); authors note that without high-quality feedback LLM self-correction can fail. Reflection is only used during training in main experiments; inference-time reflection normally needs ground-truth feedback and so is not assumed available. Paper also warns self-training can amplify model biases and that applicability beyond agent tasks (where feedback is accessible) is limited.",
            "comparison_to_other_methods": "Compared to plain self-training (no reflector), Re-ReST yields better-quality training data and higher final performance. Compared against methods that distill from stronger models (FireAct, LUMOS which use GPT-3.5/4), Re-ReST with open-source toolchains achieved comparable or better performance in HotpotQA under the authors' experimental setup. Paper also contrasts with inference-time self-reflection methods (Reflexion, Self-Refine): Re-ReST confines reflection to training where oracle feedback is available and thus avoids inference-time feedback requirements.",
            "ablation_study_results": "Reported ablations: (1) increasing k (more samples per instance) improves solved-instance count but does not reach reflector-corrected solved-instance count and cannot outperform Re-ReST; (2) training the reflector outperforms zero-shot reflector correction; (3) self-consistency + reflector (3 agent samples + 3 reflector samples) outperforms self-consistency with 6 agent-only samples (EM 32.0% vs 30.8% in one reported table); (4) DPO integration with reflector-preference pairs yields general improvement or parity with supervised-positive-only training.",
            "uuid": "e8772.0",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Reflector",
            "name_full": "Reflection model R (reflector) used in Re-ReST",
            "brief_description": "A separate LLM trained to convert low-quality agent-generated trajectories into corrected ones using explicit environmental feedback; used to expand high-quality pseudo-labels for self-training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Instantiations: Llama-2-13B, Llama-3-8B, Llama-2-7B (depending on task)",
            "model_description": "An LLM (same families as agents) finetuned (LoRA) to perform self-correction given (x, incorrect generation, environmental feedback) and optionally produce intermediate reasoning about what to change before emitting corrected output.",
            "reflection_method_name": "Generate-then-reflect / feedback-guided self-correction (reflector)",
            "reflection_method_description": "Given the task input x, agent trial ŷ, and environment feedback E(x,ŷ) (binary success, unit-test failure messages, retrieval correctness), the reflector outputs a corrected trajectory ỹ. The reflector is trained on pairs of incorrect agent outputs with feedback and corresponding correct outputs (MLE objective), plus additional zero-shot generated data for robustness.",
            "task_name": "Same multi-domain set as Re-ReST (HotpotQA, ALFWorld, MBPP, GQA, VPGen)",
            "task_description": "Reflector is used across agentic tasks where environment can provide evaluative feedback (e.g., unit tests for code, retrieval correctness, task success/failure signals).",
            "performance_with_reflection": "When used to generate training data, the reflector increased the number of solvable training instances and improved downstream agent performance (e.g., contributed the reported +2.0% on HotpotQA and +14.1% on ALFWorld over self-training). In test-time self-consistency experiments, using 3 agent + 3 reflector samples produced EM 32.0% vs 30.8% for 6 agent-only samples (table reported).",
            "performance_without_reflection": "Without reflector (self-training only) the agent achieves lower scores (e.g., HotpotQA self-training improvements but lower than Re-ReST; ALFWorld success 37.3% with self-training vs +14.1% additional with reflector).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External-feedback-conditioned correction: reflector conditions on the agent's previous trial and explicit environment feedback; trained on (incorrect, feedback, correct) tuples using MLE. No shared parameters with the agent; single-step correction per sample in experiments.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Empirical comparisons showing more solved instances discovered and higher final agent performance than sampling-only self-training. Ablation shows trained reflector outperforms zero-shot reflector.",
            "limitations_or_failure_cases": "Effectiveness depends on high-quality, informative environmental feedback; the paper limits reflection to one iteration for efficiency and notes multiple iterative corrections are possible but not evaluated extensively here. Test-time usage without oracle feedback is challenging; the authors propose self-consistency as a workaround but gains are modest compared to oracle-enabled reflection.",
            "comparison_to_other_methods": "Related to prior inference-time self-reflection methods (Reflexion, Self-Refine) but differs in being applied during training with oracle feedback and used to produce data for finetuning rather than doing online inference corrections.",
            "ablation_study_results": "Training the reflector yields better downstream agent performance than using a pretrained LLM zero-shot as reflector (Table 5 / ablation summary in text).",
            "uuid": "e8772.1",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-Training (agent-side)",
            "name_full": "Self-training of language agents with model-generated pseudo-labels",
            "brief_description": "A self-training pipeline where the agent samples multiple outputs per unlabeled instance, a scoring function selects high-quality pseudo-labels (using environmental feedback), and the agent is finetuned on the selected pseudo-labeled set.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2 / Llama-3 families (as used for agents in experiments)",
            "model_description": "Agent LLMs finetuned with LoRA on selected self-generated trajectories; typical sampling k=3 generations per instance in experiments.",
            "reflection_method_name": "Self-training (no explicit reflector)",
            "reflection_method_description": "For each unlabeled instance x, sample k generations ŷ_j ~ M(y|x); evaluate generations via environment E(x,ŷ); if score &gt; threshold, add (x,ŷ) to agent training set D_M; finetune agent on D_M. Sampling hyperparameter k typically set to 3 in experiments; selection uses exact-match or unit-test pass signals depending on task.",
            "task_name": "HotpotQA, ALFWorld, MBPP, GQA, VPGen (same suite of agent tasks)",
            "task_description": "Agentic tasks where environment provides scoring (e.g., exact match for QA, binary success for ALFWorld, unit-test pass for code tasks).",
            "performance_with_reflection": "N/A (this entry is the baseline without reflector).",
            "performance_without_reflection": "Self-training (without reflector) improved performance over base: paper summary reports self-training improved HotpotQA by +7.6% (over baseline) and ALFWorld by +28.4%. Example: HotpotQA Llama-2: 20.0%→27.6% EM after self-training; ALFWorld: base 8.9% success → 37.3% after self-training.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Not a reflection method; mechanism is sampling + environment scoring + selection for finetuning.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative gains reported for multiple tasks (HotpotQA, ALFWorld, MBPP). The paper explicitly compares self-training to few-shot baselines and to Re-ReST.",
            "limitations_or_failure_cases": "Quality of pseudo-labels is critical; sampling more generations increases data but plateaus and cannot match reflector-corrected data quality. Discards low-quality samples which might be salvageable. Also potential to amplify existing model biases through self-training.",
            "comparison_to_other_methods": "Compared directly against Re-ReST (self-training + reflector) showing Re-ReST yields superior results; compared against other agent finetuning methods that distill from stronger models (FireAct, LUMOS) and shown to be competitive without relying on closed-source strong models.",
            "ablation_study_results": "Paper shows increasing k (more samples per instance) increases number of solvable instances but still falls short of reflector-solved instances, and model performance does not always improve with more data; indicates quality &gt; quantity and supports reflector use.",
            "uuid": "e8772.2",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-consistency + Reflection (test-time)",
            "name_full": "Self-consistency-decoding combined with reflector outputs for inference-time reflection without oracle feedback",
            "brief_description": "An inference-time technique where multiple agent outputs are sampled, the reflector is applied to each sample regardless of oracle feedback, and final answer is aggregated by self-consistency (majority voting) to enable reflection-like benefits without ground-truth feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2 / Llama-3 used in experiments",
            "model_description": "Agent and reflector models from the Re-ReST training pipeline; during inference reflection is applied even without environment feedback and aggregation is performed via self-consistency.",
            "reflection_method_name": "Self-consistency with reflector-applied samples",
            "reflection_method_description": "Sample N agent reasoning traces; for each, run the reflector to produce a 'reflected' version of that trace (reflection here is done without oracle ground-truth feedback); aggregate all final answers (agent and reflected) by majority vote (self-consistency) to produce final output. Paper experiments use configurations like 3 agent samples + 3 reflector samples vs 6 agent-only samples.",
            "task_name": "HotpotQA (reported), generalizable to other tasks",
            "task_description": "Multi-hop QA with multiple sampled reasoning traces and final answer aggregation.",
            "performance_with_reflection": "Reported example (HotpotQA): Self-consistency with 6 agent samples -&gt; EM 30.8%; Self-consistency with 3 agent + 3 reflector samples -&gt; EM 32.0%; Oracle (3 agents + 3 reflectors with ground-truth feedback) -&gt; 36.8%.",
            "performance_without_reflection": "Self-consistency (6 agent samples) baseline EM 30.8% in table reported.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Inference-time application of the trained reflector to each sampled agent trace without using environment-provided ground-truth feedback, followed by self-consistency majority vote aggregation.",
            "number_of_iterations": 1,
            "evidence_for_improvement": "Modest empirical improvement reported: 3 agent + 3 reflector samples (32.0% EM) outperform 6 agent-only samples (30.8% EM) on HotpotQA; oracle feedback yields larger gains (36.8%).",
            "limitations_or_failure_cases": "Reflection without ground-truth feedback is known to often fail; paper acknowledges this and frames self-consistency+reflector as a partial mitigation with modest gains. Oracle (ground-truth) reflection is substantially stronger, showing the key role of good feedback. Authors note that high-quality feedback is often unavailable at test time.",
            "comparison_to_other_methods": "Compared directly to agent-only self-consistency (more agent samples) and to oracle-assisted reflection; shows reflector+self-consistency outperforms agent-only self-consistency but is weaker than oracle-supported reflection.",
            "ablation_study_results": "Table 5 style comparisons: EM numbers for different sampling/reflection configurations (6 agents vs 3 agents+3 reflectors vs oracle) are provided as an ablation of inference-time strategies.",
            "uuid": "e8772.3",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Self-Refine",
            "name_full": "Self-Refine (iterative refinement with self-feedback)",
            "brief_description": "Prior work (Madaan et al., 2023) that iteratively has an LLM evaluate and refine its own outputs to improve answers at inference time.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Described as an inference-time iterative self-reflection method (mentioned in related work).",
            "reflection_method_name": "Self-Refine (iterative self-feedback)",
            "reflection_method_description": "LLM iteratively evaluates its output and refines it based on its own critique, usually at inference time without oracle environment feedback (prior work noted in related work).",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Prompting-based iterative self-critique and refinement at inference time (as described in the cited literature).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mentioned as related inference-time reflection approach; no experiment in this paper using that method directly.",
            "limitations_or_failure_cases": "Paper cites work (Huang et al., 2024) showing that inference-time self-correction without high-quality external feedback can fail; uses this to motivate training-time, feedback-guided reflection instead.",
            "comparison_to_other_methods": "Listed among prior inference-time reflection approaches and contrasted with Re-ReST which performs reflection during training with oracle feedback.",
            "ablation_study_results": null,
            "uuid": "e8772.4",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (language agents with verbal reinforcement learning)",
            "brief_description": "Prior work (Shinn et al., 2023) that uses agent verbal feedback and internal reflection to guide subsequent generations; an inference-time agent reflection approach referenced in the paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "Prior LLM-based agent framework enabling verbal reinforcement and reflection; cited as related work.",
            "reflection_method_name": "Reflexion (verbal reinforcement learning / reflection)",
            "reflection_method_description": "Agents reflect on prior failures and produce verbal feedback to shape future behavior; typically inference-time and leverages environment interactions.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Agent produces verbal feedback based on prior trials and environment responses and uses that to guide future trials (prior work).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Mentioned in related work; Re-ReST is contrasted as using reflection during training with oracle feedback to produce data for finetuning.",
            "limitations_or_failure_cases": "Paper notes inference-time reflection methods like Reflexion rely on test-time feedback or suffer when high-quality feedback is absent; motivates Re-ReST's training-time reflection.",
            "comparison_to_other_methods": "Compared conceptually in related work; Re-ReST differs by using reflection at training time with ground-truth feedback to generate improved pseudo-labels.",
            "ablation_study_results": null,
            "uuid": "e8772.5",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Huang et al. (2024) Claim",
            "name_full": "Claim that LLMs cannot self-correct reasoning without high-quality feedback (Huang et al., 2024)",
            "brief_description": "Referenced empirical claim that large LMs often fail to self-correct reasoning unless provided with high-quality external feedback, motivating the requirement of environment feedback in Re-ReST.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "N/A (empirical claim about self-correction)",
            "reflection_method_description": "Observation that LLMs' capability to self-improve is heavily dependent on high-quality, accurate ground-truth feedback; without it, inference-time self-correction methods can degrade performance.",
            "task_name": "",
            "task_description": "",
            "performance_with_reflection": "",
            "performance_without_reflection": "",
            "has_performance_comparison": false,
            "mechanism_of_reflection": "",
            "number_of_iterations": null,
            "evidence_for_improvement": "Paper cites this work as motivation and as evidence that oracle/environment feedback is important; uses that to justify performing reflection during training (with oracle feedback) rather than relying on inference-time self-correction.",
            "limitations_or_failure_cases": "Used by authors to explain why inference-time reflection without oracle feedback can fail and why training-time reflector with environment signals is preferred.",
            "comparison_to_other_methods": "Serves as critique/context for prior inference-time reflection methods (Self-Refine, Reflexion); Re-ReST attempts to address the limitation by using feedback during training.",
            "ablation_study_results": null,
            "uuid": "e8772.6",
            "source_info": {
                "paper_title": "Re-ReST: Reflection-Reinforced Self-Training for Language Agents",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Self-Refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Reinforced Self-Training (ReST) for language modeling",
            "rating": 2,
            "sanitized_title": "reinforced_selftraining_rest_for_language_modeling"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        }
    ],
    "cost": 0.0201545,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Re-ReST: Reflection-Reinforced Self-Training for Language Agents
7 Jul 2024</p>
<p>Zi-Yi Dou zdou@cs.ucla.edu 
University of California
Los Angeles</p>
<p>Cheng-Fu Yang cfyang@cs.ucla.edu 
University of California
Los Angeles</p>
<p>Xueqing Wu xueqing.wu@cs.ucla.edu 
University of California
Los Angeles</p>
<p>Kai-Wei Chang kwchang@cs.ucla.edu 
University of California
Los Angeles</p>
<p>Nanyun Peng 
University of California
Los Angeles</p>
<p>Re-ReST: Reflection-Reinforced Self-Training for Language Agents
7 Jul 20242E6AAF0D4101471B9C254A871B0504AEarXiv:2406.01495v2[cs.CL]
Finetuning language agents with reasoningaction trajectories is effective, but obtaining these trajectories from human annotations or stronger models is costly and sometimes impractical.In this paper, we investigate the use of self-training in language agents, which can generate supervision from the agent itself, offering a promising alternative without relying on human or stronger model demonstrations.Self-training, however, requires high-quality model-generated samples, which are hard to obtain for challenging language agent tasks.To address this, we present Reflection-Reinforced Self-Training (Re-ReST), which uses a reflector to refine low-quality generated samples during self-training.The reflector takes the agent's output and feedback from an external environment (e.g., unit test results in code generation) to produce improved samples.This technique enhances the quality of inferior samples and efficiently enriches the self-training dataset with higher-quality samples.We conduct extensive experiments on open-source language agents across tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-toimage generation.The results demonstrate the effectiveness of self-training and Re-ReST in language agent tasks, with self-training improving baselines by 7.6% on HotpotQA and 28.4% on AlfWorld, and Re-ReST further boosting performance by 2.0% and 14.1%, respectively.Our studies also confirm the efficiency of using a reflector to generate high-quality samples for self-training.Moreover, we demonstrate a method to employ reflection during inference without ground-truth feedback, addressing the limitation of previous reflection work.Our code is released at https://github.com/PlusLabNLP/Re-ReST.</p>
<p>Introduction</p>
<p>Large language models (LLMs) (Kenton and Toutanova, 2019;Touvron et al., 2023;Achiam (Chen et al., 2023;Yin et al., 2024) distill knowledge from stronger models (e.g., GPT-4) to weaker ones (e.g., Llama-2).In contrast, we adopt self-training and improve it with reflection to improve agents more autonomously, which reduces reliance on external propriety models and maintains a fully open-source framework.et al., 2023) have demonstrated potential in interacting with external environments and addressing practical interactive tasks, resulting in a new class -language agents (Nakano et al., 2021;Yao et al., 2022).Finetuning LLMs for agentic tasks has proven effective, yet existing works rely on data generated by stronger models (e.g., GPT-4) (Chen et al., 2023;Yin et al., 2024), which are not always available (e.g., to improve the strongest model).</p>
<p>Among the potential techniques to improve agents (Ouyang et al., 2022;Wang et al., 2023b;Li et al., 2024;Chen et al., 2024), self-training holds promise for enhancing agent performance for challenging agentic tasks.The self-training process typically involves refining the model by generating samples, assessing their quality through rewards, and updating the model by training on high-quality samples.Compared with existing agent training methods (Chen et al., 2023;Yin et al., 2024), selftraining can autonomously improve agents and reduce the discrepancy between the agent's training data and its original predictions.Additionally, as in Figure 1, self-training can potentially allow for the development of performant agents within a fully open-source framework, without relying on closedsource, proprietary models.Given these benefits, we propose to investigate the use of self-training in language agents in this paper.</p>
<p>Figure 2: An overview of our Re-ReST method.Our approach incorporates self-training in language agent tasks by sampling multiple outputs from an agent and using positive samples for training.To enhance the effectiveness of self-training in language agents, we introduce a reflector mechanism.If a sample is incorrect, the reflector adjusts the agent's output based on environmental feedback.The corrected sample is then incorporated into the training data, thereby improving the overall self-training process.</p>
<p>However, one significant challenge for applying self-training in language agent tasks lies in the acquisition of high-quality samples to achieve good performance.Specifically, self-training requires a substantial amount of high-quality samples, while relying solely on model-generated samples can be inefficient, particularly for language agent tasks that demand multi-step reasoning and long-horizon planning.As a result, it is challenging to obtain good samples solely through sampling.Moreover, the common practice of discarding low-quality samples neglects their potential for improvement and effective utilization, thus limiting the overall efficacy of self-training methods.</p>
<p>To address these issues, we propose Reflection-Reinforced Self-Training (Re-ReST), which enhances the self-training algorithm using a reflection model.Re-ReST incorporates a reflector during self-training, which improves sample quality by utilizing environmental feedback such as execution successes and unit test outcomes.Specifically, the reflector transforms lower-quality samples into higher-quality ones, leveraging the capability of LLMs to self-improve when provided with accurate ground-truth feedback (Huang et al., 2024).Consequently, it enriches the training dataset, enabling more effective bootstrapping.After training, only the agent model is used for inference, ensuring no additional computational burden during testing.Unlike existing self-reflection methods (Madaan et al., 2023;Shinn et al., 2023;Pan et al., 2023), Re-ReST only requires access to feedback during training, not during inference, making our setting more realistic and practical.</p>
<p>We conduct extensive experiments with opensource LLMs across a wide range of tasks, including multi-hop question answering, sequential decision-making, code generation, visual question answering, and text-to-image generation.Our results first demonstrate the potential of self-training in language agent tasks, showing improvements over few-shot baselines in long-horizon planning tasks, with gains of 7.6% on HotpotQA and 28.4% on AlfWorld.By incorporating Re-ReST, we further enhance performance significantly by 2.0% and 14.1% on HotpotQA and AlfWorld, respectively, achieving results better or comparable to models relying on commercial APIs.Ablation studies confirm the efficiency of the reflection model in generating high-quality self-training samples.Furthermore, we explore using our reflection model during inference with self-consistency decoding, which improves the model performance while alleviating the need for ground-truth feedback required by previous work (Huang et al., 2024).Additionally, we demonstrate the application of our method in preference optimization objectives.</p>
<p>2 Method: Re-ReST Self-Training.Formally, given a dataset U = {x i } N i=1 , self-training begins by using a base model M to generate a pseudo-label ŷi = M(x i ) for each instance x i ∈ U .Subsequently, a subset of {(x i , ŷi )} N i=1 is selected based on a scoring function, and M is finetuned on this selected subset.For language agents, we define the label y as a trajectory comprising interleaved thoughts and actions, as described in ReAct (Yao et al., 2022).We propose adopting the self-training paradigm by training language agents with their self-generated thought-action trajectories.</p>
<p>Overview of Re-ReST.Obtaining high-quality samples through self-sampling can be challenging, particularly for complex language agent tasks.To address this issue, we introduce Re-ReST, which aims to enhance the pseudo-label generation pro-cess in self-training for language agents.As illustrated in Figure 2, we propose improving lowquality samples using a reflection model with external feedback.We then enrich the self-training data by incorporating these corrected generations.This process generates high-quality samples efficiently by correcting low-quality ones with ground-truth feedback during training.</p>
<p>Components</p>
<p>Our method involves two models, including a language agent M that generates text and actions, and a reflection model R that improves a low-quality sample.The reflection model R has access to an external environment E that can provide external feedback to a generated sample (e.g.numerical scores and/or verbal error information).We illustrate each of these modules in the following part.</p>
<p>Language Agent.The language agent M is built upon a large language model (LLM) that is trained or prompted to generate thoughts and actions given a task.Formally, given an instance x i , the agent M generates its output ŷ ∼ M(y|x) containing its actions.The agent can first generate its reasoning traces before outputting its actions, which has been demonstrated to improve the model performance and interpretability (Yao et al., 2022).</p>
<p>Reflector.The reflection model R is also instantiated as an LLM, the goal of which is to improve the language agent's generations given external feedback.We assume that during training, an external environment E can evaluate a generated sample and provide feedback E(x, ŷ) to the agent.The feedback can be a binary success status and/or error information.For example, in code generation tasks, the environment can execute the model-generated code on unit tests, providing information on whether the code has syntax errors and whether it can pass the unit tests.</p>
<p>Having access to such an environment is important in our setting, as it has been shown that an LLM cannot perform self-correction without highquality external feedback (Huang et al., 2024).The reflection model generates a corrected sample ỹ ∼ R(y|x, ŷ, E(x, ŷ)) given the task information x, the agent generation ŷ, and the environmental feedback E(x, ŷ).It can optionally first state its reasoning process (e.g., which specific actions could be corrected) before generating the corrected answer.)The use of the reflection model can improve self-training by finding good solutions efficiently because of the additional information provided (i.e., the agent's previous trial and the environmental feedback.)We do not share the model parameters between the agent and reflector in this paper.</p>
<p>Data Generation</p>
<p>We then describe how we generate self-training data for the language agent M. The data generation process involves two steps, including the initial generation step with the language agent itself and the reflection step with the reflector, and we obtain the agent-generated dataset D M and reflectorgenerated dataset D R from the two steps.</p>
<p>Initial Generation.As in the standard setup, given an instance x, we sample k generations {ŷ j } k j=1 from the current language agent model ŷj ∼ M(y|x).Then, the environment E scores the generation and provides feedback E(x, ŷj )).If the score exceeds a threshold, we add the instance to (x, ŷj ) to the training data D M .In practice, we observe that setting k = 3 achieves a good balance between efficiency and effectiveness.</p>
<p>Reflection with Environmental Feedback.The initial generation step only relies on the agent model M itself to generate data.For a sampled generation ŷj , if the score does not pass the threshold, we will feed it to the reflection model for refinement.The reflector takes as inputs the task information x, the agent's prior generation ŷj , and the environmental feedback E(x, ŷj )), and then generates the corrected sample ỹj ∼ R(x, ŷj , E(x, ŷj )).The corrected sample ỹj will also be evaluated by the environment and we will add it to the reflectorgenerated training dataset D R if its score exceeds the threshold.While the reflection procedure can be iteratively applied multiple times as per Shinn et al. (2023), in this study, we limit this process to a single iteration for the sake of efficiency.This means that each generated sample ŷj is allowed a maximum of one refined counterpart ỹj .</p>
<p>Model Training and Inference</p>
<p>We first train the reflector R parameterized by θ R and then use the trained reflector to generate the reflection data D R .Afterward, we combine D R and the agent's self-generated data D M to train the agent model M parameterized by θ M .</p>
<p>Reflector Training.While base LLMs can perform self-reflection or self-correction without any finetuning given ground-truth feedback (Shinn et al., 2023), we propose to further improve its reflection ability with the self-generated data.First, from the initial generation step, we obtain multiple generations {y j } k j=1 from the agent model M. For each correct generation y w and incorrect generation y l with its environmental feedback E(x, ŷl ) in {y j } k j=1 , we will add the instance ⟨x, y l , E(x, ŷl ), y w ⟩ to the agent-generated dataset D R M for reflector training.In addition, the reflector generates its self-training dataset in a zero-shot manner D R R similar to the agent initial generation step.Combining the two generated datasets, we train the reflector on D R M ∪ D R R with the standard maximum log-likelihood objective first before generating the training data D R for the language agent:
LMLE(θR) = −E (x,y l ,y w )∼D R M ∪D R R log p θ R (y w |x, y l ).
(1)</p>
<p>Language Agent Training.After we have the base language agent to generate the self-training data D M and the improved reflector to generate the reflector-generated data D R , we train the language agent jointly on D M ∪ D R :
L M LE (θ M ) = −E (x,y)∼D M ∪D R log p θ M (y|x).
(2) Besides the maximum log-likelihood objective, because the reflection training and data generation process involves the use of preference pairs, it is natural to use preference optimization objectives such as DPO (Rafailov et al., 2023) for training, which we will discuss in the experiment section.</p>
<p>Inference.During inference, accessing highquality environmental feedback is often challenging, which can cause inference-time self-reflection algorithms to fail (Huang et al., 2024).Therefore, we only have the agent M directly output generations without the reflector during inference.This approach eliminates the need for feedback and avoids any additional computational overhead.A potential method to integrate the reflector into the inference process involves first training a scorer to evaluate the agent's output.If the score falls below a certain threshold, self-correction can then be performed, which we leave as a future direction.Additionally, we propose performing reflection regardless of environmental feedback and employing self-consistency to derive the final results from both the agent's outputs and the reflector's outputs, as shown in the experiment section.</p>
<p>Experiments</p>
<p>We experiment with multi-hop reasoning, sequential decision-making, code generation, visual question answering, and text-to-image generation.We present the experimental settings and results for each task.In all our experiments, we advocate for the use of open-source models and aim to avoid black-box, closed-source commercial models whenever possible.</p>
<p>Multi-Hop Reasoning</p>
<p>Dataset.We use the HotpotQA dataset (Yang et al., 2018), a well-established question-answering dataset featuring multi-hop reasoning and knowledge retrieval.It is constructed based on Wikipedia and an agent needs to retrieve and reason over multiple supporting documents to answer a question.We sample 5,000 training instances randomly for self-training and 500 instances from the development set for evaluation as in Chen et al. (2023).</p>
<p>Model Setup.We build both the agent model and the reflector upon the Llama-2-13B and Llama-3-8B models (Touvron et al., 2023).Note that different from previous work (Shinn et al., 2023;Chen et al., 2023;Yin et al., 2024), we do not employ a stronger language model such as GPT-3.5/4for data generation or self-reflection, ensuring that the models do not benefit from knowledge distillation.Following Shinn et al. (2023), we use the ReAct (Yao et al., 2022) method where at each step, the agent model first generates its thoughts and then performs an action.The action is chosen from (1) Search[entity], which searches the exact entity on Wikipedia, (2) Lookup[keyword], which localizes a keyword in the retrieved passages, and ( 3) Finish[answer], which returns the answer and finishes the task.We use a free Wikipedia API1 for passage retrieval and keyword lookup.</p>
<p>Training and Evaluation Setup.We use 2-shot prompting for few-shot agent and reflector data generation as in Shinn et al. (2023).For each training instance, the agent model samples 3 generations.The generation is evaluated with the exact match metric (i.e., if the generated answer is exactly the same as the ground-truth answer).The retrieval and evaluation results are given to the reflector as the environmental feedback for self-correction.We use Low-Rank Adaptation (LoRA) (Hu et al., 2022) for training the language models for efficiency.The agent and reflector models are trained for 3 epochs with a learning rate of 3e-4.</p>
<p>Main Results.We list the main results in Table 1.As shown in the table, self-training can significantly improve the model performance from an EM score of 20.0% to 27.6% for Llama-2 and from 30.0% to 34.4% for Llama-3.However, only 37.1% and 48.3% of the training instances are correctly solved by the agent model and are used for self-training respectively.By integrating our reflector model into the process, the agent can solve more training instances and thus have more data for training the agent model, increasing the EM scores significantly.In addition to our implemented models, following previous work (FireAct (Chen et al., 2023) and LUMOS (Yin et al., 2024)) that use GPT-3.5/4for data generation and model finetuning, we employ GPT-4 to generate 0.5k instances and first train the agents with the GPT-4 generated data before selftraining.Results demonstrate that 1) self-training is a stronger baseline than FireAct under a fair setting where the same QA tool is used; 2) we can achieve comparable or better performance of our model than these methods, even though both of them use strong knowledge retrieval models (i.e., SerpAPI2 for FireAct and GPT-4 for LUMOS), which are costly and non-scalable.By contrast, we use the free Wikipedia API.</p>
<p>Sequential Decision-Making</p>
<p>Dataset.We also assess the proposed approach on sequential decision-making using ALF-World (Shridhar et al., 2021).ALFWorld comprises a collection of text-based settings designed to test an agent's ability to complete multi-step tasks across diverse interactive environments.Following Yao et al. (2022); Shinn et al. (2023), we operate under the assumption that the agents are devoid of any access to successful trajectories, relying solely on a binary indicator of task success or failure.Our evaluation encompasses testing the agent across 134 previously unseen environments, spanning six diverse tasks.These tasks range from locating concealed items and transporting objects to interacting with objects using other items.</p>
<p>Model Setup.We build the agent and the reflector upon the Llama2-7b (Touvron et al., 2023).At each step, the agent can either contemplate its next move or generate admissible actions for execution as in Yao et al. (2022).Following the heuristics outlined by Shinn et al. (2023), we trigger the reflector model for self-reflection if the agent repeats an action with the same response over three cycles, or if it performs over 30 actions in an environment.</p>
<p>Training and Evaluation Setup.We use oneshot prompting instead of the two-shot prompting in Shinn et al. (2023) for the models so that we can better fit a trajectory into the context window of Llama-2.We train the agent and reflector models on the collected trajectories for 2 epochs with a learning rate of 2e-5 using LoRA.</p>
<p>Results.As shown in Table 2, it is evident that the base Llama model faces challenges in adapting to the experimental environment, but self-training can significantly improve the model performance.A significant point to highlight is that the model operates without access to complete trajectories during the experiment.Despite this limitation, it demonstrates a notable improvement in performance within unseen environments-increasing the success rate from 8.9% to 37.3% through the utilization of self-augmented trajectories.Furthermore, the implementation of the reflector contributes a 14.1% uplift in success rates, which affirms the efficacy of our proposed method.</p>
<p>Programming: Code Generation and</p>
<p>Visual Question Answering Dataset.For code generation, we experiment with the Python code writing task on MBPP (Austin et al., 2021) and visual programming on GQA (Hudson and Manning, 2019) For GQA, following Surís et al. (2023), we build the agent by providing a pre-defined set of visual APIs (e.g.object detection) and prompt the model to generate code using the APIs.</p>
<p>Training and Evaluation Setup.For MBPP, we use zero-shot and three-shot prompting for zeroshot agent and reflector data generation.For GQA, we follow the prompt in Surís et al. (2023) for the model for sample generation.For both datasets, the agent model samples 3 generations per training instance as before.We do not use the provided ground truths for MBPP training for consistency with the other experimental settings.The agent and reflector models are trained for 3 epochs with a learning rate of 3e-4 using LoRA.The BLIP-2 generated results are treated as the environmental feedback for the reflector.We do not use zero-shot reflection results to train the reflector because LLMs cannot perform this task without finetuning.The agent and reflector are trained for 2 epochs with a learning rate of 1e-5 using LoRA.(Rafailov et al., 2023), and integrating DPO into our method can generally improve the model performance.</p>
<p>Results. As in</p>
<p>Results. As shown in</p>
<p>Analysis</p>
<p>Re-ReST v.s.Self-Training with More Samples.</p>
<p>We investigate if we can simply sample more generations from the language agent for self-training and achieve comparable performance with our reflectoraugmented method.Specifically, we try to sample k generations for each instance, where k is set to 1, 2, 3, 4, 5, 6, and use the generated samples for self-training.As shown in Figure 3, if we keep sampling more generations from the language agent, the agent can indeed solve more instances and we can obtain an increasing amount of data for self-training.However, 1) the number of solved instances is still lower than the number of reflectorsolved instances, demonstrating that the reflector can find the correct solutions more efficiently than sampling; 2) the model performance is not always improved with more training data and it cannot outperform our method even when trained with more generated samples, indicating that the quality of the self-training data is also important and our reflector can generate training data effectively for the agent.</p>
<p>Effect of Training the Reflector.</p>
<p>As illustrated, we propose to first train the reflector before using it to generate the self-training data.In this part, we investigate if we can use the reflector to perform self-correction in a zero-shot manner and then train the language agent.As in Table 5, we find that while the reflector can perform self-correction without any finetuning and improve the performance of the language agent, further improvements can be made if we specifically train the model for selfcorrection, demonstrating the effectiveness of our proposed reflector training strategy.</p>
<p>Test-Time Reflection without Ground-Truth Feedback.Previously, our reflector functions only during training and is not used during inference because it is often impossible to obtain ground-truth feedback, which is required for reflection methods to work (Huang et al., 2024).In this section, we propose employing selfconsistency (Wang et al., 2023a) to enable test-time reflection and address this limitation.Selfconsistency is a decoding technique that combines multiple model predictions by sampling various reasoning paths and then selecting the most consistent answer through a majority vote.This approach allows us to apply the reflector during inference.Specifically, we sample multiple answers from our model and perform reflection on each output, regardless of correctness.We then aggregate all the answers using self-consistency.As in Table 6, integrating our reflector with self-consistency (3 agent samples and 3 reflection samples) achieves improvements over baseline (self-consistency with 6 model samples).This demonstrates the potential application of our method during inference, overcoming the current limitation of requiring groundtruth feedback for reflection methods.</p>
<p>Re-ReST with Direct Preference Optimization.</p>
<p>Our reflector turns incorrect samples into correct ones, naturally making negative-positive pairs suitable for preference optimization objectives such as DPO.In this part, we investigate the application of DPO in our method.As in Table 7, integrating DPO into our method can generally improve or achieve comparable performance with training models only with supervised training on positive samples, indicating our compatibility with DPO.</p>
<p>Conclusion</p>
<p>Our study studies the applications of self-training in language agents and improves it with Reflection-Reinforced Self-Training (Re-ReST), an approach that efficiently obtains high-quality samples for self-training with a reflector.Our experiments demonstrate that Re-ReST outperforms selftraining methods across various tasks, confirming the efficiency and effectiveness of incorporating a reflection mechanism.Within the proposed framework, in the future, we can improve the reflection mechanism and develop better training paradigms for the agent and reflector.</p>
<p>Limitations</p>
<p>Our approach is predicated on the availability of ground-truth feedback during the training process.While this assumption holds true for many language agent tasks, it presents challenges when applied to broader contexts.Specifically, acquiring accurate ground-truth feedback can be difficult in diverse, real-world scenarios.This limitation underscores a key aspect of our study: it is primarily concentrated on language agent tasks, thereby neglecting the potential applications and implications within the broader scope of general language modeling.This suggests the need for future research to explore and address the complexities of applying our methods to general language modeling tasks, where ground-truth feedback may not be as readily accessible or reliable.Another potential risk of the method is that through self-training, the biases encoded in LLMs can be amplified, and careful calibrations should be conducted before the deployment of our method.</p>
<p>A Related Work</p>
<p>In this section, we first overview the research progress in language agents, then briefly describe self-training and self-correction methods for improving language agents.We also summarize the major differences between our work and previous language agent methods in Table 8.</p>
<p>Language Agents.Language agents refer to language models that interact with the world in general.It has been demonstrated that LLMs can perform actions by generating specific commands (Nakano et al., 2021;Huang et al., 2022;Ahn et al., 2022) and calling external tool APIs (Lu et al., 2023;Schick et al., 2023;Gou et al., 2024).By integrating the model reasoning and acting abilities, ReAct (Yao et al., 2022) asks an LLM to first generate reasoning traces and then act accordingly, which is then improved by follow-up works through inference-time techniques such as reflection (Shinn et al., 2023) and planning (Yao et al., 2023;Yang et al., 2023).Recently, finetuning agents (Chen et al., 2023;Yin et al., 2024) have attracted attention from the research community.However, most of the existing works attempt to distill knowledge from a relatively strong LLM (e.g., GPT-4) to a weaker LLM (e.g., LLaMa-2).By contrast, our work bootstraps a language agent's performance by utilizing its own reflective ability without using external models.</p>
<p>Self-Training for Language Models.Various self-training algorithms have been proposed to improve language models (He et al., 2019;Huang et al., 2023;Dong et al., 2023;Gulcehre et al., 2023;Yuan et al., 2024), with the general idea being to improve models with self-generated samples in an unsupervised or semi-supervised manner.He et al. ( 2019) is one early work in applying self-training to generative language models and points out the importance of introducing noises during pseudo-label generation to increase the sample diversity.In the large language model era, Gulcehre et al. (2023) propose Reinforced Self-Training (ReST), where they use a scoring function to select self-generated samples and augment the training data.Similarly, Yuan et al. (2024) proposes self-rewarding that scores samples with the LLM itself and trains the model with direct preference optimization (DPO) (Rafailov et al., 2023) on the scored samples.Self-training has also been employed to improve the chain-of-thought reason-ing (Nye et al., 2022;Wei et al., 2022) ability of LLMs (Uesato et al., 2022).For example, Zelikman et al. (2022) propose to ask an LLM to generate rationales given questions and improve the LLM with its own generated reasoning.Re-ReST falls under the self-training paradigm, and different from previous work, our aim is to generate useful samples efficiently for self-training.</p>
<p>Self-Reflection/Self-Correction for Language Models.Several works have used LLMs to reflect on their generations with internal or external feedback and correct their errors (Welleck et al., 2023;Wang et al., 2023c;Shinn et al., 2023;Madaan et al., 2023;Kim et al., 2024;Ji et al., 2024).A majority of this line of research is focused on improving LLMs during inference.For example, Self-Refine (Madaan et al., 2023) proposes to have LLMs iteratively evaluate their generations, based on which they improve their generations.Similarly, Shinn et al. (2023) use LLM agents to reflect on its generations and their environment feedback, then guide the next generation with the generated verbal feedback.As pointed out by Huang et al. (2024), high-quality external feedback is essential for these self-correction models, without which existing techniques actually decrease model performance.However, such high-quality feedback is often unavailable during the test time, thus we propose to use Re-ReST only during training and perform corrections with oracle feedback from environments, ensuring its effectiveness in correcting the model generations.In addition, during the test time, the corrected generations are distilled into the language model, thus directly generating the answer without introducing overhead during inference.</p>
<p>B Prompts</p>
<p>Work</p>
<p>Agent Training</p>
<p>Agent Reflection Finetuning GPT-Free w/ G.T. Feedback w/o G.T. Feedback FireAct (Chen et al., 2023) ✓ ✗ ✗ ✗ LUMOS (Yin et al., 2024) ✓ ✗ ✗ ✗ Reflexion (Shinn et al., 2023) ✗ ✗ ✓ ✗ Self-Refine (Madaan et al., 2023)
✗ ✗ ✓ ✗ Re-ReST ✓ ✓ ✓ ✓
Table 8: Comparisons with previous language agent methods.We propose to finetune LLMs for language agent tasks with self-generated data, while previous work such as FireAct and LUMOS rely on stronger LLMs such as GPT-4 to perform knowledge distillation.In addition, we propose to use the agent's reflection ability to improve the self-training efficiency, where the reflection can function both with and without ground-truth feedback, addressing the limitation of previous agent reflection methods (Shinn et al., 2023;Madaan et al., 2023;Huang et al., 2024).</p>
<p>Prompt Solve a question answering task with interleaving Thought, Action, Observation steps.Thought can reason about the current situation, and Action can be three types:</p>
<p>(1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists.If not, it will return some similar entities to search.</p>
<p>(2) Lookup[keyword], which returns the next sentence containing keyword in the last passage successfully found by Search.</p>
<p>(3) Finish [answer], which returns the answer and finishes the task.You may take as many steps as necessary.</p>
<p>{In-context examples} Question: What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?Thought 1: I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.The implementation failed the test cases where the input integers are 1 and 2. The issue arises because the code does not add the two integers together, but instead subtracts the second integer from the first.To fix this issue, we should change the operator from '-' to '+' in the return statement.This will ensure that the function returns the correct output for the given input.</p>
<p>[improved impl]: "'python def add(a: int, b: int): """ Given integers a and b, return the total value of a and b. """ return a + b "'"' ... {In-context examples} You are in the middle of a room.Looking quickly around you, you see a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a countertop 1, a garbagecan 1, a handtowelholder 2, a handtowelholder 1, a sinkbasin 2, a sinkbasin 1, a toilet 1, a toiletpaperhanger 1, and a towelholder 1.Your task is to: put some spraybottle on toilet.&gt; think: To solve the task, I need to find and take a sparybottle, then put it on toilet.OK. &gt; think: First I need to find a spraybottle.A spraybottle is more likely to appear in cabinet (1-4), countertop (1), toilet (1), sinkbasin (1-2), garbagecan (1).I can check one by one, starting with cabinet 1. OK. &gt; go to cabinet 1 On the cabinet 1, you see a cloth 1, a soapbar 1, a soapbottle 1. &gt; go to cabinet 2 The cabinet 2 is closed.&gt; open cabinet 2 You open the cabinet 2. The cabinet 2 is open.In it, you see a candle 1, and a spraybottle 2. &gt; think: Now I find a spraybottle (2).Next, I need to take it.OK &gt; take spraybottle 2 from cabinet 2 You pick up the spraybottle 2 from the cabinet 2. &gt; think: Now I take a spraybottle (2).Next, I need to put it in/on toilet 1. OK. &gt; go to toilet 1 On the toilet 1, you see a soapbottle 2. &gt; put spraybottle 2 in/on toilet 1 You put the spraybottle 2 in/on the toilet 1.</p>
<p>User {Input task and previous trial}</p>
<p>User {Input question} Here is the task: You are in the middle of a room.Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1.Your task is to: examine the bowl with the desklamp.{Reflection Results} think: I was stuck in a loop in which I continually picked up the alarmclock 1 instead of turning on the desklamp.</p>
<p>Figure 1 :
1
Figure 1: Previous agent training methods(Chen et al., 2023;Yin et al., 2024) distill knowledge from stronger models (e.g., GPT-4) to weaker ones (e.g., Llama-2).In contrast, we adopt self-training and improve it with reflection to improve agents more autonomously, which reduces reliance on external propriety models and maintains a fully open-source framework.</p>
<p>Table 15 :
15
Example Prompt Template on the ALFWorld dataset.A prompt includes (a) {In-context example} which is a complete trajectory from a successful trial.(b) {Input question} describes the initial environment and the instruction of the task, and (c) {Reflection Results} encapsulates the self-reflection results from the reflector model.</p>
<p>Table 1 :
1
On HotpotQA, our method enables a better usage of the training data compared with self-training and improves self-training for LLama-2/3-based agents.Also, adding only 0.5k GPT-generated data enables our agents with the free Wikipedia API to achieve comparable or better performance than methods with commercial APIs.
. The MBPP
Roziere et al. (2023)23), the agent model is given the unit test cases during code generation.Similarly, the reflection model is given the agent generation and its unit test results as the environmental feedback, and then generates a corrected version.</p>
<p>Table 2 :
2
Results on the ALFWorld dataset.Re-ReST substantially increases the sampling accuracy and outperforms self-training in terms of success rate even upon employing a reflector.</p>
<p>Table 4 :
4
Table 3, for MBPP, because CodeLlama is trained on a large amount of code Re-ReST can outperform self-training in text-to-image generation when applied to VPGen and evaluated with VPEval (Cho et al., 2023) on multiple dimensions.
ModelMBPP Sample Acc.P@1Sample Acc.Score GQAZero-Shot-48.6-40.9Self-Training66.954.544.741.9Re-ReST77.356.455.742.6Table 3: Re-ReST improves self-training on code gen-eration and visual programming tasks.generation corpus, the base CodeLlama model canachieve a decent performance without any fine-tuning. The high pass rate results in many of thetraining instances being used for self-training. Af-ter self-training on the MBPP training data, themodel performance can be improved from 48.6%to 54.5%. The reflector model can generate moreself-training data and the pass rate can be improvedwith the reflector-generated data. For GQA, simi-lar improvements can be seen, indicating that ourmethod is also applicable in visual programming.3.4 Text-to-Image Generation
Cho et al. (2023)onduct experiments in text-toimage generation.Specifically, we use the dataset constructed byCho et al. (2023).Their dataset evaluates the model's generated images in multiple dimensions and has training data for the spatial, Figure 3: In self-training, increasing the number of generations per instance initially improves model performance, but this effect plateaus.Additionally, both model performance and the number of solved training instances are lower than with Re-ReST, indicating our reflector can efficiently and effectively generate highquality self-training data.scale,andcount dimensions.For each dimension, the evaluation set consists of 1,000 instances.The training dataset consists of 36,920/18,200/1,560 instances for the spatial/scale/count dimensions.Training and Evaluation Setup.We use VP-Gen to perform inference on their training data, and evaluate the generations using VPEval(Cho</p>
<p>Table 5 :
5
While directly using a pretrained LLM as our reflector improves self-training, training the reflector specifically for self-correction further improves the agent performance.
ModelEMBase27.6S.C. (6 agents)30.8S.C. (3 agents + 3 reflectors)32.0Oracle (3 agents + 3 reflectors)36.8
(Li et al., 2023a)a)k relies on ground-truth feedback for test-time reflection (Oracle).In contrast, we propose to use self-consistency(Wang et al., 2023a)to enable our reflector to be applied during inference without ground-truth feedback and achieve improvements, demonstrating the potential of applying our method during the test time.etal., 2023).Specifically, during evaluation, a visual question answering model (BLIP-2(Li et al., 2023a)) is used to determine if the generated images correctly capture the input text information.</p>
<p>Table 4
4, our method con-
training is applied, further improved significantly with our Re-ReST method across all the dimensions.The results demonstrate promising applications of our model in the multimodal generation domain with a language agent as a backend.</p>
<p>Table 7 :
7
Our method is compatible with direct preference optimization (DPO)</p>
<p>Table 9 :
9
Prompt template for the HotpotQA agent.{In-context examples} {Input question} Prompt Solve a question answering task with a reflection on a previous trial and interleaving Thought, Action, Observation steps.Reflection can reason on a previous trial that failed and develop a new strategy to avoid repeating the same mistake.Thought can reason about the current situation, and Action can be three types: (1) Search[entity], which searches the exact entity on Wikipedia and returns the first paragraph if it exists.If not, it will return some similar entities to search.The Deliberate Stranger is a book about American serial killer Ted Bundy written by Seattle Times reporter Richard W. Larsen that was published in 1980.The book spawned a television miniseries of the same title, starring Mark Harmon as Bundy, that aired on NBC on May 4, 1986.Thought 2: The subject of the book, Ted Bundy, was executed in 1989.Action 2: Finish[1989] Reflection: I should have searched for Ted Bundy first before I finished answering the question.Thought 1: I need to first search The Deliberate Stranger and find its subject, and then find out which year the subject was executed.Action 1: Search[The Deliberate Stranger] Observation 1: The Deliberate Stranger is a book about American serial killer Ted Bundy written by Seattle Times reporter Richard W. Larsen that was published in 1980.The book spawned a television miniseries of the same title, starring Mark Harmon as Bundy, that aired on NBC on May 4, 1986.Thought 2: The subject of The Deliberate Stranger is Ted Bundy.I need to search for Ted Bundy and find out which year he was executed.
(2) Lookup[keyword], which returns the next sentence containing keyword in the last passage successfullyfound by Search.(3) Finish[answer], which returns the answer and finishes the task.You need to reflect on the previous trial and then restart a new trial.{In-context examples}Question: When was the subject of the book The Deliberate Stranger executed?Previous Trial:Thought 1: I need to search The Deliberate Stranger and find when the subject was executed.Action 1: Search[The Deliberate Stranger]Observation 1: MarieBurr', 'Murder of Georgann Hawkins', 'Zac Efron']Thought 3: I need to change the keyword for search.Action 3: Search[Ted Bundy executed]Observation 3: Theodore Robert Bundy (Cowell; November 24, 1946 January 24, 1989) was an Americanserial killer who kidnapped, raped and murdered dozens of young women and girls during the 1970s andpossibly earlier. After more than a decade of denials, he confessed to 30 murders committed in seven statesbetween 1974 and 1978. His true victim total is unknown.Bundy often employed charm to disguise hismurderous intent when kidnapping victims, and extended this tactic vis-vis law enforcement, the media andthe criminal justice system to maintain his claims of innocence...Thought 4: Ted Bundy was executed on January 24, 1989.Action 4: Finish[January 24, 1989]...UserQuestion: {Input question and previous trial}
Action 2: Search[Ted Bundy] Observation 2: Could not find [Ted Bundy].Similar: ['Ted Bundy', 'Ted Bundy (film)', 'Ted Bundy: American Boogeyman', 'Conversations with a Killer: The Ted Bundy Tapes', 'Murder of Rita Curran', 'Ted Bundy: Falling for a Killer', 'Extremely Wicked, Shockingly Evil and Vile', 'Disappearance of Ann</p>
<p>Table 10 :
10
Prompt template for the HotpotQA reflector.{In-context examples} {Input question and previous trial}
PromptYou are an expert Python programmer, and here is your task: {input task}Your code should pass these tests:{unit tests}
Your code should start with a [PYTHON] tag and end with a [/PYTHON] tag.</p>
<p>Table 11 :
11
Prompt template for the MBPP agent.{unit tests} {input task} Prompt You are an AI Python assistant.You will be given the user input, your past incorrect function implementation, and a series of unit tests.Write your reflection on the function implementation and correct your implementation (copy the function signature and its docstring).
{In-context examples}[previous impl]:"'pythondef add(a: int, b: int):"""Given integers a and b, return the total value of a and b."""return a -b"'[unit test results from previous impl]:Tested passed:Tests failed:assert add(1, 2) == 3 # output: -1assert add(1, 2) == 4 # output: -1[reflection on previous impl]:</p>
<p>Table 12 :
12
Prompt template for the MBPP reflector.{In-context examples} {Input task and previous trial} Prompt class ImagePatch: """A Python class containing a crop of an image centered around a particular object, as well as relevant information.Methods --find(object_name: str)-&gt;List[ImagePatch] Returns a list of new ImagePatch objects containing crops of the image centered around any objects found in the image matching the object_name.simple_query(question: str=None)-&gt;str Returns the answer to a basic question asked about the image.If no question is provided, returns the answer to "What is this?".exists(object_name: str)-&gt;bool Returns True if the object specified by object_name is found in the image, and False otherwise.verify_property(property: str)-&gt;bool Returns True if the property is met, and False otherwise.best_text_match(string1: str, string2: str)-&gt;str Returns the string that best matches the image.crop(left: int, lower: int, right: int, upper: int)-&gt;ImagePatch Returns a new ImagePatch object containing a crop of the image at the given coordinates.
"""{Detailed API definition}{In-context examples}{Input question}</p>
<p>Table 13 :
13
Prompt template for the GQA agent.Full prompt is released in https://github.com/cvlab-columbia/viper/blob/main/prompts/benchmarks/gqa.prompt.{Detailed API definition} {Incontext examples} {Input question} Prompt Interact with a household to solve a task.Here is an example.</p>
<p>https://python.langchain.com/docs/integrations/tools/wikipedia
https://serpapi.com/
Prompt I am writing code to handle visual question answering tasks by calling computer vision APIs.My code is wrong, and I hope you can help correct it.{Input question and previous trial}Your response should start with your reasoning and analysis.Then, you should write the correct code wrapped in <code>python and</code>.The correct code should be a function with signature `def exe-cute_command(image) -&gt; str:-Below are the available APIs and some example usages: ```python class ImagePatch:"""A Python class containing a crop of an image centered around a particular object, as well as relevant information.Methods
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint</p>
<p>Anthony Michael Ahn, Noah Brohan, Yevgen Brown, Omar Chebotar, Byron Cortes, Chelsea David, Chuyuan Finn, Keerthana Fu, Karol Gopalakrishnan, Hausman, Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, Program synthesis with large language models. 2021arXiv preprint</p>
<p>Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, Shunyu Yao, FireAct: Toward language agent fine-tuning. 2023arXiv preprint</p>
<p>Self-play fine-tuning converts weak language models to strong language models. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu, 2024arXiv preprint</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with 90%* chatgpt quality. </p>
<p>Visual programming for text-to-image generation and evaluation. Jaemin Cho, Abhay Zala, Mohit Bansal, 2023NeurIPS</p>
<p>RAFT: Reward ranked finetuning for generative foundation model alignment. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Tong Shum Kashun, Zhang, 2023TMLR</p>
<p>ToRA: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, 2024ICLR</p>
<p>Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Reinforced Self-Training (ReST) for language modeling. 2023arXiv preprint</p>
<p>Revisiting self-training for neural sequence generation. Junxian He, Jiatao Gu, Jiajun Shen, Marc'aurelio Ranzato, ICLR. 2019</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, ICLR. 2022</p>
<p>Large language models can self-improve. Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, EMNLP. 2023</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 2024ICLR</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. Wenlong Huang, Pieter Abbeel, Deepak Pathak, Igor Mordatch, ICML. 2022</p>
<p>GQA: A new dataset for real-world visual reasoning and compositional question answering. Drew A Hudson, Christopher D Manning, CVPR. 2019</p>
<p>Aligner: Achieving efficient alignment through weak-to-strong correction. Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Juntao Dai, Yaodong Yang, 2024arXiv preprint</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. NAACL. Jacob Devlin, Ming-Wei Chang, Kenton , Lee Kristina, Toutanova , 2019</p>
<p>Language models can solve computer tasks. Geunwoo Kim, Pierre Baldi, Stephen Mcaleer, 2024NeurIPS</p>
<p>BLIP-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, ICML. 2023a</p>
<p>Self-alignment with instruction backtranslation. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason E Weston, Mike Lewis, ICLR. 2024</p>
<p>GLIGEN: Open-set grounded text-to-image generation. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, Yong Jae Lee, CVPR. 2023b</p>
<p>Chameleon: Plug-and-play compositional reasoning with large language models. Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao, 2023NeurIPS</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, 2023NeurIPS</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, WebGPT: Browser-assisted questionanswering with human feedback. 2021arXiv preprint</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Deep Learning for Code Workshop. 2022</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, 2022NeurIPS</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , 2023arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, 2023NeurIPS</p>
<p>Jonas Baptiste Roziere, Fabian Gehring, Sten Gloeckle, Itai Sootla, Gat, Ellen Xiaoqing, Yossi Tan, Jingyu Adi, Tal Liu, Jérémy Remez, Rapin, Code Llama: Open foundation models for code. 2023arXiv preprint</p>
<p>ToolFormer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023NeurIPS</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 2023NeurIPS</p>
<p>AlfWorld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, 2021ICLR</p>
<p>ViperGPT: Visual inference via python execution for reasoning. Dídac Surís, Sachit Menon, Carl Vondrick, ICCV. 2023</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv preprintet al. 2023.</p>
<p>Solving math word problems with process-and outcomebased feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, 2022arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, ICLR. 2023a</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, ACL. 2023b</p>
<p>Enable language models to implicitly learn self-improvement from data. Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu, Heng Ji, 2023carXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, 2022NeurIPS</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, ICLR. 2023</p>
<p>Lacma: Language-aligning contrastive learning with meta-actions for embodied instruction following. Cheng-Fu Yang, Yen-Chun Chen, Jianwei Yang, Xiyang Dai, Lu Yuan, Yu-Chiang Frank, Wang , Kai-Wei Chang, arXiv:2310.123442023arXiv preprint</p>
<p>HotpotQA: A dataset for diverse, explainable multi-hop question answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, EMNLP. 2018</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, 2023NeurIPS</p>
<p>ReAct: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, ICLR. 2022</p>
<p>LUMOS: Learning agents with unified data, modular design, and open-source llms. Faeze Da Yin, Abhilasha Brahman, Khyathi Ravichander, Kai-Wei Chandu, Yejin Chang, Bill Choi, Lin Yuchen, 2024ACL</p>
<p>Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, Jason Weston, Self-rewarding language models. 2024arXiv preprint</p>
<p>STaR: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, 2022NeurIPS</p>            </div>
        </div>

    </div>
</body>
</html>
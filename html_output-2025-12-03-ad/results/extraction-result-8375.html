<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8375 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8375</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8375</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-5969eff0e72e4a5bc0c7392c700be74a01ac2822</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/5969eff0e72e4a5bc0c7392c700be74a01ac2822" target="_blank">A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> A novel algorithm is presented by which neural networks may implement composition for any finite group via mathematical representation theory and it is shown that networks consistently learn this algorithm by reverse engineering model logits and weights.</p>
                <p><strong>Paper Abstract:</strong> Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned -- as well as the order they develop -- are arbitrary.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8375.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8375.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GCR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Group Composition via Representations (GCR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mechanistic algorithm by which networks compute composition in any finite group by embedding group elements as matrices (representations), using ReLU MLP activations to multiply these matrices to get rho(ab), and linearly unembedding via character (trace) computations tr(rho(ab) rho(c^{-1})) to produce logits peaked at c=ab.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>One-hidden-layer ReLU MLP and one-layer ReLU Transformer (small models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>MLP: left and right embeddings (d=256) into a 128-unit hidden ReLU MLP then linear unembed to n logits; Transformer: one-layer decoder-only ReLU Transformer, token embeddings d=128, 4 attention heads, MLP 512 units (small models trained from scratch on group composition).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Group composition tasks (includes modular addition as composition on cyclic groups C_n; composition on S5, S6, A5, dihedral groups D_n). Modular addition (C_113, C_118) is treated as a special case.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Embeddings memorize flattened representation matrices rho(a), rho(b) (i.e., matrix entries such as cos/sin for cyclic groups); MLP nonlinearity implements matrix multiplication to compute rho(ab) in hidden activations; unembedding stores rho(c^{-1}) and computes logits as characters tr(rho(ab) rho(c^{-1})), maximizing at correct c. The full pipeline is linear readout of characters after nonlinear production of rho(ab).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Logit similarity (correlating logits l(a,b,c) with character vectors chi_rho(ab c^{-1})); projection of embeddings/unembeddings/hidden activations onto representation subspaces; change-of-basis to recover explicit matrices from hidden subspace; neuron clustering (neurons grouped by representation); ablation tests (ablating representation-directed subspaces in embeddings/MLP/unembed; projecting to only algorithmic subspace; replacing neurons with explicit representation matrix elements); progress measures (restricted vs excluded loss) and monitoring training dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Mainline S5 MLP: logit similarity with chi_sign = 0.509, with chi_standard = 0.767; two key representations explain 84.8% of logit variance. Baseline test loss (main S5 run) ~2.38e-6. Table 3 (selected examples, averaged over seeds): C_113 MLP W_a FVE 99.53%, W_b 99.39%, W_U 98.05%; MLP neurons FVE 90.25% (by representation), rho(ab) component 12.03%; test loss 1.63e-05; C_118 test loss 5.39e-06; S_5 MLP W_a 100%, W_b 99.99%, W_U 94.14%; MLP FVE 88.91%; test loss 1.02e-05. Approximating logits using only key characters reduces loss by ~70% relative; projecting to only key unembed directions can decrease loss (~12% decrease for mainline ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Two classes of solutions occur: memorization circuits (fit training pairs) and generalizing GCR circuits; networks can get stuck in memorization and require a 'cleanup' phase (grokking) to shed memorization via weight decay. If only low-capacity or poorer representations (e.g., sign-only) are learned, generalization is poor. Multiplication implemented via ReLUs is approximate, producing residual variance (~12% residual in standard neurons) which is nonessential noise. Different seeds/architectures learn different sets of representations (non-uniqueness), causing variability and occasional failure to generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Multiple converging lines: (1) Logits correlate strongly with characters of a sparse set of irreducible representations (logit similarity numbers above). (2) Embedding and unembedding matrices project almost entirely onto small representation subspaces (e.g., W_a/W_b/W_U rank ~16+1 for S5 corresponding to two key irreps). (3) Hidden activations contain rho(ab); change-of-basis recovers representation matrices with MSE <1e-8 for standard and exact for sign; neurons cluster by representation. (4) Ablations: ablating rho_standard(ab) in MLP increases loss from ~2.38e-6 to 7.55; ablating rho_sign(ab) raises loss to 0.0009; restricting to only predicted algorithmic subspaces maintains or improves performance; replacing neurons with direct representation matrix elements reduces loss by ~70%. (5) Progress measures: restricted/excluded losses track the formation of GCR circuit and memorization respectively and explain grokking dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Not all theoretical irreps are learned (e.g., 6d for S5 never learned in runs). Which irreps become 'key' varies across random seeds and architectures (Transformers tended to learn fewer representations). Representations with higher dimension are sometimes learned despite an expectation of learning simpler ones; ordering of representation acquisition is not deterministic. Multiplication via single-layer ReLU is imperfect and yields residual directions that are non-essential or noisy. Some ablations of non-key directions improve performance, showing networks also store spurious components.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8375.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8375.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fourier/trig algorithm (modular addition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fourier Transform / Trigonometric Identity Algorithm for Modular Addition (prior work, mapped to GCR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An algorithm previously reverse-engineered for modular addition in Transformers in which embeddings encode cos(omega k * a), sin(...) terms (Fourier modes), MLP neurons compute cos/sin for sums a+b via trig identities, and logits are computed as sums of cos(omega (a+b-c)) across frequencies; in this paper recognized as the cyclic-group special case of GCR where irreducible representations are 2D rotation matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Progress measures for grokking via mechanistic interpretability</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>One-layer Transformer (as in Nanda et al. 2023) and small Transformers used here for cyclic groups</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only one-layer Transformer variants trained on modular addition (e.g., C_113) with similar small dimensions as used by Nanda et al.; in this paper both MLP and Transformer runs on cyclic groups replicate the Fourier-mode behavior but reinterpreted in representation-theoretic terms.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Modular addition (addition mod p), treated as composition in cyclic group C_n.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Embeddings represent distinct Fourier modes: cos(omega_k * a), sin(omega_k * a) which correspond to matrix elements of 2D irreducible representations of the cyclic group; MLP implements combination (sum) via trig identities producing cos(omega_k (a+b)) terms; unembed computes 2 cos(omega_k (a+b-c)) whose sum peaks at correct c.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>As in GCR: inspect logits vs predicted sin/cos characters; project embeddings and MLP activations onto frequency/mode subspaces; ablations and restricted/excluded loss metrics; mapping of Fourier modes to irreducible representations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>For cyclic groups (C_113, C_118) networks obtain very low test losses (Table 3: C_113 MLP test loss 1.63e-05; transformers even lower e.g., C_113 transformer test 2.67e-07), with large fractions of embedding/unembed variance explained by cyclic representation subspaces (e.g., W_a ~99.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Same memorization vs generalization dynamics (grokking). If insufficient set of frequencies/representations learned, generalization degrades. Some frequency modes may never be learned leading to poorer logits; learning order and selection vary by seed.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Direct mapping from sine/cosine terms found in embeddings/MLP activations to the 2D representation matrix elements; recovered characters equal 2 cos(omega_k (a+b-c)), matching predicted logit form; projections and ablations on cyclic tasks replicate Nanda et al.'s observations and fit into the GCR formalism.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Fourier/trig framing is specific to cyclic groups; the representation-theoretic GCR generalizes it. The same frequency-based algorithm need not be the chosen solution in non-cyclic groups, and networks may prefer other irreps or combinations; selection of frequencies (modes) varies across runs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8375.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8375.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grokking / Progress measures</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grokking phenomenon and progress measures (restricted vs excluded loss)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observation that models can suddenly generalize long after memorizing training data (grokking); progress measures (restricted loss tracking the generalizing circuit, excluded loss tracking memorization) are used to dissect training phases: memorization, circuit formation, cleanup, and stable generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Small MLPs and Transformers trained on group composition tasks (S5, C_113, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same architectures as above; trained with weight decay and full-batch gradient descent for many epochs leading to grokking dynamics in many runs.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Group composition tasks (including modular addition) where training uses subset of multiplication table and test requires generalization to unseen pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_representation</strong></td>
                            <td>Not a representation per se, but training dynamics where initially networks implement memorization circuits, later form generalizing GCR circuits (representation-theoretic), and then prune memorization via weight decay leading to sudden test accuracy increase (grokking).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_method</strong></td>
                            <td>Define and compute restricted loss (project MLP activations onto rho(ab) key-representation subspace and compute loss) and excluded loss (remove key-representation subspace from activations and compute training loss), monitor sum of squared weights, representation FVE over training, and evolution of logit similarity over epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirical dynamics: memorization (epochs 0-2k) reduces excluded/train loss; circuit formation (2.2k-87k) sees restricted loss start falling while excluded loss rises; cleanup (87k-120k) restricted loss drops further and test loss sharply drops (grokking). Exact loss numbers depend on run; see Table 3 test losses (e.g., S5 MLP test loss ~1.02e-05).</td>
                        </tr>
                        <tr>
                            <td><strong>error_types_or_failure_modes</strong></td>
                            <td>Grokking arises because memorization circuits initially dominate; until cleanup prunes memorization the generalizing circuit's benefits to test loss are masked. Networks sometimes plateau between grok phases if additional representations are learned later, causing multiple grok events.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Temporal alignment of restricted/excluded loss curves with representation learning in embeddings/MLP/unembed (Figures 6, 9, 10), decrease in sum-of-squared-weights during circuit formation/cleanup (weight decay effect), and ablation studies showing that restricting to predicted generalizing subspaces preserves performance while excluding them increases loss.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_challenges</strong></td>
                            <td>Progress measures assume memorization and generalization occupy distinct MLP subspaces; if memorization uses the same privileged subspace, measures may be confounded. The timing/order of representation acquisition can vary by seed producing variable grokking behaviors (including multiple grokking phases).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Progress measures for grokking via mechanistic interpretability <em>(Rating: 2)</em></li>
                <li>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets <em>(Rating: 2)</em></li>
                <li>In-context Learning and Induction Heads <em>(Rating: 1)</em></li>
                <li>A mathematical framework for transformer circuits <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8375",
    "paper_id": "paper-5969eff0e72e4a5bc0c7392c700be74a01ac2822",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [
        {
            "name_short": "GCR",
            "name_full": "Group Composition via Representations (GCR)",
            "brief_description": "A mechanistic algorithm by which networks compute composition in any finite group by embedding group elements as matrices (representations), using ReLU MLP activations to multiply these matrices to get rho(ab), and linearly unembedding via character (trace) computations tr(rho(ab) rho(c^{-1})) to produce logits peaked at c=ab.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "One-hidden-layer ReLU MLP and one-layer ReLU Transformer (small models)",
            "model_description": "MLP: left and right embeddings (d=256) into a 128-unit hidden ReLU MLP then linear unembed to n logits; Transformer: one-layer decoder-only ReLU Transformer, token embeddings d=128, 4 attention heads, MLP 512 units (small models trained from scratch on group composition).",
            "arithmetic_task_type": "Group composition tasks (includes modular addition as composition on cyclic groups C_n; composition on S5, S6, A5, dihedral groups D_n). Modular addition (C_113, C_118) is treated as a special case.",
            "mechanism_or_representation": "Embeddings memorize flattened representation matrices rho(a), rho(b) (i.e., matrix entries such as cos/sin for cyclic groups); MLP nonlinearity implements matrix multiplication to compute rho(ab) in hidden activations; unembedding stores rho(c^{-1}) and computes logits as characters tr(rho(ab) rho(c^{-1})), maximizing at correct c. The full pipeline is linear readout of characters after nonlinear production of rho(ab).",
            "probing_or_intervention_method": "Logit similarity (correlating logits l(a,b,c) with character vectors chi_rho(ab c^{-1})); projection of embeddings/unembeddings/hidden activations onto representation subspaces; change-of-basis to recover explicit matrices from hidden subspace; neuron clustering (neurons grouped by representation); ablation tests (ablating representation-directed subspaces in embeddings/MLP/unembed; projecting to only algorithmic subspace; replacing neurons with explicit representation matrix elements); progress measures (restricted vs excluded loss) and monitoring training dynamics.",
            "performance_metrics": "Mainline S5 MLP: logit similarity with chi_sign = 0.509, with chi_standard = 0.767; two key representations explain 84.8% of logit variance. Baseline test loss (main S5 run) ~2.38e-6. Table 3 (selected examples, averaged over seeds): C_113 MLP W_a FVE 99.53%, W_b 99.39%, W_U 98.05%; MLP neurons FVE 90.25% (by representation), rho(ab) component 12.03%; test loss 1.63e-05; C_118 test loss 5.39e-06; S_5 MLP W_a 100%, W_b 99.99%, W_U 94.14%; MLP FVE 88.91%; test loss 1.02e-05. Approximating logits using only key characters reduces loss by ~70% relative; projecting to only key unembed directions can decrease loss (~12% decrease for mainline ablation).",
            "error_types_or_failure_modes": "Two classes of solutions occur: memorization circuits (fit training pairs) and generalizing GCR circuits; networks can get stuck in memorization and require a 'cleanup' phase (grokking) to shed memorization via weight decay. If only low-capacity or poorer representations (e.g., sign-only) are learned, generalization is poor. Multiplication implemented via ReLUs is approximate, producing residual variance (~12% residual in standard neurons) which is nonessential noise. Different seeds/architectures learn different sets of representations (non-uniqueness), causing variability and occasional failure to generalize.",
            "evidence_for_mechanism": "Multiple converging lines: (1) Logits correlate strongly with characters of a sparse set of irreducible representations (logit similarity numbers above). (2) Embedding and unembedding matrices project almost entirely onto small representation subspaces (e.g., W_a/W_b/W_U rank ~16+1 for S5 corresponding to two key irreps). (3) Hidden activations contain rho(ab); change-of-basis recovers representation matrices with MSE &lt;1e-8 for standard and exact for sign; neurons cluster by representation. (4) Ablations: ablating rho_standard(ab) in MLP increases loss from ~2.38e-6 to 7.55; ablating rho_sign(ab) raises loss to 0.0009; restricting to only predicted algorithmic subspaces maintains or improves performance; replacing neurons with direct representation matrix elements reduces loss by ~70%. (5) Progress measures: restricted/excluded losses track the formation of GCR circuit and memorization respectively and explain grokking dynamics.",
            "counterexamples_or_challenges": "Not all theoretical irreps are learned (e.g., 6d for S5 never learned in runs). Which irreps become 'key' varies across random seeds and architectures (Transformers tended to learn fewer representations). Representations with higher dimension are sometimes learned despite an expectation of learning simpler ones; ordering of representation acquisition is not deterministic. Multiplication via single-layer ReLU is imperfect and yields residual directions that are non-essential or noisy. Some ablations of non-key directions improve performance, showing networks also store spurious components.",
            "uuid": "e8375.0",
            "source_info": {
                "paper_title": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Fourier/trig algorithm (modular addition)",
            "name_full": "Fourier Transform / Trigonometric Identity Algorithm for Modular Addition (prior work, mapped to GCR)",
            "brief_description": "An algorithm previously reverse-engineered for modular addition in Transformers in which embeddings encode cos(omega k * a), sin(...) terms (Fourier modes), MLP neurons compute cos/sin for sums a+b via trig identities, and logits are computed as sums of cos(omega (a+b-c)) across frequencies; in this paper recognized as the cyclic-group special case of GCR where irreducible representations are 2D rotation matrices.",
            "citation_title": "Progress measures for grokking via mechanistic interpretability",
            "mention_or_use": "use",
            "model_name": "One-layer Transformer (as in Nanda et al. 2023) and small Transformers used here for cyclic groups",
            "model_description": "Decoder-only one-layer Transformer variants trained on modular addition (e.g., C_113) with similar small dimensions as used by Nanda et al.; in this paper both MLP and Transformer runs on cyclic groups replicate the Fourier-mode behavior but reinterpreted in representation-theoretic terms.",
            "arithmetic_task_type": "Modular addition (addition mod p), treated as composition in cyclic group C_n.",
            "mechanism_or_representation": "Embeddings represent distinct Fourier modes: cos(omega_k * a), sin(omega_k * a) which correspond to matrix elements of 2D irreducible representations of the cyclic group; MLP implements combination (sum) via trig identities producing cos(omega_k (a+b)) terms; unembed computes 2 cos(omega_k (a+b-c)) whose sum peaks at correct c.",
            "probing_or_intervention_method": "As in GCR: inspect logits vs predicted sin/cos characters; project embeddings and MLP activations onto frequency/mode subspaces; ablations and restricted/excluded loss metrics; mapping of Fourier modes to irreducible representations.",
            "performance_metrics": "For cyclic groups (C_113, C_118) networks obtain very low test losses (Table 3: C_113 MLP test loss 1.63e-05; transformers even lower e.g., C_113 transformer test 2.67e-07), with large fractions of embedding/unembed variance explained by cyclic representation subspaces (e.g., W_a ~99.5%).",
            "error_types_or_failure_modes": "Same memorization vs generalization dynamics (grokking). If insufficient set of frequencies/representations learned, generalization degrades. Some frequency modes may never be learned leading to poorer logits; learning order and selection vary by seed.",
            "evidence_for_mechanism": "Direct mapping from sine/cosine terms found in embeddings/MLP activations to the 2D representation matrix elements; recovered characters equal 2 cos(omega_k (a+b-c)), matching predicted logit form; projections and ablations on cyclic tasks replicate Nanda et al.'s observations and fit into the GCR formalism.",
            "counterexamples_or_challenges": "Fourier/trig framing is specific to cyclic groups; the representation-theoretic GCR generalizes it. The same frequency-based algorithm need not be the chosen solution in non-cyclic groups, and networks may prefer other irreps or combinations; selection of frequencies (modes) varies across runs.",
            "uuid": "e8375.1",
            "source_info": {
                "paper_title": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Grokking / Progress measures",
            "name_full": "Grokking phenomenon and progress measures (restricted vs excluded loss)",
            "brief_description": "Observation that models can suddenly generalize long after memorizing training data (grokking); progress measures (restricted loss tracking the generalizing circuit, excluded loss tracking memorization) are used to dissect training phases: memorization, circuit formation, cleanup, and stable generalization.",
            "citation_title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
            "mention_or_use": "use",
            "model_name": "Small MLPs and Transformers trained on group composition tasks (S5, C_113, etc.)",
            "model_description": "Same architectures as above; trained with weight decay and full-batch gradient descent for many epochs leading to grokking dynamics in many runs.",
            "arithmetic_task_type": "Group composition tasks (including modular addition) where training uses subset of multiplication table and test requires generalization to unseen pairs.",
            "mechanism_or_representation": "Not a representation per se, but training dynamics where initially networks implement memorization circuits, later form generalizing GCR circuits (representation-theoretic), and then prune memorization via weight decay leading to sudden test accuracy increase (grokking).",
            "probing_or_intervention_method": "Define and compute restricted loss (project MLP activations onto rho(ab) key-representation subspace and compute loss) and excluded loss (remove key-representation subspace from activations and compute training loss), monitor sum of squared weights, representation FVE over training, and evolution of logit similarity over epochs.",
            "performance_metrics": "Empirical dynamics: memorization (epochs 0-2k) reduces excluded/train loss; circuit formation (2.2k-87k) sees restricted loss start falling while excluded loss rises; cleanup (87k-120k) restricted loss drops further and test loss sharply drops (grokking). Exact loss numbers depend on run; see Table 3 test losses (e.g., S5 MLP test loss ~1.02e-05).",
            "error_types_or_failure_modes": "Grokking arises because memorization circuits initially dominate; until cleanup prunes memorization the generalizing circuit's benefits to test loss are masked. Networks sometimes plateau between grok phases if additional representations are learned later, causing multiple grok events.",
            "evidence_for_mechanism": "Temporal alignment of restricted/excluded loss curves with representation learning in embeddings/MLP/unembed (Figures 6, 9, 10), decrease in sum-of-squared-weights during circuit formation/cleanup (weight decay effect), and ablation studies showing that restricting to predicted generalizing subspaces preserves performance while excluding them increases loss.",
            "counterexamples_or_challenges": "Progress measures assume memorization and generalization occupy distinct MLP subspaces; if memorization uses the same privileged subspace, measures may be confounded. The timing/order of representation acquisition can vary by seed producing variable grokking behaviors (including multiple grokking phases).",
            "uuid": "e8375.2",
            "source_info": {
                "paper_title": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Progress measures for grokking via mechanistic interpretability",
            "rating": 2
        },
        {
            "paper_title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
            "rating": 2
        },
        {
            "paper_title": "In-context Learning and Induction Heads",
            "rating": 1
        },
        {
            "paper_title": "A mathematical framework for transformer circuits",
            "rating": 1
        }
    ],
    "cost": 0.01643775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Toy Model of Universality: Reverse Engineering how Networks Learn Group Operations</h1>
<p>Bilal Chughtai ${ }^{1}$ Lawrence Chan ${ }^{2}$ Neel Nanda ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Universality is a key hypothesis in mechanistic interpretability - that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned - as well as the order they develop - are arbitrary.</p>
<h2>1. Introduction</h2>
<p>Do models converge on the same solutions to a task, or are the algorithms implemented arbitrary and unpredictable? The universality hypothesis (Olah et al., 2020; Li et al., 2016) asserts that models learn similar features and circuits across different models when trained on similar tasks. This is an open question of significant importance to the field of mechanistic interpretability. The field focuses on reverse engineering state-of-the-art models by identifying circuits (Elhage et al., 2021; Olsson et al., 2022; Nanda et al., 2023; Wang et al., 2022), subgraphs of networks consisting sets of tightly linked features and the weights between them.(Olah et al., 2020). Recently, the field of mechanistic interpretability has increasingly shifted towards finding small, toy models easier to interpret, and employing labor intensive approaches to reverse-engineering specific features</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. The algorithm implemented by a one hidden layer MLP for arbitrary group composition. Given two input group elements $a$ and $b$, the model learns representation matrices $\rho(a)$ and $\rho(b)$ in its embeddings. Using the ReLU activations in its MLP layer, it then multiplies these matrices, computing $\rho(a) \rho(b)=\rho(a b)$. Finally, it 'reads off' the logits for each output group element $c$ by computing characters - the matrix trace $\operatorname{tr} \rho\left(a b c^{-1}\right)$, denoted $\chi_{\rho}\left(a b c^{-1}\right)$, which is maximized when $c=a b$.
and circuits in detail (Elhage et al., 2021; Wang et al., 2022; Nanda et al., 2023). If the universality hypothesis holds, then the insights and principles found by studying small models will transfer to state-of-the-art models that are used in practice. But if universality is false, then although we may learn some general principles from small models, we should shift focus to developing scalable, more automated interpretability techniques that can directly interpret models of genuine interest.</p>
<p>In this work, we study to what extent the universality hypothesis is true by interpreting networks trained on composition of group elements in various finite groups ${ }^{1}$. We focus on composition of arbitrary groups as this defines a large family of related tasks, forming an algorithmic test bed for investigating universality. We first exhibit a general algorithm by which networks can compute compositions of elements in an arbitrary finite group, using concepts from the mathematical field of representation ${ }^{2}$ and character theory. We</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>do this by building upon the work of Nanda et al. (2023), that reverse-engineered networks trained to grok modular addition ${\bmod p}$ and found the networks used a Fourier transform and trigonometry (trig) identity based algorithm to compute logits. We show that this ad-hoc, trig identitybased algorithm is a special case of our algorithm and that distinct Fourier modes are better thought of as distinct irreducible representations of the cyclic group. Our algorithm and how we find it implemented in network components is described in Figure 1.</p>
<p>Representation theory bridges linear algebra and group theory, and studies how group elements can be thought of as matrices. At a high level, our algorithm embeds group elements as such matrices, uses its ReLU activations to perform matrix multiplication, and uses the unembed to convert back to group elements. We prove correctness of our algorithm using results from representation theory in Section 4.</p>
<p>We verify our understanding of a model trained to perform group composition with four lines of evidence in Section 5. (1) the logits are as predicted by the algorithm over a set of key representations $\rho$. (2) the embeddings and unembeddings purely consist of a memorized lookup table, converting the inputs and outputs to the relevant representation matrices $\rho(a), \rho(b)$ and $\rho\left(c^{-1}\right)$. (3) the MLP neurons calculate $\rho(a b)$, and we can explicitly extract these representation matrices from network activations. Further, we can read off the neuron-logit map directly from weights, and neurons cluster by representation. (4) ablating the components of weights and activations predicted by our algorithm destroys performance, while ablating parts we predict are noise does not affect loss, and often improves it.</p>
<p>Finally, we use our mechanistic understanding of models to investigate the universality hypothesis in Section 6. We break universality down into strong and weak forms. Strong universality claims that the same features and circuits arise in all models that are trained in similar ways; weak universality claims that there are underlying principles to be understood, but that any given model will implement these principles in features and circuits in a somewhat arbitrary way. While models consistently implement our algorithm across groups and architectures by learning representationtheoretic features and circuits, we find that the choice of specific representations used by networks varies considerably. Moreover, the number of representations learned and order of representations learned is not consistent across different hyperparameters or random seeds. We consider this to be compelling evidence for weak universality, but against strong universality: interpreting a single network is insufficient for understanding behavior across networks.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Train (blue) and test (red) accuracy (left) and train and test loss (right) of an MLP trained on group composition on $S_{5}$, the permutation group of order 5, over 50 random seeds. These models consistently exhibit grokking: they quickly overfit early in training, but then suddenly generalize much later. The bolded line denotes average accuracy/loss.</p>
<h2>2. Related Work</h2>
<p>Comparing Neural Representations. In the past several years, a wide variety of post-hoc approaches have been used to study the relationship between the representations learned by neural networks, initiated by Li et al. (2016). Methods often compare internal representations of one network to another, though it is unclear whether these methods truly measure what we want, as networks are highly non linear and may learn similar features in different ways. Empirically however, techniques such as Canonical Correlation Analysis (Morcos et al., 2018), Centered Kernel Alignment (Kornblith et al., 2019) and variations are able to quantify representation similarity. Other techniques used include model stitching (Bansal et al., 2021) and neuroscience-inspired methods (Mehrer et al., 2020).</p>
<p>Mechanistic Interpretability and Universality. In contrast, we are able to compare the learned representations of models to a known ground truth, through first reverse engineering the employed algorithm completely and thereby understanding the full set of features. We employ a Circuitsbased mechanistic interpretability approach, as pioneered by Cammarata et al. (2020), Elhage et al. (2021) and Olsson et al. (2022). In mechanistic interpretability, neural representation similarity is studied together with algorithm similarity under the term 'universality'. Olah et al. (2020) demonstrated the universality hypothesis in image models through the presence of curve detector and high-low frequency detector features in early layers of many models, while also showing the circuits implementing them are analogous.</p>
<p>Group Theory. Group theoretic tasks have in the past been used to probe the capability of neural networks to perform symbolic and algorithmic reasoning. (Zhang et al., 2022) evaluate and fine tune language models to implement group actions in context. Liu et al. (2022a) study how Transformers learn group theoretic automata.</p>
<p>Phase Changes and Emergence. Recent work has observed emergent behavior in neural networks: models often</p>
<p>quickly develop qualitatively different behavior as they are scaled up (Ganguli et al., 2022; Wei et al., 2022). Brown et al. (2020) find that, while total loss scales predictably with model size, models' ability to perform specific tasks can change abruptly with scale. McGrath et al. (2022) find that AlphaZero quickly learns many human chess concepts between 10k and 30k training steps and reinvents human opening theory between 25 k and 60 k training steps.</p>
<p>Grokking. Grokking is a form of emergence, first reported by (Power et al., 2022), who trained small networks on algorithmic tasks, finding that test accuracy often increased sharply, long after maximizing train accuracy. Liu et al. (2022b) construct further small examples of grokking, which they use to compute phase diagrams with four separate 'phases' of learning. Davies et al. (2022) unify the phenomena of grokking and double descent as instances of phenomena dependent on 'pattern learning speeds'. Our findings agree with Liu et al. (2022c) in that grokking seems intrinsically linked to the relationship between performance and weight norms; and with Barak et al. (2023) and Nanda et al. (2023) in showing that the networks make continuous progress toward a generalizing algorithm, which may be tracked over training using continuous progress measures.</p>
<h2>3. Setup and Background</h2>
<h3>3.1. Task Description</h3>
<p>We train models to perform group composition on finite groups $G$ of order $|G|=n$. The input to the model is an ordered pair $(a, b)$ with $a, b \in G$ and we train to predict the group element $c=a b$. In our mainline experiment, we use an architecture consisting of left and right embeddings ${ }^{3}$, a one hidden layer MLP, and unembedding $W_{U}$. This architecture is presented in Figure 1 and elaborated upon in Appendix C. We note that the task presented in Nanda et al. (2023) is a special case of our task, as addition mod 113 is equivalent to composition for $G=C_{113}$, the cyclic group of 113 elements. We train our models in a similar manner to Nanda et al. (2023), details may be found in Appendix C.</p>
<h3>3.2. Mathematical Representation Theory</h3>
<p>The core claims of our work build on a rich sub-field of pure mathematics named Representation Theory. We introduce the key definitions and results used throughout here, but discuss and motivate other relevant results in Appendix D. Further details and proofs beyond this may be found in e.g. Alperin \&amp; Bell (1995).</p>
<p>A (real) representation is a homomorphism, i.e. a map preserving the group structure, $\rho: G \rightarrow G L\left(\mathbb{R}^{d}\right)$ from the group $G$, to a $d$-dimensional general linear group, the set of invertible square matrices of dimension $d$. Representations</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>are in general reducible, in a manner we make precise in the Appendix. For each group $G$, there exist a finite set of fundamental irreducible representations. The character of a representation is the trace of the representation $\chi_{\rho}: G \rightarrow \mathbb{R}$ given by $\chi_{\rho}(g)=\operatorname{tr}(\rho(g))$. A key fact our algorithm depends on is that character's are maximal when $\rho(g)=I$, the identity matrix (Theorem D.7). In particular, the character of the identity element, $\chi_{\rho}(e)$, is maximal.</p>
<p>Example. The cyclic group $C_{n}$ is generated by a single element $r$ and naturally represents the set of rotational symmetries of an n-gon, where $r$ corresponds to rotation by $2 \pi / n$. This motivates a 2 dimensional representation - a set of $n 2 \times 2$ matrices, one for each group element:</p>
<p>$$
\rho\left(r^{k}\right)=\left(\begin{array}{cc}
\cos \left(\frac{2 \pi k}{n}\right) &amp; -\sin \left(\frac{2 \pi k}{n}\right) \
\sin \left(\frac{2 \pi k}{n}\right) &amp; \cos \left(\frac{2 \pi k}{n}\right)
\end{array}\right)
$$</p>
<p>for element $r^{k}$, corresponding to rotation by $\theta=2 \pi k / n$. This representation is irreducible, since there is no subspace of $\mathbb{R}^{2}$ on which the set of rotation matrices restricts - they each rotate the whole space. The character of each representation element is the trace $\chi_{\rho}\left(r^{k}\right)=2 \cos \theta$, which is maximized at $\theta=0$, where the group element $r^{0}=e$ and corresponding matrix $I_{2}$ are both the identity.</p>
<h2>4. An Algorithm for Group Composition</h2>
<p>We now present an algorithm, which we call group composition via representations (GCR), on an arbitrary group $G$ equipped with a representation $\rho$ of dimension $d$. The algorithm and it's map onto network components are described in Figure 1. We are not aware of this algorithm existing in any prior literature.
(1) Map inputs $a$ and $b$ to $d \times d$ matrices $\rho(a), \rho(b)$.
(2) Compute the matrix product $\rho(a) \rho(b)=\rho(a b)$.
(3) For each output logit $c$, compute the characters $\operatorname{tr}\left(\rho(a b) \rho\left(c^{-1}\right)\right)=\operatorname{tr}\left(\rho\left(a b c^{-1}\right)\right)=\chi_{\rho}\left(a b c^{-1}\right)$.
Crucially, Theorem D. 7 implies $a b \in \operatorname{argmax}<em _rho="\rho">{c} \chi</em>=e$. If $\rho$ is faithful (see Definition D.5), this argmax is unique.}\left(a b c^{-1}\right)$, so that logits are maximised on $c^{*}=a b$, where $a b c^{-1</p>
<p>In our networks, we find the terms $\rho(a)$ and $\rho(b)$ in the embeddings and $\rho(a b)$ in MLP activations. Note, as $\rho(a b)$ is present in the final hidden layer activations and $W_{U}$ learns $\rho\left(c^{-1}\right)$ in weights, the map to logits is entirely linear:</p>
<p>$$
\rho(a b) \rightarrow \operatorname{tr} \rho(a b) \rho\left(c^{-1}\right)=\sum_{i j}\left(\rho(a b) \odot \rho\left(c^{-1}\right)^{T}\right)_{i j}
$$</p>
<p>where $\odot$ denotes the element-wise product of matrices.
Each finite group $G$ is equipped with a finite set of $k$ irreducible representations (Definition D.2) Since any representation may be decomposed into a finite set of irreducible</p>
<p>representations (Theorems D. 3 and D.4) we may restrict our attention to these irreducible representations. It is then useful to think about our algorithm for a fixed group $G$ as a family of $k$ independent circuits indexed by choice irreducible representation $\rho$. In general, a single network may choose any subset of these $k$ circuits to implement, so that the observed logits are a linear combination of characters from multiple representations. From now on, each representation may be assumed to be irreducible, and we will drop the word. Since each representation has $\chi_{\rho}\left(a b c^{-1}\right)$ maximized on the correct answers, using multiple representations gives constructive interference at $c^{<em>}=a b$, giving $c^{</em>}$ a large logit. Theorem D. 9 implies characters are orthogonal over distinct representations, a fact we use in Section 5.1.</p>
<p>Example. Our GCR algorithm is a generalization of the seemingly ad-hoc algorithm presented in Nanda et al. (2023) for modular addition, which in our framing is composition on the cyclic group of 113 elements, $C_{113}$. Each element of our algorithm maps onto their Fourier multiplication algorithm, with representations $\rho=\mathbf{2}<em k="k">{\mathbf{k}}$ (which we define in Appendix D.1.1) corresponding to frequency $\omega</em>$.}=\frac{2 \pi k}{n</p>
<p>Nanda et al. (2023) found embeddings learn the terms $\cos \left(\omega_{k} a\right), \sin \left(\omega_{k} a\right), \cos \left(\omega_{k} b\right)$ and $\sin \left(\omega_{k} b\right)$, precisely the matrix elements of $\rho(a)$ and $\rho(b)$. The terms $\cos \left(\omega_{k}(a+b)\right)$ and $\sin \left(\omega_{k}(a+b)\right)$ found in the MLP neurons correspond directly to the matrix elements of $\rho(a b)$. Finally we find by direction calculation, or by using the group homomorphism property of representations, that the characters:</p>
<p>$$
\begin{aligned}
&amp; \chi\left(a b c^{-1}\right) \
&amp; =\operatorname{tr}\left(\rho\left(a b c^{-1}\right)\right) \
&amp; =\operatorname{tr}\left(\begin{array}{cc}
\cos \left(\omega_{k}(a+b-c)\right) &amp; -\sin \left(\omega_{k}(a+b-c)\right) \
\sin \left(\omega_{k}(a+b-c)\right) &amp; \cos \left(\omega_{k}(a+b-c)\right)
\end{array}\right) \
&amp; =2 \cos \left(\omega_{k}(a+b-c)\right)
\end{aligned}
$$</p>
<p>are precisely the form of logits found, which summed over many key frequencies $k$, corresponding to distinct irreducible representations.</p>
<h2>5. Reverse Engineering Permutation Group Composition in a One Layer ReLU MLP</h2>
<p>We follow the approach of Nanda et al. (2023) in reverse engineering a single mainline model trained on a fixed group, and then showing our interpretation is robust and generic later in Section 6, by analyzing models of different architectures trained on composition on several different groups, over different random initializations. We produce several lines of mechanistic evidence that the GCR algorithm is being employed, mostly mirroring those in Nanda et al. (2023).</p>
<p>In our mainline experiment, we train the MLP architecture described in Section 3.1 on the permutation (or symmetric) group of 5 elements, $S_{5}$, of order $|G|=n=120$. Note that unlike $C_{113}$ studied by Nanda et al. (2023), $S_{5}$ is not abelian, so the composition is non-commutative. We present a detailed analysis of this case as symmetric groups are in some sense the most fundamental group, as every group is isomorphic to a subgroup of a symmetric group (Cayley's Theorem D.10). So, understanding composition on the symmetric group implies understanding, in theory, of composition on any group. The (non trivial) irreducible representations of $S_{5}$ are named sign, standard, standard_sign, 5d_a, 5d_b, and 6d, are of dimensions $d={1,4,4,5,5,6}$ and are listed in Appendix D.1.3.</p>
<p>The GCR algorithm predicts that logits are sums of characters. This is a strong claim, which we directly verify in a black-box manner - we need not peer directly into network internals to check this. We do so by comparing the model's logits $l(a, b, c)$ on all input pairs $(a, b)$ and outputs $c$ with the algorithms character predictions $\chi_{\rho}\left(a b c^{-1}\right)$ for each representation $\rho$. We find the logits can be explained well with only a very sparse set of directions in logit space, corresponding to the characters of the 'standard' and 'sign' representations. From now on we call these two representations the key representations.</p>
<p>The remainder of our approaches are white-box and involve direct access to internal model weights and activations. First, we inspect the mechanisms implemented in model weights. We find the embeddings and unembeddings to be memorized look up tables, converting inputs and outputs to the relevant representation element in the key representations. As the number of representations learned is low, the embedding and unembedding matrices are low rank.</p>
<p>We then find MLP activations calculate $\rho(a b)$, and are able to explicitly extract these representation matrices. Additionally, MLP neurons cluster into distinct representations, and we can read off the linear map from neurons to logits as being precisely the final step of the GCR algorithm.</p>
<p>Finally, we use ablations to confirm our interpretation is faithful. We ablate components predicted by our algorithm to verify performance is hampered, and ablate components predicted to be noise, leaving only our algorithm, and show performance is maintained.</p>
<h3>5.1. Logit Attribution</h3>
<p>Logit similarity. We call the correlation between the logits $l(a, b, c)$ and characters $\chi_{\rho}\left(a b c^{-1}\right)$ the logit similarity. We call representations with logit similarity (see Appendix E.5) greater than $0.005$ 'key'.</p>
<p>Our model has logit similarity 0.509 with $\chi_{\text {sign }}$ and 0.767 with $\chi_{\text {standard }}$, and zero with all other representation characters. Theorem D. 8 implies these character vectors are</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. The observed $0^{\text {th }}$ logit (left) over all pairs of inputs $a$ (yaxis) and $b$ (x-axis). The GCR algorithm's logit predictions $\chi_{\text {sign }}$ (middle) and $\chi_{\text {standard }}$ (right) in the key representations. The observed logit appears to be a linear combination of the characters in the key representations. Note that all logits here have been normalized to range $[-1,1]$.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Evolution of logit similarity over training for each of the six non trivial representations. We see the sign representation is learned around epoch 250, and the standard around epoch 50k. None of the other representations contribute to logits via the GCR algorithm at the end of training. We therefore call the sign and standard representations 'key'.
orthogonal, so we may approximate the logits with these two directions. Doing so explains $84.8 \%$ of the variance of logits. This is surprising - the 120 output logits are explained well by only two directions. As confirmation for the correctness of our algorithm, if we evaluate test loss only using this logit approximation, we see a reduction in loss by $70 \%$ relatively. If we ablate the remaining $15 \%$ of logits, loss does not change.</p>
<h3>5.2. Embeddings and Unembeddings</h3>
<p>Each representation is a set of $n d \times d$ matrices, which by flattening we can think of as a set of $d^{2}$ vectors of dimension $n$. We call the subspace of $\mathbb{R}^{n}$ spanned by these vectors representation space. Theorem D. 9 implies these subspaces are orthogonal for distinct representations, and Theorem D. 3 implies the direct sum of each of these subspaces over all representations is $\mathbb{R}^{n}$. Any embedding or unembedding of $n$ group elements lies in $\mathbb{R}^{n \times h}$ for some $h$, so a natural operation is to project embeddings and unembeddings onto representation space over the $n$ dimension. Our definitions of embedding matrices $W_{a}, W_{b}$ and $W_{U}$ may be found in Appendix C.1, and details regarding how we perform the
projection in Appendix E.5.
We find evidence of representations in embeddings and unembeddings. We find that the embedding matrices and the unembed matrix are well approximated by a sparse set of representations (Table 1), and that the representations contained in all three are the same. This is surprising: each embedding and unembedding can potentially be of rank 120 , but is only of rank $16+1$, corresponding precisely to the two key representations. Qualitatively, the progress of representation learning is similar across all three embedding and unembedding matrices, with each representation being learned suddenly at roughly the same time, see Figure 9.</p>
<p>Table 1. Percentage of embedding matrices explained by subspaces corresponding to representations. We see the same two key representations explain almost all of the variance of each embedding matrix, and the non-key representations explain almost none.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$W_{a}$</th>
<th style="text-align: center;">$W_{b}$</th>
<th style="text-align: center;">$W_{U}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SIGN</td>
<td style="text-align: center;">$6.95 \%$</td>
<td style="text-align: center;">$6.95 \%$</td>
<td style="text-align: center;">$9.58 \%$</td>
</tr>
<tr>
<td style="text-align: left;">STANDARD</td>
<td style="text-align: center;">$93.0 \%$</td>
<td style="text-align: center;">$93.0 \%$</td>
<td style="text-align: center;">$84.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">RESIDUAL</td>
<td style="text-align: center;">$0.00 \%$</td>
<td style="text-align: center;">$0.00 \%$</td>
<td style="text-align: center;">$5.96 \%$</td>
</tr>
</tbody>
</table>
<h3>5.3. MLP Neurons</h3>
<p>MLP neurons calculate $\rho(a b)$. From the embeddings, neurons have inputs $\rho(a)$ and $\rho(b)$, and use their non-linearity to calculate $\rho(a b)$. We make this calculation explicit in the 1d case in Appendix E.2. To demonstrate this, we follow the approach taken with embeddings. We define for each representation a hidden representation subspace of rank $d^{2}$ of $\mathbb{R}^{n^{2}}$, and consider the projection of the hidden layer onto these subspaces.</p>
<p>Neurons cluster by representation. Our neurons cluster into disjoint categories, corresponding to key representations. This clustering is identical on neuron inputs and outputs. 7 neurons are 'sign neurons': these neurons completely represent $\rho_{\text {sign }}(a)$ in the left embedding and $\rho_{\text {sign }}(b)$ in the right embedding. On post-activation outputs, they represent some linear combination of $\rho_{\text {sign }}(a), \rho_{\text {sign }}(b)$, and $\rho_{\text {sign }}(a b)$, but not any other representation. 119 neurons are correspondingly 'standard neurons'. The final 2 neurons are always off.</p>
<p>In Table 2 we find $88.0 \%$ of the variance of standard neurons can be explained by the directions corresponding to $\rho(a), \rho(b)$ and $\rho(a b)$. For sign neurons, this fraction of variance of neurons explained is $99.9 \%$. We validate by ablation the residual $12.0 \%$ of standard neurons does not affect performance. We hypothesize this term is a side product of the network performing multiplication with a single ReLU, and discuss this multiplication step further in Appendix E.2. Evolution of percentage of MLP activations explained by each representation is presented in Figure 10.</p>
<p>Table 2. Percentage of the variance of MLP neurons explained by subspaces corresponding to representations of group elements $a$, $b$ and $a b$. Almost all of the variance of neurons within each key representation cluster is explained by subspaces corresponding to the representation, and all neurons are in a single cluster.</p>
<table>
<thead>
<tr>
<th>CLUSTER</th>
<th>$\rho(a)$</th>
<th>$\rho(b)$</th>
<th>$\rho(a b)$</th>
<th>RESIDUAL</th>
</tr>
</thead>
<tbody>
<tr>
<td>SIGN</td>
<td>$33.3 \%$</td>
<td>$33.3 \%$</td>
<td>$33.3 \%$</td>
<td>$0.00 \%$</td>
</tr>
<tr>
<td>STANDARD</td>
<td>$39.6 \%$</td>
<td>$37.1 \%$</td>
<td>$11.3 \%$</td>
<td>$12.1 \%$</td>
</tr>
</tbody>
</table>
<p>Only the $\rho(a b)$ component of MLP neurons is important. The GCR algorithm doesn't make use of $\rho(a)$ or $\rho(b)$ directly to compute $\chi_{\rho}\left(a b c^{-1}\right)$. We confirm the model too only makes use of $\rho(a b)$ type terms by ablating directions corresponding to $\rho(a)$ and $\rho(b)$ or otherwise in MLP activations and verifying loss doesn't change.</p>
<p>On the other hand, ablating directions corresponding to $\rho(a b)$ in the key representations severely damages loss. Baseline loss is $2.38 \times 10^{-6}$. Ablating $\rho_{\text {standard }}(a b)$ increases loss to 7.55 , while ablating $\rho_{\text {sign }}(a b)$ increases loss to 0.0009 .</p>
<p>We may explicitly recover representation matrices from hidden activations. By changing basis (via Figure 11) on the hidden representation subspace corresponding to $\rho(a b)$, we may recover the matrices $\rho(a b)$. The learned sign representation matrices agree with $\rho_{\text {sign }}(a b)$ completely, and the learned standard representation matrices agree with $\rho_{\text {standard }}(a b)$ with MSE loss $&lt;10^{-8}$. We cannot recover representation matrices for representations not learned.</p>
<h3>5.4. Logit Computation</h3>
<p>Maps to the logits are localised by representation. The unembedding map $W_{U}$ restricts to each key representation neuron cluster. This restricted map, following a similar approach to Section 5.2, has almost all components in the corresponding output representation subspace. Defining $W_{\rho}$ as the map from $\rho$-neurons to logits, we find $W_{\text {sign }}$ has $99.9 \%$ variance explained by output sign representation space, and $W_{\text {standard }}$ has $93.4 \%$ explained by output standard representation space.</p>
<p>The linear map in representation basis. As noted in Section 4, the final step of the GCR algorithm may be implemented in a single linear operation (Equation 1). Given $\rho(a b)$ is present in MLP neurons, the unembedding need simply learn the inverse representation matrices $\rho\left(c^{-1}\right)$. We verify the network implements this step as predicted by our algorithm in Figure 5.</p>
<h3>5.5. Correctness Checks: Ablations</h3>
<p>In previous sections, we showed various components of the model were well approximated by intermediate terms of the
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. The map from the subspace corresponding to $\rho_{\text {standard }}(a b)$ in the MLP neurons to logits. We obtain this by changing basis of $W_{U}$ on both sides, to align with $\rho(a b)$ representation space on the left, and $\rho\left(c^{-1}\right)$ on the right. This matrix implements step 3 in the GCR algorithm, mapping $\rho(a b)$ to $\chi_{\rho}\left(a b c^{-1}\right)=\operatorname{tr}\left(\rho(a b) \rho\left(c^{-1}\right)\right)$. The sparse and uniform matrix shown corresponds precisely to the trace calculation between two $4 \times 4$ matrices as in Equation 1.
proposed GCR algorithm. To verify these approximations are faithful, we perform two types of additional ablations. We exclude components in the algorithm and verify loss increases, and we restrict to these same components and demonstrate loss remains the same or decreases.</p>
<p>MLP neurons. In Section 5.3, we identified sets of neurons that could be manipulated to recover representation matrix elements $\rho(a b)$. If we replace these neurons with the corresponding representation matrix elements directly, we find loss decreases by $70 \%$ (to $7.00 \times 10^{-7}$ ).</p>
<p>Unembeddings. In Section 5.4, we found $W_{U}$ is well approximated by $16+1$ directions, corresponding to representation space on the two key representations. If we project MLP neurons to only these directions, ablating the $5.96 \%$ residual in $W_{U}$, we find loss decreases by $12 \%$, while if we project to only this residual, loss increases to 4.80 , random.</p>
<p>Logits. In Section 5.1 we found observed logits were well approximated by the GCR algorithm in the key representations. We find ablating our algorithm's predictions in the key representations damages loss, to 0.0006 by excluding the sign representation, to 7.23 excluding the standard representation, and to 7.60 excluding both, significantly worse than random. Ablating other directions improves performance.</p>
<h3>5.6. Understanding Training Dynamics using Progress Measures</h3>
<p>A limitation of prior work on using hidden progress measures from mechanistic explanations as a methodology for understanding emergence (Nanda et al., 2023) is that the technique developed may not generalize beyond one specific task. We demonstrate their results are robust by replicating them in our network trained on $S_{5}$.</p>
<p>We argue that the network implements two classes of cir-</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Evolution of the two progress measures over training. The vertical lines delineate 3 phases of training: memorization, circuit formation, and cleanup (and a final stable phase). Excluded loss tracks the progress of the memorization circuit, and accordingly falls during the first phase, rising after during circuit formation and cleanup. Restricted loss tracks the progress of the generalized algorithm, and has started falling by the end of circuit formation. Note that grokking occurs during cleanup, only after restricted loss has started to fall.
cuit - first, 'memorizing' circuits, and later, 'generalizing' circuits. Both are valid solutions on the training distribution. To disentangle these, we define two progress measures. Restricted loss tracks only the performance of the generalizing circuit via our algorithm. Excluded loss is the opposite, tracking the performance of only the memorizing circuit, and so is only evaluated on the training data. We find that on our mainline model, training splits into three partially overlapping phases - memorization, circuit formation, and cleanup. During circuit formation, the network smoothly transitions from memorizing to generalizing. Since test performance requires a general solution and no memorization, grokking occurs during cleanup. Further discussion may be found in Appendix E.1.</p>
<p>In our mainline experiments, we use weight decay as the primary regularization scheme. Other regularizers are also capable of exhibiting grokking. Our results mirror (Nanda et al., 2023): we find models grok generic group composition under dropout, and the methodology of progress measures can too be used to understand grokking in this case.</p>
<p>We sometimes find further phase changes. Figure 14 demonstrates two phases of grokking in a seperate run, caused by learning of different representations at distinct times.</p>
<h2>6. Universality</h2>
<p>In this section, we investigate to what extent the universality hypothesis (Olah et al., 2020; Li et al., 2016) holds on our collection of group composition tasks. Here, 'features' correspond to irreducible representations of group elements ${ }^{4}$ and 'circuits' correspond to precisely how networks manipulate these with their weights.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. (Left) The number of times each representation is learned over 50 seeds, for $S_{5}$ trained on the MLP architecture. We see the 1d sign and 4d standard representations are most commonly learned, standard_sign (4d), 5d_a and 5d_b are learned approximately equally and less often, and 6 d is never learned. (Right) The number of key representations of these 50 runs. Most commonly we have two key representations (typically sign and standard), but sometimes we learn more.</p>
<p>We interpret models of MLP and Transformer architectures (Appendix C) trained on group composition for seven groups: $C_{113}, C_{118}, D_{59}, D_{61}, S_{5}, S_{6}$ and $A_{5}$, each on four seeds. We find evidence for weak universality: our models are all characterized by a family of circuits corresponding to our GCR algorithm across all group representations. We however find evidence against strong universality: our models learn different representations, implying that specific features and circuits will differ across models.</p>
<p>All our networks implement the GCR algorithm. We first argue for weak universality via universality of our algorithm and universality of a family of features and circuits involving them. Following the approach of Section 5, we understand each layer of our network as steps in the algorithm as presented in Section 4. (Table 3). Steps 1 and 3 - we analyze embedding and unembedding matrices, showing that their fraction of variance explained (FVE) by subspaces corresponding to the key representations is high. Each group has its own family of representations, and each model learns its own set of key representations (i.e. representations with non-zero logit similarity). Where applicable, our metrics track only these key representations of any given model. For Step 2, we show the MLP activations are well explained by the terms $\rho(a), \rho(b)$, and importantly $\rho(a b)$ in the key representations. Finally, as evidence our algorithm is entirely responsible for performance, we show the final values of the progress measures of restricted and excluded loss.</p>
<p>Specific representations learned vary between random seeds. Each group has several representations that can be learned. Under strong universality, we would expect the representations learned to be consistent across random seeds when trained on the same group. In general, we do not find this to be true (Figure 7). When there are multiple valid solutions to a problem, the model somewhat arbitrarily chooses between them - even when the training data and architecture are identical.</p>
<p>Table 3. Results from all groups on both MLP and Transformer architectures, averaged over 4 seeds. We find that that features for matrices in the key representations are learned consistently, and explain almost all of the variance of embeddings and unembeddings. We find that terms corresponding to $\rho(a b)$ are consistently present in the MLP neurons, as expected by our algorithm. Excluding and restricting to these terms in the key representations damages performance/does not affect performance respectively.</p>
<table>
<thead>
<tr>
<th>Group</th>
<th>MLP</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>Transformer</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>FVE</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Loss</td>
<td></td>
<td></td>
<td>FVE</td>
<td></td>
<td></td>
<td></td>
<td>Loss</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>$W_{a}$</td>
<td>$W_{b}$</td>
<td>$W_{U}$</td>
<td>MLP</td>
<td>$\rho(a b)$</td>
<td>Test</td>
<td>Exc.</td>
<td>Res.</td>
<td>$W_{E}$</td>
<td>$W_{L}$</td>
<td>MLP</td>
<td>$\rho(a b)$</td>
<td>Test</td>
<td>Exc.</td>
<td>Res.</td>
</tr>
<tr>
<td>$C_{113}$</td>
<td>99.53\%</td>
<td>99.39\%</td>
<td>98.05\%</td>
<td>90.25\%</td>
<td>12.03\%</td>
<td>1.63e-05</td>
<td>5.95</td>
<td>6.88e-03</td>
<td>95.18\%</td>
<td>99.52\%</td>
<td>92.12\%</td>
<td>16.77\%</td>
<td>2.67e-07</td>
<td>9.42</td>
<td>2.12e-02</td>
</tr>
<tr>
<td>$C_{118}$</td>
<td>99.75\%</td>
<td>99.74\%</td>
<td>98.43\%</td>
<td>95.84\%</td>
<td>13.26\%</td>
<td>5.39e-06</td>
<td>8.72</td>
<td>3.60e-03</td>
<td>94.05\%</td>
<td>99.64\%</td>
<td>94.63\%</td>
<td>17.11\%</td>
<td>1.73e-07</td>
<td>15.93</td>
<td>2.55e-01</td>
</tr>
<tr>
<td>$D_{59}$</td>
<td>99.71\%</td>
<td>99.73\%</td>
<td>98.52\%</td>
<td>87.68\%</td>
<td>12.44\%</td>
<td>6.34e-06</td>
<td>12.37</td>
<td>1.60e-06</td>
<td>98.58\%</td>
<td>98.53\%</td>
<td>85.01\%</td>
<td>10.85\%</td>
<td>3.20e-06</td>
<td>46.42</td>
<td>2.82e-05</td>
</tr>
<tr>
<td>$D_{61}$</td>
<td>99.26\%</td>
<td>99.45\%</td>
<td>98.26\%</td>
<td>87.61\%</td>
<td>12.48\%</td>
<td>1.79e-05</td>
<td>12.00</td>
<td>1.69e-06</td>
<td>98.33\%</td>
<td>97.40\%</td>
<td>85.59\%</td>
<td>11.11\%</td>
<td>1.63e-02</td>
<td>41.64</td>
<td>9.60e-02</td>
</tr>
<tr>
<td>$S_{5}$</td>
<td>100.00\%</td>
<td>99.99\%</td>
<td>94.14\%</td>
<td>88.91\%</td>
<td>12.13\%</td>
<td>1.02e-05</td>
<td>11.72</td>
<td>2.21e-07</td>
<td>99.84\%</td>
<td>99.97\%</td>
<td>85.28\%</td>
<td>10.23\%</td>
<td>1.43e-07</td>
<td>17.77</td>
<td>4.44e-09</td>
</tr>
<tr>
<td>$S_{6}$</td>
<td>99.65\%</td>
<td>99.78\%</td>
<td>93.67\%</td>
<td>86.38\%</td>
<td>8.98\%</td>
<td>4.95e-05</td>
<td>12.17</td>
<td>2.66e-06</td>
<td>99.94\%</td>
<td>99.93\%</td>
<td>86.32\%</td>
<td>9.35\%</td>
<td>2.21e-06</td>
<td>291.67</td>
<td>1.05e-06</td>
</tr>
<tr>
<td>$A_{5}$</td>
<td>99.04\%</td>
<td>99.31\%</td>
<td>93.27\%</td>
<td>86.69\%</td>
<td>10.26\%</td>
<td>1.94e-05</td>
<td>9.82</td>
<td>5.28e-07</td>
<td>97.53\%</td>
<td>97.40\%</td>
<td>83.56\%</td>
<td>8.22\%</td>
<td>4.88e-02</td>
<td>19.76</td>
<td>7.70e-04</td>
</tr>
</tbody>
</table>
<p>It is not the case that networks learn simple representations over complex representations. If strong universality is true, we hypothesized networks would learn 'simple' representations over more complex ones, according to some sensible measure of complexity.</p>
<p>We naively thought that the complexity of a general representation would correlate with it's dimension ${ }^{5}$. For $S_{5}$, since the 4 dimensional representations are the lowest faithful representations, we expected representations of at most this dimension to be learned, and the model to choose arbitrarily between learning either of the two of them, or both. Empirically, we found this claim to be false. In particular, networks commonly learned higher dimensional representations, as can be seen in Figure 7. We also see in Figures 7 and 8 that the network preferred the standard representation over the standard_sign representation, when in fact standard_sign offers better performance for fixed weight norm.</p>
<p>While not deterministic, Figure 7 shows at least a probabilistic trend between our naive feature complexity and learning frequency, suggesting meaningful measures of feature complexity may exist. One complication here is that, as discussed in Section 5.6, models are trading off weight against performance. Representations with more degrees of freedom may also offer better performance for fixed total weight norm, so which the model may prefer, and thus which is least complex, is unclear.</p>
<p>Number of representations learned varies. Across seeds, in addition to different representations being learned, we too find different numbers of representations are learned, also shown in Figure 7. This is surprising to us. We additionally find that Transformers consistently learn fewer representations than MLPs, despite having more parameters. We view this as further evidence against the strongest forms of circuit and feature universality, and suggests there is a degree of</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Mean evolution of logit similarity of each non trivial representation of $S_{5}$ over training averaged over 50 random seeds. We observe the sign representation is consistently learned early in training, and the standard representation is also often learned. Notably, the standard_sign representation is of comparable complexity to the standard representation, but learned to a lesser degree.
randomness in what solutions models learn.
Lower dimensional representations are generally (but not always) learned first. Under any reasonable definition of complexity, the 1d sign representation is simpler than other $S_{5}$ representations. Figure 8 shows that the sign representation is consistently learned first. While it is very easily learned, it also generalizes poorly. In contrast, higher dimensional faithful features are harder to learn but generalize better. These correspond to type 1 and type 3 patterns according to the taxonomy presented in Davies et al. (2022). We however do not find evidence that all representations are learned in strict order of dimension, against our naive hypothesis's predictions, further evidence against strong universality.</p>
<h2>7. Conclusion and Discussion</h2>
<p>In this work, we use mechanistic interpretability to show that small neural networks perform group composition via an interpretable, representation theory-based algorithm, across several groups and architectures. We then define progress measures (Barak et al., 2023; Nanda et al., 2023) to study how the internals of networks develop over the course of</p>
<p>training. We use this understanding to study the universality hypothesis - that networks trained on similar tasks learn analogous features and algorithms. We find evidence for weak but not strong forms of universality: while all the networks studied use a variant of the GCR algorithm, different networks (with the same architecture) may learn different sets of representations, and even networks that use the same representations may learn them in different orders. This suggests that reverse engineering particular behaviors in single networks is insufficient for fully understanding that network behavior in general. That being said, even if strong universality fails in general, there is still promise that a 'periodic table' of universal features, akin to the representations in our group theoretic task, may exist in general for real tasks. We include further discussion on how this work fits into the wider field of mechanistic interpretability in Appendix A. Below, we discuss some areas of future work, with further discussion in Appendix F.</p>
<p>Further investigation of universality in algorithmic tasks. We raise many questions in Section 6 regarding which representations networks learn. Better understanding the learning rates and generalization properties of features offers a promising direction for future work in understanding network universality. Further understanding the probabilistic nature of which features are learned and at what time may too have future relevance. In particular, lottery tickets (Frankle \&amp; Carbin, 2019) may be present in initialized weights that could allow the learned features of a trained network to be anticipated before training.</p>
<p>More realistic tasks and models. In this work, we studied the behaviour of small models on group composition tasks. However, we did not explore whether our results apply to larger models that perform practical tasks. Future work could, for example, study universality in language models in the style of induction heads in Olsson et al. (2022).</p>
<p>Understanding inductive biases of neural networks. A key question in the science of deep learning is understanding which classes of algorithms are natural for neural networks to express. Our work suggests that the GCR algorithm is in some sense a 'natural' way for networks to perform group composition (Appendix G). A more comprehensive understanding of the building blocks of neural networks could speed up interpretability work while helping us better understand larger models.</p>
<h2>Author Contributions</h2>
<p>Bilal Chughtai was the primary research contributor and lead the project. He wrote the code, ran all experiments, reverse engineered the weights of the network trained on composition on $S_{5}$ in Section 5, and used this to automate the process of reverse engineering many more models in Section 6. He also wrote the paper.</p>
<p>Lawrence Chan provided significant help clarifying, framing and distilling the results, and with editing the final manuscript.</p>
<p>Neel Nanda supervised and mentored the entire project. He developed the complete version of the GCR algorithm based on Sam Marks's original version, and showed that it suffices to use a single faithful representation, and aided in editing the final manuscript.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank Joe Benton and Sam Marks for a conversation at a party that sparked this project and for seeing the connection between representation theory and composition of $S_{5}$, and additionally to Sam for contributing the core idea of the GCR algorithm.</p>
<p>We are also grateful to Joe Benton, Joseph Bloom, Stephen Casper, Ben Edelman, Jeremy Gillen, Stefan Heimersheim, Adam Jermyn, Cassidy Laidlaw, Eric Michaud and Martin Wattenberg for providing generous and valuable feedback on our manuscript. Over the course of the project, our thinking and exposition was also greatly clarified through correspondence with Spencer Becker-Kahn, Paul Colognese, Alan Cooney and Jacob Merizian.</p>
<p>BC would like to thank the SERI MATS 2.1 program, particularly Joe Collman and Maris Sala, for providing an excellent research environment during the entire project. BC was also supported by SERI MATS for the duration of the project.</p>
<p>We trained our models using PyTorch (Paszke et al., 2019) and performed our data analysis using NumPy (Harris et al., 2020) and Pandas (McKinney, 2010). We made use of SymPy (Meurer et al., 2017) to handle permutation group operations, and TransformerLens (Nanda, 2023) to cache internal model activations for interpretability. Our figures were made using Plotly (Inc., 2015).</p>
<h2>References</h2>
<p>Alperin, J. L. and Bell, R. B. Groups and Representations, volume 162 of Graduate Texts in Mathematics. Springer, New York, NY, 1995. ISBN 978-0-387-94526-2 978-1-4612-0799-3. doi: 10.1007/978-1-4612-0799-3.</p>
<p>Bansal, Y., Nakkiran, P., and Barak, B. Revisiting Model Stitching to Compare Neural Representations, June 2021.</p>
<p>Barak, B., Edelman, B. L., Goel, S., Kakade, S., Malach, E., and Zhang, C. Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit, January 2023.</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,</p>
<p>Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language Models are Few-Shot Learners, July 2020.</p>
<p>Cammarata, N., Carter, S., Goh, G., Olah, C., Petrov, M., Schubert, L., Voss, C., Egan, B., and Lim, S. K. Thread: Circuits. Distill, 5(3):e24, March 2020. ISSN 2476-0757. doi: 10.23915/distill. 00024.</p>
<p>Davies, X., Langosco, L., and Krueger, D. Unifying Grokking and Double Descent. In NeurIPS ML Safety Workshop, December 2022.</p>
<p>Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. A mathematical framework for transformer circuits, 2021.</p>
<p>Frankle, J. and Carbin, M. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, March 2019.</p>
<p>Ganguli, D., Hernandez, D., Lovitt, L., DasSarma, N., Henighan, T., Jones, A., Joseph, N., Kernion, J., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Elhage, N., Showk, S. E., Fort, S., Hatfield-Dodds, Z., Johnston, S., Kravec, S., Nanda, N., Ndousse, K., Olsson, C., Amodei, D., Amodei, D., Brown, T., Kaplan, J., McCandlish, S., Olah, C., and Clark, J. Predictability and Surprise in Large Generative Models. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pp. 1747-1764, June 2022. doi: 10.1145/3531146.3533229.</p>
<p>Goh, G., , N. C., , C. V., Carter, S., Petrov, M., Schubert, L., Radford, A., and Olah, C. Multimodal neurons in artificial neural networks. Distill, 2021. doi: 10.23915/ distill. 00030.</p>
<p>Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Ro, J. F., Wiebe, M., Peterson, P., Grard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585(7825):357-362, September 2020. ISSN 1476-4687. doi: 10.1038/s41586-020-2649-2.</p>
<p>Inc., P. T. Collaborative data science. https://plot.ly, 2015.
Kornblith, S., Norouzi, M., Lee, H., and Hinton, G. Similarity of Neural Network Representations Revisited, July 2019.</p>
<p>Li, K., Hopkins, A. K., Bau, D., Vigas, F., Pfister, H., and Wattenberg, M. Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task, February 2023.</p>
<p>Li, Y., Yosinski, J., Clune, J., Lipson, H., and Hopcroft, J. Convergent Learning: Do different neural networks learn the same representations?, February 2016.</p>
<p>Lindner, D., Kramr, J., Rahtz, M., McGrath, T., and Mikulik, V. Tracr: Compiled Transformers as a Laboratory for Interpretability, January 2023.</p>
<p>Liu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. Transformers Learn Shortcuts to Automata, October 2022a.</p>
<p>Liu, Z., Kitouni, O., Nolte, N., Michaud, E. J., Tegmark, M., and Williams, M. Towards Understanding Grokking: An Effective Theory of Representation Learning, October 2022b.</p>
<p>Liu, Z., Michaud, E. J., and Tegmark, M. Omnigrok: Grokking Beyond Algorithmic Data, October 2022c.</p>
<p>McGrath, T., Kapishnikov, A., Tomaev, N., Pearce, A., Hassabis, D., Kim, B., Paquet, U., and Kramnik, V. Acquisition of Chess Knowledge in AlphaZero. Proceedings of the National Academy of Sciences, 119(47):e2206625119, November 2022. ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas. 2206625119.</p>
<p>McKinney, W. Data Structures for Statistical Computing in Python. Proceedings of the 9th Python in Science Conference, pp. 56-61, 2010. doi: 10.25080/ Majora-92bf1922-00a.</p>
<p>Mehrer, J., Spoerer, C. J., Kriegeskorte, N., and Kietzmann, T. C. Individual differences among deep neural network models. Nature Communications, 11(1): 5725, November 2020. ISSN 2041-1723. doi: 10.1038/ s41467-020-19632-w.</p>
<p>Meurer, A., Smith, C. P., Paprocki, M., ertk, O., Kirpichev, S. B., Rocklin, M., Kumar, Am., Ivanov, S., Moore, J. K., Singh, S., Rathnayake, T., Vig, S., Granger, B. E., Muller, R. P., Bonazzi, F., Gupta, H., Vats, S., Johansson, F., Pedregosa, F., Curry, M. J., Terrel, A. R., Rouka, ., Saboo, A., Fernando, I., Kulal, S., Cimrman, R., and Scopatz, A. SymPy: Symbolic computing in Python. PeerJ Computer Science, 3:e103, January 2017. ISSN 2376-5992. doi: 10.7717/peerj-cs. 103.</p>
<p>Morcos, A. S., Raghu, M., and Bengio, S. Insights on representational similarity in neural networks with canonical correlation, October 2018.</p>
<p>Nanda, N. A Comprehensive Mechanistic Interpretability Explainer \&amp; Glossary. https://www.neelnanda.io/glossary, December 2022.</p>
<p>Nanda, N. TransformerLens, January 2023.
Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J. Progress measures for grokking via mechanistic interpretability, January 2023.</p>
<p>Neyshabur, B., Tomioka, R., and Srebro, N. In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning, April 2015.</p>
<p>Olah, C., Mordvintsev, A., and Schubert, L. Feature Visualization. Distill, 2(11):e7, November 2017. ISSN 2476-0757. doi: 10.23915/distill. 00007.</p>
<p>Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. Zoom In: An Introduction to Circuits. Distill, 5(3):e00024.001, March 2020. ISSN 2476-0757. doi: 10.23915/distill.00024.001.</p>
<p>Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., Drain, D., Ganguli, D., Hatfield-Dodds, Z., Hernandez, D., Johnston, S., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. In-context Learning and Induction Heads, September 2022.</p>
<p>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An Imperative Style, High-Performance Deep Learning Library, December 2019.</p>
<p>Power, A., Burda, Y., Edwards, H., Babuschkin, I., and Misra, V. Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets, January 2022.</p>
<p>Sellam, T., Yadlowsky, S., Wei, J., Saphra, N., D'Amour, A., Linzen, T., Bastings, J., Turc, I., Eisenstein, J., Das, D., Tenney, I., and Pavlick, E. The MultiBERTs: BERT Reproductions for Robustness Analysis, March 2022.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention Is All You Need, December 2017.</p>
<p>Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 small, November 2022.</p>
<p>Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., and Fedus, W. Emergent Abilities of Large Language Models, October 2022.</p>
<p>Weiss, G., Goldberg, Y., and Yahav, E. Thinking Like Transformers, July 2021.</p>
<p>Zhang, Y., Backurs, A., Bubeck, S., Eldan, R., Gunasekar, S., and Wagner, T. Unveiling Transformers with LEGO: A synthetic reasoning task, July 2022.</p>
<h1>A. Relevance for Mechanistic Interpretality</h1>
<p>How might this work influence interpretability work on real models? We view our work as a contribution towards where to direct effort in the field. Mechanistic interpretability focuses on reverse engineering neural networks, and providing mechanistic explanations for model behaviors.</p>
<p>Recently, the field has been making good progress towards understanding how networks implement behavior in a range of contexts. Initial work successfully reverse engineered neurons in computer vision models, (Olah et al., 2020; 2017; Goh et al., 2021), finding certain neurons represent interpretable human concepts. Other work has found interpretable components of Transformer language models, such as 'induction heads', responsible for copying from earlier in the context window and consequently in context learning (Olsson et al., 2022). Wang et al. (2022) were able to reverse engineer a large subgraph of GPT-2, responsible for successful completions of the indirect object identification task (IOI). Nanda et al. (2023) were able to reverse engineer Transformers trained to perform modular addition, and through doing so, understand why these models grokked. Mechanistic interpretability has also been applied to AlphaZero and to a model trained to play Othello (McGrath et al., 2022; Li et al., 2023) and has been able to demonstrate these networks too learn human understandable concepts.</p>
<p>Much of this work focuses on a single, small model, sometimes with the explicitly stated goal of generalizing to large foundation models (Elhage et al., 2021). Wang et al. (2022) for instance only investigated one model (GPT-2 small). This is often motivated by the universality hypothesis (Olah et al., 2020) - that there exist canonical solutions to tasks that networks consistently implement - but investigations into single small models may be too specific. If the universality hypothesis is true, work on small or single models may generalize directly to other/larger models of genuine interest. But if not, the mechanistic interpretability community may be wasting substantial effort and should focus instead on directly interpreting models of genuine interest, or creating tools to automate this process. Better understanding the universality hypothesis is therefore important.</p>
<p>Prior work in mechanistic interpretability has sometimes found similar features and circuits across a range of models. Different computer vision models were found to contain similar and interpretable "curve detector" and "high low frequency detector" neurons in early layers (Olah et al., 2020). Sometimes, the same feature has been found to be computed by different circuits - such as induction heads in Transformer language models, as noted in the appendix here (Olsson et al., 2022). However, no one so far has comprehensively and systematically studied the question of how well mechanistic explanations generalize across models, and how big a weakness focusing on a single model is.</p>
<p>In our work, we sought to answer this question. We chose a toy task, where we were (to our surprise) able to fully enumerate all possible solutions through the different representations which were of varying complexity. Our methods allowed us to inspect which of these ground truth features networks had learned. Through doing so, we found that reverse engineering one model was insufficient to understand behavior in general. Our mainline S5 model only gave us insights into two of the possible circuits used to solve the task (corresponding to the sign and standard representations), out of a possible six. Only after studying many more models were we able to observe all the different mechanisms used to implement the single behavior.</p>
<p>We view our work as a proof of concept that by reverse-engineering circuits in many models, one can build a comprehensive periodic table of features that permits understanding of how networks implement behavior in general. Practically speaking, we then suggest that those studying model behaviors should perform "robustness checks" in many models to truly understand all possible mechanisms behind behavior. This may have future relevance to auditing models via mechanistic interpretability. There exist resources that permit the study of universality in language models already, such as MultiBert (Sellam et al., 2022), which offers a set of similar models trained on different random seeds, much like our models. One could begin by studying the IOI circuit (Wang et al., 2022) in these models, and examining whether the same mechanism is universally learned, and if not, how large the family of possible mechanisms truly are.</p>
<h2>B. Similarities and differences with prior work on reverse engineering modular addition</h2>
<p>Here, we summarize the prior work of Nanda et al. (2023) that we build on, and detail where our experimental approaches differ. We note our contributions differ in that we use our mechanistic understanding to study universality. The authors train a one-layer Transformer model, of same type as we use (Section C) on modular addition. They find strong evidence it performs a completely understandable algorithm involving discrete Fourier transforms of the two inputs at various frequencies, and then makes use of various trigonometric identities to combine these. The key result which we generalize is that given inputs $a$ and $b$ the network computes $\cos (\omega(a+b-c))$ for each possible output $z$ over some fixed set of frequencies $\omega$. Taking the argmax of this expression over $c$ gives the correct answer. One can track the progress of this computation faithfully</p>
<p>through the Transformers activations and weights.
Using this mechanistic understanding, the authors define the concept of a 'progress measure' that underlies the emergent behavior of grokking, a qualitative and discontinuous change in model behavior. They find that the training history of the model can be separated into three stages. First, the model memorizes the training data. Then, the circuit components for the general algorithm form smoothly. Finally, the memorized algorithm is cleaned up and removed as it is more complex and not favored by weight decay. Grokking occurs during cleanup, at the critical point after which the learned general algorithm is competitive with the 'memorized' algorithm - performance of the general algorithm is heavily hampered by 'noise' from the memorized algorithm. Crucially, the progress measures show that the components responsible for grokking arise before the sharp discontinuity in test loss.</p>
<p>We follow this approach closely. Our techniques in Section 5 are heavily inspired by Nanda et al.'s approach. Our precise analysis though differs substantially. Fourier transforms are elegant, but specific to the modular addition task. We instead work with representation matrices, and subspaces.</p>
<p>On modular addition of 113 elements, i.e. group composition on $C_{113}$, we are able to replicate their results in our framing. As discussed in Section 4, their algorithm maps precisely onto our GCR algorithm, and both approaches may be used to understand the cyclic group task. The mapping of their findings onto ours is fairly clear for embeddings, unembeddings and logits. For MLP neurons, they found that most neurons were well explained by a quadratic form of sinusoidal functions of the 9 terms within a single frequency. This quadratic form shared coefficients in such a way such that this had 2 redundant degrees of freedom, giving 7 terms. In our case, MLP neurons contain information pertaining to $\rho(a), \rho(b)$ and $\rho(a b)$. In the special case of cyclic representations (see Appendix D.1.1), each of these terms has 2 degrees of freedom by antisymmetry. Adding a constant gives precisely the same seven terms.</p>
<h1>C. Architecture Details</h1>
<p>Our mainline model is trained on $40 \%$ of all $n^{2}$ entries in the multiplication table of the group. We use full batch gradient descent. We use weight decay with $\lambda=1$, and the AdamW optimizer, with learning rate $\gamma=0.001, \beta_{1}=0.9$ and $\beta_{2}=0.98$. We perform 250,000 epochs of training. As there are only $n^{2}$ possible input pairs, we evaluate test loss and accuracy on all pairs of inputs not used for training.</p>
<h2>C.1. MLP</h2>
<p>Our MLP architecture is summarized in Figure 1. Inputs $a$ and $b$ are encoded as $n$ dimensional one-hot vectors. Each one-hot vector is embedded with $d=256$. These are concatenated to form a 512 dimensional vector, which is fed into a $h=128$ linear layer, with no bias term. ${ }^{6}$ The output is mapped via an unembedding linear map, $W_{U}$, to $n$ logits, corresponding to each of the $n$ group elements. We did not tie the left embedding, right embedding or unembedding matrices. This is a simplified version of the Transformer architecture used by Nanda et al. (2023) (described below) which removes attention. Attention is both empirically irrelevant in this prior work, and not predicted to be necessary by our algorithm. The form of logits is therefore</p>
<p>$$
\text { Logits }=\text { W_U @ ReLU( W_MLP @ [W_left @ a, W_right @ b]) }
$$</p>
<p>Note that the embedding matrices and linear layer have no non-linearity between them. When interpreting model calculations we will tie these matrices, and think of the $a$ and $b$ embeddings as the result of passing inputs through both layers. This methodology is inspired by (Elhage et al., 2021), ${ }^{7}$. The remainder of the operation of the linear layer is then to add these two 'total' embeddings and pass them through a ReLU. That is,</p>
<p>$$
\text { Logits }=\text { W_U @ ReLU( W_a @ a + W_b @ b) }
$$</p>
<p>where</p>
<p>$$
\mathrm{W} _\mathrm{a}=\mathrm{W} _\mathrm{MLP}[: \mathrm{d},:] @ \mathrm{~W} _ \text {left } \quad \mathrm{W} _\mathrm{b}=\mathrm{W} _\mathrm{MLP}[\mathrm{~d}:,:] @ \mathrm{~W} _ \text {right }
$$</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>C.1.1. CHOICE OF NETWORK SIZE</h1>
<p>We note this architecture is over parameterized for our tasks. Smaller networks, with fewer parameters, often struggled to generalize consistently due to optimization issues. We chose a hidden layer size of 128 to avoid these. We do not think the choice of network size generally affected our results. To verify this, we repeated our mainline $S_{5}$ experiment many more times, on networks with hidden size ranging from 32 to 256 . Of those that did generalize, we saw the GCR algorithm was consistently implemented. We did not see a noticeable effect of network parameter count on which representations were learned. Interestingly, networks consistently learned the sign representation early on, even if they did not successfully generalize later. Sometimes, a generalized network with a small hidden layer would throw away the sign representation late in training to make room for another, higher dimensional, representation, with more generalization power.</p>
<h2>C.2. Transformer</h2>
<p>Our Transformer architecture for other runs is a decoder only architecture is based on Vaswani et al. (2017). It is identical to the set up for mainline experiments in Nanda et al. (2023). The input to the model is of the form "a b =", where a and b are encoded as $n$-dimensional one-hot vectors, and ' $=$ ' is a special token above which we read the output c. We use a one-layer ReLU Transformer, token embeddings with $\mathrm{d}=128$, learned positional embeddings, 4 attention heads of dimension $d / 4=32$, and $n=512$ hidden units in the MLP. At points we analyze it's embedding $W_{E}$, MLP layer, and map to logits $W_{L}=W_{U} W_{\text {out }}$, ignoring the residual skip connection, which we find empirically is not utilized significantly for our tasks.</p>
<h2>D. Mathematical Representation Theory</h2>
<p>In this section we present the results from group, representation, and character theory we make use of. We begin by motivating our use of representation theory in this context. Groups are an abstraction of the idea of symmetry. In practice though, groups are not purely abstract objects, and tend to arise due to their action on other things. Often, these things are naturally attached to some vector space $V$, such that $G$ gives rise to a linear action $\rho$ on $V$, which we call a representation.</p>
<p>Representation theory appears in several physical systems and is of fundamental importance to science. While groups encode the symmetries of physical systems, representations prescribe the set of possible actions of these symmetries on physical vector spaces. For instance, the representation theory of the particular Lie groups encoding symmetry transformations of spacetime determine the particles predicted by the standard model, which we observe in the universe.</p>
<p>Definition D.1. A linear representation $\rho$ is a group homomorphism $\rho: G \rightarrow G L(V)$ where $G L(V)$ denotes the general linear group of some vector space $V$ over a field $\mathbb{F}$, the set of linear maps on $V$.</p>
<p>We focus on real representations, i.e. group homomorphisms $\rho: G \rightarrow G L\left(\mathbb{R}^{d}\right)$, the set of real invertible $d \times d$ matrices. We give some concrete examples of such representations of particular groups in Section D.1. We hypothesize representations are a natural way for a neural networks to implement operations on group elements. Representing group elements in a linear algebra theoretic manner seems like it would be advantageous to a networks natural operations of matrix multiplication and addition. We discuss this observation further in Appendix G.</p>
<p>Definition D.2. Let $\rho: G \rightarrow G L(V)$ be a linear representation. $\rho$ is said to be irreducible if $\rho$ has no $G$-stable subspace. That is, there is no subspace of $V$ on which $\rho$ defines a sub-representation of $G$.</p>
<p>From now on, we will use the term irrep to refer to irreducible representations. Irreps are the key object of interest. This is due to Maschke's Theorem.</p>
<p>Theorem D.3. (Maschke) Every representation of a finite group G is a direct sum of irreducible representations. That is, there exists some basis in which all representation matrices are block diagonal, where the block sizes $d_{1}, \ldots, d_{k}$ are the same for all $\rho(g)$ with $g \in G$.</p>
<p>Example. Every group $G$ has a representation for any dimension d mapping each group element to identity matrix $I_{d}$. This is the direct sum of $d$ one-dimensional irreducible representations named the 'trivial' representation, given by $\rho(g)=1$ for all $g \in G$.</p>
<p>This representation isn't practically useful, as the network can not use these representations to perform calculations on group elements. We will often exclude the trivial representation and refer to non-trivial representations. There are a finite number of these due to</p>
<p>Theorem D.4. Let $G$ be a group of order $n$ and let $\rho_{i}$ be distinct (up to isomorphism) irreducible representations of $G$ over some splitting field $\mathbb{F}$. Let $d_{i}$ be the dimension of $\rho_{i}$, and $r$ be the number of irreducible representations. Then $n=d_{1}^{2}+\cdots+d_{r}^{2}$.
Some representations are more useful to the network than others:
Definition D.5. A representation $\rho$ is said to be faithful if different elements $g$ of $G$ are represented by distinct linear maps $\rho(g)$. In other words, the group homomorphism $\rho: G \rightarrow G L(V)$ is injective.
Faithful representations are the most useful to the network, though we will often see networks also make use of lower degree non-faithful representations too.
Character theory forms an important part of representation theory, and will be important to our use case.
Definition D.6. Let V be a finite-dimensional vector space over a field $\mathbb{F}$ and let $\rho: G \rightarrow G L(V)$ be a representation of a group G on V . The character of $\rho$ is the function $\chi_{\rho}: G \rightarrow \mathbb{F}$ given by $\chi_{\rho}(g)=\operatorname{tr} \rho(g)$, the trace of the representation matrix.</p>
<p>We now present some useful facts about characters. Character's are class functions - that is, they take a constant value on each conjugacy class of the group. Note too that</p>
<p>$$
\chi\left(g^{-1}\right)=\overline{\chi(g)}
$$</p>
<p>In the case of real representations this implies</p>
<p>$$
\chi\left(a b c^{-1}\right)=\chi\left(\left(a b c^{-1}\right)^{-1}\right)=\chi\left(c(a b)^{-1}\right)=\chi\left((a b)^{-1} c\right)
$$</p>
<p>where in the final step we used the cyclic property of trace. $\chi\left((a b)^{-1} c\right)$ is naively an alternative valid computation the network could use to compute correct answers, and this shows it is equivalent to the GCR algorithm.</p>
<p>Theorem D.7. Let $G$ be a group, and $\rho: G \rightarrow G L\left(\mathbb{R}^{d}\right)$ a real representation of it of dimension $d$. For $g \in G$, $\chi_{\rho}(g)=\operatorname{tr} \rho(g) \leq d$ with equality iff $\rho(g)=I$.</p>
<p>Proof. Let $|G|=n$. Since $\rho$ is a group representation, and the order of elements in a group divide $n, \rho(g)^{n}=I$ for all $g$. The eigenvalues of $\rho(g)$ are therefore $n$ 'th roots of unity, so each character is a sum of roots of unity. By the triangle inequality, the claim holds.</p>
<p>Theorem D.8. (Schur's Orthogonality Relation of Characters) The space of complex-valued class functions of a finite group $G$ is endowed with a natural inner product, given by</p>
<p>$$
\langle\alpha, \beta\rangle=\frac{1}{|G|} \sum_{g \in G} \alpha(g) \overline{\beta(g)}
$$</p>
<p>where $\overline{\beta(g)}$ denotes the complex conjugate. With respect to this inner product, the irreducible characters form an orthonormal basis for the space of class functions, yielding the orthogonality relation</p>
<p>$$
\left\langle\chi_{i}, \chi_{j}\right\rangle= \begin{cases}0 &amp; \text { if } i \neq j \ 1 &amp; \text { if } i=j\end{cases}
$$</p>
<p>Theorem D.9. (Schur's Orthogonality Relation of Matrix Elements) Let $\rho_{\lambda}$ be irreducible representations of a finite group $G$ of dimension $d_{\lambda}$ with matrix presentations $\Gamma_{i j}^{\lambda}$. Without loss of generality, we may assume $\Gamma^{\lambda}$ is unitary, as any matrix representation is equivalent to a unitary representation.
Then</p>
<p>$$
\sum_{g \in G} \overline{\Gamma^{\lambda}(g)}{ }<em i_prime="i^{\prime">{i j} \Gamma</em>
$$} j^{\prime}}^{\mu}=\delta^{\lambda_{\mu}} \delta_{i i^{\prime}} \delta_{j j^{\prime}} \frac{|G|}{d_{\lambda}</p>
<p>Note that the overbar denotes a complex conjugate, and the unitarity assumption only affects the constant, not the orthogonality.</p>
<h1>D.1. Explicit Groups and Representations</h1>
<p>Our methods for reverse engineering networks require mechanistic understanding of the precise form of representations. Here, we describe the irreducible representation matrices for particular groups. The classification of irreducible representations for any given group requires some machinery not presented here, and which we don't require for the purposes of our work. We just state the key results.</p>
<h2>D.1.1. IRREDUCIBLE REPRESENTATIONS OF THE CYCLIC GROUP</h2>
<p>The cyclic group $C_{n}$ encodes rotational symmetries of an n-gon. Over the reals, the irreducible representations of $C_{n}$ fall into three classes. Note that Theorem D. 4 does not apply here as $\mathbb{C}$ is a splitting field for $C_{n}$, but $\mathbb{R}$ is not.</p>
<ol>
<li>the 1-dimensional trivial representation $\mathbf{1}$</li>
<li>the 1-dimensional sign representation $\mathbf{1}_{\mathbf{s g n}}$, which only appears if the group order is even.</li>
<li>the 2-dimensional standard representations $\mathbf{2}_{\mathbf{k}}$ of rotations in the Euclidean plane by angles that are integer multiples of $\frac{2 \pi k}{n}$ for $k \in \mathbb{N} 0&lt;k&lt;n / 2$. The representation matrices may be written explicitly as</li>
</ol>
<p>$$
\rho_{k}(x)=\left(\begin{array}{cc}
\cos \left(\frac{2 \pi k}{n} x\right) &amp; -\sin \left(\frac{2 \pi k}{n} x\right) \
\sin \left(\frac{2 \pi k}{n} x\right) &amp; \cos \left(\frac{2 \pi k}{n} x\right)
\end{array}\right)
$$</p>
<p>Note the complex representations are much simpler, consisting of the $n^{\prime}$ th roots of unity. The sign representation appears then due to -1 being a root of unity iff $n$ even. For $k=n / 2$, the 2 d representation is the direct sum of two copies of the sign representation, so is not irreducible, and for $k&gt;n / 2$ we have the isomorphism $\mathbf{2}<em _mathbf_k="\mathbf{k">{\mathbf{n}-\mathbf{k}} \rightleftharpoons \mathbf{2}</em>$.}</p>
<h2>D.1.2. IRREDUCIble REPRESENTATIONS OF THE DiHEDRAL GROUP</h2>
<p>We focus on dihedral groups $D_{n}=\langle r, s| r^{n}=s^{2}=e, s r s=r^{-1}\rangle$, with $n$ odd. These encode all symmetries of an n-gon, rotational and reflectional. The representations of these groups are much the same as those of cyclic groups, and fall into three categories.</p>
<ol>
<li>the 1-dimensional trivial representation $\mathbf{1}$</li>
<li>the 1-dimensional sign representation $\mathbf{1}_{\mathbf{s g n}}$, mapping $\langle r\rangle$, i.e. rotations, to 1 , and the coset, i.e. reflections to -1 .</li>
<li>the 2-dimensional standard representations $\mathbf{2}_{\mathbf{k}}$, corresponding to rotations and reflections in the Euclidean plane.</li>
</ol>
<p>$$
\begin{aligned}
\rho_{k}\left(r^{l}\right) &amp; =\left(\begin{array}{cc}
\cos \left(\frac{2 \pi k}{n} l\right) &amp; -\sin \left(\frac{2 \pi k}{n} l\right) \
\sin \left(\frac{2 \pi k}{n} l\right) &amp; \cos \left(\frac{2 \pi k}{n} l\right)
\end{array}\right) \
\rho_{k}\left(r^{l} s\right) &amp; =\left(\begin{array}{cc}
\cos \left(\frac{2 \pi k}{n} l\right) &amp; \sin \left(\frac{2 \pi k}{n} l\right) \
\sin \left(\frac{2 \pi k}{n} l\right) &amp; -\cos \left(\frac{2 \pi k}{n} l\right)
\end{array}\right)
\end{aligned}
$$</p>
<h2>D.1.3. IRREDUCIble REPRESENTATIONS OF THE SYMMETRIC GROUP</h2>
<p>Our mainline experiments involve the permutation, or symmetric, group of 5 elements, denoted $S_{5}$. We denote general permutation groups of n elements $S_{n}$. This is an interested group to look at due to Cayley's Theorem:</p>
<p>Theorem D.10. (Cayley) Every group is isomorphic to a subgroup of a symmetric group.
We list the lowest dimensional irreps of $S_{n}$ in Table 4. These may be fairly easily constructed. We constructed trivial irreps in Appendix D, but to recap, this just maps every group element to the scalar 1.</p>
<p>The sign representation are a set of $1 \times 1$ matrices representing a kind of parity. Permutations may be decomposed as a (non unique) sequence of swaps. The parity of this number of swaps is in fact well defined, and defines a subgroup of the symmetric group named the alternating group. Mapping this alternating group to +1 , and the coset to -1 gives the sign</p>
<p>Table 4. The lowest degree irreps for $S_{n}$ for $n \geq 7$, and their dimension. For $n \leq 7$, additional symmetries give rise to other low dimensional irreps on top of these.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$S_{n}$ IRREP</th>
<th style="text-align: center;">DIMENSION</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">TRIVIAL</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">SIGN</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: left;">STANDARD</td>
<td style="text-align: center;">$n-1$</td>
</tr>
<tr>
<td style="text-align: left;">STANDARD $\otimes$ SIGN</td>
<td style="text-align: center;">$n-1$</td>
</tr>
</tbody>
</table>
<p>representation. In general, any group containing a subgroup of index 2 is naturally endowed with a sign representation in a similar manner.</p>
<p>Next is the standard representation. This is essentially the set of permutation matrices $-n \times n$ square binary matrices, with only one 1 in each row and column, and 0 s elsewhere. This representation has dimension $n$, though, not $n-1$. This is because it turns out to be reducible. Recalling Definition D.2, this has an invariant subspace under the action of $G$, spanned by the vector sum of all basis elements. The irreducible representations recovered are the standard and trivial representations.</p>
<p>Standard $\otimes$ Sign denotes the tensor product of the standard and sign representations, which is just their matrix product as the sign representation is 1 dimensional.
$S_{5}$ has three higher degree representations, which I denote 5d_a, 5d_b, 6d. We omit their construction here.</p>
<h1>D.1.4. IRREDUCIble REPRESENTATIONS OF $A_{5}$</h1>
<p>As a subgroup of $S_{5}, A_{5}$ inherits representations from $S_{5}$. However, the six dimensional representation of $S_{5}$ becomes reducible, splitting into two three dimensional irreps of $A_{5}$. We omit details here.</p>
<h2>E. Additional Reverse Engineering of Mainline Model</h2>
<p>Here we give further evidence our mainline model trained on $S_{5}$ performs the GCR algorithm as detailed in Section 4, and give further details regarding our methods.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Evolution of the fraction of the left embedding (top left), right embedding (top right), and unembedding (bottom) explained by $\rho(a), \rho(b)$ and $\rho\left(e^{-1}\right)$ respectively. Representations are learned suddenly and at approximately the same time across all the embeddings, evidence that they are learned as part of the GCR algorithm. As the representation spaces form an orthogonal decomposition of $\mathbb{R}^{n}$, the terms will always add up to 1 , so we draw the reader's attention to the sparsity over embeddings. At initialization, each representation explains $d^{2} /|G|$ of the embedding due to randomness.</p>
<h1>E.1. Progress Measures</h1>
<p>Here, we provide further discussion on how we use progress measures to understand grokking generalization in our models. We first give more full definitions of our progress measures below.</p>
<p>Restricted Loss. We restrict the MLP activations to the terms corresponding to $\rho(a b)$ in the key representations, a $16+1$ dimensional subspace of $\mathbb{R}^{128}$, and then map this restricted MLP layer to logits. By doing so, we isolate the performance of the generalising algorithm. This assumes that the memorising algorithm has no privileged subspace in the MLP layer.</p>
<p>Excluded Loss. The opposite of restricted loss. Instead of keeping the key representations, we remove only those representations from the MLP neurons, and see how this affects loss. Having removed the generalising solution, this isolates the performance of the memorising solution. This therefore makes sense to measure only on the training data, which we do.</p>
<p>The three phases of training we define are as follows, and can be seen in Figures 6 and 12.
Memorization. (Epochs 0-2k) We first observe a decline of both excluded and train loss, with test and restricted loss both remaining high. In other words, the model memorizes the training data. The sum of squared weights peaks at the end of memorization, so weight decay does not prefer these memorized circuits. As test loss increases but restricted loss stays constant as no progress towards generalization is made, the ratio of test loss to restricted loss rises.</p>
<p>Circuit Formation. (Epochs 2.2k-87k) In this phase, excluded loss rises, sum of squared weights falls (Figure 12), restricted loss starts to fall, and train and test loss stay flat. This suggests that the models behavior on the train set transitions smoothly from the memorising solution to the generalizing solution. The fall in the sum of squared weights suggests that circuit formation likely happens due to weight decay. Notably, the circuit is formed well before grokking.</p>
<p>Cleanup. (Epochs 87k-120k) In this phase, restricted loss continues to drop, test loss suddenly drops, sum of squared weights sharply drops, and the ratio of test to restricted loss is variable and then sharply decreases (Figure 12). As the generalising circuit both solves the task well and has lower weight at comparable performance as compared with memorisation circuits on the training set, weight decay encourages the network to shed the memorised solution. Weight decay contributes an important inductive bias of our networks (Neyshabur et al., 2015). The slight rise in restricted loss at the very end of training</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Evolution of the fraction of the MLP neurons explained by $\rho(a)$ (top left), $\rho(b)$ (top right), $\rho(a b)$ (bottom left), and the sum of all three over all representations (bottom right). These track the same timing as representation learning in the embeddings and unembeddings, further evidence for our algorithm. Note that in order to perform step 2 in the GCR algorithm, $\rho(a b)$ must be calculated. If a representation has $\rho(a)$ and $\rho(b)$ represented but not $\rho(a b)$ then the representation has not been learned.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11. Change of basis matrix from projected MLP space standard representation space. Note some neurons correspond to blocks of 4 cells in the representation basis - these correspond to standard representation matrix rows. Neurons in other clusters can be explicitly seen as being off in this change of basis matrix.
is too a result of weight being traded off against performance - multiplying the entire circuit by a fixed constant $r&gt;1$ will reduce loss, though also requires more weight.</p>
<h1>E.2. Full Circuit Analysis: Sign Representation</h1>
<p>In Nanda et al. (2023), the authors primarily analyze 2d representations via Fourier transforms, and we primarily analyze 4d standard representations in our mainline model. Treating sines and cosines as separate objects adds complexity, which we avoid by unifying them as matrix elements of the same representation. However, two dimensional features retain some redundancy over choice of basis, or equivalently, choice of rotation axis. So in general, some manipulation of activations and weights is necessary to interpret the model.</p>
<p>The sign representation on the other hand is a one dimensional representation of certain groups. This computational subgraph may be understood by directly inspecting activations and weights, without ever having to change basis. We demonstrate this</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 12. The sum of squared weights (left), and ratio of test loss and restricted loss (right). The sum of squared weights decreases smoothly during circuit formation and more sharply during cleanup, indicating both phases are linked to weight decay. Intuitively, restricted loss is us artificially cleaning up some the model (besides $W_{U}$ ), while test loss requires both circuit formation and cleanup. So a large discrepancy shows the rate of circuit formation outstrips the rate of cleanup during grokking.
<img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 13. Excluded (left) and restricted loss (right), separated out by representation. As with the results of Section 5.6, this shows the model interpolates between memorizing and generalizing. In the restricted loss plot, we see the sign representation is incapable of solving the task alone, but contributes several orders of loss improvement when coupled with the standard representation, as can be seen in excluded loss.
simplicity on our mainline model.
MLP neuron activations are 'blocky'. We can identify interpretable activation patterns by inspection. Working backwards we identify embeddings directly learn $\pm \operatorname{sign}(a)$ and $\pm \operatorname{sign}(b)$.
We then can write out, for $x, y$ some positive constants and $n_{i}$ neuron $i$ :</p>
<p>$$
\begin{aligned}
n_{2} &amp; =x \operatorname{ReLU}(+\operatorname{sign}(a)+\operatorname{sign}(b)) \
n_{8} &amp; =x \operatorname{ReLU}(-\operatorname{sign}(a)-\operatorname{sign}(b)) \
n_{17} &amp; =x \operatorname{ReLU}(+\operatorname{sign}(a)+\operatorname{sign}(b)) \
n_{65} &amp; =x \operatorname{ReLU}(-\operatorname{sign}(a)-\operatorname{sign}(b)) \
n_{111} &amp; =x \operatorname{ReLU}(+\operatorname{sign}(a)-\operatorname{sign}(b)) \
n_{113} &amp; =y \operatorname{ReLU}(-\operatorname{sign}(a)+\operatorname{sign}(b)) \
n_{120} &amp; =x \operatorname{ReLU}(+\operatorname{sign}(a)-\operatorname{sign}(b))
\end{aligned}
$$</p>
<p>In general, interpreting the matrix multiplication operation is challenging, though in the one dimensional case it turns out to be simple. We see that the MLP performs multiplication of signs via ReLU and addition. For instance</p>
<p>$$
n_{2}+n_{8}+n_{111}+n_{113}=2 x \times \operatorname{sign}(a) \times \operatorname{sign}(b)
$$</p>
<p><img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 14. (Left) Train and test loss of the mainline model, only on a different random seed. (Right) Logit similarity of this run over training. We see two phases of grokking. The model initially groks as the memorizing circuit is cleaned up in presence of the valid general standard circuit. Loss then plateaus as the 5 d.b circuit is learned around epoch 100 k , before the model groks again as cleanup continues.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 15. The seven 'sign neuron' activations over the whole distribution of inputs. Each activates uniformly on inputs, with form some multiple of $\mathbb{1}(\operatorname{sign}(a)= \pm 1) \mathbb{1}(\operatorname{sign}(b)= \pm 1)$, where $\pm$ are independent.</p>
<p>This is essentially computing an XOR gate on the inputs, and in particular not multiplication of arbitrary inputs, which is why the network can implement this operation perfectly. Note that we need a minimum of four neurons to implement this operation in this manner ${ }^{8}$. Empirically, we found that the number of sign neurons was often four exactly. In this case, neuron 113 appears to be used in two such multiplication calculations.</p>
<p>We expect that higher dimensional matrix multiplication is implemented similarly - see further discussion in Appendix E.3.
Map to logits. Calling neurons $2,8,17$ and 65 positive, and neurons 111, 113, and 120 negative, we find that $W_{U} \mid_{+} \sim$ $+\operatorname{sign}\left(c^{-1}\right)$ and $W_{U} \mid-\sim-\operatorname{sign}\left(c^{-1}\right)$, thus this circuit contributes positively to logits on correct signs and negatively to wrong signs, giving a contribution $\chi_{\text {sign }}\left(a b c^{-1}\right)$ to logits.</p>
<h1>E.3. Implementing Multiplication via ReLUs</h1>
<p>Here we briefly discuss how networks may implement multiplication in a single layer. Our GCR algorithm necessitates this in step 2, and we provide a simple example of this occurring in Appendix E.2.</p>
<p>Networks can multiply activations to some extent in one layer, though may not be able to do so perfectly, and also may put redundant information into additional directions (as we suspect comprises the $12 \%$ residual of standard MLP neurons in Section 5.3). Note in this context that multiplication is not generic multiplication, but multiplication of a fixed set of elements. Most of our representation matrices have entries ${0,-1,1}$ on which multiplication can be implemented in a finite set of ReLU's with a bias as for instance</p>
<p>$$
x \times y=\operatorname{ReLU}(x+y-1)+\operatorname{ReLU}(-x-y-1)-\operatorname{ReLU}(x-y-1)-\operatorname{ReLU}(-x+y-1)
$$</p>
<p>Changing the network architecture may aid it's ability to perform multiplication. Changing activation function to $x^{2}$ for instance permits multiplication generically as</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ If $x, y \in{0,1}$ then $x$ XOR $y=\operatorname{ReLU}(x-y)+\operatorname{ReLU}(y-x)$ is a solution in two neurons.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{1}$ Code and a demo notebook are available at https://github.com/bilal-chughtai/rep-theory-mech-interp
${ }^{2}$ We note our use of the word 'representation' is distinct to the&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
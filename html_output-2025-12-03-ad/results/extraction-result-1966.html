<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1966 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1966</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1966</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-41.html">extraction-schema-41</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <p><strong>Paper ID:</strong> paper-276408331</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.10894v1.pdf" target="_blank">Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation</a></p>
                <p><strong>Paper Abstract:</strong> Achieving athletic loco-manipulation on robots requires moving beyond traditional tracking rewards - which simply guide the robot along a reference trajectory - to task rewards that drive truly dynamic, goal-oriented behaviors. Commands such as"throw the ball as far as you can"or"lift the weight as quickly as possible"compel the robot to exhibit the agility and power inherent in athletic performance. However, training solely with task rewards introduces two major challenges: these rewards are prone to exploitation (reward hacking), and the exploration process can lack sufficient direction. To address these issues, we propose a two-stage training pipeline. First, we introduce the Unsupervised Actuator Net (UAN), which leverages real-world data to bridge the sim-to-real gap for complex actuation mechanisms without requiring access to torque sensing. UAN mitigates reward hacking by ensuring that the learned behaviors remain robust and transferable. Second, we use a pre-training and fine-tuning strategy that leverages reference trajectories as initial hints to guide exploration. With these innovations, our robot athlete learns to lift, throw, and drag with remarkable fidelity from simulation to reality.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1966.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1966.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UAN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised Actuator Net</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A learned residual corrective actuator model trained from real joint-encoder rollouts (no torque sensors) that outputs corrective torques δτ to the simulator to minimize transition mismatch between sim and real, capturing lag, nonlinear friction, and hysteresis from harmonic drive transmissions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Athletic loco-manipulation tasks (arm-only throwing ablations; whole-body ball throwing, dumbbell snatch (lifting), sled pulling)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>throw phases ≈ 2.5–3.5s (training episodes 20s); whole-body episodes up to tens of seconds</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>mixed: throwing is low-contact (grasp then ballistic); dumbbell snatch and sled pull are contact-rich</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>corrective output torque residuals that capture actuator lag, nonlinear friction, hysteresis and reflected transmission effects from harmonic drives; implicitly models lag/delay and non-linearities rather than explicit Coulomb/viscous coefficients</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>High-fidelity residual model for arm actuators: learned corrective torques trained on real rollouts combined with rigid-body simulation for other dynamics (hybrid learned+physics approach). No domain randomization applied to arm actuator properties when UAN is used.</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Mean-square joint position error on training and unseen test trajectories; task metrics: throw distance (m), maximum lifted mass (kg), sled pull force/distance (N/m). Example reported values: real ball throw ≈ 20 m (hardware), lifting policies could lift up to 8 kg in sim and lifted 5–10 lb on hardware, sled pulled against 113 N over 10 m on hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Qualitatively smallest sim-to-real gap among compared methods; for throwing the real robot threw slightly farther than sim (UAN produced farthest throws on hardware). Exact numerical gap not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td>Ablations against Default, DR, ROA, supervised Actuator Net, and CEM show actuator modeling (lag and nonlinearities introduced by harmonic reducers) is critical for transfer; UAN best fits training/test trajectories and generalizes to unseen throw trajectory; CEM reduced overshoot by slowing arm; supervised Actuator Net captured lag but diverged on long rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Yes — compared against Default (no correction), Domain Randomization (DR), Regularized Online Adaptation (ROA), supervised Actuator Net (torque labels from motor current), and CEM optimization of friction/armature; UAN achieved the best joint-position fit and the best hardware throwing performance and smallest sim-to-real gap.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Overall pipeline used DR for many system params (terrain roughness, friction, restitution, link mass and COM, PD gains and stall torques for leg actuators, policy lag length). Important: when UAN is used, no DR was applied to arm joint properties — the arm was calibrated via UAN instead.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Unitree B2 quadruped with Unitree Z1 Pro arm (mobile quadruped manipulator), 19 actuated joints (3 per leg ×4 + 6 arm + 1 gripper).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>When UAN is absent, inaccuracies in modeling harmonic drives (nonlinear friction, hysteresis, lag) cause reward-hacking and unstable behaviors; Default/DR/ROA baselines produced unstable or failing throws and worse hardware performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Explicitly learning a residual corrective torque model from real encoder transitions (without torque sensors) that captures lag and nonlinear transmission effects (friction, hysteresis) is sufficient to substantially reduce sim-to-real gap for high-acceleration, athletic loco-manipulation; harmonic-drive-specific nonlinearities, not just simple parameter randomization, are critical to model.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1966.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1966.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Actuator Net (supervised)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Actuator Network (torque labels from motor current)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-network actuator model trained in a supervised fashion using estimated torques from motor current measurements to predict actuator output torque (following prior actuator-net approaches), used here as an ablation for the Z1 Pro arm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arm-only throwing (ablation) and general arm behavior modeling</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>training rollouts included up to 5-minute trajectories; test throw trajectory ~5s</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>low-contact (ball throwing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>learned mapping from history of position/velocity errors to output torque using torque labels estimated from motor current; models lag effects and some dynamics captured by current-derived torque estimates</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Relies on motor current→torque estimate as a surrogate for output torque (simplifies/ignores nonlinearities introduced by harmonic reducers and reflected inertia inaccuracies), which is inaccurate for high-reduction harmonic drives</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>Moderate-fidelity learned model constrained by quality of torque labels; can capture lag in short rollouts but fails in long rollouts for harmonic-drive arm.</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Mean-square joint position error on training and test rollouts; throw distance on hardware (performed worse than UAN). No absolute numeric MSE values reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Improved over baseline in simulation but transferred worse than UAN; diverged on long (5 min) training rollouts and produced worse hardware throw performance.</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td>Actuator Net captured some lag but diverged on long rollouts; supervised torque-label strategy insufficient to capture harmonic reducer nonlinearities, leading to poorer transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Compared directly to UAN, CEM, DR, ROA and Default — Actuator Net improved over baseline in short regimes but performed worse than UAN on generalization and hardware transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>When used for arm modeling, no DR of arm joint properties was applied (same as UAN experiment setup); overall pipeline used DR for other subsystems.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Unitree B2 quadruped with Unitree Z1 Pro arm</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Failure attributed to reliance on motor current as torque proxy and inability to capture nonlinearities (hysteresis, friction) of harmonic drives, causing long-run divergence and poor transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Supervised actuator nets trained on current-derived torque labels can capture lag but are insufficient for harmonic drives with strong nonlinearities and reflected inertia; unlabeled residual approaches (UAN) better capture the necessary dynamics for robust sim-to-real.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1966.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1966.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CEM identification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-Entropy Method (parameter optimization of friction/armature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parameter-search approach using cross-entropy method to optimize friction, frictional damping, and armature parameters to minimize mean-square joint position error between simulation and hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Actuator modeling ablation evaluated on arm rollouts and throwing</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>training/test rollouts (seconds to minutes); specific throw trajectory ~5s</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>low-contact for throwing experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>explicit physics parameters (static/dynamic friction, frictional damping, armature/inertia) adjusted to better match joint position trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Limited parameter set (friction/damping/armature) — does not model hysteresis or more complex nonlinear lag/hysteresis introduced by harmonic reducers</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>Physics-parameter tuning (higher fidelity than Default/DR when tuned) but still limited by the parametric form; produced reduced overshoot by effectively slowing actuator response</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Mean-square joint position error on training data and unseen test trajectory; throwing performance on hardware (worse than UAN).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Improved simulator fit relative to some baselines and reduced overshoot; nonetheless did not transfer as well as UAN for throwing (larger sim-to-real gap than UAN).</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td>CEM tuning reduced overshoot and matched lower joint velocities seen on hardware (slowing actuator), demonstrating friction/armature parameters affect overshoot and response speed; however, it could not capture full nonlinear behavior of harmonic drives.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Compared against learned residuals (UAN, Actuator Net) and DR/ROA/Default; CEM improved fit in some respects but underperformed relative to UAN.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>CEM tuned physics parameters directly; arm DR was not applied when using explicit calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Unitree B2 quadruped with Unitree Z1 Pro arm</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Limitations due to restricted parameterization — cannot capture hysteresis and more complex lag behaviors imposed by harmonic reducers, leading to suboptimal transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Optimizing a small set of physics parameters (friction, damping, armature) can reduce overshoot and help match slow responses, but cannot fully replace a residual learned model for actuators with complex non-linear transmission effects.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1966.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1966.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DR baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain Randomization baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard domain randomization of dynamics and environment parameters (PD gains, friction, armature, masses, terrain) applied to improve robustness to sim-to-real mismatch; used as a baseline for actuator modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Pre-training / finetuning tasks (locomotion and loco-manipulation, incl. throwing ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>pre-training and finetuning episodes (seconds to many seconds); command sampling every 7s</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>varies by task (mixed across tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>PD gains, stall torques, friction, armature (randomized ranges rather than specifically identified)</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Randomized parameter ranges (no explicit modeling of harmonic-drive-specific nonlinearities, hysteresis or lag beyond the chosen randomization ranges)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>Lower-fidelity mitigation strategy: sampling over parameter distributions to gain robustness rather than directly modeling hardware-specific nonlinearities</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task rewards and metrics (throw distance, joint position errors); policies trained with DR alone produced unstable behaviors on hardware for athletic arm tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Poor transfer for highly dynamic arm tasks: Default/DR baselines produced unstable behaviors and failed to throw ball in hardware ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td>DR alone insufficient to cover nonlinearities present in harmonic-drive arm; excessive DR can also reduce peak performance if policy cannot identify key parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Compared to UAN and other identification methods; DR underperformed relative to UAN for arm-dominated athletic tasks though still used for legs and environment randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Terrain roughness, friction, restitution, link mass/CoM, PD gains and stall torques for leg actuators, policy lag length; ranges provided in appendix (not numerically repeated in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Unitree B2 quadruped with Unitree Z1 Pro arm</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>DR did not capture harmonic-drive-specific non-linearities (hysteresis, friction, lag), allowing reward-hacking and unstable policies in high-acceleration arm tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Domain randomization of generic actuator parameters is insufficient to guarantee transfer for actuators with complex transmission nonlinearities; explicit real-to-sim actuator calibration is necessary for high-acceleration manipulation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1966.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1966.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Regularized Online Adaptation (online system identification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online system-identification module (Regularized Online Adaptation) used as a baseline to adapt simulated dynamics online during policy execution; evaluated against UAN and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Whole-body loco-manipulation and arm throwing ablations</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>online adaptation during episodes (seconds); pre-training/finetuning timescales similar to other methods</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>varies by task (mixed)</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>online identification of selected simulator parameters (not detailed per-parameter in main text), intended to adapt to varying dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Relies on a parametric adaptation model and priors; not targeted to capture harmonic-drive hysteresis/lag in detail</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>Adaptive low-to-moderate fidelity: online identification supplements DR but may be limited by parametric model assumptions</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task performance and simulated-vs-real behavior in throwing ablations; ROA produced unstable behaviors on arm throwing compared to UAN.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Did not match UAN; ROA policies produced unstable behaviors in hardware throwing ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td>Online adaptation (ROA) and DR together did not sufficiently capture arm nonlinearities introduced by harmonic drives; explicit actuator residual modeling performed better.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Compared to Default/DR/ActuatorNet/CEM/UAN — ROA underperformed relative to UAN for arm tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>ROA was tested as augmentation to DR baselines (DR ranges as above); details in appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Unitree B2 quadruped with Unitree Z1 Pro arm</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>ROA's parametric online identification and regularization did not capture the harmonic-drive-specific nonlinear phenomena, leading to unstable deployment for athletic arm tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>Online parametric system identification can help, but for complex transmission nonlinearities (harmonic drives) a learned residual corrective model trained on real state transitions is more effective for sim-to-real transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1966.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1966.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer experiments for robotic manipulation that report details about actuator dynamics modeling, parameter fidelity, task characteristics, and transfer performance.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Default baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Default simulator (no actuator correction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline simulator configuration with no additional actuator calibration or learned residuals; used to show the degree of sim-to-real mismatch for athletic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arm throwing and whole-body loco-manipulation baselines</td>
                        </tr>
                        <tr>
                            <td><strong>task_timescale</strong></td>
                            <td>same episodic timescales as the other experiments (seconds to tens of seconds)</td>
                        </tr>
                        <tr>
                            <td><strong>task_contact_ratio</strong></td>
                            <td>mixed depending on task (throwing low-contact; lifting and sled pull contact-rich)</td>
                        </tr>
                        <tr>
                            <td><strong>task_precision_requirement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_modeled</strong></td>
                            <td>Standard simulator actuator model (PD-controlled actuators with default parameters from URDF and simulator defaults)</td>
                        </tr>
                        <tr>
                            <td><strong>actuator_parameters_simplified</strong></td>
                            <td>Ignored harmonic-drive-specific nonlinearities, hysteresis, reflected inertia effects and complex lag behaviors</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level_description</strong></td>
                            <td>Low-fidelity actuator modeling for harmonic-drive arm; standard physics-only sim with default parameters</td>
                        </tr>
                        <tr>
                            <td><strong>parameter_specific_fidelity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success and stability on hardware (e.g., throw distance) — Default failed to throw in hardware ablation and exhibited unstable behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_vs_real_performance</strong></td>
                            <td>Large sim-to-real gap; policies trained in Default sim produced unstable behaviors and failed to transfer for high-acceleration arm tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>sensitivity_analysis_results</strong></td>
                            <td>Demonstrated that lack of actuator-specific calibration leads to reward-hacking and failure to transfer, motivating the need for UAN or other calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_reported</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>Serves as lowest-fidelity comparison in the ablations; performed worst compared to DR, ROA, Actuator Net, CEM, and UAN.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>robot_type</strong></td>
                            <td>Unitree B2 quadruped with Unitree Z1 Pro arm</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_analysis</strong></td>
                            <td>Failures are attributed to unmodeled harmonic-drive effects (nonlinear friction, hysteresis, lag) and inaccurate torque estimation — leading to reward hacking and unstable policies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_for_theory</strong></td>
                            <td>A default physics simulator without actuator-specific calibration can be grossly misleading for high-acceleration manipulation tasks; actuator-specific dynamics (especially for harmonic drives) must be modeled or learned for successful sim-to-real transfer.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning agile and dynamic motor skills for legged robots <em>(Rating: 2)</em></li>
                <li>Tossingbot: Learning to throw arbitrary objects with residual physics <em>(Rating: 2)</em></li>
                <li>Improving domain transfer of robot dynamics models with geometric system identification and learned friction compensation <em>(Rating: 2)</em></li>
                <li>Residual model learning for microrobot control <em>(Rating: 1)</em></li>
                <li>Learning quadrupedal locomotion over challenging terrain <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1966",
    "paper_id": "paper-276408331",
    "extraction_schema_id": "extraction-schema-41",
    "extracted_data": [
        {
            "name_short": "UAN",
            "name_full": "Unsupervised Actuator Net",
            "brief_description": "A learned residual corrective actuator model trained from real joint-encoder rollouts (no torque sensors) that outputs corrective torques δτ to the simulator to minimize transition mismatch between sim and real, capturing lag, nonlinear friction, and hysteresis from harmonic drive transmissions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_name": "Athletic loco-manipulation tasks (arm-only throwing ablations; whole-body ball throwing, dumbbell snatch (lifting), sled pulling)",
            "task_timescale": "throw phases ≈ 2.5–3.5s (training episodes 20s); whole-body episodes up to tens of seconds",
            "task_contact_ratio": "mixed: throwing is low-contact (grasp then ballistic); dumbbell snatch and sled pull are contact-rich",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "corrective output torque residuals that capture actuator lag, nonlinear friction, hysteresis and reflected transmission effects from harmonic drives; implicitly models lag/delay and non-linearities rather than explicit Coulomb/viscous coefficients",
            "actuator_parameters_simplified": null,
            "fidelity_level_description": "High-fidelity residual model for arm actuators: learned corrective torques trained on real rollouts combined with rigid-body simulation for other dynamics (hybrid learned+physics approach). No domain randomization applied to arm actuator properties when UAN is used.",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Mean-square joint position error on training and unseen test trajectories; task metrics: throw distance (m), maximum lifted mass (kg), sled pull force/distance (N/m). Example reported values: real ball throw ≈ 20 m (hardware), lifting policies could lift up to 8 kg in sim and lifted 5–10 lb on hardware, sled pulled against 113 N over 10 m on hardware.",
            "sim_vs_real_performance": "Qualitatively smallest sim-to-real gap among compared methods; for throwing the real robot threw slightly farther than sim (UAN produced farthest throws on hardware). Exact numerical gap not reported.",
            "sensitivity_analysis_performed": true,
            "sensitivity_analysis_results": "Ablations against Default, DR, ROA, supervised Actuator Net, and CEM show actuator modeling (lag and nonlinearities introduced by harmonic reducers) is critical for transfer; UAN best fits training/test trajectories and generalizes to unseen throw trajectory; CEM reduced overshoot by slowing arm; supervised Actuator Net captured lag but diverged on long rollouts.",
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "Yes — compared against Default (no correction), Domain Randomization (DR), Regularized Online Adaptation (ROA), supervised Actuator Net (torque labels from motor current), and CEM optimization of friction/armature; UAN achieved the best joint-position fit and the best hardware throwing performance and smallest sim-to-real gap.",
            "domain_randomization_used": true,
            "domain_randomization_details": "Overall pipeline used DR for many system params (terrain roughness, friction, restitution, link mass and COM, PD gains and stall torques for leg actuators, policy lag length). Important: when UAN is used, no DR was applied to arm joint properties — the arm was calibrated via UAN instead.",
            "robot_type": "Unitree B2 quadruped with Unitree Z1 Pro arm (mobile quadruped manipulator), 19 actuated joints (3 per leg ×4 + 6 arm + 1 gripper).",
            "transfer_failure_analysis": "When UAN is absent, inaccuracies in modeling harmonic drives (nonlinear friction, hysteresis, lag) cause reward-hacking and unstable behaviors; Default/DR/ROA baselines produced unstable or failing throws and worse hardware performance.",
            "key_finding_for_theory": "Explicitly learning a residual corrective torque model from real encoder transitions (without torque sensors) that captures lag and nonlinear transmission effects (friction, hysteresis) is sufficient to substantially reduce sim-to-real gap for high-acceleration, athletic loco-manipulation; harmonic-drive-specific nonlinearities, not just simple parameter randomization, are critical to model.",
            "uuid": "e1966.0"
        },
        {
            "name_short": "Actuator Net (supervised)",
            "name_full": "Supervised Actuator Network (torque labels from motor current)",
            "brief_description": "A neural-network actuator model trained in a supervised fashion using estimated torques from motor current measurements to predict actuator output torque (following prior actuator-net approaches), used here as an ablation for the Z1 Pro arm.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_name": "Arm-only throwing (ablation) and general arm behavior modeling",
            "task_timescale": "training rollouts included up to 5-minute trajectories; test throw trajectory ~5s",
            "task_contact_ratio": "low-contact (ball throwing)",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "learned mapping from history of position/velocity errors to output torque using torque labels estimated from motor current; models lag effects and some dynamics captured by current-derived torque estimates",
            "actuator_parameters_simplified": "Relies on motor current→torque estimate as a surrogate for output torque (simplifies/ignores nonlinearities introduced by harmonic reducers and reflected inertia inaccuracies), which is inaccurate for high-reduction harmonic drives",
            "fidelity_level_description": "Moderate-fidelity learned model constrained by quality of torque labels; can capture lag in short rollouts but fails in long rollouts for harmonic-drive arm.",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Mean-square joint position error on training and test rollouts; throw distance on hardware (performed worse than UAN). No absolute numeric MSE values reported in text.",
            "sim_vs_real_performance": "Improved over baseline in simulation but transferred worse than UAN; diverged on long (5 min) training rollouts and produced worse hardware throw performance.",
            "sensitivity_analysis_performed": true,
            "sensitivity_analysis_results": "Actuator Net captured some lag but diverged on long rollouts; supervised torque-label strategy insufficient to capture harmonic reducer nonlinearities, leading to poorer transfer.",
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "Compared directly to UAN, CEM, DR, ROA and Default — Actuator Net improved over baseline in short regimes but performed worse than UAN on generalization and hardware transfer.",
            "domain_randomization_used": false,
            "domain_randomization_details": "When used for arm modeling, no DR of arm joint properties was applied (same as UAN experiment setup); overall pipeline used DR for other subsystems.",
            "robot_type": "Unitree B2 quadruped with Unitree Z1 Pro arm",
            "transfer_failure_analysis": "Failure attributed to reliance on motor current as torque proxy and inability to capture nonlinearities (hysteresis, friction) of harmonic drives, causing long-run divergence and poor transfer.",
            "key_finding_for_theory": "Supervised actuator nets trained on current-derived torque labels can capture lag but are insufficient for harmonic drives with strong nonlinearities and reflected inertia; unlabeled residual approaches (UAN) better capture the necessary dynamics for robust sim-to-real.",
            "uuid": "e1966.1"
        },
        {
            "name_short": "CEM identification",
            "name_full": "Cross-Entropy Method (parameter optimization of friction/armature)",
            "brief_description": "A parameter-search approach using cross-entropy method to optimize friction, frictional damping, and armature parameters to minimize mean-square joint position error between simulation and hardware.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_name": "Actuator modeling ablation evaluated on arm rollouts and throwing",
            "task_timescale": "training/test rollouts (seconds to minutes); specific throw trajectory ~5s",
            "task_contact_ratio": "low-contact for throwing experiments",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "explicit physics parameters (static/dynamic friction, frictional damping, armature/inertia) adjusted to better match joint position trajectories",
            "actuator_parameters_simplified": "Limited parameter set (friction/damping/armature) — does not model hysteresis or more complex nonlinear lag/hysteresis introduced by harmonic reducers",
            "fidelity_level_description": "Physics-parameter tuning (higher fidelity than Default/DR when tuned) but still limited by the parametric form; produced reduced overshoot by effectively slowing actuator response",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Mean-square joint position error on training data and unseen test trajectory; throwing performance on hardware (worse than UAN).",
            "sim_vs_real_performance": "Improved simulator fit relative to some baselines and reduced overshoot; nonetheless did not transfer as well as UAN for throwing (larger sim-to-real gap than UAN).",
            "sensitivity_analysis_performed": true,
            "sensitivity_analysis_results": "CEM tuning reduced overshoot and matched lower joint velocities seen on hardware (slowing actuator), demonstrating friction/armature parameters affect overshoot and response speed; however, it could not capture full nonlinear behavior of harmonic drives.",
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "Compared against learned residuals (UAN, Actuator Net) and DR/ROA/Default; CEM improved fit in some respects but underperformed relative to UAN.",
            "domain_randomization_used": false,
            "domain_randomization_details": "CEM tuned physics parameters directly; arm DR was not applied when using explicit calibration.",
            "robot_type": "Unitree B2 quadruped with Unitree Z1 Pro arm",
            "transfer_failure_analysis": "Limitations due to restricted parameterization — cannot capture hysteresis and more complex lag behaviors imposed by harmonic reducers, leading to suboptimal transfer.",
            "key_finding_for_theory": "Optimizing a small set of physics parameters (friction, damping, armature) can reduce overshoot and help match slow responses, but cannot fully replace a residual learned model for actuators with complex non-linear transmission effects.",
            "uuid": "e1966.2"
        },
        {
            "name_short": "DR baseline",
            "name_full": "Domain Randomization baseline",
            "brief_description": "Standard domain randomization of dynamics and environment parameters (PD gains, friction, armature, masses, terrain) applied to improve robustness to sim-to-real mismatch; used as a baseline for actuator modeling.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_name": "Pre-training / finetuning tasks (locomotion and loco-manipulation, incl. throwing ablations)",
            "task_timescale": "pre-training and finetuning episodes (seconds to many seconds); command sampling every 7s",
            "task_contact_ratio": "varies by task (mixed across tasks)",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "PD gains, stall torques, friction, armature (randomized ranges rather than specifically identified)",
            "actuator_parameters_simplified": "Randomized parameter ranges (no explicit modeling of harmonic-drive-specific nonlinearities, hysteresis or lag beyond the chosen randomization ranges)",
            "fidelity_level_description": "Lower-fidelity mitigation strategy: sampling over parameter distributions to gain robustness rather than directly modeling hardware-specific nonlinearities",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Task rewards and metrics (throw distance, joint position errors); policies trained with DR alone produced unstable behaviors on hardware for athletic arm tasks.",
            "sim_vs_real_performance": "Poor transfer for highly dynamic arm tasks: Default/DR baselines produced unstable behaviors and failed to throw ball in hardware ablations.",
            "sensitivity_analysis_performed": true,
            "sensitivity_analysis_results": "DR alone insufficient to cover nonlinearities present in harmonic-drive arm; excessive DR can also reduce peak performance if policy cannot identify key parameters.",
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "Compared to UAN and other identification methods; DR underperformed relative to UAN for arm-dominated athletic tasks though still used for legs and environment randomization.",
            "domain_randomization_used": true,
            "domain_randomization_details": "Terrain roughness, friction, restitution, link mass/CoM, PD gains and stall torques for leg actuators, policy lag length; ranges provided in appendix (not numerically repeated in main text).",
            "robot_type": "Unitree B2 quadruped with Unitree Z1 Pro arm",
            "transfer_failure_analysis": "DR did not capture harmonic-drive-specific non-linearities (hysteresis, friction, lag), allowing reward-hacking and unstable policies in high-acceleration arm tasks.",
            "key_finding_for_theory": "Domain randomization of generic actuator parameters is insufficient to guarantee transfer for actuators with complex transmission nonlinearities; explicit real-to-sim actuator calibration is necessary for high-acceleration manipulation.",
            "uuid": "e1966.3"
        },
        {
            "name_short": "ROA",
            "name_full": "Regularized Online Adaptation (online system identification)",
            "brief_description": "An online system-identification module (Regularized Online Adaptation) used as a baseline to adapt simulated dynamics online during policy execution; evaluated against UAN and other baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_name": "Whole-body loco-manipulation and arm throwing ablations",
            "task_timescale": "online adaptation during episodes (seconds); pre-training/finetuning timescales similar to other methods",
            "task_contact_ratio": "varies by task (mixed)",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "online identification of selected simulator parameters (not detailed per-parameter in main text), intended to adapt to varying dynamics",
            "actuator_parameters_simplified": "Relies on a parametric adaptation model and priors; not targeted to capture harmonic-drive hysteresis/lag in detail",
            "fidelity_level_description": "Adaptive low-to-moderate fidelity: online identification supplements DR but may be limited by parametric model assumptions",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Task performance and simulated-vs-real behavior in throwing ablations; ROA produced unstable behaviors on arm throwing compared to UAN.",
            "sim_vs_real_performance": "Did not match UAN; ROA policies produced unstable behaviors in hardware throwing ablation.",
            "sensitivity_analysis_performed": true,
            "sensitivity_analysis_results": "Online adaptation (ROA) and DR together did not sufficiently capture arm nonlinearities introduced by harmonic drives; explicit actuator residual modeling performed better.",
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "Compared to Default/DR/ActuatorNet/CEM/UAN — ROA underperformed relative to UAN for arm tasks.",
            "domain_randomization_used": true,
            "domain_randomization_details": "ROA was tested as augmentation to DR baselines (DR ranges as above); details in appendix.",
            "robot_type": "Unitree B2 quadruped with Unitree Z1 Pro arm",
            "transfer_failure_analysis": "ROA's parametric online identification and regularization did not capture the harmonic-drive-specific nonlinear phenomena, leading to unstable deployment for athletic arm tasks.",
            "key_finding_for_theory": "Online parametric system identification can help, but for complex transmission nonlinearities (harmonic drives) a learned residual corrective model trained on real state transitions is more effective for sim-to-real transfer.",
            "uuid": "e1966.4"
        },
        {
            "name_short": "Default baseline",
            "name_full": "Default simulator (no actuator correction)",
            "brief_description": "Baseline simulator configuration with no additional actuator calibration or learned residuals; used to show the degree of sim-to-real mismatch for athletic tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_name": "Arm throwing and whole-body loco-manipulation baselines",
            "task_timescale": "same episodic timescales as the other experiments (seconds to tens of seconds)",
            "task_contact_ratio": "mixed depending on task (throwing low-contact; lifting and sled pull contact-rich)",
            "task_precision_requirement": null,
            "actuator_parameters_modeled": "Standard simulator actuator model (PD-controlled actuators with default parameters from URDF and simulator defaults)",
            "actuator_parameters_simplified": "Ignored harmonic-drive-specific nonlinearities, hysteresis, reflected inertia effects and complex lag behaviors",
            "fidelity_level_description": "Low-fidelity actuator modeling for harmonic-drive arm; standard physics-only sim with default parameters",
            "parameter_specific_fidelity": null,
            "transfer_success_metric": "Task success and stability on hardware (e.g., throw distance) — Default failed to throw in hardware ablation and exhibited unstable behaviors.",
            "sim_vs_real_performance": "Large sim-to-real gap; policies trained in Default sim produced unstable behaviors and failed to transfer for high-acceleration arm tasks.",
            "sensitivity_analysis_performed": true,
            "sensitivity_analysis_results": "Demonstrated that lack of actuator-specific calibration leads to reward-hacking and failure to transfer, motivating the need for UAN or other calibration.",
            "computational_cost_reported": false,
            "computational_cost_details": null,
            "fidelity_comparison": "Serves as lowest-fidelity comparison in the ablations; performed worst compared to DR, ROA, Actuator Net, CEM, and UAN.",
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "robot_type": "Unitree B2 quadruped with Unitree Z1 Pro arm",
            "transfer_failure_analysis": "Failures are attributed to unmodeled harmonic-drive effects (nonlinear friction, hysteresis, lag) and inaccurate torque estimation — leading to reward hacking and unstable policies.",
            "key_finding_for_theory": "A default physics simulator without actuator-specific calibration can be grossly misleading for high-acceleration manipulation tasks; actuator-specific dynamics (especially for harmonic drives) must be modeled or learned for successful sim-to-real transfer.",
            "uuid": "e1966.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning agile and dynamic motor skills for legged robots",
            "rating": 2
        },
        {
            "paper_title": "Tossingbot: Learning to throw arbitrary objects with residual physics",
            "rating": 2
        },
        {
            "paper_title": "Improving domain transfer of robot dynamics models with geometric system identification and learned friction compensation",
            "rating": 2
        },
        {
            "paper_title": "Residual model learning for microrobot control",
            "rating": 1
        },
        {
            "paper_title": "Learning quadrupedal locomotion over challenging terrain",
            "rating": 1
        }
    ],
    "cost": 0.02089125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation</p>
<p>Nolan Fey 
Improbable AI Lab Massachusetts Institute of Technology
02139CambridgeMA</p>
<p>Gabriel B Margolis 
Improbable AI Lab Massachusetts Institute of Technology
02139CambridgeMA</p>
<p>Martin Peticco 
Improbable AI Lab Massachusetts Institute of Technology
02139CambridgeMA</p>
<p>Pulkit Agrawal 
Improbable AI Lab Massachusetts Institute of Technology
02139CambridgeMA</p>
<p>Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation
E1167264E614F4F692CE7827A2703903
Achieving athletic loco-manipulation on robots requires moving beyond traditional tracking rewards-which simply guide the robot along a reference trajectory-to task rewards that drive truly dynamic, goal-oriented behaviors.Commands such as "throw the ball as far as you can" or "lift the weight as quickly as possible" compel the robot to exhibit the agility and power inherent in athletic performance.However, training solely with task rewards introduces two major challenges: these rewards are prone to exploitation (reward hacking), and the exploration process can lack sufficient direction.To address these issues, we propose a two-stage training pipeline.First, we introduce the Unsupervised Actuator Net (UAN), which leverages real-world data to bridge the sim-to-real gap for complex actuation mechanisms without requiring access to torque sensing.UAN mitigates reward hacking by ensuring that the learned behaviors remain robust and transferable.Second, we use a pre-training and fine-tuning strategy that leverages reference trajectories as initial hints to guide exploration.With these innovations, our robot athlete learns to lift, throw, and drag with remarkable fidelity from simulation to reality.</p>
<p>I. INTRODUCTION</p>
<p>General whole-body control comes naturally to animals after years of evolution, yet it remains a long-standing challenge in robotics.Fluid whole-body motion requires balancing multiple competing tasks and constraints that depend on both the robot's morphology and its environment [40].Recent work [6,32] demonstrates that sim-to-real reinforcement learning (RL), using methods such as Proximal Policy Optimization (PPO) [37], is a promising paradigm for learning these behaviors by leveraging parallel simulations [36].</p>
<p>For dynamic, goal-oriented loco-manipulation, it is natural to train robots with task rewards-commands like "throw the ball as far as possible" or "lift the weight as quickly as possible" that drive athletic behaviors.However, these task rewards pose two major challenges: (i) they are prone to reward hacking, where the policy exploits imperfections in the simulation, and (ii) the exploration process can lack sufficient guidance.To circumvent these issues, many works on simto-real transfer instead train whole-body controllers (WBCs) to track dense reference motions [4,6,12,21,32].Dense tracking objectives provide strong regularization by constraining the policy to adhere to a reference trajectory-thereby reducing reward hacking-and they offer a structured path for 1 Authors are also affiliated with Computer Science and Artificial Laboratory (CSAIL), the Laboratory for Information and Decision Systems (LIDS), and the MIT-IBM Watson AI Lab at MIT. Correspondence to nolanfey@mit.edu</p>
<p>Ball Throw</p>
<p>Dumbbell Snatch Sled Pull Fig. 1: Sim-to-real transfer of athletic loco-manipulation.We reduce the sim-to-real gap for a quadruped manipulator by learning a corrective model for the simulated actuator dynamics based on real-world data, formulated as an unsupervised actuator net (UAN).Policies trained with the corrected simulator exhibit improved sim-to-real transfer and push the limits of the robot's physical capabilities in athletic tasks involving whole-body coordination.Videos of the robot's behaviors are available at https://uan.csail.mit.edu/.exploration.However, this strategy relies on defining highquality reference commands a priori, which in turn demands access to high-quality reference data.For robots with nonhuman morphologies like legged manipulators, obtaining such data is particularly challenging, and the resulting reference commands may not capture the optimal, athletic strategies that a policy might otherwise discover.</p>
<p>To fully harness the benefits of task rewards, it is crucial to ensure that the simulation faithfully replicates real-world arXiv:2502.10894v1[cs.RO] 15 Feb 2025 dynamics.Inaccurate simulation models allow policies to exploit imperfections, leading to reward hacking, particularly so when the reward is underspecified.Although techniques like domain randomization [46,47,49] and online system identification [14,17,22,27,29,33] address this by sampling over parameter distributions, they rely on a priori assumptions that may not fully capture the complex dynamics of real hardware.For instance, harmonic drive actuators exhibit non-linear friction, hysteresis, and lag-behaviors that render traditional proxies like motor current unreliable for torque estimation.</p>
<p>A promising alternative is to enhance the simulation's physics model directly with real-world data, focusing on accurately modeling the actuator dynamics.With this motivation, we introduce the Unsupervised Actuator Net (UAN), a framework for learning corrective actuator models without the need for torque sensors.UAN is trained using reinforcement learning to predict corrective torques, δτ = π UAN (e), by minimizing discrepancies between simulated and real-world joint encoder measurements.In doing so, UAN effectively bridges the sim-to-real gap even for robots with complex transmission mechanisms and noisy or unavailable torque measurements.</p>
<p>Building on this enhanced simulation environment, we address the challenge of guided exploration for athletic behaviors.Rather than enforcing strict adherence to a reference trajectory, we propose treating it as a hint to guide exploration.In our approach, a WBC is first pre-trained on random base velocities and end-effector pose commands to establish a strong motion prior.Then, to learn a new athletic behavior, we initialize the controller with a reference trajectory and finetune it using a task-specific reward-allowing the policy to depart from the reference when beneficial.</p>
<p>In summary, our paper presents an easy-to-use training pipeline for whole-body athletic behaviors that reliably transfer to reality.First, we employ the Unsupervised Actuator Net (UAN) to calibrate actuator dynamics and mitigate reward hacking, ensuring our simulator accurately reflects real-world physics.With this improved simulation environment, we then pre-train a whole-body controller (WBC) to establish fundamental motion skills and fine-tune it with task-specific rewards-using a reference trajectory merely as a hint to guide exploration.This integrated approach enables our robot to perform dynamic tasks such as throwing, lifting, and dragging with remarkable fidelity.</p>
<p>II. METHOD</p>
<p>Our training pipeline (see Figure 2) is separated into two phases: 1) real-to-sim calibration (Section II-A) and 2) WBC training (Sections II-B and II-C).The real-to-sim calibration phase involves collecting data on the real robot and training a UAN to close the sim-to-real gap for non-ideal actuation mechanisms.Similar to past work [7,21,31,42], our WBC training is split into two distinct sub-phases: pre-training (Section II-B) and fine-tuning (Section II-C).After pre-training, the policy can track reference trajectories if provided as a sequence of base velocity and end effector pose commands.During the fine-tuning phase, the policy observes a reference task trajectory.This helps warm start exploration when learning a new task because the policy can simply track these commands to achieve reasonable task performance.Through training with the task reward itself rather than a tracking reward, the policy learns how to depart from the reference trajectory to achieve higher task performance.Our simulation environments for the pre-training and fine-tuning phases rely on the same strategies for sim-to-real transfer, including domain randomization (Section II-B) and a UAN (Section II-A).</p>
<p>Our experiments consider a Unitree B2 quadruped with a modified Unitree Z1 Pro arm mounted on its back.The quadruped is 65 cm tall when standing and weighs 60 kg, while the arm is 74 cm fully extended and weighs 6.8 kg.The system has 19 actuated joints: 3 for each leg, 6 for the arm, and 1 for the gripper.</p>
<p>A. Unsupervised Actuator Net</p>
<p>Some actuators are challenging to model in simulation, especially when they have complex transmission mechanisms.In such cases, standard domain randomization and online system identification techniques may be insufficient, and instead, it is preferable to learn to model the actuator directly from hardware data.Previous approaches rely on output torque sensing [13], which is still uncommon in consumer hardware, to learn how to predict the motor's torque.Alternatively, we propose a method for matching the transition dynamics of the actuator such that
min fsim ||f real (s, τ ) − f sim (s, τ )||.(1)
To influence the simulator dynamics, f sim , we learn a residual model, π UAN (e, that observes a history of position and velocity errors, e, and outputs a corrective torque, δτ , for the simulator such that
min πUAN ||f real s, τ − f sim (s, τ + π UAN (e)) ||.(2)
The corrective torques needed to minimize the transition error are unlabeled, so we parametrize π UAN as a neural network and train it with RL. 1) Architecture and observation space: The network is designed as a 2-layer MLP with layer sizes [128, 128] and ELU activations.It is executed at every simulation time step (5 ms).Assuming each arm joint is identical, a single UAN is shared across all of the arm's actuators, with each actuator being processed independently by the shared network [13].We constrain the observation space to include a history of the past 20 (equivalent to 100 ms) position and velocity errors for each relevant actuator.These design choices help prevent overfitting to other aspects of the training data, such as inertial coupling.Also, sharing the data across actuators improves data efficiency.For example, the actuator net is trained on data with various loads, as actuators closer to the robot's base generally experience more load than those near the gripper.
= " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d D B b B V U l E q s u i G 5 c V 7 A O a E C a T S T t 0 8 m D m R i i h 4 M Z f c e N C E b f + h D v / x k m b h b Y e G O Z w z r 3 c e 4 + f C q 7 A s r 6 N y s r q 2 v p G d b O 2 t b 2 z u 2 f u H 3 R V k k n K O j Q R i e z 7 R D H B Y 9 Y B D o L 1 U 8 l I 5 A v W 8 8 c 3 h d 9 7 Y F L x J L 6 H S c r c i A x j H n J K Q E u e e e Q E T A D B j p + I Q E 0 i / e U O k G z q g W f W r Y Y 1 A 1 4 m d k n q q E T b M 7 + c I K F Z x G K g g i g 1 s K 0 U 3 J x I 4 F S w a c 3 J F E s J H Z M h G 2 g a k 4 g p N 5 / d M M W n W g l w m E j 9 Y s A z 9 X d H T i J V 7 K c r I w I j t e g V 4 n / e I I P w y s 1 5 n G b A Y j o f F G Y C Q 4 K L Q H D A J a M g J p o Q K r n e F d M R k Y S C j q 2 m Q 7 A X T 1 4 m 3 f O G 3 W w 0 7 y 7 q r e s y j i o 6 R i f o D N n o E r X Q L W q j D q L o E T 2 j V / R m P B k v x r v x M S + t G G X P I f o D 4 / M H w x y Y R A = = &lt; / l a t e x i t &gt; ωω t Whole- Simulator Reinforcement Learning &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a a 4 Z S t W 7 b a Z o B F L 5 g U c f y Y R U x x o = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 1 W X R j c s K 9 g F N K Z P p p B 0 6 m Y S Z G 6 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 e m D g c M 6 9 3 D M n S K Q w 6 L p f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 T J x q x t s s l r H u B d R w K R R v o 0 D J e 4 n m N A o k 7 w b T 2 9 z v P n J t R K w e c J b w Q U T H S o S C U b S S 7 0 c U J 0 G Y 0 f k Q h 9 W a W 3 c X I H + J V 5 A a F G g N q 5 / + K G Z p x B U
y S Y 3 p e 2 6 C g 4 x q F E z y e c V P D U 8 o m 9 I x 7 1 u q a M T N I F t k n p M z q 4 x I G G v 7 F J K F + n M j o 5 E x s y i w k 3 l G s + r l 4 n 9 e P 8 X w e p A J l a T I F
V s e C l N J M C Z 5 A W Q k N G c o Z 5 Z Q p o X N S t i E a s r Q 1 l S x J X i r X / 5 L O h d 1 r 1 F v 3 F / W m j d F H W U 4 g V M 4 B w + u o A l 3 0 I I 2 M E j g C V 7 g 1 U m d Z + f N e V + O l p x i 5 x h + w f n 4 B n B d k f c = &lt; / l a t e x i t &gt; a t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D m / f y 6 M + T f q s S M W w K 6 L I S 0 E / t f s = " &gt; A A A B 8 3 i c b V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w V R K R 6 r L o x m U F + 4 C m l M n 0 p h 0 6 m Y S Z G 6 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 e m D g c M 6 9 3 D M n S K Q w 6 L p f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 T J x q D m 0 e y 1 j 3 A m Z A C g V t F C i h l 2 h g U S C h G 0 x v c 7 / 7 C N q I W D 3 g L I F B x M Z K h I I z t J L v R w w n Q Z j B f I j D a s 2 t u w v Q v 8 Q r S I 0 U a A 2 r n / 4 o 5 m k E C r l k x v Q 9 N 8 F B x j Q K L m F e 8 V M D C e N T N o a + p Y p F Y A b Z I v O c n l l l R M N Y 2 6 e Q L t S f G x m L j J l F g Z 3 M M 5 p V L x f / 8 / o p h t e D T K g k R V B 8 e S h M J c W Y 5 g X Q k d D A U c 4 s Y V w L m 5 X y C d O M o 6 2 p Y k v w V
r / 8 l 3 Q u 6 l 6 j 3 r i / r D V v i j r K 5 I S c k n P i k S v S J H e k R d q E k 4 Q 8 k R f y 6 q T O s / P m v C 9 H S 0 6 x c 0 x + w f n 4 B n Z 5 k f s = &lt; / l a t e x i t &gt; e t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 C D z + h F i i / h n z m / S P c G 6 J V j 1
J j A = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o M g C G F X J H o M e v G Y g H l A s o T Z S W 8 y Z n Z 2 m Z k V Q s g X e P G g i F c / y Z t / 4 y T Z g y Y W N B R V 3 X R 3 B Y n g 2 r j u t 5 N b W 9 / Y 3 M p v F 3 Z 2 9 / Y P i o d H T R 2 n i m G D x S J W 7 Y B q F F x i w 3 A j s J 0 o p F E g s B W M 7 m Z + 6 w m V 5 r F 8 M O M E / Y g O J A 8 5 o 8 Z K 9 Y t e s e S W 3 T n I K v E y U o I M t V 7 x q 9 u P W R q h N E x Q r T u e m x h / Q p X h T O C 0 0 E 0 1 J p S N 6 A A 7 l k o a o f Y n 8 0 O n 5 M w q f R L G y p Y 0 Z K 7 + n p j Q S O t x F N j O i J q h X v Z m 4 n 9 e J z X h j T / h M k k N S r Z Y F K a C m J j M v i Z 9 r p A Z M b a E M s X t r Y Q N q a L M 2 G w K N g R v + e V V 0A Q 3 6 H l f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 j E o 1 Z W 2 q h N K 9 k B g m u G R t 5 C h Y L 9 G M x K F g 3 X B 6 m / v d R 6 Y N V / I B Z w k b x G Q s e c Q p Q S s F Q U x w E k a Z m g 9 x W K 1 5 d W 8 B 9 y / x C 1 K D A q 1 h 9 T M Y K Z r G T C I V x J i + 7 y U 4 y I h G T g W b V 4 L U s I T Q K R m z v q W S x M w M s k X m u X t m l Z E b K W 2 f
R H e h / t z I S G z M L A 7 t Z J 7 R r H q 5 + J / X T z G 6 H m R c J i k y S Z e H o l S 4 q N y 8 A H f E N a M o Z p Y Q q r n N 6 t I J 0 Y S i r a l i S / B X v / y X d C 7 q f q P e u L + s N W + K O s p w A q d w D j 5 c Q R P u o A V t o J D A E 7 z A q 5 M 6 z 8 6 b 8 7 4 c L T n F z j H 8 g v P x D Y W / k g U = &lt; / l a t e x i t &gt;
A m Z A C g V t F C i h l 2 h g U S C h G 0 x v c 7 / 7 C N q I W D 3 g L I F B x M Z K h I I z t J L v R w w n Q Z j B f I j D a s 2 t u w v Q v 8 Q r S I 0 U a A 2 r n / 4 o 5 m k E C r l k x v Q 9 N 8 F B x j Q K L m F e 8 V M D C e N T N o a + p Y p F Y A b Z I v O c n l l l R M N Y 2 6 e Q L t S f G x m L j J l F g Z 3 M M 5 p V L x f / 8 / o p h t e D T K g k R V B 8 e S h M J c W Y 5 g X Q k d D A U c 4 s Y V w L m 5 X y C d O M o 6 2 p Y k v w V r / 8 l 3 Q u 6 l 6 j 3 r i / r D V v i j r K 5 I S c k n P i k S v S J H e k R d q E k 4 Q 8 k R f y 6 q T O s / P m v C 9 H S 0 6 x c 0 x + w f n 4 B n Z 5 k f s = &lt; / l a t e x i t &gt; e t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 C D z + h F i i / h n z m / S P c G 6 J V j 1 J j A = " &gt; A A A B 6 H i c b V D L S g N B E O y N r x h f U Y 9 e B o M g C G F X J H o M e v G Y g H l A s o T Z S W 8 y Z n Z 2 m Z k V Q s g X e P G g i F c / y Z t / 4 y T Z g y Y W N B R V 3 X R 3 B Y n g 2 r j u t 5 N b W 9 / Y 3 M p v F 3 Z 2 9 / Y P i o d H T R 2 n i m G D x S J W 7 Y B q F F x i w 3 A j s J 0 o p F E g s B W M 7 m Z + 6 w m V 5 r F 8 M O M E / Y g O J A 8 5 o 8 Z K 9 Y t e s e S W 3 T n I K v E y U o I M t V 7 x q 9 u P W R q h N E x Q r T u e m x h / Q p X h T O C 0 0 E 0 1 J p S N 6 A A 7 l k o a o f Y n 8 0 O n 5 M w q f R L G y p Y 0 Z K 7 + n p j Q S O t x F N j O i J q h X v Z m 4 n 9 e J z X h j T / h M k k N S r Z Y F K a C m J j M v i Z 9 r p A Z M b a E M s X t r Y Q N q a L M 2 G w K N g R v + e V V 0 r w s e 5 V y p X 5 V q t 5 m c e T h B E 7 h H D y 4 h i r c Q w 0 a w A D h G V 7 h z X l 0 X p x 3 5 2 P R m n O y m W P 4 A + f z B 3 U X j L o = &lt; / l a t e x i t &gt; + Reinforcement Learning
&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 w w V I c t q 4 / P z w M
K 2 b d 1 T R X c L 7 7 o = " &gt; A A A C D X i c b Z D L S g M x F I Y z X m u 9 j b p 0 E 6 y C I J Q Z k e p G K L p x W c F e o B 2 H T J p p Q z O Z I T k j l q E v 4 M Z X c e N C E b f u 3 f k 2 p h d B W 3 8 I / P n O O S T n D x L B N T j O l z U 3 v 7 C 4 t J x b y a + u r W 9 s 2 l v b N R 2 n i r I q j U W s G g H R T H D J q s B B s E a i G I k C w e p B 7 3 J Y r 9 8 x p X k s b 6 C f M C 8 i H c l D T g k Y 5 N v 7 y g d 8 j t V t 1 l I R B q J 7 A w O O f g B J 7 8 3 d t w t O 0 R k J z x p 3 Y g p o o o p v f 7 b a M U 0 j J o E K o n X T d R L w M q K A U 8 E G + V a q W U J o j 3 R Y 0 1 h J I q a 9 b L T N A B 8 Y 0 s Z h r M y R g E f 0 9 0 R G I q 3 7 U W A 6 I w J d P V 0 b w v 9 q z R T C M y / j M k m B S T p + K E w F h h g P o 8 F t r h g F 0 T e G U M X N X z H t E k U o m A D z J g R 3 e u V Z U z s u u q V i 6 f q k U L 6 Y x J F D u 2 g P H S I X n a I y u k I V V E U U P a A n 9 I J e r U f r 2 X q z 3 s e t c 9 Z k Z g f 9 k f X x D X k x m z g = &lt; / l a t e x i t &gt; r t = r task t + r aux t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c K M v n X w / h u W E k P j F n j R Z / S v K 2 S 8 = " &gt; A A A C G X i c b Z D L S g M x F I Y z 9 V b r b d S l m 2 A R X E i Z E a k u i 2 5 c V r A X a M c h k 2 b a 0 C Q z J J l C G f o a b n w V N y 4 U c a k r 3 8 Z M O 4 J t / S H w 8 5 1 z y D l / E D O q t O N 8 W 4 W V 1 b X 1 j e J m a W t 7 Z 3 f P 3 j 9 o q i i R m D R w x C L Z D p A i j A r S 0 F Q z 0 o 4 l Q T x g p B U M b 7 J 6 a 0 S k o p G 4 1 + O Y e B z 1 B Q 0 p R t o g 3 3 Y 6 X Y 7 0 I A j T 0 e Q h 7 U o O M e 9 N f H 0 G f 3 k 8 x z 3 f L j s V Z y q 4 b N z c l E G u u m 9 / d n s R T j g R G j O k V M d 1 Y u 2 l S G q K G Z m U u o k i M c J D 1 C c d Y w X i R H n p 9 L I J P D G k B 8 N I m i c 0 n N K / E y n i S o 1 5 Y D q z d d V i L Y P / 1 T q J D q + 8 l I o 4 0 U T g 2 U d h w q C O Y B Y T 7 F F J s G Z j Y x C W 1 O w K 8 Q B J h L U J s 2 R C c B d P X j b N 8 4 p b r V T v L s q 1 6 z y O I j g C x + A U u O A S 1 M A t q I M G w O A R P I N X 8 G Y 9 W S / W u / U x a y 1 Y + c w h m J P 1 9 Q P h 6 a F 8 &lt; / l a t e x i t &gt; [v cmd t , p cmd t ] Reference Real &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L X u K K i s X G 0 S 2 t 2 6 S + t u O D c 4 f c 5 4 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R K R 6 r L o x m U F + 4 C m l M l 0 0 g 6 d z I S Z G 6 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 e m D g c M 6 9 3 D M n T A Q 3 6 H l f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 j E o 1 Z W 2 q h N K 9 k B g m u G R t 5 C h Y L 9 G M x K F g 3 X B 6 m / v d R 6 Y N V / I B Z w k b x G Q s e c Q p Q S s F Q U x w E k a Z m g 9 x W K 1 5 d W 8 B 9 y / x C 1 K D A q 1 h 9 T M Y K Z r G T C I V x J i + 7 y U 4 y I h G T g W b V 4 L U s I T Q K R m z v q W S x M w M s k X m u X t m l Z E b K W 2 f R H e h / t z I S G z M L A 7 t Z J 7 R r H q 5 + J / X T z G 6 H m R c J i k y S Z e H o l S 4 q N y 8 A H f E N a M o Z p Y Q q r n N 6 t I J 0 Y S i r a l i S / B X v / y X d C 7 q f q P e u L + s N W + K O s p w A q d w D j 5 c Q R P u o A V t o J D A E 7 z A q 5 M 6 z 8 6 b 8 7 4 c L T n F z j H 8 g v P x D Y W / k g U = &lt; / l a t e x i t &gt; o t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c K M v n X w / h u W E k P j F n j R Z / S v K 2 S 8 = " &gt; A A A C G X i c b Z D L S g M x F I Y z 9 V b r b d S l m 2 A R X E i Z E a k u i 2 5 c V r A X a M c h k 2 b a 0 C Q z J J l C G f o a b n w V N y 4 U c a k r 3 8 Z M O 4 J t / S H w 8 5 1 z y D l / E D O q t O N 8 W 4 W V 1 b X 1 j e J m a W t 7 Z 3 f P 3 j 9 o q i i R m D R w x C L Z D p A i j A r S 0 F Q z 0 o 4 l Q T x g p B U M b 7 J 6 a 0 S k o p G 4 1 + O Y e B z 1 B Q 0 p R t o g 3 3 Y 6 X Y 7 0 I A j T 0 e Q h 7 U o O M e 9 N f H 0 G f 3 k 8 x z 3 f L j s V Z y q 4 b N z c l E G u u m 9 / d n s R T j g R G j O k V M d 1 Y u 2 l S G q K G Z m U u o k i M c J D 1 C c d Y w X i R H n p 9 L I J P D G k B 8 N I m i c 0 n N K / E y n i S o 1 5 Y D q z d d V i L Y P / 1 T q J D q + 8 l I o 4 0 U T g 2 U d h w q C O Y B Y T 7 F F J s G Z j Y x C W 1 O w K 8 Q B J h L U J s 2 R C c B d P X j b N 8 4 p b r V T v L s q 1 6 z y O I j g C x + A U u O A S 1 M A t q I M G w O A R P I N X 8 G Y 9 W S / W u / UO V i i X i H v K U e j E e C x Y x g k F L v t l 0 g 4 S H K o / 1 V b i A s 5 k P v t m y 2 / Y c 1 i p x K t J C F X q + + e W G C c l i K o B w r N T I s V P w C i y B E U 5 n D T d T N M V k i s d 0 p K n A M V V e M Q 8 / s 0 6 1 E l p R I v U R Y M 3 V 3 x s F j l W Z T 0 / G G C Z q 2 S v F / 7 x R B t G V V z C R Z k A F W T w U Z d y C x C q b s E I m K Q G e a 4 K J Z D q r R S Z Y Y g K 6 r 4 Y u w V n + 8 i o Z n L e d T r t z d 9 H q X l d 1 1 N E x O k F n y E G X q I t u U Q / 1 E U E 5 e k a vO V i i X i H v K U e j E e C x Y x g k F L v t l 0 g 4 S H K o / 1 V b i A s 5 k P v t m y 2 / Y c 1 i p x K t J C F X q + + e W G C c l i K o B w r N T I s V P w C i y B E U 5 n D T d T N M V k i s d 0 p K n A M V V e M Q 8 / s 0 6 1 E l p R I v U R Y M 3 V 3 x s F j l W Z T 0 / G G C Z q 2 S v F / 7 x R B t G V V z C R Z k A F W T w U Z d y C x C q b s E I m K Q G e a 4 K J Z D q r R S Z Y Y g K 6 r 4 Y u w V n + 8 i o Z n L e d T r t z d 9 H q X l d 1 1 N E x O k F n y E G X q I t u U Q / 1 E U E 5 e k a v= " &gt; A A A C A 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d D B b B V U l E q s u i G 5 c V 7 A O a E C a T S T t 0 8 m D m R i i h 4 M Z f c e N C E b f + h D v / x k m b h b Y e G O Z w z r 3 c e 4 + f C q 7 A s r 6 N y s r q 2 v p G d b O 2 t b 2 z u 2 f u H 3 R V k k n K O j Q R i e z 7 R D H B Y 9 Y B D o L 1 U 8 l I 5 A v W 8 8 c 3 h d 9 7 Y F L x J L 6 H S c r c i A x j H n J K Q E u e e e Q E T A D B j p + I Q E 0 i / e U O k G z q g W f W r Y Y 1 A 1 4 m d k n q q E T b M 7 + c I K F Z x G K g g i g 1 s K 0 U 3 J x I 4 F S w a c 3 J F E s J H Z M h G 2 g a k 4 g p N 5 / d M M W n W g l w m E j 9 Y s A z 9 X d H T i J V 7 K c r I w I j t e g V 4 n / e I I P w y s 1 5 n G b A Y j o f F G Y C Q 4 K L Q H D A J a M g J p o Q K r n e F d M R k Y S C j q 2 m Q 7 A X T 1 4 m 3 f O G 3 W w 0 7 y 7 q r e s y j i o 6 R i f o D N n o E r X Q L W q j D q L o E T 2 j V / R m P B k v x r v x M S + t G G X P I f o D 4 / M H w x y Y R A = = &lt; / l a t e x i t &gt; ωω t PD &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F X M B i n e 0 4 o G Q d / y V J J 2 6 y G n 3 P 2 w = " &gt; A A A B / H i c b V D N S s N A G N z U v 1 r / o j 1 6 C R b B U 0 l E q s e i F 4 8 V b C 0 0 I W w 2 m 3 b p Z h N 2 v w g h 1 F f x 4 k E R r z 6 I N 9 / G T Z u D t g 4 s O 8 x 8 H z s 7 Q c q Z A t v + N m p r 6 x u b W / X t x s 7 u 3 v 6 B e X g 0 U E k m C e 2 T h C d y G G B F O R O 0 D w w 4 H a a S 4 j j g 9 C G Y 3 p T + w y O V i i X i H v K U e j E e C x Y x g k F L v t l 0 g 4 S H K o / 1 V b i A s 5 k P v t m y 2 / Y c 1 i p x K t J C F X q + + e W G C c l i K o B w r N T I s V P w C i y B E U 5 n D T d T N M V k i s d 0 p K n A M V V e M Q 8 / s 0 6 1 E l p R I v U R Y M 3 V 3 x s F j l W Z T 0 / G G C Z q 2 S v F / 7 x R B t G V V z C R Z k A F W T w U Z d y C x C q b s E I m K Q G e a 4 K J Z D q r R S Z Y Y g K 6 r 4 Y u w V n + 8 i o Z n L e d T r t z d 9 H q X l d 1 1 N E x O k F n y E G X q I t u U Q / 1 E U E 5 e k a v 6 M 1 4 M l 6 M d + N j M V o
z q p 0 m + g P j 8 w e z T 5 V 4 &lt; / l a t e x i t &gt; ω t Fig. 2: Unsupervised Actuator Network (UAN) approach for real-to-sim-to-real.Our training pipeline involves three steps: 1) Train a UAN to close the sim-to-real gap for actuators with complex transmission mechanisms by mapping a history of joint position and velocity errors, e t , to corrective torques, δτ t , 2) Pre-train a WBC using random motion references (base velocity and EE pose), then and fine-tune it on an athletic task reward with the UAN in loop, and 3) Deploy.During the fine-tuning phase, the WBC initially tracks the task-specific reference, and then gradually learns to depart from the reference to maximize task performance.</p>
<p>2) Data collection: We collect data on the real hardware to construct a dataset of transitions {(s t , τ t , s t+1 ) i } N i=0 from each actuator.Our intention during data collection was to sufficiently cover the state space to avoid overfitting.Thus, we opted not to use policy data, and instead, collected data with three types of action sequences: 1) square waves, 2) sine waves, and 3) gaussian noise.For the square and sine wave data, we passed torque commands to one actuator at a time, while keeping the rest of the actuators at a fixed position target.We swept 12 different combinations of amplitude and frequency for each wave, resulting in about 50 seconds of data for each actuator.For the gaussian noise data, we passed torque commands to all the robot's joints simultaneously.We sampled a new action from a gaussian distribution every 5 to 400 ms for about 5 minutes.</p>
<p>3) Training Environment: We designed the training environment in Isaac Sim [30] with 4096 parallel environments.We train policies with the RSL-RL implementation [36] of PPO [37] with default hyperparameters, minus a few modifications (see Appendix A for the full list of learning algorithm hyperparameters).Following Radosavovic et al. [34], we apply a separate, fixed learning rate to the critic while using an adaptive learning rate for the actor.Additionally, we divide the data of each epoch into four mini-batches for the actor while using the entire batch for the critic, as we found that larger batch sizes produce more stable gradients and result in lower value function loss.</p>
<p>4) Task Design: For each environment at each timestep, we uniformly sample a real-world transition, (s t , τ t , s t+1 ) k , and set the state of the simulator to match s t and the initial torque to τ t .After policy inference, we modify the torque by adding the correction, δτ t , and then step the simulator.We then compute the reward as
r UAN t = r sim−to−real t + r smoothness t (3)
where r sim−to−real t aims to minimize the difference between the real joint position and the simulated joint position, and r smoothness t biases exploration to gradual deviations.For a complete list of reward terms, please refer to Appendix A.</p>
<p>Each training episode consists of a 20 s rollout executing the torque sequence from the hardware data from δτ t to δτ t+20s .Through training on rollouts, the actuator net learns to remain stable across many simulation time steps.</p>
<p>B. Whole-body Controller Pre-training</p>
<p>Before training on task-specific behaviors, we pre-train the WBC to learn foundational trajectory-tracking skills.Our training scheme builds upon the method proposed in [6] by incorporating a strategy for learning to track an EE orientation command.As in Section II-A3, we designed the training environment in Isaac Sim with 4096 parallel environments and trained the policies with PPO [36,37] (using separate learning rates and batch sizes for the actor and critic).</p>
<p>1) Policy Architecture: The WBC is a control policy, a t = π θ (o t−H:t ), where the action at time t, a t , is a vector of position targets for each of the robot's joints and o t−H:t is an observation history of length H = 10 timesteps (200 ms).We parameterize π θ as a 3-layer multi-layer perceptron (MLP) with layer sizes [512, 512, 512] and ELU activations.The value function approximator network has the same architecture but does not share weights with the policy.</p>
<p>2) Observation Space: The policy's observation space consists of proprioceptive readings from the robot's onboard sensors, including the gravity vector projected in the robot's body frame g, a base velocity command v cmd t , an end effector pose command p cmd t , the joint positions q, the joint velocities q, the previous actions a t−1 , and a timing variable ω t = sin(2πf t) with f = 2.2 Hz corresponding to the gait cycle frequency.Additionally, the observation includes a d-dimensional task embedding vector z t (set to zero during pre-training).</p>
<p>3) Sim-to-Real Considerations: Our approach for bridging the sim-to-real gap uses a combination of domain randomization (DR) and real-to-sim calibration.To learn locomotion behaviors robust to terrain variations, we randomize terrain roughness, friction, and restitution.To account for inaccuracies in the robot's URDF, we randomize the mass and center of mass position of each of the robot's links.We also randomize the PD gains and stall torques for each actuator in the robot's legs, and the policy lag length to learn robustness to latencies observed on hardware.To encourage learning recovery behaviors, we randomize the initial joint and body states of the robot and periodically perturb it with external forces and torques at the base, hips, feet, and end-effector, following the approach proposed in [8].The DR ranges used for both pre-training and fine-tuning are provided in Appendix A.</p>
<p>Inspired by [43], we clip the commanded motor torques τ such that
τ ≥ −τ max 1 + max min q qmax , 0 , −1 , (4) τ ≤ τ max 1 − max min q qmax , 1 , 0 . (5)
where τ max and qmax are the maximum torques and velocities of the actuators, respectively.This clipping strategy enforces a physical motor constraint by ensuring that torque commands do not demand power beyond the motor's maximum output capacity.Furthermore, we clip the arm torques a second time to satisfy the constraint
|τ | ⊤ | q| ≤ P max ,(6)
where P max is the maximum total power of the arm joints, because we found experimentally this helps prevent the arm from entering a power protect state enforced by the robot's manufacturer.</p>
<p>Since typical DR strategies were insufficient for athletic behaviors in the arm (which uses harmonic drives), we incorporate the UAN (Section II-A) for the arm actuators.Therefore, no DR is applied to arm joint properties.</p>
<p>4) Task Specification: The pre-training task for the WBC is to track a desired base velocity and EE pose.The velocity command, v cmd t = v cmd x,t , v cmd y,t , ω cmd z,t , consists of a desired forward velocity v cmd x,t , a desired lateral velocity v cmd y,t , and a desired yaw-rate ω cmd z,t .We command the EE pose in a yawrotated frame aligned with the robot's center of mass at a fixed height above the terrain.The choice of frame encourages the robot to coordinate with its legs to expand its workspace.include tracking terms (EE pose, base velocity) and gait terms, while r aux t includes regularization and smoothing terms.The EE tracking term rewards minimizing the distance between four key points, where one key point is positioned at the frame's origin, and the others are positioned along each axis of the frame.Full details are provided in Appendix A.</p>
<p>6) Command Sampling Scheme: We adopt the approach first proposed in [6] to sample commands during training.We sample a new base velocity command and a new goal Fig. 3: Unitree Z1 Pro arm.This arm's harmonic actuators behave substantially differently from the quasi-direct-drive motors common in small legged robots.This image also shows the reinforcements we designed to ensure that the limit on athleticism comes from actuation rather than the linkage structural integrity.end effector pose every 7 seconds of simulation time.Upon sampling, the command is linearly interpolated (over 2 to 5 seconds) from the previous command.While this sampling scheme suffices for foundational loco-manipulation skills, it may be too smooth for highly agile motions -this motivates our task-specific fine-tuning (Section II-C).</p>
<p>C. Task-Specific Finetuning</p>
<p>After pre-training, the policy can track reference trajectories, but struggles on high-acceleration tasks.To address this, we fine-tune the policy directly with task rewards.The same WBC base policy weights can be reused for multiple task policies, thus avoiding repeated pre-training.</p>
<p>1) Initialization: The policy weights are initialized to those learned during pre-training.To avoid policy collapse, we set a low initial learning rate (1 × 10 −5 ) for the actor and retain the standard deviation from pre-training.Additionally, we set the entropy coefficient in PPO to zero during fine-tuning to improve training stability.</p>
<p>2) Reference trajectory and task embedding: During finetuning, the policy receives a task-specific reference trajectory and a one-hot task embedding to inform which phase of the task (e.g., set-up, execute, settle) is active.We handdesigned the reference trajectories through joint interpolation and forward kinematics, but they could also come from an expert policy or human demonstration.</p>
<p>3) Fine-tuning with task reward: The environment for finetuning phase extends that of pre-training (same DR ranges, external pushes, etc.).The reward becomes r i t + r aux t , where r i t is task-specific.Initially, the policy tracks the reference, aiding exploration; later, it learns to deviate to maximize task performance.</p>
<p>III. EXPERIMENTAL SETUP</p>
<p>We chose the Unitree B2 with Unitree Z1 Pro arm as our hardware platform, and we consider three athletic tasks: throwing, weight lifting, and sled pulling (see Section IV-C).Fig. 4: UAN improves simulator accuracy and real throwing performance.UAN (Ours) achieves lower sim-to-real difference in throw distance as compared to standard baselines, resulting in a better real throw distance.For this comparison, we train and test policies with a fixed-base arm, to avoid the risk of the legged base falling during performance-critical ablations.</p>
<p>Structural upgrades to the arm were custom designed and fabricated to withstand the high loads during athletic behaviors (see Section III-A).Following our UAN training (Section II-A), we pre-trained a WBC (Section II-B) and then fine-tuned policies for each task (Section II-C).Ablations comparing our method with alternatives are described in Section IV-A and Section IV-B.</p>
<p>A. Arm Modifications</p>
<p>During development, the Unitree Z1 Pro arm experienced structural failures at links 2 and 4, with minor deformations at link 5.The damage resulted from the highly dynamic movements in the athletic experiments, which applied loads to the links that exerted excessive stress and strain on the links exceeding the material's yield strength.Modifications were made to reinforce links 2, 3, 4, and 5 by adding supports at the joints.This prevents loads from being transferred solely through the motors which are cantilevered.A mass-efficient aluminum square tube was used for link 2, which experiences the highest stress of all the links.Idler bearings are used to apply support at the motor outputs without restricting their movement.In the URDF, link masses, centers of mass, and inertias were updated based on CAD calculations and the parallel axis theorem.Figure 3 shows the reinforced arm.</p>
<p>IV. EXPERIMENTAL RESULTS</p>
<p>In this section, we report ablations that identify the contribution of key system components and present results for the athletic tasks.Supplemental videos are provided on the project website: https://uan.csail.mit.edu/.</p>
<p>Our experiments address the following questions: 1) Does our unsupervised actuator net reduce the sim-toreal gap and improve transfer?2) What are the benefits of our two-stage pre-training and fine-tuning pipeline relative to alternatives?</p>
<p>3) Does our approach enable sim-to-real transfer of athletic whole-body control tasks?</p>
<p>A. Comparing System Identification Approaches</p>
<p>We compare several methods for modeling the actuator dynamics of the Unitree Z1 Pro arm in Isaac Sim.In particular, we consider: 1) Default: The baseline simulator with no additional modifications.2) DR: The simulator augmented with domain randomization (randomizing PD gains, friction, and armature parameters).3) ROA: A domain randomization baseline enhanced with an online system identification module via Regularized Online Adaptation [6].4) Actuator Net: A supervised actuator network following Hwangbo et al. [13] where torque labels are estimated from the motor current.(Note that these labels do not capture the nonlinear effects introduced by the harmonic reducers.)5) CEM: A method in which friction, frictional damping, and armature parameters are optimized using the crossentropy method to minimize the mean-square joint position error between simulation and hardware.6) UAN: Our proposed unsupervised actuator network that learns corrective torques without requiring torque supervision, thereby capturing both lag and nonlinearities from harmonic reduction.</p>
<p>We first evaluate the modeling accuracy of these approaches by reporting the mean-square joint position error on both the training data and on an unseen test trajectory (see Figure 7).Our results show that the UAN method achieves the best fit, suggesting excellent generalization.For example, detailed windows of simulator rollouts for a single arm joint are provided in Figures 7b (training data) and 7c (test data); additional results for the other arm joints are included in the appendix (Figures 8 and 9).In our observations, the CEM method helps prevent overshoot (by effectively slowing the arm to match the lower joint velocities seen on hardware).</p>
<p>Actuator Net can improve over the baseline by capturing lag effects, but it diverged on the 5 min rollouts on the training data.However, only UAN achieves a tight fit to the training data, thanks to its capacity to model the nonlinear effects introduced by the harmonic reducers.A shown by Figure 7, UAN can even accurately capture the arm's response to Gaussian noise control input, which is commonly used for exploration in reinforcement learning but represents a challenging regime for accurate simulation where the baseline methods degrade substantially.</p>
<p>To further assess these system identification methods in a task context, we trained arm-only throwing policies in simulation augmented with each approach and deployed them on hardware.The average throwing performance in simulation and reality is presented in Figure 4.In simulation, although the Actuator Net and CEM produced a promising throw, its behavior did not transfer as well to hardware.In contrast, the UAN policy achieved the farthest throws on hardware with the smallest sim-to-real gap.Meanwhile, the Default, DR, and ROA policies produced unstable behaviors-the Default policy, for instance, strayed excessively and failed to throw the ball at all.</p>
<p>B. Finetuning Foundational WBC</p>
<p>We compare four throwing policies to assess the impact of our pre-training and fine-tuning approaches:</p>
<p>1) No-Fine-Tuning: a pre-trained WBC that tracks a throwing reference trajectory.</p>
<p>2) No-Pre-Training: a throwing policy trained from scratch.3) No-E2E: a high-level policy that outputs commands for a frozen pre-trained WBC. 4) Ours: our method that initializes with the pre-trained WBC and fine-tunes with RL.All methods observe a hand-designed throwing reference trajectory.</p>
<p>Figure 1 presents the performance of each throwing policy across 100 simulated throws.No-Fine-Tuning successfully throws the ball by tracking the reference, but its performance is sub-optimal.However, the strong performance of No-E2E shows that the WBC's performance can be improved by providing a better reference trajectory.Still, the No-E2E policy does not perform to the maximum capability of the hardware.Through RL finetuning with the task reward, Ours can learn to throw farther while using a reduced peak power output in its leg motors.While No-Pre-Training could theoretically push the capabilities of the hardware, in practice, it struggles to do so due to exploration challenges.We found that No-Pre-Training achieved similar throwing performance to No-E2E, despite hitting a larger peak power output in its legs.</p>
<p>C. Hardware Results</p>
<p>1) Ball throwing: The task objective is to throw a 100 g ball as far as possible.Because grasping and releasing a ball directly is challenging for our gripper, a small bucket is attached to the robot's EE.The policy leans back prior to throwing, then pushes with its hind legs while swinging its arm forward to launch the ball.Figure 6 shows side-byside snapshots from simulation and hardware.On hardware, the ball was thrown approximately 20 m, with the real robot throwing slightly further than in simulation -possibly due to inaccuracies in the ball-bucket contact modeling.</p>
<p>2) Dumbbell snatch: The goal is to lift a dumbbell with the EE and hold it stably.The dumbbell is simulated by modifying the gripper's mass.The robot first lowers its EE to the ground, at which point the mass is added to its gripper.Then, its commanded to lift the weight in the air.When lifting, the robot is rewarded for maximizing the z position of its EE.</p>
<p>When training the lifting policy, we randomized the mass of the robot's EE from 0 to 10 kg.At convergence, the policy could consistently lift weights up to 8kg, but struggled to stabilize heavier weights above its body.Since the robot's arm is much weaker than the legs, the policy learns to pitch its base backwards to swing the weight upwards into the air.Figure 6 includes snapshots of the learned lifting behavior in simulation and reality.During hardware experiments, we secured the dumbbells inside the robot's gripper with a belt to prevent it from slipping out of the robot's grasp, We found the Z1 arm could not lift even a 5 lb.dumbbell to an upright position through simple joint interpolation.We first verified the whole-body policy could lift a 5 lb.dumbbell and then progressed to a 10 lb.dumbbell.In both experiments, the robot lifted the weight above its base and maintained it there stably for over 5 s.</p>
<p>3) Sled pull: In this task, the robot pulls a heavy sled attached by a rope to its EE.The sled is modeled as avirtual, 3-dimensional mass-spring-damper system.The robot is rewarded for tracking a backward base velocity while minimizing lateral drift.The policy learns to adopt a low stance to maintain balance and extend its arm to avoid applying unnecessary torques to the arm's actuators.In simulation, policies successfully pulled weights up to 150 kg.On hardware, the robot pulled a cart resisting a friction force of 113 N over 10 meters; a heavier cart (requiring 230 N) was only pulled about 0.5 meters.</p>
<p>V. LIMITATIONS</p>
<p>Our fine-tuning approach requires a task reference trajectory, which may not be available for all robot morphologies or tasks.It also necessitates per-task engineering of the training environment (reward functions, object simulation, etc.).Future work might employ generative models to automatically synthesize task references.Additionally, our unsupervised actuator net focuses on the arm actuators.Extending real-to-sim calibration to other robot subsystems and modeling structural integrity are promising future directions.</p>
<p>VI. RELATED WORK</p>
<p>A. Whole-Body Control</p>
<p>Walking robots with arms present a formidable challenge for control due to their many degrees of freedom and complex dynamics.A typical paradigm is to implement a WBC that optimizes actuation to achieve control objectives considering a model of the robot's kinematics and dynamics [41].WBC approaches based on offline trajectory optimization or online   We rolled out three real-world joint trajectories: square &amp; sine waves at each joint, Gaussian noise across all the joints, and a throw.Square waves, sine waves, and gaussian noise were seen during training, while the throw was not.We found that Actuator-Net error remains bounded on the 5 s throw trajectory but diverges when rolling out the 5 min training trajectories, while the UAN learned to remain accurate across long rollouts through RL training.</p>
<p>optimization with reduced-order models have achieved considerable success in dynamic walking and manipulation [1,3,28,44].Recently, reinforcement learning in simulation has enabled whole-body control that can naturally handle model uncertainty, e.g.uncertain terrain and robot properties [6].In the case of reinforcement learning-based whole-body control, the controller is a neural network that is commanded with an input reference position [4,6], force [32], or whole-body pose [5,12,23,24] and outputs joint-space actions.</p>
<p>It is common to teleoperate legged-armed robots by parsing a reference trajectory from a human's movements in real-time and tracking it with a WBC; such an approach can accomplish expressive [4], forceful [32], or dexterous [7] tasks.One may also train a high-level policy to select reference trajectories or a latent representation autonomously in place of the teleoperator, using either learning from demonstration [7,10] or reinforcement learning [21,24].However, some tasks may not be achievable by any choice of reference trajectory if they require a motion outside the training distribution of the WBC.It is challenging to formulate a generic pre-training scheme for whole-body control that anticipates all kinds of tasks one might want to perform for humanoids, motion capture datasets can provide diverse feasible reference commands [23], but for quadruped manipulators, pre-training commonly defaults to tracking procedurally generated smooth trajectories within the workspace [6].</p>
<p>To avoid the reliance on high-quality pre-training, another possibility is to discard the explicit notion of reference trajectories altogether and directly train end-to-end policies for specific tasks such as fall recovery [25], door opening [38], or soccer [11,15,16].This enables the policy to learn highly dynamic motions to optimize the task reward, but, in practice, these motions can be hard to find due to fundamental exploration challenges in RL.We address this challenge by initializing the policy with pre-trained WBC weights and a reference trajectory.</p>
<p>B. Overcoming the sim-to-real gap</p>
<p>Prior work proposed simulated athletic tasks as a benchmark for learned whole-body control [42,24], though they left simto-real transfer as future work.In contrast, other studies have demonstrated sim-to-real transfer of athletic tasks on small robots with transparent actuators [11,15,16].Achieving simto-real transfer for athletic behaviors on large robots with nonideal actuators is especially challenging because even minor modeling discrepancies can lead to reward hacking.To address this, we introduce UAN, which leverages real-world data to bridge the sim-to-real gap.</p>
<p>DR is a common strategy to mitigate discrepancies between simulation and reality [17,18].In the field of dynamic legged robots, common parameters to randomize include the proportional and derivative gains of each joint, the stall torques, the link masses and inertias, and terrain properties [18,51].Excessive DR can reduce peak performance if the policy cannot identify key parameters of the environment necessary to optimize its reward function.To overcome this challenge, previous work employed teacher-student frameworks, where a student policy learns to imitate an expert policy that has access to privileged observations related to its environment [6,17,18].Alternatively, the policy may learn online system identification directly from an observation history.Some policy architectures (i.e., CNNs [20] and transformers [33]) have been shown to achieve in-context adaptation without relying on a teacher-student distillation.</p>
<p>Accurate system identification can reduce reliance on DR by mitigating the sim-to-real gap directly.Methods for identifying inertial properties typically rely on least-squares estimation [2], including a notable approach that leverages insights about the geometric structure of the robot's dynamics to provide robustness against local optima [19].This method was applied to identify the inertial parameters of the MIT Humanoid [39].In our work, we rely on the inertial properties provided in the manufacturer's URDF file.</p>
<p>Actuator modeling methods traditionally rely on parameterized physics models to capture effects such as static friction, dynamic friction, and reflected inertia [8], the last of which can be set through the "armature" setting in physics simulations such as Isaac Sim [26] and MuJoCo [48].This approach can be insufficient for actuators with complex transmission mechanisms.To address this, Hwangbo et al. [13] proposed learning an actuator net, which is a neural network trained to predict an actuator's output torque from a history of position and velocity errors.The actuator net was added to the simulator during policy training to reduce the sim-to-real gap in ANYmal's series elastic actuators.Their approach, however, relies on torque sensing, which is uncommon in consumer robotic hardware.Schwendeman et al. [39] avoided reliance on an output torque sensor when training an actuator net by measuring the torques from current.However, this is only accurate in low-reduction and low-torque-density actuators which are efficiently backdriveable and have minimal reflected inertia.In contrast, our approach, UAN, employs an actuator net without relying on torque data.Instead, we train the network to predict corrective torques for the simulator that minimize the discrepancy between the simulated and realworld transition dynamics.</p>
<p>When ground-truth labels are unavailable (i.e., the robot's actuators lack torque sensing), they can be discovered through interaction to better match the real-world dynamics.For example, Zeng et al. [50] learned a residual model to better predict the ballistic motions of objects, enabling a manipulator to accurately throw them.Similarly, Gruenstein et al. [9] proposed learning residual actions for a simplified dynamics model for a legged microbot so that it transits to the same future states as a more complex dynamics model.In another study, Sontakke et al. [45] proposed learning a corrective external force policy to improve simulation accuracy for a buoyancy assisted legged robot.Mentee Robotics has publicly stated that they applied RL to train a delta action model using real-world data to overcome the sim-to-real gap on their humanoid, but the technical details of their approach remain unpublished [35].While we also apply RL to correct our simulation model, we specifically target the sim-to-real gap for the robot's actuators with harmonic drives, which are notably hard to model.This focus leverages the parts of the simulator that are more accurate (i.e., rigid body mechanics) to reduce overfitting and also avoids reliance on a motion capture system.</p>
<p>VII. CONCLUSION</p>
<p>Legged manipulators promise enhanced strength and a larger workspace by coordinating arms and legs.We proposed a training pipeline that first pre-trains a whole-body controller and then fine-tunes it using task rewards, while simultaneously reducing the sim-to-real gap via our UAN.Our experimental results on ball throwing, dumbbell lifting, and sled pulling demonstrate the viability of this approach.Future work may extend the sim-to-real calibration to additional subsystems and incorporate structural integrity constraints directly in the training.Future work may extend the real-to-sim calibration to additional subsystems and incorporate structural integrity constraints directly in the training.</p>
<p>APPENDIX A TRAINING DETAILS</p>
<p>The PPO hyperparameters across all tasks are provided in Table I.The ranges for domain randomization are provided in Table II</p>
<p>APPENDIX B TASK ENVIRONMENTS</p>
<p>The auxiliary rewards (and scales) for each task match those in Table III underneath the dashed line.</p>
<p>A. Ball Throwing</p>
<p>The ball throwing task has three separate task states: throw-set-up, throw, and settle.The policy knows which state it is in by observing its task embedding, which includes a onehot vector.Each state has a separate task reward for the desired behavior, such that
r ball t = bthrow-set-upr throw-set-up t + bthrowr throw t + bsettler settle t
where bi is 1 when the environment is in state i and 0 otherwise.The task reward for each state is
r throw-set-up t = 5 − (pz − 0.6) 2 − g 2 x − g 2 y + ||qi,t − q ref i,t ||1 , r throw t = 20 max v ball x,t , 0 + 20 max v ball z,t , 0 − 0.5 v ball y,t 2 , r settle t = 5 − (pz − 0.6) 2 − g 2 x − g 2 y .
The throw-set-up reward encourages an upright posture and tracking a trajectory to bring back the arm for a throw, then the throw reward is to maximize the ball's forward and upward velocities while minimizing its lateral velocity.The settle reward encourages the robot to stand upright after throwing and avoid falling over.</p>
<p>The environment transitions from throw-set-up to throw after 2.5 s in simulation time, and it transitions from throw to settle at 3.5 s.We set the ball's state to an arbitrary position at the start of the episode, and then we place it in the robot's bucket after 1.5 s.If the robot drops the ball in the throw-set-up, the environment terminates.</p>
<p>With this set up, we found that the policy would sometimes learn to lean during the throw-set-up task and eventually fall over unless it transitioned to the throw state.Thus, we modified the environment so that the policy remains in the throw state with a probability of 0.3.Similarly, we added a probability that the robot enters the settle state on reset to help the policy learn to stay still after executing the throw.</p>
<p>We found that the policy may repeat the throwing motion multiple times during the throw state because it cannot sense whether it is holding the 100 g ball through proprioception.Thus, we include a timer in the task embedding that linearly increments from 0 to 1 over 1 second during only the throw state, else it is 0. The policy can key into this timer to infer whether it has already thrown the ball.</p>
<p>B. Dumbbell Lifting</p>
<p>We separate the dumbbell lifting task into two states: snatch-set-up and snatch.The task rewards for each state are rsnatch-set-up = 5
3 i 1 3 exp −2 p cmd i,t − pi,t − 0.005||F EE t − F EE t−1 ||, r snatch t = 4p EE z,t − 0.005||F EE t − F EE t−1 || − {4,5,6} i |q arm i | 2 .
The reward during snatch-set-up encourages tracking a reference that guides the EE near the ground.The force smoothness term penalizes the robot for suddenly slamming its EE into the ground.</p>
<p>The snatch task reward is to maximize the end-effector height.To avoid damaging the robot's arm, we added a reward term penalizes deviations from the wrist's nominal, straightened position.We used a similar strategoy to the throwing task to ensure stable transitions-the environment transitions from snatch-set-up to snatch after 2.5 s with a probability of 0.9.Without this addition, the robot may fall over during an extended snatch-set-up state.</p>
<p>C. Sled Pulling</p>
<p>We model the sled as a virtual mass-spring-damper.When an episode begins, the robot is at a distance l from the sled.The pulling force between the EE and the sled is defined as
F pull t = max 1000 |d − l| 2 2 − ḋ 2 2 , 0 ,
A lateral force is applied on the sled based on the friction and mass:
F sled = F pull x + µ m sled g,
The sled task also has two states, sled-set-up and pull, where the reward terms are where δyt and δθ yaw t are deviations from the starting state.The environment transitions from sled-set-up to pull after 2.5 s of simulation time with full probability.Every 7 seconds, forward base velocity commands are sampled uniformly from 0 to −1 ms −1 with a probability of 0.8.Otherwise, the policy is given a command of 0 ms −1 .We found that the sled policies tended to drift excessively in the base-y and -yaw directions since the policy does not observe the robot's base position and orientation.Thus, when the forward base velocity command is non-zero, we set the lateral and yaw velocity commands as v cmd y,t = −0.5δyt,
ω cmd z,t = −0.5δθ yaw t ,(7)
to help guide the policy to pull the sled straight.On hardware, we provide forward, lateral, and yaw velocity commands from a joystick.</p>
<p>APPENDIX C UAN TRAIN &amp; TEST FITS</p>
<p>Simulator rollouts of each system identification method on training data and test data are provided in Figures 8 and 9</p>
<p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E m z R 6 7 a z K w D V U P e r E x J z o p L 4 m w o</p>
<p>r w s e 5 V y p X 5 V q t 5 m c e T h B E 7 h H D y 4 h i r c Q w 0 a w A D h G V 7 h z X l 0 X p x 3 5 2 P R m n O y m W P 4 A + f z B 3 U X j L o = &lt; / l a t e x i t &gt;+Real-world Rollouts&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " V 8 z X M n O I f q 7 + s w v x 6 g q M P l 1 / c 3 4= " &gt; A A A C J X i c b Z D L S g M x F I Y z 9 V 5 v o y 7 d B I s g i G V G R F 0 o V N 2 4 k g r W C m 0 t m T S 1 o b k M y R m h D H 0 Z N 7 6 K G x c W E V z 5 K q b t I N 4 O B P 5 8 5 5 K c P 4 o F t x A E 7 1 5 u Y n J q e m Z 2 L j + / s L i 0 7 K + s X l u d G M o q V A t t b i J i m e C K V Y C D Y D e x Y U R G g l W j 7 t k w X 7 1 n x n K t r q A X s 4 Y k d 4 q 3 O S X g U N M / M r d p 3 U h c O b n o N w E f 4 + x u u d w B v e N G i S H f / u J S a + g o Z q 3 D T b 8 Q F I N R 4 L 8 i z E Q B Z V F u + o N 6 S 9 N E M g V U E G tr Y R B D I y U G O B W s n 6 8 n l s W E d s k d q z m p i G S 2 k Y 6 2 7 O N N R 1 q 4 r Y 0 7 C v C I f u 9 I i b S 2 J y N X K Q l 0 7 O / c E P 6 X q y X Q P m y k X M U J M E X H D 7 U T g U H j o W W 4 x Q 2 j I H p O E G q 4 + y u m H W I I B W d s 3 p k Q / l 7 5 r 7 j e L Y b 7 x f 3 L v U L p N L N j F q 2 j D b S F Q n S A S u g c l V E F U f S A n t A L G n i P 3 r P 3 6 r 2 N S 3 N e 1 r O G f o T 3 8 Q l T 1 a U 5 &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " L X u K K i s X G 0 S 2 t 2 6 S + t u O D c 4 f c 5 4 = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u g k V w V R K R 6 r L o x m U F + 4 C m l M l 0 0 g 6 d z I S Z G 6 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 e m D g c M 6 9 3 D M n T</p>
<p>t e x i t s h a 1 _ b a s e 6 4 = " a a 4 Z S t W 7 b a Z o B F L 5 g U c f y Y R U x x o = " &gt; A A A B 8 3 i c bV D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 1 W X R j c s K 9 g F N K Z P p p B 0 6 m Y S Z G 6 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 e m D g c M 6 9 3 D M n S K Q w 6 L p f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 T J x q x t s s l r H u B d R w K R R v o 0 D J e 4 n m N A o k 7 w b T 2 9 z v P n J t R K w e c J b w Q U T H S o S C U b S S 7 0 c U J 0 G Y 0 f k Q h 9 W a W 3 c X I H + J V 5 A a F G g N q 5 / + K G Z p x B Uy S Y 3 p e 2 6 C g 4 x q F E z y e c V P D U 8 o m 9 I x 7 1 u q a M T N I F t k n p M z q 4 x I G G v 7 F J K F + n M j o 5 E x s y i w k 3 l G s + r l 4 n 9 e P 8 X w e p A J l a T I F V s e C l N J M C Z 5 A W Q k N G c o Z 5 Z Q p o X N S t i E a s r Q 1 l S x J X i r X / 5 L O h d 1 r 1 F v 3 F / W m j d F H W U 4 g V M 4 B w + u o A l 3 0 I I 2 M E j g C V 7 g 1 U m d Z + f N e V + O l p x i 5 x h + w f n 4 B n B d k f c = &lt; / l a t e x i t &gt; a t Actuator Net &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D m / f y 6 M + T f q s S M W w K 6 L I S 0 E / t f s = " &gt; A A A B 8 3 i c b V D L S s N A F J 3 U V 6 2 v q k s 3 g 0 V w V R K R 6 r L o x m U F + 4 C m l M n 0 p h 0 6 m Y S Z G 6 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 e m D g c M 6 9 3 D M n S K Q w 6 L p f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 T J x q D m 0 e y 1 j 3</p>
<p>x a y 1 Y + c w h m J P 1 9 Q P h 6 a F 8 &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " a a 4 Z S t W 7 b aZ o B F L 5 g U c f y Y R U x x o = " &gt; A A A B 8 3 i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i Q i 1 W X R j c s K 9 g F N K Z P p p B 0 6 m Y S Z G 6 G E / o Y b F 4 q 4 9 W f c + T d O 2 i y 0 e m D g c M 6 9 3 D M n S K Q w 6 L p f T m l t f W N z q 7 x d 2 d n d 2 z + o H h 5 1 T J x q x t s s l r H u B d R w K R R v o 0 D J e 4 n m N A o k 7 w b T 2 9 z v P n J t R K w e c J b w Q U T H S o S C U b S S 7 0 c U J 0 G Y 0 f k Q h 9 W a W 3 c X I H + J V 5 A a F G g N q 5 / + K G Z p x B Uy S Y 3 p e 2 6 C g 4 x q F E z y e c V P D U 8 o m 9 I x 7 1 u q a M T N I F t k n p M z q 4 x I G G v 7 F J K F + n M j o 5 E x s y i w k 3 l G s + r l 4 n 9 e P 8 X w e p A J l a T I F V s e C l N J M C Z 5 A W Q k N G c o Z 5 Z Q p o X N S t i E a s r Q 1 l S x J X i r X / 5 L O h d 1 r 1 F v 3 F / W m j d F H W U 4 g V M 4 B w + u o A l 3 0 I I 2 M E j g C V 7 g 1 U m d Z + f N e V + O l p x i 5 x h + w f n 4 B n B d k f c = &lt; / l a t e x i t &gt; a t PD UAN &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F X M B i n e 0 4 o G Q d / y V J J 2 6 y G n 3 P 2 w = " &gt; A A A B / H i c b V D N S s N A G N z U v 1 r / o j 1 6 C R b B U 0 l E q s e i F 4 8 V b C 0 0 I W w 2 m 3 b p Z h N 2 v w g h 1 F f x 4 k E R r z 6 I N 9 / G T Z u D t g 4 s O 8 x 8 H z s 7 Q c q Z A t v + N m p r 6 x u b W / X t x s 7 u 3 v 6 B e X g 0 U E k m C e 2 T h C d y G G B F O R O 0 D w w 4 H a a S 4 j j g 9 C G Y 3 p T + w y</p>
<p>6 M 1 4 M l 6 M d + N j M V o z q p 0 m + g P j 8 w e z T 5 V 4 &lt; / l a t e x i t &gt; ω t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F X M B i n e 0 4 o G Q d / y V J J 2 6 y G n 3 P 2 w = " &gt; A A A B / H i c b V D N S s N A G N z U v 1 r / o j 1 6 C R b B U 0 l E q s e i F 4 8 V b C 0 0 I W w 2 m 3 b p Z h N 2 v w g h 1 F f x 4 k E R r z 6 I N 9 / G T Z u D t g 4 s O 8 x 8 H z s 7 Q c q Z A t v + N m p r 6 x u b W / X t x s 7 u 3 v 6 B e X g 0 U E k m C e 2 T h C d y G G B F O R O 0 D w w 4 H a a S 4 j j g 9 C G Y 3 p T + w y</p>
<p>6 M 1 4 M l 6 M d + N j M V o z q p 0 m + g P j 8 w e z T 5 V 4 &lt; / l a t e x i t &gt; ω t &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " E m z R 6 7 a z K w D V U P e r E x J z o p L 4 m w o</p>
<p>Fig. 5 :
5
Fig.5: End-to-end fine-tuning from a pre-trained WBC leads to the best task performance.Throwing evaluation metrics across 100 simulated throws for four policies: Our fine-tuned WBC (Ours) achieves the longest throw distance with lower peak leg power as compared to a throwing policy trained from scratch (No-Pre-Training) or a high-level policy for a frozen WBC (No-E2E).The WBC before finetuning (No-Fine-Tuning) has the lowest peak leg power but throws the ball a much shorter distance.</p>
<p>Fig. 6 :
6
Fig. 6: Real and simulated snapshots of athletic tasks.Visualizing simulated and real rollouts of whole-body behaviors.</p>
<p>Fig. 7 :
7
Fig.7: UAN achieves the tightest real-to-sim fit to the training data, as well as a throw trajectory unseen during training.We rolled out three real-world joint trajectories: square &amp; sine waves at each joint, Gaussian noise across all the joints, and a throw.Square waves, sine waves, and gaussian noise were seen during training, while the throw was not.We found that Actuator-Net error remains bounded on the 5 s throw trajectory but diverges when rolling out the 5 min training trajectories, while the UAN learned to remain accurate across long rollouts through RL training.</p>
<p>Fig. 8 :
8
Fig. 8: Comparison of system identification methods on open-loop rollouts of the training data.</p>
<p>Fig. 9 :
9
Fig. 9: Sim-to-real gap for a throwing trajectory unseen during training.</p>
<p>. Table III details the WBC reward components, while Table IV shows the reward function for UAN training.</p>
<p>.
HYPERPARAMETERUAN VALUE PRE-TRAIN VALUE FINE-TUNE VALUEDISCOUNT FACTOR0.9950.990.99GAE PARAMETER0.950.950.95ENTROPY COEFFICIENT0.00.010.0ACTOR LEARNING RATEADAPTIVEADAPTIVEADAPTIVECRITIC LEARNING RATE5.E-45.E-45.E-4KL THRESHOLD0.010.010.01HORIZON962424NUMBER OF ENVIRONMENTS409640964096ACTOR MINIBATCH SIZE983042457624576CRITIC MINIBATCH SIZE3932169830498304# OF MINI EPOCHS555OPTIMIZERADAMWADAMWADAMWWEIGHT DECAY0.010.010.01</p>
<p>TABLE I :
I
PPO Hyperparameters</p>
<p>TABLE II :
II
WBC &amp; Fine-tune Domain Randomization
REWARD COMPONENTTERMSCALEEE POSE TRACKING3 i1 3 exp −2 p cmd i,t − pi,t5.0LINEAR VELOCITY TRACKINGexp −4 v cmd x,t , v cmd y,t− [vx,t, vy,t]2 22.0ANGULAR VELOCITY TRACKINGexp −4 ω cmd z,t − ωz,t21.0GAITi∈{FR,FL,RR,RL} {∼ ci} 1{p i z,t &lt; 0.043}-0.5NO SLIPi∈{FR,FL,RR,RL} {ci} exp −0.1 v i t2 2-0.5FOOT CLEARANCEi∈{FR,FL,RR,RL} {∼ ci} p cmd,i z,t− p i z,t2 2-40.0MECHANICAL POWER|τ t • qt |-0.0001ACTION SMOOTHNESS|at − at−1| 2 2 + 1 2 |at − 2at−1 + at−2| 2 2-0.05LINEAR VELOCITY Z|vz,t| 2-2.0ANGULAR VELOCITY XY|[ωx,t, ωy,t]| 2 2-0.05JOINT POSITIONSq default t− q t-0.25COLLISION{|F arm t| 2 2 &gt; 0.1 OR F leg t2 2&gt; 0.1}-5.0JOINT POSITION LIMITS-10.0CONTACT FORCEi∈{FR,FL,RR,RL} F i t2 2-0.000004
i − min(qi,t − q min i,t , 0) + max(qi,t − q max i,t , 0)</p>
<p>TABLE III
III: WBC RewardsREWARD COMPONENTTERMSCALEJOINT POSITIONS (L1)q real t− q sim t-1.5JOINT POSITIONS (RELAXED)exp −100 q real
ACKNOWLEDGMENTWe thank the members of the Improbable AI lab-especially Sandor Felber, Chen Bo Calvin Zhang, Srinath Mahankali, and Zhang-Wei Hong-for helpful discussions and feedback.We acknowledge Unitree Robotics for technical support provided for their robots.We are grateful to MIT Supercloud and the Lincoln Laboratory Supercomputing Center for providing HPC resources.We also acknowledge the MIT CSAIL Living Lab project for providing robot hardware.This research was partly supported by Hyundai Motor Company, the MIT-IBM Watson AI Lab, and the National Science Foundation under Cooperative Agreement PHY-2019786 (The NSF AI Institute for Artificial Intelligence and Fundamental Interactions, http://iaifi.org/)and the National Science Foundation Graduate Research Fellowship under Grant No. 2141064.This research was also sponsored by the United States Air Force Research Laboratory and the United States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative Agreement Number FA8750-19-2-1000.Research was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-21-1-0328.The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Air Force or the U.S. Government.The U.S. Government is authorized to reproduce and distribute reprints for Government purposes, notwithstanding any copyright notation herein.AUTHOR CONTRIBUTIONS• Nolan Fey contributed to ideation, implementation of the entire system, experimental evaluation, and writing.• Gabriel B. Margolis contributed to ideation, implementation of some parts of the system, and writing.• Martin Peticco contributed to hardware modifications and writing.• Pulkit Agrawal advised the project and contributed to its development, experimental design, and writing.
Dynamic whole-body robotic manipulation. Yeuhi Abe, Benjamin Stephens, Michael P Murphy, Alfred A Rizzi, Unmanned Systems Technology XV. SPIE20138741</p>
<p>Estimation of inertial parameters of manipulator loads and links. G Christopher, Chae H Atkeson, John M An, Hollerbach, 10.1177/027836498600500306The International Journal of Robotics Research. 531986</p>
<p>Alma-articulated locomotion and manipulation for a torquecontrollable robot. Dario Bellicoso, Koen Krämer, Markus Stäuble, Dhionis Sako, Fabian Jenelten, Marko Bjelonic, Marco Hutter, 2019 International conference on robotics and automation (ICRA). IEEE2019</p>
<p>Xuxin Cheng, Yandong Ji, Junming Chen, Ruihan Yang, Ge Yang, Xiaolong Wang, arXiv:2402.16796Expressive whole-body control for humanoid robots. 2024arXiv preprint</p>
<p>Learning multi-modal whole-body control for real-world humanoid robots. Pranay Dugar, Aayam Shrestha, Fangzhou Yu, Bart Van Marum, Alan Fern, arXiv:2408.072952024arXiv preprint</p>
<p>Deep wholebody control: Learning a unified policy for manipulation and locomotion. Zipeng Fu, Xuxin Cheng, Deepak Pathak, Conference on Robot Learning. PMLR2023</p>
<p>Humanplus: Humanoid shadowing and imitation from humans. Zipeng Fu, Qingqing Zhao, Qi Wu, Gordon Wetzstein, Chelsea Finn, 2024</p>
<p>Design and control of a bipedal robotic character. Ruben Grandia, Espen Knoop, Michael Hopkins, Georg Wiedebach, Jared Bishop, Steven Pickles, David Müller, Moritz Bächer, 10.15607/RSS.2024.XX.103072024</p>
<p>Residual model learning for microrobot control. Joshua Gruenstein, Tao Chen, Neel Doshi, Pulkit Agrawal, 2021</p>
<p>Umi on legs: Making manipulation policies mobile with manipulation-centric whole-body controllers. Huy Ha, Yihuai Gao, Zipeng Fu, Jie Tan, Shuran Song, arXiv:2407.103532024arXiv preprint</p>
<p>Learning agile soccer skills for a bipedal robot with deep reinforcement learning. Tuomas Haarnoja, Ben Moran, Guy Lever, Sandy H Huang, Dhruva Tirumala, Jan Humplik, Markus Wulfmeier, Saran Tunyasuvunakool, Noah Y Siegel, Roland Hafner, Michael Bloesch, Kristian Hartikainen, Arunkumar Byravan, Leonard Hasenclever, Yuval Tassa, Fereshteh Sadeghi, Nathan Batchelor, Federico Casarini, Stefano Saliceti, Charles Game, Neil Sreendra, Kushal Patel, Marlon Gwira, Andrea Huber, Nicole Hurley, Francesco Nori, Raia Hadsell, Nicolas Heess, 10.1126/scirobotics.adi8022Science Robotics. 2470-9476989April 2024</p>
<p>Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu Liu, Guanya Shi, arXiv:2403.04436Learning human-tohumanoid real-time whole-body teleoperation. 2024arXiv preprint</p>
<p>Learning agile and dynamic motor skills for legged robots. Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, Marco Hutter, 10.1126/scirobotics.aau5872Science Robotics. 2470-9476426January 2019</p>
<p>Concurrent training of a control policy and a state estimator for dynamic and robust legged locomotion. Gwanghyeon Ji, Juhyeok Mun, Hyeongjun Kim, Jemin Hwangbo, 10.1109/LRA.2022.3151396IEEE Robotics and Automation Letters. 722022</p>
<p>Hierarchical reinforcement learning for precise soccer shooting skills using a quadrupedal robot. Yandong Ji, Zhongyu Li, Yinan Sun, Xue Bin Peng, Sergey Levine, Glen Berseth, Koushil Sreenath, 10.1109/IROS47612.2022.99819842022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). 2022</p>
<p>Dribblebot: Dynamic legged manipulation in the wild. Yandong Ji, B Gabriel, Pulkit Margolis, Agrawal, 10.1109/ICRA48891.2023.101603252023 IEEE International Conference on Robotics and Automation (ICRA). 2023</p>
<p>Rma: Rapid motor adaptation for legged robots. Ashish Kumar, Zipeng Fu, Deepak Pathak, Jitendra Malik, 2021</p>
<p>Learning quadrupedal locomotion over challenging terrain. Joonho Lee, Jemin Hwangbo, Vladlen Lorenz Wellhausen, Marco Koltun, Hutter, 10.1126/scirobotics.abc5986Science Robotics. 2470-9476547October 2020</p>
<p>Geometric robot dynamic identification: A convex programming approach. Taeyoon Lee, Patrick M Wensing, Frank C Park, 10.1109/TRO.2019.2926491IEEE Transactions on Robotics. 3622020</p>
<p>Reinforcement learning for versatile, dynamic, and robust bipedal locomotion control. Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, Koushil Sreenath, 2024</p>
<p>Minghuan Liu, Zixuan Chen, Xuxin Cheng, Yandong Ji, Ruihan Yang, Xiaolong Wang, arXiv:2403.16967Visual whole-body control for legged loco-manipulation. 2024arXiv preprint</p>
<p>Hybrid internal model: Learning agile legged locomotion with simulated robot response. Junfeng Long, Zirui Wang, Quanyi Li, Jiawei Gao, Liu Cao, Jiangmiao Pang, 2024</p>
<p>Perpetual humanoid control for real-time simulated avatars. Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Zhengyi Luo, Jiashun Wang, Kangni Liu, Haotian Zhang, Chen Tessler, Jingbo Wang, Ye Yuan, Jinkun Cao, Zihui Lin, Fengyi Wang, arXiv:2407.00187Sports environments for physically simulated humanoids. 2024arXiv preprint</p>
<p>Combining learning-based locomotion policy with model-based manipulation for legged mobile manipulators. Yuntao Ma, Farbod Farshidian, Takahiro Miki, Joonho Lee, Marco Hutter, IEEE Robotics and Automation Letters. 722022</p>
<p>Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, arXiv:2108.10470Isaac gym: High performance gpu-based physics simulation for robot learning. 2021arXiv preprint</p>
<p>Rapid locomotion via reinforcement learning. Ge Gabriel B Margolis, Kartik Yang, Tao Paigwar, Pulkit Chen, Agrawal, 2022</p>
<p>High degree-of-freedom dynamic manipulation. Benjamin Michael P Murphy, Yeuhi Stephens, Alfred A Abe, Rizzi, Unmanned Systems Technology XIV. SPIE20128387</p>
<p>Dreamwaq: Learning robust quadrupedal locomotion with implicit terrain imagination via deep reinforcement learning. Aswin Made, Byeongho Nahrendra, Hyun Yu, Myung, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>NVIDIA. Nvidia isaac-sim. May 2022</p>
<p>Guoping Pan, Qingwei Ben, Zhecheng Yuan, Guangqi Jiang, Yandong Ji, Jiangmiao Pang, Houde Liu, Huazhe Xu, arXiv:2403.17367Roboduet: A framework affording mobile-manipulation and cross-embodiment. 2024arXiv preprint</p>
<p>Learning force control for legged manipulation. Tifanny Portela, Yandong Gabriel B Margolis, Pulkit Ji, Agrawal, arXiv:2405.014022024arXiv preprint</p>
<p>Real-world humanoid locomotion with reinforcement learning. Ilija Radosavovic, Tete Xiao, Bike Zhang, Trevor Darrell, Jitendra Malik, Koushil Sreenath, 2023</p>
<p>Learning humanoid locomotion over challenging terrain. Ilija Radosavovic, Sarthak Kamat, Trevor Darrell, Jitendra Malik, 2024</p>
<p>Ai for humanoid robotics -a lecture by mentee robotics' ceo, prof. lior wolf. Mentee Robotics, Aug 2024</p>
<p>Learning to walk in minutes using massively parallel deep reinforcement learning. Nikita Rudin, David Hoeller, Philipp Reist, Marco Hutter, 2022</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, Proximal policy optimization algorithms. 2017</p>
<p>Curiosity-driven learning of joint locomotion and manipulation tasks. Clemens Schwarke, Victor Klemm, Matthijs Van Der Boon, Marko Bjelonic, Marco Hutter, Proceedings of The 7th Conference on Robot Learning. The 7th Conference on Robot LearningPMLR2023229</p>
<p>Improving domain transfer of robot dynamics models with geometric system identification and learned friction compensation. Laura Schwendeman, Andrew Saloutos, Elijah Stanger-Jones, Sangbae Kim, 10.1109/Humanoids57100.2023.103752332023 IEEE-RAS 22nd International Conference on Humanoid Robots (Humanoids). 2023</p>
<p>Synthesis of whole-body behaviors through hierarchical control of behavioral primitives. Luis Sentis, Oussama Khatib, 10.1142/S0219843605000594International Journal of Humanoid Robotics. 02042005</p>
<p>A whole-body control framework for humanoids operating in human environments. Luis Sentis, Oussama Khatib, Proceedings 2006 IEEE International Conference on Robotics and Automation. 2006 IEEE International Conference on Robotics and AutomationIEEE2006. ICRA 2006. 2006</p>
<p>Humanoidbench: Simulated humanoid benchmark for whole-body locomotion and manipulation. Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, Pieter Abbeel, 2024</p>
<p>Actuator-constrained reinforcement learning for highspeed quadrupedal locomotion. Young-Ha Shin, Tae-Gyu Song, Gwanghyeon Ji, Hae-Won Park, 2023</p>
<p>A unified mpc framework for whole-body dynamic locomotion and manipulation. Jean-Pierre Sleiman, Farbod Farshidian, Maria Vittoria Minniti, Marco Hutter, IEEE Robotics and Automation Letters. 632021</p>
<p>Residual physics learning and system identification for sim-to-real transfer of policies on buoyancy assisted legged robots. Nitish Sontakke, Hosik Chae, Sangjoon Lee, Tianle Huang, Dennis W Hong, Sehoon Hal, 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2023</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, Vincent Vanhoucke, 2018</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel, 2017</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, 10.1109/IROS.2012.63861092012 IEEE/RSJ International Conference on Intelligent Robots and Systems. 2012</p>
<p>Dynamics randomization revisited:a case study for quadrupedal locomotion. Zhaoming Xie, Xingye Da, Buck Michiel Van De Panne, Animesh Babich, Garg, 2021</p>
<p>Tossingbot: Learning to throw arbitrary objects with residual physics. Andy Zeng, Shuran Song, Johnny Lee, Alberto Rodriguez, Thomas Funkhouser, IEEE Transactions on Robotics. 3642020</p>
<p>Chong Zhang, Wenli Xiao, Tairan He, Guanya Shi, Wococo, arXiv:2406.06005Learning whole-body humanoid control with sequential contacts. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
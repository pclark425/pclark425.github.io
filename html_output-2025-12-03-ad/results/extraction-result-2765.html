<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2765 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2765</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2765</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-218486872</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2005.00811v1.pdf" target="_blank">Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we consider the recent trend of evaluating progress on reinforcement learning technology by using text-based environments and games as evaluation environments. This reliance on text brings advances in natural language processing into the ambit of these agents, with a recurring thread being the use of external knowledge to mimic and better human-level performance. We present one such instantiation of agents that use commonsense knowledge from ConceptNet to show promising performance on two text-based environments.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2765.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2765.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Belief+KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-aware RL agent (Belief graph + ConceptNet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-based reinforcement learning agent that combines a dynamically constructed belief graph (from observations) with an external commonsense knowledge graph (ConceptNet) using GCNs to improve action selection in TextWorld games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Belief+KG Agent (KG Full / KG Evolve)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical GRU encoders produce observation and action embeddings; a graph-based knowledge extractor merges a dynamic belief graph (from the game's textual observations) with a commonsense subgraph extracted from ConceptNet; node embeddings (Numberbatch) are updated with L-layer GCN message passing to produce a graph encoding which is concatenated with state and action embeddings and scored by an MLP. Trained with Advantage Actor-Critic (A2C).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-based partially-observable games where the agent receives textual observations and issues text commands to accomplish goals (e.g., Kitchen Cleanup: put objects in correct locations; Cooking Recipe: retrieve ingredients).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>graph-based memory (dynamic belief graph + external commonsense KG)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Knowledge represented as a graph (nodes = entities/concepts, edges = relations); belief and commonsense subgraphs are merged into a single graph G_t; GCN-updated node embeddings are averaged to a single graph vector.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Entities and relations mentioned in observations (belief of world state: objects, locations, relations) and commonsense triples from ConceptNet relevant to mentioned entities (AtLocation, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Graph is encoded via message passing (L-layer GCN) into node embeddings Z_t which are averaged into a single graph vector g_t; this g_t is concatenated with state and action embeddings for scoring (i.e., retrieval = aggregated GCN encoding focused on entities mentioned / merged).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated each time-step: belief graph nodes added/deleted based on current textual observation; commonsense subgraph is extracted from the history of observations and either provided fully at start (Full) or revealed incrementally as entities are observed (Evolve).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To represent and track world state symbolically, prune/down-weight implausible actions, reduce exploration, and guide planning toward goal-relevant concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Knowledge-aware agents outperform baselines (Random and Simple text-only) on the Kitchen Cleanup and Cooking Recipe tasks; the evolve-graph (incremental KG) setting yields the best performance in Kitchen Cleanup (KG Evolve > KG Full > Simple > Random). Exact numeric metrics are not provided in the paper text (figures referenced but numbers not listed).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Simple (text-only) agent achieved lower average score and required more interactions than knowledge-aware agents; Random baseline performs worst. Exact numeric metrics are not provided in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Incremental (evolve) provision of commonsense knowledge is more effective than giving the full KG at once; full KG can overwhelm the agent and cause noisy exploration. Effectiveness is task-dependent: in some tasks (simple cooking-recipe retrieving a single ingredient in same room), ground-truth belief graph (GATA Full) can outperform commonsense KG.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Commonsense KG can overwhelm the agent if provided in full, introducing noise and leading to ineffective exploration; benefits depend on task relevance of the KG (KG may be unhelpful or detrimental when it contains little task-relevant information).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>Evolve-graph (incremental commonsense KG merged with belief graph) performed best for the Kitchen Cleanup task; however, for the Cooking Recipe task GATA Full (ground-truth belief graph) performed best, indicating that the best configuration is task-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2765.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2765.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Aided Transformer Agent (GATA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent that learns a dynamic belief graph of the game state and uses it to plan and generalize in text-based games; the belief graph is used to prune action space and guide exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on textbased games.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GATA</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Represents the game state as a dynamic belief graph learned during exploration and uses that symbolic structure to aid planning and action selection (cited from Adhikari et al., 2020); in this paper GATA is used as a baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-based games in TextWorld; in the experiments used for comparison, cooking-recipe games (single ingredient, same room) and other TextWorld tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>belief graph (dynamic world-state graph)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Dynamic graph of entities and relations representing the (learned or observed) state; treated as the agent's internal symbolic state representation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Observed entities, their locations and relations as perceived from textual observations (state information).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Belief graph is generated/updated based on observations during exploration (evolve-graph) or can be provided as full ground-truth belief graph (full).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Prune action space, enable efficient planning and generalization by providing a symbolic state representation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>GATA Full (ground-truth belief graph) performed significantly better than other agents on the Cooking Recipe task in these experiments (paper notes GATA Full outperformed Belief+KG variants). Exact numeric metrics are not provided in the paper text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Ground-truth belief graph (full) can be more beneficial than commonsense KG when the task provides sufficient ground-truth state information or when commonsense KG offers little relevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>GATA Full (using ground-truth/full belief graph) performed best on the evaluated simple cooking-recipe task, suggesting full belief graph can outperform external commonsense KG when it contains direct task-relevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2765.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2765.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-DQN (graph-based DQN for text games)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that learns a belief graph during exploration and uses it to prune action space and improve efficiency in text-adventure games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Graph-based deep RL agent (KG-DQN) that constructs and uses a knowledge/belief graph of the environment to inform action selection and reduce the action space (cited from Ammanabrolu & Riedl, 2019).</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>TextWorld / text-adventure games</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-adventure environments where agents must interpret textual observations and take text actions to reach goals; KG-DQN uses a learned belief graph for this.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>belief graph</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Dynamic graph representing learned belief of world state (entities and relations).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Discovered entities, relations and state information inferred while exploring the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Belief graph is updated during exploration based on observed textual information.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Prune invalid or implausible actions, enable efficient exploration and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Mentioned as related prior work showing that learned belief graphs can prune action space and enable efficient exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning dynamic knowledge graphs to generalize on textbased games. <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Building dynamic knowledge graphs from text using machine reading comprehension. <em>(Rating: 2)</em></li>
                <li>Learn what not to learn: Action elimination with deep reinforcement learning. <em>(Rating: 1)</em></li>
                <li>Textworld: A learning environment for text-based games. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2765",
    "paper_id": "paper-218486872",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "Belief+KG",
            "name_full": "Knowledge-aware RL agent (Belief graph + ConceptNet)",
            "brief_description": "A text-based reinforcement learning agent that combines a dynamically constructed belief graph (from observations) with an external commonsense knowledge graph (ConceptNet) using GCNs to improve action selection in TextWorld games.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Belief+KG Agent (KG Full / KG Evolve)",
            "agent_description": "Hierarchical GRU encoders produce observation and action embeddings; a graph-based knowledge extractor merges a dynamic belief graph (from the game's textual observations) with a commonsense subgraph extracted from ConceptNet; node embeddings (Numberbatch) are updated with L-layer GCN message passing to produce a graph encoding which is concatenated with state and action embeddings and scored by an MLP. Trained with Advantage Actor-Critic (A2C).",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld",
            "game_description": "Text-based partially-observable games where the agent receives textual observations and issues text commands to accomplish goals (e.g., Kitchen Cleanup: put objects in correct locations; Cooking Recipe: retrieve ingredients).",
            "uses_memory": true,
            "memory_type": "graph-based memory (dynamic belief graph + external commonsense KG)",
            "memory_structure": "Knowledge represented as a graph (nodes = entities/concepts, edges = relations); belief and commonsense subgraphs are merged into a single graph G_t; GCN-updated node embeddings are averaged to a single graph vector.",
            "memory_content": "Entities and relations mentioned in observations (belief of world state: objects, locations, relations) and commonsense triples from ConceptNet relevant to mentioned entities (AtLocation, etc.).",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Graph is encoded via message passing (L-layer GCN) into node embeddings Z_t which are averaged into a single graph vector g_t; this g_t is concatenated with state and action embeddings for scoring (i.e., retrieval = aggregated GCN encoding focused on entities mentioned / merged).",
            "memory_update_strategy": "Updated each time-step: belief graph nodes added/deleted based on current textual observation; commonsense subgraph is extracted from the history of observations and either provided fully at start (Full) or revealed incrementally as entities are observed (Evolve).",
            "memory_usage_purpose": "To represent and track world state symbolically, prune/down-weight implausible actions, reduce exploration, and guide planning toward goal-relevant concepts.",
            "performance_with_memory": "Knowledge-aware agents outperform baselines (Random and Simple text-only) on the Kitchen Cleanup and Cooking Recipe tasks; the evolve-graph (incremental KG) setting yields the best performance in Kitchen Cleanup (KG Evolve &gt; KG Full &gt; Simple &gt; Random). Exact numeric metrics are not provided in the paper text (figures referenced but numbers not listed).",
            "performance_without_memory": "Simple (text-only) agent achieved lower average score and required more interactions than knowledge-aware agents; Random baseline performs worst. Exact numeric metrics are not provided in the paper text.",
            "has_memory_ablation": true,
            "memory_effectiveness_findings": "Incremental (evolve) provision of commonsense knowledge is more effective than giving the full KG at once; full KG can overwhelm the agent and cause noisy exploration. Effectiveness is task-dependent: in some tasks (simple cooking-recipe retrieving a single ingredient in same room), ground-truth belief graph (GATA Full) can outperform commonsense KG.",
            "memory_limitations": "Commonsense KG can overwhelm the agent if provided in full, introducing noise and leading to ineffective exploration; benefits depend on task relevance of the KG (KG may be unhelpful or detrimental when it contains little task-relevant information).",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "Evolve-graph (incremental commonsense KG merged with belief graph) performed best for the Kitchen Cleanup task; however, for the Cooking Recipe task GATA Full (ground-truth belief graph) performed best, indicating that the best configuration is task-dependent.",
            "uuid": "e2765.0",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "GATA",
            "name_full": "Graph Aided Transformer Agent (GATA)",
            "brief_description": "An agent that learns a dynamic belief graph of the game state and uses it to plan and generalize in text-based games; the belief graph is used to prune action space and guide exploration.",
            "citation_title": "Learning dynamic knowledge graphs to generalize on textbased games.",
            "mention_or_use": "use",
            "agent_name": "GATA",
            "agent_description": "Represents the game state as a dynamic belief graph learned during exploration and uses that symbolic structure to aid planning and action selection (cited from Adhikari et al., 2020); in this paper GATA is used as a baseline for comparison.",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld",
            "game_description": "Text-based games in TextWorld; in the experiments used for comparison, cooking-recipe games (single ingredient, same room) and other TextWorld tasks.",
            "uses_memory": true,
            "memory_type": "belief graph (dynamic world-state graph)",
            "memory_structure": "Dynamic graph of entities and relations representing the (learned or observed) state; treated as the agent's internal symbolic state representation.",
            "memory_content": "Observed entities, their locations and relations as perceived from textual observations (state information).",
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": "Belief graph is generated/updated based on observations during exploration (evolve-graph) or can be provided as full ground-truth belief graph (full).",
            "memory_usage_purpose": "Prune action space, enable efficient planning and generalization by providing a symbolic state representation.",
            "performance_with_memory": "GATA Full (ground-truth belief graph) performed significantly better than other agents on the Cooking Recipe task in these experiments (paper notes GATA Full outperformed Belief+KG variants). Exact numeric metrics are not provided in the paper text.",
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Ground-truth belief graph (full) can be more beneficial than commonsense KG when the task provides sufficient ground-truth state information or when commonsense KG offers little relevant information.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "GATA Full (using ground-truth/full belief graph) performed best on the evaluated simple cooking-recipe task, suggesting full belief graph can outperform external commonsense KG when it contains direct task-relevant information.",
            "uuid": "e2765.1",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "KG-DQN",
            "name_full": "KG-DQN (graph-based DQN for text games)",
            "brief_description": "A prior approach that learns a belief graph during exploration and uses it to prune action space and improve efficiency in text-adventure games.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "mention_or_use": "mention",
            "agent_name": "KG-DQN",
            "agent_description": "Graph-based deep RL agent (KG-DQN) that constructs and uses a knowledge/belief graph of the environment to inform action selection and reduce the action space (cited from Ammanabrolu & Riedl, 2019).",
            "base_model_size": null,
            "game_benchmark_name": "TextWorld / text-adventure games",
            "game_description": "Text-adventure environments where agents must interpret textual observations and take text actions to reach goals; KG-DQN uses a learned belief graph for this.",
            "uses_memory": true,
            "memory_type": "belief graph",
            "memory_structure": "Dynamic graph representing learned belief of world state (entities and relations).",
            "memory_content": "Discovered entities, relations and state information inferred while exploring the environment.",
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": "Belief graph is updated during exploration based on observed textual information.",
            "memory_usage_purpose": "Prune invalid or implausible actions, enable efficient exploration and planning.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Mentioned as related prior work showing that learned belief graphs can prune action space and enable efficient exploration.",
            "memory_limitations": null,
            "comparison_with_other_memory_types": null,
            "best_memory_configuration": null,
            "uuid": "e2765.2",
            "source_info": {
                "paper_title": "Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge",
                "publication_date_yy_mm": "2020-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on textbased games.",
            "rating": 2
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "rating": 2
        },
        {
            "paper_title": "Building dynamic knowledge graphs from text using machine reading comprehension.",
            "rating": 2
        },
        {
            "paper_title": "Learn what not to learn: Action elimination with deep reinforcement learning.",
            "rating": 1
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games.",
            "rating": 2
        }
    ],
    "cost": 0.0111375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge
2 May 2020</p>
<p>Keerthiram Murugesan keerthiram.murugesan@ibm.com 
Pushkar Shukla pushkarshukla@ttic.edu 
Mrinmaya Sachan mrinmaya@ttic.edu 
Pavan Kapanipathi kapanipa@us.ibm.com 
Enhancing Text-based Reinforcement Learning Agents with Commonsense Knowledge
2 May 2020AD650EF8DCF26DE441D89D4F14C87F68arXiv:2005.00811v1[cs.AI]
In this paper, we consider the recent trend of evaluating progress on reinforcement learning technology by using text-based environments and games as evaluation environments.This reliance on text brings advances in natural language processing into the ambit of these agents, with a recurring thread being the use of external knowledge to mimic and better human-level performance.We present one such instantiation of agents that use commonsense knowledge from ConceptNet to show promising performance on two text-based environments.</p>
<p>Introduction</p>
<p>Over the years, simulation environments and games have been used extensively to showcase and drive advances in reinforcement learning technology.A recent environment that has received much focus is TextWorld (TW) (Côté et al., 2018), where an agent must interact with an external environment to achieve goals while maximizing reward -all of this using only the modality of text.TextWorld and similar text-based tasks seek to bring advances in natural language processing (NLP) and question answering solutions to agent-based reinforcement learning techniques, and vice-versa.</p>
<p>A common thread inherent in solutions to some of the NLP tasks is that mere text-based techniques cannot achieve or beat the human-level performance and that NLP systems must instead learn how to utilize additional knowledge from external sources such as knowledge bases (KBs) and knowledge graphs (KGs) to improve their overall performance.Figure 1 presents a running example that illustrates this: in the figure, the additional knowledge that must be utilized effectively by the * Both student authors contributed equally.</p>
<p>agent is presented in the bottom left corner under the ConceptNet heading.</p>
<p>In general, the use of external knowledge to improve the accuracy of NLP tasks has garnered significant attention from the community.Specifically, for tasks like natural language inference (NLI), recent work (Kapanipathi et al., 2020;Wang et al., 2019) has shown that while external knowledge can bring in useful information, this must be balanced by the context-specific relevance of the new information fed into the system.If this is not done properly, there is a very high risk of overwhelming the agent/algorithm with too much information, leading to poor decisions and performance.</p>
<p>In this paper, we present a novel approach to the use of external knowledge from the Concept-Net (Liu and Singh, 2004;Speer et al., 2017) knowledge graph to reduce the exploration space for a Reinforcement Learning (RL) agent.Specifically, we consider an RL based agent that is able to model the world around it at two levels -a local, or belief, graph that describes its current belief of the state of the world; and a global or commonsense graph of entities that are related to that state -and the interaction between those two levels.The belief graph provides the agent with a symbolic way to represent its current perception of the world, which can be easily combined with symbolic commonsense knowledge from the commonsense graph.This two-level representation of the world and the knowledge about it follows the model proposed in the Graph Aided Transformer Agent (GATA) (Adhikari et al., 2020) framework.</p>
<p>Using this model, we are able to show a significant improvement in the performance of an RL agent in a kitchen cleanup task that is set in the TextWorld setting.An example of such a kitchen cleanup task is shown in Figure 1: the agent is given an initial observation (which is used to pro-</p>
<p>Goal</p>
<p>Clean up the kitchen</p>
<p>ConceptNet</p>
<p>Apple</p>
<p>Refrigerator AtLocation</p>
<p>Plate</p>
<p>Cabinet AtLocation</p>
<p>Agent</p>
<p>Best action trajectory 1.Take the apple from the table 2. Take the plate from the table 3. Open the refrigerator 4. Put the apple in the refrigerator 5. Open the cabinet 6.Put the plate in the cabinet Plausible Actions 1. Open the cabinet 2. Eat the apple 3. Put the apple in the cabinet 4. ... duce the first iteration of the agent's belief graph), with the final goal of cleaning up the kitchen.The agent has to produce the list of actions that are necessary to achieve this goal: that list is given on the right hand side.Finally, the additional external knowledge from the ConceptNet knowledge graph -which makes up the global graph for our agent -is shown at the bottom left.In the case of this running example, the agent may discover from ConceptNet that apples are usually located in refrigerators, and plates are located in cabinets.We will use this kitchen cleanup instance as a running example throughout the paper.</p>
<p>By evaluating our approach on two different tasks -a kitchen cleanup task as above, and an additional cooking recipe task -we can show that the interaction between the belief and commonsense graphs can reduce the exploration of the RL agent in comparison to the purely text-based model.However, we are also able to demonstrate a more nuanced point: merely providing an agent with commonsense knowledge is not sufficient to improve its performance.Indeed, oftentimes it is detrimental to the agent's performance.We show that this is due to the agent being overwhelmed with too much commonsense knowledge, and discuss how different tasks and settings have different demands on the knowledge that is used by an agent.</p>
<p>Related Work</p>
<p>We start out with a look at work that is related to our focus area, which we categorize into three primary areas below.Our work sits at the intersection of knowledge graphs and the use of commonsense (and external) knowledge to make reinforcement learning more efficient; and our improvements are showcased in TextWorld and adjacent text-based domains.</p>
<p>Knowledge Graphs</p>
<p>Graphs have become a common way to represent knowledge.These knowledge graphs consist of a set of concepts (nodes) connected by relationships (edges).Well-known knowledge graphs (KGs) that are openly available include Freebase (Bollacker et al., 2008), DBpedia (Auer et al., 2007), WordNet (Miller, 1995), and ConceptNet (Speer et al., 2017).Each of these KGs comprises different types of knowledge.For the tasks that our work considers, we found that the commonsense knowledge available in ConceptNet is more suitable than the encyclopedic knowledge from DBpedia or Freebase -we hence focus on this.Since our approach considers the KG as a generic graph structure, it is amenable to the use of any of the KGs mentioned here.</p>
<p>Knowledge graphs have been used to perform reasoning to improve performance in various domains, particularly within the NLP community.In particular, KGs have been leveraged for tasks such as Entity Linking (Hoffart et al., 2012), Question Answering (Sun et al., 2018;Das et al., 2017;Atzeni and Atzori, 2018), Sentiment Analysis (Recupero et al., 2015;Atzeni et al., 2018) and Natural Language Inference (Kapanipathi et al., 2020).Different techniques have been explored for their use.In most cases, knowledge graph embeddings such as TransH (Wang et al., 2014) and Com-plEx (Trouillon et al., 2016) are used to vectorize the concepts and relationships in a KG as input to a learning framework.Reinforcement learning has also been used to find relevant paths in a knowledge graph for knowledge base question answering (Das et al., 2017).Sun et al. (2018) and Kapanipathi et al. (2020) find sub-graphs from the corresponding KGs and encode them using a graph convolutional networks (Kipf and Welling, 2016) for question answering and natural language inference respectively.</p>
<p>External Knowledge for Sample Efficient Reinforcement Learning</p>
<p>A key challenge for current reinforcement learning (RL) technology is the low sample efficiency (Kaelbling et al., 1998).RL techniques require a large amount of interaction with the environment which can be very expensive.This has prevented the use of RL in real-world decision-making problems.In contrast, humans possess a wealth of commonsense knowledge which helps them solve problems in the face of incomplete information.Inspired by this, there have been a few recent attempts on adding prior or external knowledge to RL approaches.Notably, Garnelo et al. (2016) propose Deep Symbolic RL, which combines aspects of symbolic AI with neural networks and reinforcement learning as a way to introduce common sense priors.However, their work is mainly theoretical.There has also been some work on policy transfer (Bianchi et al., 2015), which studies how knowledge acquired in one environment can be re-used in another environment; and experience replay (Wang et al., 2016;Lin, 1992Lin, , 1993) ) which studies how an agent's previous experiences can be stored and then later reused.In contrast to the above, in this paper, we explore the use of commonsense knowledge stored in knowledge graphs such as ConceptNet as a way to improve sample efficiency in text-based RL agents.To the best of our knowledge, there is no prior work that explores how commonsense knowledge can be used to make RL agents more efficient.</p>
<p>RL Environments and TextWorld</p>
<p>Games are a rich domain for studying grounded language and how information from the text can be utilized in controlled applications.Notably, in this line of research, Branavan et al. ( 2012) builds an RL-based game player that utilizes text manuals to learn strategies for Civilization II; and Narasimhan et al. (2015) build an RL-based game player for multi-user Dungeon games.In both cases, the text is analyzed and control strategies are learned jointly using feedback from the gaming environment.Similarly, in the vision domain, there has been work on building automatic video game players (Koutník et al., 2013;Mnih et al., 2016).</p>
<p>Our work builds on a recently introduced text-based game TextWorld (Côté et al., 2018).TextWorld is a sandbox learning environment for training and evaluating RL-based agents on textbased games.Since its introduction and other such tools, there has been a large body of work devoted to improving performance on this benchmark.One interesting line of work on TextWorld is on learning symbolic (typically graphical) representations of the agent's belief of the state of the world.Notably, Ammanabrolu and Riedl (2019) proposed KG-DQN and Adhikari et al. ( 2020) proposed GATA; both represent the game state as a belief graph learned during exploration.This graph is used to prune the action space, enabling more efficient exploration.Similar approaches for building dynamic belief graphs have also been explored in the context of machine comprehension of procedural text (Das et al., 2018).In our work, we also represent the world as a belief graph.Moreover, we also explore how the belief graph can be used with commonsense knowledge for efficient exploration.</p>
<p>The LeDeepChef system (Adolphs and Hofmann, 2019), which investigates the generalization capabilities of text-based RL agents as they learn to transfer their cooking skills to neverbefore-seen recipes in unfamiliar house environments, is also related to our work.They achieve transfer by additionally supervising the model with a list of the most common food items in Freebase, allowing their agent to generalize to hitherto unseen recipes and ingredients.</p>
<p>Finally, Zahavy et al. (2018) propose the Action-Elimination Deep Q-Network (AE-DQN) which learns to predict invalid actions in the textadventure game Zork, and eliminates them using contextual bandits.This allows the model to efficiently handle the large action space.The use of common sense knowledge in our work potentially has the same effect of down-weighting implausible actions.</p>
<p>TextWorld as a POMDP</p>
<p>Text-based games can be seen as partially observable Markov decision processes (POMDP) (Kaelbling et al., 1998) where the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state.As an agent interacts with a TextWorld game instance, at each turn, several lines of text describe the state of the game; and the player can issue a text command to change the state in some desirable way (typically, in order to move towards a goal).</p>
<p>Formally, let (S, T, A, Ω, O, R, γ) denote the underlying TextWorld POMDP.Here, S denotes the set of states, A denotes the action space, T denotes the state transition probabilities, Ω denotes the set of observations, O denotes the set of conditional observation probabilities, and γ ∈ [0, 1] is the discount factor.The agent's observation o t at time step t depends on the current state s t and the previous action a t−1 .The agent receives a reward at time step t: r t = R(s t , a t ) and the agent's goal is to maximize the expected discounted sum of rewards:
E[ ∑ t γ t r t ].
TextWorld allows the agent to perceive and interact with the environment via the modality of text.Thus, the observation o t is presented by the environment as a sequence of tokens (o t = {o 1 t , . . .o N t }).Similarly, each action a is also denoted as a sequence of tokens {a 1 , . . ., a M }.</p>
<p>Model Description</p>
<p>In order to solve the above POMDP, we design a model that can leverage commonsense knowledge and learn a graph-structured representation of its belief of the world state.The high-level architecture of the model contains three major components, namely the input encoder, a graphbased knowledge extractor, and the action prediction module.The input encoding layers are used for encoding the observation at time step t and the list of admissible actions.The graph-based knowledge extractor tries to extract knowledge from two different sources.First, it makes use of external commonsense knowledge to improve the ability of the agent to select the correct action at each time step.Secondly, the belief about the environment (world state) perceived by the agent is also captured by a belief graph that is generated dynamically from the textual observations from the game.The information from both sources is then aggregated together in a single graph.The actionprediction module takes as input the encoded ob-servation states, the encoded list of admissible actions and the encoded aggregated graph, and predicts an action for each step.Figure 2 provides a compact visualization of our approach.We describe the various components of our model below.</p>
<p>Input Encoder</p>
<p>At any time step t, the agent observes a textual description of the current state provided as a sequence of tokens o t = (o 1 t , . . ., o N t ).Given the current observation o t , we use pre-trained GloVe embeddings (Pennington et al., 2014)
i = (a 1 i , . . . , a M i ) ∈ A t as a sequence of d-dimensional pretrained GloVe embeddings c 1 i , . . . , c M i .
The agent relies on a hierarchical encoder architecture to model the current state as a vector s t , based on o t and the previous observations.First, a GRU-based encoder is used to process the sequence x 1 t , . . ., x N t of the GloVe embeddings associated with o t .This allows representing the current observation as a single h-dimensional vector o t ∈ R h , where h is the output dimensionality of the GRU.Formally, o t is computed as o t = h N t , with h k t = GRU(h k−1 t , x k t ), for k = 1, . . ., N. In the previous equation, GRU(•) refers to the forward propagation of a gated recurrent unit (Cho et al., 2014).Then, the sequence of previous observations up to o t is encoded in a similar way into a vector s t = GRU(s t−1 , o t ) ∈ R h .We do the same to represent each admissible action a i as
a i = a M i , with a k i = GRU(a k−1 i , c k i ), for k = 1, . . . , M.</p>
<p>Graph-based Knowledge Integration</p>
<p>We enhance our text-based RL agent by allowing it to access a graph that captures both commonsense knowledge and the agent's current belief of the world state.Formally, we assume that, at each time step t, the agent has access to a graph G t = (V t , E t ), where V t is the set of nodes and E t ⊆ V 2 t denotes the edges of the graph.The graph is updated dynamically at each time step t and new nodes are either added or deleted based on the textual observation o t .</p>
<p>As mentioned, G t encodes both commonsense knowledge and the belief of the world state.Commonsense knowledge is extracted from the history of the observations by linking the entities mentioned in the text to an external KG.This allows extracting a commonsense knowledge graph, which is a subgraph of the external source of knowledge providing information about the entities of interest.In our experiments, we use Con-ceptNet (Speer et al., 2017) as the external knowledge graph.On the other hand, the observation o t is also used to update a dynamically generated belief graph as in recent work by Adhikari et al. (2020).The graph aggregation is performed by merging the belief and commonsense knowledge graphs based on the entity mentions.This helps to reduce the noise extracted from updating both the belief and the commonsense graphs.As shown in Figure 2, the commonsense knowledge graph, and the belief graph are updated based on the observations, and then they are aggregated to form a single graph G t .The graph G t at time step t is processed by a graph encoder as follows.First, pretrained KG embeddings are used to map the set of nodes V t into a feature matrix
E t = [e 1 t , . . . , e |V t | t ] ∈ R f ×|V t | ,
where each column e i t ∈ R f is the embedding of node i ∈ V t .We use Numberbatch embeddings (Speer et al., 2017) to create the matrix E t .Such feature matrix provides initial node embeddings that are iteratively updated by message passing between the nodes of G t , using L stacked GCN (graph convolutional network) layers (Kipf and Welling, 2016), where L is an hyperparameter of the model.The output of this process is an updated matrix
Z t = [z 1 t , . . . , z |V | t ] ∈ R h×|V t | .
We then compute a graph encoding g t for G t by simply averaging over the columns of Z t , namely:
g t = 1 |V t | |V t | ∑ i=1 z i t .
In our experiments, we use the updated KG embeddings to create a graph-based encoding vector for each action as described in Section 4.1, in addition to the graph encoding g t .This approach has shown to be a better integration of the knowledge graph at each time step.</p>
<p>Action Prediction</p>
<p>The action ât selected by the agent at time step t is computed based on the state embedding s t , the graph embedding g t and the action candidates a 1 , . . ., a |A t | .First, all these vectors are concatenated together into a single vector r t = [g t ; s t ; a 1 ; . . .; a |A t | ].Then, we compute a vector p t ∈ R |A t | with a probability score for each action a i ∈ A t as:
p t = so f tmax(W 1 • ReLU(W 2 • r t + b 2 ) + b 1 )
where W 1 ,W 2 , b 1 , and b 2 are learnable parameters of the model.The final action chosen by the agent is then given by the one with the maximum probability score, namely ât = arg max i p t,i .</p>
<p>Learning</p>
<p>Following the winning strategy in the First-TextWorld competition (Adolphs and Hofmann, 2019), we use the Advantage Actor-Critic (A2C) framework (Mnih et al., 2016) to train the agent and optimize the action selector on reward signals from training games.</p>
<p>Experiments</p>
<p>In this section, we report on experiments to study the role of commonsense knowledge-based RL agents in the TextWorld environment.We evaluate and compare our agent on two sets of game instances: 1) Kitchen Cleanup Task, and 2) Cooking Recipe Task.</p>
<p>Kitchen Cleanup Task</p>
<p>First, we use TextWorld (Côté et al., 2018) to generate a game/task to assess the performance gain using commonsense knowledge graphs such as ConceptNet.We generate the game with 10 objects relevant to the game, and 5 distractor objects spread across the room.The goal of the agent is to tidy the room (kitchen) by putting the objects in the right place.We create a set of realistic kitchen cleanup goals for the agent: for instance, take apple from the table and put apple inside the refrigerator.Since information on concepts that map to the objects in the room is explicitly provided in ConceptNet (Apple → AtLocation → Refrigerator), the main hypothesis underlying the creation of this game is that leveraging the commonsense knowledge could allow the agent to achieve a higher reward while reducing the number of interactions with the environment.</p>
<p>The agent is presented with the textual description of a kitchen, consisting of the location of different objects in the kitchen and their spatial relationship to the other objects.The agent uses this information to select the next action to perform in the environment.Whenever the agent takes an object and puts it in the target location, it receives a reward and its total score goes up by one point.The maximum score that can be achieved by the agent in this kitchen cleanup task is equal to 10.In addition to the textual description, we extract the commonsense knowledge graph from ConceptNet based on the text description.Figure 3 shows an instance of the commonsense knowledge graph created during the agent's interaction with the environment.Note that even for the simple kitchen cleanup task that we model (see Figure 1 for details), the commonsense knowledge graph contains more than 20 entities (nodes) and a similar number of relations (edges).This visualization is useful, as it lends a basis for our upcoming discussion on agents being overwhelmed with too much commonsense knowledge.</p>
<p>Results on Kitchen Cleanup</p>
<p>We compare our knowledge-aware RL agents (KG Full and KG Evolve) against two baselines for performance comparison: Random, where the agent chooses an action randomly at each step; and Simple, where the agent chooses the next action using the text description alone and ignores the commonsense knowledge graph.The knowledgeaware RL agents, on the other hand, use the commonsense knowledge graph to choose the next action.The graph is provided in either full-graph setting where all the commonsense relationships between the objects are given at the beginning of the game (KG Full); or evolve-graph setting where only the commonsense relationship between the objects seen/interacted by the agent until the current steps are revealed (KG Evolve).We record the average score achieved by each agent and the average number of interactions (moves) with the environment as our evaluation metrics.Figure 4 shows the results for the kitchen cleanup task averaged over 5 runs, with 500 episodes per run.</p>
<p>Discussion of Kitchen Cleanup</p>
<p>As expected, we see that agents that use the textual description and additionally the commonsense knowledge outperform the baseline random agent.We are also able to demonstrate clearly that the knowledge-aware agent outperforms the simple agent with the help of commonsense knowledge.The knowledge-aware agent with the evolve-graph setting outperforms both the simple agent as well as the agent with the full-graph setting.We believe that when an agent has access to the full commonsense knowledge graph at the beginning of the game, the agent gets overwhelmed by the amount of knowledge given; and is prone to making noisy explorations in the environment.On the other hand, feeding the commonsense knowledge gradually during the agent's learning process provides more focus to the exploration, and drives it toward the concepts related to the rest of the goals.These results can also be seen as an RL-centric agent-based validation of similar results shown in the broader NLP literature by the work of (Kapanipathi et al., 2020).</p>
<p>Cooking Recipe Task</p>
<p>Next, we evaluate the performance of our agent on the cooking recipe task by using 20 different games generated by (Adhikari et al., 2020).These games follow a recipe-based cooking theme, with a single ingredient in a single room (difficulty level 1).The goal is to collect that specific ingredient to prepare a meal from a given recipe.As in our previous task, we compare our agent with the Simple agent.In addition to the simple agent, we compare our agent with the GATA agent (Adhikari et al., 2020) which uses the belief graph for effective planning and generalization.As used throughout this paper, the belief graph represents the state of the current game based on the textual description from the environment.Similar to commonsense knowledge, the belief graph can be fed to the agent as a full-graph (GATA Full) or an evolve-graph (GATA Evolve) and then aggregated as the current graph.It is worth noting that the full belief graph is considered as the ground truth state information in the TextWorld environment: it is the graph used by the TextWorld environment internally to modify the state information and the list of admissible actions.On the other hand, the evolve-belief graph is generated based on the observed state information.</p>
<p>Results on Cooking Recipe</p>
<p>We compare both the simple and GATA agents against our agent which uses the commonsense knowledge extracted from ConceptNet.As before, we consider a full-graph setting and evolvegraph setting where either the full commonsense knowledge graph is available at the beginning of the game, or it is fed incrementally as the game proceeds, respectively.For this task, we aggregate the commonsense knowledge graph with the belief graph (Belief+KG Full and Belief+KG Evolve).</p>
<p>Figure 5 shows the results for the Cooking recipe task averaged over 5 runs and 20 games, with 100 episodes per run.As before, all the agents outperform the simple agent, which shows that using different state representations such as the belief graph and additional information such as commonsense knowledge improves the performance of an agent.</p>
<p>Discussion of Cooking Recipe</p>
<p>We observe that the evolve-graph setting for both the GATA and Belief+KG performs better than the Belief+KG Full, as feeding more information can lead to noisy exploration as observed in the earlier task.More interestingly, we observe that GATA Full performs significantly better than the other agents.We believe that the reason for this result is the difficulty of the task at hand, and the process via which these cooking games are generated.Since the cooking recipe task (difficulty level 1) entails retrieving a single ingredient from the same room that the agent is present in, there are no meaningful concepts related to the current state that can be leveraged from the commonsense knowledge for better exploration.Even with the difficult task setting in this game environment (dif-ficulty level 10 with 3 ingredients spread across 6 rooms), the ingredients are randomly chosen and spread across the rooms.In such a game setting, the ground truth full belief graph is more beneficial than the commonsense knowledge graph.This is an interesting negative result, in that it shows that there can still be scenarios and domains where commonsense knowledge may not necessarily help an agent.We are actively exploring further settings of the cooking recipe task in order to understand and frame this effect better.</p>
<p>Conclusion</p>
<p>Previous approaches for text-based games like TextWorld primarily focused on text understanding and reinforcement learning for learning control policies and were thus sample inefficient.In contrast, humans utilize their commonsense knowledge to efficiently act in the world.As a step towards bridging this gap, we investigated the novel problem of using commonsense knowledge to build efficient RL agents for text-based games.</p>
<p>We proposed a technique that symbolically represents the agent's belief of the world, and then combines that belief with commonsense knowledge from the ConceptNet knowledge graph in order to act in the world.We evaluated our approach on multiple tasks and environments and showed that commonsense knowledge can help the agent act efficiently and accurately.We also showcased some interesting negative results with respect to agents being overwhelmed with too much commonsense knowledge.We are currently actively studying this problem, and future work will report in more detail on this phenomenon.</p>
<p>Figure 1 :
1
Figure 1: An illustration of our Kitchen Cleanup game.The agent perceives the world via text and has been given the goal of cleaning up the kitchen.As shown here, the agent can leverage commonsense knowledge from ConceptNet to reduce the exploration and achieve the goal.</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Example of the commonsense knowledge graph extracted from ConceptNet for the Kitchen Cleanup Task</p>
<p>Figure 5 :
5
Figure 5: Comparison of agents for the Cooking Recipe task with belief graph and/or commonsense knowledge graph (averaged over 5 runs).</p>
<p>Observation</p>
<p>You've entered a kitchen.You see a closed cabinet and a refrigerator.Here's a dining table.You see a plate and an apple on the table.</p>
<p>to represent o t as a sequence of d-dimensional vectors x 1 t , . . ., x N t , where each x k t ∈ R d is the glove embedding of the k-th observed token o k t , k = 1, . . ., N. Similarly, given the set A t of admissible actions at time step t, we represent each action a</p>
<p>AcknowledgmentsWe thank Sadhana Kumaravel, Gerald Tesauro, and Murray Campbell for their feedback and help with this work.
Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, William L Hamilton, arXiv:2002.09127Learning dynamic knowledge graphs to generalize on textbased games. 2020arXiv preprint</p>
<p>Ledeepchef: Deep reinforcement learning agent for families of text-based games. Leonard Adolphs, Thomas Hofmann, ArXiv, abs/1909.016462019</p>
<p>Playing text-adventure games with graph-based deep reinforcement learning. Prithviraj Ammanabrolu, Mark Riedl, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>What is the cube root of 27? Question answering over codeontology. Mattia Atzeni, Maurizio Atzori, The Semantic Web -ISWC 2018 -17th International Semantic Web Conference. Lecture Notes in Computer Science. Springer2018</p>
<p>Using frame-based resources for sentiment analysis within the financial domain. Mattia Atzeni, Amna Dridi, Diego Reforgiato Recupero, Progress in AI. 742018</p>
<p>Dbpedia: A nucleus for a web of open data. Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, The semantic web. 2007</p>
<p>Transferring knowledge as heuristics in reinforcement learning: A case-based approach. Reinaldo Ac Bianchi, Luiz A CelibertoJr, Paulo E Santos, Jackson P Matsuura, Ramon Lopez De Mantaras, Artificial Intelligence. 2262015</p>
<p>Freebase: a collaboratively created graph database for structuring human knowledge. Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, Jamie Taylor, Proceedings of the 2008 ACM SIGMOD international conference on Management of data. the 2008 ACM SIGMOD international conference on Management of dataAcM2008</p>
<p>Learning to win by reading manuals in a monte-carlo framework. David Srk Branavan, Regina Silver, Barzilay, Journal of Artificial Intelligence Research. 432012</p>
<p>Learning phrase representations using RNN encoder-decoder for statistical machine translation. Kyunghyun Cho, Bart Van Merrienboer, C ¸aglar Gülc ¸ehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, 10.3115/v1/d14-1179Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarACL2014. 2014. October 25-29, 2014A meeting of SIGDAT, a Special Interest Group of the ACL</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, Adam Trischler, CoRR, abs/1806.115322018</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Workshop on Computer Games. Springer2018</p>
<p>Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning. Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer, Luke Vilnis, Ishan Durugkar, Akshay Krishnamurthy, Alex Smola, Andrew Mccallum, arXiv:1711.058512017arXiv preprint</p>
<p>Building dynamic knowledge graphs from text using machine reading comprehension. Rajarshi Das, Tsendsuren Munkhdalai, Xingdi Yuan, Adam Trischler, Andrew Mccallum, CoRR, abs/1810.056822018</p>
<p>Marta Garnelo, Kai Arulkumaran, Murray Shanahan, arXiv:1609.05518Towards deep symbolic reinforcement learning. 2016arXiv preprint</p>
<p>Kore: keyphrase overlap relatedness for entity disambiguation. Johannes Hoffart, Stephan Seufert, Dat Ba Nguyen, Martin Theobald, Gerhard Weikum, Proceedings of the 21st ACM international conference on Information and knowledge management. the 21st ACM international conference on Information and knowledge management2012</p>
<p>Planning and acting in partially observable stochastic domains. Leslie Pack, Kaelbling Michael L Littman, Anthony R Cassandra, Artificial intelligence. 1011-21998</p>
<p>Infusing knowledge into the textual entailment task using graph convolutional networks. Pavan Kapanipathi, Veronika Thost, Sankalp Siva, Spencer Patel, Ibrahim Whitehead, Avinash Abdelaziz, Maria Balakrishnan, Kshitij Chang, Chulaka Fadnis, Bassem Gunasekara, Nicholas Makni, Mattei, 2020AAAIKartik Talamadupula, and Achille Fokoue</p>
<p>Semisupervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016arXiv preprint</p>
<p>Evolving largescale neural networks for vision-based reinforcement learning. Jan Koutník, Giuseppe Cuccu, Jürgen Schmidhuber, Faustino Gomez, Proceedings of the 15th annual conference on Genetic and evolutionary computation. the 15th annual conference on Genetic and evolutionary computation2013</p>
<p>Self-improving reactive agents based on reinforcement learning, planning and teaching. Long-Ji Lin, Machine learning. 83-41992</p>
<p>Reinforcement learning for robots using neural networks. Long-Ji Lin, 1993Carnegie-Mellon Univ Pittsburgh PA School of Computer ScienceTechnical report</p>
<p>Conceptnet-a practical commonsense reasoning tool-kit. Hugo Liu, Push Singh, BT technology journal. 2242004</p>
<p>Wordnet: a lexical database for english. George A Miller, Communications of the ACM. 38111995</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. 2016</p>
<p>Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, arXiv:1506.08941Language understanding for textbased games using deep reinforcement learning. 2015arXiv preprint</p>
<p>Glove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, 10.3115/v1/d14-1162Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. the 2014 Conference on Empirical Methods in Natural Language ProcessingDoha, QatarACL2014. 2014. October 25-29, 2014A meeting of SIGDAT, a Special Interest Group of the ACL</p>
<p>Sentilo: Frame-based sentiment analysis. Diego Reforgiato Recupero, Valentina Presutti, Sergio Consoli, Aldo Gangemi, Andrea Giovanni Nuzzolese, 10.1007/s12559-014-9302-zCognitive Computation. 722015</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robert Speer, Joshua Chin, Catherine Havasi, AAAI. 2017</p>
<p>Open domain question answering using early fusion of knowledge bases and text. Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn Mazaitis, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018Ruslan Salakhutdinov, and William Cohen</p>
<p>Complex embeddings for simple link prediction. Théo Trouillon, Johannes Welbl, Sebastian Riedel, Éric Gaussier, Guillaume Bouchard, International Conference on Machine Learning. 2016</p>
<p>Improving natural language inference using external knowledge in the science questions domain. Xiaoyan Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu, Kartik Talamadupula, Ibrahim Abdelaziz, Maria Chang, Achille Fokoue, Bassem Makni, Nicholas Mattei, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Knowledge graph embedding by translating on hyperplanes. Zhen Wang, Jianwen Zhang, Jianlin Feng, Zheng Chen, AAAI. 2014</p>
<p>Ziyu Wang, Victor Bapst, Nicolas Heess, Volodymyr Mnih, Remi Munos, Koray Kavukcuoglu, Nando De Freitas, arXiv:1611.01224Sample efficient actorcritic with experience replay. 2016arXiv preprint</p>
<p>Learn what not to learn: Action elimination with deep reinforcement learning. Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, Shie Mannor, Advances in Neural Information Processing Systems. 2018</p>            </div>
        </div>

    </div>
</body>
</html>
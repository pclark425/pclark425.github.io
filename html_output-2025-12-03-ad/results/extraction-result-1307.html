<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1307 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1307</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1307</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-254484530</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.06092v1.pdf" target="_blank">Sim-to-Real Transfer for Visual Reinforcement Learning of Deformable Object Manipulation for Robot-Assisted Surgery</a></p>
                <p><strong>Paper Abstract:</strong> Automation holds the potential to assist surgeons in robotic interventions, shifting their mental work load from visuomotor control to high level decision making. Reinforcement learning has shown promising results in learning complex visuomotor policies, especially in simulation environments where many samples can be collected at low cost. A core challenge is learning policies in simulation that can be deployed in the real world, thereby overcoming the sim-to-real gap. In this letter, we bridge the visual sim-to-real gap with an image-based reinforcement learning pipeline based on pixel-level domain adaptation and demonstrate its effectiveness on an image-based task in deformable object manipulation. We choose a tissue retraction task because of its importance in clinical reality of precise cancer surgery. After training in simulation on domain-translated images, our policy requires no retraining to perform tissue retraction with a 50% success rate on the real robotic system using raw RGB images. Furthermore, our sim-to-real transfer method makes no assumptions on the task itself and requires no paired images. This letter introduces the first successful application of visual sim-to-real transfer for robotic manipulation of deformable objects in the surgical field, which represents a notable step towards the clinical translation of cognitive surgical robotics.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1307.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1307.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SOFA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation Open Framework Architecture (SOFA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A finite-element-based soft-body simulation framework used to model deformable tissues and interactive surgical scenes; used here to train visuomotor RL policies for a tissue retraction task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SOFA: A multi-model framework for interactive physical simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>SOFA (Simulation Open Framework Architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A multi-model interactive physical simulation framework using finite element methods to simulate soft-body mechanics; in this work it models a rectangular soft tissue as a tetrahedral mesh, attachment constraints, and a simplified representation of the da Vinci Research Kit (dVRK) gripper and instrument shaft within a constrained workspace.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics (soft-body biomechanics / surgical robotics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity FEM-based soft-body simulator — realistic mesh-based deformation modeling for tissue but simplified/approximate modeling for robot dynamics and collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Uses tetrahedral mesh finite-element modeling of soft tissue with fixed attachment points; models the distal part of the dVRK PSM including gripper and instrument shaft with a fixed remote center-of-motion (RCM); constrained workspace; static calibrated camera viewpoint producing 256×256 RGB images; simplified collision detection based on a bounding box around the tissue in its non-deformed state (not full contact mechanics); does not model cable-driven actuator dynamics of the real PSM or sensor/measurement noise; collision model is more permissive than reality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>π_g and π_s (PPO-trained visuomotor policies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agents trained with Proximal Policy Optimization (Stable Baselines3). Policy and value networks are separate convolutional neural networks (three conv layers with kernels 8/4/3 and strides 4/2/1, followed by FC-512); policy outputs Gaussian means for 3 task-space velocities and has learnable log std parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Tissue Retraction — a two-phase deformable-object manipulation task: grasp a point on a soft tissue and retract it to expose a visual target (long-horizon visuomotor control on deformable tissue).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Policy learned to grasp after ~200,000 environment steps; full tissue-retraction behavior learned by ~2,000,000 steps; both π_s and π_g achieved 100% success in simulation by end of training.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world dVRK robotic system (da Vinci Research Kit) with Intel RealSense D435 camera (real robotic setup); also evaluated in a 'digital twin' scenario where simulation observations are generated with real robot states.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Multiple evaluation scenarios: (1) π_s(Os) (digital-twin loop): 32/32 success (100%); (2) π_g(G(Os)) (policy trained on translated images, executed on translated-simulation observations with digital twin): 25/32 (78.1%); (3) π_g(Or) (policy trained on translated sim images, executed on raw real camera images): 16/32 (50%); (4) π_g(G(Or)) (real images passed through translation model G then to policy): 13/32 (≈40.6%). The paper highlights a 50% success rate on the real robotic system for π_g when receiving raw RGB images (π_g(Or)).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper notes that certain simulator simplifications did not preclude transfer (π_s succeeded despite the simulator not modeling measurement noise or cable-driven actuator dynamics), but it emphasizes critical simulator requirements: precise calibration of camera perspective between sim and real is essential, and collision/contact fidelity matters because a permissive collision model in simulation caused excessive collisions in reality. The authors conclude that accurate appearance (for DA) and calibrated viewpoint are necessary, whereas some dynamic details (e.g., cable-driven mechanism modeling) were not required for success in their evaluated scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported failures include: policies trained on translated images (π_g) were less robust to differences in real-world dynamics and safety constraints than π_s; applying the image-translation model to real images (π_g(G(Or))) reduced performance, indicating the translator can remove information needed to compensate for novel dynamics; simplified collision modeling in simulation led to excessive tissue stress / collisions in reality; single-GAN image translation performed worse than an ensemble of GANs (ensemble improved transfer), and the image translation models required precise camera calibration and unpaired image data capture.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Sim-to-Real Transfer for Visual Reinforcement Learning of Deformable Object Manipulation for Robot-Assisted Surgery', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SOFA: A multi-model framework for interactive physical simulation. <em>(Rating: 2)</em></li>
                <li>Sim-to-Real Reinforcement Learning for Deformable Object Manipulation. <em>(Rating: 2)</em></li>
                <li>Sim-to-real transfer of robotic control with dynamics randomization. <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world. <em>(Rating: 2)</em></li>
                <li>Sim-to-Real via Sim-to-Sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. <em>(Rating: 1)</em></li>
                <li>Retinagan: An object-aware approach to sim-to-real transfer. <em>(Rating: 1)</em></li>
                <li>Rlcyclegan: Reinforcement learning aware simulation-to-real. <em>(Rating: 1)</em></li>
                <li>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1307",
    "paper_id": "paper-254484530",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "SOFA",
            "name_full": "Simulation Open Framework Architecture (SOFA)",
            "brief_description": "A finite-element-based soft-body simulation framework used to model deformable tissues and interactive surgical scenes; used here to train visuomotor RL policies for a tissue retraction task.",
            "citation_title": "SOFA: A multi-model framework for interactive physical simulation.",
            "mention_or_use": "use",
            "simulator_name": "SOFA (Simulation Open Framework Architecture)",
            "simulator_description": "A multi-model interactive physical simulation framework using finite element methods to simulate soft-body mechanics; in this work it models a rectangular soft tissue as a tetrahedral mesh, attachment constraints, and a simplified representation of the da Vinci Research Kit (dVRK) gripper and instrument shaft within a constrained workspace.",
            "scientific_domain": "mechanics (soft-body biomechanics / surgical robotics)",
            "fidelity_level": "medium-fidelity FEM-based soft-body simulator — realistic mesh-based deformation modeling for tissue but simplified/approximate modeling for robot dynamics and collisions.",
            "fidelity_characteristics": "Uses tetrahedral mesh finite-element modeling of soft tissue with fixed attachment points; models the distal part of the dVRK PSM including gripper and instrument shaft with a fixed remote center-of-motion (RCM); constrained workspace; static calibrated camera viewpoint producing 256×256 RGB images; simplified collision detection based on a bounding box around the tissue in its non-deformed state (not full contact mechanics); does not model cable-driven actuator dynamics of the real PSM or sensor/measurement noise; collision model is more permissive than reality.",
            "model_or_agent_name": "π_g and π_s (PPO-trained visuomotor policies)",
            "model_description": "Reinforcement learning agents trained with Proximal Policy Optimization (Stable Baselines3). Policy and value networks are separate convolutional neural networks (three conv layers with kernels 8/4/3 and strides 4/2/1, followed by FC-512); policy outputs Gaussian means for 3 task-space velocities and has learnable log std parameters.",
            "reasoning_task": "Tissue Retraction — a two-phase deformable-object manipulation task: grasp a point on a soft tissue and retract it to expose a visual target (long-horizon visuomotor control on deformable tissue).",
            "training_performance": "Policy learned to grasp after ~200,000 environment steps; full tissue-retraction behavior learned by ~2,000,000 steps; both π_s and π_g achieved 100% success in simulation by end of training.",
            "transfer_target": "Real-world dVRK robotic system (da Vinci Research Kit) with Intel RealSense D435 camera (real robotic setup); also evaluated in a 'digital twin' scenario where simulation observations are generated with real robot states.",
            "transfer_performance": "Multiple evaluation scenarios: (1) π_s(Os) (digital-twin loop): 32/32 success (100%); (2) π_g(G(Os)) (policy trained on translated images, executed on translated-simulation observations with digital twin): 25/32 (78.1%); (3) π_g(Or) (policy trained on translated sim images, executed on raw real camera images): 16/32 (50%); (4) π_g(G(Or)) (real images passed through translation model G then to policy): 13/32 (≈40.6%). The paper highlights a 50% success rate on the real robotic system for π_g when receiving raw RGB images (π_g(Or)).",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "minimal_fidelity_discussion": "The paper notes that certain simulator simplifications did not preclude transfer (π_s succeeded despite the simulator not modeling measurement noise or cable-driven actuator dynamics), but it emphasizes critical simulator requirements: precise calibration of camera perspective between sim and real is essential, and collision/contact fidelity matters because a permissive collision model in simulation caused excessive collisions in reality. The authors conclude that accurate appearance (for DA) and calibrated viewpoint are necessary, whereas some dynamic details (e.g., cable-driven mechanism modeling) were not required for success in their evaluated scenarios.",
            "failure_cases": "Reported failures include: policies trained on translated images (π_g) were less robust to differences in real-world dynamics and safety constraints than π_s; applying the image-translation model to real images (π_g(G(Or))) reduced performance, indicating the translator can remove information needed to compensate for novel dynamics; simplified collision modeling in simulation led to excessive tissue stress / collisions in reality; single-GAN image translation performed worse than an ensemble of GANs (ensemble improved transfer), and the image translation models required precise camera calibration and unpaired image data capture.",
            "uuid": "e1307.0",
            "source_info": {
                "paper_title": "Sim-to-Real Transfer for Visual Reinforcement Learning of Deformable Object Manipulation for Robot-Assisted Surgery",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SOFA: A multi-model framework for interactive physical simulation.",
            "rating": 2,
            "sanitized_title": "sofa_a_multimodel_framework_for_interactive_physical_simulation"
        },
        {
            "paper_title": "Sim-to-Real Reinforcement Learning for Deformable Object Manipulation.",
            "rating": 2,
            "sanitized_title": "simtoreal_reinforcement_learning_for_deformable_object_manipulation"
        },
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization.",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world.",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "Sim-to-Real via Sim-to-Sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks.",
            "rating": 1,
            "sanitized_title": "simtoreal_via_simtosim_dataefficient_robotic_grasping_via_randomizedtocanonical_adaptation_networks"
        },
        {
            "paper_title": "Retinagan: An object-aware approach to sim-to-real transfer.",
            "rating": 1,
            "sanitized_title": "retinagan_an_objectaware_approach_to_simtoreal_transfer"
        },
        {
            "paper_title": "Rlcyclegan: Reinforcement learning aware simulation-to-real.",
            "rating": 1,
            "sanitized_title": "rlcyclegan_reinforcement_learning_aware_simulationtoreal"
        },
        {
            "paper_title": "Using simulation and domain adaptation to improve efficiency of deep robotic grasping.",
            "rating": 1,
            "sanitized_title": "using_simulation_and_domain_adaptation_to_improve_efficiency_of_deep_robotic_grasping"
        }
    ],
    "cost": 0.010964999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-To-Real Transfer for Visual Reinforcement Learning of Deformable Object Manipulation for Robot-Assisted Surgery
10 Jun 2024</p>
<p>Paul Maria Scheikl 
P. M. Scheikl
F. Mathis-Ullrich are with the Institute for Anthropomatics and Robotics
Karlsruhe Institute of Technology
B. Gyenes76131KarlsruheGermany</p>
<p>Eleonora Tagliabue 
Balázs Gyenes 
P. M. Scheikl
F. Mathis-Ullrich are with the Institute for Anthropomatics and Robotics
Karlsruhe Institute of Technology
B. Gyenes76131KarlsruheGermany</p>
<p>Martin Wagner 
Diego Dall'alba 
Paolo Fiorini 
Franziska Mathis-Ullrich franziska.ullrich@kit.edu 
P. M. Scheikl
F. Mathis-Ullrich are with the Institute for Anthropomatics and Robotics
Karlsruhe Institute of Technology
B. Gyenes76131KarlsruheGermany</p>
<p>Department of Computer Science
University of Verona
37134VeronaItaly</p>
<p>Department for General
Visceral and Transplan-tation Surgery
Heidelberg University Hospital
69120HeidelbergGermany</p>
<p>Sim-To-Real Transfer for Visual Reinforcement Learning of Deformable Object Manipulation for Robot-Assisted Surgery
10 Jun 2024E20BBF05A75C401CAF9BADB68537C424arXiv:2406.06092v1[cs.RO]Manuscript received: August 10, 2022; Revised November 16, 2022; Accepted November 27, 2022.Surgical Robotics: LaparoscopyReinforcement LearningComputer Vision for Medical Robotics
Automation holds the potential to assist surgeons in robotic interventions, shifting their mental work load from visuomotor control to high level decision making.Reinforcement learning has shown promising results in learning complex visuomotor policies, especially in simulation environments where many samples can be collected at low cost.A core challenge is learning policies in simulation that can be deployed in the real world, thereby overcoming the sim-to-real gap.In this work, we bridge the visual sim-to-real gap with an image-based reinforcement learning pipeline based on pixellevel domain adaptation and demonstrate its effectiveness on an image-based task in deformable object manipulation.We choose a tissue retraction task because of its importance in clinical reality of precise cancer surgery.After training in simulation on domain-translated images, our policy requires no retraining to perform tissue retraction with a 50% success rate on the real robotic system using raw RGB images.Furthermore, our simto-real transfer method makes no assumptions on the task itself and requires no paired images.This work introduces the first successful application of visual sim-to-real transfer for robotic manipulation of deformable objects in the surgical field, which represents a notable step towards the clinical translation of cognitive surgical robotics.</p>
<p>I. INTRODUCTION</p>
<p>L EARNING behaviors in simulation and transferring them to real robotic systems (sim-to-real) is a prominent topic of research in robot-assisted surgery since learning on a real surgical robotic system is often infeasible [1]- [4].Training in simulation enables end-to-end Reinforcement Learning (RL) of complex tasks in a safe and controlled environment, without requiring direct access to the real surgical robotic system.Training on real surgical robotic systems is impractical as RL algorithms often require millions of environment interactions to train a policy and unsafe behavior during training may damage the system [3].</p>
<p>Existing works on sim-to-real policy transfer for robotassisted surgery utilize low-dimensional observations such as the state of the robot and known goal positions, so that the inputs to the policy are the same during training in simulation and execution on the real robotic system [1], [2], [5].The success of state-based policies heavily relies on accurate extraction of task-relevant information from the scene.This makes it highly challenging to exploit state-based methods in surgical applications involving the manipulation of deformable tissues.The large configuration space of deformable objects cannot be fully extracted from data provided by standard surgical sensors (e.g.endoscopic camera) and is difficult to describe in a compact state.Image-based approaches, on the other hand, can learn task relevant features directly from sensor data.In this way, they are able to infer relevant information that is otherwise inaccessible to state-based approaches.However, image-based approaches have never been demonstrated in robot-assisted surgery due to the difficulty of transferring learned policies to real systems across the large visual domain gap between simulated and real images [5], [6].</p>
<p>Several methodologies for transferring image-based policies across the visual domain gap between simulation and reality are currently investigated.Domain Randomization (DR) addresses the visual sim-to-real gap by randomly augmenting visual parameters of the simulation, e.g.texture and lighting, such that the trained policy learns to extract generalized, task-relevant visual features.DR approaches have been shown to translate well into reality [7]- [9] but they can be highly task specific and hard to tune [10], [11].In contrast, pixel-level Domain Adaptation (DA) addresses the visual sim-to-real gap by directly transforming the images from one domain into the other.Recent related works employ Generative Adversarial Networks (GANs) to transform images for robotic grasping tasks [12]- [15].These works share the limitation that large amounts of real world data are required to train the GANs.Furthermore, application-specific auxiliary tasks must be defined in order to stabilize GAN training to avoid mode collapse and hallucinated objects in the translated images [13].This restricts their use to scenarios where these auxiliary tasks are applicable and may require additional data generation.Ho et al. [12] collect data from 135 000 task executions for GAN training.As an auxiliary task, they enforce predicting consistent bounding boxes by an object detection model for original and translated images.James et al. [14] invert the problem by training a GAN to translate images from domain-randomized simulations into a canonical simulation and show that the GAN is also able to translate images from reality into the canonical simulation.Most of these works employ CycleGANs [16] as their DA model.As an alternative to CycleGAN, CUT [17] and DCL [18] maximize mutual information between patches of the original and translated image through a contrastive learning approach.In robotics research, CUT is employed to generate synthetically labelled data for computer vision tasks such as semantic segmentation [19], [20] but has not been investigated for DA in RL.</p>
<p>In this work, we propose a pipeline for image-based RL of a surgical robotic task.Sim-to-real transfer is achieved through pixel-level DA using a contrastive GAN.</p>
<p>The contributions of this work are two-fold: 1) In the field of surgical robotics, we present the first successful sim-to-real transfer of an image-based RL policy to a real surgical robotic system.A visuomotor policy is trained in simulation and evaluated in reality without retraining.The approach is shown for Tissue Retraction (TR) (see Fig. 1), a common long-horizon task in deformable object manipulation.By relying on image inputs, the presented approach may be applied to other image-based tasks in robotic control, without the need of designing hand-crafted task-specific policies and is thus not limited to TR. the contrastive loss is independent of the RL task and can thus be considered a task-agnostic approach.This is the first successful application of pixel-level DA in RL for deformable object manipulation.</p>
<p>II. METHODS</p>
<p>The goal of this work is to train a visuomotor policy in a simulation of robot-assisted surgery on visual observations, such that the policy can be deployed on the real system without retraining.An overview of training and evaluation settings is given in Fig. 2.During policy learning, images from simulation o s are translated into the image domain of the real system o r before being passed to the policy π that generates actions a which are applied to the simulation.The required translation model G is learned from unpaired examples of both the real and simulation domain and frozen during policy learning.Compared to related work, this presents a more general approach for sim-to-real transfer of visuomotor policies in deformable object manipulation as it does not require application-specific auxiliary tasks that stabilize training at the cost of restricting their transferability to other tasks.</p>
<p>A. Domain Adaptation</p>
<p>A policy π s trained on simulated images is unsuitable for deployment in the real world because of the visual domain gap between observations in simulation and reality, even though the underlying dynamics and reward structure are similar.Sequential decision making problems are frequently formalized as Markov Decision Processs (MDPs).MDPs can be generalized to Partially Observable Markov Decision Processs (POMDPs) [21] that assume that the dynamics of the process are determined by an MDP, while the true state of the process is hidden and cannot directly be observed.As the true state of the deformable object is not accessible, and we can only observe the scene through image observations, we formally interpret the TR tasks in reality and simulation as two POMDPs that differ in their transition T (physical domain gap) and observation O (visual domain gap) functions.Hidden states of the processes are assumed to be the same in simulation and reality.</p>
<p>The goal of pixel-level DA is bridging the visual domain gap by changing the appearance of an image while task-relevant information is preserved.G is trained on unpaired image data collected in simulation and on the real system such that different lighting conditions are represented in the dataset, and then frozen for policy training.A policy π g is then trained in simulation on translated observations ôr .Thus, the policy π g can be directly deployed on raw images o r from the real camera during execution on the real robotic system.In order to encourage retaining features of real images, G also learns an identity mapping of real images by minimizing the distance between real images o r and their translation ô′ r = G(o r ).In contrast to previous work, this approach requires knowledge about the Cartesian positions of the robot's gripper, grasping point, and end point only during training in simulation and not during execution on the real system.</p>
<p>Here, we investigate two methods for training contrastive UI2I models, CUT [17] and DCL [18], with CycleGAN [16] as a baseline.Contrastive learning learns to associate a query to a positive example and to contrast the query to negative examples.In the case of CUT and DCL, the query is a patch [17] of the translated image, the positive is the patch at the same location in the original image, and the negatives are patches from the original image at other locations.Image fidelity is compared manually, with only the best method used for policy learning as translation model G.During policy learning, the instance of G used at each time step is sampled uniformly from an ensemble of 7 models from different training runs to compensate for possible bias of the individual GAN instances toward a specific lighting condition in the dataset.Intuitively, this is a form of visual domain randomization since the appearance of the translated images is slightly different for each specific instance of G but the content of the images is the same.</p>
<p>B. Reinforcement Learning</p>
<p>1) Tissue Retraction Task: TR is an elementary surgical task that consists of grasping and pulling deformable tissue in order to expose a target area to the endoscopic camera.Furthermore, it is important to provide an appropriate force to tension the tissue for dissection without ripping tissue apart.TR is thus of utmost importance, especially in cancer surgery, in order to dissect cancerous lymphatic tissue off of major blood vessels.Due to its ubiquity in surgical procedures, autonomous execution of TR has previously been investigated in the literature [2], [3], [22], [23].In this work, we implement TR of a rectangular soft tissue [24].The position of the target, as well as the attachment points to the surrounding environment, are assumed to be known from pre-operative data to limit complexity, even if in reality they will change during surgical dissection.The task is executed on the da Vinci Research Kit (dVRK) using a single Patient Side Manipulator (PSM), as illustrated in Fig. 3.</p>
<p>2) Learning Environment: The TR task is implemented in the Simulation Open Framework Architecture (SOFA) [25] framework (see Fig. 3 (a)), relying on a finite element method.The tissue is modeled as a mesh of tetrahedral elements with its top right part fixed to simulate the attachment to the board in the real environment.The robot is modeled as the distal part of a dVRK PSM including the gripper and instrument shaft respecting a fixed remote center of motion.Motion of the robot is constrained to a 83 mm high, 180 mm deep, and 140 mm wide workspace box above the tissue similar to the typical operative space of the PSM.The gripper starting position in each episode is uniformly sampled from the workspace with a minimum height of 40 mm.</p>
<p>The TR task is divided into grasping and retracting phases.The positions of the grasping and end points, as well as the target, are fixed for training and evaluation.The grasping point was selected above the target area, while the end point was chosen to achieve good visibility of the target area in the image observations, similar to previous work [2].</p>
<p>In the grasping phase, the gripper can move freely in the workspace.If the distance between the gripper and the desired grasping point decreases below 3 mm and the gripper is below the tissue surface, the tissue is automatically grasped and the retraction phase is entered.In the retraction phase, the tissue is attached to the gripper.The episode is completed successfully when the distance between the gripper and the desired end point reduces to 3 mm or less.Each episode is limited to 1 000 steps before the environment is automatically reset.Collisions are detected between the gripper jaw tips and the tissue, based on whether the gripper is within a bounding box around the tissue in its non-deformed state.</p>
<p>3) Reward, Observation and Action Space: The simulation environment adheres to the OpenAI gym standard [26].The policy outputs the parameters of a Gaussian distribution, from which actions consisting of task space velocities are sampled.The continuous action is clipped to the interval [−1, 1], scaled such that the limits of the action space correspond to a maximum robot velocity of 3 mm s −1 in each direction, and an action repeat of 3 is applied.The environment generates a 256×256 RGB image from a static camera perspective, which is then translated by the image-to-image translation model G.The four most recent images are concatenated and scaled down to a resolution of 84 × 84, resulting in observations of size 84 × 84 × 12 that are subsequently passed to the policy.Observation space and network architecture are based on [27].</p>
<p>The reward function is split into grasping r g (Eq.4) and retraction r r (Eq.5) phases to match the two phases of the TR task.In the grasping phase, the agent receives a negative reward proportional to the distance between the current gripper position x t and the grasping point x g , normalized by the absolute size of the workspace:
d g = − ||x g − x t || 2 ||x max − x min || 2(1)
where x max and x min are the corner points of the workspace bounding box.It further receives a constant negative reward equivalent to the reward seen at the beginning of the retraction phase based on grasping point x g and end point x e :
c g = −α * ||x g − x e || 2 ||x max − x min || 2 .(2)
A weight of α = 1.2 ensures that the agent is incentivized to transition to the retraction phase.In order to enforce safe policy behaviors on the real robot, an additional term p c penalizes collisions between gripper and tissue proportional to the distance to the grasping point, and a further term p w penalizes actions that would violate the workspace boundaries.Finally, the reward function assigns a one-time positive reward of e g = 1 to a successful grasp.In the retraction phase, the agent receives a negative reward d e proportional to its distance to the end point, normalized by the absolute size of the workspace:
d e = − ||x e − x t || 2 ||x max − x min || 2 .(3)
Additionally, a one-time positive reward of e r = 1 is awarded when the episode is successfully completed.Thus, the overall reward function for grasping and retraction is
r g = d g + c g + p c + p w + e g ,(4)
r r = d e + e r .</p>
<p>(5)</p>
<p>4) Proximal Policy Optimization: Stable Baselines 3 [28] and its implementation of Proximal Policy Optimization (PPO) [29] are utilized to train the agent.The agent is trained with a discount of γ = 0.995 and λ GAE = 0.95 for a total of 10 7 environment steps over 8 parallel environments.The policy is updated after every 8 × 128 environment steps with a minibatch size of 256 and 4 epochs per PPO iteration.Learning rate and clip ratio follow a linearly decreasing schedule starting at 0.1 and 2.5 • 10 −4 , respectively.PPO's ratio clip is set to 0.2, the value and entropy loss coefficients are 0.5 and 0.001, respectively, and gradients are clipped to a maximum norm of 0.5.</p>
<p>5) Agent Architecture:</p>
<p>The agent is split into policy and value estimation networks that do not share learnable parameters but are similar in architecture.Both networks consist of three convolutional layers with square kernel sizes 8, 4, 3 and strides of 4, 2, 1, followed by a fully connected layer with 512 neurons.The policy network has a head with 3 neurons for predicting the mean of a Gaussian distribution corresponding to the task-space velocities, and the value network has a head with 1 neuron.The policy network also contains 3 learnable log standard deviation parameters that do not depend on the input.ReLU non-linearities are applied after all layers except the final layer.</p>
<p>6) Curriculum Learning: Curriculum learning [30] is an approach to gradually increase the difficulty of a learning environment in order to simplify learning complex tasks.This work employs curriculum learning to tune the weight of the collision and workspace violation terms of the reward function during grasping r g (Eq.4).The curriculum of the terms p c and p w of r g are defined as
p c = −w c ||x g − x t || 2 ||x max − x min || 2 , if in collision(6)
p w = −w w , if action violates workspace (7) with factors w c and w w linearly increasing from 0.0 to 10 and 0.2, respectively, over 10 7 environment steps.</p>
<p>III. EXPERIMENTAL EVALUATION</p>
<p>The experimental setup consists of tissue represented by a rectangular silicone sheet attached to a board at a fixed set of attachment points, following the approach in [24].The target is represented by a red circular marker on the board.The PSM on the dVRK is equipped with a ProGrasp instrument, as illustrated in Fig. 3.An Intel RealSense D435 camera is used to capture monocular RGB images constituting the visual observations.</p>
<p>A. Unpaired Image-To-Image Translation</p>
<p>The UI2I models CUT, DCL, and CycleGAN are each trained with 7 random seeds.Each training run is executed for 48 hours on four NVIDIA A100-40 GPUs with a minibatch size of 4. The unpaired image dataset is created by performing grasping and retracting with different starting, grasping, and end points 82 times under 3 uncontrolled lighting conditions, for a total of 246 TR executions.RGB image observations are captured at 30 Hz. Data is captured on the real system and in simulation with a resolution of 256 × 256.This dataset is split randomly into 90% for training and 5% each for validation and testing.</p>
<p>B. Sim-To-Real Evaluation Scenarios</p>
<p>The experimental goal is to evaluate how well a policy that is trained on translated images in simulation performs on the real robotic setup.This is done without manually accounting for physical inaccuracies, such as modelling the robot dynamics, or environment conditions, such as changes in lighting, posing a more realistic and challenging task.
π s π g Reality Simulation G o s o s o r o r a a a a G(o r ) G(o s ) z z π s (O s ) π g (G(O s )) π g (O r ) π g (G(O r ))
Fig. 4: Illustrated data flow for the four sim-to-real evaluation scenarios.All policy actions a are executed on the real robot.For scenarios πs(Os) and πg(G(Os)) the simulation is updated with the real robot states s to generate observations os from simulation.Scenarios πg(Or) and πg(G(Or)) receive observations or from the real system.</p>
<p>The proposed approach is evaluated in four different scenarios as illustrated in Fig. 4. The first scenario, denoted π s (O s ), evaluates the physical domain gap between simulation and reality by conditioning on observations o s from the simulation under state transitions T r from the real system.Policy π s is trained in simulation without image translation and then executed on the real system in a digital twin approach, where the gripper position in simulation is set to match the gripper position on the real system.The subscript s indicates that this policy is conditioned on observations o s from simulation.The policy receives observations from simulation but predicts actions that are applied to the real system instead of the simulation.The second scenario, denoted π g (G(O s )), follows the same approach, but additionally translates simulation images during training and execution.Policy π g is trained on translated images ôr = G(o s ) in simulation as indicated by the subscript g.The third scenario, denoted π g (O r ), is the target scenario of the sim-to-real transfer.The same policy π g receives real images o r during execution.The fourth scenario, denoted π g (G(O r )), evaluates whether the image translation model can mitigate the influence of changes in lighting conditions on the real system.Images o r from the real system are translated by G before being passed to the policy, exploiting the learned identity mapping of real images as described in Section II-A.</p>
<p>Each scenario is executed from 32 different starting positions.The starting positions are determined by dividing the robot's planar workspace into a 4 × 4 grid to generate 16 starting positions on the XY-plane.Each starting position is executed on two starting heights (z = 60 mm and z = 70 mm) to sample different heights within the allowed workspace.</p>
<p>C. Evaluation Metrics</p>
<p>Policy performance is evaluated based on trajectory outcome and quality metrics.This work considers four different trajectory outcomes: (1) success if the policy completes both grasping and retracting phases, exposing the visual target as shown in Fig. 3; (2) partial success if the policy completes the grasping phase but does not expose the visual target; (3) tissue stress, where execution is aborted prematurely if the gripper applies excessive stress on the tissue through collisions before grasping; and (4) time out if the policy fails to reach Steps episode steps in collision steps in workspace violation episode return Fig. 5: Smoothed learning curves for a training run of πg.Average episode return, episode length, and steps in collision and workspace violation are shown over total steps in the learning environment.Three phases are identifiable during learning: when the agent is predominantly learning to grasp (purple), when it is learning to retract (blue), and when it is mainly optimizing to reduce episode length and collisions (green).</p>
<p>the grasping point within the time limit of 1 000 steps.The termination criterion of outcome ( 3) is triggered if the gripper moves more than 8 mm laterally in collision.The evaluated trajectory quality metrics are the number of steps in collision and the absolute path length.</p>
<p>IV. RESULTS</p>
<p>A. Unpaired Image-To-Image Translation</p>
<p>The unpaired image dataset contains 278 735 images from the real system collected over 2.58 hours and 108 994 images generated in simulation.Figure 6 (a) illustrates that both CUT and DCL successfully learn the image translation task, while CycleGAN does not, producing inconsistent visual features and spurious features.On visual assessment, however, DCL produces more consistent results for images from early steps in the retraction phase and is thus used as the image translation model G for RL. Figure 6 (b) illustrates a case where DCL successfully translates the simulation image, but CUT fails to correctly change the appearance of the gripper for the closed grasp.The manual inspection of 600 randomly chosen images showed that 60% for CycleGAN, 30.5% for CUT, and 6.5% for DCL of the translated images showed inconsistent or spurious visual features.</p>
<p>B. Training in Simulation</p>
<p>Learning TR on translated images in simulation requires roughly 2 million environment steps as illustrated in Fig. 5.The policy learns to grasp successfully after roughly 200 000 environment steps, much faster than learning the retraction part of the task.The impact of curriculum learning can be seen in the number of environment steps spent in collision and workspace violation.The task is learned quickly, yet with many safety violations.After learning the task and progressively increasing the importance of safe actions, the unsafe parts of the trajectories decrease while task success continues to improve.The learning can be roughly separated into three phases as indicated in Fig. 5: the agent learning to grasp the tissue, the agent learning to retract the tissue, and the agent optimizing its learned behaviour to reduce episode length, collisions, and workspace violations.Both policies π s</p>
<p>C. Sim-To-Real Evaluation</p>
<p>The trajectory outcomes and quality metrics as described in Section III-C are presented in Fig. 7 and Tab.I. Scenario π s (O s ), where the baseline policy π s is executed in simulation with the digital twin in the loop, was successful from all starting positions.Obtained results for scenario π g (G(O s )), where policy π g is executed on translated images with the digital twin in the loop, include three trajectories that were aborted due to excessive collisions, and four timed out without solving the grasping task.When the same policy π g receives real images for scenario π g (O r ), the observed success rate was further reduced to 16/32.In contrast to the two previous experiments, three trajectories solved the grasping task, but were not successful in exposing the visual target.The final scenario π g (G(O r )), with policy π g executed on real images passed through the image translation model G, reaches a success rate of 13/32.The amount of steps in collision increased over all four scenarios leading to an increase in the number of trajectories terminated due to excessive tissue stress.Figure 8 illustrates the relation between starting positions and trajectory outcome for scenario π g (O r ) in more detail.Starting positions near the rear left corner (x, y) = (−50.0,−86.0) mm of the tissue tend to time out without grasping, while starting positions near the rear right corner (x, y) = (50.0,−86.0) mm tend to terminate due to excessive collisions.</p>
<p>V. DISCUSSION</p>
<p>The results of scenario π g (O r ) show that the policy trained in simulation on the transformed observation function G(O s ) is able to perform the TR task directly from raw camera images from the real observation function O r .This indicates that the trained model G is able to bridge the visual domain gap.The intuition for scenario π g (G(O r )) was that G may mitigate the influence of changes in illumination by translating real images from different lighting conditions into a consistent appearance.Surprisingly, the policy performs worse in this scenario than the policy on real images directly.This also supports the claim that the image-to-image translation results in the loss of some image information necessary to compensate for the novel dynamics on the real robot.</p>
<p>On the real evaluation setup, starting positions near the rear right corner collide excessively with the tissue.These starting positions were furthest away from the grasping point and thus also lead to the longest trajectories.Longer trajectories were observed to have more collisions, since the simplified collision model in simulation is somewhat more permissive than reality.Future work will investigate additional reward terms that encourage safe policies that are robust to changes in environment dynamics.The trajectory outcomes of evaluation scenario π s (O s ) show that policy π s is robust to the physical domain gap between simulation and the real system even though the simulation neither models measurement inaccuracies nor the dynamic behavior of the cable driven mechanism of the PSM.Both π s (O s ) and π g (G(O s )) are evaluated on observations from the same distribution as they were trained on, but under different dynamics through the digital twin.The drop in performance when comparing π s (O s ) to π g (G(O s )) shows that policy π g learned a behavior that does not translate well to the additional safety constraints, i.e. trajectory terminated on excessive tissue stress, as described in Section III-C.This result indicates that learning on translated images results in policies less robust to changes in dynamics.This may be due to the relative visual complexity of the translated images compared to the ones from simulation, which may make it more difficult for the policy to infer the environment's state from the observation.Preliminary tests with policies trained on a single GAN showed much lower success rates than the proposed method that used an ensemble of GANs.This strengthens the idea that using an ensemble of GANs may be interpreted as a form of domain randomization.Future work may further investigate how employing an ensemble of GANs compares to classical visual domain randomization.</p>
<p>The GAN used in this work was trained with a total of 246 TR trajectories and occasional random motions in the environment.This is substantially less data than stateof-the-art methods that achieve comparable task success in the real world, but require 10 000 task executions [12] and additional task specific auxiliary tasks or real world labels.Data collection for TR task execution was straightforward since the required motion could be planned from predefined start and end points of the motion.Tasks where motion planning has to consider the elastic behavior and dynamics of the deformable object may complicate data collection.Simulation and evaluation setup were calibrated to have the same camera perspective and object positions.This choice was made to limit the image-to-image translation task to changes in image appearance [12], [13].</p>
<p>The imbalance of required training steps for learning the two-phased task may indicate that the initially learned features relevant for solving the grasp phase contradict the features relevant for the retraction phase, and that relearning features that can be generalized to both phases requires prolonged learning time.Additional challenges were encountered over the course of this work that are not explicitly described in this contribution, but should be mentioned to aid future work in the field.(1) Learning success in simulation was highly dependent on the camera perspective.Multiple different camera positions were evaluated and some, that often did not substantially differ to the human eye, were so detrimental to training, that the task was not learnable by the agent.(2) Reward normalization was essential for fast and repeatable training success.(3) Normalizing the real observations with running mean and standard deviation to mitigate the effects of changes in O r caused by changing lighting conditions is unlikely to yield better results as changes in lighting change more than just the brightness of an image.Different lighting conditions may also change the overall hue of an image as well as the presence of shadows.</p>
<p>The achieved task success rate of 50% is aligned with results obtained in state-of-the-art works for image-based sim-to-real transfer in robotics [10], although we expect that task success could be further improved by including more sophisticated methods such as hierarchical RL to learn a policy per phase, pretraining the agent with behavioral cloning, or including additional DR as proposed in [14].We do not include these methods here to focus on DA rather than the absolute task success.Preliminary experiments with the same approach and hyperparameters on a different surgical robotic task, a Tissue Manipulation setup similar to [4], yield comparable results and a 63% task success rate.In contrast to TR, Tissue Manipulation is a robotic control task in which a visual marking on a deformable object must be aligned with an overlayed target location on the image observation.The manipulation is indirect because the deformable object is manipulated at a grasping point that does not coincide with the visual marking of interest.Thus, the policy must learn to model the deformation in order to align the marking with the target location.In comparison to [4], our method does not rely on an image-processing pipeline to generate state-observations and does not require training the policy on the real robotic system.We plan to extend these preliminary experiments to show that the approach can be transferred to other image-based tasks without changes to the pipeline.Moreover, initial tests were performed with DR for the TR task but did not yield noticeable benefits over naive sim-to-real transfer without DA and was thus not included as a separate experiment.</p>
<p>Although the proposed pipeline is able to perform imagebased sim-to-real transfer of a soft tissue manipulation task, some limitations remain that will be addressed in future work.In the context of surgical task learning, the method may be transferred to other tasks without modification and creation of auxiliary tasks for stabilization.The learned weights of the image translation model, however, are unlikely to perform successfully on new tasks.Despite requiring significantly less data compared to related works [12], [13], it is necessary to record unpaired image data to train the translation models on a new task.From an image-to-image translation perspective however, the greatest limiting factor for practical application is that changes in camera perspective introduce additional changes in image content, not only in appearance.The presented approach thus relies on a precise calibration of camera positions between real system and simulation.Achieving view independence would increase the practical applicability of the approach for more realistic surgical scenarios where camera perspectives change over the course of the task and calibration between real setup and simulation is challenging.</p>
<p>Building on the achieved results, future work will investigate the approach on setups with increased realism that include complex visual features, a broader physical domain gap, and non-calibrated camera perspectives.</p>
<p>VI. CONCLUSION</p>
<p>This work is the first successful demonstration of sim-toreal transfer of a visuomotor policy in the context of robotic surgery.A data efficient approach for sim-to-real transfer through DA utilizing a contrastive GAN is presented.The policy is trained in a soft-body simulation on image observations and then transferred to a real robotic system without retraining the policy.Furthermore, the contrastive GAN approach does not require task specific auxiliary tasks, and requires much less data to train compared to most examples from literature.Evaluation of the policy on the real system demonstrates successful sim-to-real transfer for a TR task and points out several critical challenges that must be addressed in future work.The successful transfer of an image based policy from simulation to a real robotic system for deformable object manipulation paves the way towards making RL viable for robotic surgery and is a sizeable leap towards cognitive surgical robots.</p>
<p>Fig. 1 :
1
Fig. 1: Experimental setup for tissue retraction.We combine an Intel Re-alSense camera and the da Vinci Research Kit with a ProGrasp instrument grasping a yellow sheet of silicone attached to a board.</p>
<p>2 )Fig. 2 :
22
Fig. 2: Overview of training and evaluation settings.A policy π is trained in simulation on translated observations ôr = G(os).During evaluation on the robotic system, the policy receives real image observations or.The actions a of the policy are deltas in the gripper's Cartesian coordinates to solve a tissue retraction task.</p>
<p>To this effect, this work investigates training an Unpaired Image-To-Image (UI2I) translation model G : O s → O r that translates images o s from the simulation's observation function O s , such that the translated images ôr = G(o s ) are indistinguishable from samples o r ∼ O r (o | ẑ) taken from the real observation function O r , where ẑ is the hidden state.UI2I methods utilize unpaired image data, making it feasible to train G without access to hidden states of either environment.</p>
<p>Fig. 3 :
3
Fig. 3: (a) Tissue retraction scene implemented in SOFA with illustrated coordinate system and (b) experimental setup on the real robotic system.</p>
<p>Fig. 6 :Fig. 7 :
67
Fig. 6: Images from simulation and their translations by CycleGAN, CUT, and DCL, as well as real images (a) and a magnified view (b) on Simulation, CUT, and DCL images from the second row of (a).</p>
<p>Fig. 8 :
8
Fig. 8: Trajectory outcome of case πg(Or) for 16 evaluated starting positions on the XY-plane.Each starting position was executed on two starting heights (z = 60 mm and z = 70 mm).</p>
<p>TABLE I :
I
Evaluation results for the scenarios defined in Section III-B with regard to the metrics defined in Section III-C.
ScenarioSuccess RatePath LengthCollisionsπs(Os)32/32210 mm2.93 stepsπg(G(Os))25/32235 mm12.16 stepsπg(Or)16/32219 mm14.56 stepsπg(G(Or))13/32210 mm18.08 stepsand π g achieve a task success rate of 100% in simulation bythe end of training.
This paper was recommended for publication by Editor Pietro Valdastri upon evaluation of the Associate Editor and Reviewers' comments.This work was supported by the Karlsruhe House of Young Scientists (KHYS), the Helmholtz Association under the joint research school "HIDSS4Health -Helmholtz Information and Data Science School for Health", and the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme under grant agreement No. 742671 (ARS).
Multilateral surgical pattern cutting in 2D orthotropic gauze with deep reinforcement learning policies for tensioning. B Thananjeyan, A Garg, S Krishnan, C Chen, L Miller, K Goldberg, IEEE Int. Conf. Robot. Autom. (ICRA. 2017</p>
<p>Soft tissue simulation environment to learn manipulation tasks in autonomous robotic surgery. E Tagliabue, A Pore, D Dall'alba, E Magnabosco, M Piccinelli, P Fiorini, IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS). 2020</p>
<p>Learning from demonstrations for autonomous soft-tissue retraction. A Pore, E Tagliabue, M Piccinelli, D Dall'alba, A Casals, P Fiorini, Int. Symp. Med. Robot. (ISMR). 2021</p>
<p>Autonomous Tissue Manipulation via Surgical Robot Using Learning Based Model Predictive Control. C Shin, P W Ferguson, S A Pedram, J Ma, E P Dutson, J Rosen, IEEE Int. Conf. Robot. Autom. (ICRA). May 2019</p>
<p>Learning intraoperative organ manipulation with context-based reinforcement learning. C D'ettorre, S Zirino, N N Dei, A Stilli, E De Momi, D Stoyanov, Int. J. Comput. Assist. Radiol. Surg. 2022</p>
<p>Cooperative Assistance in Robotic Surgery through Multi-Agent Reinforcement Learning. P M Scheikl, IEEE/RSJ Int. 2021</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, IEEE/RSJ Int. Conf. Intell. Robot. Syst. 2017</p>
<p>Evaluation of Domain Randomization Techniques for Transfer Learning. S Grün, S Höninger, P M Scheikl, B Hein, T Kröger, Adv. Robot. (ICAR). Int. Conf2019</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, IEEE Int. Conf. Robot. Autom. (ICRA). 2018</p>
<p>Learning dexterous in-hand manipulation. O M Andrychowicz, Int. J. Rob. Res. 3912020</p>
<p>Sim-to-Real Reinforcement Learning for Deformable Object Manipulation. J Matas, S James, A J Davison, Conf. Robot. Learn. (CORL). 2018</p>
<p>Retinagan: An object-aware approach to sim-to-real transfer. D Ho, K Rao, Z Xu, E Jang, M Khansari, Y Bai, IEEE Int. Conf. Robot. Autom. (ICRA). 109262021</p>
<p>Rlcyclegan: Reinforcement learning aware simulation-to-real. K Rao, C Harris, A Irpan, S Levine, J Ibarz, M Khansari, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2020</p>
<p>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. S James, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 126372019</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, IEEE Int. Conf. Robot. Autom. (ICRA). 2018</p>
<p>Unpaired image-to-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, IEEE Int. Conf. Comput. Vis. 2017</p>
<p>Contrastive learning for unpaired image-to-image translation. T Park, A A Efros, R Zhang, J.-Y Zhu, Europ. Conf. Comp. Vis. (ECCV). 2020</p>
<p>Dual contrastive learning for unsupervised image-to-image translation. J Han, M Shoeiby, L Petersson, M A Armin, Conf. Comput. Vis. Pattern Recognit. Workshops (CVPR). IEEE Comput. Soc2021</p>
<p>Synthetic-to-real domain adaptation using contrastive unpaired translation. B T Imbusch, M Schwarz, S Behnke, IEEE Int. Conf. Autom. Sci. Eng. (CASE). 2022</p>
<p>Selfsupervised transparent liquid segmentation for robotic pouring. G Narasimhan, K Zhang, B Eisner, X Lin, D Held, IEEE Int. Conf. Robot. Autom. (ICRA). 2022IEEE</p>
<p>Acting optimally in partially observable stochastic domains. A R Cassandra, L P Kaelbling, M L Littman, Aaai. 199494</p>
<p>Safe reinforcement learning using formal verification for tissue retraction in autonomous robotic-assisted surgery. A Pore, IEEE/RSJ Int. 2021</p>
<p>Autonomous tissue retraction in robotic assisted minimally invasive surgery-a feasibility study. A Attanasio, IEEE Robot. Autom. Lett. 542020</p>
<p>Deliberation in autonomous robotic surgery: a framework for handling anatomical uncertainty. E Tagliabue, D Meli, D Dall'alba, P Fiorini, IEEE Int. Conf. Robot. Autom. (ICRA). 11862022</p>
<p>SOFA: A multi-model framework for interactive physical simulation. F Faure, Soft Tissue Biomechanical Modeling for Computer Assisted Surgery. 201211</p>
<p>Openai gym. G Brockman, arXiv:1606.015402016arXiv preprint</p>
<p>Human-level control through deep reinforcement learning. V Mnih, nature. 51875402015</p>
<p>Stable-Baselines3: Reliable Reinforcement Learning Implementations. A Raffin, A Hill, A Gleave, A Kanervisto, M Ernestus, N Dormann, J. Mach. Learn. Res. 222682021</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, 2017</p>
<p>Curriculum learning for reinforcement learning domains: A framework and survey. S Narvekar, B Peng, M Leonetti, J Sinapov, M E Taylor, P Stone, J. Mach. Learn. Res. 2112022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7421 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7421</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7421</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-249431481</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2022.emnlp-industry.2.pdf" target="_blank">DynaMaR: Dynamic Prompt with Mask Token Representation</a></p>
                <p><strong>Paper Abstract:</strong> Recent research has shown that large language models pretrained using unsupervised approaches can achieve significant performance improvement on many downstream tasks. Typically when adapting these language models to downstream tasks, like a classification or regression task, we employ a fine-tuning paradigm in which the sentence representation from the language model is input to a task-specific head; the model is then fine-tuned end-to-end. However, with the emergence of models like GPT-3, prompt-based fine-tuning has been proven to be a successful approach for few-shot tasks. Inspired by this work, we study discrete prompt technologies in practice. There are two issues that arise with the standard prompt approach. First, it can overfit on the prompt template. Second, it requires manual effort to formulate the downstream task as a language model problem. In this paper, we propose an improvement to prompt-based fine-tuning that addresses these two issues. We refer to our approach as DynaMaR -- Dynamic Prompt with Mask Token Representation. Results show that DynaMaR can achieve an average improvement of 10% in few-shot settings and improvement of 3.7% in data-rich settings over the standard fine-tuning approach on four e-commerce applications.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7421.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7421.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PFt-CLS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Promptless Fine-tuning using [CLS] token representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard fine-tuning baseline that feeds the pre-trained transformer's [CLS] token representation into a task-specific head rather than using any prompt; used as the reference baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom encoder-only Transformer (internal)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer with 38 hidden layers, 1024 embedding size, 16 attention heads, trained from product-catalog pretraining using LANS optimizer and a 32K BPE tokenizer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>500M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VE, MM, MG, PP (four proprietary e-commerce tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>VE: binary pairwise variation detection; MM: binary pairwise music match; MG: 303-way single-document music genre classification; PP: single-document regression to predict price.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Promptless classification/regression using [CLS] token embedding as sentence representation</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>No prompt inserted; standard fine-tuning: [CLS] output vector fed into task-specific head. Used as baseline against which prompt variants were compared. Evaluated in both few-shot (≈20 examples/class for classification, 1% sample for regression) and data-rich settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used as baseline for reported relative improvements (PRAUC for VE/MM, accuracy for MG, RMSE for PP)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline reference (set to 0% improvement) across VE, MM, MG, PP in few-shot comparisons reported in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot: ≈20 labeled examples per class for classification tasks; regression few-shot: 1% of full training; data-rich: full training set. Validation every 2 steps (few-shot) / 100 steps (data-rich).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DynaMaR: Dynamic Prompt with Mask Token Representation', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7421.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7421.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PFt-Avg</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Promptless Fine-tuning using Average Pooling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A promptless baseline that uses average pooling over sequence outputs as the sentence representation for the task-specific head instead of [CLS]; evaluated to compare representation choices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom encoder-only Transformer (internal)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer (38 layers, 1024 dim, 16 heads) pretrained on product catalog.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>500M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VE, MM, MG, PP (four proprietary e-commerce tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as PFt-CLS baseline tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Promptless classification/regression using average-pooled sequence outputs</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / representation choice</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>No prompt; sentence embedding obtained by averaging token-level outputs across sequence; otherwise same fine-tuning procedure and predictor head as PFt-CLS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relative improvement percentage compared to PFt-CLS (PRAUC, accuracy, RMSE aggregated into % improvements reported by authors).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Few-shot relative to PFt-CLS: VE -1.5%, MM +7.2%, MG -3.7%, PP -8.8%, Avg -1.7%</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>PFt-CLS (baseline reference, 0% by construction)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Average change vs PFt-CLS: -1.7% (few-shot aggregate reported); individual tasks shown above.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot and data-rich reported; values above from few-shot table. Same training hyperparameters and predictor head as other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DynaMaR: Dynamic Prompt with Mask Token Representation', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7421.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7421.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NP-Prefix</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Null Prompt - Prefix</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt variant that prepends a [MASK] token to the original input and uses the [MASK] token representation as the sentence embedding for the task-specific head, avoiding handcrafted prompt templates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom encoder-only Transformer (internal)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer (38 layers, 1024 dim, 16 heads) pretrained on product catalog.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>500M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VE, MM, MG, PP (four proprietary e-commerce tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same e-commerce tasks as other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Null prompt (prepend single [MASK] token) and use [MASK] representation for prediction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt placement</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Single [MASK] token inserted at the prefix of input (i.e., before original text); no natural-language prompt added; [MASK] representation fed to task-specific head. Compared with NP-Suffix to study positional effects.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relative improvement percentage compared to PFt-CLS (PRAUC/accuracy/RMSE aggregated into % improvements reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Few-shot relative to PFt-CLS: VE -1.0%, MM +4.1%, MG -2.0%, PP +2.6%, Avg +0.9%</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>PFt-CLS (0% baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Average +0.9% (few-shot); mixed per-task changes as listed.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot: ≈20 examples/class (classification) or 1% for regression. Authors also note NP approaches improved average performance in data-rich settings (no per-task data-rich numbers provided in main text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DynaMaR: Dynamic Prompt with Mask Token Representation', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7421.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7421.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NP-Suffix</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Null Prompt - Suffix</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A null-prompt variant that appends a [MASK] token to the original input and uses the [MASK] token representation for the task-specific head; used to examine positional effects of the mask.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom encoder-only Transformer (internal)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer (38 layers, 1024 dim, 16 heads) pretrained on product catalog.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>500M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VE, MM, MG, PP (four proprietary e-commerce tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Null prompt (append single [MASK] token) and use [MASK] representation for prediction</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt placement</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Single [MASK] token inserted at the suffix of input (i.e., after original text); no natural-language prompt added. Compared directly against NP-Prefix to show positional differences due to distance between [MASK] and task-relevant tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relative improvement percentage compared to PFt-CLS (PRAUC/accuracy/RMSE aggregated into % improvements reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Few-shot relative to PFt-CLS: VE -2.6%, MM +0.2%, MG -1.6%, PP +6.7%, Avg +0.7%</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>PFt-CLS (0% baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Average +0.7% (few-shot); per-task changes listed above. Authors note positional differences likely arise from different distances between [MASK] and important tokens (example: sentiment word distance).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot and data-rich (few-shot values shown). Same optimization and early stopping as other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DynaMaR: Dynamic Prompt with Mask Token Representation', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7421.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7421.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FiTeR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fixed Prompt with Mask Token Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A static prompt approach that appends a fixed natural-language prompt (template) with a [MASK] token; the [MASK] token representation is fed to a task-specific head instead of using the pre-trained MLM head.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom encoder-only Transformer (internal)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer pretrained on product catalog; uses generated prompt templates (manually and via T5 paraphrase) chosen as a fixed prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>500M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VE, MM, MG, PP (four proprietary e-commerce tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same e-commerce tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Fixed natural-language prompt template appended or inserted with a [MASK] token; single static template during training and inference</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt template</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>A single handcrafted or paraphrased prompt template chosen and held fixed for both training and inference. Mask token representation used as embedding for task-specific predictor head. Selected prompts were drawn from a generated candidate pool.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relative improvement percentage compared to PFt-CLS (PRAUC/accuracy/RMSE aggregated into % improvements reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Few-shot relative to PFt-CLS: VE -0.7%, MM +13.9%, MG -1.1%, PP +7.3%, Avg +4.9%</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>PFt-CLS (0% baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Average +4.9% (few-shot); FiTeR outperforms null-prompt variants on average, indicating benefit from task-specific natural-language prompt information despite overfitting risk.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompt candidates generated (10 manual seeds + 10 paraphrases via T5); a single template selected as inference prompt. Few-shot and data-rich evaluations conducted.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DynaMaR: Dynamic Prompt with Mask Token Representation', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7421.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7421.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DynaMaR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Prompt with Mask Token Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposed method that constructs a diverse dynamic prompt pool (multiple prompt templates selected by a prompt dissimilarity score) and randomly selects a prompt each training step while using the [MASK] token representation as input to a task-specific head to mitigate prompt overfitting and remove answer-engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom encoder-only Transformer (internal)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same encoder-only Transformer (38 layers, 1024 dim, 16 heads) pretrained on product-catalog dataset; uses a dynamic prompt pool (selected via normalized Hamming + Levenshtein dissimilarity) during training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>500M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>VE, MM, MG, PP (four proprietary e-commerce tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same e-commerce tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Dynamic prompt pool: multiple natural-language prompt templates (selected for dissimilarity) applied randomly per training step; use [MASK] token representation as embedding for predictor head</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt diversity</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>For each task, 20 candidate templates (10 manual + 10 paraphrases via T5) were generated; a pairwise prompt dissimilarity score (normalized Hamming on position/format bits + normalized Levenshtein on text) was used to select a pool (typically 5 templates). During training, a template is randomly sampled per step. Inference uses the best-performing template from the pool. Pool sizes compared: 1, 3, 5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Relative improvement percentage compared to PFt-CLS (PRAUC/accuracy/RMSE aggregated into % improvements reported); also average task-level improvements reported across tasks and settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Few-shot relative to PFt-CLS: VE +0.8%, MM +15.8%, MG -0.5%, PP +23.8%, Avg +10.0%. Data-rich: average improvement over PFt-CLS reported as +3.7% (across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>PFt-CLS (0% baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Few-shot average +10.0% vs PFt-CLS; Data-rich average +3.7% vs PFt-CLS. DynaMaR outperforms FiTeR on all tasks in both settings except MG in data-rich setting (no per-task data-rich numbers provided in excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot: ≈20 labeled examples per class (classification), 1% sample for regression; prompt pools of sizes 1, 3, 5 compared (Figure 3); pairwise dissimilarity threshold set at 0.5 for selection; early stopping and validation described in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DynaMaR: Dynamic Prompt with Mask Token Representation', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7421.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7421.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-related overfitting / Mask position bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-related overfitting and mask-position-induced bias</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed phenomenon that models can overfit to words/phrases in prompt templates and to the position of the [MASK] token, causing spurious correlations and degraded generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom encoder-only Transformer (internal)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer (38 layers) with masked-language-model pretraining; attention concentrates on prompt region according to paper's Figure 1.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>500M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Illustrative example: sentiment classification (movie reviews); also relevant across the four e-commerce tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Sentiment example: classifying whether a review is positive/negative; in paper this phenomenon motivates dynamic prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompt templates with [MASK] token (static placement/wording)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt wording and position</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors describe how prompts like 'Does the user like the movie?[MASK]' can cause the model to predict positive class for negative reviews because the word 'like' co-occurs with positive labels and the MLM attends more to tokens near the mask. Position (prefix vs suffix) also changes distance between mask and informative tokens, altering performance (observed in NP-Prefix vs NP-Suffix results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Explains observed per-task/per-format performance differences (e.g., NP-Prefix vs NP-Suffix differences); motivates DynaMaR dynamic-pool approach which yields improved average performance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Qualitative observation illustrated by example and by BERT attention distribution (Figure 1); leads to design choice of randomly sampling diverse prompts during training.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DynaMaR: Dynamic Prompt with Mask Token Representation', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7421.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7421.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt pool size effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of dynamic prompt pool size on performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical observation reported in the paper that larger dynamic prompt pools (more diverse templates) lead to better downstream performance when using DynaMaR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Custom encoder-only Transformer (internal)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same encoder-only Transformer described for other experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>500M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate across VE, MM, MG, PP</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Authors compare average improvement percentages across tasks for pool sizes 1, 3, 5.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Dynamic prompt pool of variable size (number of distinct templates used during training)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt diversity</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Pools constructed by selecting templates with pairwise dissimilarity > 0.5; experiments compare pool size = 1, 3, 5 and report increasing performance with larger pool size (visualized in Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average improvement percentage across four tasks (relative to PFt-CLS)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative/aggregate: performance increases as pool size increases from 1→3→5 (numeric per-task breakdown not fully enumerated in excerpt; Figure 3 shows monotonic improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Pool size = 1 (equivalent to fixed single prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Authors report monotonic improvement; DynaMaR with pool size 5 used in main reported results (few-shot Avg +10.0%).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Prompt dissimilarity threshold 0.5 used to select diverse templates; candidates generated 20 per task then 5 selected for pool in main experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DynaMaR: Dynamic Prompt with Mask Token Representation', 'publication_date_yy_mm': '2022-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Cutting down on prompts and parameters: Simple few-shot learning with language models <em>(Rating: 2)</em></li>
                <li>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing <em>(Rating: 2)</em></li>
                <li>Making pre-trained language models better few-shot learners <em>(Rating: 2)</em></li>
                <li>Adapting language models zero-shot learning by meta-tuning on dataset and prompt collections <em>(Rating: 2)</em></li>
                <li>How can we know what language models know? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7421",
    "paper_id": "paper-249431481",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "PFt-CLS",
            "name_full": "Promptless Fine-tuning using [CLS] token representation",
            "brief_description": "Standard fine-tuning baseline that feeds the pre-trained transformer's [CLS] token representation into a task-specific head rather than using any prompt; used as the reference baseline in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Custom encoder-only Transformer (internal)",
            "model_description": "Encoder-only Transformer with 38 hidden layers, 1024 embedding size, 16 attention heads, trained from product-catalog pretraining using LANS optimizer and a 32K BPE tokenizer.",
            "model_size": "500M",
            "task_name": "VE, MM, MG, PP (four proprietary e-commerce tasks)",
            "task_description": "VE: binary pairwise variation detection; MM: binary pairwise music match; MG: 303-way single-document music genre classification; PP: single-document regression to predict price.",
            "problem_format": "Promptless classification/regression using [CLS] token embedding as sentence representation",
            "format_category": "prompt style",
            "format_details": "No prompt inserted; standard fine-tuning: [CLS] output vector fed into task-specific head. Used as baseline against which prompt variants were compared. Evaluated in both few-shot (≈20 examples/class for classification, 1% sample for regression) and data-rich settings.",
            "performance_metric": "Used as baseline for reported relative improvements (PRAUC for VE/MM, accuracy for MG, RMSE for PP)",
            "performance_value": "Baseline reference (set to 0% improvement) across VE, MM, MG, PP in few-shot comparisons reported in the paper",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Few-shot: ≈20 labeled examples per class for classification tasks; regression few-shot: 1% of full training; data-rich: full training set. Validation every 2 steps (few-shot) / 100 steps (data-rich).",
            "statistical_significance": null,
            "uuid": "e7421.0",
            "source_info": {
                "paper_title": "DynaMaR: Dynamic Prompt with Mask Token Representation",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "PFt-Avg",
            "name_full": "Promptless Fine-tuning using Average Pooling",
            "brief_description": "A promptless baseline that uses average pooling over sequence outputs as the sentence representation for the task-specific head instead of [CLS]; evaluated to compare representation choices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Custom encoder-only Transformer (internal)",
            "model_description": "Encoder-only Transformer (38 layers, 1024 dim, 16 heads) pretrained on product catalog.",
            "model_size": "500M",
            "task_name": "VE, MM, MG, PP (four proprietary e-commerce tasks)",
            "task_description": "Same as PFt-CLS baseline tasks.",
            "problem_format": "Promptless classification/regression using average-pooled sequence outputs",
            "format_category": "prompt style / representation choice",
            "format_details": "No prompt; sentence embedding obtained by averaging token-level outputs across sequence; otherwise same fine-tuning procedure and predictor head as PFt-CLS.",
            "performance_metric": "Relative improvement percentage compared to PFt-CLS (PRAUC, accuracy, RMSE aggregated into % improvements reported by authors).",
            "performance_value": "Few-shot relative to PFt-CLS: VE -1.5%, MM +7.2%, MG -3.7%, PP -8.8%, Avg -1.7%",
            "baseline_performance": "PFt-CLS (baseline reference, 0% by construction)",
            "performance_change": "Average change vs PFt-CLS: -1.7% (few-shot aggregate reported); individual tasks shown above.",
            "experimental_setting": "Few-shot and data-rich reported; values above from few-shot table. Same training hyperparameters and predictor head as other baselines.",
            "statistical_significance": null,
            "uuid": "e7421.1",
            "source_info": {
                "paper_title": "DynaMaR: Dynamic Prompt with Mask Token Representation",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "NP-Prefix",
            "name_full": "Null Prompt - Prefix",
            "brief_description": "A prompt variant that prepends a [MASK] token to the original input and uses the [MASK] token representation as the sentence embedding for the task-specific head, avoiding handcrafted prompt templates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Custom encoder-only Transformer (internal)",
            "model_description": "Encoder-only Transformer (38 layers, 1024 dim, 16 heads) pretrained on product catalog.",
            "model_size": "500M",
            "task_name": "VE, MM, MG, PP (four proprietary e-commerce tasks)",
            "task_description": "Same e-commerce tasks as other methods.",
            "problem_format": "Null prompt (prepend single [MASK] token) and use [MASK] representation for prediction",
            "format_category": "prompt style / prompt placement",
            "format_details": "Single [MASK] token inserted at the prefix of input (i.e., before original text); no natural-language prompt added; [MASK] representation fed to task-specific head. Compared with NP-Suffix to study positional effects.",
            "performance_metric": "Relative improvement percentage compared to PFt-CLS (PRAUC/accuracy/RMSE aggregated into % improvements reported).",
            "performance_value": "Few-shot relative to PFt-CLS: VE -1.0%, MM +4.1%, MG -2.0%, PP +2.6%, Avg +0.9%",
            "baseline_performance": "PFt-CLS (0% baseline)",
            "performance_change": "Average +0.9% (few-shot); mixed per-task changes as listed.",
            "experimental_setting": "Few-shot: ≈20 examples/class (classification) or 1% for regression. Authors also note NP approaches improved average performance in data-rich settings (no per-task data-rich numbers provided in main text excerpt).",
            "statistical_significance": null,
            "uuid": "e7421.2",
            "source_info": {
                "paper_title": "DynaMaR: Dynamic Prompt with Mask Token Representation",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "NP-Suffix",
            "name_full": "Null Prompt - Suffix",
            "brief_description": "A null-prompt variant that appends a [MASK] token to the original input and uses the [MASK] token representation for the task-specific head; used to examine positional effects of the mask.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Custom encoder-only Transformer (internal)",
            "model_description": "Encoder-only Transformer (38 layers, 1024 dim, 16 heads) pretrained on product catalog.",
            "model_size": "500M",
            "task_name": "VE, MM, MG, PP (four proprietary e-commerce tasks)",
            "task_description": "Same as other baselines.",
            "problem_format": "Null prompt (append single [MASK] token) and use [MASK] representation for prediction",
            "format_category": "prompt style / prompt placement",
            "format_details": "Single [MASK] token inserted at the suffix of input (i.e., after original text); no natural-language prompt added. Compared directly against NP-Prefix to show positional differences due to distance between [MASK] and task-relevant tokens.",
            "performance_metric": "Relative improvement percentage compared to PFt-CLS (PRAUC/accuracy/RMSE aggregated into % improvements reported).",
            "performance_value": "Few-shot relative to PFt-CLS: VE -2.6%, MM +0.2%, MG -1.6%, PP +6.7%, Avg +0.7%",
            "baseline_performance": "PFt-CLS (0% baseline)",
            "performance_change": "Average +0.7% (few-shot); per-task changes listed above. Authors note positional differences likely arise from different distances between [MASK] and important tokens (example: sentiment word distance).",
            "experimental_setting": "Few-shot and data-rich (few-shot values shown). Same optimization and early stopping as other methods.",
            "statistical_significance": null,
            "uuid": "e7421.3",
            "source_info": {
                "paper_title": "DynaMaR: Dynamic Prompt with Mask Token Representation",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "FiTeR",
            "name_full": "Fixed Prompt with Mask Token Representation",
            "brief_description": "A static prompt approach that appends a fixed natural-language prompt (template) with a [MASK] token; the [MASK] token representation is fed to a task-specific head instead of using the pre-trained MLM head.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Custom encoder-only Transformer (internal)",
            "model_description": "Encoder-only Transformer pretrained on product catalog; uses generated prompt templates (manually and via T5 paraphrase) chosen as a fixed prompt.",
            "model_size": "500M",
            "task_name": "VE, MM, MG, PP (four proprietary e-commerce tasks)",
            "task_description": "Same e-commerce tasks.",
            "problem_format": "Fixed natural-language prompt template appended or inserted with a [MASK] token; single static template during training and inference",
            "format_category": "prompt style / prompt template",
            "format_details": "A single handcrafted or paraphrased prompt template chosen and held fixed for both training and inference. Mask token representation used as embedding for task-specific predictor head. Selected prompts were drawn from a generated candidate pool.",
            "performance_metric": "Relative improvement percentage compared to PFt-CLS (PRAUC/accuracy/RMSE aggregated into % improvements reported).",
            "performance_value": "Few-shot relative to PFt-CLS: VE -0.7%, MM +13.9%, MG -1.1%, PP +7.3%, Avg +4.9%",
            "baseline_performance": "PFt-CLS (0% baseline)",
            "performance_change": "Average +4.9% (few-shot); FiTeR outperforms null-prompt variants on average, indicating benefit from task-specific natural-language prompt information despite overfitting risk.",
            "experimental_setting": "Prompt candidates generated (10 manual seeds + 10 paraphrases via T5); a single template selected as inference prompt. Few-shot and data-rich evaluations conducted.",
            "statistical_significance": null,
            "uuid": "e7421.4",
            "source_info": {
                "paper_title": "DynaMaR: Dynamic Prompt with Mask Token Representation",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "DynaMaR",
            "name_full": "Dynamic Prompt with Mask Token Representation",
            "brief_description": "Proposed method that constructs a diverse dynamic prompt pool (multiple prompt templates selected by a prompt dissimilarity score) and randomly selects a prompt each training step while using the [MASK] token representation as input to a task-specific head to mitigate prompt overfitting and remove answer-engineering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Custom encoder-only Transformer (internal)",
            "model_description": "Same encoder-only Transformer (38 layers, 1024 dim, 16 heads) pretrained on product-catalog dataset; uses a dynamic prompt pool (selected via normalized Hamming + Levenshtein dissimilarity) during training.",
            "model_size": "500M",
            "task_name": "VE, MM, MG, PP (four proprietary e-commerce tasks)",
            "task_description": "Same e-commerce tasks.",
            "problem_format": "Dynamic prompt pool: multiple natural-language prompt templates (selected for dissimilarity) applied randomly per training step; use [MASK] token representation as embedding for predictor head",
            "format_category": "prompt style / prompt diversity",
            "format_details": "For each task, 20 candidate templates (10 manual + 10 paraphrases via T5) were generated; a pairwise prompt dissimilarity score (normalized Hamming on position/format bits + normalized Levenshtein on text) was used to select a pool (typically 5 templates). During training, a template is randomly sampled per step. Inference uses the best-performing template from the pool. Pool sizes compared: 1, 3, 5.",
            "performance_metric": "Relative improvement percentage compared to PFt-CLS (PRAUC/accuracy/RMSE aggregated into % improvements reported); also average task-level improvements reported across tasks and settings.",
            "performance_value": "Few-shot relative to PFt-CLS: VE +0.8%, MM +15.8%, MG -0.5%, PP +23.8%, Avg +10.0%. Data-rich: average improvement over PFt-CLS reported as +3.7% (across tasks).",
            "baseline_performance": "PFt-CLS (0% baseline)",
            "performance_change": "Few-shot average +10.0% vs PFt-CLS; Data-rich average +3.7% vs PFt-CLS. DynaMaR outperforms FiTeR on all tasks in both settings except MG in data-rich setting (no per-task data-rich numbers provided in excerpt).",
            "experimental_setting": "Few-shot: ≈20 labeled examples per class (classification), 1% sample for regression; prompt pools of sizes 1, 3, 5 compared (Figure 3); pairwise dissimilarity threshold set at 0.5 for selection; early stopping and validation described in paper.",
            "statistical_significance": null,
            "uuid": "e7421.5",
            "source_info": {
                "paper_title": "DynaMaR: Dynamic Prompt with Mask Token Representation",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Prompt-related overfitting / Mask position bias",
            "name_full": "Prompt-related overfitting and mask-position-induced bias",
            "brief_description": "Observed phenomenon that models can overfit to words/phrases in prompt templates and to the position of the [MASK] token, causing spurious correlations and degraded generalization.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "Custom encoder-only Transformer (internal)",
            "model_description": "Encoder-only Transformer (38 layers) with masked-language-model pretraining; attention concentrates on prompt region according to paper's Figure 1.",
            "model_size": "500M",
            "task_name": "Illustrative example: sentiment classification (movie reviews); also relevant across the four e-commerce tasks",
            "task_description": "Sentiment example: classifying whether a review is positive/negative; in paper this phenomenon motivates dynamic prompting.",
            "problem_format": "Natural-language prompt templates with [MASK] token (static placement/wording)",
            "format_category": "prompt style / prompt wording and position",
            "format_details": "Authors describe how prompts like 'Does the user like the movie?[MASK]' can cause the model to predict positive class for negative reviews because the word 'like' co-occurs with positive labels and the MLM attends more to tokens near the mask. Position (prefix vs suffix) also changes distance between mask and informative tokens, altering performance (observed in NP-Prefix vs NP-Suffix results).",
            "performance_metric": null,
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": "Explains observed per-task/per-format performance differences (e.g., NP-Prefix vs NP-Suffix differences); motivates DynaMaR dynamic-pool approach which yields improved average performance.",
            "experimental_setting": "Qualitative observation illustrated by example and by BERT attention distribution (Figure 1); leads to design choice of randomly sampling diverse prompts during training.",
            "statistical_significance": null,
            "uuid": "e7421.6",
            "source_info": {
                "paper_title": "DynaMaR: Dynamic Prompt with Mask Token Representation",
                "publication_date_yy_mm": "2022-06"
            }
        },
        {
            "name_short": "Prompt pool size effect",
            "name_full": "Effect of dynamic prompt pool size on performance",
            "brief_description": "Empirical observation reported in the paper that larger dynamic prompt pools (more diverse templates) lead to better downstream performance when using DynaMaR.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Custom encoder-only Transformer (internal)",
            "model_description": "Same encoder-only Transformer described for other experiments.",
            "model_size": "500M",
            "task_name": "Aggregate across VE, MM, MG, PP",
            "task_description": "Authors compare average improvement percentages across tasks for pool sizes 1, 3, 5.",
            "problem_format": "Dynamic prompt pool of variable size (number of distinct templates used during training)",
            "format_category": "prompt style / prompt diversity",
            "format_details": "Pools constructed by selecting templates with pairwise dissimilarity &gt; 0.5; experiments compare pool size = 1, 3, 5 and report increasing performance with larger pool size (visualized in Figure 3).",
            "performance_metric": "Average improvement percentage across four tasks (relative to PFt-CLS)",
            "performance_value": "Qualitative/aggregate: performance increases as pool size increases from 1→3→5 (numeric per-task breakdown not fully enumerated in excerpt; Figure 3 shows monotonic improvement).",
            "baseline_performance": "Pool size = 1 (equivalent to fixed single prompt)",
            "performance_change": "Authors report monotonic improvement; DynaMaR with pool size 5 used in main reported results (few-shot Avg +10.0%).",
            "experimental_setting": "Prompt dissimilarity threshold 0.5 used to select diverse templates; candidates generated 20 per task then 5 selected for pool in main experiments.",
            "statistical_significance": null,
            "uuid": "e7421.7",
            "source_info": {
                "paper_title": "DynaMaR: Dynamic Prompt with Mask Token Representation",
                "publication_date_yy_mm": "2022-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Cutting down on prompts and parameters: Simple few-shot learning with language models",
            "rating": 2,
            "sanitized_title": "cutting_down_on_prompts_and_parameters_simple_fewshot_learning_with_language_models"
        },
        {
            "paper_title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "rating": 2,
            "sanitized_title": "pretrain_prompt_and_predict_a_systematic_survey_of_prompting_methods_in_natural_language_processing"
        },
        {
            "paper_title": "Making pre-trained language models better few-shot learners",
            "rating": 2,
            "sanitized_title": "making_pretrained_language_models_better_fewshot_learners"
        },
        {
            "paper_title": "Adapting language models zero-shot learning by meta-tuning on dataset and prompt collections",
            "rating": 2,
            "sanitized_title": "adapting_language_models_zeroshot_learning_by_metatuning_on_dataset_and_prompt_collections"
        },
        {
            "paper_title": "How can we know what language models know?",
            "rating": 1,
            "sanitized_title": "how_can_we_know_what_language_models_know"
        }
    ],
    "cost": 0.01505575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DynaMaR: Dynamic Prompt with Mask Token Representation</p>
<p>Xiaodi Sun xiaodisun315@gmail.com 
Sunny Rajagopalan sunny.rg@gmail.com 
Priyanka Nigam nigamp@amazon.com 
Weiyi Lu weiyilu@amazon.com 
Yi Xu 
Iman Keivanloo imanke@amazon.com 
Belinda Zeng zengb@amazon.com 
Trishul Chilimbi trishulc@amazon.com 
Microsoft 
Google 
Amazon 
DynaMaR: Dynamic Prompt with Mask Token Representation
C7A1727FB2DD07DDABD33D746F968B23
Recent research has shown that large language models pretrained using unsupervised approaches can achieve significant performance improvement on many downstream tasks.Typically when adapting these language models to downstream tasks, like a classification or regression task, we employ a finetuning paradigm in which the sentence representation from the language model is input to a task-specific head; the model is then fine-tuned end-to-end.However, with the emergence of models like GPT-3, prompt-based fine-tuning has been proven to be a successful approach for few-shot tasks.Inspired by this work, we study discrete prompt technologies in practice.There are two issues that arise with the standard prompt approach.First, it can overfit on the prompt template.Second, it requires manual effort to formulate the downstream task as a language model problem.In this paper, we propose an improvement to prompt-based finetuning that addresses these two issues.We refer to our approach as DynaMaR -Dynamic Prompt with Mask Token Representation.Results show that DynaMaR can achieve an average improvement of 10% in few-shot settings and improvement of 3.7% in data-rich settings over the standard fine-tuning approach on four e-commerce applications.</p>
<p>Introduction</p>
<p>Unsupervised pre-trained Language Models (LMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) have achieved state-of-the-art performance on many natural language understanding tasks.In general, these models are fine-tuned for different tasks through the addition of a taskspecific head on top of the [CLS] token representation (Scao and Rush, 2021).</p>
<p>An alternative method to applying LMs on downstream tasks is through discrete prompts.A discrete prompt is an additional text phrase inserted along with the original input text that encapsulates the task of interest.By adding the prompt, we convert the downstream task into a masked language (MLM) problem.For example, to classify the sentiment of a movie review, "I hate this movie.",we can append a prompt to the input to get "I hate this movie.It was [MASK]".The pre-trained language model is thus prompted to identify the sentiment of the input statement and classify the [MASK] token as "terrible" instead of "great" (Liu et al., 2021).In this paper, we call a function that includes a prompt and its position information a prompt template.</p>
<p>Prompt-based approaches have shown success in low-data regimes (Petroni et al., 2019;Schick and Schütze, 2021;Jiang et al., 2020;Gao et al., 2021;Lester et al., 2021).Prompt-based fine-tuning is beneficial in few-shot learning, because it provides extra task information to the model through the prompt text (Schick and Schütze, 2021).However, when we explore this technique in practice, two issues have arisen.First, the trained model can overfit on words or phrases within the prompt and on the position of the [MASK] token in the prompt (Zhong et al., 2021).For example, in movie review sentiment analysis, when we append the prompt, "Does the user like the movie?[MASK]", to a negative review, "This is a bad movie.", the trained model is inclined to predict the positive class, because the word "like" frequently appears in positive reviews and the masked language model has greater attention on the words/phrases that are closer to the mask token as shown in Figure 1.We call this issue prompt-related overfitting in this work.</p>
<p>We tackle prompt-related overfitting by introducing a dynamic prompt approach.In this approach, we create a prompt pool consisting of multiple prompt templates.To construct this pool, we generate a set of prompt candidates and filter by a similarity score we propose, called the pairwise prompt dissimilarity score (detailed in Section 3).We then introduce the dynamic component of the algorithm by randomly selecting a prompt template from the pool and applying to the input for each training step.For example, in the movie review sentiment analysis task, the trained model will randomly see either "does the user like the movie?[MASK]" or "does the user dislike the movie?[MASK]" appended to the original input.This prevents the model to overfit on spurious correlations between words in the prompt and the class label.</p>
<p>In addition, as previously mentioned, the standard prompt-based fine-tuning setup can be inefficient.It requires significant input and answer engineering to reformulate the downstream tasks as MLM problems (Liu et al., 2021).This process is time-consuming especially for tasks with large numbers of classes.Besides, another disadvantage of the standard setup is that it cannot be directly applied to regression problems, as they cannot be easily converted to MLM problems.To simplify this process, we fine-tune the model by feeding the mask token representation into a task-specific classifier/predictor head instead of the pre-trained MLM head to avoid the answer engineering process, as shown in Figure 2. We refer to our promptbased approach with these two improvements as Dynamic Prompt with Mask Token Representation (DynaMaR).We apply DynaMaR to both few-shot and data-rich settings and, for the first time, show improvement gains across four tasks not only in few-shot settings but also in data-rich settings.</p>
<p>Our contributions include: (1) proposing Dyna-MaR, which can be applied without reformulating downstream tasks into language problems and is robust to prompt-related overfitting, (2) showing DynaMaR can achieve improvements in both fewshot and data-rich settings, (3) proposing a prompt dissimilarity score to evaluate the degree of dissimilarity between two prompt templates and to help construct a diverse dynamic prompt pool, (4) demonstrating that a larger dynamic prompt pool achieves better performance on downstream tasks.</p>
<p>Related Work</p>
<p>Our work can be divided into three components: language model fine-tuning, prompt generation, and the design of the prompt template.</p>
<p>Language Model Fine-tuning is the main focus of our work.Recently, a large amount of research has focused on improved language model finetuning methods (Howard and Ruder, 2018;Dodge et al., 2020;Lee et al., 2020;Zhang et al., 2021).These works mainly focus on optimization and regularization techniques to stabilize finetuning.In contrast to these works, Gao et al. (2021) describe the concept of prompt-based fine-tuning for language models.We adapt and simplify the core ideas from this work to create a simple yet efficient prompt-based fine-tuning approach.</p>
<p>Prompt Generation is a key process in promptbased fine-tuning.The choice of prompt significantly influences performance.The most natural way to generate prompts is through manual design.Petroni et al. (2019) employ manually generated prompts with ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) models.They evaluate on the LAMA (LAnguage Model Analysis) benchmark (Bordes et al., 2013;Nickel et al., 2016) without fine-tuning and conclude that the model is able to recall knowledge learned from the pre-training tasks.While manually crafting prompts is intuitive, creating effective prompts through manual effort requires time, experience, and expertise.To address this issue, a number of automatic prompt searching methods have been proposed.For example, Jiang et al. (2020) propose a data mining-based method that searches for a prompt based on the shortest path between the original inputs and answers.They also propose paraphrasing-based methods that take a seed prompt and paraphrase it into several semantically similar expressions.Gao et al. (2021) treat prompt generation as a text generation task and utilize T5, a sequence-to-sequence pretrained model, in the template search process.They generate templates by specifying the position to insert a prompt template and then inputting samples into T5 to decode the templates.These automatic approaches achieve comparable performance to manually designed prompts.Besides, Logan IV et al. (2021) propose the null prompt method.Instead of generating prompts, they concatenate a [MASK] token with original inputs and it performs competitively to manually designed prompts.In our experiments, we utilize the prompt generation methods to create candidates for the dynamic prompt pool, while also including the null prompt approach as one of the baselines.</p>
<p>Prompt Template Design Factors are the factors that we take into consideration to create a metric that informs how prompts are selected for the dynamic prompt pool.Numerous previous works analyze prompt template design factors and the impact of prompt design on performance.Liu et al. (2021) summarize the factors that influence the application of prompt-related technologies in language models.Logan IV et al. ( 2021) note that the order in which the original input and the [MASK] token are concatenated is an important consideration.Zhong et al. (2021) propose to unify the prompts into a question-answering format.These previous works indicate that prompt construction impacts performance.To this end, we hypothesize that diversity in the set of prompt templates is an important factor in the performance of the model and propose a prompt dissimilarity score for measuring diversity.</p>
<p>Our Method: DynaMaR</p>
<p>In this section, we describe details of our approach, DynaMaR.Before explaining the training process, we define two concepts: the dynamic prompt pool and the inference prompt.</p>
<p>Dynamic Prompt Pool is a pool of prompt templates from which a prompt template will be randomly selected and applied to the input during training.</p>
<p>Inference Prompt is the prompt template used during inference.It is selected from the set of templates in the dynamic prompt pool.In general, it is the prompt template among those in the dynamic prompt pool that can achieve the highest performance in a fixed prompt setting.</p>
<p>We generate the candidates for the dynamic prompt pool and inference prompt through manual generation and paraphrasing-based methods proposed by Jiang et al. (2020).However, we do not include all candidates in the dynamic prompt pool.We want to ensure the prompts within a pool are sufficiently diverse so that the model will not overfit on any of them.Therefore, we introduce a prompt dissimilarity score to measure the level of dissimilarity between these candidates.We consider three factors in developing this metric: (1) prompt position, or whether to append or prepend the prompt to the input or even insert into the middle of pairwise inputs, (2) prompt wording or the prompt word selection, and (3) prompt format, or whether to create prompts in statement format or in the question-answering format proposed by Zhong et al. (2021).To define the prompt dissimilarity score, we first introduce the normalized Hamming distance and the normalized Levenshtein distance.</p>
<p>Normalized Hamming Distance is equal to the number of different bits between two binary representations divided by the length of the binary representations (Norouzi et al., 2012).Let HD(b i , b j ) be the Hamming distance between binary representations b i and b j with equal length K.The equation of normalized Hamming distance N HD(b i , b j ) then follows:
HD(b i , b j ) = K k=1 |b ik − b jk |,(1)N HD(b i , b j ) = HD(b i , b j )/K. (2)
Normalized Levenshtein Distance is equal to the minimum number of operations (substitution, insertion and deletion) required to transform a given string into another string divided by the length of the longer string and is calculated in a recursive fashion (Yujian and Bo, 2007).Let LD(s i , s j ) be the Levenshtein distance between string s i and s j .Let |s i | and |s j | be the length of prompt string s i and s j , respectively.Let t(x) be a function that keeps a string of all but the first character of x.The equation of the normalized Levenshtein distance N LD(s i , s j ) follows:
LD(si, sj) =                  |si|, if |si| = 0; |sj|, if |sj| = 0; LD(t(si), t(sj)), if |si| = |sj|; 1+ min LD(t(s i ),s j ), LD(s i ,t(s j )), LD(t(s i ),t(s j ))
, otherwise.</p>
<p>(3)
N LD(si, sj) = LD(s i ,s j ) |s i | , if |sj| ≤ |si|, LD(s i ,s j ) |s j | , if |si| &lt; |sj|.(4)
Suppose we generate N prompt templates.Let p i and p j be two prompt templates with s i , s j as prompt strings, respectively, where i = j and i, j ∈ {1, 2, . . ., N }.We treat the prompt position and format information as categorical variables and convert them into binary representations, b i , b j .Let P DS(p i , p j ) denote the prompt dissimilarity score between prompt templates p i and p j .The prompt dissimilarity score equation can be found below:
P DS(p i , p j ) = N HD(b i , b j ) + N LD(s i , s j ).
(5) In our experiment, we use 0.5 as the pairwise prompt dissimilarity score threshold.We add the prompt templates that have prompt dissimilarity score larger than the threshold to others to a dynamic prompt pool.During the training process, we randomly pick one prompt template from the pool for each training step and apply it to the original input.We treat the mask token representation from the modified input as the sentence embedding and train the model by directly feeding it into a task-specific predictor head.</p>
<p>Experiment</p>
<p>Data</p>
<p>In this experiment, we use four e-commerce proprietary datasets: (1) Variation Elimination (VE), (2) Music Match (MM), (3) Music Genre (MG), and (4) Price Prediction (PP).VE is a binary classification problem with pairwise-document inputs where the label identifies whether two items are the variations of the same product or not.For example, similar shirts (from the same producer and brand) in different sizes or colors are considered to be variations.MM is a binary classification problem with pairwise-document inputs that identifies whether two music tracks from different sources are the same or not.MG is a 303-way classification problem with single-document inputs that classifies music tracks to genres.PP is a regression problem with single-document inputs that aims to estimate the sales price based on the product information.It should be noted that the percentage of inputs with number of tokens larger than 512 in VE, MM, MG, PP are 90%, 75%, 82%, 1%, respectively.</p>
<p>For each task, we split the dataset into three parts: (1) train, (2) validation, and (3) test.We use the full training dataset for the data-rich settings.We also sample multiple few-shot training datasets for fewshot learning settings.In few-shot learning, each classification dataset contains roughly 20 samples for each class.For the regression task (i.e., PP), we randomly sample 1% of the full training dataset as a few-shot training dataset.</p>
<p>Model and Tokenizer Setup</p>
<p>For training the tokenizer, we collect an English product catalog dataset with text features including title, description, and detail bullet points.We train a 32K BPE vocabulary on this dataset using the SentencePiece library (Kudo and Richardson, 2018).</p>
<p>We create a 500M parameter transformer encoder-only model, with 38 hidden layers, 1024 embedding size, 16 attention heads, and maximum sequence length of 512.We train the model using the LANS optimizer (Zheng et al., 2020) with a batch size of 8192 and a learning rate of 10 −4 on the product catalog dataset.</p>
<p>Prompt Generation and Selection</p>
<p>To create the dynamic prompt pool for our tasks, we first generate 20 prompt templates for each task and select 5 out of them using the prompt dissimilarity score.Specifically, for each task, we first manually design 10 prompt templates.By treating prompt template generation as paraphrase generation task (Jiang et al., 2020), we use these 10 prompt templates as seeds to generate another 10 templates per task by leveraging the public T5 paraphrase generation model from Hugging Face1 .Afterwards, we use the prompt dissimilarity score to select 5 prompt templates out of the 20 based on the method discussed at the end of Section 3. The selected prompt templates are used as each task's dynamic prompt pool.For inference, we evaluate each template in the dynamic prompt pool through the evaluation process discussed in Section 4.5, and select the prompt template that produces the performance on each task.Table 5 shows the dynamic prompt templates as well as the inference prompt selected for each task.</p>
<p>Fine-tuning (Ft) Methods</p>
<p>We compare DynaMaR with the following approaches:</p>
<p>• Promptless Fine-tuning -CLS (PFt-CLS) is our baseline approach where we fine-tune the model by feeding the [CLS] token representation into a predictor head.</p>
<p>• Promptless Fine-tuning -Average Pooling (PFt-Avg) fine-tunes the model by using the average of sequence output for prediction.</p>
<p>• Null Prompt -Prefix (NP-Prefix) prepends the [MASK] token to the original inputs and fine-tunes the model by feeding the [MASK] token representation into a predictor head.This approach avoids the issue where the model overfits the prompt template since it does not require a template.</p>
<p>• Null Prompt -Suffix (NP-Suffix) is the same as the above approach except that the [MASK] token is appended to the inputs instead of being prepended.</p>
<p>• Fixed Prompt with Mask Token Representation (FiTeR) utilizes a static prompt template in both the training and inference stages and fine-tunes the model by feeding the [MASK] token representation into a predictor head.</p>
<p>Note that we use a task-specific predictor head in combination with all above approaches including the prompt-based fine-tuning methods, which typically use the pre-trained MLM head for prediction.The reason is that we have a regression task as one of our evaluation datasets, and as already discussed in Section 1, it is not straight forward to convert regression tasks into MLM tasks.</p>
<p>Model Training and Evaluation Setup</p>
<p>As mentioned in Section 1, we measure the performance in both few-shot and data-rich settings.For both VE and MM, we use Area Under the Precision-Recall Curve (PRAUC) as the evaluation metric.For MG, we use classification accuracy as the evaluation metric.For PP, we use Root Mean Square Error (RMSE) as the evaluation metric.We validate the performance every 2 training steps in the few-shot settings and every 100 steps in the data-rich settings.We use early stopping with a patience of 3 validation steps to select the best model for each task.We then evaluate the best models on the test datasets.For few-shot learning, we report the average performance across multiple few-shot datasets per task to reduce the variation in performance.In Table 1 and Table 2, we calculate and report the improvement percentage, which is the ratio of positive change as compared to PFt performance.</p>
<p>Results</p>
<p>Table 1 and 2 show the performance results for both few-shot and data-rich settings.In both settings, PFt-Avg shows degradation in average of  performance compared to PFt-CLS.This shows that average pooling generates worse sentence representations than does taking the [CLS] token representation.</p>
<p>In contrast, both null prompt approaches show improvement in average performance compared to PFt-CLS in both few-shot and data-rich settings.The improvement could be a result of aligning the format of the downstream tasks and that of the pretraining task.By changing the input format to be similar to that of the MLM task, we reduce the amount of data that are required to coach the model to learn the new task.</p>
<p>Also, there is a difference in the performance of NP-suffix and NP-prefix.This is likely due to the positional differences of the [MASK] token in the two methods.For example, suppose we want to perform sentiment analysis on a sentence like "I love the movie".Prepending or appending the [MASK] token would result in different distances between [MASK] and the word "love", which holds the key information for classification.Such positional differences could lead to different performance even though the two methods are very similar in spirit.</p>
<p>Another observation is that FiTer shows higher improvement in average of performance compared to null prompt approaches.Recall that FiTer introduces task information through the prompt templates, while the null prompt approaches do not, which supposedly addresses the issue where the model overfits the prompt templates.Hence, the results show that the benefits of adding the extra task information outweigh the possible performance loss caused by the prompt-related overfitting issue.</p>
<p>Finally, DynaMar outperforms FiTer on all tasks in both setting, with the only exception being MG in the data-rich setting.This indicates that increasing the diversity of prompt templates used during training will improve model generalization.We also observe that DynaMar does not show significant improvement over PFt-CLS on both MG and VE.This is because both tasks contain a large num-ber of documents with length longer than 512, as mentioned in Section 4.1.As a result of this, we need to truncate more of the original inputs for these tasks in order to insert prompts, which can lead to information loss.Thus, DynaMar is less efficient in problems with long documents.</p>
<p>Analysis</p>
<p>Larger dynamic prompt pool, better performance.The size of the dynamic prompt pool influences the performance.We compare the average improvement percentage across four tasks with the size of dynamic prompt pool = 1, 3, 5 (prompt information can be found in Appendix A).From Figure 3, we can see that performance improves as the dynamic prompt pool is made larger.</p>
<p>Limitations and Future Directions</p>
<p>As mentioned in Section 4.6, our method does not show substantial improvement on tasks involving long documents.Besides, the threshold of prompt disimilarity score can be treated as a parameter.This work lack of a study on the effect of this threshold.In addition, we focus on e-commerce related English classification/regression tasks in this work, the performance of our method in other nature language processing use cases remains unexplored.As a next step, we will conduct additional studies on these three topics.</p>
<p>Conclusion</p>
<p>In this work, we discuss methods for generating prompts and propose a way to select prompt templates to include in the dynamic prompt pool.Also, we show that using the mask representation of a prompt either equals or improves upon the performance of standard fine-tuning on four e-commerce applications in both few-shot and data-rich settings.In addition, we find DynaMaR outperforms the fixed prompt approach in both settings.Furtherwe show that a larger dynamic prompt pool leads to improved model performance when employing DynaMaR.</p>
<p>Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. 2021.Adapting language models zero-shot learning by meta-tuning on dataset and prompt collections.In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>A Dynamic Prompt Pool with Different Sizes</p>
<p>We need to define two prompt-related parameters while using DynaMaR: the dynamic prompt pool and the inference prompt.The list of prompts in the pool and the inference prompt selected for dynamic prompt pool sizes of 1, 3, and 5 can be found in Table 3, Table 4, and Table 5, respectively.</p>
<p>Figure 1 :
1
Figure 1: BERT Attention Distribution.The figure shows that the MLM model puts greater attention on the prompt than the original input.</p>
<p>Figure 2 :
2
Figure 2: Fine-tuning approach demonstration.</p>
<p>Figure 3 :
3
Figure 3: Pool Size vs Improvement Percentage.</p>
<p>Table 1 :
1
Few-shot Learning Performance Comparison.
Ft Method VEMMMGPPAvgPFt-CLS00000PFt-Avg-1.5%+7.2%-3.7% -8.8%-1.7%NP-Prefix-1.0%+4.1%-2.0% +2.6%+0.9%NP-Suffix-2.6%+0.2%-1.6% +6.7%+0.7%FiTeR-0.7%+13.9%-1.1% +7.3%+4.9%DPMR+0.8% +15.8% -0.5% +23.8% +10.0%</p>
<p>Table 2 :
2
Data-rich Performance Comparison.</p>
<p>https://huggingface.co/Vamsi/T5_Paraphrase_ Paws
(5
Translating embeddings for modeling multirelational data. Antoine Bordes, Nicolas Usunier, Alberto García-Durán, Jason Weston, Oksana Yakhnenko, Conference on Neural Information Processing Systems (NeurIPS). 2013</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). 2019</p>
<p>Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, Noah A Smith, ArXiv. 2020</p>
<p>Making pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, Association for Computational Linguistics (ACL). 2021</p>
<p>Universal language model fine-tuning for text classification. Jeremy Howard, Sebastian Ruder, Association for Computational Linguistics (ACL). 2018</p>
<p>How can we know what language models know?. Zhengbao Jiang, Frank F Xu, J Araki, Graham Neubig, Association for Computational Linguistics (ACL). 2020</p>
<p>Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. Taku Kudo, John Richardson, Conference on Empirical Methods in Natural Language Processing (EMNLP). 2018</p>
<p>Mixout: Effective regularization to finetune large-scale pretrained language models. Cheolhyoung Lee, Kyunghyun Cho, Wanmo Kang, International Conference on Learning Representations (ICLR). 2020</p>
<p>The power of scale for parameter-efficient prompt tuning. Brian Lester, Rami Al-Rfou, Noah Constant, Conference on Empirical Methods in Natural Language Processing (EMNLP). 2021</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ArXiv2021</p>
<p>Roberta: A robustly optimized bert pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, ArXiv. 2019</p>
<p>Cutting down on prompts and parameters: Simple few-shot learning with language models. I V Robert L Logan, Ivana Balavzevi'c, Eric Wallace, Fabio Petroni, Sameer Singh, Sebastian Riedel, Conference on Neural Information Processing Systems (NeurIPS). 2021</p>
<p>A review of relational machine learning for knowledge graphs. Maximilian Nickel, Kevin P Murphy, Evgeniy Volker Tresp, Gabrilovich, Proceedings of the IEEE. the IEEE2016</p>
<p>Hamming distance metric learning. Mohammad Norouzi, David J Fleet, Ruslan Salakhutdinov, Conference on Neural Information Processing Systems (NeurIPS). 2012</p>
<p>Deep contextualized word representations. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). 2018</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. EMNLP-IJCNLP2019</p>
<p>How many data points is a prompt worth?. Le Teven, Alexander M Scao, Rush, Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). 2021</p>
<p>It's not just size that matters: Small language models are also few-shot learners. Timo Schick, Hinrich Schütze, Conference of the North American Chapter of the Association for Computational Linguistics -Human Language Technologies (NAACL-HLT). 2021</p>
<p>A normalized levenshtein distance metric. Li Yujian, Liu Bo, IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI). 2007</p>
<p>Revisiting fewsample bert fine-tuning. Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations (ICLR). 2021</p>
<p>Accelerated large batch optimization of bert pretraining in 54 minutes. Shuai Zheng, Haibin Lin, Sheng Zha, Mu Li, ArXiv2020</p>            </div>
        </div>

    </div>
</body>
</html>
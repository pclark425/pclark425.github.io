<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8753 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8753</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8753</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-267751249</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.11436v2.pdf" target="_blank">Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement</a></p>
                <p><strong>Paper Abstract:</strong> Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others. We discovered that such a contrary is due to LLM's bias in evaluating their own output. In this paper, we formally define LLM's self-bias - the tendency to favor its own generation - using two statistics. We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks. We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks. Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias. To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks. The code and data are released at https://github.com/xu1998hz/llm_self_bias.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8753.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8753.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_self-refine_MT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 iterative self-refinement on machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source GPT-4 was used as both generator and evaluator in an iterative generate-feedback-refine pipeline on low-to-medium resource Flores-200 translation directions; the paper finds GPT-4 exhibits self-bias that amplifies across iterations: GPT-4 perceives improvements that are not borne out by reference BLEURT scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source OpenAI multimodal/LLM (exact parameter count not specified in paper); strong instruction-following capability as used via OpenAI API.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate initial translation; prompt same LLM to produce MQM-style feedback (error location/type/severity) on the candidate; condition on candidate+feedback to produce a refined output; accept refined output only if the LLM's self-feedback score indicates improvement. Iterations run up to 10 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (Flores-200 low/med-resource directions: Yor→En, Jav→En, Arm→En, Ig→En)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate sentences from low-to-medium resource languages into English; evaluation via BLEURT mapped to MQM-like -25..0 scale (quantile-mapped) and some human MQM annotations for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Paper reports that BLEURT (mapped to MQM) does not show true quality improvement across iterations for GPT-4, while GPT-4's self-evaluated scores increase (false positive perceived improvement). No numeric improvement in BLEURT is reported; human MQM validations show bias amplification (higher bias and distance skewness after 10 iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Initial-generation BLEURT / mapped MQM scores used as baseline; paper reports little-to-no true BLEURT improvement after iterative self-refine despite model's self-reported improvements. (Exact BLEURT numbers per iteration not given in main text.)</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered MQM-style feedback (in-context examples), LLM-as-evaluator, iterative conditional generation accepting refinements only when self-feedback score improves.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative evidence shows self-feedback scores increase across iterations for GPT-4 but mapped BLEURT (reference-based) does not improve; human MQM annotations on samples confirm the LLM's self-evaluation inflates scores (bias and distance-skewness increase).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-bias amplification: GPT-4 tends to favor outputs in its own style and optimizes toward false positive corrections; iterative self-refinement increases bias and does not yield true translation quality gains. Example case study shows large increases in LLM's self-score while human score remains low.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared against external feedback (InstructScore) where external feedback reduced bias and elicited BLEURT improvement; GPT-4 shows more robustness than smaller/simpler models but still succumbs to bias amplification under self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Paper shows external feedback (oracle InstructScore) reduces bias and yields true BLEURT improvement for GPT-4; larger models (discussion) are more resilient but still show self-bias early.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8753.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8753.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5_self-refine_MT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo iterative self-refinement on machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Openly-accessed GPT-3.5-Turbo was evaluated in the same self-refine pipeline on Flores-200 language pairs and shows higher self-bias amplification than stronger models (GPT-4/Gemini), with self-reported improvements not matched by reference metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source OpenAI model (Turbo family) used via API; instruction-tuned but weaker than GPT-4 in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same iterative generate-feedback-refine loop using MQM-style prompts and accepting refinements only when self-feedback indicates improvement; experiments run up to 10 iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (Flores-200 low/med-resource directions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate low-to-medium resource languages into English; evaluate with BLEURT mapped to MQM and occasional human MQM annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Paper reports GPT-3.5-Turbo exhibits amplified self-bias through iterations; self-evaluated improvements are larger than true BLEURT improvements. True performance (BLEURT / mapped MQM) shows saturation and no meaningful gains after several iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline initial-generation performance (no self-refine) serves as comparison; paper states true improvements saturate (e.g., coverage and BLEURT do not increase commensurately), exact numeric baselines not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted MQM-style feedback via in-context examples; LLM-as-evaluator; iterative conditional refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative curves show self-reported scores rising faster than true BLEURT; human MQM evaluation on samples confirms increased bias and skewness after 10 iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Higher bias amplification relative to GPT-4 and Gemini; self-refinement can drive false-positive optimization and does not reliably improve true translation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared qualitatively to GPT-4 and Gemini; more biased than GPT-4/Gemini. External feedback reduces bias for models including GPT-3.5-Turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Noted effect that smaller/less capable models (like GPT-3.5-Turbo) show larger bias amplification; external feedback helps reduce bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8753.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8753.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini_self-refine_MT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini iterative self-refinement on machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google Gemini (gemini‑pro) used in self-refine experiments on Flores-200 pairs; shows some robustness relative to smaller models but still exhibits self-bias amplification across iterations and distributional skew shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini (gemini-pro)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source Google Gemini model used via API; strong instruction-following capabilities per authors' selection.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate initial output, obtain MQM-style feedback from same LLM, produce refined output conditionally; accept only if self-feedback indicates improvement; experiments up to 10 iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (Flores-200 low/med-resource directions)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate low-to-medium resource languages to English; evaluate with BLEURT mapped to MQM and human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Paper reports Gemini perceives improvements across iterations (self-feedback rises) while BLEURT often does not improve; Gemini shows shifts in skewness (right→left in Jav→En), indicating complex bias dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline generation performance (no self-refine) compared to iterative self-refine; true BLEURT improvements not observed despite self-reported gains.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based MQM-style self-evaluation and iterative refinement; Gemini receives only last-iteration output/feedback at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Self-feedback scores increased across iterations but reference BLEURT did not match these perceived improvements; distance-skewness trends described (decrease then increase) indicate bias amplification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Gemini sometimes refuses to answer sensitive content causing lower format accuracy on subset of examples; still vulnerable to self-bias amplification over iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to GPT-4 and GPT-3.5-Turbo; Gemini is more robust than some open-source models but still exhibits amplified self-bias; external feedback reduces bias for Gemini as well.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Appendix notes skewness dynamics per language pair; external feedback experiments show bias reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8753.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8753.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA2_size_ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA2 (7B/13B/70B) self-refine size ablation on translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source LLaMA2-chat models of 7B, 13B, and 70B parameters were compared on Yor→En translation to study effect of model size on self-bias amplification; larger 70B shows less amplification and plateaus after ~5 iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA2-7B / LLaMA2-13B / LLaMA2-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned LLaMA2 chat models with 7B, 13B, and 70B parameters respectively (checkpoints cited in Appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-refine (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same generate-feedback-refine loop using MQM-style feedback prompts; iterations up to 10; bias and dSkew measured across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (Flores-200 Yor→En experiments reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Yoruba→English translation on Flores-200 subset with BLEURT (quantile-mapped) and bias/dSkew estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>LLaMA2-70B exhibits self-bias initially but bias plateaus after the 5th iteration; smaller 7B and 13B models continue to amplify self-bias in later iterations. True BLEURT improvements occur with larger models when paired with external feedback (general trend reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline (0th iteration) used for comparison; smaller models had greater bias growth from baseline across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-based MQM-style self-evaluation and iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Size-dependent curves: 70B shows less bias amplification and plateau, suggesting larger models are more resilient to self-bias; reported qualitatively and shown in Figure 10.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even 70B exhibits self-bias early; plateaus but does not fully eliminate bias without external feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared sizes directly; larger models outperform smaller ones in resisting bias amplification. External feedback further improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Model-size ablation demonstrates monotonic relationship: larger parameter count → reduced self-bias amplification (qualitative/curve evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8753.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8753.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CommonGen_self-refine</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-refinement (iterative missing-concept feedback) on CommonGen Hard</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs (GPT-4, GPT-3.5-Turbo, Gemini) were used to self-evaluate missing concept coverage via ICL examples and iteratively refine outputs; self-estimated coverage rises faster than true coverage and performance saturates around iteration 5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5-Turbo, Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source instruction-tuned models accessed via APIs; GPT-4 and Gemini stronger than GPT-3.5-Turbo per experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-refine with missing-concept feedback (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Initial generation from ~30 concepts; provide two in-context learning (ICL) feedback examples where LLM lists missing concepts; model revises output to improve concept coverage; accept revisions only when LLM feedback score improves. Iterations up to 10.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Constrained text generation (CommonGen Hard)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate fluent/commonsense sentences covering a list of concepts; evaluation is strict string-match full-coverage metric (1 if all concepts appear, else 0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>True full-coverage (string-match) improves modestly then saturates (e.g., GPT-3.5 reaches ~20% coverage around iteration 5), while LLM self-estimated coverage rises much faster; self-bias (difference between self-score and true coverage) amplifies across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Initial-generation coverage baseline (iteration 0) used; iterative self-refine yields modest true coverage gains but larger self-estimated gains (leading to bias amplification). Exact numeric coverage values are reported qualitatively (e.g., GPT-3.5 ~20% at iter 5).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompted ICL examples for missing-concept detection; LLM-as-evaluator using explicit binary coverage feedback; iterative refinement conditioned on feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative coverage curves: some true coverage improvement but rate and magnitude much smaller than self-reported improvements; saturation after ~5 iterations; plots shown in Figure 5.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>LLMs overestimate their improvements (self-bias); refinement can saturate quickly; GPT-4 has higher initial coverage and therefore fewer easy gains leaving it to incorrectly judge small missing items.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Not compared to chain-of-thought; compared qualitatively across models (GPT-4 vs GPT-3.5-Turbo vs Gemini); external feedback not explicitly applied in CommonGen experiments in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Observation that higher initial coverage (GPT-4) leads to different bias dynamics; no formal ablation beyond model comparisons reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8753.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8753.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Math_self-refine_and_self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative self-refinement and self-consistency verification on MATH dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs (GPT-4, GPT-3.5-Turbo, Gemini, open-source models) were evaluated on the MATH dataset using self-refine where feedback is correctness (1/0) and an alternate pipeline replacing self-evaluation with self-consistency majority-vote verification; both pipelines exhibited increased self-bias and skewness across iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4, GPT-3.5-Turbo, Gemini, and others</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source instruction-tuned LLMs accessed via APIs; for math experiments standard prompts produce step-by-step reasoning and final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-refine (binary correctness feedback) and self-consistency (majority-vote verifier)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Self-refine: compare previous solution to ground truth answer and output 1 if matched else 0; iterate refinement only when feedback indicates improvement. Self-consistency variant: sample multiple reasoning paths (10), majority vote to propose an answer; if majority differs from initial, it replaces the initial and yields binary feedback; iterate. Experiments report up to 10 iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Mathematical reasoning (MATH test set)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve math competition-style problems requiring multi-step reasoning; evaluate as exact-match correctness (1 if final answer matches ground truth, else 0).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Paper reports increases in bias and distance-skewness across iterative self-refine and in the self-consistency pipeline; no reliable increase in true accuracy is reported — bias indicates models favor some solutions and create biased ensembles. Exact accuracy numbers per iteration are not provided in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline initial-generation accuracy used; iterative self-refine and self-consistency do not reliably improve true accuracy and instead increase self-bias.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered binary correctness feedback or self-consistency majority-vote verification; iterative conditional refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>10</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Figures show that bias and dSkew increase across iterations for math; self-consistency replacement also increases bias leading to contaminated ensembles rather than improved true performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-consistency and self-refine introduce or amplify bias toward certain reasoning paths; ensembles produced by self-consistency can be biased; models may not correct reasoning errors reliably and may worsen.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Self-consistency used as an alternative evaluator is shown in Appendix D to also increase bias and skewness; compared qualitatively to self-refine and baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Appendix D reports that replacing self-evaluation with self-consistency still yields increased bias and skewness across iterations (qualitative/curve evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8753.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8753.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-rewarding_pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-rewarding pipeline (LLM-as-reward) on translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper studies the self-rewarding pipeline (sample k candidates, use same LLM as reward model to pick top candidate, then train on top samples) and finds that increasing k amplifies self-bias (models more likely to favor own generations), worsening selection bias.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-MOE, Mixtral-MOE, LLaMA2-7B (open-source models in study)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned mixture-of-experts models (Mixtral 8x7B, Deepseek-MOE 16B) and LLaMA2-7B; used to sample k candidates per input.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-rewarding (LLM-as-reward / candidate ranking)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>For each input sample, generate k candidates (k in {1,4,8,16,32}) then use same LLM as reward model to score/rank candidates (MQM-style) and select top-performing samples for further training; measure self-bias of reward scores.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (Yoruba→English subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translation with candidate sampling and LLM-as-reward selection; bias measured between LLM self-scores and BLEURT/human approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Paper reports increased Bias and distance-skewness as k (candidate pool size) increases for all tested open-source models; this indicates stronger self-rewarding amplifies self-bias and can amplify false positives in selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>k=1 (no selection) baseline shows lower bias; as k grows, self-bias increases.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Sampling multiple candidate outputs and using the generating model itself as a reward/scoring function (prompted MQM feedback) to select top candidate(s).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>No improvement; instead reported harm: bias and dSkew increase monotonically with candidate pool size, showing increased likelihood of selecting false-positive 'best' candidates as judged by the biased model.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Self-rewarding pipeline amplifies self-bias especially with larger candidate pools, risking selection of suboptimal samples for downstream training and reinforcing model's own errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Contrasted with external feedback selection (oracle InstructScore) which would avoid this amplification; no training-on-top-selected-samples experiments reported in main text, only bias analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Varying k constitutes an ablation: increasing k leads to increased Bias and dSkew across models (reported in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8753.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8753.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>External_feedback_InstructScore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>External reference-based feedback via InstructScore (oracle) to guide refinement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors used InstructScore (a reference-based feedback model that can access reference text) as an external (oracle-like) evaluator to provide error location/type/severity; external feedback reduced self-bias and elicited true BLEURT improvements during self-refine.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InstructScore (external evaluator) used with GPT-4, GPT-3.5-Turbo, Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>InstructScore is a reference-based feedback model (repo 'xu1998hz/InstructScore' cited) that inspects candidate and reference to provide fine-grained MQM-style annotations (error location/type/severity); treated as oracle feedback in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>external-feedback-guided refinement</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Replace LLM-as-evaluator with InstructScore which supplies MQM-style error annotations (without explicit corrections); LLMs then refine outputs using that external feedback. Experiments run across 5 refinement steps in figure cited.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation (Yoruba→English subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translation evaluation comparing BLEURT, self-feedback evaluated performance, and external feedback evaluated performance across iterative refinement steps.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>With external InstructScore feedback, all tested LLMs (GPT-4, GPT-3.5-Turbo, Gemini) show reduced bias estimates and consistent BLEURT improvements across iterations (true quality improves), as external feedback converges toward BLEURT assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Self-feedback (LLM-as-evaluator) baseline shows bias amplification and no BLEURT improvement; external feedback yields better outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Use of an external reference-based evaluator (InstructScore) to produce MQM-style feedback; prompt parsing ensures same format as self-feedback; LLMs then refine conditioned on this external feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Figures and a case study (Table 4) show InstructScore-guided refinements reduce Bias and dSkew and produce monotonic BLEURT improvements across iterations; a concrete Yoruba→English example shows human score improving from -11 to -1 across refinements when guided by InstructScore.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>InstructScore is oracle-like because it has access to references; while it does not provide explicit corrections (only annotations), its oracle access limits direct generalization to settings without references. The refinement still relies on the LLM's internal correction capability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Direct comparison: external InstructScore vs LLM self-feedback. External feedback consistently reduces bias and elicits true BLEURT improvement while self-feedback amplifies bias and produces false-positive perceived gains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Using external feedback versus self-feedback is presented as an ablation/mitigation: external feedback curves (dotted) lie below self-feedback curves (solid), indicating reduced bias and improved BLEURT when external feedback applied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8753.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8753.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-bias_definition_and_metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formal definition of self-bias using statistical bias and distance skewness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper formally defines LLM self-bias as the tendency to favor its own generations and proposes two metrics: Bias(θ) (mean difference between model's quality prediction and true quality) and distance skewness (dSkew) to measure asymmetry of E[θ]-θ distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (metric / analytic contribution)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model; analytic/statistical definitions applied to LLM evaluation outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>self-bias estimation (Bias(θ) and distance skewness)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Bias(θ) = average (E[θi] - θi) across samples where E[θ] is model's predicted quality and θ is human or proxy true quality; distance skewness dSkew_n(X) measures asymmetry of E[θ]-θ distribution to capture directional bias not shown by mean alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Applied across tasks: translation (MQM/BLEURT), constrained generation (coverage), mathematical reasoning (binary correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Metrics used to analyze self-refine and self-rewarding pipelines across Flores-200, CommonGen Hard, and MATH datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Not applicable (metrics measure bias). Paper reports these metrics increase (bias and dSkew) across iterations for many models/tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Used to compare baseline (iteration 0) vs iterative self-refine; Bias and dSkew increase after iterative self-refine in most experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Analytic measurement computed from LLM's self-evaluations and reference/human or proxy true scores (BLEURT quantile-mapped to MQM for translation; binary correctness for math; string-match coverage for CommonGen).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Not an improvement method; provides quantitative evidence that self-feedback inflates scores (Bias>0) and that distributional asymmetry grows (dSkew increases) through iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Bias(θ)=0 can mask asymmetric distributions, hence the need for dSkew; both metrics rely on a valid true-quality signal (human or reliable proxy). When proxies are imperfect (e.g., BLEURT approximations), conclusions require validation (authors used some human MQM checks).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Designed to complement mean-bias measures; distance skewness specifically intended to reveal asymmetry missed by mean bias.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models cannot self-correct reasoning yet <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models <em>(Rating: 2)</em></li>
                <li>Self-rewarding language models <em>(Rating: 2)</em></li>
                <li>LLMs as narcissistic evaluators: When ego inflates evaluation scores <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 1)</em></li>
                <li>Can large language models really improve by self-critiquing their own plans? <em>(Rating: 1)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8753",
    "paper_id": "paper-267751249",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "GPT-4_self-refine_MT",
            "name_full": "GPT-4 iterative self-refinement on machine translation",
            "brief_description": "Closed-source GPT-4 was used as both generator and evaluator in an iterative generate-feedback-refine pipeline on low-to-medium resource Flores-200 translation directions; the paper finds GPT-4 exhibits self-bias that amplifies across iterations: GPT-4 perceives improvements that are not borne out by reference BLEURT scores.",
            "citation_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-source OpenAI multimodal/LLM (exact parameter count not specified in paper); strong instruction-following capability as used via OpenAI API.",
            "reflection_method_name": "self-refine (iterative self-feedback)",
            "reflection_method_description": "Generate initial translation; prompt same LLM to produce MQM-style feedback (error location/type/severity) on the candidate; condition on candidate+feedback to produce a refined output; accept refined output only if the LLM's self-feedback score indicates improvement. Iterations run up to 10 in experiments.",
            "task_name": "Machine translation (Flores-200 low/med-resource directions: Yor→En, Jav→En, Arm→En, Ig→En)",
            "task_description": "Translate sentences from low-to-medium resource languages into English; evaluation via BLEURT mapped to MQM-like -25..0 scale (quantile-mapped) and some human MQM annotations for validation.",
            "performance_with_reflection": "Paper reports that BLEURT (mapped to MQM) does not show true quality improvement across iterations for GPT-4, while GPT-4's self-evaluated scores increase (false positive perceived improvement). No numeric improvement in BLEURT is reported; human MQM validations show bias amplification (higher bias and distance skewness after 10 iterations).",
            "performance_without_reflection": "Initial-generation BLEURT / mapped MQM scores used as baseline; paper reports little-to-no true BLEURT improvement after iterative self-refine despite model's self-reported improvements. (Exact BLEURT numbers per iteration not given in main text.)",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered MQM-style feedback (in-context examples), LLM-as-evaluator, iterative conditional generation accepting refinements only when self-feedback score improves.",
            "number_of_iterations": 10,
            "evidence_for_improvement": "Quantitative evidence shows self-feedback scores increase across iterations for GPT-4 but mapped BLEURT (reference-based) does not improve; human MQM annotations on samples confirm the LLM's self-evaluation inflates scores (bias and distance-skewness increase).",
            "limitations_or_failure_cases": "Self-bias amplification: GPT-4 tends to favor outputs in its own style and optimizes toward false positive corrections; iterative self-refinement increases bias and does not yield true translation quality gains. Example case study shows large increases in LLM's self-score while human score remains low.",
            "comparison_to_other_methods": "Compared against external feedback (InstructScore) where external feedback reduced bias and elicited BLEURT improvement; GPT-4 shows more robustness than smaller/simpler models but still succumbs to bias amplification under self-feedback.",
            "ablation_study_results": "Paper shows external feedback (oracle InstructScore) reduces bias and yields true BLEURT improvement for GPT-4; larger models (discussion) are more resilient but still show self-bias early.",
            "uuid": "e8753.0",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5_self-refine_MT",
            "name_full": "GPT-3.5-Turbo iterative self-refinement on machine translation",
            "brief_description": "Openly-accessed GPT-3.5-Turbo was evaluated in the same self-refine pipeline on Flores-200 language pairs and shows higher self-bias amplification than stronger models (GPT-4/Gemini), with self-reported improvements not matched by reference metrics.",
            "citation_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Closed-source OpenAI model (Turbo family) used via API; instruction-tuned but weaker than GPT-4 in experiments.",
            "reflection_method_name": "self-refine (iterative self-feedback)",
            "reflection_method_description": "Same iterative generate-feedback-refine loop using MQM-style prompts and accepting refinements only when self-feedback indicates improvement; experiments run up to 10 iterations.",
            "task_name": "Machine translation (Flores-200 low/med-resource directions)",
            "task_description": "Translate low-to-medium resource languages into English; evaluate with BLEURT mapped to MQM and occasional human MQM annotation.",
            "performance_with_reflection": "Paper reports GPT-3.5-Turbo exhibits amplified self-bias through iterations; self-evaluated improvements are larger than true BLEURT improvements. True performance (BLEURT / mapped MQM) shows saturation and no meaningful gains after several iterations.",
            "performance_without_reflection": "Baseline initial-generation performance (no self-refine) serves as comparison; paper states true improvements saturate (e.g., coverage and BLEURT do not increase commensurately), exact numeric baselines not provided in main text.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted MQM-style feedback via in-context examples; LLM-as-evaluator; iterative conditional refinement.",
            "number_of_iterations": 10,
            "evidence_for_improvement": "Quantitative curves show self-reported scores rising faster than true BLEURT; human MQM evaluation on samples confirms increased bias and skewness after 10 iterations.",
            "limitations_or_failure_cases": "Higher bias amplification relative to GPT-4 and Gemini; self-refinement can drive false-positive optimization and does not reliably improve true translation quality.",
            "comparison_to_other_methods": "Compared qualitatively to GPT-4 and Gemini; more biased than GPT-4/Gemini. External feedback reduces bias for models including GPT-3.5-Turbo.",
            "ablation_study_results": "Noted effect that smaller/less capable models (like GPT-3.5-Turbo) show larger bias amplification; external feedback helps reduce bias.",
            "uuid": "e8753.1",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Gemini_self-refine_MT",
            "name_full": "Gemini iterative self-refinement on machine translation",
            "brief_description": "Google Gemini (gemini‑pro) used in self-refine experiments on Flores-200 pairs; shows some robustness relative to smaller models but still exhibits self-bias amplification across iterations and distributional skew shifts.",
            "citation_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
            "mention_or_use": "use",
            "model_name": "Gemini (gemini-pro)",
            "model_description": "Closed-source Google Gemini model used via API; strong instruction-following capabilities per authors' selection.",
            "reflection_method_name": "self-refine (iterative self-feedback)",
            "reflection_method_description": "Generate initial output, obtain MQM-style feedback from same LLM, produce refined output conditionally; accept only if self-feedback indicates improvement; experiments up to 10 iterations.",
            "task_name": "Machine translation (Flores-200 low/med-resource directions)",
            "task_description": "Translate low-to-medium resource languages to English; evaluate with BLEURT mapped to MQM and human validation.",
            "performance_with_reflection": "Paper reports Gemini perceives improvements across iterations (self-feedback rises) while BLEURT often does not improve; Gemini shows shifts in skewness (right→left in Jav→En), indicating complex bias dynamics.",
            "performance_without_reflection": "Baseline generation performance (no self-refine) compared to iterative self-refine; true BLEURT improvements not observed despite self-reported gains.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-based MQM-style self-evaluation and iterative refinement; Gemini receives only last-iteration output/feedback at each step.",
            "number_of_iterations": 10,
            "evidence_for_improvement": "Self-feedback scores increased across iterations but reference BLEURT did not match these perceived improvements; distance-skewness trends described (decrease then increase) indicate bias amplification.",
            "limitations_or_failure_cases": "Gemini sometimes refuses to answer sensitive content causing lower format accuracy on subset of examples; still vulnerable to self-bias amplification over iterations.",
            "comparison_to_other_methods": "Compared to GPT-4 and GPT-3.5-Turbo; Gemini is more robust than some open-source models but still exhibits amplified self-bias; external feedback reduces bias for Gemini as well.",
            "ablation_study_results": "Appendix notes skewness dynamics per language pair; external feedback experiments show bias reduction.",
            "uuid": "e8753.2",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "LLaMA2_size_ablation",
            "name_full": "LLaMA2 (7B/13B/70B) self-refine size ablation on translation",
            "brief_description": "Open-source LLaMA2-chat models of 7B, 13B, and 70B parameters were compared on Yor→En translation to study effect of model size on self-bias amplification; larger 70B shows less amplification and plateaus after ~5 iterations.",
            "citation_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
            "mention_or_use": "use",
            "model_name": "LLaMA2-7B / LLaMA2-13B / LLaMA2-70B",
            "model_description": "Open-source instruction-tuned LLaMA2 chat models with 7B, 13B, and 70B parameters respectively (checkpoints cited in Appendix).",
            "reflection_method_name": "self-refine (iterative self-feedback)",
            "reflection_method_description": "Same generate-feedback-refine loop using MQM-style feedback prompts; iterations up to 10; bias and dSkew measured across steps.",
            "task_name": "Machine translation (Flores-200 Yor→En experiments reported)",
            "task_description": "Yoruba→English translation on Flores-200 subset with BLEURT (quantile-mapped) and bias/dSkew estimation.",
            "performance_with_reflection": "LLaMA2-70B exhibits self-bias initially but bias plateaus after the 5th iteration; smaller 7B and 13B models continue to amplify self-bias in later iterations. True BLEURT improvements occur with larger models when paired with external feedback (general trend reported).",
            "performance_without_reflection": "Baseline (0th iteration) used for comparison; smaller models had greater bias growth from baseline across iterations.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-based MQM-style self-evaluation and iterative refinement.",
            "number_of_iterations": 10,
            "evidence_for_improvement": "Size-dependent curves: 70B shows less bias amplification and plateau, suggesting larger models are more resilient to self-bias; reported qualitatively and shown in Figure 10.",
            "limitations_or_failure_cases": "Even 70B exhibits self-bias early; plateaus but does not fully eliminate bias without external feedback.",
            "comparison_to_other_methods": "Compared sizes directly; larger models outperform smaller ones in resisting bias amplification. External feedback further improves results.",
            "ablation_study_results": "Model-size ablation demonstrates monotonic relationship: larger parameter count → reduced self-bias amplification (qualitative/curve evidence).",
            "uuid": "e8753.3",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "CommonGen_self-refine",
            "name_full": "Self-refinement (iterative missing-concept feedback) on CommonGen Hard",
            "brief_description": "LLMs (GPT-4, GPT-3.5-Turbo, Gemini) were used to self-evaluate missing concept coverage via ICL examples and iteratively refine outputs; self-estimated coverage rises faster than true coverage and performance saturates around iteration 5.",
            "citation_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5-Turbo, Gemini",
            "model_description": "Closed-source instruction-tuned models accessed via APIs; GPT-4 and Gemini stronger than GPT-3.5-Turbo per experiments.",
            "reflection_method_name": "self-refine with missing-concept feedback (ICL)",
            "reflection_method_description": "Initial generation from ~30 concepts; provide two in-context learning (ICL) feedback examples where LLM lists missing concepts; model revises output to improve concept coverage; accept revisions only when LLM feedback score improves. Iterations up to 10.",
            "task_name": "Constrained text generation (CommonGen Hard)",
            "task_description": "Generate fluent/commonsense sentences covering a list of concepts; evaluation is strict string-match full-coverage metric (1 if all concepts appear, else 0).",
            "performance_with_reflection": "True full-coverage (string-match) improves modestly then saturates (e.g., GPT-3.5 reaches ~20% coverage around iteration 5), while LLM self-estimated coverage rises much faster; self-bias (difference between self-score and true coverage) amplifies across iterations.",
            "performance_without_reflection": "Initial-generation coverage baseline (iteration 0) used; iterative self-refine yields modest true coverage gains but larger self-estimated gains (leading to bias amplification). Exact numeric coverage values are reported qualitatively (e.g., GPT-3.5 ~20% at iter 5).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompted ICL examples for missing-concept detection; LLM-as-evaluator using explicit binary coverage feedback; iterative refinement conditioned on feedback.",
            "number_of_iterations": 10,
            "evidence_for_improvement": "Quantitative coverage curves: some true coverage improvement but rate and magnitude much smaller than self-reported improvements; saturation after ~5 iterations; plots shown in Figure 5.",
            "limitations_or_failure_cases": "LLMs overestimate their improvements (self-bias); refinement can saturate quickly; GPT-4 has higher initial coverage and therefore fewer easy gains leaving it to incorrectly judge small missing items.",
            "comparison_to_other_methods": "Not compared to chain-of-thought; compared qualitatively across models (GPT-4 vs GPT-3.5-Turbo vs Gemini); external feedback not explicitly applied in CommonGen experiments in main text.",
            "ablation_study_results": "Observation that higher initial coverage (GPT-4) leads to different bias dynamics; no formal ablation beyond model comparisons reported.",
            "uuid": "e8753.4",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Math_self-refine_and_self-consistency",
            "name_full": "Iterative self-refinement and self-consistency verification on MATH dataset",
            "brief_description": "LLMs (GPT-4, GPT-3.5-Turbo, Gemini, open-source models) were evaluated on the MATH dataset using self-refine where feedback is correctness (1/0) and an alternate pipeline replacing self-evaluation with self-consistency majority-vote verification; both pipelines exhibited increased self-bias and skewness across iterations.",
            "citation_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
            "mention_or_use": "use",
            "model_name": "GPT-4, GPT-3.5-Turbo, Gemini, and others",
            "model_description": "Closed-source instruction-tuned LLMs accessed via APIs; for math experiments standard prompts produce step-by-step reasoning and final answer.",
            "reflection_method_name": "self-refine (binary correctness feedback) and self-consistency (majority-vote verifier)",
            "reflection_method_description": "Self-refine: compare previous solution to ground truth answer and output 1 if matched else 0; iterate refinement only when feedback indicates improvement. Self-consistency variant: sample multiple reasoning paths (10), majority vote to propose an answer; if majority differs from initial, it replaces the initial and yields binary feedback; iterate. Experiments report up to 10 iterations.",
            "task_name": "Mathematical reasoning (MATH test set)",
            "task_description": "Solve math competition-style problems requiring multi-step reasoning; evaluate as exact-match correctness (1 if final answer matches ground truth, else 0).",
            "performance_with_reflection": "Paper reports increases in bias and distance-skewness across iterative self-refine and in the self-consistency pipeline; no reliable increase in true accuracy is reported — bias indicates models favor some solutions and create biased ensembles. Exact accuracy numbers per iteration are not provided in main text.",
            "performance_without_reflection": "Baseline initial-generation accuracy used; iterative self-refine and self-consistency do not reliably improve true accuracy and instead increase self-bias.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered binary correctness feedback or self-consistency majority-vote verification; iterative conditional refinement.",
            "number_of_iterations": 10,
            "evidence_for_improvement": "Figures show that bias and dSkew increase across iterations for math; self-consistency replacement also increases bias leading to contaminated ensembles rather than improved true performance.",
            "limitations_or_failure_cases": "Self-consistency and self-refine introduce or amplify bias toward certain reasoning paths; ensembles produced by self-consistency can be biased; models may not correct reasoning errors reliably and may worsen.",
            "comparison_to_other_methods": "Self-consistency used as an alternative evaluator is shown in Appendix D to also increase bias and skewness; compared qualitatively to self-refine and baseline.",
            "ablation_study_results": "Appendix D reports that replacing self-evaluation with self-consistency still yields increased bias and skewness across iterations (qualitative/curve evidence).",
            "uuid": "e8753.5",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-rewarding_pipeline",
            "name_full": "Self-rewarding pipeline (LLM-as-reward) on translation",
            "brief_description": "The paper studies the self-rewarding pipeline (sample k candidates, use same LLM as reward model to pick top candidate, then train on top samples) and finds that increasing k amplifies self-bias (models more likely to favor own generations), worsening selection bias.",
            "citation_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
            "mention_or_use": "use",
            "model_name": "Deepseek-MOE, Mixtral-MOE, LLaMA2-7B (open-source models in study)",
            "model_description": "Open-source instruction-tuned mixture-of-experts models (Mixtral 8x7B, Deepseek-MOE 16B) and LLaMA2-7B; used to sample k candidates per input.",
            "reflection_method_name": "self-rewarding (LLM-as-reward / candidate ranking)",
            "reflection_method_description": "For each input sample, generate k candidates (k in {1,4,8,16,32}) then use same LLM as reward model to score/rank candidates (MQM-style) and select top-performing samples for further training; measure self-bias of reward scores.",
            "task_name": "Machine translation (Yoruba→English subset)",
            "task_description": "Translation with candidate sampling and LLM-as-reward selection; bias measured between LLM self-scores and BLEURT/human approximations.",
            "performance_with_reflection": "Paper reports increased Bias and distance-skewness as k (candidate pool size) increases for all tested open-source models; this indicates stronger self-rewarding amplifies self-bias and can amplify false positives in selection.",
            "performance_without_reflection": "k=1 (no selection) baseline shows lower bias; as k grows, self-bias increases.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Sampling multiple candidate outputs and using the generating model itself as a reward/scoring function (prompted MQM feedback) to select top candidate(s).",
            "number_of_iterations": null,
            "evidence_for_improvement": "No improvement; instead reported harm: bias and dSkew increase monotonically with candidate pool size, showing increased likelihood of selecting false-positive 'best' candidates as judged by the biased model.",
            "limitations_or_failure_cases": "Self-rewarding pipeline amplifies self-bias especially with larger candidate pools, risking selection of suboptimal samples for downstream training and reinforcing model's own errors.",
            "comparison_to_other_methods": "Contrasted with external feedback selection (oracle InstructScore) which would avoid this amplification; no training-on-top-selected-samples experiments reported in main text, only bias analysis.",
            "ablation_study_results": "Varying k constitutes an ablation: increasing k leads to increased Bias and dSkew across models (reported in Table 3).",
            "uuid": "e8753.6",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "External_feedback_InstructScore",
            "name_full": "External reference-based feedback via InstructScore (oracle) to guide refinement",
            "brief_description": "The authors used InstructScore (a reference-based feedback model that can access reference text) as an external (oracle-like) evaluator to provide error location/type/severity; external feedback reduced self-bias and elicited true BLEURT improvements during self-refine.",
            "citation_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
            "mention_or_use": "use",
            "model_name": "InstructScore (external evaluator) used with GPT-4, GPT-3.5-Turbo, Gemini",
            "model_description": "InstructScore is a reference-based feedback model (repo 'xu1998hz/InstructScore' cited) that inspects candidate and reference to provide fine-grained MQM-style annotations (error location/type/severity); treated as oracle feedback in experiments.",
            "reflection_method_name": "external-feedback-guided refinement",
            "reflection_method_description": "Replace LLM-as-evaluator with InstructScore which supplies MQM-style error annotations (without explicit corrections); LLMs then refine outputs using that external feedback. Experiments run across 5 refinement steps in figure cited.",
            "task_name": "Machine translation (Yoruba→English subset)",
            "task_description": "Translation evaluation comparing BLEURT, self-feedback evaluated performance, and external feedback evaluated performance across iterative refinement steps.",
            "performance_with_reflection": "With external InstructScore feedback, all tested LLMs (GPT-4, GPT-3.5-Turbo, Gemini) show reduced bias estimates and consistent BLEURT improvements across iterations (true quality improves), as external feedback converges toward BLEURT assessments.",
            "performance_without_reflection": "Self-feedback (LLM-as-evaluator) baseline shows bias amplification and no BLEURT improvement; external feedback yields better outcomes.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Use of an external reference-based evaluator (InstructScore) to produce MQM-style feedback; prompt parsing ensures same format as self-feedback; LLMs then refine conditioned on this external feedback.",
            "number_of_iterations": 5,
            "evidence_for_improvement": "Figures and a case study (Table 4) show InstructScore-guided refinements reduce Bias and dSkew and produce monotonic BLEURT improvements across iterations; a concrete Yoruba→English example shows human score improving from -11 to -1 across refinements when guided by InstructScore.",
            "limitations_or_failure_cases": "InstructScore is oracle-like because it has access to references; while it does not provide explicit corrections (only annotations), its oracle access limits direct generalization to settings without references. The refinement still relies on the LLM's internal correction capability.",
            "comparison_to_other_methods": "Direct comparison: external InstructScore vs LLM self-feedback. External feedback consistently reduces bias and elicits true BLEURT improvement while self-feedback amplifies bias and produces false-positive perceived gains.",
            "ablation_study_results": "Using external feedback versus self-feedback is presented as an ablation/mitigation: external feedback curves (dotted) lie below self-feedback curves (solid), indicating reduced bias and improved BLEURT when external feedback applied.",
            "uuid": "e8753.7",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Self-bias_definition_and_metrics",
            "name_full": "Formal definition of self-bias using statistical bias and distance skewness",
            "brief_description": "Paper formally defines LLM self-bias as the tendency to favor its own generations and proposes two metrics: Bias(θ) (mean difference between model's quality prediction and true quality) and distance skewness (dSkew) to measure asymmetry of E[θ]-θ distribution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (metric / analytic contribution)",
            "model_description": "Not a model; analytic/statistical definitions applied to LLM evaluation outputs.",
            "reflection_method_name": "self-bias estimation (Bias(θ) and distance skewness)",
            "reflection_method_description": "Bias(θ) = average (E[θi] - θi) across samples where E[θ] is model's predicted quality and θ is human or proxy true quality; distance skewness dSkew_n(X) measures asymmetry of E[θ]-θ distribution to capture directional bias not shown by mean alone.",
            "task_name": "Applied across tasks: translation (MQM/BLEURT), constrained generation (coverage), mathematical reasoning (binary correctness)",
            "task_description": "Metrics used to analyze self-refine and self-rewarding pipelines across Flores-200, CommonGen Hard, and MATH datasets.",
            "performance_with_reflection": "Not applicable (metrics measure bias). Paper reports these metrics increase (bias and dSkew) across iterations for many models/tasks.",
            "performance_without_reflection": "Used to compare baseline (iteration 0) vs iterative self-refine; Bias and dSkew increase after iterative self-refine in most experiments.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Analytic measurement computed from LLM's self-evaluations and reference/human or proxy true scores (BLEURT quantile-mapped to MQM for translation; binary correctness for math; string-match coverage for CommonGen).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Not an improvement method; provides quantitative evidence that self-feedback inflates scores (Bias&gt;0) and that distributional asymmetry grows (dSkew increases) through iterations.",
            "limitations_or_failure_cases": "Bias(θ)=0 can mask asymmetric distributions, hence the need for dSkew; both metrics rely on a valid true-quality signal (human or reliable proxy). When proxies are imperfect (e.g., BLEURT approximations), conclusions require validation (authors used some human MQM checks).",
            "comparison_to_other_methods": "Designed to complement mean-bias measures; distance skewness specifically intended to reveal asymmetry missed by mean bias.",
            "ablation_study_results": null,
            "uuid": "e8753.8",
            "source_info": {
                "paper_title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models cannot self-correct reasoning yet",
            "rating": 2,
            "sanitized_title": "large_language_models_cannot_selfcorrect_reasoning_yet"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models",
            "rating": 2,
            "sanitized_title": "selfcheckgpt_zeroresource_blackbox_hallucination_detection_for_generative_large_language_models"
        },
        {
            "paper_title": "Self-rewarding language models",
            "rating": 2,
            "sanitized_title": "selfrewarding_language_models"
        },
        {
            "paper_title": "LLMs as narcissistic evaluators: When ego inflates evaluation scores",
            "rating": 2,
            "sanitized_title": "llms_as_narcissistic_evaluators_when_ego_inflates_evaluation_scores"
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 1,
            "sanitized_title": "teaching_large_language_models_to_selfdebug"
        },
        {
            "paper_title": "Can large language models really improve by self-critiquing their own plans?",
            "rating": 1,
            "sanitized_title": "can_large_language_models_really_improve_by_selfcritiquing_their_own_plans"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 1,
            "sanitized_title": "gpt4_technical_report"
        }
    ],
    "cost": 0.02020375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement
18 Jun 2024</p>
<p>Wenda Xu wendaxu@cs.ucsb.edu 
University of California
Santa Barbara</p>
<p>Guanglei Zhu guanglez@cs.cmu.edu 
Carnegie Mellon University</p>
<p>Xuandong Zhao xuandongzhao@cs.ucsb.edu 
University of California
Santa Barbara</p>
<p>Liangming Pan liangmingpan@cs.ucsb.edu 
University of California
Santa Barbara</p>
<p>Lei Li leili@cs.cmu.edu 
Carnegie Mellon University</p>
<p>William Yang Wang william@cs.ucsb.edu 
University of California
Santa Barbara</p>
<p>Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement
18 Jun 2024F2664D3D3308DE1F92E38A31FBA87F3AarXiv:2402.11436v2[cs.CL]
Recent studies show that large language models (LLMs) improve their performance through self-feedback on certain tasks while degrade on others.We discovered that such a contrary is due to LLM's bias in evaluating their own output.In this paper, we formally define LLM's self-bias -the tendency to favor its own generation -using two statistics.We analyze six LLMs (GPT-4, GPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text generation, and mathematical reasoning tasks.We find that self-bias is prevalent in all examined LLMs across multiple languages and tasks.Our analysis reveals that while the self-refine pipeline improves the fluency and understandability of model outputs, it further amplifies self-bias.To mitigate such biases, we discover that larger model size and external feedback with accurate assessment can significantly reduce bias in the self-refine pipeline, leading to actual performance improvement in downstream tasks.The code and data are released at https://github.com/xu1998hz/llm_self_bias.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown strong capabilities in many NLP tasks.While these models still make mistakes, recent studies show that "self-refine" (also known as "self-reflection") is promising to rectify errors based on LLM's selffeedback (Madaan et al., 2024;Chen et al., 2024;Shinn et al., 2024;Manakul et al., 2023;Pan et al., 2023).Meanwhile, opposite study also shows that LLMs fail to correct their mistakes and their performance even gets worse after self-feedback (Huang et al., 2023b).These contradictory results suggest that LLM's self-feedback is unreliable.Selfrefine procedure relies on LLM's evaluation capability of the generated text.We hypothesize that if there is a bias during the self-evaluation process, such bias will be amplified during iterative self-Figure 1: How LLM's self-feedback inflates scores compared to human assessment.Bias is the mean difference between LLM and human scores, while skewness (Dskew) measures the asymmetry of their distribution around zero.Non-biased estimation will have Dskew=0.</p>
<p>refinement.This is consistent with a prior finding that LM-based metrics (e.g.BARTScore) exhibit "narcissism" during self-evaluation, i.e., the metric model favors text generated by the same underlying language model in the context of summarization tasks (Liu et al., 2023b).However, it remains unclear whether bias exists universally in LLMs across a wide range of tasks.How to quantify such biases?How does this "narcissism" impact LLM's self-refinement?</p>
<p>In this work, we define "self-bias" to the degree that an LLM favors its own generation.We propose to use two principled statistics to estimate self-bias in LLM's self-refinement procedure.The first one measures the degree of inflation in the LLM's self-evaluation compared to the true (human) evaluation.The second measures whether LLM's self-evaluation is skewed compared to the ture estimate.Figure 1 illustrates these two statis-tics.We examine self-bias scores on six diverse LLMs, covering four languages across three distinct tasks: machine translation, constrained text generation, and mathematical reasoning.We find that self-bias is universal in self-refine and selfrewarding pipelines, regardless of the languages and tasks.This bias causes LLMs to optimize for false positive corrections rather than improving the actual output quality.</p>
<p>We further investigate what is the real benefit of self-refine.We find that while the self-refine pipeline improves the fluency and understandability of model outputs, it does not necessarily lead to intended improvements as specified in the prompt.Moreover, LLMs may favor texts that mirror their style, potentially leading to false positive optimization and reduced diversity in text generation.To mitigate the self-bias, we propose two solutions: increasing the model size and incorporating external feedback to provide accurate assessment, thereby directing the LLM towards more accurate self-correction.Our contributions are:</p>
<ol>
<li>
<p>We formally define the self-bias of an LLM using two principled estimated statistics.</p>
</li>
<li>
<p>We quantify self-biases for six diverse LLMs and find that self-bias amplifies during selfrefine across many languages and tasks.</p>
</li>
<li>
<p>We observe two factors that contribute to selfbias and pinpoint two directions to mitigate it and elicit LLMs' self-correction ability.</p>
</li>
</ol>
<p>Related Work</p>
<p>Large Language Model Self-correction.Recent works demonstrate that LLM can utilize its own feedback signal to refine itself (Madaan et al., 2024;Chen et al., 2024;Shinn et al., 2024).Wang et al. (2023) further proposed to sample diverse reasoning paths and use a majority vote to find the most confident answer.Huang et al. (2023a) leverages self-consistency to further fine-tune the LLM on the most confident reasoning path with diverse instruction formats.On the other hand, LLM's selffeedback can also be used as a reward signal to further align LLM to follow instructions (Gulcehre et al., 2023;Yuan et al., 2024).</p>
<p>Despite some demonstrations of performance improvements, most findings indicate that LLMs struggle to rectify their initial mistakes, and their performance even worsens after self-correction (Huang et al., 2023b;Tyen et al., 2023;Ke et al., 2023).This issue arises because the quality of the model's self-generated feedback is bounded by its existing knowledge and abilities (Stechly et al., 2023;Hong et al., 2023).Therefore, internal feedback may not offer any extra advantage for improving the results; it might even steer the model away from the correct answer (Valmeekam et al., 2023).However, prior works only had empirical observations on this phenomenon, while lacking a quantitative analysis.Moreover, prior works only focus on specific tasks, such as reasoning or code generation.In this work, we are the first to quantitatively analyze the self-bias of different LLMs across three tasks and four languages, which provides a novel and generalizable view to address the perils of self-refine.</p>
<p>LLMs as Evaluators.et al. (2023b) point out that reference-free metrics are inherently biased on their own outputs.</p>
<p>Although the above empirical studies provide valuable insights, they lack a formal definition to quantify those biases nor provide a connection to the self-refine framework.In this work, we define and quantify self-bias and provide the first in-depth analysis of its impact on the self-refine pipeline.</p>
<p>We analyze potential bias attributions and pinpoint two mitigation directions.</p>
<p>Quantifying Self-Bias</p>
<p>This section outlines the approach used to quantify the self-bias exhibited by LLMs in an iterative selfrefinement pipeline.We employ statistical bias and distance skewness (Szekely and Móri, 2006) estimation to measure self-bias.</p>
<p>Iterative Self-Refinement in LLMs</p>
<p>Self-refinement is an inference time method, in which the LLM first generates a response y i to a given prompt x, and then the same LLM generates feedback f i based on the candidate output y i and input x.Based on feedback f i , input x, and candidate output y i , the LLM then generates a refined output r i .LLM iterates between the feedback and the refinement steps, continuing until it reaches a predetermined number of iterations.At each refinement step, the refined output will only be accepted if it demonstrates superior quality compared to the previously generated text.The quality of the text is assessed through self-feedback from the language model itself.At each feedback or refinement step, LLM only sees the last iteration's generation or feedback, without accessing the entire history of output or feedback.</p>
<p>Bias Estimation</p>
<p>We estimate the self-bias of LLMs using the statistical bias definition.This bias is characterized by the disparity between an LLM's predicted quality score and the expected quality score, as follows:
Bias( θ) = 1 n n i=1 (E[ θi ] − θ i ),(1)
where E[ θi ] is an expected LLM's quality prediction at sample i, and θ i denotes the true quality of sample i. Ideally, θ i should be derived from human annotations, for example, multidimensional quality metrics (MQM) human annotations (Freitag et al., 2021) for machine translation, or predefined criteria such as word coverage for constrained text generation (Madaan et al., 2024).The LLM's quality prediction is expected to precisely follow the human annotation procedure or predefined criteria, ensuring consistency between θ and E[ θ].When Bias( θ) &gt; 0, the LLM assigns a higher quality score to its own sample compared to the expected quality score.When Bias( θ) &lt; 0, the LLM underestimates the sample quality compared to the expected quality score.The larger the value of Bias( θ), the more pronounced the LLM's bias against its own samples.</p>
<p>Distance Skewness Estimation</p>
<p>In an ideal scenario, an unbiased LLM should have equal chance of over-estimation and underestimation of text quality (Bias( θ) = 0), resulting We use distance skewness to measure the asymmetry of distribution.Therefore, using two meta-metrics as complimentary, we can measure the self-bias of LLM.</p>
<p>in a perfectly symmetric distribution when plotting E[ θ] − θ.However, Bias( θ) = 0 does not guarantee a symmetric distribution (In Figure 2, one tail could be long and thin, while the other is short and fat, yet they balance out overall).Therefore, we introduce another meta-metric, distance skewness, to measure the asymmetry of E[ θ]−θ's distribution.Specifically,
dSkew n (X) = 1 − i,j ∥x i − x j ∥ i,j ∥x i + x j − 2γ∥ ,(2)
where x i and x j are two independent identical random examples drawn from E[ θ] − θ. dSkew n (X) measures the asymmetry of X with respect to γ. Distance skewness ranges between 0 and 1. dSkew n (X) equals 0 if and only if X is diagonally distributed respect to γ. dSkew n (X) equals 1 if and only if X is distributed at a constant on one side of γ.A higher distance skewness indicates a more asymmetric distribution of E[ θ] − θ.In our experimental setup, we use both bias and distance skewness to measure the model's bias towards its quality prediction.</p>
<p>4 Analyzing LLM's Self-Bias</p>
<p>Experimental Setup</p>
<p>We include three closed-source LLMs (GPT-4 (Achiam et al., 2023), GPT-3.5-Turbo and Gemini (Team et al., 2023)) and three open-source LLMs (LLaMA2-7B (Touvron et al., 2023), Mixtral-MOE 8x7B (Jiang et al., 2024) and DeepSeekMoE 16B (Dai et al., 2024)).These models have been shown to have strong instruction-following capabilities (Madaan et al., 2024;Shinn et al., 2024), making them well-suited to demonstrate self-bias.</p>
<p>For each model, we first prompt it to produce the initial generation.Then, we prompt the model to generate the feedback for the initial generation.The model takes in both the feedback and the prior step generation to produce a refined output.We will only accept refinement if the feedback score is improved on the refined output.We listed specific model API/checkpoints in Appendix Section A.</p>
<p>Machine Translation.We evaluated LLMs on Flores-200 (Costa-jussà et al., 2022) dataset with four language pairs: Yoruba to English (Yor-En), Javanese to English (Jav-En), Armenian to English (Arm-En), and Igbo to English (Ig-En), using 100 test examples per language pair.We concentrate on low-to-medium resource language pairs, as Kocmi et al. (2023) indicate that LLMs like GPT-4 already perform at a nearly human-like level in high resource language pairs such as Chinese-to-English, leaving limited potential for further improvement through self-refine.</p>
<p>To ensure high-quality evaluations, we utilized feedback prompts based on the MQM human annotation from Freitag et al. (2021), as in Kocmi and Federmann (2023).LLMs will input source text and candidate text and output feedback, including error location, error type, and severity labels.We adopt the same error scoring as Freitag et al. (2021), assigning −1 for minor errors and −5 for major errors, with a score range of 0 to −25 (0 for perfect translations, −25 for samples with more than five severe errors).The details of the prompts are provided in the Appendix Table 8, 9 and 10.</p>
<p>Ideally, human raters would have evaluated each sample, but due to cost and scalability constraints, we utilized the reference-based learned metric BLEURT (Sellam et al., 2020) as an approximation of human judgments.BLEURT generates quality scores based on the similarity between candidate and reference translations.To align BLEURT's score distribution with that of human ratings, we employed quantile mapping (Cannon et al., 2015), yielding a score range from 0 to -25.Although automatic metrics are primarily used, we also conduct modified MQM human evaluations (Freitag et al., 2021) for validation purposes.Our bias estimation ranged from -25 to 25.Details on quantile mapping are provided in the Appendix Section B.</p>
<p>Constrained Text Generation.We conducted experiments on commonsense text generation, following (Lin et al., 2020).We tested LLMs on 100 examples from the CommonGen Hard dataset.For each testing instance, the large language model (LLM) received approximately 30 concepts and was tasked with generating a fluent and logically sound text.To generate the initial output, we adopted a similar prompt design to that of (Lin et al., 2020).Next, we provided two ICL feedback examples to help the LLM identify missing concepts in its initial output.In each feedback example, the LLM was given concept words and the previous generation and asked to indicate any missing concepts.This feedback allowed the LLM to revise its output and generate a text with better coverage of the input concepts.The details of the prompts are included in the Appendix Table 12, 13 and 14.</p>
<p>To evaluate the coverage of the generated texts, we adopted the evaluation metric used in (Madaan et al., 2024).This metric uses strict string matching to determine whether each concept word from the input appears in the generated text (metric outputs 1 if all concepts are covered and 0 otherwise).From feedback of LLM's missing concepts, we assigned a binary score (0 or 1) to each text based on its full coverage of concepts.Since our string-matching metric and LLM feedback score were on the same scale, we were able to compute bias and distance skewness directly.The range of bias estimation is between −1 to 1.</p>
<p>Mathematical Reasoning.We conducted experiments on mathematical reasoning.We tested LLMs on 100 examples from the MATH testing set (Hendrycks et al., 2021).For each instance, LLM receives a problem statement and generates a step-by-step solution with a final answer.In this task, we use the self-refine pipeline by providing the feedback on the step-by-step solution.In each iteration, the previous solution will be compared against the ground truth answer, outputting 1 if they are matched and 0 otherwise.Therefore, we can directly compute bias and distance skewness.The range of bias estimation is between −1 to 1.The details of the prompts are included in the Appendix Table 11.In addition, we also conducted experiments by replacing the self-evaluation (LLM as evaluator) with self-consistency verification (selfconsistency as an evaluator) (Huang et al., 2023a).</p>
<p>We include those results in the Appendix D.</p>
<p>Self-Bias Amplification during Iterative Refinement</p>
<p>Machine Translation.In Figure 3, we illustrate that all large language models (LLMs) exhibit a self-bias in the self-refine pipeline.Notably, opensource LLMs and GPT-3.5-Turbotend to exhibit higher levels of self-bias throughout iterations than stronger instruction-following LLMs, such as GPT-4 and Gemini.This suggests that GPT-4 and Gemini possess a certain level of capability in resisting self-bias.However, despite some robustness demonstrated by GPT-4 and Gemini, we observe a consistent amplification of self-bias through the self-refine pipeline across four language directions, indicating that even these advanced LLMs are susceptible to self-bias amplification.</p>
<p>In Figure 4, we illustrate a comparison between GPT-4 and Gemini's quality assessments of their own outputs and performance measured by reference-based BLEURT over ten iterations.Our findings suggest that the primary reason for the amplification of bias during self-refine iteration is that actual performance does not improve through iterations.Instead, GPT-4 and Gemini mistakenly perceive performance improvements in their refined outputs.This discrepancy between the false positive performance measure and the true performance measure grows larger with each iteration.The appendix Section C details Gemini's shift from right-skewed to left-skewed distribution, resulting in a decrease in distance skewness during early iterations and an increase in later ones.</p>
<p>Constrained Text Generation. Figure 5 depicts the amplification of self-bias through ten self-refine iterations in constrained text generation for GPT-3.5-Turbo,GPT-4, and Gemini.Notably, GPT-4 exhibits a higher bias estimation at earlier iterations compared to GPT-3.5-Turbo and Gemini.This can be attributed to GPT-4's higher coverage ratio at to its counterparts (GPT-3.5-Turboat around 2%). Consequently, GPT-4 struggles to identify a few missing concepts, while GPT-3.5-Turboand Gemini have more coverage issues and can easily identify missing input concepts.</p>
<p>As GPT-3.5-Turbo reaches 20% coverage around the 5th iteration, it experiences a significant rise in bias and skewness estimation.It is worth noting that the rate of LLM's self-estimated improvements is much higher than the true coverage improvements.This phenomenon results in a saturation of performance improvements after the 5th iteration for both GPT-4 and GPT-3.5-Turbo.</p>
<p>Mathematical Reasoning.Figure 6 illustrates that all large language models (LLMs) exhibit an increase in bias and skewness estimation in the iterative self-refine pipeline.This suggests that LLMs introduce self-biases towards some math solutions during self-refine.</p>
<p>Human Evaluation on Bias Estimation.We employ one graduate student to annotate 50 examples from the 0th and 10th iteration of GPT-4, GPT-3.5-Turbo and Gemini's outputs at Yor-En, respectively.The human rater compares candidate text against reference and labels error location, error type, and severity labels at candidate text.The scoring scheme follows MQM style (Freitag et al., 2021) Figure 5: We evaluate the bias and distance skewness of generated texts produced by GPT-4, GPT-3.5-Turbo, and Gemini on the CommonGen dataset, across selfrefinement steps.Additionally, we report the coverage of GPT-3.5-Turbo and GPT-4 compared to true concept coverage.We show that the rate of LLM's self-estimated improvements is much higher than the true coverage improvements, which leads to self-bias amplification.are included in the Appendix Table 5, 6 and 7), which is consistent with the BLEURT assessment.</p>
<p>In Table 1, both increasing bias and distance skewness estimation demonstrate that all LLMs have significantly increased their self-bias with 10 iterative refinements.In the following case study, we examine self-bias in GPT-4.Our observations reveal that GPT-4's self-feedback mechanism led to the optimization of false positives, resulting in an amplification of self-bias over three iterations.In section 5, we demonstrate two potential alleviation that we can use to mitigate this self-bias.</p>
<p>Self-bias Example at GPT-4</p>
<p>Yoruba text: Ní bayii a ni àwon eku oloshu merin ti ko ni dayabetesi telele to ti ni ayabetesi," o she afikun.</p>
<p>Reference English text: "We now have 4-month-old mice that are non-diabetic that used to be diabetic," he added.</p>
<p>(Red span indicates a major error and blue span indicates a minor error annotated by GPT-4.)</p>
<p>GPT-4's 1st generation [Human: -11, GPT4: -11, Bias: 0]: "At this point, we have four rats without diabetes that have developed diabetes," he added.</p>
<p>GPT-4's 1st refinement [Human: -12, GPT4: -10, Bias: 2]: "Currently, we have four healthy rats that have developed diabetes," he clarified.</p>
<p>GPT-4's 2nd refinement [Human: -11, GPT4: 0, Bias: 11]: "Presently, we have four non-diabetic rats that have developed diabetes," he elaborated.</p>
<p>GPT-4 GPT-3.5-In Table 2, we include format accuracy for all LLMs.We observed that all LLMs have either perfect or nearly perfect format at first and fifth iteration of self-feedback.This is expected as we explicitly provide three in-context examples to control the output format.We found that different LLMs make different format mistakes.For example, DeepSeekMOE produces one or two garbage outputs and GPT-3.5-Turboproduces two or three free form outputs, like "The machine translation  is incorrect as it provides an alternative translation that does not match the source text."We conclude that this is due to their intrinsic instability of their instruction following capabilities.Gemini model contains surprisingly low format accuracy compared to other LLMs.This is due to the Gemini model refusing to generate any content that involves sensitive topics.There are 7 sentences in our testing set, Gemini refuses to provide responses.However, since our study focuses on self-bias amplification at iterations, this will not impact our experimental conclusions (The effects canceled out when comparing 1st and 5th iteration).</p>
<p>)OXHQF\</p>
<p>8QGHUVWDQGELOLW\</p>
<p><em>37 </em>377XUER *HPLQL</p>
<p>What improves after self-refinement?</p>
<p>Self-refinement can improve fluency and understandability but not quality.We demonstrate that LLM with biased feedback can impede the model's self-refine process.This raises a natural question: if an LLM does not improve its generation quality, does it improve in any other aspects throughout the iterative refine phase?To investigate this, we utilize the learned metric UniEval (Zhong et al., 2022) to measure the LLM's improvement beyond quality metrics.UniEval, a multidimensional learned metric, estimates various evaluation dimensions, including fluency, understandability, engagement and more.We focus on two dimensions, fluency and understandability, which UniEval is not trained on task-specific data.Our results, illustrated in Figure 6, show that GPT-4, GPT-3.5-Turbo, and Gemini consistently exhibit improvements in both fluency and understandability.This suggests an alternative perspective on the self-refine pipeline, indicating that while an LLM may not strictly adhere to instruction-following in terms of quality improvements, it can still improve certain intrinsic text qualities, such as fluency and understandability.Figure 8: We used Madlad400-10b to translate 100 Yor-En translations and asked GPT-4, GPT-3.5-Turbo, and Gemini to paraphrase 100 translations.We show the BLEURT and LLM scores before and after paraphrasing.In the lower right of the figure, we show the bias estimation before and after paraphrasing.GPT-4 and Gemini have negative self-bias before paraphrasing.After paraphrasing, all LLMs increase their bias against their paraphrased outputs.</p>
<p>LLMs favor texts that follow their style.To explore this propensity, we conducted experiments to investigate if LLMs display a preference for outputs that align with their generation style.We asked the GPT4, GPT-3.5-Turbo, and Gemini model to paraphrase external translation outputs.In this prompt, LLMs aimed not to improve the quality of translations but rather to rewrite sentences in their corresponding styles.Using the multilingual translation system Madlad400-10b (Kudugunta et al., 2023), we produced 100 Yoruba-to-English translations.Subsequently, each LLM was instructed to paraphrase the generated sentences.Our findings, shown in Figure 8, reveal that GPT-4 and Gemini have negative self-bias before paraphrasing.However, after paraphrasing, all LLMs showed an in- creased bias against their paraphrased outputs.This is mainly attributed to a decline in quality performance post-paraphrasing, with LLMs erroneously perceiving these paraphrased outputs as indicative of improvements.</p>
<p>Self-Bias is Amplified at Self-Rewarding Pipeline</p>
<p>In this section, we will explore the concept of selfbias in the self-rewarding pipeline, as outlined in (Yuan et al., 2024).The pipeline begins with an instruction fine-tuned large language model (LLM).Initially, we generate k candidate responses for each input provided to the LLM.Next, the same LLM is used as a reward model to identify the bestperforming candidate or to rank pairs within the collection of samples.Finally, various training objectives are applied to further train the LLM using the top-performing samples.</p>
<p>To illustrate the potential drawbacks of this pipeline, we carried out experiments on Yoruba to English translation task using three opensource LLMs: Deepseek-MOE, MixtralMOE, and LLaMA2-7B.For each source input, we sampled k candidate responses from each model.Subsequently, we obtained self-feedback scores on these candidates employing the prompt detailed in Section 4.1 and computed the corresponding self-bias.We varied k across 1, 4, 8, 16, and 32 to examine the influence of sample size on the self-bias within the self-rewarding pipeline.</p>
<p>As shown in Table 3, we observed that all LLMs displayed an increase in bias and distance skewness as the sample size increased.This occurs when the LLM has a biased estimation of its self-feedback, and this bias can be amplified when the sample size is increased to find the top-performing candidate according to the self-feedback.9: Using an external feedback model, we provide external feedback for GPT-4, GPT-3.5-Turbo, and Gemini in Yoruba-to-English translation task, across 5 refinement steps.We compare the models' true performance (measured by BLEURT) against external feedback-evaluated performance and self-feedback evaluated performance.Additionally, we plot the bias estimation for the three LLMs, considering both feedback types over 5 iterative refinement steps.</p>
<p>Alleviating Self-Bias</p>
<p>External Feedback Reduces Self-Bias.We demonstrated that self-feedback from a large language model can self-amplify bias with iterative refinement.We aim to answer if external feedback with low bias estimation can improve the model's generation performance and elicit self-correction capability.We leverage a reference-based feedback model, InstructScore (Xu et al., 2023), to provide external feedback.InstructScore will take in both reference and candidate text and output finegrained feedback, including error location, severity label, and error type.To ensure a fair comparison, we parse all outputs with the same format as selffeedback.Since InstructScore can access reference text to provide feedback, we recognize this external feedback as oracle feedback.However, models will only receive information about error location, error type, and severity labels.Therefore, refinement</p>
<p>External Feedback Example at GPT-4</p>
<p>Yoruba text: Ní bayii a ni àwon eku oloshu merin ti ko ni dayabetesi telele to ti ni ayabetesi," o she afikun.</p>
<p>Reference English text: "We now have 4-month-old mice that are non-diabetic that used to be diabetic," he added.(Red span indicates a major error and blue span indicates a minor error annotated by GPT-4.)</p>
<p>GPT-4's 1st generation [Human: -11, InstructScore: -10, Bias: 1]: "At this point, we have four rats without diabetes that have developed diabetes," he added.</p>
<p>GPT-4's 1st refinement [Human: -2, InstructScore: -6, Bias: -4]: "At this point, we have four mice without diabetes that were diabetic," he added.</p>
<p>GPT-4's 2nd refinement [Human: -1, InstructScore: -1, Bias: 0]: "We now have 4-month-old mice that are non-diabetic that were diabetic," he added.</p>
<p>Table 4: This case study demonstrates that external feedback (oracle) from InstructScore (Xu et al., 2023) can remain low self-bias during iterative self-refine.By providing accurate error type, error location, and severity labels, InstructScore effectively elicits GPT-4's selfcorrection capability and improves its translation quality.Despite InstructScore's oracle-like role (which it can access reference text to make error annotations), it does not provide explicit corrections, requiring GPT-4 to rely on its internal knowledge for corrections.</p>
<p>still relies on LLM's self-correction capability.</p>
<p>In Figure 9, we demonstrate that external feedback with accurate assessment can significantly lower the model's bias at iterative refinement (shown at the lower right of the figure.All dotted curves are below solid curves with corresponding colors).Interestingly, both Gemini and GPT-4's bias estimation is improved throughout the refinement process, as the external feedback model can over-penalize low-quality outputs.As refinement proceeds, the external feedback model converges to BLEURT quality assessment that samples achieve improved quality.Most importantly, we demonstrate that all LLMs with external feedback can elicit their self-correction ability with consistent BLEURT improvements at self-refine iterations.We include a case study example in Table 4.Our finding of model improvement is consistent with prior study (Xu et al., 2024) and we further demonstrate that external feedback can significantly reduce self-bias.Larger Model Reduces Self-Bias.In Figure 10, we demonstrate that LLMs with larger parameter size can have less self-bias throughout self-refinement steps.Specifically, we tested the LLaMA2 models with 7B, 13B, and 70B parameters on Yoruba-to-English (Yor-En) translation tasks.Our findings indicate that while the LLaMA2-70B model exhibits self-bias in the earlier iterations, its self-bias begins to plateau after the 5th iteration.In contrast, the 7B and 13B models continue to amplify their self-bias in later iterations.This observation aligns with prior work (Huang et al., 2023a), which posited that larger LLMs possess better self-refinement capabilities.Our study contributes to this discussion from the perspective of self-bias, proposing that larger LLMs are more resilient to self-bias.Consequently, they can assess their own outputs more accurately and possess a greater capacity for self-correction.</p>
<p>Conclusion</p>
<p>In this study, we define and quantify self-bias in LLMs with two principled estimated statistics.Our experiments across six LLM families, four languages, and three tasks reveal that self-bias is prevalent in self-refine or self-rewarding pipelines.This biased self-feedback leads to false positive objectives, hindering performance improvements during iterative refinement.Further analysis reveals that while LLM improves fluency and understanding of its generated text, they do not necessarily progress in the intended direction, such as improving quality in machine translation or expanding coverage in concept-to-word generation.Instead, LLMs tend to favor texts that adhere to their inherent styles.Finally, our research suggests that larger models are more resistant to self-bias, and incorporating external feedback significantly reduces bias, leading to performance improvements in LLMs.</p>
<p>Limitations</p>
<p>In this study, we focus on quantifying the self-bias exhibited by LLMs in the self-refine pipeline.We demonstrate that self-bias will be amplified in the self-refine or self-rewarding pipeline and negatively impacts the optimization process.However, in subsequent research, it would be worthwhile to explore the measurement of bias that exists between different LLMs, as well as the bias that arises when comparing original models and their knowledgedistilled counterparts.The following questions remain open: Does LLM have more bias towards LLMs that follow the same pretraining procedure, data, or learning objectives?Does LLM have more bias to the LLMs within the same language model families?Do knowledge-distilled LLMs have more biases over the original LLMs, such as Vicuna to GPT4 or Alpaca to ChatGPT?We leave these interesting avenues for future research.</p>
<p>Ethical Statement</p>
<p>All the benchmark data that we used during experiments is publicly available.We assure that the benchmark data does not contain risk or toxic content.The annotater was compensated fairly and did not disclose any privacy information during the annotation process.All the open sourced models can be accessed online and all the closed source models have publicly accessible APIs.The annotaters were allowed to label sensitive information if necessary.The annotater is fully aware that the data we collected from him/her will be used for research purposes.The total human annotation period took six hours and the annotator was paid above local minimum wage.We used Mistral Medium, Grammarly and ChatGPT API to polish some of our writings.</p>
<p>The findings of this research have far-reaching implications for the broader linguistic and technological communities, particularly in the preservation and revitalization of endangered or low-resource languages.By identifying and mitigating self-bias in large language models (LLMs), this work paves the way for significant improvements in machine translation for languages that are underrepresented in digital platforms and datasets.</p>
<p>The ability to reduce bias in the self-refine pipeline of LLMs can lead to more accurate and nuanced translations, thereby enhancing the quality and accessibility of digital content in low-resource languages.This advancement is critical for preserving the cultural heritage and knowledge embodied in these languages, which are at risk of disappearing.Through improved translation capabilities, communities can more easily access global information in their native languages, fostering educational opportunities and cultural exchange.This contributes to the preservation of linguistic diversity and promotes a more inclusive digital ecosystem.</p>
<p>A Model API/Checkpoints</p>
<p>This section provides a pointer to checkpoints that we used during experiment.All opensource models are available on the Hugging Face platform.For LLaMA2, we use "metallama/Llama-2-(7, 13, 70)b-chat-hf" respectively.</p>
<p>For Mixtral MOE, we use "mistralai/Mixtral-8x7B-Instruct-v0.1".For DeepSeekMoE, we use "deepseek-ai/deepseek-moe-16b-chat".For InstructScore, we use "xu1998hz/InstructScore".</p>
<p>For the translation model Madlad400-10b, we use "google/madlad400-10b-mt". We used GPT-3.5-Turboand GPT-4 from OpenAI platform (https://platform.openai.com).We use gemini-pro from Google Gemini API.</p>
<p>B Quantile Mapping</p>
<p>While BLEURT (Sellam et al., 2020) correlates highly with human judgments (Freitag et al., 2022), its scale of roughly 0 to 1 is incompatible with the MQM human annotations, which range from -25 to 0. A linear mapping is not feasible, as the BLEURT score is not calibrated to the human score, meaning a BLEURT score of 0.8 does not correspond to -5 in MQM annotations.</p>
<p>To address this issue, we employ quantile mapping (Cannon et al., 2015) to transform the BLEURT score into the distribution of human scores.This method involves learning a mapping function that maps the quantiles or percentiles of the predictive distribution to those of the observed distribution.In this case, our predictive distribution is derived from the BLEURT score distribution, while our observed distribution comes from the corresponding human score distribution.</p>
<p>We utilize the WMT22 shared metric task (Freitag et al., 2022) to obtain mapped BLEURT-human scoring pairs.In this shared metric task, each translation generated by different translation model is rated by humans using the MQM human rating scale.We also run BLEURT on the same set of translations to obtain BLEURT scores, resulting in 28125 mapped BLEURT-human scoring pairs.</p>
<p>We then perform the following steps: 1) Separately sort the data of the two distributions in ascending order.2) Compute the cumulative distribution function (CDF) for each distribution.3) Learn an interpolation function that maps the percentiles of the first distribution to the percentiles of the sec-</p>
<p>%LDVRQ0DWK5HDVRQLQJ 'VNHZRQ0DWK5HDVRQLQJ</p>
<p><em>37 </em>HPLQL *377XUER This process maps the BLEURT score distribution to the human score distribution (from -25 to 0) while preserving the relative ordering of BLEURT scores.In our experiments, we used the latest BLEURT model, BLEURT-20 checkpoint (Pu et al., 2021), which demonstrates the highest correlation to the human judgments among its variants.</p>
<p>C Gemini's Skewness at Translation</p>
<p>Specifically, in the Java-English (Jav-En) language pair, Gemini initially assigns lower quality scores to its output compared to BLEURT assessments during early iterations, resulting in an underestimation of output performance.This phenomenon accounts for the decrease in distance skewness at the beginning, as the right-skewed distribution becomes more neutral.However, as bias accumulates in later iterations, the distribution shifts towards a left-skewed distribution, leading to an increase in distance skewness.</p>
<p>D Self-consistency results on Math reasoning</p>
<p>We slightly modify the self-refine pipeline by replacing the self-evaluation with self-consistency verification (Huang et al., 2023a).Namely, with the initial solution, LLM will generate an additional ten reasoning paths and a majority vote for a proposed answer.If the proposed answer is inconsistent with the prior solution, we will output a binary score of 0, and the initial answer will be replaced by the proposed answer.Otherwise, we will output a score of 1, and no change will be made to the initial answer.Figure 11 illustrates that all large language models (LLMs) exhibit an increase in bias and skewness estimation in the iterative self-consistency pipeline.This suggests that LLMs introduce self-biases towards certain reasoning paths during self-refine, ultimately leading to a biased ensemble across multiple reasoning paths.</p>
<p>E Additional Results</p>
<p>In Table 5, we include human evaluation results and GPT-4's quality scores for the 0th and 10th iteration of refinement generation at Yorba-to-English.In Table 6, we include human evaluation and GPT-3.5-Turbo'squality assessment on the 0th and 10th iteration of refinement generation at Yorba-to-English.In Table 7, we include human evaluation and Gemini's quality assessment on the 0th and 10th iterations of refinement generation.In Figure 12, we include full bias and distance skewness for Yor-En, Jav-En, Arm-En and Ig-En translations on Flo-res200.</p>
<p>Output for translation:</p>
<p>Can you please turn off the WiFi, I'm done.</p>
<p>Table 8: Those are the translation in context learning example we used to prompt all LLMs across four language directions at Flores200.In this example, the source translation is Yourba text "O ko ago ilekun WiFi, O wi.".The English output text is from LLaMA2-7B's generation "Can you please turn off the WiFi, I'm done.".</p>
<p>In-context-learning prompt for LLM's Self-feedback at translation: You are an annotator for the quality of machine translation.Your task is to identify errors and assess the quality of the translation.</p>
<p>Based on the source segment and machine translation surrounded with triple backticks, identify error types in the translation and classify them.The categories of errors are: accuracy (addition, mistranslation, omission, untranslated text), fluency (character encoding, grammar, inconsistency, punctuation, register, spelling), locale convention (currency, date, name, telephone, or time format) style (awkward), terminology (inappropriate for context, inconsistent use), non-translation, other, or no-error.Each error is classified as one of three categories: critical, major, and minor.Critical errors inhibit comprehension of the text.Major errors disrupt the flow, but what the text is trying to say is still understandable.Minor errors are technically errors, but do not disrupt the flow or hinder comprehension.</p>
<p>Source: "'大众点评乌鲁木齐家居商场频道为您提供高铁居然之家地址，电话，营业时间等最新商户信息， 找装修公司，就上大众点评"' Translation: "'Urumqi Home Furnishing Store Channel provides you with the latest bussiness information such as the address, telephone number, bussiness hours, etc., of high-speed rail, and find a decoration company, and go to the reviews."'Annotate errors in the translation.MQM annotations:</p>
<p>"of high-speed rail" is a critical accuracy/addition error "go to the reviews" is a major accuracy/mistranslation error "etc.," is a minor style/awkwards error Source: "'I do apologise about this, we must gain permission from the account holder to discuss an order with another person, I apologise if this was done previously, however, I would not be able to discuss this with yourself without the account holders permission."'Translation: "'Ich entschuldige mich dafür, wir müssen die Erlaubnis einholen, um eine Bestellung mit einer anderen Person zu besprechen.Ich entschuldige mich, falls dies zuvor geschehen wäre, aber ohne die Erlaubnis des Kontoinhabers wäre ich nicht in der Lage, dies mit dir involvement."'Annotate errors in the translation.MQM annotations:</p>
<p>'involvement' is a major accuracy/mistranslation error 'the account holder' is a major accuracy/omission error 'wäre' is a minor fluency/grammar error 'dir' is a minor fluency/register error Source: "'Talks have resumed in Vienna to try to revive the nuclear pact, with both sides trying to gauge the prospects of success after the latest exchanges in the stop-start negotiations."'Translation: "'Ve Vídni se ve Vídni obnovily rozhovory o oživení jaderného paktu, přičemže obě partaje se snaží posoudit vyhlídky na úspěch po posledních výměnách v jednáních."'Annotate errors in the translation.MQM annotations:</p>
<p>'ve Vídni' is a major accuracy/addition error 'the stop-start' is a major accuracy/omission error 'partaje' is a minor terminology/inappropriate for context error Source: "'Talks have resumed in Vienna to try to revive the nuclear pact, with both sides trying to gauge the prospects of success after the latest exchanges in the stop-start negotiations.</p>
<p>Figure 2 :
2
Figure 2: Bias( θ) = 0 does not guarantee a symmetric distribution of E[ θ] − θ.One tail could be long and thin, while the other is short and fat (shown in the right figure).We use distance skewness to measure the asymmetry of distribution.Therefore, using two meta-metrics as complimentary, we can measure the self-bias of LLM.</p>
<p>Figure 3 :
3
Figure3: Average Bias and Dskew estimations for Yor-En, Jav-En, Arm-En, and Ig-En translations on FLo-res200, with the x-axis showing self-refine steps, reveal that all LLMs exhibit self-bias, where open-source LLMs exhibit higher levels than GPT-4 and Gemini.</p>
<p>Figure 6 :
6
Figure6: Bias and distance skewness in generated texts from GPT-4, GPT-3.5-Turbo, and Gemini are measured on MATH testing set throughout the self-refinement steps.Results show an increase in bias and skewness of some math solutions during iterative self-refine.</p>
<p>Figure 7 :
7
Figure7: We measure the fluency and understandability aspects of GPT-4, GPT-3.5-Turbo, and Gemini's generated texts at Yor-En through self-refine steps.Despite no gains in quality, all LLMs have consistent performance improvements in fluency and understandability.</p>
<p>Figure 10 :
10
Figure 10: We show that bias and distance skewness estimation on LLaMA-2 7B, 13B, and 70B models at Yor-En translation across self-refinement steps.LLM with larger parameter size can have less self-bias.</p>
<p>Figure 11 :
11
Figure 11: Bias and distance skewness in generated texts from GPT-4, GPT-3.5-Turbo, and Gemini are measured on MATH testing set throughout the self-refinement steps.Results show an increase in bias and skewness during iterative self-consistency, causing biased ensembles in reasoning paths.</p>
<p>Figure 12 :
12
Figure12: Full Bias and Dskew estimations for Yor-En, Jav-En, Arm-En, and Ig-En translations on FLores200, with the x-axis showing self-refine steps, reveal that all LLMs exhibit self-bias, where open-source LLMs exhibit higher levels than GPT-4 and Gemini.</p>
<p>"' Translation: "'Ve Vídni se ve Vídni obnovily rozhovory o oživení jaderného paktu, přičemže obě partaje se snaží posoudit vyhlídky na úspěch po posledních výměnách v jednáních."'Annotate errors in the translation.MQM annotations: Source: "'O ko ago ilekun WiFi, O wi."' Translation: "'He locked the WiFi door, he said."'Annotate errors in the translation.MQM annotations: Output for Feedback: 'He locked the WiFi door' is a critical accuracy/mistranslation error</p>
<p>Table 1 :
1
We report human evaluation on GPT-4, GPT-3.5-Turbo and Gemini's quality assessment on 0th and 10th iteration of refinement generation at Yor-En.We used Bias and Dskew estimation to demonstrate bias found by human evaluation.All LLMs have significantly increased self-bias after 10 iterations.
TurboGemini</p>
<p>Table 2 :
2
We report human evaluation of format accuracy at six LLM's outputs.We observed that all LLMs have either perfect or nearly perfect format at first and fifth iteration of self-feedback at Yor-En translation.Mixtral stands for MixtralMOE and DeepS stands for DeepSeek-MoE that we used in the experiment.</p>
<p>Table 5 :
5
This table presents human evaluation resultsand GPT-4's quality scores for the 0th and 10th iteration of refinement generation performed at Yor-En.Bias and Dskew estimates are included to quantify the biases identified through human evaluation.
Human Evaluation Human GPT-3.5 Bias Dskew0th Iteration-22.2-2.6119.6 0.80310th Iteration-21.9-0.0321.9 0.885Table 6: We report human evaluation and GPT-3.5-Turbo's quality assessment on the 0th and 10th iterationof refinement generation at Yor-En.Human Evaluation Human Gemini Bias Dskew0th Iteration-17.3-8.929.62 0.35510th Iteration-18.3-0.7217.6 0.766Table 7: We report human evaluation and Gemini'squality assessment on the 0th and 10th iterations ofrefinement generation at Yor-En.</p>
<p>Table 9 :
9
Those are the feedback in context learning examples (three examples) that we used to prompt all LLMs across four language directions at Flores200.In this example, the source translation is Yourba text "O ko ago ilekun WiFi, O wi.".The English candidate text is "He locked the WiFi door" from GPT-4.GPT-4's self-feedback outputs "'He locked the WiFi door' is a critical accuracy/mistranslation error".</p>
<p>AcknowledgementsThis work was supported by the National Science Foundation award #2048122.L.L. is partly supported by a gift from Apple Inc.The views expressed are those of the author and do not reflect the official policy or position of the funding agencies.We thank Yuanjing Wei for conducting the human evaluation in our experiment.Source: "'大众点评乌鲁木齐家居商场频道为您提供高铁居然之家地址，电话，营业时间等最新商户信息，找 装修公司，就上大众点评"' Translation: "'Urumqi Home Furnishing Store Channel provides you with the latest bussiness information such as the address, telephone number, bussiness hours, etc., of high-speed rail, and find a decoration company, and go to the reviews."'Feedback: 'of high-speed rail' is a critical accuracy/addition error 'go to the reviews' is a major accuracy/mistranslation error 'etc.,' is a minor style/awkward error Improved Chinese-to-English translation: Dianping Urumqi Renovation and Design Channel will provide you with the address, phone number, operation time and other information of HSR Easyhome, and please come to Dianping if you are looking for a renovation company.Source: "'I do apologise about this, we must gain permission from the account holder to discuss an order with another person, I apologise if this was done previously, however, I would not be able to discuss this with yourself without the account holders permission."'Translation: "'Ich entschuldige mich dafür, wir müssen die Erlaubnis einholen, um eine Bestellung mit einer anderen Person zu besprechen.Ich entschuldige mich, falls dies zuvor geschehen wäre, aber ohne die Erlaubnis des Kontoinhabers wäre ich nicht in der Lage, dies mit dir involvement."'Feedback: 'involvement' is a major accuracy/mistranslation error 'the account holder' is a major accuracy/omission error 'wäre' is a minor fluency/grammar error 'dir' is a minor fluency/register error Improved English-to-German translation: Ich bitte um Entschuldigung, aber wir benötigen das Einverständnis des Kontoinhabers, um eine Bestellung mit einer anderen Person zu besprechen, falls es schon eingeholt wurde, entschuldige ich mich, aber ich kann dies ohne das Einverständnis des Kontoinhabers nicht mit Ihnen besprechen.Source: "'Talks have resumed in Vienna to try to revive the nuclear pact, with both sides trying to gauge the prospects of success after the latest exchanges in the stop-start negotiations."'Translation: "'Ve Vídni se ve Vídni obnovily rozhovory o oživení jaderného paktu, přičemže obě partaje se snaží posoudit vyhlídky na úspěch po posledních výměnách v jednáních."'Feedback: 've Vídni' is a major accuracy/addition error 'the stop-start' is a major accuracy/omission error 'partaje' is a minor terminology/inappropriate for context error Improved English-to-Cezch translation: Ve Vídni byly obnoveny rozhovory o oživení jaderného paktu a obě strany se snaží odhadnout, jaké jsou vyhlídky na úspěch po posledních výměnách názorů v rámci přerušených jednání.Table10: Those are the refinement in context learning examples (three examples) that we used to prompt all LLMs across four language directions at Flores200.In this example, the source translation is Yourba text "O ko ago ilekun WiFi, O wi.".The English text is GPT-4's output "He locked the WiFi door.".We gathered GPT-4's self-feedbak: "'He locked the WiFi door' is a critical accuracy/mistranslation error".GPT-4 outputs a refined output: "He secured the WiFi connection, he said.",according to the feedback and prior step generation.Prompt for LLM's initial generation for reasoning: You are a competitive math problem solver.Please generate a step-by-step solution.Your final answer must be enclosed in LaTeX's boxed tag.How many vertical asymptotes does the graph of y = 2 x 2 +x−6 have?Output for math answer: A vertical asymptote is a vertical line that the graph of a function approaches but never crosses.A function has a vertical asymptote at x = k if it has infinite limits as x approaches k from the left or right.For the rational function y = 2 x 2 +x−6 , we know that it could have vertical asymptotes wherever the denominator is zero, because division by zero is undefined and yields infinity.Therefore, to find the vertical asymptotes, we solve the equation x 2 + x − 6 = 0. We apply factorization and get (x − 2)(x + 3) = 0.This gives us x = 2 and x = −3.In any case, we need to check if any of these could be holes of vertical asymptotes.This would be the case if x = 2 or x = −3 also made the numerator zero.However, the numerator is a constant and won't be zero.Therefore this function has two vertical asymptotes at x = 2 and x = −3.The answer is 2 .Table11: This is the prompt we used to generate mathematical reasoning.The output in this example comes from GPT-4.We first used this prompt to generate one reasoning path as initial generation.Then, we sample 10 different reasoning paths and use majority vote to find the most confident answer.To generate self-feedback, we compare majority voted answer and initial answer.If they are matched, feedback indicates answer is correct and no change should be made.If they are not matched, the feedback indicates answer is wrong and inital answer will be replaced by the majority voted answer.We alternate above two steps and iteratively refine the reasoning paths.Concept-to-text prompt for LLM's initial generation: You are generating text based on specified words.Do not provide any explanations or text apart from the text output.Please generate a sentence that contains the exact string matches for the following concepts:['vest', 'snow', 'remove', 'silhouette', 'dunk', 'step', 'item', 'toddler', 'plaster', 'wall', 'piece', 'tool', 'circle', 'pull', 'wrap', 'headphone', 'deck', 'suit', 'press', 'mother', 'pit', 'cupcake', 'priest', 'rink', 'counter', 'rest', 'piano', 'wine', 'serve', 'winter'] Output for Concept-to-Text generation:In the winter, the toddler wearing a vest, pressed a piece of wet plaster against the wall in a circle, while his mother baking a cupcake was removing the snow from the deck with a tool; switching steps to the rhythm of a piano piece echoing from the headphones wrapped around her neck, she caught a glimpse of the priest serving wine, his silhouette dunking a cupcake into a pit at the rink counter, under the restful gaze of a suit-clad man.Table12: This is the concept-to-text prompt we used to test all LLMs at Commongen Hard.This example contains the output from GPT-4.Concept-to-text in context learning prompt for LLM's self-feedback:We want to create a sentence that contains all the specified concepts.Please provide feedback on the following sentences.The feedback should list all missing concepts.If all concepts are covered, output 'all covered' Concepts: ['dog', 'frisbee', 'catch', 'throw'] Generated Sentence: A dog leaps to catch a thrown frisbee.Feedback: all covered Concepts: ['dog', 'frisbee', 'throw'] Generated Sentence: Two dogs are throwing frisbees at each other .Feedback: ['catch']Concepts:['vest', 'snow', 'remove', 'silhouette', 'dunk', 'step', 'item', 'toddler', 'plaster', 'wall', 'piece', 'tool', 'circle', 'pull', 'wrap', 'headphone', 'deck', 'suit', 'press', 'mother', 'pit', 'cupcake', 'priest', 'rink', 'counter', 'rest', 'piano', 'wine', 'serve', 'winter'] Generated Sentence: In the winter, the toddler wearing a vest, pressed a piece of wet plaster against the wall in a circle, while his mother baking a cupcake was removing the snow from the deck with a tool; switching steps to the rhythm of a piano piece echoing from the headphones wrapped around her neck, she caught a glimpse of the priest serving wine, his silhouette dunking a cupcake into a pit at the rink counter, under the restful gaze of a suit-clad man.Feedback:Output for Concept-to-Text feedback: all covered Table13: This is the in-context learing (ICL) concept-to-text prompt (two ICL examples) we used to generate LLM's self-feedback at Commongen Hard.This example contains the self-feedback from GPT-4.Concept-to-text prompt for LLM's refinement: Concepts: ['fruit', 'motorcycle', 'perform', 'jacket', 'vehicle', 'place', 'mat', 'walk', 'world', 'area', 'kiss', 'mother', 'pass', 'report', 'club', 'axis', 'tricep', 'patient', 'listen', 'owner', 'uniform', 'floor', 'hamburger', 'use', 'wine', 'cross', 'bull', 'sell', 'lawn', 'friend']Generated sentence: In a world where a fruit can perform like a motorcycle, I found my mother wearing a leather jacket, passing by her vehicle to walk towards the place where she runs a club.Not far from that area, my friend, a patient person, was ready to sell mats, used for gym workouts, especially for triceps.Being the owner of the shop, he always wears a uniform.While I was listening to this report, sitting cross-legged on the floor and munching my hamburger, I missed seeing a bull wandering on the lawn just outside.Just then, I received an unexpected kiss from my wife who brought me a glass of wine from our cellar which also acts as an axis for all the happenings of my life.Missing Concepts: ['use', 'lawn'] Please revise generated sentence which covers all missing concepts and all given concepts, with the exact string matches.New sentence:Output for Concept-to-Text refinement: In a world where a fruit can perform like a motorcycle, I found my mother wearing a leather jacket, passing by her vehicle to walk towards the place where she runs a club.Not far from that area, my friend, a patient person, was ready to sell mats, used for gym workouts, especially for triceps.Being the owner of the shop, he always wears a uniform.While I was listening to this report, sitting cross-legged on the floor and munching my hamburger, I missed seeing a bull wandering on the lawn just outside.Just then, I received an unexpected kiss from my wife who brought me a glass of wine from our cellar which also acts as an axis for all the happenings of my life.Table14: This is the concept-to-text refinement prompt we used to generate LLM's refinement at Commongen Hard.This example contains the refinement from GPT-4.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Bias correction of gcm precipitation by quantile mapping: How well do methods preserve changes in quantiles and extremes. Alex J Cannon, Stephen R Sobie, Trevor Q , 10.1175/JCLI-D-14-00754.1Journal of Climate. 28172015Murdock</p>
<p>Speak, memory: An archaeology of books known to ChatGPT/GPT-4. Kent Chang, Mackenzie Cramer, Sandeep Soni, David Bamman, 10.18653/v1/2023.emnlp-main.453Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2023Singapore</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>James Marta R Costa-Jussà, Onur Cross, Maha Çelebi, Kenneth Elbayad, Kevin Heafield, Elahe Heffernan, Janice Kalbassi, Daniel Lam, Jean Licht, Maillard, arXiv:2207.04672No language left behind: Scaling human-centered machine translation. 2022arXiv preprint</p>
<p>Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. Daniel Deutsch, Rotem Dror, and Dan Roth. Damai Dai, Chengqi Deng, Chenggang Zhao, R X Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Y Wu, Zhenda Xie, Y K Li, Panpan Huang, Fuli Luo, Chong Ruan, Zhifang Sui, Wenfeng Liang, 10.18653/v1/2022.emnlp-main.753Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2024. 2022On the limitations of reference-free evaluations of generated text</p>
<p>Experts, errors, and context: A large-scale study of human evaluation for machine translation. Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, Wolfgang Macherey, 10.1162/tacl_a_00437Transactions of the Association for Computational Linguistics. Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins92021</p>
<p>Results of WMT22 metrics shared task: Stop using BLEU -neural metrics are better and more robust. Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics</p>
<p>Gptscore: Evaluate as you desire. Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu ; Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, Nando De Freitas, 2023. 2023Reinforced self-training (rest) for language modeling</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>A closer look at the selfverification abilities of large language models in logical reasoning. Ruixin Hong, Hongming Zhang, Xinyu Pang, Dong Yu, Changshui Zhang, ; Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, Jiawei Han, 10.18653/v1/2023.emnlp-main.67CoRR, abs/2311.07954Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023. 2023aLarge language models can self-improve</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 10.48550/arXiv.2310.01798CoRR, abs/2310.017982023b</p>
<p>Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Szymon Yang, Teven Antoniak, Théophile Le Scao, Thibaut Gervet, Thomas Lavril, Timothée Wang, William El Lacroix, Pei Sayed, Bosi Ke, Zhuoer Wen, Xiao Feng, Xuanyu Liu, Jiale Lei, Shengyuan Cheng, Aohan Wang, Yuxiao Zeng, Hongning Dong, Jie Wang, Minlie Tang, Huang, 10.18653/v1/2023.wmt-1.1CoRR, abs/2311.18702Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popović, and Mariya Shmatova. Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Philipp Koehn, Benjamin Marie, Christof Monz, Makoto Morishita, Kenton Murray, Anton Dvorkovich, Christian Federmann, Mark Fishel; SingaporeAssociation for Computational Linguistics2024. 2023. 2023Proceedings of the Eighth Conference on Machine Translation</p>
<p>GEMBA-MQM: Detecting translation quality error spans with GPT-4. Tom Kocmi, Christian Federmann, 10.18653/v1/2023.wmt-1.64Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingaporeAssociation for Computational Linguistics2023</p>
<p>CommonGen: A constrained text generation challenge for generative commonsense reasoning. Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae , Myung Kim, Dongyeop Kang ; Sneha, Isaac Rayburn Kudugunta, Biao Caswell, Xavier Zhang, Derrick Garcia, Aditya Xin, Romi Kusupati, Ankur Stella, Orhan Bapna, Firat, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Wangchunshu Bill Yuchen Lin, Ming Zhou, Pei Shen, Chandra Zhou, Yejin Bhagavatula, Xiang Choi, Ren, Online. Association for Computational Linguistics2023. 2023. 2020Findings of the Association for Computational Linguistics: EMNLP 2020</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023a</p>
<p>Llms as narcissistic evaluators: When ego inflates evaluation scores. Yiqi Liu, Nafise Sadat Moosavi, Chenghua Lin, 2023b</p>
<p>SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, 10.18653/v1/2023.emnlp-main.557Advances in Neural Information Processing Systems, 36. Potsawee Manakul, Adian Liusie, and Mark Gales. Association for Computational Linguistics2024. 2023Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Singapore</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, William Yang, Wang , 10.48550/arXiv.2308.03188CoRR, abs/2308.031882023</p>
<p>Learning compact metrics for mt. Amy Pu, Won Hyung, Ankur P Chung, Sebastian Parikh, Thibault Gehrmann, Sellam, Proceedings of EMNLP. EMNLP2021</p>
<p>. Thibault Sellam, Dipanjan Das, Ankur P Parikh, </p>
<p>Bleurt: Learning robust metrics for text generation. Proceedings of ACL. ACL</p>
<p>GPT-4 doesn't know it's wrong: An analysis of iterative prompting for reasoning problems. CoRR, abs/2310.12397. Gabor Szekely and Tamás Móri. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, 10.1081/STA-100105689Advances in Neural Information Processing Systems. Kaya Stechly, Matthew Marquez, Subbarao Kambhampati, 2024. 2023. 200636A characteristic measure of asymmetry and its application for testing diagonal symmetry</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805arXiv:2307.092882023. Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, 2023arXiv preprint</p>
<p>Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, Victor Carbune, 10.48550/arXiv.2311.08516abs/2311.08516Llms cannot find reasoning errors, but can correct them! CoRR. 2023</p>
<p>Can large language models really improve by self-critiquing their own plans?. Karthik Valmeekam, Matthew Marquez, Subbarao Kambhampati, 10.48550/arXiv.2310.08118CoRR, abs/2310.081182023</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Llmrefine: Pinpointing and refining large language models via finegrained actionable feedback. Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang, Zhongtao Liu, William Yang Wang, Lei Li, Markus Freitag ; Zhenqiao Song, Markus Freitag, William Wang, Lei Li, 10.18653/v1/2023.emnlp-main.365Proceedings of 2024 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) -Findings. Wenda Xu, Danqing Wang, Liangming Pan. 2024 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL) -Findings. Wenda Xu, Danqing Wang, Liangming PanAssociation for Computational Linguistics2024. 2023Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Singapore</p>
<p>Judging LLM-as-ajudge with MT-bench and chatbot arena. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar ; Lianmin, Wei-Lin Zheng, Ying Chiang, Siyuan Sheng, Zhanghao Zhuang, Yonghao Wu, Zi Zhuang, Zhuohan Lin, Dacheng Li, Eric Li, Hao Xing, Joseph E Zhang, Ion Gonzalez, Stoica, Thirtyseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2024. 2023Self-rewarding language models</p>
<p>Towards a unified multi-dimensional evaluator for text generation. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Ji Heng, Jiawei Han, 10.18653/v1/2022.emnlp-main.131Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>In-context-learning prompt for LLM's refinement at translation: Please fix all errors. You can rewrite translation if translation is bad. </p>            </div>
        </div>

    </div>
</body>
</html>
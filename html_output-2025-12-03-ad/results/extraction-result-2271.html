<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2271 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2271</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2271</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-267782940</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.14424v3.pdf" target="_blank">Automating psychological hypothesis generation with AI: when large language models meet causal graph</a></p>
                <p><strong>Paper Abstract:</strong> Leveraging the synergy between causal knowledge graphs and a large language model (LLM), our study introduces a groundbreaking approach for computational hypothesis generation in psychology. We analyzed 43,312 psychology articles using a LLM to extract causal relation pairs. This analysis produced a specialized causal graph for psychology. Applying link prediction algorithms, we generated 130 potential psychological hypotheses focusing on “well-being”, then compared them against research ideas conceived by doctoral scholars and those produced solely by the LLM. Interestingly, our combined approach of a LLM and causal graphs mirrored the expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p = 0.007 and t(59) = 4.32, p < 0.001, respectively). This alignment was further corroborated using deep semantic analysis. Our results show that combining LLM with machine learning techniques such as causal knowledge graphs can revolutionize automated discovery in psychology, extracting novel insights from the extensive literature. This work stands at the crossroads of psychology and artificial intelligence, championing a new enriched paradigm for data-driven hypothesis generation in psychological research.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2271.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2271.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMCG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Causal Graph algorithm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated pipeline combining a large language model (GPT-4) to extract causal concept pairs from full-text psychology articles, storage in a Neo4j causal knowledge graph, node embedding + link prediction to propose novel concept pairs, and GPT-4 prompting to convert high-probability pairs into explicit research hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based Causal Graph (LLMCG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three-stage system: (1) literature retrieval from PMC Open Access subset and text cleaning; (2) causal pair extraction using GPT-4 on 43,312 psychology articles with engineered prompts to output standardized cause-effect pairs (JSON) and human-in-the-loop sampling for quality control; extracted pairs are stored as nodes/edges in a Neo4j graph (197k concepts, 235k edges); (3) hypothesis generation by (a) computing node embeddings via node2vec, (b) computing similarity scores for unconnected node pairs and filtering by threshold, (c) assessing link likelihood using neighbour-overlap (Jaccard) measures, (d) ranking candidate pairs and selecting top pairs, and (e) prompting GPT-4 to produce natural-language causal hypotheses from the candidate concept pairs. Expert vetting (hand-selection) is optionally applied to pick higher-quality hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>psychology (positive psychology, well-being)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration (automated hypothesis generation / discovery)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Primary: human expert ratings on 'novelty' (experts rated hypotheses and scores were standardized per reviewer as z-scores; mean/median/max z-scores used for comparisons). Secondary: deep semantic measures — semantic distance in BERT embedding space (BERT encodings of hypotheses and pairwise semantic distance distributions; visualized with t-SNE).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Reported group-level statistical outcomes: LLMCG (Random-selected) novelty > Control-Claude: t(59)=3.34, p=0.007, Cohen's d=0.8809 (mean z-score comparison). Expert-selected LLMCG showed a trend vs Control-Claude (t(59)=2.49, p=0.085, Cohen's d=0.6226) and significant superiority on max novelty (t(59)=3.12, p=0.014, d=0.6987). In ablation vs GPT-4 alone, LLMCG novelty differences were highly significant (see Table 6: novelty comparisons: t ≈ 6.60, p < 0.0001, Cohen's d ≈ 0.73). (Scores reported as standardized z-scores and tested via ANOVA/t-tests.)</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Feasibility was not explicitly defined as a separate metric; the paper used 'usefulness' (utility) as the practical/feasibility proxy measured by the same human expert ratings (experts rated usefulness/utility). No computational feasibility proxy (e.g., resource/cost/technical-complexity score) was used.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>No specific feasibility numeric scale reported beyond human usefulness ratings. Group-level result: no significant differences between groups on usefulness (group factor non-significant; reported F(3,116)=5.25 with p=0.553 for mean usefulness distribution), and comparisons vs GPT-4 showed non-significance (Table 6: usefulness mean t ≈ 1.31, p ≈ 0.1937, Cohen's d ≈ 0.149).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>No direct quantitative trade-off analysis between novelty and feasibility/usefulness is reported (no correlation or Pareto analysis between novelty and usefulness). Authors discuss novelty and utility as two orthogonal evaluation axes and report that LLMCG improved novelty while usefulness differences were not significant, but they do not report explicit tradeoff statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Pipeline-level heuristics to increase novelty/relevance: (a) constrain candidate link set by node2vec similarity threshold; (b) use Jaccard similarity of neighbourhoods to estimate link likelihood and rank candidates; (c) filter and prioritize top-ranked pairs for hypothesis generation; (d) optional human expert hand-selection to focus on higher-quality hypotheses. No explicit multi-objective optimization (novelty vs feasibility) algorithm was applied.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Three senior psychology professors rated novelty and usefulness for 120 hypotheses (randomized). Novelty: grouping factor significant (ANOVA F(3,116)=6.92, p=0.0002, R^2≈15.19%). Pairwise: Random-selected LLMCG > Control-Claude (t(59)=3.34, p=0.007, d=0.8809); Control-Human > Control-Claude (t(59)=4.32 / 4.36 reported across tables, p<0.001, d≈1.12). LLMCG often matched or approached PhD-student novelty. Usefulness: no significant group differences (mean usefulness grouping factor non-significant). Overall: LLMCG produced hypotheses with higher novelty than LLM-only (Claude/GPT-4) while usefulness was comparable across groups.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Control-Human (PhD students), Control-Claude (Claude-2 LLM), GPT-4 alone (ablation), and Expert-selected vs Random-selected partitions within LLMCG.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>LLMCG outperformed an LLM-only baseline (GPT-4 / Claude-2) on human-rated novelty (statistically significant; e.g., LLMCG vs GPT-4: t≈6.60, p<0.0001, Cohen's d≈0.73). LLMCG's novelty was comparable to hypotheses from PhD students (no significant differences in some pairwise tests); usefulness (proxy for feasibility) showed no significant differences across groups. Deep semantic analysis (BERT + t-SNE) supported that LLMCG hypotheses occupied a broader semantic spectrum than LLM-only outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Applied to psychology (well-being). The causal-graph constraint (built from 43k psychology papers) appears to steer LLM creativity toward domain-relevant and more novel hypotheses compared to unconstrained LLM outputs; no claims about other domains were empirically tested in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating psychological hypothesis generation with AI: when large language models meet causal graph', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2271.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2271.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pretrained Transformer 4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model used in this work both to extract causal pairs from text chunks and to render candidate causal-concept pairs into explicit natural-language hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 (as extractor and hypothesis generator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used for (a) causal knowledge extraction from segmented full-text articles with engineered prompts that request JSON-format concept-pair (cause, effect) and relationship type; (b) verification/standardization of extracted pairs during filtering; (c) as a hypothesis generator converting high-probability candidate concept pairs from the graph into textual causal hypotheses. Operated under API token/rate constraints (60 requests / 150k tokens per minute) and chunking (<=4000 tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>psychology (used to extract from psychology literature and generate psychology hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration (information extraction and hypothesis text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Novelty assessed by human expert ratings (same protocol as LLMCG) and by BERT semantic distance analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>GPT-4 alone (ablation) produced hypotheses with significantly lower human-rated novelty than LLMCG in the ablation comparison (LLMCG vs GPT-4 novelty: t≈6.60, p<0.0001, Cohen's d≈0.73; exact per-group mean z-scores not separately listed for GPT-4 in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Usefulness rated by human experts (same instrument as for LLMCG) served as the feasibility/proxy metric.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>No significant difference in usefulness between GPT-4 and LLMCG (Table 6: usefulness mean t ≈ 1.31, p ≈ 0.1937, Cohen's d ≈ 0.149).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>No explicit quantitative tradeoff reported for GPT-4 outputs; compared qualitatively through separate novelty and usefulness ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Not applicable beyond prompt engineering and GPT-4 internal capacities; no graph constraints applied in the GPT-4-only baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>60 GPT-4-generated hypotheses were evaluated by the same three experts and scored lower on novelty than the LLMCG-generated set (statistically significant). Usefulness ratings did not differ significantly from LLMCG.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Compared directly to LLMCG (ablation study) and to human-generated and Claude-2 outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>GPT-4-only hypotheses: lower novelty (significant) relative to LLMCG; usefulness similar (non-significant differences).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Within psychology/well-being, unconstrained LLM outputs tended to be less novel than graph-constrained (LLMCG) outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating psychological hypothesis generation with AI: when large language models meet causal graph', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2271.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2271.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-2 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude-2 (LLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comparative large language model used as a control baseline for hypothesis generation; 50 candidate hypotheses were produced and then filtered by GPT-4 to select top 30 for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Claude-2 (control LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Claude-2 was prompted iteratively (10 hypotheses per prompt) to generate a set of hypotheses focused on well-being; 50 candidate outputs were generated and GPT-4 was used to select the top 30 for downstream human evaluation to ensure comparable complexity/depth.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>psychology (well-being)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration (hypothesis generation baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Human expert novelty ratings (same protocol) and BERT semantic distance analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Control-Claude (Claude-2) had significantly lower novelty ratings than Random-selected LLMCG and Control-Human: pairwise comparisons Control-Claude vs Control-Human t(59)=4.36 p<0.001 d≈1.1192; Control-Claude vs Random-selected LLMCG t(59)=3.34 p=0.007 d≈0.8809 (Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Human expert usefulness ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>Usefulness ratings varied widely for Control-Claude; group-wise usefulness differences with other groups were not significant in many comparisons. Exact mean usefulness per group not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>No explicit tradeoff analysis; Control-Claude tended to produce less novel hypotheses while usefulness did not systematically differ.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Not applicable beyond iterative prompting and subsequent GPT-4 filtering of outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Claude-generated hypotheses rated lower for novelty by human reviewers compared to LLMCG and PhD student groups; usefulness ratings showed high variance and no consistent superiority.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Serves as LLM-only baseline vs LLMCG and human baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Claude-2 outputs scored significantly lower on novelty compared to LLMCG and human PhD outputs (see Table 5 pairwise tests).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>In well-being domain, unconstrained LLM (Claude-2) outputs were less novel than LLMCG outputs derived from causal graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating psychological hypothesis generation with AI: when large language models meet causal graph', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2271.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2271.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LinkPrediction (node2vec+Jaccard)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Node2vec + similarity + Jaccard link prediction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based candidate-hypothesis generation method that embeds graph nodes with node2vec, filters candidate unconnected pairs by embedding similarity, and scores link likelihood using Jaccard similarity of neighbourhoods, then ranks candidates to propose new causal pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>node2vec embedding + similarity threshold + Jaccard neighbourhood scoring</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure: embed graph nodes with node2vec to produce vector representations capturing topology; compute pairwise similarity for unconnected node pairs and discard low-similarity pairs; for high-similarity pairs compute Jaccard similarity of their neighbor sets to estimate probability of a link (plausible causal relation); rank pairs by estimated probability and select top pairs to feed into GPT-4 for hypothesis text generation. This method operationalizes link prediction to propose novel causal hypotheses from the constructed causal graph.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>psychology (applied to a causal concept graph derived from psychology literature)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration (discover latent links / hypotheses in concept graph)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Heuristic ranking by embedding similarity then Jaccard neighbourhood overlap; thresholding to reduce candidate set; downstream LLM generation and optional human expert selection for quality control.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Example case: inferring a plausible link between 'behavioral inhibition system (BIS)' and 'interference' via shared connections to BAS and BAS reward response — demonstrates how topological inference can suggest empirically testable hypotheses in psychology.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating psychological hypothesis generation with AI: when large language models meet causal graph', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2271.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2271.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT+t-SNE semantic analysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT embeddings with t-SNE visualization (deep semantic analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method mapping each candidate hypothesis into BERT embedding space to analyze semantic structure and distances among hypotheses, then reducing dimensionality with t-SNE for visualization and quantitative comparison (semantic distance distributions).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BERT encoding + t-SNE dimensionality reduction</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Each hypothesis is encoded into a high-dimensional vector using BERT; these vectors are compared via pairwise distances (semantic distance) and reduced to 2D using t-SNE for visualization of clusters and dispersion. Semantic distance distributions were then statistically analyzed (ANOVA and post-hoc tests) to compare semantic variety across hypothesis origin groups.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>general scientific-hypothesis semantic analysis (applied to psychology hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>analysis / evaluation (measuring novelty via semantic distance)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Semantic distance in BERT-embedding space (pairwise distances) and clustering as visualized with t-SNE; used as an objective proxy for novelty and semantic diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>ANOVA on semantic distances showed a very large group effect: F(3,1652)=84.1611, p < 0.00001, R^2 ≈ 86.96%. Pairwise differences reported (Table 7) include Control-Claude vs Control-Human t≈16.41 p<0.0001 Cohen's d=2.6949, Control-Claude vs Random-selected LLMCG Cohen's d≈2.1175 (p<0.0001), and others—indicating substantial semantic-distance differences between groups.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Indirect: semantic distance (novelty proxy) differences track human novelty ratings, but no direct measurement of feasibility vs semantic distance tradeoff is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Used to compare LLMCG, LLM-only, and human hypotheses in semantic space.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Control-Human group exhibited larger semantic distances vs other groups (consistent with human novelty judgments). LLMCG groups showed broader semantic coverage than LLM-only (Control-Claude).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>In well-being hypotheses, semantic-space metrics aligned with human novelty evaluations, supporting BERT-distance as an objective proxy for novelty in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating psychological hypothesis generation with AI: when large language models meet causal graph', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2271.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2271.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert ratings of novelty and usefulness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A blinded human-evaluation protocol where three senior psychology professors independently rated each hypothesis on novelty and usefulness; ratings were standardized (z-scores) and aggregated (mean, median, max) for statistical comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Blinded expert rating protocol (novelty & usefulness)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three senior psychology professors (mean age 42.33) were given randomized hypotheses and asked to rate each on two axes: novelty and usefulness (utility). Individual reviewer ratings were standardized to z-scores to account for rater bias; aggregate metrics (mean z-score, median, and maximum) were used for ANOVA and post-hoc pairwise comparisons. Inter-rater agreement quantified with Spearman correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>psychology (evaluation of psychology hypotheses)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>evaluation / quality assessment</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Expert numerical ratings on a novelty scale (converted to standardized z-scores per reviewer and aggregated). Also measured inter-rater agreement with Spearman correlations.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>ANOVA: F(3,116)=6.92, p=0.0002, R^2≈15.19% for mean novelty z-scores. Pairwise significant tests: Random-selected LLMCG vs Control-Claude t(59)=3.34 p=0.007 d=0.8809; Control-Human vs Control-Claude t(59)=4.36 p<0.001 d≈1.1192. Median- and max-based analyses confirmed significant group differences; max novelty ANOVA: F(3,116)=7.20 p=0.0002 R^2≈15.70%.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Expert numerical ratings on 'usefulness' (utility) scale; aggregated similarly and tested across groups.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>Group factor for mean usefulness not significant: F(3,116)=5.25, p=0.553. Pairwise comparisons vs GPT-4/LLMCG not significant (Table 6: usefulness mean t ≈ 1.31, p ≈ 0.1937).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>No explicit correlation or tradeoff statistics between novelty and usefulness reported. Authors note that novelty and usefulness are distinct evaluation axes and present separate analyses for each.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Used z-score standardization to mitigate rater bias and multiple aggregation strategies (mean, median, max) to assess robustness of novelty results.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Expert raters showed moderate inter-rater agreement on novelty and usefulness (Spearman correlations: e.g., reviewer1 vs reviewer2 novelty r=0.387 p<0.0001; reviewer2 vs reviewer3 usefulness r=0.376 p<0.0001). Experts rated LLMCG and PhD-student hypotheses higher on novelty than LLM-only (Claude) outputs; usefulness scores did not significantly differ across groups.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Expert judgments varied by reviewer specializations; disagreements illustrate the subjectivity of novelty/usefulness assessment even among senior domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating psychological hypothesis generation with AI: when large language models meet causal graph', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2271.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2271.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neo4j Graph DB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neo4j graph database (storage and analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graph database used to store extracted causal concept nodes and directed relationships with attributes, enabling graph traversal, node/edge querying, and running graph algorithms for link prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Neo4j (graph database for causal knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Extracted causal pairs and concept metadata stored in Neo4j; nodes represent concepts, edges represent directed causal relationships with attributes (directionality, interpretation). Neo4j facilitated graph-algorithm operations (node2vec embedding, topological queries, neighborhood computations) required for link prediction and candidate discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>knowledge-graph construction for psychology literature</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>knowledge representation and query for hypothesis discovery</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Supports the pipeline's link-prediction and filtering steps (node2vec embeddings computed over graph stored in Neo4j).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Database contained ~197k concepts and ~235k connections derived from 43,312 psychology papers; authors note ~13% of extracted pairs did not align with human expert estimation, indicating room for extraction-improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automating psychological hypothesis generation with AI: when large language models meet causal graph', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Predicting research trends with semantic and neural networks with an application in quantum physics <em>(Rating: 2)</em></li>
                <li>Identification of research hypotheses and new knowledge from scientific literature <em>(Rating: 2)</em></li>
                <li>Unifying large language models and knowledge graphs: a roadmap <em>(Rating: 2)</em></li>
                <li>Causal reasoning and large language models: opening a new frontier for causality <em>(Rating: 2)</em></li>
                <li>Refining the causal loop diagram: a tutorial for maximizing the contribution of domain expertise in computational system dynamics modeling <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2271",
    "paper_id": "paper-267782940",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "LLMCG",
            "name_full": "LLM-based Causal Graph algorithm",
            "brief_description": "An automated pipeline combining a large language model (GPT-4) to extract causal concept pairs from full-text psychology articles, storage in a Neo4j causal knowledge graph, node embedding + link prediction to propose novel concept pairs, and GPT-4 prompting to convert high-probability pairs into explicit research hypotheses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "LLM-based Causal Graph (LLMCG)",
            "system_description": "Three-stage system: (1) literature retrieval from PMC Open Access subset and text cleaning; (2) causal pair extraction using GPT-4 on 43,312 psychology articles with engineered prompts to output standardized cause-effect pairs (JSON) and human-in-the-loop sampling for quality control; extracted pairs are stored as nodes/edges in a Neo4j graph (197k concepts, 235k edges); (3) hypothesis generation by (a) computing node embeddings via node2vec, (b) computing similarity scores for unconnected node pairs and filtering by threshold, (c) assessing link likelihood using neighbour-overlap (Jaccard) measures, (d) ranking candidate pairs and selecting top pairs, and (e) prompting GPT-4 to produce natural-language causal hypotheses from the candidate concept pairs. Expert vetting (hand-selection) is optionally applied to pick higher-quality hypotheses.",
            "research_domain": "psychology (positive psychology, well-being)",
            "problem_type": "open-ended exploration (automated hypothesis generation / discovery)",
            "novelty_metric": "Primary: human expert ratings on 'novelty' (experts rated hypotheses and scores were standardized per reviewer as z-scores; mean/median/max z-scores used for comparisons). Secondary: deep semantic measures — semantic distance in BERT embedding space (BERT encodings of hypotheses and pairwise semantic distance distributions; visualized with t-SNE).",
            "novelty_score": "Reported group-level statistical outcomes: LLMCG (Random-selected) novelty &gt; Control-Claude: t(59)=3.34, p=0.007, Cohen's d=0.8809 (mean z-score comparison). Expert-selected LLMCG showed a trend vs Control-Claude (t(59)=2.49, p=0.085, Cohen's d=0.6226) and significant superiority on max novelty (t(59)=3.12, p=0.014, d=0.6987). In ablation vs GPT-4 alone, LLMCG novelty differences were highly significant (see Table 6: novelty comparisons: t ≈ 6.60, p &lt; 0.0001, Cohen's d ≈ 0.73). (Scores reported as standardized z-scores and tested via ANOVA/t-tests.)",
            "feasibility_metric": "Feasibility was not explicitly defined as a separate metric; the paper used 'usefulness' (utility) as the practical/feasibility proxy measured by the same human expert ratings (experts rated usefulness/utility). No computational feasibility proxy (e.g., resource/cost/technical-complexity score) was used.",
            "feasibility_score": "No specific feasibility numeric scale reported beyond human usefulness ratings. Group-level result: no significant differences between groups on usefulness (group factor non-significant; reported F(3,116)=5.25 with p=0.553 for mean usefulness distribution), and comparisons vs GPT-4 showed non-significance (Table 6: usefulness mean t ≈ 1.31, p ≈ 0.1937, Cohen's d ≈ 0.149).",
            "tradeoff_evidence": "No direct quantitative trade-off analysis between novelty and feasibility/usefulness is reported (no correlation or Pareto analysis between novelty and usefulness). Authors discuss novelty and utility as two orthogonal evaluation axes and report that LLMCG improved novelty while usefulness differences were not significant, but they do not report explicit tradeoff statistics.",
            "optimization_strategy": "Pipeline-level heuristics to increase novelty/relevance: (a) constrain candidate link set by node2vec similarity threshold; (b) use Jaccard similarity of neighbourhoods to estimate link likelihood and rank candidates; (c) filter and prioritize top-ranked pairs for hypothesis generation; (d) optional human expert hand-selection to focus on higher-quality hypotheses. No explicit multi-objective optimization (novelty vs feasibility) algorithm was applied.",
            "human_evaluation": true,
            "human_evaluation_results": "Three senior psychology professors rated novelty and usefulness for 120 hypotheses (randomized). Novelty: grouping factor significant (ANOVA F(3,116)=6.92, p=0.0002, R^2≈15.19%). Pairwise: Random-selected LLMCG &gt; Control-Claude (t(59)=3.34, p=0.007, d=0.8809); Control-Human &gt; Control-Claude (t(59)=4.32 / 4.36 reported across tables, p&lt;0.001, d≈1.12). LLMCG often matched or approached PhD-student novelty. Usefulness: no significant group differences (mean usefulness grouping factor non-significant). Overall: LLMCG produced hypotheses with higher novelty than LLM-only (Claude/GPT-4) while usefulness was comparable across groups.",
            "comparative_baseline": "Control-Human (PhD students), Control-Claude (Claude-2 LLM), GPT-4 alone (ablation), and Expert-selected vs Random-selected partitions within LLMCG.",
            "comparative_results": "LLMCG outperformed an LLM-only baseline (GPT-4 / Claude-2) on human-rated novelty (statistically significant; e.g., LLMCG vs GPT-4: t≈6.60, p&lt;0.0001, Cohen's d≈0.73). LLMCG's novelty was comparable to hypotheses from PhD students (no significant differences in some pairwise tests); usefulness (proxy for feasibility) showed no significant differences across groups. Deep semantic analysis (BERT + t-SNE) supported that LLMCG hypotheses occupied a broader semantic spectrum than LLM-only outputs.",
            "domain_specific_findings": "Applied to psychology (well-being). The causal-graph constraint (built from 43k psychology papers) appears to steer LLM creativity toward domain-relevant and more novel hypotheses compared to unconstrained LLM outputs; no claims about other domains were empirically tested in this paper.",
            "uuid": "e2271.0",
            "source_info": {
                "paper_title": "Automating psychological hypothesis generation with AI: when large language models meet causal graph",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 (used)",
            "name_full": "GPT-4 (Generative Pretrained Transformer 4)",
            "brief_description": "A state-of-the-art large language model used in this work both to extract causal pairs from text chunks and to render candidate causal-concept pairs into explicit natural-language hypotheses.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4 (as extractor and hypothesis generator)",
            "system_description": "Used for (a) causal knowledge extraction from segmented full-text articles with engineered prompts that request JSON-format concept-pair (cause, effect) and relationship type; (b) verification/standardization of extracted pairs during filtering; (c) as a hypothesis generator converting high-probability candidate concept pairs from the graph into textual causal hypotheses. Operated under API token/rate constraints (60 requests / 150k tokens per minute) and chunking (&lt;=4000 tokens).",
            "research_domain": "psychology (used to extract from psychology literature and generate psychology hypotheses)",
            "problem_type": "open-ended exploration (information extraction and hypothesis text generation)",
            "novelty_metric": "Novelty assessed by human expert ratings (same protocol as LLMCG) and by BERT semantic distance analyses.",
            "novelty_score": "GPT-4 alone (ablation) produced hypotheses with significantly lower human-rated novelty than LLMCG in the ablation comparison (LLMCG vs GPT-4 novelty: t≈6.60, p&lt;0.0001, Cohen's d≈0.73; exact per-group mean z-scores not separately listed for GPT-4 in paper tables).",
            "feasibility_metric": "Usefulness rated by human experts (same instrument as for LLMCG) served as the feasibility/proxy metric.",
            "feasibility_score": "No significant difference in usefulness between GPT-4 and LLMCG (Table 6: usefulness mean t ≈ 1.31, p ≈ 0.1937, Cohen's d ≈ 0.149).",
            "tradeoff_evidence": "No explicit quantitative tradeoff reported for GPT-4 outputs; compared qualitatively through separate novelty and usefulness ratings.",
            "optimization_strategy": "Not applicable beyond prompt engineering and GPT-4 internal capacities; no graph constraints applied in the GPT-4-only baseline.",
            "human_evaluation": true,
            "human_evaluation_results": "60 GPT-4-generated hypotheses were evaluated by the same three experts and scored lower on novelty than the LLMCG-generated set (statistically significant). Usefulness ratings did not differ significantly from LLMCG.",
            "comparative_baseline": "Compared directly to LLMCG (ablation study) and to human-generated and Claude-2 outputs.",
            "comparative_results": "GPT-4-only hypotheses: lower novelty (significant) relative to LLMCG; usefulness similar (non-significant differences).",
            "domain_specific_findings": "Within psychology/well-being, unconstrained LLM outputs tended to be less novel than graph-constrained (LLMCG) outputs.",
            "uuid": "e2271.1",
            "source_info": {
                "paper_title": "Automating psychological hypothesis generation with AI: when large language models meet causal graph",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Claude-2 (baseline)",
            "name_full": "Anthropic Claude-2 (LLM baseline)",
            "brief_description": "A comparative large language model used as a control baseline for hypothesis generation; 50 candidate hypotheses were produced and then filtered by GPT-4 to select top 30 for evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Claude-2 (control LLM)",
            "system_description": "Claude-2 was prompted iteratively (10 hypotheses per prompt) to generate a set of hypotheses focused on well-being; 50 candidate outputs were generated and GPT-4 was used to select the top 30 for downstream human evaluation to ensure comparable complexity/depth.",
            "research_domain": "psychology (well-being)",
            "problem_type": "open-ended exploration (hypothesis generation baseline)",
            "novelty_metric": "Human expert novelty ratings (same protocol) and BERT semantic distance analyses.",
            "novelty_score": "Control-Claude (Claude-2) had significantly lower novelty ratings than Random-selected LLMCG and Control-Human: pairwise comparisons Control-Claude vs Control-Human t(59)=4.36 p&lt;0.001 d≈1.1192; Control-Claude vs Random-selected LLMCG t(59)=3.34 p=0.007 d≈0.8809 (Table 5).",
            "feasibility_metric": "Human expert usefulness ratings.",
            "feasibility_score": "Usefulness ratings varied widely for Control-Claude; group-wise usefulness differences with other groups were not significant in many comparisons. Exact mean usefulness per group not reported numerically.",
            "tradeoff_evidence": "No explicit tradeoff analysis; Control-Claude tended to produce less novel hypotheses while usefulness did not systematically differ.",
            "optimization_strategy": "Not applicable beyond iterative prompting and subsequent GPT-4 filtering of outputs.",
            "human_evaluation": true,
            "human_evaluation_results": "Claude-generated hypotheses rated lower for novelty by human reviewers compared to LLMCG and PhD student groups; usefulness ratings showed high variance and no consistent superiority.",
            "comparative_baseline": "Serves as LLM-only baseline vs LLMCG and human baselines.",
            "comparative_results": "Claude-2 outputs scored significantly lower on novelty compared to LLMCG and human PhD outputs (see Table 5 pairwise tests).",
            "domain_specific_findings": "In well-being domain, unconstrained LLM (Claude-2) outputs were less novel than LLMCG outputs derived from causal graphs.",
            "uuid": "e2271.2",
            "source_info": {
                "paper_title": "Automating psychological hypothesis generation with AI: when large language models meet causal graph",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LinkPrediction (node2vec+Jaccard)",
            "name_full": "Node2vec + similarity + Jaccard link prediction pipeline",
            "brief_description": "A graph-based candidate-hypothesis generation method that embeds graph nodes with node2vec, filters candidate unconnected pairs by embedding similarity, and scores link likelihood using Jaccard similarity of neighbourhoods, then ranks candidates to propose new causal pairs.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "node2vec embedding + similarity threshold + Jaccard neighbourhood scoring",
            "system_description": "Procedure: embed graph nodes with node2vec to produce vector representations capturing topology; compute pairwise similarity for unconnected node pairs and discard low-similarity pairs; for high-similarity pairs compute Jaccard similarity of their neighbor sets to estimate probability of a link (plausible causal relation); rank pairs by estimated probability and select top pairs to feed into GPT-4 for hypothesis text generation. This method operationalizes link prediction to propose novel causal hypotheses from the constructed causal graph.",
            "research_domain": "psychology (applied to a causal concept graph derived from psychology literature)",
            "problem_type": "open-ended exploration (discover latent links / hypotheses in concept graph)",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Heuristic ranking by embedding similarity then Jaccard neighbourhood overlap; thresholding to reduce candidate set; downstream LLM generation and optional human expert selection for quality control.",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Example case: inferring a plausible link between 'behavioral inhibition system (BIS)' and 'interference' via shared connections to BAS and BAS reward response — demonstrates how topological inference can suggest empirically testable hypotheses in psychology.",
            "uuid": "e2271.3",
            "source_info": {
                "paper_title": "Automating psychological hypothesis generation with AI: when large language models meet causal graph",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "BERT+t-SNE semantic analysis",
            "name_full": "BERT embeddings with t-SNE visualization (deep semantic analysis)",
            "brief_description": "A method mapping each candidate hypothesis into BERT embedding space to analyze semantic structure and distances among hypotheses, then reducing dimensionality with t-SNE for visualization and quantitative comparison (semantic distance distributions).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "BERT encoding + t-SNE dimensionality reduction",
            "system_description": "Each hypothesis is encoded into a high-dimensional vector using BERT; these vectors are compared via pairwise distances (semantic distance) and reduced to 2D using t-SNE for visualization of clusters and dispersion. Semantic distance distributions were then statistically analyzed (ANOVA and post-hoc tests) to compare semantic variety across hypothesis origin groups.",
            "research_domain": "general scientific-hypothesis semantic analysis (applied to psychology hypotheses)",
            "problem_type": "analysis / evaluation (measuring novelty via semantic distance)",
            "novelty_metric": "Semantic distance in BERT-embedding space (pairwise distances) and clustering as visualized with t-SNE; used as an objective proxy for novelty and semantic diversity.",
            "novelty_score": "ANOVA on semantic distances showed a very large group effect: F(3,1652)=84.1611, p &lt; 0.00001, R^2 ≈ 86.96%. Pairwise differences reported (Table 7) include Control-Claude vs Control-Human t≈16.41 p&lt;0.0001 Cohen's d=2.6949, Control-Claude vs Random-selected LLMCG Cohen's d≈2.1175 (p&lt;0.0001), and others—indicating substantial semantic-distance differences between groups.",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": "Indirect: semantic distance (novelty proxy) differences track human novelty ratings, but no direct measurement of feasibility vs semantic distance tradeoff is reported.",
            "optimization_strategy": null,
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": "Used to compare LLMCG, LLM-only, and human hypotheses in semantic space.",
            "comparative_results": "Control-Human group exhibited larger semantic distances vs other groups (consistent with human novelty judgments). LLMCG groups showed broader semantic coverage than LLM-only (Control-Claude).",
            "domain_specific_findings": "In well-being hypotheses, semantic-space metrics aligned with human novelty evaluations, supporting BERT-distance as an objective proxy for novelty in this setting.",
            "uuid": "e2271.4",
            "source_info": {
                "paper_title": "Automating psychological hypothesis generation with AI: when large language models meet causal graph",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Human expert evaluation",
            "name_full": "Human expert ratings of novelty and usefulness",
            "brief_description": "A blinded human-evaluation protocol where three senior psychology professors independently rated each hypothesis on novelty and usefulness; ratings were standardized (z-scores) and aggregated (mean, median, max) for statistical comparison.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Blinded expert rating protocol (novelty & usefulness)",
            "system_description": "Three senior psychology professors (mean age 42.33) were given randomized hypotheses and asked to rate each on two axes: novelty and usefulness (utility). Individual reviewer ratings were standardized to z-scores to account for rater bias; aggregate metrics (mean z-score, median, and maximum) were used for ANOVA and post-hoc pairwise comparisons. Inter-rater agreement quantified with Spearman correlations.",
            "research_domain": "psychology (evaluation of psychology hypotheses)",
            "problem_type": "evaluation / quality assessment",
            "novelty_metric": "Expert numerical ratings on a novelty scale (converted to standardized z-scores per reviewer and aggregated). Also measured inter-rater agreement with Spearman correlations.",
            "novelty_score": "ANOVA: F(3,116)=6.92, p=0.0002, R^2≈15.19% for mean novelty z-scores. Pairwise significant tests: Random-selected LLMCG vs Control-Claude t(59)=3.34 p=0.007 d=0.8809; Control-Human vs Control-Claude t(59)=4.36 p&lt;0.001 d≈1.1192. Median- and max-based analyses confirmed significant group differences; max novelty ANOVA: F(3,116)=7.20 p=0.0002 R^2≈15.70%.",
            "feasibility_metric": "Expert numerical ratings on 'usefulness' (utility) scale; aggregated similarly and tested across groups.",
            "feasibility_score": "Group factor for mean usefulness not significant: F(3,116)=5.25, p=0.553. Pairwise comparisons vs GPT-4/LLMCG not significant (Table 6: usefulness mean t ≈ 1.31, p ≈ 0.1937).",
            "tradeoff_evidence": "No explicit correlation or tradeoff statistics between novelty and usefulness reported. Authors note that novelty and usefulness are distinct evaluation axes and present separate analyses for each.",
            "optimization_strategy": "Used z-score standardization to mitigate rater bias and multiple aggregation strategies (mean, median, max) to assess robustness of novelty results.",
            "human_evaluation": true,
            "human_evaluation_results": "Expert raters showed moderate inter-rater agreement on novelty and usefulness (Spearman correlations: e.g., reviewer1 vs reviewer2 novelty r=0.387 p&lt;0.0001; reviewer2 vs reviewer3 usefulness r=0.376 p&lt;0.0001). Experts rated LLMCG and PhD-student hypotheses higher on novelty than LLM-only (Claude) outputs; usefulness scores did not significantly differ across groups.",
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Expert judgments varied by reviewer specializations; disagreements illustrate the subjectivity of novelty/usefulness assessment even among senior domain experts.",
            "uuid": "e2271.5",
            "source_info": {
                "paper_title": "Automating psychological hypothesis generation with AI: when large language models meet causal graph",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Neo4j Graph DB",
            "name_full": "Neo4j graph database (storage and analysis)",
            "brief_description": "Graph database used to store extracted causal concept nodes and directed relationships with attributes, enabling graph traversal, node/edge querying, and running graph algorithms for link prediction.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Neo4j (graph database for causal knowledge)",
            "system_description": "Extracted causal pairs and concept metadata stored in Neo4j; nodes represent concepts, edges represent directed causal relationships with attributes (directionality, interpretation). Neo4j facilitated graph-algorithm operations (node2vec embedding, topological queries, neighborhood computations) required for link prediction and candidate discovery.",
            "research_domain": "knowledge-graph construction for psychology literature",
            "problem_type": "knowledge representation and query for hypothesis discovery",
            "novelty_metric": null,
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Supports the pipeline's link-prediction and filtering steps (node2vec embeddings computed over graph stored in Neo4j).",
            "human_evaluation": null,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": null,
            "domain_specific_findings": "Database contained ~197k concepts and ~235k connections derived from 43,312 psychology papers; authors note ~13% of extracted pairs did not align with human expert estimation, indicating room for extraction-improvement.",
            "uuid": "e2271.6",
            "source_info": {
                "paper_title": "Automating psychological hypothesis generation with AI: when large language models meet causal graph",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Predicting research trends with semantic and neural networks with an application in quantum physics",
            "rating": 2,
            "sanitized_title": "predicting_research_trends_with_semantic_and_neural_networks_with_an_application_in_quantum_physics"
        },
        {
            "paper_title": "Identification of research hypotheses and new knowledge from scientific literature",
            "rating": 2,
            "sanitized_title": "identification_of_research_hypotheses_and_new_knowledge_from_scientific_literature"
        },
        {
            "paper_title": "Unifying large language models and knowledge graphs: a roadmap",
            "rating": 2,
            "sanitized_title": "unifying_large_language_models_and_knowledge_graphs_a_roadmap"
        },
        {
            "paper_title": "Causal reasoning and large language models: opening a new frontier for causality",
            "rating": 2,
            "sanitized_title": "causal_reasoning_and_large_language_models_opening_a_new_frontier_for_causality"
        },
        {
            "paper_title": "Refining the causal loop diagram: a tutorial for maximizing the contribution of domain expertise in computational system dynamics modeling",
            "rating": 1,
            "sanitized_title": "refining_the_causal_loop_diagram_a_tutorial_for_maximizing_the_contribution_of_domain_expertise_in_computational_system_dynamics_modeling"
        }
    ],
    "cost": 0.01810025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automating psychological hypothesis generation with AI: when large language models meet causal graph</p>
<p>Song Tong 0000-0002-4183-8454
Department of Psychological and Cognitive Sciences
Tsinghua University
BeijingChina</p>
<p>Positive Psychology Research Center
School of Social Sciences
Tsinghua University
BeijingChina</p>
<p>AI for Wellbeing Lab
Tsinghua University
BeijingChina</p>
<p>Institute for Global Industry
Tsinghua University
BeijingChina</p>
<p>Kai Mao 
Kindom KK
TokyoJapan</p>
<p>Zhen Huang 
Positive Psychology Research Center
School of Social Sciences
Tsinghua University
BeijingChina</p>
<p>Yukun Zhao zhaoyukun@tsinghua.edu.cn 
Positive Psychology Research Center
School of Social Sciences
Tsinghua University
BeijingChina</p>
<p>Kaiping Peng pengkp@tsinghua.edu.cn 
Department of Psychological and Cognitive Sciences
Tsinghua University
BeijingChina</p>
<p>Positive Psychology Research Center
School of Social Sciences
Tsinghua University
BeijingChina</p>
<p>AI for Wellbeing Lab
Tsinghua University
BeijingChina</p>
<p>Institute for Global Industry
Tsinghua University
BeijingChina</p>
<p>Automating psychological hypothesis generation with AI: when large language models meet causal graph
E2B44C3CFD95C3763FE421DFF2D6FCFB10.1057/s41599-024-03407-5Received: 8 November 2023; Accepted: 25 June 2024;
Leveraging the synergy between causal knowledge graphs and a large language model (LLM), our study introduces a groundbreaking approach for computational hypothesis generation in psychology.We analyzed 43,312 psychology articles using a LLM to extract causal relation pairs.This analysis produced a specialized causal graph for psychology.Applying link prediction algorithms, we generated 130 potential psychological hypotheses focusing on "wellbeing", then compared them against research ideas conceived by doctoral scholars and those produced solely by the LLM.Interestingly, our combined approach of a LLM and causal graphs mirrored the expert-level insights in terms of novelty, clearly surpassing the LLM-only hypotheses (t(59) = 3.34, p = 0.007 and t(59) = 4.32, p &lt; 0.001, respectively).This alignment was further corroborated using deep semantic analysis.Our results show that combining LLM with machine learning techniques such as causal knowledge graphs can revolutionize automated discovery in psychology, extracting novel insights from the extensive literature.This work stands at the crossroads of psychology and artificial intelligence, championing a new enriched paradigm for data-driven hypothesis generation in psychological research.</p>
<p>Introduction</p>
<p>I</p>
<p>n an age in which the confluence of artificial intelligence (AI) with various subjects profoundly shapes sectors ranging from academic research to commercial enterprises, dissecting the interplay of these disciplines becomes paramount (Williams et al., 2023).In particular, psychology, which serves as a nexus between the humanities and natural sciences, consistently endeavors to demystify the complex web of human behaviors and cognition (Hergenhahn and Henley, 2013).Its profound insights have significantly enriched academia, inspiring innovative applications in AI design.For example, AI models have been molded on hierarchical brain structures (Cichy et al., 2016) and human attention systems (Vaswani et al., 2017).Additionally, these AI models reciprocally offer a rejuvenated perspective, deepening our understanding from the foundational cognitive taxonomy to nuanced esthetic perceptions (Battleday et al., 2020;Tong et al., 2021).Nevertheless, the multifaceted domain of psychology, particularly social psychology, has exhibited a measured evolution compared to its tech-centric counterparts.This can be attributed to its enduring reliance on conventional theory-driven methodologies (Henrich et al., 2010;Shah et al., 2015), a characteristic that stands in stark contrast to the burgeoning paradigms of AI and data-centric research (Bechmann and Bowker, 2019;Wang et al., 2023).</p>
<p>In the journey of psychological research, each exploration originates from a spark of innovative thought.These research trajectories may arise from established theoretical frameworks, daily event insights, anomalies within data, or intersections of interdisciplinary discoveries (Jaccard and Jacoby, 2019).Hypothesis generation is pivotal in psychology (Koehler, 1994;McGuire, 1973), as it facilitates the exploration of multifaceted influencers of human attitudes, actions, and beliefs.The HyGene model (Thomas et al., 2008) elucidated the intricacies of hypothesis generation, encompassing the constraints of working memory and the interplay between ambient and semantic memories.Recently, causal graphs have provided psychology with a systematic framework that enables researchers to construct and simulate intricate systems for a holistic view of "bio-psychosocial" interactions (Borsboom et al., 2021;Crielaard et al., 2022).Yet, the labor-intensive nature of the methodology poses challenges, which requires multidisciplinary expertise in algorithmic development, exacerbating the complexities (Crielaard et al., 2022).Meanwhile, advancements in AI, exemplified by models such as the generative pretrained transformer (GPT), present new avenues for creativity and hypothesis generation (Wang et al., 2023).</p>
<p>Building on this, notably large language models (LLMs) such as GPT-3, GPT-4, and Claude-2, which demonstrate profound capabilities to comprehend and infer causality from natural language texts, a promising path has emerged to extract causal knowledge from vast textual data (Binz and Schulz, 2023;Gu et al., 2023).Exciting possibilities are seen in specific scenarios in which LLMs and causal graphs manifest complementary strengths (Pan et al., 2023).Their synergistic combination converges human analytical and systemic thinking, echoing the holistic versus analytic cognition delineated in social psychology (Nisbett et al., 2001).This amalgamation enables fine-grained semantic analysis and conceptual understanding via LLMs, while causal graphs offer a global perspective on causality, alleviating the interpretability challenges of AI (Pan et al., 2023).This integrated methodology efficiently counters the inherent limitations of working and semantic memories in hypothesis generation and, as previous academic endeavors indicate, has proven efficacious across disciplines.For example, a groundbreaking study in physics synthesized 750,000 physics publications, utilizing cutting-edge natural language processing to extract 6368 pivotal quantum physics concepts, culminating in a semantic network forecasting research trajectories (Krenn and Zeilinger, 2020).Additionally, by integrating knowledge-based causal graphs into the foundation of the LLM, the LLM's capability for causative inference significantly improves (Kıcıman et al., 2023).</p>
<p>To this end, our study seeks to build a pioneering analytical framework, combining the semantic and conceptual extraction proficiency of LLMs with the systemic thinking of the causal graph, with the aim of crafting a comprehensive causal network of semantic concepts within psychology.We meticulously analyzed 43,312 psychological articles, devising an automated method to construct a causal graph, and systematically mining causative concepts and their interconnections.Specifically, the initial sifting and preparation of the data ensures a high-quality corpus, and is followed by employing advanced extraction techniques to identify standardized causal concepts.This results in a graph database that serves as a reservoir of causal knowledge.In conclusion, using node embedding and similarity-based link prediction, we unearthed potential causal relationships, and thus generated the corresponding hypotheses.</p>
<p>To gauge the pragmatic value of our network, we selected 130 hypotheses on "well-being" generated by our framework, comparing them with hypotheses crafted by novice experts (doctoral students in psychology) and the LLM models.The results are encouraging: Our algorithm matches the caliber of novice experts, outshining the hypotheses generated solely by the LLM models in novelty.Additionally, through deep semantic analysis, we demonstrated that our algorithm contains more profound conceptual incorporations and a broader semantic spectrum.</p>
<p>Our study advances the field of psychology in two significant ways.Firstly, it extracts invaluable causal knowledge from the literature and converts it to visual graphics.These aids can feed algorithms to help deduce more latent causal relations and guide models in generating a plethora of novel causal hypotheses.Secondly, our study furnishes novel tools and methodologies for causal analysis and scientific knowledge discovery, representing the seamless fusion of modern AI with traditional research methodologies.This integration serves as a bridge between conventional theory-driven methodologies in psychology and the emerging paradigms of data-centric research, thereby enriching our understanding of the factors influencing psychology, especially within the realm of social psychology.</p>
<p>Methodological framework for hypothesis generation</p>
<p>The proposed LLM-based causal graph (LLMCG) framework encompasses three steps: literature retrieval, causal pair extraction, and hypothesis generation, as illustrated in Fig. 1.In the literature gathering phase, ~140k psychology-related articles were downloaded from public databases.In step two, GPT-4 were used to distil causal relationships from these articles, culminating in the creation of a causal relationship network based on 43,312 selected articles.In the third step, an in-depth examination of these data was executed, adopting link prediction algorithms to forecast the dynamics within the causal relationship network for searching the highly potential causality concept pairs.</p>
<p>Step 1: Literature retrieval.The primary data source for this study was a public repository of scientific articles, the PMC Open Access Subset.Our decision to utilize this repository was informed by several key attributes that it possesses.The PMC Open Access Subset boasts an expansive collection of over 2 million full-text XML science and medical articles, providing a substantial and diverse base from which to derive insights for our research.Furthermore, the open-access nature of the articles not only enhances the transparency and reproducibility of our methodology, but also ensures that the results and processes can be independently accessed and verified by other researchers.Notably, the content within this subset originates from recognized journals, all of which have undergone rigorous peer review, lending credence to the quality and reliability of the data we leveraged.Finally, an added advantage was the rich metadata accompanying each article.These metadata were instrumental in refining our article selection process, ensuring coherent thematic alignment with our research objectives in the domains of psychology.</p>
<p>To identify articles relevant to our study, we applied a series of filtering criteria.First, the presence of certain keywords within article titles or abstracts was mandatory.Some examples of these keywords include "psychol", "clin psychol", and "biol psychol".Second, we exploited the metadata accompanying each article.The classification of articles based on these metadata ensured alignment with recognized thematic standards in the domains of psychology and neuroscience.Upon the application of these criteria, we managed to curate a subset of approximately 140K articles that most likely discuss causal concepts in both psychology and neuroscience.</p>
<p>Step 2: Causal pair extraction.The process of extracting causal knowledge from vast troves of scientific literature is intricate and multifaceted.Our methodology distils this complex process into four coherent steps, each serving a distinct purpose.(1) Article selection and cost analysis: Determines the feasibility of processing a specific volume of articles, ensuring optimal resource allocation.( 2) Text extraction and analysis: Ensures the purity of the data that enter our causal extraction phase by filtering out nonrelevant content.(3) Causal knowledge extraction: Uses advanced language models to detect, classify, and standardize causal factors relationships present in texts.(4) Graph database storage: Facilitates structured storage, easy retrieval, and the possibility of advanced relational analyses for future research.This streamlined approach ensures accuracy, consistency, and scalability in our endeavor to understand the interplay of causal concepts in psychology and neuroscience.</p>
<p>Text extraction and cleaning.After a meticulous cost analysis detailed in Appendix A, our selection process identified 43,312 articles.This selection was strategically based on the criterion that the journal titles must incorporate the term "Psychol", signifying their direct relevance to the field of psychology.The distributions of publication sources and years can be found in Table 1.Extracting the full texts of the articles from their PDF sources was an essential initial step, and, for this purpose, the PyPDF2 Python library was used.This library allowed us to seamlessly extract and concatenate titles, abstracts, and main content from each PDF article.However, a challenge arose with the presence of extraneous sections such as references or tables, in the extracted texts.The implemented procedure, employing regular expressions in Python, was not only adept at identifying variations of the term "references" but also ascertained whether this section appeared as an isolated segment.This check was critical to ensure that the identified that the "references" section was indeed distinct, marking the start of a reference list without continuation into other text.Once identified as a standalone entity, the next step in the method was to efficiently remove the reference section and its subsequent content.</p>
<p>Causal knowledge extraction method.In our effort to extract causal knowledge, the choice of GPT-4 was not arbitrary.While several models were available for such tasks, GPT-4 emerged as a frontrunner due to its advanced capabilities (Wu et al., 2023), extensive training on diverse data, with its proven proficiency in understanding context, especially in complex scientific texts   (Cheng et al., 2023;Sanderson, 2023).Other models were indeed considered; however, the capacity of GPT-4 to generate coherent, contextually relevant responses gave our project an edge in its specific requirements.</p>
<p>The extraction process commenced with the segmentation of the articles.Due to the token constraints inherent to GPT-4, it was imperative to break down the articles into manageable chunks, specifically those of 4000 tokens or fewer.This approach ensured a comprehensive interpretation of the content without omitting any potential causal relationships.The next phase was prompt engineering.To effectively guide the extraction capabilities of GPT-4, we crafted explicit prompts.A testament to this meticulous engineering is demonstrated in a directive in which we asked the model to elucidate causal pairs in a predetermined JSON format.For a clearer understanding, readers are referred to Table 2, which elucidates the example prompt and the subsequent model response.After extraction, the outputs were not immediately cataloged.A filtering process was initiated to ascertain the standardization of the concept pairs.This process weeded out suboptimal outputs.Aiding in this quality control, GPT-4 played a pivotal role in the verification of causal pairs, determining their relevance, causality, and ensuring correct directionality.Finally, while extracting knowledge, we were aware of the constraints imposed by the GPT-4 API.There was a conscious effort to ensure that we operated within the bounds of 60 requests and 150k tokens per minute.This interplay of prompt engineering and stringent filtering was productive.</p>
<p>In addition, we conducted an exploratory study to assess GPT-4's discernment between "causality" and "correlation" involved four graduate students (mean age 31 ± 10.23), each evaluating relationship pairs extracted from their familiar psychology articles.The experimental details and results can be found in Appendix A and Table A1.The results showed that out of 289 relationships identified by GPT-4, 87.54% were validated.Notably, when GPT-4 classified relationships as causal, only 13.02% (31/238) were recognized as non-relationship, while 65.55% (156/238) agreed upon as causality.This shows that GPT-4 can accurately extract relationships (causality or correlation) in psychological texts, underscoring the potential as a tool for the construction of causal graphs.</p>
<p>To enhance the robustness of the extracted causal relationships and minimize biases, we adopted a multifaceted approach.</p>
<p>Recognizing the indispensable role of human judgment, we periodically subjected random samples of extracted causal relationships to the scrutiny of domain experts.Their valuable feedback was instrumental in the real-time fine-tuning the extraction process.Instead of heavily relying on referenced hypotheses, our focus was on extracting causal pairs, primarily from the findings mentioned in the main texts.This systematic methodology ultimately resulted in a refined text corpus distilled from 43,312 articles, which contained many conceptual insights and were primed for rigorous causal extraction.</p>
<p>Graph database storage.Our decision to employ Neo4j as the database system was strategic.Neo4j, as a graph database (Thomer and Wickett, 2020), is inherently designed to capture and represent complex relationships between data points, an attribute that is essential for understanding intricate causal relationships.Beyond its technical prowess, Neo4j provides advantages such as scalability, resilience, and efficient querying capabilities (Webber, 2012).It is particularly adept at traversing interconnected data points, making it an excellent fit for our causal relationship analysis.The mined causal knowledge finds its abode in the Neo4j graph database.Each pair of causal concepts is represented as a node, with its directionality and interpretations stored as attributes.Relationships provide related concepts together.Storing the knowledge graph in Neo4j allows for the execution of the graph algorithms to analyze concept interconnectivity and reveal potential relationships.</p>
<p>The graph database contains 197k concepts and 235k connections.Table 3 encapsulates the core concepts and provides a vivid snapshot of the most recurring themes; helping us to understand the central topics that dominate the current psychological discourse.A comprehensive examination of the core concepts extracted from 43,312 psychological papers, several distinct patterns and focal areas emerged.In particular, there is a clear balance between health and illness in psychological research.The prominence of terms such as "depression", "anxiety", and "symptoms of depression magnifies the commitment in the discipline to understanding and addressing mental illnesses.However, juxtaposed against these are positive terms such as "life satisfaction" and "sense of happiness", suggesting that psychology not only fixates on challenges but also delves deeply into the nuances of positivity and well-being.Furthermore, the</p>
<p>Prompt:</p>
<p>From the "text" below, extract the key causal and correlational relationships described directly in the given text by analyzing reasoning and evidence within the text.Exclude any relationships that are attributed to or cited from other research studies.Format the relationships in JSON format with the following fields: 'concept_pair': A list representation of the cause and effect concepts in the relationship, in [cause, effect] order.'relationship': 'causality' or 'correlation' indicating the type of relationship.'positive/negative': If the extracted relationship is causality, indicate whether it's a positive or negative causality relationship.If it's a correlation relationship, reply as None.{ "PMC8451848", 'concept_pair': [openness to change values, well-being], 'relationship':"causality", 'positive/negative':"positive" }, { "PMC6085571", 'concept_pair': [cognitive reappraisal, Psychological well-being], 'relationship':"causality", 'positive/negative':"positive" } significance given to concepts such as "life satisfaction", "sense of happiness", and "job satisfaction" underscores an increasing recognition of emotional well-being and job satisfaction as integral to overall mental health.Intertwining the realms of psychology and neuroscience, terms such as "microglial cell activation", "cognitive impairment", and "neurodegenerative changes" signal a growing interest in understanding the neural underpinnings of cognitive and psychological phenomena.In addition, the emphasis on "self-efficacy", "positive emotions", and "self-esteem" reflect the profound interest in understanding how self-perception and emotions influence human behavior and wellbeing.Concepts such as "age", "resilience", and "creativity" further expand the canvas, showcasing the eclectic and comprehensive nature of inquiries in the field of psychology.</p>
<p>Overall, this analysis paints a vivid picture of modern psychological research, illuminating its multidimensional approach.It demonstrates a discipline that is deeply engaged with both the challenges and triumphs of human existence, offering holistic insight into the human mind and its myriad complexities.</p>
<p>Step 3: Hypothesis generation using link prediction.In the quest to uncover novel causal relationships beyond direct extraction from texts, the technique of link prediction emerges as a pivotal methodology.It hinges on the premise of proposing potential causal ties between concepts that our knowledge graph does not explicitly connect.The process intricately weaves together vector embedding, similarity analysis, and probability-based ranking.Initially, concepts are transposed into a vector space using node2vec, which is valued for its ability to capture topological nuances.Here, every pair of unconnected concepts is assigned a similarity score, and pairs that do not meet a set benchmark are quickly discarded.As we dive deeper into the higher echelons of these scored pairs, the likelihood of their linkage is assessed using the Jaccard similarity of their neighboring concepts.Subsequently, these potential causal relationships are organized in descending order of their derived probabilities, and the elite pairs are selected.</p>
<p>An illustration of this approach is provided in the case highlighted in Figure A1.For instance, the behavioral inhibition system (BIS) exhibits ties to both the behavioral activation system (BAS) and the subsequent behavioral response of the BAS when encountering reward stimuli, termed the BAS reward response.Simultaneously, another concept, interference, finds itself bound to both the BAS and the BAS Reward Response.This configuration hints at a plausible link between the BIS and interference.Such highly probable causal pairs are not mere intellectual curiosity.They act as springboards, catalyzing the genesis of new experimental designs or research hypotheses ripe for empirical probing.In essence, this capability equips researchers with a cutting-edge instrument, empowering them to navigate the unexplored waters of the psychological and neurological domains.</p>
<p>Using pairs of highly probable causal concepts, we pushed GPT-4 to conjure novel causal hypotheses that bridge concepts.To further elucidate the process of this method, Table 4 provides some examples of hypotheses generated from the process.Such hypotheses, as exemplified in the last row, underscore the potential and power of our method for generating innovative causal propositions.</p>
<p>Hypotheses evaluation and results</p>
<p>In this section, we present an analysis focusing on quality in terms of novelty and usefulness of the hypotheses generated.According to existing literature, these dimensions are instrumental in encapsulating the essence of inventive ideas (Boden, 2009;McCarthy et al., 2018;Miron-Spektor and Beenen, 2015).These parameters have not only been quintessential for gauging creative concepts, but they have also been adopted to evaluate the caliber of research hypotheses (Dowling and Lucey, 2023;Krenn and Zeilinger, 2020;Oleinik, 2019).Specifically, we evaluate the quality of the hypotheses generated by the proposed LLMCG algorithm in relation to those generated by PhD students from an elite university who represent human junior experts, the LLM model, which represents advanced AI systems, and the research ideas refined by psychological researchers which represents cooperation between AI and humans.</p>
<p>Microbiome diversity</p>
<p>Well-being Pandemic flourishing: Some individuals may experience a sense of "flourishing" or thriving during pandemic events despite the widespread stress and adversity.</p>
<p>Divergent thinking exercises</p>
<p>Well-being Divergent thinking exercises can expand one's sense of self and purpose, which then synergistically improve well-being through an upward spiral.Engaging in creative thinking exercises can broaden one's perspectives on identity and meaning in life.This expanded sense of self and purpose may then mutually reinforce each other and spiral upward to enhance well-being.</p>
<p>Online social connectivity</p>
<p>Well-being Virtual resilience: Online social connectivity and access to well-being resources can build "virtual resilience" and enhance well-being during stressful events like pandemics.</p>
<p>Sense of shared purpose and belonging</p>
<p>Well-being A sense of shared purpose and belonging within your social groups is necessary for freedom, choice and self-determination to enhance well-being.The evaluation comprises three main stages.In the first stage, the hypotheses are generated by all contributors, including steps taken to ensure fairness and relevance for comparative analysis.In the second stage, the hypotheses from the first stage are independently and blindly reviewed by experts who represent the human academic community.These experts are asked to provide hypothesis ratings using a specially designed questionnaire to ensure statistical validity.The third stage delves deeper by transforming each research idea into the semantic space of a bidirectional encoder representation from transformers (BERT) (Lee et al., 2023), allowing us to intricately analyze the intrinsic reasons behind the rating disparities among the groups.This semantic mapping not only pinpoints the nuanced differences, but also provides potential insights into the cognitive constructs of each hypothesis.</p>
<p>Evaluation procedure</p>
<p>Selection of the focus area for hypothesis generation.Selecting an appropriate focus area for hypothesis generation is crucial to ensure a balanced and insightful comparison of the hypothesis generation capacities between various contributors.In this study, our goal is to gauge the quality of hypotheses derived from four distinct contributors, with measures in place to mitigate potential confounding variables that might skew the results among groups (Rubin, 2005).Our choice of domain is informed by two pivotal criteria: the intricacy and subtlety of the subject matter and familiarity with the domain.It is essential that our chosen domain boasts sufficient complexity to prompt meaningful hypothesis generation and offer a robust assessment of both AI and human contributors" depth of understanding and creativity.Furthermore, while human contributors should be well-acquainted with the domain, their expertise need not match the vast corpus knowledge of the AI.</p>
<p>In terms of overarching human pursuits such as the search for happiness, positive psychology distinguishes itself by avoiding narrowly defined, individual-centric challenges (Seligman and Csikszentmihalyi, 2000).This alignment with our selection criteria is epitomized by well-being, a salient concept within positive psychology, as shown in Table 3. Well-being, with its multidimensional essence that encompass emotional, psychological, and social facets, and its central stature in both research and practical applications of positive psychology (Diener et al., 2010;Fredrickson, 2001;Seligman and Csikszentmihalyi, 2000), becomes the linchpin of our evaluation.The growing importance of well-being in the current global context offers myriad novel avenues for hypothesis generation and theoretical advancement (Forgeard et al., 2011;Madill et al., 2022;Otu et al., 2020).Adding to our rationale, the Positive Psychology Research Center at Tsinghua University is a globally renowned hub for cutting-edge research in this domain.Leveraging this stature, we secured participation from specialized Ph.D. students, reinforcing positive psychology as the most fitting domain for our inquiry.</p>
<p>Hypotheses comparison.In our study, the generated psychological hypotheses were categorized into four distinct groups, consisting of two experimental groups and two control groups.The experimental groups encapsulate hypotheses generated by our algorithm, either through random selection or handpicking by experts from a pool of generated hypotheses.On the other hand, control groups comprise research ideas that were meticulously crafted by doctoral students with substantial academic expertise in the domains and hypotheses generated by representative LLMs.In the following, we elucidate the methodology and underlying rationale for each group:</p>
<p>LLMCG algorithm output (Random-selected LLMCG).Following the requirement of generating hypotheses centred on well-being, the LLMCG algorithm crafted 130 unique hypotheses.These hypotheses were derived by LLMCG's evaluation of the most likely causal relationships related to well-being that had not been previously documented in research literature datasets.From this refined pool, 30 research ideas were chosen at random for this experimental group.These hypotheses represent the algorithm's ability to identify causal relationships and formulate pertinent hypotheses.</p>
<p>LLMCG expert-vetted hypotheses (Expert-selected LLMCG).For this group, two seasoned psychological researchers, one male aged 47 and one female aged 46, in-depth expertise in the realm of Positive Psychology, conscientiously handpicked 30 of the most promising hypotheses from the refined pool, excluding those from the Random-selected LLMCG category.The selection criteria centered on a holistic understanding of both the novelty and practical relevance of each hypothesis.With an illustrious postdoctoral journey and a robust portfolio of publications in positive psychology to their names, they rigorously sifted through the hypotheses, pinpointing those that showcased a perfect confluence of originality and actionable insight.These hypotheses were meticulously appraised for their relevance, structural coherence, and potential academic value, representing the nexus of machine intelligence and seasoned human discernment.</p>
<p>PhD students' output (Control-Human).We enlisted the expertise of 16 doctoral students from the Positive Psychology Research Center at Tsinghua University.Under the guidance of their supervisor, each student was provided with a questionnaire geared toward research on well-being.The participants were given a period of four working days to complete and return the questionnaire, which was distributed during vacation to ensure minimal external disruptions and commitments.The specific instructions provided in the questionnaire is detailed in Table B1, and each participant was asked to complete 3-4 research hypotheses.By the stipulated deadline, we received responses from 13 doctoral students, with a mean age of 31.92 years (SD = 7.75 years), cumulatively presenting 41 hypotheses related to well-being.To maintain uniformity with the other groups, a random selection was made to shortlist 30 hypotheses for further analysis.These hypotheses reflect the integration of core theoretical concepts with the latest insights into the domain, presenting an academic interpretation rooted in their rigorous training and education.Including this group in our study not only provides a natural benchmark for human ingenuity and expertise but also underscores the invaluable contribution of human cognition in research ideation, serving as a pivotal contrast to AI-generated hypotheses.This juxtaposition illuminates the nuanced differences between human intellectual depth and AI's analytical progress, enriching the comparative dimensions of our study.</p>
<p>Claude model output (Control-Claude).This group exemplifies the pinnacle of current LLM technology in generating research hypotheses.Since LLMCG is a nascent technology, its assessment requires a comparative study with well-established counterparts, creating a key paradigm in comparative research.Currently, Claude-2 and GPT-4 represent the apex of AI technology.For example, Claude-2, with an accuracy rate of 54. 4% excels in reasoning and answering questions, substantially outperforming other models such as Falcon, Koala and Vicuna, which have accuracy rates of 17.1-25.5%(Wu et al., 2023).To facilitate a more comprehensive evaluation of the new model by researchers and to increase the diversity and breadth of comparison, we chose Claude-2 as the control model.Using the detailed instructions provided in Table B2, Claude-2 was iteratively prompted to generate research hypotheses, generating ten hypotheses per prompt, culminating in a total of 50 hypotheses.Although the sheer number and range of these hypotheses accentuate the capabilities of Claude-2, to ensure compatibility in terms of complexity and depth between all groups, a subsequent refinement was considered essential.With minimal human intervention, GPT-4 was used to evaluate these 50 hypotheses and select the top 30 that exhibited the most innovative, relevant, and academically valuable insights.This process ensured the infusion of both the LLM"s analytical prowess and a layer of qualitative rigor, thus giving rise to a set of hypotheses that not only align with the overarching theme of well-being but also resonate with current academic discourse.</p>
<p>Hypotheses assessment.The assessment of the hypotheses encompasses two key components: the evaluation conducted by eminent psychology professors emphasizing novelty and utility, and the deep semantic analysis involving BERT and t-distributed stochastic neighbor embedding (t-SNE) visualization to discern semantic structures and disparities among hypotheses.</p>
<p>Human academic community.The review task was entrusted to three eminent psychology professors (all male, mean age = 42.33),who have a decade-long legacy in guiding doctoral and master"s students in positive psychology and editorial stints in renowned journals; their task was to conduct a meticulous evaluation of the 120 hypotheses.Importantly, to ensure unbiased evaluation, the hypotheses were presented to them in a completely randomized order in the questionnaire.</p>
<p>Our emphasis was undeniably anchored to two primary tenets: novelty and utility (Cohen, 2017;Shardlow et al., 2018;Thompson and Skau, 2023;Yu et al., 2016), as shown in Table B3.Utility in hypothesis crafting demands that our propositions extend beyond mere factual accuracy; they must resonate deeply with academic investigations, ensuring substantial practical implications.Given the inherent challenges of research, marked by constraints in time, manpower, and funding, it is essential to design hypotheses that optimize the utilization of these resources.On the novelty front, we strive to introduce innovative perspectives that have the power to challenge and expand upon existing academic theories.This not only propels the discipline forward but also ensures that we do not inadvertently tread on ground already covered by our contemporaries.</p>
<p>Deep semantic analysis.While human evaluations provide invaluable insight into the novelty and utility of hypotheses, to objectively discern and visualize semantic structures and the disparities among them, we turn to the realm of deep learning.Specifically, we employ the power of BERT (Devlin et al., 2018).BERT, as highlighted by Lee et al. (2023), had a remarkable potential to assess the innovation of ideas.By translating each hypothesis into a high-dimensional vector in the BERT domain, we obtain the profound semantic core of each statement.However, such granularity in dimensions presents challenges when aiming for visualization.</p>
<p>To alleviate this and to intuitively understand the clustering and dispersion of these hypotheses in semantic space, we deploy the t-SNE (t-distributed Stochastic Neighbor Embedding) technique (Van der Maaten and Hinton, 2008), which is adept at reducing the dimensionality of the data while preserving the relative pairwise distances between the items.Thus, when we map our BERT-encoded hypotheses onto a 2D t-SNE plane, an immediate visual grasp on how closely or distantly related our hypotheses are in terms of their semantic content.Our intent is twofold: to understand the semantic terrains carved out by the different groups and to infer the potential reasons for some of the hypotheses garnered heightened novelty or utility ratings from experts.The convergence of human evaluations and semantic layouts, as delineated by Algorithm 1 in Appendix B, reveal the interplay between human intuition and the inherent semantic structure of the hypotheses.</p>
<p>Results</p>
<p>Qualitative analysis by topic analysis.To better understand the underlying thought processes and the topical emphasis of both PhD students and the LLMCG model, qualitative analyses were performed using visual tools such as word clouds and connection graphs, as detailed in Appendix B. The word cloud, as a graphical representation, effectively captures the frequency and importance of terms, providing direct visualization of the dominant themes.Connection graphs, on the other hand, elucidate the relationships and interplay between various themes and concepts.Using these visual tools, we aimed to achieve a more intuitive and clear representation of the data, allowing for easy comparison and interpretation.</p>
<p>Observations drawn from both the word clouds and the connection graphs in Figures B1 and B2 provide us with a rich tapestry of insights into the thought processes and priorities of Ph.D. students and the LLMCG model.For instance, the emphasis in the Control-Human word cloud on terms such as "robot" and "AI" indicates a strong interest among Ph.D. students in the nexus between technology and psychology.It is particularly fascinating to see a group of academically trained individuals focusing on the real world implications and intersections of their studies, as shown by their apparent draw toward trending topics.This not only underscores their adaptability but also emphasizes the importance of contextual relevance.Conversely, the LLMCG groups, particularly the Expert-selected LLMCG group, emphasize the community, collective experiences, and the nuances of social interconnectedness.This denotes a deep-rooted understanding and application of higher-order social psychological concepts, reflecting the model"s ability to dive deep into the intricate layers of human social behavior.</p>
<p>Furthermore, the connection graphs support these observations.The Control-Human graph, with its exploration of themes such as "Robot Companionship" and its relation to factors such as "heart rate variability (HRV)", demonstrates a confluence of technology and human well-being.The other groups, especially the Random-selected LLMCG group, yield themes that are more societal and structural, hinting at broader determinants of individual well-being.</p>
<p>Analysis of human evaluations.To quantify the agreement among the raters, we employed Spearman correlation coefficients.The results, as shown in Table B5, reveal a spectrum of agreement levels between the reviewer pairs, showcasing the subjective dimension intrinsic to the evaluation of novelty and usefulness.In particular, the correlation between reviewer 1 and reviewer 2 in novelty (Spearman r = 0.387, p &lt; 0.0001) and between reviewer 2 and reviewer 3 in usefulness (Spearman r = 0.376, p &lt; 0.0001) suggests a meaningful level of consensus, particularly highlighting their capacity to identify valuable insights when evaluating hypotheses.</p>
<p>The variations in correlation values, such as between reviewer 2 and reviewer 3 (r = 0.069, p = 0.453), can be attributed to the diverse research orientations and backgrounds of each reviewer.Reviewer 1 focuses on social ecology, reviewer 3 specializes in neuroscientific methodologies, and reviewer 2 integrates various views using technologies like virtual reality, and computational methods.In our evaluation, we present specific hypotheses cases to illustrate the differing perspectives between reviewers, as detailed in Table B4 and Figure B3.For example, C5 introduces the novel concept of "Virtual Resilience".Reviewers 1 and 3 highlighted its originality and utility, while reviewer 2 rated it lower in both categories.Meanwhile, C6, which focuses on social neuroscience, resonated with reviewer 3, while reviewers 1 and 2 only partially affirmed it.These differences underscore the complexity of evaluating scientific contributions and highlight the importance of considering a range of expert opinions for a comprehensive evaluation.</p>
<p>This assessment is divided into two main sections: Novelty analysis and usefulness analysis.</p>
<p>Novelty analysis.In the dynamic realm of scientific research, measuring and analyzing novelty is gaining paramount importance (Shin et al., 2022).ANOVA was used to analyze the novelty scores represented in Fig. 2a, and we identified a significant influence of the group factor on the mean novelty score between different reviewers.Initially, z-scores were calculated for each reviewer"s ratings to standardize the scoring scale, which were then averaged.The distinct differences between the groups, as visualized in the boxplots, are statistically underpinned by the results in Table 5.The ANOVA results revealed a pronounced effect of the grouping factor (F(3116) = 6.92, p = 0.0002), with variance explained by the grouping factor (R-squared) of 15.19%.</p>
<p>Further pairwise comparisons using the Bonferroni method, as delineated in Table 5 and visually corroborated by Fig. 2a; significant disparities were discerned between Random-selected LLMCG and Control-Claude (t(59) = 3.34, p = 0.007) and between Control-Human and Control-Claude (t(59) = 4.32, p &lt; 0.001).The Cohen's d values of 0.8809 and 1.1192 respectively indicate that the novelty scores for the Random-selected LLMCG and Control-Human groups are significantly higher than those for the Control-Claude group.Additionally, when considering the cumulative distribution plots to the right of Fig. 2a, we observe the distributional characteristics of the novel scores.For example, it can be observed that the Expert-selected LLMCG curve portrays a greater concentration in the middle score range when compared to the Control-Claude, curve but dominates in the high novelty scores (highlighted in dashed rectangle).Moreover, comparisons involving Control-Human with both Random-selected LLMCG and Expert-selected LLMCG did not manifest statistically significant variances, indicating aligned novelty perceptions among these groups.Finally, the comparisons between Expert-selected LLMCG and Control-Claude (t(59) = 2.49, p = 0.085) suggest a trend toward significance, with a Cohen's d value of 0.6226 indicating generally higher novelty scores for Expert-selected LLMCG compared to Control-Claude.</p>
<p>To mitigate potential biases due to individual reviewer inclinations, we expanded our evaluation to include both median and maximum z-scores from the three reviewers for each hypothesis.These multifaceted analyses enhance the robustness of our results by minimizing the influence of extreme values and potential outliers.First, when analyzing the median novelty scores, the ANOVA test demonstrated a notable association with the grouping factor (F(3,116) = 6.54, p = 0.0004), which explained 14.41% of the variance.As illustrated in Table 5, pairwise evaluations revealed significant disparities between Control-Human and Control-Claude (t(59) = 4.01, p = 0.001), with Control-Human performing significantly higher than Control-Claude (Cohen's d = 1.1031).Similarly, there were significant differences between Random-selected LLMCG and Control-Claude (t(59) = 3.40, p = 0.006), where Random-selected LLMCG also significantly outperformed Control-Claude (Cohen's d = 0.8875).Interestingly, the comparison of Expert-selected LLMCG with Control-Claude (t(59) = 1.70, p = 0.550) and other group pairings did not include statistically significant differences.</p>
<p>Subsequently, turning our attention to maximum novelty scores provided crucial insights, especially where outlier scores may carry significant weight.The influence of the grouping factor was evident (F(3,116) = 7.20, p = 0.0002), indicating an explained variance of 15.70%.In particular, clear differences emerged between Control-Human and Control-Claude (t(59) = 4.36, p &lt; 0.001), and between Random-selected LLMCG and Control-Claude (t(59) = 3.47, p = 0.004).A particularly intriguing observation was the significant difference between Expert-selected LLMCG and Control-Claude (t(59) = 3.12, p = 0.014).The Cohen's d values of 1.1637, 1.0457, and 0.6987 respectively indicate that the novelty scores for the Control-Human, Random-selected LLMCG, and Expert-selected LLMCG groups are significantly higher than those for the Control-Claude group.Together, these analyses offer a multifaceted perspective on novelty evaluations.Specifically, the results of the median analysis echo and support those of the mean, reinforcing the reliability of our assessments.The discerned significance between Control-Claude and Expert-selected LLMCG in the median data emphasizes the intricate differences, while also pointing to broader congruence in novelty perceptions.</p>
<p>Usefulness analysis.Evaluating the practical impact of hypotheses is crucial in scientific research assessments.In the mean useful spectrum, the grouping factor did not exert a significant influence (F(3,116) = 5.25, p = 0.553).Figure 2b presents the utility score distributions between groups.The narrow interquartile range of Control-Human suggests a relatively consistent assessment among reviewers.On the other hand, the spread and outliers in the Control-Claude distribution hint at varied utility perceptions.Both LLMCG groups cover a broad score range, demonstrating a mixture of high and low utility scores, while the Expert-selected LLMCG gravitates more toward higher usefulness scores.The smoothed line plots accompanying Fig. 2b further detail the score densities.For instance, Random-selected LLMCG boasts several high utility scores, counterbalanced by a smattering of low scores.Interestingly, the distributions for Control-Human and Expert-selected LLMCG appear to be closely aligned.While mean utility scores provide an overarching view, the nuances within the boxplots and smoothed plots offer deeper insights.This comprehensive understanding can guide future endeavors in content generation and evaluation, spotlighting key areas of focus and potential improvements.</p>
<p>Comparison between the LLMCG and GPT-4.To evaluate the impact of integrating a causal graph with GPT-4, we performed an ablation study comparing the hypotheses generated by GPT-4 alone and those of the proposed LLMCG framework.For this experiment, 60 hypotheses were created using GPT-4, following the detailed instructions in Table B2.Furthermore, 60 hypotheses for the LLMCG group were randomly selected from the remaining pool of 70 hypotheses.Subsequently, both sets of hypotheses were assessed by three independent reviewers for novelty and usefulness, as previously described.</p>
<p>Table 6 shows a comparison between the GPT-4 and LLMCG groups, highlighting a significant difference in novelty scores   (Johnson et al., 2023).</p>
<p>In the distribution of semantic distances (Fig. 4), we observed that the Control-Human group exhibits a distinctively greater semantic distance in comparison to the other groups, emphasizing their unique semantic orientations.The statistical support for this observation is derived from the ANOVA results, with a significant F-statistic (F(3,1652) = 84.1611,p &lt; 0.00001), underscoring the impact of the grouping factor.This factor explains a remarkable 86.96% of the variance, as indicated by the R-squared value.Multiple comparisons, as shown in Table 7, further elucidate the subtleties of these group differences.Control-Human and Control-Claude exhibit a significant contrast in their semantic distances, as highlighted by the t value of 16.41 and the adjusted p value ( &lt; 0.0001).This difference indicates distinct thought patterns or emphasis in the two groups.Notably, Control-Human demonstrates a greater semantic distance (Cohen's d = 1.1630).Similarly, a comparison of the Control-Claude and LLMCG models reveals pronounced differences (Cohen's d &gt; 0.9), more so with the Expert-selected LLMCG (p &lt; 0.0001).A comparison of Control-Human with the LLMCG models shows divergent semantic orientations, with statistically significant larger distances than Random-selected LLMCG (p = 0.0036) and a trend toward difference with Expert-selected LLMCG (p = 0.0687).Intriguingly, the two LLMCG groups-Random-selected and Expert-selected-exhibit similar semantic distances, as evidenced by a nonsignificant p value of 0.4362.Furthermore, the significant distinctions we observed, particularly between the Control-Human and other groups, align with human evaluations of novelty.This coherence indicates that the BERT space representation coupled with statistical analyses could effectively mimic human judgment.Such results underscore the potential of this approach for automated hypothesis testing, paving the way for more efficient and streamlined semantic evaluations in the future.</p>
<p>In general, visual and statistical analyses reveal the nuanced semantic landscapes of each group.While the Ph.D. students' shared background influences their clustering, the machine models exhibit a comprehensive grasp of topics, emphasizing the intricate interplay of individual experiences, academic influences, and algorithmic understanding in shaping semantic representations.</p>
<p>This investigation carried out a detailed evaluation of the various hypothesis contributors, blending both quantitative and qualitative analyses.In terms of topic analysis, distinct variations were observed between Control-Human and LLMCG, the latter presenting more expansive thematic coverage.For human evaluation, hypotheses from Ph.D. students paralleled the LLMCG in novelty, reinforcing AI's growing competence in mirroring human innovative thinking.Furthermore, when juxtaposed with AI models such as Control-Claude, the LLMCG exhibited increased novelty.Deep semantic analysis via t-SNE and BERT representations allowed us to intuitively grasp semantic essence of hypotheses, signaling the possibility of future automated hypothesis assessments.Interestingly, LLMCG appeared to encompass broader complementary domains compared to human input.Taken together, these findings highlight the emerging role of AI in hypothesis generation and provide key insights into hypothesis evaluation across diverse origins.</p>
<p>General discussion</p>
<p>This research delves into the synergistic relationship between LLM and causal graphs in the hypothesis generation process.Our findings underscore the ability of LLM, when integrated with causal graph techniques, to produce meaningful hypotheses with increased efficiency and quality.By centering our investigation on "well-being" we emphasize its pivotal role in psychological studies and highlight the potential convergence of technology and society.A multifaceted assessment approach to evaluate quality by topic analysis, human evaluation and deep semantic analysis demonstrates that AI-augmented methods not only outshine LLM-only techniques in generating hypotheses with superior novelty and show quality on par with human expertise but also boast the capability for more profound conceptual incorporations and a broader semantic spectrum.Such a multifaceted lens of assessment introduces a novel perspective for the scholarly realm, equipping researchers with an enriched understanding and an innovative toolset for hypothesis generation.At its core, the melding of LLM and causal graphs signals a promising frontier, especially in regard to dissecting cornerstone psychological constructs such as "well-being".This marriage of methodologies, enriched by the comprehensive assessment angle, deepens our comprehension of both the immediate and broader ramifications of our research endeavors.</p>
<p>The prominence of causal graphs in psychology is profound, they offer researchers a unified platform for synthesizing and hypothesizing diverse psychological realms (Borsboom et al., 2021;Uleman et al., 2021).Our study echoes this, producing groundbreaking hypotheses comparable in depth to early expert propositions.Deep semantic analysis bolstered these findings, emphasizing that our hypotheses have distinct cross-disciplinary merits, particularly when compared to those of individual doctoral scholars.However, the traditional use of causal graphs in psychology presents challenges due to its demanding nature, often requiring insights from multiple experts (Crielaard et al., 2022).Our research, however, harnesses LLM's causal extraction, automating causal pair derivation and, in turn, minimizing the need for extensive expert input.The union of the causal graphs' systematic approach with AI-driven creativity, as seen with LLMs, paves the way for the future of psychological inquiry.Thanks to advancements in AI, barriers once created by causal graphs' intricate procedures are being dismantled.Furthermore, as the era of big data dawns, the integration of AI and causal graphs in psychology augments research capabilities, but also brings into focus the broader implications for society.This fusion provides a nuanced understanding of the intricate sociopsychological dynamics, emphasizing the importance of adapting research methodologies in tandem with technological progress.</p>
<p>In the realm of research, LLMs serve a unique purpose, often by acting as the foundation or baseline against which newer methods and approaches are assessed.The demonstrated productivity enhancements by generative AI tools, as evidenced by Noy and Zhang (2023), indicate the potential of such LLMs.In our investigation, we pit the hypotheses generated by such substantial models against our integrated LLMCG approach.Intriguingly, while these LLMs showcased admirable practicality in their hypotheses, they substantially lagged behind in terms of innovation when juxtaposed with the doctoral student and LLMCG group.This divergence in results can be attributed to the causal network curated from 43k research papers, funneling the vast knowledge reservoir of the LLM squarely into the realm of scientific psychology.The increased precision in hypothesis generation by these models fits well within the framework of generative networks.Tong et al. (2021) highlighted that, by integrating structured constraints, conventional neural networks can accurately generate semantically relevant content.One of the salient merits of the causal graph, in this context, is its ability to alleviate inherent ambiguity or interpretability challenges posed by LLMs.By providing a systematic and structured framework, the causal graph aids in unearthing the underlying logic and rationale of the outputs generated by LLMs.Notably, this finding echoes the perspective of Pan et al. (2023), where the integration of structured knowledge from knowledge graphs was shown to provide an invaluable layer of clarity and interpretability to LLMs, especially in complex reasoning tasks.Such structured approaches not only boost the confidence of researchers in the hypotheses derived but also augment the transparency and understandability of LLM outputs.In essence, leveraging causal graphs may very well herald a new era in model interpretability, serving as a conduit to unlock the black box that large models often represent in contemporary research.</p>
<p>In the ever-evolving tapestry of research, every advancement invariably comes with its unique set of constraints, and our study was no exception.On the technical front, a pivotal challenge stemmed from the opaque inner workings of the GPT.Determining the exact machinations within the GPT that lead to the formation of specific causal pairs remains elusive, thereby reintroducing the ageold issue of AI's inherent lack of transparency (Buruk, 2023;Cao and Yousefzadeh, 2023).This opacity is magnified in our sparse causal graph, which, while expansive, is occasionally riddled with concepts that, while semantically distinct, converge in meaning.In tangible applications, a careful and meticulous algorithmic evaluation would be imperative to construct an accurate psychological conceptual landscape.Delving into psychology, which bridges humanities and natural sciences, it continuously aims to unravel human cognition and behavior (Hergenhahn and Henley, 2013).Despite the dominance of traditional methodologies (Henrich et al., 2010;Shah et al., 2015), the present data-centric era amplifies the synergy of technology and humanities, resonating with Hasok Chang's vision of enriched science (Chang, 2007).This symbiosis is evident when assessing structural holes in social networks (Burt, 2004) and viewing novelty as a bridge across these divides (Foster et al., 2021).Such perspectives emphasize the importance of thorough algorithmic assessments, highlighting potential avenues in humanities research, especially when incorporating large language models for innovative hypothesis crafting and verification.</p>
<p>However, there are some limitations to this research.Firstly, we acknowledge that constructing causal relationship graphs has potential inaccuracies, with ~13% relationship pairs not aligning with human expert estimations.Enhancing the estimation of relationship extraction could be a pathway to improve the accuracy of the causal graph, potentially leading to more robust hypotheses.Secondly, our validation process was limited to 130 hypotheses, however, the vastness of our conceptual landscape suggests countless possibilities.As an exemplar, the twenty pivotal psychological concepts highlighted in Table 3 alone could  spawn an extensive array of hypotheses.However, the validation of these surrounding hypotheses would unquestionably lead to a multitude of speculations.A striking observation during our validation was the inconsistency in the evaluations of the senior expert panels (as shown in Table B5).This shift underscores a pivotal insight: our integration of AI has transitioned the dependency on scarce expert resources from hypothesis generation to evaluation.In the future, rigorous evaluations ensuring both novelty and utility could become a focal point of exploration.The promising path forward necessitates a thoughtful integration of technological innovation and human expertise to fully realize the potential suggested by our study.</p>
<p>In conclusion, our research provides pioneering insight into the symbiotic fusion of LLMs, which are epitomized by GPT, and causal graphs from the realm of psychological hypothesis generation, especially emphasizing "well-being".Importantly, as highlighted by (Cao and Yousefzadeh, 2023), ensuring a synergistic alignment between domain knowledge and AI extrapolation is crucial.This synergy serves as the foundation for maintaining AI models within their conceptual limits, thus bolstering the validity and reliability of the hypotheses generated.Our approach intricately interweaves the advanced capabilities of LLMs with the methodological prowess of causal graphs, thereby optimizing while also refining the depth and precision of hypothesis generation.The causal graph, of paramount importance in psychology due to its cross-disciplinary potential, often demands vast amounts of expert involvement.Our innovative approach addresses this by utilizing LLM's exceptional causal extraction abilities, effectively facilitating the transition of intense expert engagement from hypothesis creation to evaluation.Therefore, our methodology combined LLM with causal graphs, propelling psychological research forward by improving hypothesis generation and offering tools to blend theoretical and data-centric approaches.This synergy particularly enriches our understanding of social psychology's complex dynamics, such as happiness research, demonstrating the profound impact of integrating AI with traditional research frameworks.</p>
<p>Fig. 1
1
Fig.1The hypothesis generation framework Using LLMCG Algorithm.Note: LLM stands for large language model; LLMCG algorithm stands for LLMbased causal graph algorithm, which includes the processes of literature retrieval, causal pair extraction, and hypothesis generation.</p>
<p>Fig. 2
2
Fig. 2 Comparative analysis between groups.Box plots on the left (a) and (b) depict distributions of novelty and usefulness scores, respectively, while smoothed line plots on the right demonstrate the descending order of novelty and usefulness scores and subjected to a moving average with a window size of 2. * denotes p &lt; 0.05, ** denotes p &lt;0.01.</p>
<p>Fig. 3
3
Fig. 3 The t-SNE visualization of semantic representations.Comparison of (a) novelty and (b) usefulness scores (bubble size scaled by 100) among the different groups.</p>
<p>Fig. 4
4
Fig. 4 Distribution of semantic distances among different groups in BERT space.Note: ** denotes p &lt; 0.01, **** denotes p &lt; 0.0001.</p>
<p>Table 1
1
Data sources and publication year distribution for LLMCG computation.
Quantity</p>
<p>Table 2
2
An example prompt and response.</p>
<p>Table 4
4
Example hypotheses for causal relationships.
Concept 1Concept 2 Hypothesis</p>
<p>Table 3
3
Core concepts in 43,312 psychological papers.
NumberConceptsDegree (in)1Depression20022Anxiety16063Life satisfaction8904Well-being8745Performance8346Depressive symptoms8127Mental health7648Microglial activation7349Accuracy72010Psychological distress63111Job satisfaction62312Cognitive impairment60313Neurodegeneration59714Stress55715Self-efficacy54916Neuroinflammation54117Oxidative stress53618Age53319Neuroprotection50520Resilience492</p>
<p>Table 5
5
Bonferroni post-hoc tests for pairwise comparisons of novelty scores across different groups.
ComparisonContrastCohen's dt valuep valueMean ValueControl-Claude vs. Control-Human0.71481.11924.36&lt;0.001Control-Claude vs. Random-selected LLMCG0.54780.88093.340.007Control-Claude vs. Expert-selected LLMCG0.40880.62262.490.085Median ValueControl-Claude vs. Control-Human0.76881.10314.01&lt;0.001Control-Claude vs. Random-selected LLMCG0.65150.88753.400.006Control-Claude vs. Expert-selected LLMCG0.32640.40571.700.550Max ValueControl-Claude vs. Control-Human0.78151.16374.36&lt;0.001Control-Claude vs. Random-selected LLMCG0.62311.04633.470.004Control-Claude vs. Expert-selected LLMCG0.55960.69873.120.014</p>
<p>Table 6
6
Comparison of novelty and usefulness scores between the GPT-4 and LLMCG groups.
VariableContrastCohen's dtvaluep ValueNoveltyMean Value0.73111.20556.60&lt;0.0001Median Value0.79541.14346.26&lt;0.0001Max Value0.75011.13956.24&lt;0.0001UsefulnessMean Value0.14900.23871.310.1937Median Value0.16190.20451.120.2649Max Value0.04430.07930.430.6648</p>
<p>Table 7
7
Bonferroni post-hoc tests for pairwise comparisons of deep semantic distance across different groups.
ComparisonContrastCohen's dt valuep valueControl-Claude vs. Control-Human2.69491.163016.41&lt;0.0001Control-Claude vs. Random-selected LLMCG2.11751.070113.45&lt;0.0001Control-Claude vs. Expert-selected LLMCG2.28700.915012.72&lt;0.0001Random-selected LLMCG vs. Control-Human0.57730.13962.910.0036Expert-selected LLMCG vs. Control-Human0.40780.10941.820.0687Random-selected LLMCG vs. Expert-selected LLMCG0.16950.01520.780.4362
HUMANITIES AND SOCIAL SCIENCES COMMUNICATIONS | (2024) 11:896 | https://doi.org/10.1057/s41599-024-03407-5
AcknowledgementsThe authors thank Dr. Honghong Bai (Radboud University), Dr. ChienTe Wu (The University of Tokyo), Dr. Peng Cheng (Tsinghua University), and Yusong Guo (Tsinghua University) for their great comments on the earlier version of this manuscript.This research has been generously funded by personal contributions, with special acknowledgment to K. Mao.Additionally, he conceived and developed the causality graph and AI hypothesis generation technology presented in this paper from scratch, and generated all AI hypotheses and paid for its costs.The authors sincerely thank K. Mao for his support, which enabled this research.In addition, K. Peng and S. Tong were partly supported by the Tsinghua University lnitiative Scientific Research Program (No. 20213080008), Self-Funded Project of Institute for Global Industry, Tsinghua University (202-296-001), Shuimu Scholars program of Tsinghua University (No. 2021SM157), and the China Postdoctoral International Exchange Program (No. YJ20210266).Data availabilityThe data generated and analyzed in this study are partially available within the Supplementary materials.For additional data supporting the findings of this research, interested parties may contact the corresponding author, who will provide the information upon receiving a reasonable request.Author contributionsSong Tong: Data analysis, Experiments,Writing-original draft &amp; review. KaiCompeting interestsThe author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.Ethical approvalIn this study, ethical approval was granted by the Institutional Review Board (IRB) of the Department of Psychology at Tsinghua University, China.The Research Ethics Committee documented this approval under the number IRB202306, following an extensive review that concluded on March 12, 2023.This approval indicates the research's strict compliance with the IRB's guidelines and regulations, ensuring ethical integrity and adherence throughout the study.Informed consentBefore participating, all study participants gave their informed consent.They received comprehensive details about the study's goals, methods, potential risks and benefits, confidentiality safeguards, and their rights as participants.This process guaranteed that participants were fully informed about the study's nature and voluntarily agreed to participate, free from coercion or undue influence.Additional informationSupplementary informationThe online version contains supplementary material available at https://doi.org/10.1057/s41599-024-03407-5.Correspondence and requests for materials should be addressed to Yukun Zhao or Kaiping Peng.Reprints and permission information is available at http://www.nature.com/reprintsPublisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
Capturing human categorization of natural images by combining deep networks and cognitive models. R M Battleday, J C Peterson, T L Griffiths, Nat Commun. 11154182020</p>
<p>Unsupervised by any other name: hidden layers of knowledge production in artificial intelligence on social media. A Bechmann, G C Bowker, Big Data Soc. 6120539517188195692019</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, Proc Natl Acad Sci. 1206e22185231202023</p>
<p>Computer models of creativity. M A Boden, AI Mag. 3032009</p>
<p>Network analysis of multivariate data in psychological science. D Borsboom, M K Deserno, M Rhemtulla, S Epskamp, E I Fried, R J Mcnally, Nat Rev Methods Prim. 11582021</p>
<p>Structural holes and good ideas. R S Burt, Am J Sociol. 11022004</p>
<p>O Buruk, arXiv:2304.11079Academic writing with GPT-3.5: reflections on practices, efficacy and transparency. 2023arXiv preprint</p>
<p>Extrapolation and AI transparency: why machine learning models should reveal when they make decisions beyond their training. X Cao, R Yousefzadeh, Big Data Soc. 101205395172311697312023</p>
<p>Scientific progress: beyond foundationalism and coherentism1. H Chang, R Inst Philos Suppl. 612007</p>
<p>Exploring the potential of GPT-4 in biomedical engineering: the dawn of a new era. K Cheng, Q Guo, Y He, Y Lu, S Gu, H Wu, Ann Biomed Eng. 512023</p>
<p>Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence. R M Cichy, A Khosla, D Pantazis, A Torralba, A Oliva, Sci Rep. 61277552016</p>
<p>Refining the causal loop diagram: a tutorial for maximizing the contribution of domain expertise in computational system dynamics modeling. B A Cohen, L Crielaard, J F Uleman, B D Châtel, S Epskamp, P Sloot, R Quax, Psychol Methods. 2912017. 2022How should novelty be valued in science? Elife 6:e28699</p>
<p>Bert: pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies20191</p>
<p>New well-being measures: short scales to assess flourishing and positive and negative feelings. E Diener, D Wirtz, W Tov, C Kim-Prieto, D-W Choi, S Oishi, R Biswas-Diener, Soc Indic Res. 972010</p>
<p>ChatGPT for (finance) research: the Bananarama conjecture. M Dowling, B Lucey, Financ Res Lett. 531036622023</p>
<p>Doing the right thing: measuring wellbeing for public policy. M J Forgeard, E Jayawickreme, M L Kern, M E Seligman, Int J Wellbeing. 112011</p>
<p>Surprise! Measuring novelty as expectation violation. J G Foster, F Shi, J Evans, 2021SocArXiv</p>
<p>The role of positive emotions in positive psychology: The broaden-and-build theory of positive emotions. B L Fredrickson, Am Psychol. 5632182001</p>
<p>ConceptGraphs: open-vocabulary 3D scene graphs for perception and planning. Q Gu, A Kuwajerwala, S Morin, K M Jatavallabhula, B Sen, A Agarwal, 2nd Workshop on Language and Robot Learning: Language as Grounding Henrich J, Heine SJ, Norenzayan A. 2024. 2010466Most people are not WEIRD</p>
<p>Theory construction and model-building skills: a practical guide for social scientists. R Hergenhahn, T Henley, Cengage Learning Jaccard J, Jacoby J. 2013. 2019Guilford publicationsAn introduction to the history of psychology</p>
<p>Divergent semantic integration (DSI): Extracting creativity from narratives with distributional semantic modeling. D R Johnson, J C Kaufman, B S Baker, J D Patterson, B Barbot, A E Green, Behav Res Methods. 5572023</p>
<p>E Kıcıman, R Ness, A Sharma, C Tan, arXiv:2305.00050Causal reasoning and large language models: opening a new frontier for causality. 2023arXiv preprint</p>
<p>Hypothesis generation and confidence in judgment. D J Koehler, J Exp Psychol Learn Mem Cogn. 2021994</p>
<p>Predicting research trends with semantic and neural networks with an application in quantum physics. M Krenn, A Zeilinger, Proc Natl Acad Sci. 11742020</p>
<p>Natural language processing algorithms for divergent thinking assessment. H Lee, W Zhou, H Bai, W Meng, T Zeng, K Peng, T Kumada, Proc IEEE 6th Eurasian Conference on Educational Innovation (ECEI). IEEE 6th Eurasian Conference on Educational Innovation (ECEI)2023</p>
<p>Mainstreaming global mental health: Is there potential to embed psychosocial well-being impact in all global challenges research?. A Madill, N Shloim, B Brown, Hugh - Jones, S Plastow, J Setiyawati, D , Appl Psychol Health Well-Being. 1442022</p>
<p>Novelty and usefulness trade-off: cultural cognitive differences and creative idea evaluation. M Mccarthy, C C Chen, R C Mcnamee, J Cross-Cult Psychol. 4922018</p>
<p>The yin and yang of progress in social psychology: seven koan. W J Mcguire, J Personal Soc Psychol. 2631973</p>
<p>Motivating creativity: The effects of sequential and simultaneous learning and performance achievement goals on product novelty and usefulness. E Miron-Spektor, G Beenen, Organ Behav Hum Decis Process. 1272015</p>
<p>Culture and systems of thought: holistic versus analytic cognition. R E Nisbett, K Peng, I Choi, A Norenzayan, Psychol Rev. 10822001</p>
<p>Experimental evidence on the productivity effects of generative artificial intelligence. S Noy, W Zhang, Science. 3812023</p>
<p>What are neural networks not good at? On artificial creativity. A Oleinik, Big Data Soc. 6120539517198394332019</p>
<p>Mental health and psychosocial well-being during the COVID-19 pandemic: the invisible elephant in the room. A Otu, C H Charles, S Yaya, Int J Ment Health Syst. 142020</p>
<p>Unifying large language models and knowledge graphs: a roadmap. S Pan, L Luo, Y Wang, C Chen, J Wang, X Wu, IEEE Transactions on Knowledge and Data Engineering. 3672024</p>
<p>Causal inference using potential outcomes: design, modeling, decisions. D B Rubin, J Am Stat Assoc. 1004692005</p>
<p>GPT-4 is here: what scientists think. K Sanderson, Nature. 61579547732023</p>
<p>Positive psychology: an introduction. M E Seligman, M Csikszentmihalyi, Am Psychol. 5512000</p>
<p>Big data, digital media, and computational social science: possibilities and perils. D V Shah, J N Cappella, W R Neuman, Ann Am Acad Political Soc Sci. 65912015</p>
<p>Identification of research hypotheses and new knowledge from scientific literature. M Shardlow, R Batista-Navarro, P Thompson, R Nawaz, J Mcnaught, S Ananiadou, BMC Med Inform Decis Mak. 1812018</p>
<p>Scientific collaboration, research funding, and novelty in scientific knowledge. H Shin, K Kim, D F Kogler, PLoS ONE. 177e02716782022</p>
<p>Diagnostic hypothesis generation and human judgment. R P Thomas, M R Dougherty, A M Sprenger, J Harbison, Psychol Rev. 11512008</p>
<p>Relational data paradigms: what do we learn by taking the materiality of databases seriously?. A K Thomer, K M Wickett, Big Data Soc. 7120539517209348382020</p>
<p>On the scope of scientific hypotheses. W H Thompson, S Skau, R Soc Open Sci. 1082306072023</p>
<p>Putative ratios of facial attractiveness in a deep neural network. S Tong, X Liang, T Kumada, S Iwaki, Vis Res. 1782021</p>
<p>Mapping the multicausality of Alzheimer's disease through group model building. J F Uleman, R J Melis, R Quax, E A Van Der Zee, D Thijssen, M Dresler, GeroScience. 432021</p>
<p>Visualizing data using t-SNE. L Van Der Maaten, G Hinton, J Mach Learn Res. 9112008</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A Gomez, I Polosukhin, Advances in Neural Information Processing Systems. 2017</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, Nature. 62079722023</p>
<p>A programmatic introduction to neo4j. J Webber, Proceedings of the 3rd annual conference on systems, programming, and applications: software for humanity p. the 3rd annual conference on systems, programming, and applications: software for humanity p2012</p>
<p>Investigating hybridity in artificial intelligence research. K Williams, G Berman, S Michalska, Big Data Soc. 102205395172311805772023</p>
<p>S Wu, M Koo, L Blum, A Black, L Kao, F Scalzo, I Kurtz, arXiv:2308.04709A comparative study of open-source large language models, GPT-4 and Claude 2: multiplechoice test taking in nephrology. 2023arXiv preprint</p>
<p>The Semantic Network Model of creativity: analysis of online social media data. F Yu, T Peng, K Peng, S X Zheng, Z Liu, Creat Res J. 2832016</p>            </div>
        </div>

    </div>
</body>
</html>
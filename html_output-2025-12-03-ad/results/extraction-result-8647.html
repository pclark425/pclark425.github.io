<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8647 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8647</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8647</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-268350146</p>
                <p><strong>Paper Title:</strong> Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy</p>
                <p><strong>Paper Abstract:</strong> Summary This study explores the use of large language models (LLMs) in interpreting and predicting experimental outcomes based on given experimental variables, leveraging the human-like reasoning and inference capabilities of LLMs, using selective catalytic reduction of NOx with NH3 as a case study. We implement the chain of thought (CoT) concept to formulate logical steps for uncovering connections within the data, introducing an “Ordered-and-Structured” CoT (OSCoT) prompting strategy. We compare the OSCoT strategy with the more conventional “One-Pot” CoT (OPCoT) approach and with human experts. We demonstrate that GPT-4, equipped with this new OSCoT prompting strategy, outperforms the other two settings and accurately predicts experimental outcomes and provides intuitive reasoning for its predictions.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8647.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8647.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer-based language model from OpenAI; in this study GPT-4 was used with an ordered-and-structured chain-of-thought (OSCoT) prompting strategy to analyze tabular experimental data and predict catalytic performance of Ce-based metal-oxide composites.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (GPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>catalysis/materials (prediction of NH3-SCR catalytic performance for Ce-based metal-oxide composites); referenced for MOF discovery in related work</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompt-based reasoning (Chain-of-Thought), specifically Ordered-and-Structured CoT (OSCoT) to process tabular data sequentially and produce reasoning chains used to predict experimental outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable for the primary experiments in this paper (GPT-4 was used to analyze/predict performance of existing catalyst compositions rather than to directly generate novel chemical structures); the paper cites external work where GPT-4 was used cooperatively to discover new MOFs but does not report novelty metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Tailoring via explicit persona and task-directive prompts (NH3-SCR catalysis expert role), batching of features by importance to focus reasoning on application-relevant variables; predictions target NOx conversion above/below 95%.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Prediction accuracy (classification of NOx conversion >95% as Positive): average accuracies per binary composite ranged ~60%–74%, with maxima up to ~85%; comparisons to human expert accuracies; statistical summaries (average, max, min) across five independent runs per composite.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using OSCoT prompting, GPT-4 achieved average prediction accuracies between ~60% and 74% across six binary CeM1 composites and peak accuracies up to 85%, outperforming GPT-3.5 variants and the four surveyed human experts; GPT-4 could also extrapolate, with variable success, from binary to some ternary systems (high accuracy for e.g., CeMnFe, CeTiFe; poor extrapolation for other ternaries).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly to GPT-3.5 (worse performance and some failure modes) and to human experts (GPT-4 with OSCoT outperformed all four experts tested); the paper situates GPT-4 as superior to humans for this specific multi-variable tabular prediction task but does not compare to traditional computational chemistry generative models because GPT-4 was used for prediction rather than molecule generation here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Context-length and token limits constrain inputs; GPT-4's extrapolation to ternary systems is inconsistent and depends on training batch selection; the study did not generate new molecules with GPT-4 but used it for tabular reasoning; the paper also notes broader limitations such as not testing less-advanced LLMs beyond GPT-series and inherent difficulties of interpreting tabular data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8647.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8647.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo-16k</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (turbo, 16k context)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3.5 family model variant with extended context window mentioned and used in experiments; evaluated alongside GPT-4 for CoT prompting strategies on the same catalysis prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (turbo-16k)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (GPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>catalysis/materials prediction (same NH3-SCR dataset as GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompt-based reasoning (Chain-of-Thought); both One-Pot CoT (OPCoT) and Ordered-and-Structured CoT (OSCoT) approaches were attempted</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable — used for prediction of experimental outcomes rather than generating chemical structures.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Same persona and OSCoT/OPCoT prompting schemes as GPT-4; temperature set to zero for deterministic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Prediction accuracy (classification of NOx conversion >95%); token-handling behavior (malfunction above certain token counts) was also evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-3.5 showed inferior predictive performance compared to GPT-4; it malfunctioned when using the UM2 (OPCoT) prompting in some cases and began malfunctioning when given more than ~3,000 input tokens in the OP approach; OSCoT-GPT3.5 performed worse than OSCoT-GPT4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Direct comparison in the paper is to GPT-4 (GPT-4 outperformed GPT-3.5). No comparisons to non-LLM chemical generative methods were performed for GPT-3.5 within this study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Severe context/token sensitivity (malfunction >~3,000 tokens in OP approach and failure for UM2), lower accuracy than GPT-4 on tabular catalytic prediction; not used to design novel chemicals in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8647.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8647.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow (GPT-4-based chemistry tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-4-based chemistry tool mentioned in the paper that streamlines reasoning across chemical tasks (from drug design to synthesis) by sequentially prompting the model to align intermediate actions with end goals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemCrow (GPT-4-based tool)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>GPT-based prompting/agent</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>drug design, organic synthesis, general chemistry tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>sequential prompting and tool-augmented agent workflow to guide multi-step chemical tasks (prompt-guided planning and action sequencing)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not specified in this paper; described qualitatively as aiding design and synthesis workflows but no novelty metrics provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Guides the model through task-specific sequences to accomplish domain-relevant goals (e.g., design or synthetic planning) via stepwise prompts and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in this paper; described qualitatively as improving access to chemical knowledge and streamlining reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an example of LLM-empowered tools that assist from drug design to synthesis by organizing sequential prompts; no quantitative results shown in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Not directly compared in this paper; presented as an exemplar of GPT-4-based chemistry tooling that can assist both experts and novices.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>This paper does not detail ChemCrow's limitations; general LLM concerns (hallucination, context limits, synthesizability verification) would apply but are not described specifically here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8647.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8647.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Reticular Chemist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A GPT-4 Reticular Chemist for Guiding MOF Discovery</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work (Z. Zheng et al.) where GPT-4 was applied cooperatively with human experts to aid discovery of new metal–organic frameworks (MOFs); cited as an example of LLMs contributing to material discovery pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (applied as 'Reticular Chemist')</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (GPT) applied in a domain-specific cooperative workflow</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>materials discovery (MOFs)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>cooperative workflow with human experts (prompt-driven ideation, strategy suggestion, and synthesis condition guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not specified in this paper beyond qualitative claim of discovery of a series of MOFs each synthesized with unique strategies; no quantitative novelty metrics reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Human+LLM cooperative workflows where GPT-4 aids in proposing candidate materials and synthesis strategies tailored to MOF discovery goals.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in this paper; original reference likely contains experimental synthesis and validation details.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an example in which GPT-4 contributed to discovering multiple MOFs through cooperation with human experts; this paper does not report the detailed outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Mentioned as evidence that GPT-4 can be integrated into discovery workflows; no quantitative comparison is provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>This paper does not detail limitations of that work; in general, issues like verification of model proposals, experimental validation burden, and LLM hallucination risk are implied but not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8647.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8647.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-tuned GPT-3 for molecular properties</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Task-specific fine-tuning of GPT-3 for machine learning electronic and functional properties of organic molecules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced finding that task-specific fine-tuning of GPT-3 can yield highly predictive models for chemistry ML tasks, sometimes outperforming dedicated ML models for property prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (GPT) fine-tuned for chemistry property prediction</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>prediction of molecular electronic and functional properties (organic molecules)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>task-specific fine-tuning (supervised fine-tuning on chemical datasets for property prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable — focus is property prediction rather than de novo molecule generation; novelty metrics not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Fine-tuning on domain-specific property datasets to improve predictive performance for chemistry tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Referenced qualitatively as 'highly effective and predictive' and sometimes surpassing dedicated ML models; specific metrics are not given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as evidence that GPT-family models can be fine-tuned for chemistry tasks with strong predictive performance; specific results are in the cited work, not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Paper asserts that fine-tuned GPT-3 can outperform dedicated ML models on some chemistry ML tasks (per referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not discussed in detail here; general concerns of fine-tuning include dataset size/quality and domain alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8647.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8647.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT Chemistry Assistant / MOF text-mining</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced work where ChatGPT (GPT-family) was used to assist text mining and to help predict MOF synthesis outcomes; cited as an example of LLM application in materials synthesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / GPT-4 (as applied in the referenced MOF assistant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>transformer (GPT) used for text mining and predictive assistance</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>materials (MOF) synthesis prediction and literature text mining</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompt-based assistance for extracting and interpreting literature-derived data and predicting synthesis outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not specified here; the referenced work focuses on assisting synthesis prediction rather than reporting metrics of novel molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Adaptation to the MOF synthesis domain via prompts and context drawn from literature corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in this paper; the citation suggests demonstrated utility in assisting text mining and synthesis prediction in the referenced study.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned among examples of LLMs applied to materials discovery and synthesis assistance; this paper does not provide quantitative outcomes from that work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Not compared in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not detailed here; general limitations of LLM-based text-mining (noise in literature, need for validation) implied but not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8647.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8647.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method mentioned as a promising LLM technique to incorporate user-specified datasets and external knowledge sources to support domain-specific tasks and human-LLM collaboration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Retrieval-Augmented Generation (method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>retrieval + generative transformer pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>external retrieval corpus / user-specified datasets (as used in RAG frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>knowledge-intensive chemistry tasks, literature mining, integrating experimental datasets for domain-specific prompts</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>augment LLM generation with retrieved documents / dataset context to ground outputs (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable in this paper; RAG is described as enabling incorporation of user datasets to inform LLM outputs, which could support generation/design workflows in follow-up applications.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Ensures outputs are grounded in retrieved, relevant domain material (e.g., experimental data or literature) to increase relevance to specific chemical applications.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided here; RAG is discussed as an enabling technique rather than evaluated experimentally in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as a future enabling technique to deepen LLM integration into chemistry workflows (e.g., augmenting LLMs with user datasets and facilitating RAG-based chatbots).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned as an enhancement over vanilla prompt-only approaches by enabling grounding of LLM responses in retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Paper notes RAG and LLM frameworks (e.g., LangChain) are promising but does not evaluate limitations here; potential issues include retrieval quality and integration complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8647.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8647.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangChain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain (LLM implementation framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An implementation framework referenced as simplifying the development of RAG-based chatbots and LLM pipelines for integrating external data into LLM workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LangChain (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>software/framework for LLM orchestration (retrieval + chains of calls)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>tooling for constructing RAG-based and multi-step LLM applications in chemistry and other domains</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>facilitates retrieval, prompt chaining, and integration of external datasets to direct LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not applicable in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Framework-level support to build domain-specific pipelines that ground LLM responses in user data and tools.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not applicable here; mentioned as a development aid.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an enabling technology to enhance human-LLM collaboration for chemistry tasks (no experimental results in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Tooling comparison not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8647.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8647.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Autonomous chemical research (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Autonomous chemical research with large language models (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced prior work that explores integrating LLMs into autonomous experimental discovery and lab-automation pipelines, illustrating a direction where LLMs propose experiments and coordinate robotic execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM-driven autonomous research systems (general)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>LLM (transformer) integrated with automation/agents</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>autonomous chemical discovery, experiment planning, robotics-integrated workflows</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-guided experiment ideation/planning and coordination with robotics and agents (autonomous pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not specified in this paper; referenced as a research direction where LLMs can contribute to discovering novel compounds/materials in autonomous settings.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Integration of LLM reasoning with robotic execution and experimental feedback to iterate toward application-specific targets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not provided in this paper; likely contained in the cited references.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as an advancing area where LLMs enable automation and accelerate discovery; the current paper does not present autonomous-experiment results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Not compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Not elaborated in this paper; general challenges include verification of proposals and safe autonomous operation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A GPT-4 Reticular Chemist for Guiding MOF Discovery <em>(Rating: 2)</em></li>
                <li>ChemCrow: Augmenting large-language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis <em>(Rating: 2)</em></li>
                <li>Fine-Tuning GPT-3 for Machine Learning Electronic and Functional Properties of Organic Molecules <em>(Rating: 2)</em></li>
                <li>Shaping the Water-Harvesting Behavior of Metal-Organic Frameworks Aided by Fine-Tuned GPT Models <em>(Rating: 2)</em></li>
                <li>ChatGPT Research Group for Optimizing the Crystallinity of MOFs and COFs <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Large language models for chemistry robotics <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8647",
    "paper_id": "paper-268350146",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "A large transformer-based language model from OpenAI; in this study GPT-4 was used with an ordered-and-structured chain-of-thought (OSCoT) prompting strategy to analyze tabular experimental data and predict catalytic performance of Ce-based metal-oxide composites.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_type": "transformer (GPT)",
            "model_size": null,
            "training_data": null,
            "application_domain": "catalysis/materials (prediction of NH3-SCR catalytic performance for Ce-based metal-oxide composites); referenced for MOF discovery in related work",
            "generation_method": "prompt-based reasoning (Chain-of-Thought), specifically Ordered-and-Structured CoT (OSCoT) to process tabular data sequentially and produce reasoning chains used to predict experimental outcomes",
            "novelty_of_chemicals": "Not applicable for the primary experiments in this paper (GPT-4 was used to analyze/predict performance of existing catalyst compositions rather than to directly generate novel chemical structures); the paper cites external work where GPT-4 was used cooperatively to discover new MOFs but does not report novelty metrics here.",
            "application_specificity": "Tailoring via explicit persona and task-directive prompts (NH3-SCR catalysis expert role), batching of features by importance to focus reasoning on application-relevant variables; predictions target NOx conversion above/below 95%.",
            "evaluation_metrics": "Prediction accuracy (classification of NOx conversion &gt;95% as Positive): average accuracies per binary composite ranged ~60%–74%, with maxima up to ~85%; comparisons to human expert accuracies; statistical summaries (average, max, min) across five independent runs per composite.",
            "results_summary": "Using OSCoT prompting, GPT-4 achieved average prediction accuracies between ~60% and 74% across six binary CeM1 composites and peak accuracies up to 85%, outperforming GPT-3.5 variants and the four surveyed human experts; GPT-4 could also extrapolate, with variable success, from binary to some ternary systems (high accuracy for e.g., CeMnFe, CeTiFe; poor extrapolation for other ternaries).",
            "comparison_to_other_methods": "Compared directly to GPT-3.5 (worse performance and some failure modes) and to human experts (GPT-4 with OSCoT outperformed all four experts tested); the paper situates GPT-4 as superior to humans for this specific multi-variable tabular prediction task but does not compare to traditional computational chemistry generative models because GPT-4 was used for prediction rather than molecule generation here.",
            "limitations_and_challenges": "Context-length and token limits constrain inputs; GPT-4's extrapolation to ternary systems is inconsistent and depends on training batch selection; the study did not generate new molecules with GPT-4 but used it for tabular reasoning; the paper also notes broader limitations such as not testing less-advanced LLMs beyond GPT-series and inherent difficulties of interpreting tabular data.",
            "uuid": "e8647.0",
            "source_info": {
                "paper_title": "Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-3.5-turbo-16k",
            "name_full": "GPT-3.5 (turbo, 16k context)",
            "brief_description": "A GPT-3.5 family model variant with extended context window mentioned and used in experiments; evaluated alongside GPT-4 for CoT prompting strategies on the same catalysis prediction tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (turbo-16k)",
            "model_type": "transformer (GPT)",
            "model_size": null,
            "training_data": null,
            "application_domain": "catalysis/materials prediction (same NH3-SCR dataset as GPT-4)",
            "generation_method": "prompt-based reasoning (Chain-of-Thought); both One-Pot CoT (OPCoT) and Ordered-and-Structured CoT (OSCoT) approaches were attempted",
            "novelty_of_chemicals": "Not applicable — used for prediction of experimental outcomes rather than generating chemical structures.",
            "application_specificity": "Same persona and OSCoT/OPCoT prompting schemes as GPT-4; temperature set to zero for deterministic outputs.",
            "evaluation_metrics": "Prediction accuracy (classification of NOx conversion &gt;95%); token-handling behavior (malfunction above certain token counts) was also evaluated.",
            "results_summary": "GPT-3.5 showed inferior predictive performance compared to GPT-4; it malfunctioned when using the UM2 (OPCoT) prompting in some cases and began malfunctioning when given more than ~3,000 input tokens in the OP approach; OSCoT-GPT3.5 performed worse than OSCoT-GPT4.",
            "comparison_to_other_methods": "Direct comparison in the paper is to GPT-4 (GPT-4 outperformed GPT-3.5). No comparisons to non-LLM chemical generative methods were performed for GPT-3.5 within this study.",
            "limitations_and_challenges": "Severe context/token sensitivity (malfunction &gt;~3,000 tokens in OP approach and failure for UM2), lower accuracy than GPT-4 on tabular catalytic prediction; not used to design novel chemicals in this work.",
            "uuid": "e8647.1",
            "source_info": {
                "paper_title": "Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow (GPT-4-based chemistry tool)",
            "brief_description": "A GPT-4-based chemistry tool mentioned in the paper that streamlines reasoning across chemical tasks (from drug design to synthesis) by sequentially prompting the model to align intermediate actions with end goals.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChemCrow (GPT-4-based tool)",
            "model_type": "GPT-based prompting/agent",
            "model_size": null,
            "training_data": null,
            "application_domain": "drug design, organic synthesis, general chemistry tasks",
            "generation_method": "sequential prompting and tool-augmented agent workflow to guide multi-step chemical tasks (prompt-guided planning and action sequencing)",
            "novelty_of_chemicals": "Not specified in this paper; described qualitatively as aiding design and synthesis workflows but no novelty metrics provided here.",
            "application_specificity": "Guides the model through task-specific sequences to accomplish domain-relevant goals (e.g., design or synthetic planning) via stepwise prompts and actions.",
            "evaluation_metrics": "Not provided in this paper; described qualitatively as improving access to chemical knowledge and streamlining reasoning.",
            "results_summary": "Mentioned as an example of LLM-empowered tools that assist from drug design to synthesis by organizing sequential prompts; no quantitative results shown in this study.",
            "comparison_to_other_methods": "Not directly compared in this paper; presented as an exemplar of GPT-4-based chemistry tooling that can assist both experts and novices.",
            "limitations_and_challenges": "This paper does not detail ChemCrow's limitations; general LLM concerns (hallucination, context limits, synthesizability verification) would apply but are not described specifically here.",
            "uuid": "e8647.2",
            "source_info": {
                "paper_title": "Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "GPT-4 Reticular Chemist",
            "name_full": "A GPT-4 Reticular Chemist for Guiding MOF Discovery",
            "brief_description": "Referenced work (Z. Zheng et al.) where GPT-4 was applied cooperatively with human experts to aid discovery of new metal–organic frameworks (MOFs); cited as an example of LLMs contributing to material discovery pipelines.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (applied as 'Reticular Chemist')",
            "model_type": "transformer (GPT) applied in a domain-specific cooperative workflow",
            "model_size": null,
            "training_data": null,
            "application_domain": "materials discovery (MOFs)",
            "generation_method": "cooperative workflow with human experts (prompt-driven ideation, strategy suggestion, and synthesis condition guidance)",
            "novelty_of_chemicals": "Not specified in this paper beyond qualitative claim of discovery of a series of MOFs each synthesized with unique strategies; no quantitative novelty metrics reported here.",
            "application_specificity": "Human+LLM cooperative workflows where GPT-4 aids in proposing candidate materials and synthesis strategies tailored to MOF discovery goals.",
            "evaluation_metrics": "Not provided in this paper; original reference likely contains experimental synthesis and validation details.",
            "results_summary": "Cited as an example in which GPT-4 contributed to discovering multiple MOFs through cooperation with human experts; this paper does not report the detailed outcomes.",
            "comparison_to_other_methods": "Mentioned as evidence that GPT-4 can be integrated into discovery workflows; no quantitative comparison is provided in this paper.",
            "limitations_and_challenges": "This paper does not detail limitations of that work; in general, issues like verification of model proposals, experimental validation burden, and LLM hallucination risk are implied but not enumerated.",
            "uuid": "e8647.3",
            "source_info": {
                "paper_title": "Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Fine-tuned GPT-3 for molecular properties",
            "name_full": "Task-specific fine-tuning of GPT-3 for machine learning electronic and functional properties of organic molecules",
            "brief_description": "Referenced finding that task-specific fine-tuning of GPT-3 can yield highly predictive models for chemistry ML tasks, sometimes outperforming dedicated ML models for property prediction.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (fine-tuned)",
            "model_type": "transformer (GPT) fine-tuned for chemistry property prediction",
            "model_size": null,
            "training_data": null,
            "application_domain": "prediction of molecular electronic and functional properties (organic molecules)",
            "generation_method": "task-specific fine-tuning (supervised fine-tuning on chemical datasets for property prediction)",
            "novelty_of_chemicals": "Not applicable — focus is property prediction rather than de novo molecule generation; novelty metrics not reported here.",
            "application_specificity": "Fine-tuning on domain-specific property datasets to improve predictive performance for chemistry tasks.",
            "evaluation_metrics": "Referenced qualitatively as 'highly effective and predictive' and sometimes surpassing dedicated ML models; specific metrics are not given in this paper.",
            "results_summary": "Cited as evidence that GPT-family models can be fine-tuned for chemistry tasks with strong predictive performance; specific results are in the cited work, not detailed here.",
            "comparison_to_other_methods": "Paper asserts that fine-tuned GPT-3 can outperform dedicated ML models on some chemistry ML tasks (per referenced work).",
            "limitations_and_challenges": "Not discussed in detail here; general concerns of fine-tuning include dataset size/quality and domain alignment.",
            "uuid": "e8647.4",
            "source_info": {
                "paper_title": "Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "ChatGPT Chemistry Assistant / MOF text-mining",
            "name_full": "ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis (referenced)",
            "brief_description": "Referenced work where ChatGPT (GPT-family) was used to assist text mining and to help predict MOF synthesis outcomes; cited as an example of LLM application in materials synthesis planning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / GPT-4 (as applied in the referenced MOF assistant)",
            "model_type": "transformer (GPT) used for text mining and predictive assistance",
            "model_size": null,
            "training_data": null,
            "application_domain": "materials (MOF) synthesis prediction and literature text mining",
            "generation_method": "prompt-based assistance for extracting and interpreting literature-derived data and predicting synthesis outcomes",
            "novelty_of_chemicals": "Not specified here; the referenced work focuses on assisting synthesis prediction rather than reporting metrics of novel molecules.",
            "application_specificity": "Adaptation to the MOF synthesis domain via prompts and context drawn from literature corpora.",
            "evaluation_metrics": "Not provided in this paper; the citation suggests demonstrated utility in assisting text mining and synthesis prediction in the referenced study.",
            "results_summary": "Mentioned among examples of LLMs applied to materials discovery and synthesis assistance; this paper does not provide quantitative outcomes from that work.",
            "comparison_to_other_methods": "Not compared in this paper.",
            "limitations_and_challenges": "Not detailed here; general limitations of LLM-based text-mining (noise in literature, need for validation) implied but not specified.",
            "uuid": "e8647.5",
            "source_info": {
                "paper_title": "Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Retrieval-Augmented Generation (RAG)",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A method mentioned as a promising LLM technique to incorporate user-specified datasets and external knowledge sources to support domain-specific tasks and human-LLM collaboration.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Retrieval-Augmented Generation (method)",
            "model_type": "retrieval + generative transformer pipeline",
            "model_size": null,
            "training_data": "external retrieval corpus / user-specified datasets (as used in RAG frameworks)",
            "application_domain": "knowledge-intensive chemistry tasks, literature mining, integrating experimental datasets for domain-specific prompts",
            "generation_method": "augment LLM generation with retrieved documents / dataset context to ground outputs (RAG)",
            "novelty_of_chemicals": "Not applicable in this paper; RAG is described as enabling incorporation of user datasets to inform LLM outputs, which could support generation/design workflows in follow-up applications.",
            "application_specificity": "Ensures outputs are grounded in retrieved, relevant domain material (e.g., experimental data or literature) to increase relevance to specific chemical applications.",
            "evaluation_metrics": "Not provided here; RAG is discussed as an enabling technique rather than evaluated experimentally in this study.",
            "results_summary": "Cited as a future enabling technique to deepen LLM integration into chemistry workflows (e.g., augmenting LLMs with user datasets and facilitating RAG-based chatbots).",
            "comparison_to_other_methods": "Positioned as an enhancement over vanilla prompt-only approaches by enabling grounding of LLM responses in retrieved documents.",
            "limitations_and_challenges": "Paper notes RAG and LLM frameworks (e.g., LangChain) are promising but does not evaluate limitations here; potential issues include retrieval quality and integration complexity.",
            "uuid": "e8647.6",
            "source_info": {
                "paper_title": "Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "LangChain",
            "name_full": "LangChain (LLM implementation framework)",
            "brief_description": "An implementation framework referenced as simplifying the development of RAG-based chatbots and LLM pipelines for integrating external data into LLM workflows.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LangChain (framework)",
            "model_type": "software/framework for LLM orchestration (retrieval + chains of calls)",
            "model_size": null,
            "training_data": null,
            "application_domain": "tooling for constructing RAG-based and multi-step LLM applications in chemistry and other domains",
            "generation_method": "facilitates retrieval, prompt chaining, and integration of external datasets to direct LLM outputs",
            "novelty_of_chemicals": "Not applicable in this paper.",
            "application_specificity": "Framework-level support to build domain-specific pipelines that ground LLM responses in user data and tools.",
            "evaluation_metrics": "Not applicable here; mentioned as a development aid.",
            "results_summary": "Mentioned as an enabling technology to enhance human-LLM collaboration for chemistry tasks (no experimental results in this paper).",
            "comparison_to_other_methods": "Tooling comparison not provided.",
            "limitations_and_challenges": "Not detailed in this paper.",
            "uuid": "e8647.7",
            "source_info": {
                "paper_title": "Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Autonomous chemical research (LLMs)",
            "name_full": "Autonomous chemical research with large language models (referenced)",
            "brief_description": "Referenced prior work that explores integrating LLMs into autonomous experimental discovery and lab-automation pipelines, illustrating a direction where LLMs propose experiments and coordinate robotic execution.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLM-driven autonomous research systems (general)",
            "model_type": "LLM (transformer) integrated with automation/agents",
            "model_size": null,
            "training_data": null,
            "application_domain": "autonomous chemical discovery, experiment planning, robotics-integrated workflows",
            "generation_method": "LLM-guided experiment ideation/planning and coordination with robotics and agents (autonomous pipeline)",
            "novelty_of_chemicals": "Not specified in this paper; referenced as a research direction where LLMs can contribute to discovering novel compounds/materials in autonomous settings.",
            "application_specificity": "Integration of LLM reasoning with robotic execution and experimental feedback to iterate toward application-specific targets.",
            "evaluation_metrics": "Not provided in this paper; likely contained in the cited references.",
            "results_summary": "Mentioned as an advancing area where LLMs enable automation and accelerate discovery; the current paper does not present autonomous-experiment results.",
            "comparison_to_other_methods": "Not compared here.",
            "limitations_and_challenges": "Not elaborated in this paper; general challenges include verification of proposals and safe autonomous operation.",
            "uuid": "e8647.8",
            "source_info": {
                "paper_title": "Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A GPT-4 Reticular Chemist for Guiding MOF Discovery",
            "rating": 2,
            "sanitized_title": "a_gpt4_reticular_chemist_for_guiding_mof_discovery"
        },
        {
            "paper_title": "ChemCrow: Augmenting large-language models with chemistry tools",
            "rating": 2,
            "sanitized_title": "chemcrow_augmenting_largelanguage_models_with_chemistry_tools"
        },
        {
            "paper_title": "ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis",
            "rating": 2,
            "sanitized_title": "chatgpt_chemistry_assistant_for_text_mining_and_prediction_of_mof_synthesis"
        },
        {
            "paper_title": "Fine-Tuning GPT-3 for Machine Learning Electronic and Functional Properties of Organic Molecules",
            "rating": 2,
            "sanitized_title": "finetuning_gpt3_for_machine_learning_electronic_and_functional_properties_of_organic_molecules"
        },
        {
            "paper_title": "Shaping the Water-Harvesting Behavior of Metal-Organic Frameworks Aided by Fine-Tuned GPT Models",
            "rating": 2,
            "sanitized_title": "shaping_the_waterharvesting_behavior_of_metalorganic_frameworks_aided_by_finetuned_gpt_models"
        },
        {
            "paper_title": "ChatGPT Research Group for Optimizing the Crystallinity of MOFs and COFs",
            "rating": 2,
            "sanitized_title": "chatgpt_research_group_for_optimizing_the_crystallinity_of_mofs_and_cofs"
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2,
            "sanitized_title": "autonomous_chemical_research_with_large_language_models"
        },
        {
            "paper_title": "Large language models for chemistry robotics",
            "rating": 2,
            "sanitized_title": "large_language_models_for_chemistry_robotics"
        }
    ],
    "cost": 0.014211749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy</p>
<p>Muyu Lu 
School of Energy and Environmental Engineering
University of Science and Technology Beijing
100083BeijingP.R. China</p>
<p>Fengyu Gao 
School of Energy and Environmental Engineering
University of Science and Technology Beijing
100083BeijingP.R. China</p>
<p>Xiaolong Tang txiaolong@126.com 
School of Energy and Environmental Engineering
University of Science and Technology Beijing
100083BeijingP.R. China</p>
<p>Linjiang Chen l.j.chen@bham.ac.uk 
School of Chemistry and School of Computer Science
University of Birmingham
B15 2TTBirminghamUK</p>
<p>Key Laboratory of Precision and Intelligent Chemistry
University of Science and Technology of China
230026HefeiAnhuiChina</p>
<p>Fengyu Gao
Xiaolong Tang, Linjiang Chen</p>
<p>Analysis and prediction in SCR experiments using GPT-4 with an effective chain-of-thought prompting strategy
5FC71D819384E70FB370FCF821296AFC10.1016/j.isci.2024.109451
HighlightsApplication of a large language model (LLM) in chemistry tasks A new chain-of-thought prompting strategy focusing on formulating logical steps An LLM-powered assistant that interprets, predicts, and rationalizes experimental data</p>
<p>INTRODUCTION</p>
<p>The emergence of the latest large language models (LLMs), notably GPT-3 and GPT-4, 1,2 is transforming the landscape of human-computer interaction, revolutionizing a wide range of personal and professional tasks through advanced artificial intelligence (AI) capabilities. 3,4These LLMs, trained on vast amounts of text data, are capable of generating human-like text, answering common-sense questions, and even performing tasks that require understanding and reasoning. 5,6LLMs can provide textual content creation and offer personalized interactions and recommendations. 7Their proficiency extends to tasks that require inferential reasoning, such as answering questions, solving mathematical problems, 8 and even passing bar examinations. 9Moreover, the impact of LLMs is evident in the realm of academic research.LLMs, like GPT-4, have the potential to streamline the literature review process by efficiently summarizing vast academic resources, extracting insights from the literature, and facilitating the generation of innovative research ideas. 10y enabling the analysis of extensive textual data, these models may uncover overlooked themes or patterns, offering fresh perspectives on existing research. 11LMs have started showing the potential to revolutionize chemistry by accelerating research and discovery in collaboration with human chemists.For instance, GPT-4 has been used in the discovery of new metal-organic frameworks (MOFs) through a cooperative workflow with human experts.This synergy enabled the discovery of a series of MOFs, each synthesized using unique strategies and conditions. 12In the broader landscape, LLM-empowered AI tools and agents are making strides in organic synthesis, drug discovery, and materials design.ChemCrow, 13 a GPT-4-based chemistry tool, exemplifies this.It streamlines reasoning for various chemical tasks, from drug design to synthesis.By sequentially prompting GPT-4, ChemCrow guides the model through the task, aligning actions with the end goal.This tool not only aids expert chemists but also simplifies access to chemical knowledge for novices.Moreover, task-specific fine-tuning of GPT-3 has been shown to yield highly effective and predictive models for a range of chemistry machine-learning (ML) tasks, often surpassing the performance of dedicated ML models specifically developed for these tasks. 14n this study, we hypothesize that GPT-4's language understanding capabilities, when combined with its strengths in pattern recognition and inferential reasoning, might enable effective analysis and interpretation of knowledge specific to a research topic or scientific domain.Our focus is on structured data rather than texts directly presented in scientific publications; in essence, we are not evaluating GPT-4's text mining capabilities.Instead, we aim to assess GPT-4's proficiency in recognizing patterns within structured data, which allows it to discern and capitalize on underlying trends.Such pattern recognition is invaluable when analyzing experimental variables and their associated outcomes.Furthermore, we seek to determine if GPT-4's ability for inferential reasoning enables it to make well-founded predictions based on the provided information and to assess the robustness of its rationale behind those predictions.</p>
<p>ll OPEN ACCESS</p>
<p>RESULTS</p>
<p>Binary CeM 1 metal-oxide composites</p>
<p>To prepare representative training data, rational selection by custom search (Table S2) from the dataset was conducted five runs, each generating a batch of 48 samples that covered six types of binary CeM 1 metal-oxide catalysts (M 1 = Ti, Mn, W, Sn, Mo, or Fe).The selected samples were then integrated into UM1 and UM2, respectively, to generate OP-and OS-CoT, using both GPT-3.5 and GPT-4.We evaluated the effectiveness of UM1 and UM2 in generating CoTs from GPT-3.5 and GPT-4 against three common metrics for assessing the performance of LLMs: 'disobedience', 'helpfulness', and 'honesty'.Detailed analyses are shown in Figures S4 and S5.Notably, we observed that GPT-3.5 malfunctioned when using UM2 in trying to generate OPCoT, leading to its exclusion from comparison.We examined the optimal number of input tokens for GPT-3.5-turbo-16k, as illustrated in Figure S6, revealing that it began to malfunction when burdened with more than 3,000 tokens in the OP approach.Finally, three combinations-OPCoT-GPT4, OSCoT-GPT4, and OSCoT-GPT3.5-wereincorporated into UM3 to infer the experimental performance of CeM 1 metal-oxide composites samples in five runs.</p>
<p>OP-and OS-CoTs were used to guide GPT-3.5 and GPT-4 to infer the experimental performance of each of the CeM 1 samples.As depicted in Figures 1A-1F, the average prediction accuracies of OSCoT-GPT4 for the six different binary composites reached 71.6%, 74%, 64.2%, 60%, 67.6%, and 65.6%, respectively.The maximum prediction accuracies for them reached 82%, 85%, 69%, 67%, 71%, and 73%, respectively.OSCoT-GPT4 consistently outperformed both OPCoT-GPT4 and OSCoT-GPT3.5 with the only exceptions of the maximum and average accuracy values for CeFe, for which other two combinations were more effective.Notably, the minimum accuracy values of OSCoT-GPT4 were the higher than the other two CoT-GPT combinations for all six CeM 1 samples.</p>
<p>Predicting catalysis outcomes poses a significant challenge due to the complexity and multi-step nature of the process.The reasoning route, generated by the OS prompting strategy (Figures 2 and 3), is particularly valuable in this context as it facilitates structured problem-solving.By breaking down the intricate task of catalysis prediction into smaller, logical steps, OSCoT mimics human reasoning, leading to improved understanding and interpretation of the problem.This structured reasoning not only makes the GPT-4's thought process more transparent but also enhances trust in its outputs.Additionally, implementing OSCoT-GPT4 in predicting catalysis can serve as an enhanced form of training, encouraging models to develop a deeper level of understanding and process information in a more nuanced, human-like manner.</p>
<p>Extrapolation to ternary CeM 1 M 2 metal-oxide composites</p>
<p>Next, we assessed the performance of the GPT-4, using OSCoT, in predicting outcomes for ternary composites of metal oxides by learning from experiments involving only binary composites.Specifically, we trained a GPT-4 on experiments involving CeM 1 and CeM 2 (M 1 , M 2 = Ti, Mn, W, Sn, Mo, or Fe; M 1 s M 2 ), following the same OSCoT-GPT4 training procedure as described above.We then evaluated this GPT-4's prediction performance for experiments involving the corresponding ternary CeM 1 M 2 composites.This process was independently repeated five times, each instance yielding a unique OSCoT-GPT4.Each time was trained with a distinct batch of 48 experiments for CeM 1 and another for CeM 2 , both batches being rationally selected.</p>
<p>We can partially attribute the observed extrapolation performance of the GPT-4 Assistant to its use of associations between specific metals and experimental variables in making predictions.Specifically, the GPT-4 Assistant appears to have constructed a knowledge graph from the For all composite types (A-F), five independent runs were conducted, each using one of the five batches of 48 samples for training.Prediction accuracies were evaluated using 50 samples randomly selected from the dataset, which were distinct from the training samples.The average, maximum, and minimum prediction accuracy values for each composite type were determined across the five runs.</p>
<p>binary training data, linking certain combinations of input variables to either high or low catalytic performances.When making predictions for ternary metal-oxide composites, it considered such associations.However, what remains unclear from the current data and results is the way the GPT-4 Assistant prioritizes these associations, particularly when they conflict.This uncertainty might partly account for the GPT-4 Assistant's reduced predictive performance with ternary systems, where little is known about the interplay and interactions among various metalsother than individual metals each with Ce-as represented on the knowledge graph derived from binary systems.To gain some interpretability, one could consider methods related to information geometry, which offer a structured and mathematical framework to comprehend how information is processed and represented within AI models.</p>
<p>Figure 4 presents results for eight ternary CeM 1 M 2 composites, divided into two groups: CeMnM 2 (M 2 = W, Ti, Sn, or Fe) and CeTiM 2 (M 2 = W, Sn, Mo, or Fe).We focused on these eight, out of the 15 possible permutations of CeM 1 M 2 , considering the prevalence of their corresponding binary counterparts in the dataset.For each specific CeM 1 M 2 composite, 50 experiments involving it were used to evaluate the prediction performance of the GPT-4 trained on the corresponding binary systems.Stoichiometry, as the molar ratio between the metal oxides (binary or ternary), was a variable in all cases.</p>
<p>Figure 4A shows the results of the five independent runs for each of the eight ternary systems.Notably, the GPT-4, all using the OSCoT prompting strategy, exhibited significantly different levels of prediction performance for the various ternary systems.For CeMnFe, CeMnTi, and CeTiFe, the OSCoT-GPT4 consistently yielded high prediction accuracies for the 50 ternary systems, after analyzing only the corresponding binary counterparts (48 CeM 1 + 48 CeM 2 ).In contrast, the OSCoTs-GPT4 intended for predicting CeTiSn and CeTiMo demonstrated poor performance.For CeMnW, they generated in the five independent runs showed markedly varied prediction performances.Figure 4B provides a statistical summary of the run-specific accuracies for each ternary system.Upon detailed analysis of the various OSCoTs-GPT4 with respect to the systems in Figure 4A, we could attribute the differing levels of prediction performance for the ternary systems to the varying prediction performances of the OSCoTs-GPT4 for the binary systems they were trained to analyze.For instance, the OSCoTs-GPT4 intended for CeMnFe exhibited high prediction performances for CeMn and CeFe, while the ones for CeTiMo demonstrated poor performance for CeTi and CeMo.</p>
<p>Comparison with human experts</p>
<p>To gauge the performance of the various combinations of CoTs-GPT, as described in the preceding sections, we conducted a survey involving four human experts to assess their performance on the same prediction tasks.All four experts were postgraduate research students specializing in NH 3 -SCR catalysis and had experience in synthesizing metal-oxide composites and measuring their performances for NH 3 -SCR catalysis.First, we asked Experts 1-3 to predict experimental outcomes-i.e., whether the NO x conversion would be above (Positive) or below (Negative) 95%-for 50 experiments involving binary metal-oxide composites; in Figure 5A, these results are designated as 'Without Training'.In these 50 experiments, the type of CeM 1 , its stoichiometry, synthesis conditions, and catalysis reaction conditions were all variables, though certain experiments shared the same values for certain variables.Expert 1 performed the best, attaining a In the OP method, all data points from the table are simultaneously processed to form a single CoT (termed OPCoT).Conversely, in the OS method, table data points are batched according to feature rank hierarchy, with each batch sequentially giving rise to intermediate CoTs.Each CoT incrementally builds upon the logic of the preceding one, representing a progressive development of understanding.The OSCoT materializes through iterative processing of all data point batches.The small chain icon represents the integration of messages, indicating, for example, that a connection between user_message and data points, along with the intermediate OSCoT, integrates these elements into the corresponding user message, thereby solidifying the foundation for subsequent OSCoT iterations.Table S1 details the full names of the abbreviations.prediction accuracy of 66%, while Experts 2 and 3 attained prediction accuracies of 51% and 56%, respectively, only marginally better than random guess.</p>
<p>Subsequently, after providing Experts 1-3 with the correct answers for these 50 experiments, they were given another set of 50 different experiments to predict.Their performances, denoted as 'After Training' in Figure 5A, interestingly showed no gains; in fact, Expert 1's performance appreciably worsened.Post-prediction, Experts 1-3 were interviewed and asked to summarize their rationales.As shown in Figure 5C, all three Experts applied relatively simple rules, considering no more than a couple of experimental variables.Their strategies were as follows:</p>
<p>(1) focusing on a small set of experiments to identify correlations between the experimental variables and the outcomes or (2) trying to identify a few key experimental variables that significantly influenced the outcomes when altered.While both strategies appear sensible and intuitive, they highlight the challenges humans encounter when analyzing multi-variable problems in sizable datasets.Specifically, correlations significant for a small dataset may not apply to a larger dataset.Similarly, factors that seem significant across the entire dataset may not aid in individual predictions, as the combination and balance of different factors can play an equally, or even more, important role.</p>
<p>Similar observations and conclusions were drawn when Experts 1-3 were asked to make predictions for experiments involving ternary metal-oxide composites (Figure 5B).The variation in prediction performances among the different Experts, as well as the differences in their performance across the various tasks, seem to suggest a strong element of guessing.This is not entirely unexpected, considering the challenge of retaining and processing information from tens of experiments and then applying any discerned rules and correlations to an entirely new set of experiments.An additional expert, Expert 4, who possessed similar experience in the research topic as Experts 1-3, was explicitly instructed to consider multiple experimental variables when approaching the prediction tasks (Figure 5C).Expert 4 was provided with the same sets of experiments for training, as well as the same sets for prediction, as were given to Experts 1-3.Among the four experts, Expert 4 performed the most poorly for both binary and ternary systems.</p>
<p>All four experts' performances were inferior to those of the GPT-4 using OSCoT, as discussed above (Figures 1 and 4).There are several factors that could have contributed to this.The sampling of just four human experts is far from adequate for establishing a comprehensive baseline of human performance on the prediction tasks.Nonetheless, LLMs like GPT-4 may outperform humans in predicting outcomes for complex, multi-variable chemistry experiments for several reasons.First, LLMs possess remarkable data processing capacity, allowing them to analyze and utilize information from lots of experiments simultaneously, a task that is challenging for humans.Second, LLMs consistently apply rules and patterns across datasets without experiencing cognitive fatigue or bias, in contrast to humans who might get overwhelmed by the volume or complexity of the data.Furthermore, LLMs are not susceptible to cognitive biases that can affect human analysis and conclusions.Their ability to detect subtle patterns in diverse and extensive datasets enables them to make more accurate predictions in intricate scenarios.Finally, LLMs benefit from rapid iterative learning, adapting and improving at a pace faster than the typical learning curve of human experts.Overall, the immense data processing capabilities, consistent and objective analysis, and rapid learning and pattern detection of LLMs make them well-equipped for complex tasks such as predicting outcomes in NH 3 -SCR catalysis experiments.</p>
<p>DISCUSSION</p>
<p>This study reveals that, by employing an effective OSCoT prompting strategy, GPT-4 achieved notable prediction accuracies regarding the performance of binary CeM 1 metal-oxide composite samples.The average prediction accuracies ranged from 60% to 74%, with peaks (A and B) Decomposing the complex task into general, manageable tasks, and splitting and reinforcing them to ensure thorough analysis.(C) Putting the analyzed datapoints in the rear of the messages.In Python, the f-string format employs curly braces {} to insert the content within variables into the string.Here, we use angle brackets &lt;&gt; as separators for generated sentences or paragraphs.Additionally, the variable delimiter adopted here is ''##''.</p>
<p>reaching up to 85%, outperforming human experts.These results were made possible by the OSCoT strategy's ability to break down intricate problems into sequential, manageable steps, enhancing the model's understanding and interpretation of complex tabular data.Extending the generated OSCoT by the binary CeM 1 samples to reason the performance of ternary CeM 1 M 2 samples, we observed a varied predictive performance, with some composites like CeMnFe and CeTiFe showing high accuracy, illustrating GPT-4's ability to extrapolate from binary to ternary systems.These findings demonstrate the intricate relationship between variables and causal outcomes in our curated dataset, solidifying the deductive connections implied by our table data through the application of a reasoning CoT.</p>
<p>The comparative analysis with human experts has further highlighted the advantages of employing LLMs in chemistry research.In contrast to human capabilities, LLMs like GPT-4, despite facing context length limitations, are significantly less affected by cognitive biases and are not easily overwhelmed by the immense volume and intricacy of tabular data.They demonstrate exceptional data processing prowess, reflecting some level of reasoning ability, a task that poses a considerable challenge for human experts.Moreover, their uniform application of rules and identification of patterns within table data, combined with their rapid learning and adaptability, equip them to discern subtle correlations amidst varied datasets.This leads to more precise predictions in complex research scenarios.</p>
<p>In this study, we investigated the application of GPT-4, coupled with our proposed OSCoT prompting strategy, to analyze experimental data concerning the variables and outcomes of a specific catalysis.We illustrated GPT-4's adeptness at unraveling intricate, multi-variable correlations within the catalysis.Broadly speaking, an experimental workflow may include several stages: synthesis, characterization, testing, data analysis, iteration of these processes, and/or others.][17][18][19] Looking ahead, advanced LLM techniques like Retrieval Augmented Generation (RAG) 20 will continue to enhance human-LLM collaboration on user-defined tasks.For instance, RAG facilitates the seamless incorporation of user-specified datasets, allowing for efficient access to knowledge relevant to the user's queries.LLM implementation frameworks, such as LangChain, simplify the development of RAG-based chatbots. 21These enabling techniques and their ongoing improvements will promote broader, more effective, and deeper integration of LLMs into chemistry research.They hold the promise of transforming various research tasks, including, but not limited to, the automation of labor-intensive activities like literature mining, interpretation of experimental results, and directing robotic operations.</p>
<p>Limitations of the study</p>
<p>This study investigated the capabilities of LLMs in analyzing experimental data and making predictions on related experiments, in comparison with human chemists during the post-analysis phase of chemical research.Specifically, it introduced an efficient prompt engineering technique named OSCoT for tabular data.Despite its contributions, the study has certain limitations.The method was evaluated using the state-of-the-art GPT-series models and has not been extended to other less advanced models.Furthermore, the concept of chain of thought was utilized to aid in the interpretation of tabular data.However, there remains scope for development in this area, as tabular data are inherently more complex to understand than plain text.</p>
<p>STAR+METHODS</p>
<p>Detailed methods are provided in the online version of this paper and include the following: enabling LLMs to process complex tasks through rational and logical reasoning.It breaks down intricate tasks into manageable, sequential steps.Few-shot CoTs can be used to assist LLMs in tasks that demand a consistent and logical progression, such as common-sense reasoning.In the realm of zero-shot learning, heuristic prompts like "Let's think step by step" have been shown to effectively encourage LLMs to 'think aloud', thereby enhancing their problem-solving capabilities. 23Further advancements include Automatic CoT (Auto-CoT) 24 and self-consistency. 25Auto-CoT simplifies the process of generating question sampling or reasoning paths, while self-consistency aims to improve the reliability and coherence of LLM outputs.These advancements mark significant progress in enhancing the performance and practicality of LLMs.
d KEY RESOURCES TABLE
The application of CoT lies in the interpreted feature importance in the tabular data that embodies clear deductive relationships between experimental parameters and causal catalytic performance.GPT-4-powered analysis is expected to reveal patterns that signify the varying levels of influence of different experimental factors in NH 3 -SCR catalysis.In this analysis, each inference step within the CoT reasoning paths is analogized to the evaluation of a specific experimental variable in catalysis, providing a structured approach to understanding the data.The results of this analysis are presented as CoTs, making the reasoning process and conclusions transparent.Our aim is to develop targeted prompting strategies for GPT-4 to enhance its ability to recognize patterns in structured data and produce outputs formatted as CoTs.These strategies are designed to guide GPT-4 in systematically identifying and interpreting the statistical relationship in the tabular data.</p>
<p>One-pot vs. ordered-and-structured CoT prompting LLMs often struggle with processing excessive context, necessitating careful planning in the provision of context for enhanced performance. 26dditionally, systematic review or self-reflection can significantly contribute to overall effectiveness. 27Bearing these considerations in mind, the full features of experiments in tabular form was divided into batches.In this study, the categorization process is determined by feature importance derived from machine learning interpretation.Indeed, the sequence of batch input can also be influenced by feature importance, as identified through heuristic knowledge or statistical techniques such as correlation analysis.The goal was to ensure that, within each batch, only one experimental parameter varied significantly, while all other parameters remained nearly constant across the samples.For example, in batch X, all data points (i.e., experiments) featured almost identical synthesis and catalysis conditions but varied in the compositions of the catalysts.These batches, each emphasizing a single varying experimental parameter, were then sequentially presented to GPT-4.This approach augmented GPT-4's analysis depth for individual experimental parameters, ultimately facilitating the generation of more optimal CoTs.</p>
<p>The batchwise, sequential approach is hereafter termed the ''Ordered-and-Structured'' (OS)-CoT prompting strategy.It was compared with a ''One-Pot'' (OP) prompting strategy, where all data points were inputted simultaneously.The implementation of both CoT prompting strategies is depicted in Figure 2. The OPCoT approach creates a single CoT in at once by using all available data points.In contrast, the OSCoT approach sequentially generates multiple CoTs.As each new batch of data points is introduced, a fresh CoT is created, incorporating the reasoning from the preceding CoT developed in the previous batch.Consequently, the CoTs formed by the OSCoT approach, each building upon the insights of the last.</p>
<p>Prompts for the generation and application of CoTs</p>
<p>In the web-based ChatGPT user interface, chat sessions are facilitated through user-initiated messages.In this study, we focused not on the ChatGPT interface but rather on using OpenAI's application programming interface (API).In the context of the API, it is necessary to format messages as dialogues involving typically two participant roles: the User (or the API Client) and the Assistant (the AI Model).The User is the entity, either a person or another system, that sends requests to the API.These requests, or user inputs, are what the AI model responds to.The Assistant, on the other hand, is the role played by the AI, such as GPT-4, generating responses based on the User's input.Here, we evaluate and compare the performances of both GPT-4 and GPT-3.5 (GPT-3.5-turbo-16k).Notably, we have set the ''temperature'' hyperparameter, which influences the model's prediction randomness, to zero to ensure the most consistent output as possible for the same input.</p>
<p>The effectiveness of LLM-generated responses heavily relies on the quality of task-directive prompts within in-context learning across multiple conversations.Our approach involves integrating various tactics to generate querying prompts.Figure 3 illustrates the user messages designated as UM1, which were used for OSCoT generation.Figure S1 depicts the user messages for OPCoT generation, designated as UM2.Figures S2 and S3 demonstrate the utilization of CoTs to predict the experimental performance of CeM 1 and CeM 1 M 2 samples, respectively, with user messages designated as UM3 and UM4.Additionally, given the token limitations inherent in GPT models, where both assistant (i.e., GPT) and user messages contribute to the token input capacity, careful selection of data samples is crucial.The protocol for the rational selection process and the computation of token utilization are detailed in the Supporting Information.</p>
<p>Before eliciting CoTs, careful configuration of in-context content is necessary to guide the cognitive processes of LLMs.The initial interaction with UM1, as shown in Figure 3, demonstrates the use of distinct tactics for establishing this setup, identical to those employed for OPCoT generation using UM2, as presented in Figure S1.Tactics I-IV were utilized for the prompts providing ''Clear Instructions.''GPT-4, initially agnostic to roles, can adopt a specific role when prompted.Consequently, we assigned the role of an NH 3 -SCR catalysis expert to GPT-4 for target-oriented tasks using the 'persona' tactic (I).This was followed by clarifying the input content to enhance GPT-4's focus on data points (II).Subsequently, we emphasized generating CoTs with consistent, generalized, and quantitative characteristics to establish a foundation for subsequent inferences (III).Next, we eliminated unnecessary information and requested reformatted output to streamline the process and enhance output quality (IV).</p>
<p>Tactics a-c serve the role of ''Decomposing Complex Tasks.''In particular, we outlined the overall task (a) and then broke it down into several simpler component parts to facilitate the generation of CoTs (b).The subsequent user message implemented the 'head-tail' tactic (c), placing the most crucial prompts at the beginning and end of the message.This tactic was employed to append data points at the end during each iteration of CoT generation, as LLMs tend to 'lose focus in the middle' with lengthy contextual input. 28or using CoTs to infer experimental performance, the initial interaction of UM3 or UM4 (as depicted in Figures S2 and S3, respectively) in the front-user message for context-aware adaptation employed tactics I, II, IV, and a.These tactics were used to establish the role of the NH 3 -SCR expert, specify input material, define output's structure and format, and outline the general objective.Data points and CoTs were placed at the end of messages as part of tactic c.Additionally, we employed a cautious tactic termed ''allocating thinking'' (d), crucial for conducting comprehensive analysis prior to making decisions.This tactic enhances the use of ''loud thinking,'' considering all reasoning paths of CoTs.It also moderates the pace of text completion and prevents exceeding the rate limits, which could lead to execution errors if the server is queried too quickly.</p>
<p>Figure 1 .
1
Figure 1.Comparison of the three CoT-GPT combinations in predicting experimental NO x conversion outcomes for the six binary CeM 1 composites samplesFor all composite types (A-F), five independent runs were conducted, each using one of the five batches of 48 samples for training.Prediction accuracies were evaluated using 50 samples randomly selected from the dataset, which were distinct from the training samples.The average, maximum, and minimum prediction accuracy values for each composite type were determined across the five runs.</p>
<p>Figure 2 .
2
Figure2.Illustration of CoT generation using the ''One-Pot'' and ''Ordered-and-Structured'' approaches In the OP method, all data points from the table are simultaneously processed to form a single CoT (termed OPCoT).Conversely, in the OS method, table data points are batched according to feature rank hierarchy, with each batch sequentially giving rise to intermediate CoTs.Each CoT incrementally builds upon the logic of the preceding one, representing a progressive development of understanding.The OSCoT materializes through iterative processing of all data point batches.The small chain icon represents the integration of messages, indicating, for example, that a connection between user_message and data points, along with the intermediate OSCoT, integrates these elements into the corresponding user message, thereby solidifying the foundation for subsequent OSCoT iterations.TableS1details the full names of the abbreviations.</p>
<p>Figure 3 .
3
Figure 3. Illustration of structured prompting tactics used to direct the reasoning process for OSCoT generation within user message 1 (UM1) I: Establishing the persona of the expert expected to analyze the data; II: Providing detailed instructions for input data, including the format and specific observations to note; III: Emphasizing the importance of focusing on relevant details; IV: Dictating the desired output, its structure, and word limit.(Aand B) Decomposing the complex task into general, manageable tasks, and splitting and reinforcing them to ensure thorough analysis.(C) Putting the analyzed datapoints in the rear of the messages.In Python, the f-string format employs curly braces {} to insert the content within variables into the string.Here, we use angle brackets &lt;&gt; as separators for generated sentences or paragraphs.Additionally, the variable delimiter adopted here is ''##''.</p>
<p>Figure 4 .
4
Figure 4. Prediction accuracies for different ternary CeM 1 M 2 composites by GPT Assistants after analyzing only the corresponding binary counterparts (A) Run-specific accuracy results from five independent runs for each ternary case.(B) Statistical summaries of the corresponding results in (A), with bars indicating the average of the accuracy values from the five runs and error bars representing the standard deviation of these accuracy values.</p>
<p>iScience 27, 109451, April 19, 2024
iScience 27, 109451,April 19, 2024<br />
Data and code availabilityThe authors declare that data supporting the findings of this study are available if requested.All python codes used in this study are available at https://github.com/MUYU-LU/OSCoT.SUPPLEMENTAL INFORMATIONSupplemental information can be found online at https://doi.org/10.1016/j.isci.2024.109451.ACKNOWLEDGMENTSWe gratefully acknowledge funding from the National Natural Science Foundation of China (U20A20130DECLARATION OF INTERESTSThe authors declare no competing interests.Received: November 6, 2023 Revised: January 26, 2024STAR+METHODS KEY RESOURCES TABLE RESOURCE AVAILABILITYLead contactFurther information and requests for resources should be directed to and will be fulfilled by the lead contact, Linjiang Chen (l.j.chen@bham.ac.uk).Materials availabilityThis study did not generate new unique reagents.METHODS DETAILSThe chemistry context and a case study of NH 3 -SCR catalysis Specifically, we assess the effectiveness of prompting GPT-4 to analyze and interpret experimental tabular data pertaining to a clearly defined chemistry problem.This knowledge involves variables from experiments-such as conditions and parameters-and their corresponding outcomes, all derived from the existing body of literature.The structured experimental data, akin to a JSON format, characterised by keys with corresponding values in either text or numerical form, is rendered to GPT-4.GPT-4 is then prompted to interpret this data, predict the expected experimental outcome based on specified features sets, and explain its prediction.This is informed by our recent success in discovering Ce-based metal-oxide composites for selective catalytic reduction of nitrogen oxides with ammonia,22facilitated by interpretable machine learning (ML).In that investigation, utilizing SHapley Additive exPlanations (SHAP) methods to interpret the Extreme Gradient Boosting (XGB) ensemble model trained by 5654 samples detailed various aspects and corresponding NO x conversion collected from literature brings underscoring influence of variables like reaction temperature and metal elements.The feature importance of these features can be seen as orders of analysis on features, thus embodying deductive relationship in the reasonings paths.For this purpose, we curated a dataset from existing literature, comprising 1838 unique experiments on NH 3 -SCR using Ce-based metaloxide composites.This dataset encompasses experimental variables such as the catalyst composition (binary and ternary Ce-based metaloxide composites, CeM 1 and CeM 1 M 2 (M 1 , M 2 = metal element; M 1 s M 2 ), synthesis parameters, reaction conditions, and experimental NO x conversion outcomes.We adopted a threshold of 95% NO x conversion to categorize outcomes as ''Positive'' or ''Negative''.We designed to evolve variables in the tabular data within the dataset curated here in the reasoning chain to verify whether the GPT-4 can better interpret data and make predictions.In specific, we engaged GPT-4 with the tabular data using Chain-of-Thought (CoT) in an ordered and batchwise and atonce manner.The context length of memory of GPT-4 is much longer than human and therefore we evaluated CoT prompting strategies against the performances of human chemists in predicting for the same experiments.These were critically assessed against our domain expertise and the prevailing consensus in the subject field, offering a comprehensive evaluation of GPT-4's and the chemists' predictive and reasoning capabilities.Background knowledge of chain-of-thought (CoT) promptingLLMs are adept at conducting "zero-shot" learning, leveraging knowledge from their extensive training datasets to respond to queries without needing specific prior examples.However, these models often encounter challenges in complex tasks that require advanced reasoning and planning.To overcome these hurdles, various strategies such as "few-shot" prompting and other advanced techniques have been introduced to bolster their capabilities.23The "Chain of Thought" (CoT) strategy,5
Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Adv. Neural Inf. Process. Syst. 332020</p>
<p>GPT-4. Openai, 10.48550/arXiv.2303.087742023Preprint at arXiv</p>
<p>A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt. Y Cao, S Li, Y Liu, Z Yan, Y Dai, P S Yu, L Sun, 10.48550/arXiv.2303.042262023Preprint at arXiv</p>
<p>ChatGPT and Open-AI Models: A Preliminary Review. K I Roumeliotis, N D Tselikas, Future Internet. 151922023</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, 10.48550/arXiv.2201.119032022Preprint at arXiv</p>
<p>H Liu, R Ning, Z Teng, J Liu, Q Zhou, Y Zhang, 10.48550/arXiv.2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. Preprint at arXiv. 2023</p>
<p>M Fraiwan, N Khasawneh, 10.48550/arXiv.2305.00237A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions. 2023Preprint at arXiv</p>
<p>Improving mathematical reasoning with process supervision. Openai, 2023</p>
<p>. D M Katz, M J Bommarito, S Gao, P Arredondo, 2023Gpt-4 passes the bar exam. Available at SSRN 4389233</p>
<p>ChatGPT and environmental research. J.-J Zhu, J Jiang, M Yang, Z J Ren, Environ. Sci. Technol. 572023</p>
<p>Is GPT-4 a Good Data Analyst. L Cheng, X Li, Bing , L , 10.48550/arXiv.2305.150382023Preprint at arXiv</p>
<p>ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, 10.1021/jacs.3c058192023Preprint at arXiv</p>
<p>ChemCrow: Augmenting large-language models with chemistry tools. A M Bran, S Cox, A D White, P Schwaller, 10.48550/arXiv.2304.053762023Preprint at arXiv</p>
<p>Fine-Tuning GPT-3 for Machine Learning Electronic and Functional Properties of Organic Molecules. Z Xie, X Evangelopoulos, O ¨ Omar, A Troisi, A I Cooper, L Chen, 2023</p>
<p>A GPT-4 Reticular Chemist for Guiding MOF Discovery. Z Zheng, Z Rong, N Rampal, C Borgs, J T Chayes, O M Yaghi, Angew. Chem. Int. Ed. 622023. e202311983</p>
<p>ChatGPT Research Group for Optimizing the Crystallinity of MOFs and COFs. Z Zheng, O Zhang, H L Nguyen, N Rampal, A H Alawadhi, Z Rong, T Head-Gordon, C Borgs, J T Chayes, O M Yaghi, 10.1021/acscentsci.3c01087ACS Cent. Sci. 92023</p>
<p>Shaping the Water-Harvesting Behavior of Metal-Organic Frameworks Aided by Fine-Tuned GPT Models. Z Zheng, A H Alawadhi, S Chheda, S E Neumann, N Rampal, S Liu, H L Nguyen, Y.-H Lin, Z Rong, J I Siepmann, 10.1021/jacs.3c12086J. Am. Chem. Soc. 1452023</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 6242023</p>
<p>Large language models for chemistry robotics. N Yoshikawa, M Skreta, K Darvish, S Arellano-Rubach, Z Ji, L Bjørn Kristensen, A Z Li, Y Zhao, H Xu, A Kuramshin, Aut. Robots. 2023</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Ku ¨ttler, M Lewis, W.-T Yih, T Rockta ¨schel, Adv. Neural Inf. Process. Syst. 332020</p>
<p>Knowledge-Driven Experimental Discovery of Ce-Based Metal Oxide Composites for Selective Catalytic Reduction of NO x with NH 3 through Interpretable Machine Learning. M Lu, F Gao, Y Tan, H Yi, Y Gui, Y Xu, Y Wang, Y Zhou, X Tang, L Chen, 10.1021/acsami.3c18490ACS Appl. Mater. Interfaces. 162024</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Adv. Neural Inf. Process. Syst. 352022</p>
<p>Automatic chain of thought prompting in large language models. Z Zhang, A Zhang, M Li, A Smola, 10.48550/arXiv.2210.034932022Preprint at arXiv</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, 10.48550/arXiv.2203.111712022Preprint at arXiv</p>
<p>Least-tomost prompting enables complex reasoning in large language models. D Zhou, N Scha ¨rli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, 10.48550/arXiv.2205.106252022Preprint at arXiv</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. N Shinn, B Labash, A Gopinath, 10.48550/arXiv.2303.113662023Preprint at arXiv</p>
<p>Lost in the Middle: How Language Models Use Long Contexts. N F Liu, K Lin, J Hewitt, A Paranjape, M Bevilacqua, F Petroni, P Liang, 10.48550/arXiv.2307.031722023Preprint at arXiv</p>            </div>
        </div>

    </div>
</body>
</html>
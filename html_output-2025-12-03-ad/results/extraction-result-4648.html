<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4648 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4648</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4648</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-266690878</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.17515v1.pdf" target="_blank">Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game</a></p>
                <p><strong>Paper Abstract:</strong> Multi-agent collaboration with Large Language Models (LLMs) demonstrates proficiency in basic tasks, yet its efficiency in more complex scenarios remains unexplored. In gaming environments, these agents often face situations without established coordination protocols, requiring them to make intelligent inferences about teammates from limited data. This problem motivates the area of ad hoc teamwork, in which an agent may potentially cooperate with a variety of teammates to achieve a shared goal. Our study focuses on the ad hoc teamwork problem where the agent operates in an environment driven by natural language. Our findings reveal the potential of LLM agents in team collaboration, highlighting issues related to hallucinations in communication. To address this issue, we develop CodeAct, a general agent that equips LLM with enhanced memory and code-driven reasoning, enabling the repurposing of partial information for rapid adaptation to new teammates.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4648.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4648.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeAct (Code-driven Acting Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based ad hoc teamwork agent that augments retrieval-style memory with code-driven reasoning and a Python REPL for self-debugging to infer teammate roles and make team-selection decisions in the AvalonPlay benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CodeAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A general LLM agent architecture that (1) retrieves structured factual information from a windowed global memory and leader-specific memory, (2) generates code-like stepwise reasoning (Python) to reason about teammate roles and constraints, and (3) executes and self-debugges that code via a Python REPL to produce actions (team nominations, votes). Experiments use GPT-4 as the backend LLM for CodeAct.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented episodic/global memory + leader-specific factual memory (windowed)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Implements a global memory per agent that stores identified key information plus messages from the most recent k steps (k=15). In addition, for the leader during team-selection, a leader-specific memory module retrieves private information (role-tied hints) and public information (past round events/outcomes). Memory is consulted during team selection to supply factual constraints for code-based reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Ad Hoc Teamwork (team selection) in Avalon</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>AvalonPlay: a language-based multi-agent, multi-round benchmark based on The Resistance: Avalon. Each game has hidden roles and five quest rounds; in each round a rotating leader must nominate teammates, participate in discussion/voting, and the quest outcome depends on secret votes. The leader must infer teammate roles/intents from limited, potentially deceptive natural-language communication and past quest outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AvalonPlay</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Game Win = 0.633, Quest Win = 0.593, Team Acc = 0.830 (Table 3; CodeAct used memory retrieval + code-driven reasoning; numbers are fractions of wins over 30 games / 150 quests)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>CodeAct (which explicitly uses the memory retrieval system plus code-driven reasoning) outperforms semantic-only prompting baselines (CoT and ReAct). Reported comparison: Team Acc 0.830 (CodeAct) vs 0.707 (CoT) and 0.634 (ReAct). The paper does not present an ablation isolating memory alone (no numerical with-vs-without-memory ablation reported).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No direct quantitative ablation of memory-only impact; memory design uses a fixed window (k=15) which may limit long-term facts; agents still produce hallucinations and can forget early-game information despite memory; factuality verification is not implemented (memory can help but does not fully eliminate cascading hallucinations or inconsistencies).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Combining retrieval of factual prior interactions with structured, code-like intermediate reasoning and executable self-debug (Python REPL) improves leader role inference and team-selection accuracy in ad hoc teamwork. Memory retrieval helps surface early factual events and reduce forgetting, but should be paired with verification and robust handling of hallucinations; code-driven reasoning provides more reliable inference structure than purely semantic chain-of-thought or ReAct in this setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4648.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4648.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GlobalMemory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Global memory retrieval system (per-agent windowed memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A per-agent memory that stores key identified facts and the most recent k interaction messages to constrain LLM reasoning and avoid token over-extension during multi-turn multi-agent play.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Global memory (used by baseline and CodeAct agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A lightweight retrieval-oriented memory maintained for each LLM agent that records moderator notes, discussion contributions, private role-derived hints, and public historical outcomes. Implemented as an index of identified key facts plus the most recent k messages (k=15 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented windowed episodic memory (fixed-size window k=15)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Stores moderator notes, agents' discussion contributions, private and public information; when needed the agent retrieves factual items and last k messages to include in prompts to avoid exceeding token limits and to provide factual constraints for reasoning/decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Support multi-turn role-inference and decision-making in AvalonPlay</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides factual context (private/public information and recent interaction history) that agents use while performing team selection, discussion, and voting in the AvalonPlay ad hoc teamwork benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AvalonPlay</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Memory is described as necessary to prevent token over-extension and to reduce forgetting of early-game facts; however the paper does not present a controlled quantitative experiment isolating performance with vs without this global memory. The main empirical comparisons are between prompting/reasoning strategies, not memory present/absent.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fixed window size (k=15) may still omit early-but-important events; memory retrieval does not guarantee factual verification — hallucinations and contradictory statements still occur; no ablation isolating memory effect was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>A structured retrieval memory that records moderator-provided facts and conversation contributions is helpful to surface factual constraints to the LLM and mitigate forgetting; effective use of such memory likely requires complementary mechanisms for fact verification and longer-range storage beyond a fixed recent window.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4648.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4648.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (baseline agents)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-based LLM agents (baseline and as backend for CodeAct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is used as the backend LLM for agents in AvalonPlay; baseline GPT-4 agents and GPT-4-powered CodeAct are compared showing better adaptation and reduced hallucination relative to GPT-3.5 and Llama2 in multi-turn ad hoc teamwork.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 agent (baseline and experimental variants)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents instantiated with GPT-4 (gpt-4-1106-preview) used across roles; baselines use semantic prompting variants (Base, Base+CoT, Base+ReAct) and CodeAct leverages GPT-4 plus memory+code-driven reasoning. All agents employ the global memory retrieval (k=15).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>uses the per-agent global memory retrieval (windowed k=15); leader-specific memory used when applicable</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>GPT-4 agents consult the global memory (most recent k=15 steps and identified key facts). The leader role can additionally retrieve leader-specific private/public factual items during team selection. Memory is used to avoid token over-extension and to provide factual constraints to reasoning/prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Self-play and evaluation on AvalonPlay (AHT)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate agents' ad hoc teamwork capabilities across 30 games (150 quests) per experimental condition. Metrics: Game Win (good side wins game), Quest Win (per-quest success rate), Team Selection Accuracy (leader picks team that results in successful quest).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>AvalonPlay</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 2 (GPT-4 Base with communication): Game Win = 0.500, Quest Win = 0.513, Team Acc = 0.640. Table 2 (Base without communication): Game Win = 0.466, Quest Win = 0.480, Team Acc = 0.693. Table 3 (prompting variants on GPT-4): Base+CoT -> Game Win 0.533, Quest Win 0.547, Team Acc 0.707; Base+ReAct -> Game Win 0.500, Quest Win 0.526, Team Acc 0.634. These GPT-4 runs used the paper's global memory (k=15) and role prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Paper reports that GPT-4 agents outperform GPT-3.5 and Llama2 and that GPT-4 shows improvement across rounds (better adaptation). However, no explicit numerical comparison is given for GPT-4 with vs without the global memory retrieval module. Performance comparisons are primarily across LLM sizes and prompting/reasoning strategies rather than memory ON/OFF.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Even GPT-4 agents produce hallucinations and can forget early-game information; communication channels did not reliably improve performance since agents may generate inconsistent or factually incorrect statements; memory helps but does not resolve cascading hallucinations or factual inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Stronger LLM backends (GPT-4) show better capacity to learn from observations across rounds; coupling such models with structured memory retrieval and code-driven reasoning (as in CodeAct) yields the best reported team-selection accuracy, but further work on verification and longer-term memory is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Augmenting language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Teaching large language models to self-debug <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4648",
    "paper_id": "paper-266690878",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "CodeAct",
            "name_full": "CodeAct (Code-driven Acting Agent)",
            "brief_description": "An LLM-based ad hoc teamwork agent that augments retrieval-style memory with code-driven reasoning and a Python REPL for self-debugging to infer teammate roles and make team-selection decisions in the AvalonPlay benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CodeAct",
            "agent_description": "A general LLM agent architecture that (1) retrieves structured factual information from a windowed global memory and leader-specific memory, (2) generates code-like stepwise reasoning (Python) to reason about teammate roles and constraints, and (3) executes and self-debugges that code via a Python REPL to produce actions (team nominations, votes). Experiments use GPT-4 as the backend LLM for CodeAct.",
            "memory_type": "retrieval-augmented episodic/global memory + leader-specific factual memory (windowed)",
            "memory_description": "Implements a global memory per agent that stores identified key information plus messages from the most recent k steps (k=15). In addition, for the leader during team-selection, a leader-specific memory module retrieves private information (role-tied hints) and public information (past round events/outcomes). Memory is consulted during team selection to supply factual constraints for code-based reasoning.",
            "task_name": "Ad Hoc Teamwork (team selection) in Avalon",
            "task_description": "AvalonPlay: a language-based multi-agent, multi-round benchmark based on The Resistance: Avalon. Each game has hidden roles and five quest rounds; in each round a rotating leader must nominate teammates, participate in discussion/voting, and the quest outcome depends on secret votes. The leader must infer teammate roles/intents from limited, potentially deceptive natural-language communication and past quest outcomes.",
            "benchmark_name": "AvalonPlay",
            "performance_with_memory": "Game Win = 0.633, Quest Win = 0.593, Team Acc = 0.830 (Table 3; CodeAct used memory retrieval + code-driven reasoning; numbers are fractions of wins over 30 games / 150 quests)",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "CodeAct (which explicitly uses the memory retrieval system plus code-driven reasoning) outperforms semantic-only prompting baselines (CoT and ReAct). Reported comparison: Team Acc 0.830 (CodeAct) vs 0.707 (CoT) and 0.634 (ReAct). The paper does not present an ablation isolating memory alone (no numerical with-vs-without-memory ablation reported).",
            "limitations_or_challenges": "No direct quantitative ablation of memory-only impact; memory design uses a fixed window (k=15) which may limit long-term facts; agents still produce hallucinations and can forget early-game information despite memory; factuality verification is not implemented (memory can help but does not fully eliminate cascading hallucinations or inconsistencies).",
            "key_insights": "Combining retrieval of factual prior interactions with structured, code-like intermediate reasoning and executable self-debug (Python REPL) improves leader role inference and team-selection accuracy in ad hoc teamwork. Memory retrieval helps surface early factual events and reduce forgetting, but should be paired with verification and robust handling of hallucinations; code-driven reasoning provides more reliable inference structure than purely semantic chain-of-thought or ReAct in this setting.",
            "uuid": "e4648.0",
            "source_info": {
                "paper_title": "Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "GlobalMemory",
            "name_full": "Global memory retrieval system (per-agent windowed memory)",
            "brief_description": "A per-agent memory that stores key identified facts and the most recent k interaction messages to constrain LLM reasoning and avoid token over-extension during multi-turn multi-agent play.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Global memory (used by baseline and CodeAct agents)",
            "agent_description": "A lightweight retrieval-oriented memory maintained for each LLM agent that records moderator notes, discussion contributions, private role-derived hints, and public historical outcomes. Implemented as an index of identified key facts plus the most recent k messages (k=15 in experiments).",
            "memory_type": "retrieval-augmented windowed episodic memory (fixed-size window k=15)",
            "memory_description": "Stores moderator notes, agents' discussion contributions, private and public information; when needed the agent retrieves factual items and last k messages to include in prompts to avoid exceeding token limits and to provide factual constraints for reasoning/decision-making.",
            "task_name": "Support multi-turn role-inference and decision-making in AvalonPlay",
            "task_description": "Provides factual context (private/public information and recent interaction history) that agents use while performing team selection, discussion, and voting in the AvalonPlay ad hoc teamwork benchmark.",
            "benchmark_name": "AvalonPlay",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Memory is described as necessary to prevent token over-extension and to reduce forgetting of early-game facts; however the paper does not present a controlled quantitative experiment isolating performance with vs without this global memory. The main empirical comparisons are between prompting/reasoning strategies, not memory present/absent.",
            "limitations_or_challenges": "Fixed window size (k=15) may still omit early-but-important events; memory retrieval does not guarantee factual verification — hallucinations and contradictory statements still occur; no ablation isolating memory effect was reported.",
            "key_insights": "A structured retrieval memory that records moderator-provided facts and conversation contributions is helpful to surface factual constraints to the LLM and mitigate forgetting; effective use of such memory likely requires complementary mechanisms for fact verification and longer-range storage beyond a fixed recent window.",
            "uuid": "e4648.1",
            "source_info": {
                "paper_title": "Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "GPT-4 (baseline agents)",
            "name_full": "GPT-4-based LLM agents (baseline and as backend for CodeAct)",
            "brief_description": "GPT-4 is used as the backend LLM for agents in AvalonPlay; baseline GPT-4 agents and GPT-4-powered CodeAct are compared showing better adaptation and reduced hallucination relative to GPT-3.5 and Llama2 in multi-turn ad hoc teamwork.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-4 agent (baseline and experimental variants)",
            "agent_description": "Agents instantiated with GPT-4 (gpt-4-1106-preview) used across roles; baselines use semantic prompting variants (Base, Base+CoT, Base+ReAct) and CodeAct leverages GPT-4 plus memory+code-driven reasoning. All agents employ the global memory retrieval (k=15).",
            "memory_type": "uses the per-agent global memory retrieval (windowed k=15); leader-specific memory used when applicable",
            "memory_description": "GPT-4 agents consult the global memory (most recent k=15 steps and identified key facts). The leader role can additionally retrieve leader-specific private/public factual items during team selection. Memory is used to avoid token over-extension and to provide factual constraints to reasoning/prompting.",
            "task_name": "Self-play and evaluation on AvalonPlay (AHT)",
            "task_description": "Evaluate agents' ad hoc teamwork capabilities across 30 games (150 quests) per experimental condition. Metrics: Game Win (good side wins game), Quest Win (per-quest success rate), Team Selection Accuracy (leader picks team that results in successful quest).",
            "benchmark_name": "AvalonPlay",
            "performance_with_memory": "Table 2 (GPT-4 Base with communication): Game Win = 0.500, Quest Win = 0.513, Team Acc = 0.640. Table 2 (Base without communication): Game Win = 0.466, Quest Win = 0.480, Team Acc = 0.693. Table 3 (prompting variants on GPT-4): Base+CoT -&gt; Game Win 0.533, Quest Win 0.547, Team Acc 0.707; Base+ReAct -&gt; Game Win 0.500, Quest Win 0.526, Team Acc 0.634. These GPT-4 runs used the paper's global memory (k=15) and role prompts.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Paper reports that GPT-4 agents outperform GPT-3.5 and Llama2 and that GPT-4 shows improvement across rounds (better adaptation). However, no explicit numerical comparison is given for GPT-4 with vs without the global memory retrieval module. Performance comparisons are primarily across LLM sizes and prompting/reasoning strategies rather than memory ON/OFF.",
            "limitations_or_challenges": "Even GPT-4 agents produce hallucinations and can forget early-game information; communication channels did not reliably improve performance since agents may generate inconsistent or factually incorrect statements; memory helps but does not resolve cascading hallucinations or factual inconsistency.",
            "key_insights": "Stronger LLM backends (GPT-4) show better capacity to learn from observations across rounds; coupling such models with structured memory retrieval and code-driven reasoning (as in CodeAct) yields the best reported team-selection accuracy, but further work on verification and longer-term memory is needed.",
            "uuid": "e4648.2",
            "source_info": {
                "paper_title": "Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Augmenting language models with long-term memory",
            "rating": 2,
            "sanitized_title": "augmenting_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2,
            "sanitized_title": "reflexion_an_autonomous_agent_with_dynamic_memory_and_selfreflection"
        },
        {
            "paper_title": "Teaching large language models to self-debug",
            "rating": 1,
            "sanitized_title": "teaching_large_language_models_to_selfdebug"
        }
    ],
    "cost": 0.011857,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game
29 Dec 2023</p>
<p>Zijing Shi zijing.shi@student.uts.edu.au 
AAII
University of Technology Sydney
Australia</p>
<p>Meng Fang meng.fang@liverpool.ac.uk 
University of Liverpool
United Kingdom</p>
<p>Shunfeng Zheng shunfeng.zheng@student.uts.edu.au 
AAII
University of Technology Sydney
Australia</p>
<p>Shilong Deng shilong.deng@liverpool.ac.uk 
University of Liverpool
United Kingdom</p>
<p>Ling Chen ling.chen@uts.edu.au 
AAII
University of Technology Sydney
Australia</p>
<p>Yali Du yali.du@kcl.ac.uk 
King's College London
United Kingdom</p>
<p>Cooperation on the Fly: Exploring Language Agents for Ad Hoc Teamwork in the Avalon Game
29 Dec 2023A8972C1017439852606C3665DF13FCADarXiv:2312.17515v1[cs.CL]
Multi-agent collaboration with Large Language Models (LLMs) demonstrates proficiency in basic tasks, yet its efficiency in more complex scenarios remains unexplored.In gaming environments, these agents often face situations without established coordination protocols, requiring them to make intelligent inferences about teammates from limited data.This problem motivates the area of ad hoc teamwork, in which an agent may potentially cooperate with a variety of teammates to achieve a shared goal.Our study focuses on the ad hoc teamwork problem where the agent operates in an environment driven by natural language.Our findings reveal the potential of LLM agents in team collaboration, highlighting issues related to hallucinations in communication.To address this issue, we develop CodeAct, a general agent that equips LLM with enhanced memory and code-driven reasoning, enabling the repurposing of partial information for rapid adaptation to new teammates.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have exhibited impressive abilities in reasoning and generalization, showing their potential in building autonomous agents and driving many advancements (Kojima et al., 2022;Xi et al., 2023).Recent research involving multiple LLM agents working together has shown success in task execution (Wang et al., 2023c;Wu et al., 2023;Dong et al., 2023).However, these studies often employ explicit coordination protocols for fully cooperative tasks, assuming full information of the teammate and the task.While some research has explored simulating LLM agents in more open-ended settings, these agents remain limited by pre-established strategies and behaviors (Park et al., 2023).</p>
<p>In dynamic environments, maintaining consistent coordination protocols among agents is a challenge.Such contexts demand an agent capable of swift adaptation and on-the-fly cooperation.Without pre-established team strategies, an agent, often referred to as the learner, must observe its teammates' behaviors and adapt effectively to achieve their shared goals.This challenge is known as the Ad Hoc Teamwork (AHT) problem (Stone et al., 2010;Macke et al., 2021).</p>
<p>A significant challenge for the learner is to infer the role of their teammates.Traditional methods rely on pre-programmed rules or data-driven algorithms trained on historical data (Mirsky et al., 2022).In contrast, LLM agents can directly communicate with their teammates in natural language.However, the communication dynamics within the environment may exhibit conflicting and competitive elements.Moreover, due to the inherent traits of LLMs, teammates may provide inaccurate or misleading information.</p>
<p>In this study, we explore the AHT abilities of LLM agents, utilizing the multi-agent game The Resistance: Avalon to simulate AHT environments.We introduce the AvalonPlay benchmark, which challenges agents with hidden roles to participate in strategic gameplay across multiple rounds.In each round, a learner is tasked with selecting teammates without prior strategies and full information.Our research integrates LLM agents into AvalonPlay to evaluate their performance in AHT.Rather than developing coordination protocols for the entire team, we prioritize designing agents adept at cooperating in environments without such explicit protocols.</p>
<p>A communication channel is designed within the AvalonPlay benchmark to enhance collaboration.We observe that LLM agents may sometimes produce hallucinations and forget early information.</p>
<p>To address this, we introduce CodeAct, a general agent leveraging the advanced reasoning capabilities of LLMs, focusing on reasoning about various potential teammate types.We begin with a memory retrieval system that collects information and utilize the knowledge gained from these interactions.Subsequently, we incorporate code-driven reasoning with action to effectively refer the roles of new teammates.With the teammate role information, our agent is well-equipped to effectively cooperate with its teammates.</p>
<p>In summary, our contributions are as follows:</p>
<p>• We explore the AHT capabilities of LLM agents, utilizing the AvalonPlay benchmark with its multi-round collaborative tasks as a testing ground for these abilities.</p>
<p>• We develop CodeAct, an LLM agent utilizing a code-driven prompting approach, aimed at boosting the agent's efficiency in AHT tasks.</p>
<p>• We conduct comprehensive experiments within the AvalonPlay benchmark, demonstrating the effectiveness of our proposed agent.</p>
<p>The AvalonPlay Benchmark</p>
<p>We introduce the AvalonPlay benchmark to explore the performance of LLM agents on AHT.This benchmark is a language-based, multi-agent platform comprised of multi-round tasks.During the gameplay, agents have limited knowledge about each other's roles.In each round, a team leader, also regarded as the learner, is selected to deduce the roles and intentions of other agents.The leader must adapt their team's strategies dynamically as the game evolves.This section details the key elements of the proposed benchmark.</p>
<p>Teammate Role</p>
<p>We utilize a setup involving seven players indexed by j, each controlled by an LLM agent.In each game, agents are randomly assigned one of six distinct roles, dividing them into two factions: the just side (including Merlin, Percival, and two Loyal Servants of Arthur) and the evil side (including Morgana, Minion, and Assassin).</p>
<p>Each role possesses unique abilities, as shown on the left of Figure 1.On the just side, Merlin is aware of the factions but not the specific roles, while Percival can recognize Merlin and Morgana but is unable to distinguish between them.The Loyal Servants of Arthur lack any special knowledge.On the evil side, in contrast, all roles can recognize their teammates.This setup puts the just side at a disadvantage due to their limited information.Our study focuses on scenarios where the just side takes the lead role, aiming to enhance the leader's ability to deduce the various roles of agents.</p>
<p>Pipeline</p>
<p>In AvalonPlay, a single game is composed of five quest rounds indexed by i.Each round, defined as a full quest cycle, is divided into the following four phases, as shown in the right of Figure 1.</p>
<p>• Leader Assignment: Each round begins with the selection of a leader who nominates teammates for the quest.The first round's leader is randomly chosen, and in subsequent rounds, Figure 2: An overview of the proposed CodeAct agent as the leader during team selection.We begins by establishing a memory retrieval system that distills information from past interactions, enabling the agent to access relevant information.Then, we integrate code-driven reasoning with action to determine teammate roles effectively.Finally, we employ an interpreter to execute the generated code, equipping the agent with self-debug capabilities.</p>
<p>leader rotates sequentially to the next player.This approach ensures diverse leader roles while incorporating randomness.</p>
<p>• Team Selection: In this phase, a leader nominates n i teammates, with the team size n i varying per round.Adhering to the game rule, the team sizes are:
n 1 = 2, n 2 = 3, n 3 = 3, n 4 = 4, n 5 = 4.
We align the leader's response into corresponding actions.For instance, the response "I choose myself and player 2 for the quest team" is mapped to the specific selection of teammates.</p>
<p>• Discussion and Voting: We design a communication protocol that enables the agent to acquire information while accounting for the potential of misinformation.Post team selection, agents participate in discussions to assess the proposed team.Following the discussion, each agent secretly votes 1 j indicates agent j's approval of the current nominated team.To enhance agents' decision-making, they are required to generate a JSON containing their reasoning and vote.A team is approved if the total votes reaches a majority (i.e., 7 j=1 1 j ≥ 4).Otherwise, the team selection is repeated with a new leader.A team is automatically formed if approval is not achieved after five consecutive attempts.</p>
<p>• Quest Execution: For an approved team, teammates from the just side must vote to approve, while teammates from the evil side must vote against.Let Q i denote the total votes of evil members.The quest is sabotaged if
Q i ≥ 1 for rounds i ∈ {1, 2, 3}, or Q i ≥ 2 for rounds i ∈ {4, 5}.
In other cases, it is completed successfully.</p>
<p>We adhere to game rule by including an assassination phase in the Avalonplay benchmark, where the Assassin identify Merlin after all rounds.However, our study aims to assess the teamwork of LLM agents, not the accuracy of assassinations, though this feature is included for prospective future research.</p>
<p>Observation Understanding</p>
<p>The AvalonPlay benchmark includes a moderator responsible for overseeing the game's progress.This involves providing action descriptions to agents and recording events that occur during gameplay.The moderator's role is non-interactive with the LLMs but adheres to predefined procedures.The information generated by the moderator is a vital part of the agents' observation.</p>
<p>We develop structured prompts to assist LLM agents in comprehending both the game's rules and its current state.In general, an LLM agent's observation is divided into four key variables to navigate the game:</p>
<p>• Game Rule: We begin with a basic understanding of the game (e.g., "You are playing the Avalon game with other players...").</p>
<p>• Role Assignment: For each player, we randomly assign their roles and create a unique role assignment prompt (e.g., Merlin is informed: "You are Merlin...Your objective is to ensure the success of the quests as much as possible").</p>
<p>• Role Identification: Roles with special abilities get extra information (e.g., Merlin receives a moderator's note: "Players 1, 2, and 6 belong to the evil side").</p>
<p>• Game History: This includes the game trajectory from previous rounds, as well as earlier time steps of the current round.</p>
<p>• Action Description: This includes a description of the next possible actions for the agent, provided by the moderator (e.g., Player 2 receives a moderator's note: "Now it's your turn, player 2. Please discuss whether to agree with the nominated team").</p>
<p>The game rule, role assignment, and role identification collectively constitute the global prompt, consistently employed at each time step.Game history and action description serve as real-time prompt, effectively capturing the game's present state.For additional details, please refer to Appendix A.</p>
<p>Methodology</p>
<p>Overview</p>
<p>Reasoning about various potential teammate types is a key issue in AHT research.To improve the agent's reasoning abilities, we introduce the Code-Act agent, which utilizes the code-based reasoning capabilities of LLMs.Our CodeAct agent does not focus on creating prompts or demonstrations for specific tasks.Instead, it emphasizes a general strategy that transforms complex semantic tasks into flexible code structures.</p>
<p>Our approach unfolds in three stages, as shown in Figure 2. Firstly, we establish a memory retrieval system to extract information from previous interactions.Then, we integrate the code-driven reasoning with action to reason teammate roles.Lastly, we employ an interpreter to execute the code and equip the agent with the self-debug capability.</p>
<p>Memory Retrieval</p>
<p>During game playing, we implement global memory for each LLM agent to prevent the overextension of token inputs.Particularly during the team selection phase, the team leader is equipped with a customized memory module, strategically designed to aid in deducing the roles of its teammates.</p>
<p>Global Memory</p>
<p>The global memory for each agent comprises identified key information along with messages from the most recent k steps.This key information includes: 1) Notes from the game moderator; 2) Contributions from agents during discussions, which reveal details of personal identities or deduce the identities of others.</p>
<p>Leader-Specific Memory LLM agents often base their reasoning on the latest information and frequently experience hallucinations, as detailed in Section 4.5.In response, for each game round's team selection stage, we systematically retrieve factual data gathered from earlier interactions to aid the current leader.This includes: 1) Private Information, derived from unique skills tied to an agent's role, like Percival's insight: "Among players 1 and 4, one is good and the other evil, but you can't discern who is who." 2) Public Information, pertaining to historical game outcomes and events, accessible to all agents, such as: "In the initial round, players 1 and 2 were selected for the quest, which ended in failure."</p>
<p>Code-driven Reasoning with Action</p>
<p>In the team selection phase, we utilize a customized memory system, allowing the current leader to deduce the roles of its teammates.Drawing inspiration from ReAct (Yao et al., 2022), our agent is designed for interactive generation of both thoughts and actions through a program in a zero-shot manner.We utilize a program that processes useful information by reasoning over the agent's current private and public data to support future actions.As illustrated in Figure 3, we further enhance this process by enabling the leader to generate reasoning substeps in a code-like format.In this study, we focus on Python, but our approach is applicable  to any programming language.Figure 4 provide an excerpt generated by our CodeAct, with more details available in Appendix B.</p>
<p>Our code-driven approach, on the one hand, leverages advantages from semantic reasoning that involve intermediate steps.On the other hand, it leverages code-based reasoning, enabling reliable inference through the formal yet expressive code structure and the powerful computational tools it offers.</p>
<p>Code Execution with Self-Debug</p>
<p>We equip the leader with the reflection mechanism (Shinn et al., 2023;Chen et al., 2023b), allowing them to refine their programs.The code-based action, once formulated, is then processed by a code interpreter.In this study, we equip the agent with a Read-Eval-Print-Loop Python interpreter (i.e., Python_REPL).If the code executes successfully, the agent's decision is informed by the re-sulting program output.Conversely, if execution falters, the agent will revise the code, taking into account the issue encountered and previous outputs to enhance its generation.</p>
<p>Experiments</p>
<p>Our experiments aim to explore the following questions: 1) The AHT capabilities of agents employing different LLMs as backend models; 2) The effect of a natural language-based communication protocol on the AHT capabilities of LLM agents; 3) The impact of diverse prompting strategies on the AHT capabilities of LLM agents.We compare the proposed CodeAct agent with other semantic-based reasoning approaches.</p>
<p>Baselines</p>
<p>To tackle question 1, we conduct a preliminary evaluation of three advanced LLMs within the multiagent AvalonPlay benchmark.These models include GPT-3.5, GPT-4, and LLaMA 2. Specifically, we utilize the versions gpt-3.5-turbo-0613,gpt-4-1106-preview, and Llama2-chat-13B respectively.These LLM agents, equipped with different backend LLMs, are deployed on both the good and evil sides to compete against one another.</p>
<p>To address question 2, we compare scenarios in AvalonPlay both with and without communication protocols.In scenarios lacking communication protocols, all players proceed to a secret vote immediately following the team leader's selection of the team.</p>
<p>To tackle question 3, we focus on the team selection by the leader, compare semantic reasoning approaches (i.e., Chain of Thought (Wei et al., 2022) and ReAct (Yao et al., 2022)) with our proposed CodeAct agent.</p>
<p>Metrics</p>
<p>We design the following three metrics to measure an LLM agent's performance in the AvalonPlay benchmark.</p>
<p>• Game Win Rate by Quest (Game Win), which refers to the percentage of games where the good side wins by completing three out of five quests.</p>
<p>• Total Quest Win Rate (Quest Win), which refers to the percentage of quests where the good side wins.</p>
<p>• Team Selection Accuracy (Team Acc), which refers to the percentage of times the just side,  when in the role of leader, selects the team for a successful quest.</p>
<p>Implementation Details</p>
<p>For all LLM agent, we set the sampling temperature as 0.7 for text generation and 0 for code generation.We set the window size k to be 15 for the global memory retrieval.During gameplay, all roles are randomly assigned to each player to avoid biasing the data towards certain players based on the turn order.</p>
<p>Main Results</p>
<p>Question 1: Model Performance.Figure 5 displays the performance of agents with three different backend LLMs, playing on different sides.Figure 6 shows the quest success rates of these LLM agents as the good side across different rounds.Our findings indicate that 1) More advanced LLMs achieve higher win rates both in games and in overall tasks when playing as the good side, with GPT-4 agents exhibiting the best performance.2) Notably, GPT-4 agents demonstrate higher quest success rates in later rounds compared to earlier ones, indicating their ability to reason from observations.In contrast, GPT-3.5 and LLaMA agents do not show significant progress across rounds.This suggests that the GPT-4 agent is more adept at learning and adapting, leading us to utilize the GPT-4 model in subsequent experiments.</p>
<p>Question 2: Communication Channel.Table 2 compares the GPT-4 agent's performance in selfplaying, both with and without communication.</p>
<p>The introduction of communication does not significantly enhance the game performance of LLM agents.This is possibly due to the fact that, besides the communication protocol, LLM agents also have access to observe and reason based on the outcomes of quests in previous rounds.These findings indicate that communication does not always lead to improved collaborative efficiency in LLM agents.We further explore this phenomenon in Section 4.5.</p>
<p>Question 3: Code-driven Reasoning.We campare the proposed CodeAct with two semantic reasoning methods: Chain of Thought (CoT) and Re-Act.Table 3 shows that our proposed CodeAgent achieves the best performance across three metrics, with a team selection accuracy of 0.830, demonstrating the superiority of code-based reasoning.</p>
<p>Quantitative Analysis</p>
<p>In our preliminary analysis of the experimental logs targeting question 1, we observe that employ-</p>
<p>The 5th Round Team Selection and Discussion Moderator: Player 3, as the team leader, please select 4 team members to undertake the quest.You may also choose yourself.Player 3: I choose player 2, player 3, player 6, and player 7 for the quest.(Team selection by the current leader) Moderator: The selected team by the leader include ['player 2', 'player 3', 'player 6', 'player 7'].Now everyone discuss if you agree the team.Player 1:</p>
<p>This team includes players who have been part of successful quests before.It's a logical choice to maintain a strong team dynamic.I agree with the team selection.Player 2:</p>
<p>Player 3, player 6, and player 7 were on the last successful quest with me, and I believe this consistency will help us ensure another victory.I'm in favor of this team.(In the previous round, players 2, 3, 6, and 7 executed the quest together and succeeded.Player 2 effectively used this information for reasoning) Player 3: I've chosen players who have either been part of successful quests or have not shown any suspicious behavior so far.It's important to maintain a strong team dynamic, and I believe this selection will lead us to victory.Player 4:</p>
<p>The team seems solid with previous successful quest members.Including player 6 could either solidify their trustworthiness or provide us with more information.I agree to proceed.Player 5:</p>
<p>The inclusion of player 6, who hasn't been on a quest recently, along with proven successful quest members, provides a chance to validate their loyalty.We should be wary, but I am inclined to give this team a chance.(Player 5, however, produced hallucination, with agent's statement being inconsistent with the facts shared by everyone) Player 6:</p>
<p>The team includes players who have consistently been part of successful quests, and as a trusted member myself, I feel confident we can achieve success once more.I agree with this selection and am prepared to contribute to the quest's success.Player 7:</p>
<p>The team selected by player 3 includes myself and other players who have been part of successful quests, which instills confidence in this group's potential for success.Player 6's inclusion is a variable, but it is essential to evaluate all players.Based on the track record of success we've had, I agree with this team selection and am optimistic about our prospects.Table 1: An example of the GPT-4 agent's participation in discussions during gameplay.In the fifth round, the agent's response is based on previous observations and demonstrates some reasoning ability.However, the agent utilizes only limited information and is prone to experiencing hallucinations.ing LLM agents in multi-turn tasks often results in memory forgetting.In cooperative tasks involving communication, these agents tend to exhibit cascading hallucinations and logical inconsistencies.We present Table 1 for a more detailed illustration.</p>
<p>Forgetting Early Information.Players typically focus on information from the last round, failing to integrate key information from the early stages of the game into their later decisions and strategies.</p>
<p>Hallucination Generation.In the game, statements that contradict a player's actual role are not classified as hallucinations, as deception is a common strategy.Similarly, fabricating scenarios to falsely accuse others is also not considered hallucinations, as this mirrors the strategies of human players.We define inconsistencies within a response and counterfactual content in an iteration as hallucinations.When GPT-3.5 and LLaMa 2 models serve as agents, they tend to produce responses inconsistent with the prompt.A notable example is their role as team leaders, where they often select a number of teammates that diverges from the required amount.Additionally, the final votes they generate may not correspond with the reasoning process outlined in the previous step.This problem is less pronounced with the GPT-4 agent during gameplay.However, all models, including GPT-4, are prone to generating responses that contradict established facts, such as modifying or denying events from previous game rounds.Table 1 provides an example of GPT-4 agent's participation in discussions.Player 5's statements are inconsistent with the facts.As player 2 points out, playe 6 had already successfully participated in the quest in the previous round.</p>
<p>This suggests that the communication protocol does not necessarily offer optimal solutions.The most important aspect is the effective verification of the factuality and correctness of communications.This distinguishes it from prior AHT research with agents trained from historical data (Mirsky et al., 2020).In our study, we address this by utilizing memory retrieval to extract factual information.Additionally, we enhance the agent's reasoning abilities for different types of teammates by integrating code-based reasoning with actions.</p>
<p>Related Work</p>
<p>LLMs and Agents.The advent of generative agents for interactive tasks marks a significant paradigm shift.Unlike traditional methods that relied on Deep Reinforcement Learning (DRL) (Shi et al., 2022(Shi et al., , 2023;;Xu et al., 2020Xu et al., , 2021Xu et al., , 2022)), which confined agents to constrained settings, the emergence of LLMs has notably broadened the scope of possibilities (OpenAI, 2023).Efforts are increasingly focused on leveraging LLMs for complex tasks like planning and reasoning (Wang et al., 2023c;Wu et al., 2023;Dong et al., 2023).Innovative prompting methods, including ReACT (Yao et al., 2022), Chain of Thought (CoT) (Wei et al., 2022), and Tree of Thought (ToT) (Yao et al., 2023), significantly enhance the planning and reasoning capabilities of LLMs.Some research integrates complex human cognitive functions into LLM agents, such as memory (Wang et al., 2023b), reflection (Shinn et al., 2023), and tool usage (Huang et al., 2023).</p>
<p>Multi-agent Interaction.Recent research has explored that utilizing the interactions among multiple LLM agents to achieve several goals (Chen et al., 2023a;Wu et al., 2023).These studies focus on two main areas.First, it involves assigning distinct roles to agents to enhance their task performance (Wang et al., 2023a).Notable applications include using multiple agents in debates to counter self-reflection biases (Liang et al., 2023;Chan et al., 2023), and diversifying agent roles for complex projects like software development (Qian et al., 2023;Hong et al., 2023).Second, researchers develop multiple LLM agents in sandbox environments to mimic real human behavior (Park et al., 2023;Zhou et al., 2023).Some of these investigate the capabilities that emerge in multi-agent settings, such as deception and leadership (Xu et al., 2023).</p>
<p>Ad Hoc Teamwork.Ad Hoc teamwork aims to generalise the agent's cooperation capacity with novel agents (Stone et al., 2010;Du et al., 2023) The progress in AHT has shifted from relying on preset action-selection rules to a data-driven strategy.This strategy employs methods like probabilistic models (Rahman et al., 2023), deep learning (Chen et al., 2020), or reinforcement learning (Rahman et al., 2021;Lou et al., 2023;Yan et al., 2023) to predict the behaviors of various agents in different states, aiming to optimize and refine the learner's behavior.Recent research has also explored the communication in AHT, considering scenarios where teammates follow a shared communication protocol (Macke et al., 2021;Mirsky et al., 2020).However, these studies are often confined to a limited multi-agent setting.Our work stands apart from previous studies by focusing on dynamic environments based on natural language communication.We employ agents powered by LLMs to execute AHT, and explore ways to improve their performance in these settings.</p>
<p>Conclusion and Future Work</p>
<p>In this study, we investigate the AHT capabilities of LLM agents using the AvalonPlay benchmark.LLMs endow agents with powerful reasoning abilities, but challenges such as early memory forgetting and hallucination generation also arise.We address these challenges by proposing the CodeAct agent.We first employ a memory retrieval system to collect factual information from prior interactions.Subsequently, we integrate a code-driven reasoning with action to discern new teammate roles, enhancing our agent's ability to cooperate more effectively.Lastly, we employ an interpreter to execute the code and equip the agent with the self-debug capability.</p>
<p>Despite our initial research, there are some unresolved issues in the AHT research with language agents that require further investigation.For intance, our AvalonPlay benchmark involves moderator-guided communication for agents, making it crucial to investigate the agent's ability to autonomously decide when to communicate.Furthermore, significant work lies in reducing the effects of hallucinations and enhancing their practical application in real-world scenarios.In this study, we employ a memory retrieval system for factual information extraction, and another promising research direction involves developing systems for evaluating and verifying facts to address these challenges.</p>
<p>Limitations</p>
<p>Regarding limitations, our study does not incorporate experience pools from human players.Future research will focus on developing robust strategies for leveraging this experience and enhancing comparison with human performance.</p>
<p>A Global Prompt for Agents</p>
<p>Here, we show the global prompt used for the LLM agent in the AvalonPlay benchmark.We provide the game rule and role assignment as the global prompt.Figure 7 displays the specific game rule for all agents, while Table 4 shows the role assignments for different agents.</p>
<p>You are playing the Avalon game with some other players .The moderator is also host , he organised this game and you need to follow his instructions correctly .Do not talk with the moderator .What the modeler said is unquestionable and absolutely true .There are six roles in the game .Merlin , Percival , and Loyal Servants of Arthur form the just side , with members only knowing their own role and voting to approve in secret ballots to make quests successful .Morgana , Assassin , and Minion belong to the evil side .They are privy to their teammates ' identities and vote against in secret ballots to make the quests fail .Tips : If you think a player is evil side , try not to let them be selected into the quest team .</p>
<p>B An Example of Code-driven Reasoning Generation</p>
<p>In this section, we provide a complete example of code-driven reasoning with action generation produced by the GPT-4 model, as shown in Figure 8.</p>
<p>Thought Output : Let ' s start by creating a list of 7 elements to represent the players , with None as the initial value since we don ' t know their alignment yet .We will then apply the constraints from each round to update this list .</p>
<p>-From Round 2, we know that Players 2, 4, and 7 are good .</p>
<p>-From Round 1, we know that either Player 1 or Player 3 is bad , but not both .</p>
<p>-From Round 3, we know that there is at least one bad person among Player 1, Player 2, and Player 4. Since Players 2 and 4 are good , Player 1 must be bad .-From Round 4, we know that there is at most one bad person among Player 2, Player 3 , Player 5, and Player 7. Since Player 2 and Player 7 are good , and we know Player 1 is bad , Player 3 must be good , leaving Player 5 as the only possible bad player from this group .</p>
<p>After applying these constraints , we should be able to identify the 4 players most likely to be good .</p>
<p>Let ' s write the code to apply these constraints and find the 4 good players .</p>
<p>Figure 1 :
1
Figure 1: A flowchart of the AvalonPlay benchmark showing team sides and roles on the left and a detailed round pipeline on the right.Each round includes four stages: leader assignment, team selection, discussion and voting, quest execution.</p>
<p>Figure 3 :
3
Figure 3: An example of the CodeAct agent's prompt.</p>
<p>Figure 4 :
4
Figure 4: An example of the CodeAct agent's output.</p>
<p>Figure 5 :
5
Figure 5: The results of different models acting as the good and evil sides, playing games against each other.Under each setting, 30 games were conducted, totaling 150 quests.</p>
<p>Figure 6: The turn-by-turn quest win rate results when different models assume the role of the good side.Each model, as the good side, played 90 games, resulting in 450 quests evenly distributed across 5 turns.</p>
<p>None ] * 7 # Initialize the players list with None # Apply Round 2 information players [1] = True # Player 2 is good players [3] = True # Player 4 is good players [6] = True # Player 7 is good # Apply Round 1 information # We cannot determine which one is bad yet , so we leave Player 1 and Player 3 as None # Apply Round 3 information # Since Player 2 and Player 4 are good , Player 1 must be bad players [0] = False # Player 1 is bad # Apply Round 4 information # Since Player 2 and Player 7 are good , and we know Player 1 is bad , Player 3 must be good players [2] = True # Player 3 is good # This leaves Player 5 as the only possible bad player from this group players [4] = False # Player 5 is bad # Now we can determine the remaining players ' goodness based on the total number of bad players # We already have 2 bad players ( Player 1 and Player 5) , so Player 6 must be good players [5] = True # Player 6 is good # Print the final 4 players most likely to be good good_players = [ index + 1 for index , is_good in enumerate ( players ) if is_good ] print ( good_players )Observation :[2 , 3, 4, 6, 7]   Thought : I now know the final answer Final Answer : The 4 players most likely to be good are Players 2, 3, 4, and 6.</p>
<p>Figure 8 :
8
Figure 8: An example of the code-driven reasoning with action generation.</p>
<p>Table 2 :
2
Results from the GPT-4 agent self-playing in the AvalonPlay benchmark, both with and without communication.Under each setting, 30 games were conducted, totaling 150 quests.
Agent/ MetricGame Win Quest Win Team AccBase0.5000.5130.640Base W/O Communication0.4660.4800.693</p>
<p>Table 3
3: Results of the GPT-4 agent self-playing inthe AvalonPlay benchmark using different promptingstrageties. Under each setting, 30 games were con-ducted, totaling 150 quests.Agent/ MetricGame Win Quest Win Team AccBase with CoT0.5330.5470.707Base with ReAct0.5000.5260.634CodeAct0.6330.5930.830</p>
<p>If this player is the leader , you can disagree with the establishment of this team .Reasoning based on facts you have observed and you cannot perceive information ( such as acoustic info ) other than text .Do not pretend you are other players or the moderator .Figure 7: Game rule prompt for all LLM agents in the AvalonPlay benchmark.Merlin {Game Rule} + You are Merlin, aware of the evil presences, yet you must remain discreet in sharing this knowledge.Should the assassin identify you after the just side completes three quests, the evil side shall emerge victorious.Your objective is to ensure the success of the quests.Percival {Game Rule} + As Percival, you recognize Merlin, but be wary as Morgana might mislead you by appearing as Merlin.Your allegiance is with the just side.Your objective is to ensure the success of the quests.Good Servant {Game Rule} + You are a Servant of Arthur, with no special abilities, belonging to the just side.Your objective is to ensure the success of the quests.Morgana {Game Rule} + You are Morgana, appearing as Merlin to Percival, weaving a web of confusion.Your objective is to ensure the quest fails.Assassin {Game Rule} + As the Assassin, your critical role comes into play once the just side successfully executes three quests; identifying Merlin can secure a win for the evil side.Your objective is to ensure the quest fails.Evil Minion {Game Rule} + You are a Minion of Mordred, without special abilities, on the evil side.Your objective is to ensure the quest fails.
RoleGlobal Prompt</p>
<p>Table 4 :
4
Role assignment prompt for LLM agents in various roles within the AvalonPlay benchmark.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, arXiv:2308.07201Chateval: Towards better llm-based evaluators through multi-agent debate. 2023arXiv preprint</p>
<p>Aateam: Achieving the ad hoc teamwork by employing the attention mechanism. Shuo Chen, Ewa Andrejczuk, Zhiguang Cao, Jie Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2020</p>
<p>Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, arXiv:2308.10848Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. 2023aarXiv preprint</p>
<p>Teaching large language models to self-debug. Xinyun Chen, Maxwell Lin, Nathanael Schärli, Denny Zhou, arXiv:2304.051282023barXiv preprint</p>
<p>Self-collaboration code generation via chatgpt. Yihong Dong, Xue Jiang, Zhi Jin, Ge Li, arXiv:2304.075902023arXiv preprint</p>
<p>Yali Du, Joel Z Leibo, Usman Islam, Richard Willis, Peter Sunehag, arXiv:2312.05162A review of cooperation in multi-agent learning. 2023arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Metagpt: Meta programming for multi-agent collaborative framework. 2023arXiv preprint</p>
<p>Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, arXiv:2310.03128Metatool benchmark for large language models: Deciding whether to use tools and which to use. 2023arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi, arXiv:2305.191182023arXiv preprint</p>
<p>Pecan: Leveraging policy ensemble for context-aware zero-shot human-ai coordination. Xingzhou Lou, Jiaxian Guo, Junge Zhang, Jun Wang, Kaiqi Huang, Yali Du, Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems(AAMAS). the 2023 International Conference on Autonomous Agents and Multiagent Systems(AAMAS)2023</p>
<p>Expected value of communication for planning in ad hoc teamwork. William Macke, Reuth Mirsky, Peter Stone, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2021</p>
<p>A survey of ad hoc teamwork: Definitions, methods, and open problems. Reuth Mirsky, Ignacio Carlucho, Arrasy Rahman, Elliot Fosong, William Macke, Mohan Sridharan, Peter Stone, Stefano V Albrecht, European Conference on Multiagent Systems. 2022</p>
<p>A penny for your thoughts: The value of communication in ad hoc teamwork. Reuth Mirsky, William Macke, Andy Wang, 2020Good Systems-Published ResearchHarel Yedidsion, and Peter Stone</p>
<p>Openai, Gpt-4 technical report. 2023</p>
<p>Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023arXiv preprint</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924Communicative agents for software development. 2023arXiv preprint</p>
<p>Generating teammates for training robust ad hoc teamwork agents via bestresponse diversity. Arrasy Rahman, Elliot Fosong, Ignacio Carlucho, Stefano V Albrecht, Transactions on Machine Learning Research. 2023</p>
<p>Towards open ad hoc teamwork using graph-based policy learning. Niklas Muhammad A Rahman, Filippos Hopner, Stefano V Christianos, Albrecht, International Conference on Machine Learning. PMLR2021</p>
<p>Stay moral and explore: Learn to behave morally in text-based games. Zijing Shi, Meng Fang, Yunqiu Xu, Ling Chen, Yali Du, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Self-imitation learning for action generation in text-based games. Zijing Shi, Yunqiu Xu, Meng Fang, Ling Chen, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European Chapterthe Association for Computational Linguistics2023</p>
<p>Reflexion: an autonomous agent with dynamic memory and self-reflection. Noah Shinn, Beck Labash, Ashwin Gopinath, arXiv:2303.113662023arXiv preprint</p>
<p>Ad hoc autonomous agent teams: Collaboration without pre-coordination. Peter Stone, Gal Kaminka, Sarit Kraus, Jeffrey Rosenschein, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2010</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023aarXiv preprint</p>
<p>Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei, arXiv:2306.07174Augmenting language models with long-term memory. 2023barXiv preprint</p>
<p>Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona selfcollaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, arXiv:2307.053002023carXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Autogen: Enabling next-gen llm applications via multiagent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv:2308.081552023arXiv preprint</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Generalization in text-based games via hierarchical reinforcement learning. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Chengqi Zhang, Findings of the Association for Computational Linguistics: EMNLP 2021. 2021</p>
<p>Perceiving the world: Question-guided reinforcement learning for text-based games. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Zhou, Chengqi Zhang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang, Advances in Neural Information Processing Systems. 202033</p>
<p>Exploring large language models for communication games: An empirical study on werewolf. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu, arXiv:2309.046582023arXiv preprint</p>
<p>An efficient end-to-end training approach for zero-shot humanai coordination. Xue Yan, Jiaxian Guo, Xingzhou Lou, Jun Wang, Haifeng Zhang, Yali Du ; Shunyu, Dian Yao, Jeffrey Yu, Izhak Zhao, Thomas L Shafran, Yuan Griffiths, Karthik Cao, Narasimhan, arXiv:2305.10601Tree of thoughts: Deliberate problem solving with large language models. 2023. 2023arXiv preprintThirty-seventh Conference on Neural Information Processing Systems</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>How far are large language models from agents with theory. Pei Zhou, Aman Madaan, Pranavi Srividya, Aditya Potharaju, Kevin R Gupta, Ari Mckee, Jay Holtzman, Xiang Pujara, Swaroop Ren, Aida Mishra, Nematzadeh, arXiv:2310.030512023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
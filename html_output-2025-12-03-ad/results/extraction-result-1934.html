<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1934 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1934</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1934</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-280252663</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.10672v1.pdf" target="_blank">Vision Language Action Models in Robotic Manipulation: A Systematic Review</a></p>
                <p><strong>Paper Abstract:</strong> Vision Language Action (VLA) models represent a transformative shift in robotics, with the aim of unifying visual perception, natural language understanding, and embodied control within a single learning framework. This review presents a comprehensive and forward-looking synthesis of the VLA paradigm, with a particular emphasis on robotic manipulation and instruction-driven autonomy. We comprehensively analyze 102 VLA models, 26 foundational datasets, and 12 simulation platforms that collectively shape the development and evaluation of VLAs models. These models are categorized into key architectural paradigms, each reflecting distinct strategies for integrating vision, language, and control in robotic systems. Foundational datasets are evaluated using a novel criterion based on task complexity, variety of modalities, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning. We introduce a two-dimensional characterization framework that organizes these datasets based on semantic richness and multimodal alignment, showing underexplored regions in the current data landscape. Simulation environments are evaluated for their effectiveness in generating large-scale data, as well as their ability to facilitate transfer from simulation to real-world settings and the variety of supported tasks. Using both academic and industrial contributions, we recognize ongoing challenges and outline strategic directions such as scalable pretraining protocols, modular architectural design, and robust multimodal alignment strategies. This review serves as both a technical reference and a conceptual roadmap for advancing embodiment and robotic control, providing insights that span from dataset generation to real world deployment of generalist robotic agents.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1934.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1934.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action foundation model that co-finetunes internet-scale VQA data with robot trajectory data to enable zero-shot transfer across many manipulation tasks and robot embodiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-2: Vision-language-action models transfer web knowledge to robotic control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based VLA that fuses visual, language and state encoders; co-finetuned on web-scale VQA/text and robot trajectory data and uses an adapter-based approach to handle robot-specific control (vision-language planning frozen in some deployments, lightweight action adapters for embodiment).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language + robot-trajectory co-finetuning (internet-scale VQA + demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Internet-scale VQA / image-text style corpora combined with robot trajectory datasets (robot demonstrations, language annotations); contains object descriptions and web-scale visual-language grounding but not detailed affordance/action token statistics in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation / multi-robot instruction-conditioned control</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Language-conditioned manipulation across many skills and robot platforms (evaluated on Open X-Embodiment, BridgeData); aims for zero-shot execution across dozens of tasks and multiple robot embodiments; includes real-robot validation according to the review.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>The review states RT-2 explicitly co-finetunes web VQA data with robot trajectories to transfer web knowledge to control, indicating deliberate semantic alignment between pretraining and target tasks (objects and language overlap), but no quantitative overlap table is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Reported qualitatively as High success rate and High zero-shot capability in the survey (Table 5) and stated to outperform RT-1 and SayCan on generalization; no numeric success percentages are supplied in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not analyzed in detail in this review; no attention-visualization results for RT-2 are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space clustering or representational analyses for RT-2 are reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>The review attributes RT-2's transfer to co-finetuning of language-rich web data with trajectories (semantic transfer), but provides no mechanistic probe or per-action grounding analysis in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Reported to transfer well across multiple robot embodiments and many tasks when co-finetuned with web VQA + robot data; the review notes freezing the vision-language planning module and using lightweight action adapters helps cross-embodiment transfer, but no quantitative ablations are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No numeric comparison of novel vs familiar object performance is provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Zero-shot: review reports RT-2 achieves strong zero-shot transfer capability (rated High in Table 5) across multiple robots and tasks; no exact per-task numbers in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Review notes a practice for RT-2: freeze vision-language planning module and delegate embodiment-specific control to adapters, indicating a layer-freezing strategy, but no detailed layerwise probe results are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>The review does not provide a head-to-head numeric comparison of RT-2 against vision-only pretraining baselines; claims of outperforming RT-1 and SayCan refer to other published comparisons but not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1934.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1934.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-1 (Robotics Transformer 1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early robotics transformer that maps raw images and language instructions to robot control; foundational example of end-to-end vision-language-conditioned policies for real-world control.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rt-1: Robotics transformer for real-world control at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based end-to-end policy mapping images and language to actions; uses visual and language backbones and supervised learning on real robot demonstration data (presented as a foundational VLA approach in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>supervised on robot demonstration datasets / task-specific pretraining (as summarized in review); not described as internet-scale vision-language pretraining in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Real-world robot demonstrations with paired language annotations (review summarizes RT-1 as trained on such datasets), but the review does not detail the exact corpus here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation / language-conditioned control</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Household and tabletop manipulation tasks in real robots; end-to-end mapping from egocentric images and textual instructions to continuous control (per original RT-1 framing).</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Review treats RT-1 as a baseline end-to-end mapping; it does not provide explicit semantic-alignment measurements between pretraining and downstream tasks for RT-1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not numerically reported in this review; RT-2 is stated to outperform RT-1 on generalization in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>RT-1 is cited as an end-to-end instruction-conditioned controller demonstrating grounded behaviors, but the review does not present mechanistic grounding analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>RT-1 is a baseline for comparison; review notes more recent models improve zero-shot transfer via language/web pretraining, implying RT-1 has weaker zero-shot cross-embodiment transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not highlighted as a zero-shot leader in this review (RT-1 is outperformed by RT-2 in zero-shot generalization according to the survey).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported in detail in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1934.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1934.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-conditioned tabletop manipulation system that uses CLIP visual-text alignment to build dense transport maps for pick-and-place tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cliport: What and where pathways for robotic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIPort</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines CLIP-derived visual-text features with dense transport-map policies to predict pick-and-place actions; uses CLIP for semantic visual grounding into spatial action maps.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language contrastive pretraining (CLIP image-text); policy trained on manipulation demonstrations conditioned on CLIP features.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Image-text pairs from CLIP pretraining (natural image captions) giving object and scene semantics; CLIP features used to ground textual object references to image locations; no explicit affordance labels described in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>tabletop pick-and-place / object manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Discrete and continuous pick-and-place tasks on tabletop benchmarks (Ravens/benchmarks); predicts spatial transport maps for 'where' and 'what' to pick and place. Tasks are primarily simulated or desktop-scale real setups depending on dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Review emphasizes 'dense semantic grounding using CLIP-enhanced transport maps' indicating good overlap between CLIP image-text semantics and pick/place object references; no quantitative overlap statistics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Described as achieving state-of-the-art on diverse tabletop manipulation tasks in the survey (no numeric success rates provided here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>The review does not present attention visualizations for CLIPort; grounding is via CLIP features and transport maps rather than transformer attention analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not detailed in this review; implicit that CLIP feature space provides semantic object localization.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Yes â€” review highlights dense semantic grounding: CLIP features are used to link language mentions to pixel locations used by transport policies, providing operational grounding between text and spatial actions.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not explicitly analyzed; CLIP features (higher-level semantic) are used for grounding rather than low-level visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works well for tabletop object manipulation where object categories and text descriptions align with CLIP pretraining corpora; review implies better grounding when language references match CLIP's object vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>No quantitative novel-vs-familiar object comparisons are reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not reported as a zero-shot generalist in this review (positioned as strong for tabletop tasks with CLIP grounding).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Review frames CLIP-based (vision-language) features as improving semantic grounding over vision-only baselines, but no numeric head-to-head stats are listed here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1934.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1934.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompt-based multimodal transformer policy that conditions on language prompts to perform multiple manipulation grounding tasks (pushing, grasping, stacking, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>VIMA: General robot manipulation with multimodal prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VIMA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Promptable multimodal transformer policy (encoder-decoder style) that interprets language prompts and visual inputs to produce manipulation actions; demonstrated on several grounding tasks and some real-robot demos.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>policy trained on multimodal manipulation datasets with prompt-style conditioning (review does not specify external vision-language pretraining beyond dataset training).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trained on multimodal manipulation demonstrations with paired language prompts for grounding tasks; review does not list exact corpora used for VIMA here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>language-conditioned manipulation (pushing, grasping, stacking, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Short-horizon grounding tasks involving object interactions (push, grasp, stack) with both simulated and real-robot demonstrations in prior work; action space includes motion primitives / end-effector controls.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>VIMA uses prompt conditioning to align language to manipulation primitives; review notes this enables compositional generalization, but no overlap statistics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 5 categorizes VIMA's Success Rate and Zero-Shot Capability as Medium; no numeric success rates provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Demonstrated ability to execute six grounding tasks from prompts (pushing, grasping, stacking etc.) and includes real-robot demos, indicating functional grounding between language and actions, but no mechanistic probes are supplied here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Review reports real-robot demos and compositional generalization; specific transfer failure modes or conditions are not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Rated Medium for zero-shot capability in Table 5 (no numeric detail).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not explicitly compared to vision-only pretraining in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1934.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1934.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Octo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Octo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based generalist robot policy trained at large scale (millions of trajectories) across many robot embodiments to support robust sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Octo: An open-source generalist robot policy</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Octo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based VLA using a diffusion action decoder (diffusion transformer policy) trained on a very large corpus of robot trajectories; integrates vision and language conditioning for generalist manipulation across embodiments.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>large-scale multimodal trajectory pretraining (diffusion policy trained on robot demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trained on >4M trajectories across 22 robots (review explicitly cites '4M+ trajectories across 22 robots'); dataset includes multimodal trajectories with language annotations and state/action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>generalist robotic manipulation / instruction-conditioned control / sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Long and short-horizon manipulation tasks across many robot types and embodiments; continuous trajectory generation via diffusion samplers; evaluated for sim-to-real transfer with real-robot validation according to the review.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Large-scale multimodal pretraining with language annotations aims to align language with manipulation behaviors; review states Octo achieves robust sim-to-real transfer but does not quantify semantic overlap.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 5 rates Octo as Medium success and Medium zero-shot capability; review highlights robust sim-to-real transfer after training on 4M+ trajectories but provides no numeric success percentages here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not quantified; review implies Octo relies on very large data scale (millions of trajectories) rather than sample-efficient fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Uses diffusion-based trajectory generation to produce temporally coherent motor sequences; review cites improved stability and generalization from diffusion but no explicit language-to-action grounding probes are included here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Large-pretraining scale and multi-embodiment training correlated with robust sim-to-real transfer in review; specific ablations (e.g., domain gap, visual realism) are not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Rated Medium in Table 5 for zero-shot capability; no numeric results.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1934.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1934.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DexVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DexVLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-language-action model that uses plug-in diffusion 'expert' modules to adapt across different robot embodiments without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dexvla: Visionlanguage model with plug-in diffusion expert for general robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DexVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VLA architecture employing plug-in diffusion experts (specialized diffusion modules) to enable cross-embodiment adaptation; integrates vision and language encoders with modular diffusion action decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>modular diffusion expert pretraining across multiple embodiments (review summarizes plug-in expert approach; exact corpora not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trained on multi-embodiment trajectory sets to learn expert diffusion modules; review does not provide the detailed composition of the pretraining dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation with cross-embodiment transfer</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Language-conditioned manipulation tasks across different robot kinematics where plug-in diffusion experts produce suitable control trajectories; evaluated for transfer without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Review describes DexVLA as enabling adaptation via specialized experts trained across embodiments, implying alignment between semantic tasks and embodiment-specific decoders; no quantitative semantic-overlap measures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 5 shows DexVLA rated Medium success/Medium zero-shot; review highlights cross-embodiment adaptation without fine-tuning but gives no numeric success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Adaptation without fine-tuning implies sample-efficiency benefits for new embodiments, but the review provides no numeric sample-efficiency comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Mechanism: plug-in diffusion experts map shared vision-language representations to embodiment-specific motor patterns; review reports functional cross-embodiment transfer but no probing of verb-to-affordance grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Works when diffusion experts have been trained on diverse kinematics; review notes improved cross-embodiment adaptation but lacks detailed failure-mode analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Rated Medium zero-shot capability in Table 5; no numeric detail.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported beyond modular expert design description.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1934.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1934.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pi-0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pi-0</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A lightweight 3B-parameter VLA flow model designed for fast, low-latency general robot control with >200 Hz inference and cross-embodiment generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pi-0: A visionlanguage-action flow model for general robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Pi-0</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A low-latency (200+ Hz) vision-language-action flow model optimized for fast control loops; intended to generalize to new tasks and robot embodiments while operating at high control frequencies.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>foundation VLA pretraining with architecture optimized for runtime efficiency (review does not list precise pretraining corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Review references Pi-Cross-Embodiment / cross-embodiment datasets as associated training material but does not enumerate data content; emphasis on efficient models rather than data scale.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>real-time robotic control and manipulation across embodiments</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Low-latency control tasks requiring high-frequency inference (>200 Hz) for reactive manipulation across different robot types; real-robot demonstrations indicated in review.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Review emphasizes runtime and embodiment generalization; semantic alignment specifics are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Table 5 classifies Pi-0 as Medium success/Medium zero-shot; review highlights >200 Hz low-latency operation and generalization claims, but provides no numerical success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Claims generalization to new embodiments with minimal adaptation, implying alignment of vision-language representations to action flows, but no mechanistic grounding analyses are provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Designed for cross-embodiment generalization and low-latency settings; details of what conditions degrade transfer are not given.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Reported as Medium zero-shot in Table 5; no numeric details.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1934.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1934.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TLA (Tactile-Language-Action)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal dataset and model family linking tactile signals with language and action to enable contact-rich insertion and assembly tasks with high success.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Tla: Tactile-language-action model for contact-rich manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>TLA (model / benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Integrates tactile/haptic streams with visual inputs and natural language instructions to train language-conditioned policies for contact-rich manipulation; includes a tactile-language-action dataset and specialized VLA models.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal pretraining incorporating tactile data aligned with language and action trajectories (dataset TLA contains tactile-language-action aligned demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>TLA dataset: 30K contact-rich peg-in-hole demonstrations with synchronized tactile, vision, and language annotations enabling tactile-language-action alignment; contains fine-grained contact signals relevant to insertion tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>contact-rich assembly and insertion (peg-in-hole, precise insertion)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Contact-rich, high-precision assembly tasks with continuous action trajectories and haptic feedback; action space is continuous (trajectory/joint control) and data includes high-frequency tactile/force signals; evaluated on real robotic assembly tasks according to the review.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>TLA explicitly aligns tactile signals with language instructions, providing strong modality-level alignment between action verbs (e.g., 'insert', 'press') and tactile affordances; the review highlights this alignment as a key contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>The review reports TLA models achieve '85%+ success on contact-rich tasks' (Table 5) when using tactile-language-action modeling; this is the explicit numeric performance noted in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported in this review; no attention maps or haptic-to-visual attention analyses are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported; review does not present quantitative representational analyses for tactile-language embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Yes â€” the TLA benchmark and associated models provide explicit tactile-language-action alignment for grounding contact-rich verbs to haptic signatures and motor patterns; the review cites the 85%+ success as functional evidence of grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported in detailed representational terms; the dataset design implies both low-level haptic and higher-level task semantics are used.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Designed for contact-rich tasks; review does not quantify how models trained on TLA transfer to other contact tasks without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Table 5 rates TLA zero-shot capability as High, but the review provides the explicit success rate (85%+) as in-distribution performance; zero-shot numeric details are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>The review emphasizes the added value of tactile modality versus vision-only datasets for contact-rich tasks but does not provide numeric head-to-head comparisons in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1934.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1934.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied multimodal language model that conditions language reasoning on explicit hardware/embodiment descriptors to adapt across robot platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Palm-e: An embodied multimodal language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-E</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multimodal embodied language model that fuses vision and language and encodes hardware/embodiment descriptors (hardware embeddings) to adapt reasoning and instructions to specific robot platforms.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>multimodal language + vision pretraining (embodied fine-tuning described in cited work); review summarizes PaLM-E's hardware-embedding strategy rather than listing full pretraining corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining involves large language corpora and vision-language data; review notes PaLM-E encodes explicit hardware embeddings to adapt to new platforms but does not provide full dataset composition here.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>language-conditioned reasoning and robot instruction across embodiments</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Tasks requiring language reasoning and adaptation to specific hardware descriptors to produce appropriate action plans; applies to embodied manipulation and instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Review cites PaLM-E's explicit hardware embeddings as a mechanism for aligning language reasoning with embodiment constraints; semantic alignment to objects/actions is implied but not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Not reported in numeric detail in this review; PaLM-E is cited as an approach to improve cross-embodiment adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not analyzed in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Review mentions hardware embeddings conceptually but does not provide representational analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Conceptual evidence: hardware embeddings help map high-level language reasoning into embodiment-appropriate actions; no operational grounding metrics are provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Designed to improve transfer across robot embodiments by conditioning on hardware descriptors; quantitative conditions of success/failure not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not quantified in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1934.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1934.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diffusion Policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Diffusion Policy (and Diffusion Transformer Policy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that formulate visuomotor control as a conditional denoising (diffusion) process to generate temporally coherent continuous action trajectories for manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Diffusion policy: Visuomotor policy learning via action diffusion.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Diffusion Policy / Diffusion Transformer Policy</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Action decoder approach using diffusion models (or diffusion transformers) that iteratively denoise latent trajectories conditioned on visual/language/state tokens to produce smooth, temporally-consistent control sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>trained on trajectory datasets; technique complements vision-language encoders rather than being a vision-language pretraining itself.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trained on demonstration trajectory corpora (visuomotor trajectories with associated observations and optionally language); review references diffusion-based policies applied at scale (e.g., Octo, DexVLA).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>continuous trajectory generation for robotic manipulation / visuo-lingual action generation</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Generates continuous control trajectories for manipulation tasks (grasping, insertion, assembly, general manipulation) with temporal coherence; used in both simulated and real-robot settings per cited works in review.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Diffusion policies condition on vision-language embeddings to realize semantic alignment of actions; review emphasizes improved motion smoothness and stability but does not report numeric semantic-overlap measures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Review reports diffusion-based decoders improve temporal smoothness and generalization (cited works), and that Octo trained with diffusion achieved robust sim-to-real transfer on millions of trajectories; no single-number metric is given here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not provided in the review for diffusion policies specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not provided in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Functional evidence: diffusion decoders produce temporally coherent trajectories conditioned on semantic embeddings, improving practical grounding of language-conditioned commands into motor sequences; mechanistic grounding analyses are not included here.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Review notes diffusion policies can struggle with real-time latency; also noted that large-scale pretraining improves transfer, but specific conditions are not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Used within models that show varying zero-shot abilities (e.g., Octo, DexVLA), but diffusion policy itself is not reported with standalone zero-shot numbers in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported explicitly; review mentions latency and runtime constraints as practical limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Review emphasizes diffusion yields temporally coherent action sequences; no quantified temporal-learning dynamics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rt-1: Robotics transformer for real-world control at scale. <em>(Rating: 2)</em></li>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>Cliport: What and where pathways for robotic manipulation. <em>(Rating: 2)</em></li>
                <li>VIMA: General robot manipulation with multimodal prompts. <em>(Rating: 2)</em></li>
                <li>Diffusion policy: Visuomotor policy learning via action diffusion. <em>(Rating: 2)</em></li>
                <li>Pi-0: A visionlanguage-action flow model for general robot control. <em>(Rating: 1)</em></li>
                <li>Dexvla: Visionlanguage model with plug-in diffusion expert for general robot control. <em>(Rating: 1)</em></li>
                <li>Tla: Tactile-language-action model for contact-rich manipulation. <em>(Rating: 2)</em></li>
                <li>Palm-e: An embodied multimodal language model. <em>(Rating: 2)</em></li>
                <li>Octo: An open-source generalist robot policy <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1934",
    "paper_id": "paper-280252663",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "RT-2",
            "name_full": "RT-2",
            "brief_description": "A vision-language-action foundation model that co-finetunes internet-scale VQA data with robot trajectory data to enable zero-shot transfer across many manipulation tasks and robot embodiments.",
            "citation_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control.",
            "mention_or_use": "mention",
            "model_name": "RT-2",
            "model_description": "Transformer-based VLA that fuses visual, language and state encoders; co-finetuned on web-scale VQA/text and robot trajectory data and uses an adapter-based approach to handle robot-specific control (vision-language planning frozen in some deployments, lightweight action adapters for embodiment).",
            "pretraining_type": "vision-language + robot-trajectory co-finetuning (internet-scale VQA + demonstrations)",
            "pretraining_data_description": "Internet-scale VQA / image-text style corpora combined with robot trajectory datasets (robot demonstrations, language annotations); contains object descriptions and web-scale visual-language grounding but not detailed affordance/action token statistics in this review.",
            "target_task_name": "robotic manipulation / multi-robot instruction-conditioned control",
            "target_task_description": "Language-conditioned manipulation across many skills and robot platforms (evaluated on Open X-Embodiment, BridgeData); aims for zero-shot execution across dozens of tasks and multiple robot embodiments; includes real-robot validation according to the review.",
            "semantic_alignment": "The review states RT-2 explicitly co-finetunes web VQA data with robot trajectories to transfer web knowledge to control, indicating deliberate semantic alignment between pretraining and target tasks (objects and language overlap), but no quantitative overlap table is provided.",
            "performance_with_language_pretraining": "Reported qualitatively as High success rate and High zero-shot capability in the survey (Table 5) and stated to outperform RT-1 and SayCan on generalization; no numeric success percentages are supplied in this review.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not analyzed in detail in this review; no attention-visualization results for RT-2 are reported here.",
            "embedding_space_analysis": "No embedding-space clustering or representational analyses for RT-2 are reported in this review.",
            "action_grounding_evidence": "The review attributes RT-2's transfer to co-finetuning of language-rich web data with trajectories (semantic transfer), but provides no mechanistic probe or per-action grounding analysis in this paper.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Reported to transfer well across multiple robot embodiments and many tasks when co-finetuned with web VQA + robot data; the review notes freezing the vision-language planning module and using lightweight action adapters helps cross-embodiment transfer, but no quantitative ablations are provided.",
            "novel_vs_familiar_objects": "No numeric comparison of novel vs familiar object performance is provided in this review.",
            "zero_shot_or_few_shot": "Zero-shot: review reports RT-2 achieves strong zero-shot transfer capability (rated High in Table 5) across multiple robots and tasks; no exact per-task numbers in this paper.",
            "layer_analysis": "Review notes a practice for RT-2: freeze vision-language planning module and delegate embodiment-specific control to adapters, indicating a layer-freezing strategy, but no detailed layerwise probe results are provided.",
            "negative_transfer_evidence": "Not reported in this review.",
            "comparison_to_vision_only": "The review does not provide a head-to-head numeric comparison of RT-2 against vision-only pretraining baselines; claims of outperforming RT-1 and SayCan refer to other published comparisons but not quantified here.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1934.0"
        },
        {
            "name_short": "RT-1",
            "name_full": "RT-1 (Robotics Transformer 1)",
            "brief_description": "An early robotics transformer that maps raw images and language instructions to robot control; foundational example of end-to-end vision-language-conditioned policies for real-world control.",
            "citation_title": "Rt-1: Robotics transformer for real-world control at scale.",
            "mention_or_use": "mention",
            "model_name": "RT-1",
            "model_description": "Transformer-based end-to-end policy mapping images and language to actions; uses visual and language backbones and supervised learning on real robot demonstration data (presented as a foundational VLA approach in the review).",
            "pretraining_type": "supervised on robot demonstration datasets / task-specific pretraining (as summarized in review); not described as internet-scale vision-language pretraining in this review.",
            "pretraining_data_description": "Real-world robot demonstrations with paired language annotations (review summarizes RT-1 as trained on such datasets), but the review does not detail the exact corpus here.",
            "target_task_name": "robotic manipulation / language-conditioned control",
            "target_task_description": "Household and tabletop manipulation tasks in real robots; end-to-end mapping from egocentric images and textual instructions to continuous control (per original RT-1 framing).",
            "semantic_alignment": "Review treats RT-1 as a baseline end-to-end mapping; it does not provide explicit semantic-alignment measurements between pretraining and downstream tasks for RT-1.",
            "performance_with_language_pretraining": "Not numerically reported in this review; RT-2 is stated to outperform RT-1 on generalization in the survey.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this review.",
            "embedding_space_analysis": "Not reported in this review.",
            "action_grounding_evidence": "RT-1 is cited as an end-to-end instruction-conditioned controller demonstrating grounded behaviors, but the review does not present mechanistic grounding analyses.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "RT-1 is a baseline for comparison; review notes more recent models improve zero-shot transfer via language/web pretraining, implying RT-1 has weaker zero-shot cross-embodiment transfer.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not highlighted as a zero-shot leader in this review (RT-1 is outperformed by RT-2 in zero-shot generalization according to the survey).",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported in detail in this review.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1934.1"
        },
        {
            "name_short": "CLIPort",
            "name_full": "CLIPort",
            "brief_description": "A language-conditioned tabletop manipulation system that uses CLIP visual-text alignment to build dense transport maps for pick-and-place tasks.",
            "citation_title": "Cliport: What and where pathways for robotic manipulation.",
            "mention_or_use": "mention",
            "model_name": "CLIPort",
            "model_description": "Combines CLIP-derived visual-text features with dense transport-map policies to predict pick-and-place actions; uses CLIP for semantic visual grounding into spatial action maps.",
            "pretraining_type": "vision-language contrastive pretraining (CLIP image-text); policy trained on manipulation demonstrations conditioned on CLIP features.",
            "pretraining_data_description": "Image-text pairs from CLIP pretraining (natural image captions) giving object and scene semantics; CLIP features used to ground textual object references to image locations; no explicit affordance labels described in the review.",
            "target_task_name": "tabletop pick-and-place / object manipulation",
            "target_task_description": "Discrete and continuous pick-and-place tasks on tabletop benchmarks (Ravens/benchmarks); predicts spatial transport maps for 'where' and 'what' to pick and place. Tasks are primarily simulated or desktop-scale real setups depending on dataset.",
            "semantic_alignment": "Review emphasizes 'dense semantic grounding using CLIP-enhanced transport maps' indicating good overlap between CLIP image-text semantics and pick/place object references; no quantitative overlap statistics provided.",
            "performance_with_language_pretraining": "Described as achieving state-of-the-art on diverse tabletop manipulation tasks in the survey (no numeric success rates provided here).",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": "The review does not present attention visualizations for CLIPort; grounding is via CLIP features and transport maps rather than transformer attention analysis.",
            "embedding_space_analysis": "Not detailed in this review; implicit that CLIP feature space provides semantic object localization.",
            "action_grounding_evidence": "Yes â€” review highlights dense semantic grounding: CLIP features are used to link language mentions to pixel locations used by transport policies, providing operational grounding between text and spatial actions.",
            "hierarchical_features_evidence": "Not explicitly analyzed; CLIP features (higher-level semantic) are used for grounding rather than low-level visual features.",
            "transfer_conditions": "Works well for tabletop object manipulation where object categories and text descriptions align with CLIP pretraining corpora; review implies better grounding when language references match CLIP's object vocabulary.",
            "novel_vs_familiar_objects": "No quantitative novel-vs-familiar object comparisons are reported in this review.",
            "zero_shot_or_few_shot": "Not reported as a zero-shot generalist in this review (positioned as strong for tabletop tasks with CLIP grounding).",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Review frames CLIP-based (vision-language) features as improving semantic grounding over vision-only baselines, but no numeric head-to-head stats are listed here.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1934.2"
        },
        {
            "name_short": "VIMA",
            "name_full": "VIMA",
            "brief_description": "A prompt-based multimodal transformer policy that conditions on language prompts to perform multiple manipulation grounding tasks (pushing, grasping, stacking, etc.).",
            "citation_title": "VIMA: General robot manipulation with multimodal prompts.",
            "mention_or_use": "mention",
            "model_name": "VIMA",
            "model_description": "Promptable multimodal transformer policy (encoder-decoder style) that interprets language prompts and visual inputs to produce manipulation actions; demonstrated on several grounding tasks and some real-robot demos.",
            "pretraining_type": "policy trained on multimodal manipulation datasets with prompt-style conditioning (review does not specify external vision-language pretraining beyond dataset training).",
            "pretraining_data_description": "Trained on multimodal manipulation demonstrations with paired language prompts for grounding tasks; review does not list exact corpora used for VIMA here.",
            "target_task_name": "language-conditioned manipulation (pushing, grasping, stacking, etc.)",
            "target_task_description": "Short-horizon grounding tasks involving object interactions (push, grasp, stack) with both simulated and real-robot demonstrations in prior work; action space includes motion primitives / end-effector controls.",
            "semantic_alignment": "VIMA uses prompt conditioning to align language to manipulation primitives; review notes this enables compositional generalization, but no overlap statistics provided.",
            "performance_with_language_pretraining": "Table 5 categorizes VIMA's Success Rate and Zero-Shot Capability as Medium; no numeric success rates provided in this review.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in this review.",
            "embedding_space_analysis": "Not provided in this review.",
            "action_grounding_evidence": "Demonstrated ability to execute six grounding tasks from prompts (pushing, grasping, stacking etc.) and includes real-robot demos, indicating functional grounding between language and actions, but no mechanistic probes are supplied here.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Review reports real-robot demos and compositional generalization; specific transfer failure modes or conditions are not quantified here.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Rated Medium for zero-shot capability in Table 5 (no numeric detail).",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not explicitly compared to vision-only pretraining in this review.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1934.3"
        },
        {
            "name_short": "Octo",
            "name_full": "Octo",
            "brief_description": "A diffusion-based generalist robot policy trained at large scale (millions of trajectories) across many robot embodiments to support robust sim-to-real transfer.",
            "citation_title": "Octo: An open-source generalist robot policy",
            "mention_or_use": "mention",
            "model_name": "Octo",
            "model_description": "Transformer-based VLA using a diffusion action decoder (diffusion transformer policy) trained on a very large corpus of robot trajectories; integrates vision and language conditioning for generalist manipulation across embodiments.",
            "pretraining_type": "large-scale multimodal trajectory pretraining (diffusion policy trained on robot demonstrations)",
            "pretraining_data_description": "Trained on &gt;4M trajectories across 22 robots (review explicitly cites '4M+ trajectories across 22 robots'); dataset includes multimodal trajectories with language annotations and state/action sequences.",
            "target_task_name": "generalist robotic manipulation / instruction-conditioned control / sim-to-real transfer",
            "target_task_description": "Long and short-horizon manipulation tasks across many robot types and embodiments; continuous trajectory generation via diffusion samplers; evaluated for sim-to-real transfer with real-robot validation according to the review.",
            "semantic_alignment": "Large-scale multimodal pretraining with language annotations aims to align language with manipulation behaviors; review states Octo achieves robust sim-to-real transfer but does not quantify semantic overlap.",
            "performance_with_language_pretraining": "Table 5 rates Octo as Medium success and Medium zero-shot capability; review highlights robust sim-to-real transfer after training on 4M+ trajectories but provides no numeric success percentages here.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Not quantified; review implies Octo relies on very large data scale (millions of trajectories) rather than sample-efficient fine-tuning.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this review.",
            "embedding_space_analysis": "Not reported in this review.",
            "action_grounding_evidence": "Uses diffusion-based trajectory generation to produce temporally coherent motor sequences; review cites improved stability and generalization from diffusion but no explicit language-to-action grounding probes are included here.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Large-pretraining scale and multi-embodiment training correlated with robust sim-to-real transfer in review; specific ablations (e.g., domain gap, visual realism) are not enumerated.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Rated Medium in Table 5 for zero-shot capability; no numeric results.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1934.4"
        },
        {
            "name_short": "DexVLA",
            "name_full": "DexVLA",
            "brief_description": "A vision-language-action model that uses plug-in diffusion 'expert' modules to adapt across different robot embodiments without fine-tuning.",
            "citation_title": "Dexvla: Visionlanguage model with plug-in diffusion expert for general robot control.",
            "mention_or_use": "mention",
            "model_name": "DexVLA",
            "model_description": "VLA architecture employing plug-in diffusion experts (specialized diffusion modules) to enable cross-embodiment adaptation; integrates vision and language encoders with modular diffusion action decoders.",
            "pretraining_type": "modular diffusion expert pretraining across multiple embodiments (review summarizes plug-in expert approach; exact corpora not detailed here).",
            "pretraining_data_description": "Trained on multi-embodiment trajectory sets to learn expert diffusion modules; review does not provide the detailed composition of the pretraining dataset.",
            "target_task_name": "robotic manipulation with cross-embodiment transfer",
            "target_task_description": "Language-conditioned manipulation tasks across different robot kinematics where plug-in diffusion experts produce suitable control trajectories; evaluated for transfer without task-specific fine-tuning.",
            "semantic_alignment": "Review describes DexVLA as enabling adaptation via specialized experts trained across embodiments, implying alignment between semantic tasks and embodiment-specific decoders; no quantitative semantic-overlap measures provided.",
            "performance_with_language_pretraining": "Table 5 shows DexVLA rated Medium success/Medium zero-shot; review highlights cross-embodiment adaptation without fine-tuning but gives no numeric success rates.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": "Adaptation without fine-tuning implies sample-efficiency benefits for new embodiments, but the review provides no numeric sample-efficiency comparison.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this review.",
            "embedding_space_analysis": "Not reported in this review.",
            "action_grounding_evidence": "Mechanism: plug-in diffusion experts map shared vision-language representations to embodiment-specific motor patterns; review reports functional cross-embodiment transfer but no probing of verb-to-affordance grounding.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Works when diffusion experts have been trained on diverse kinematics; review notes improved cross-embodiment adaptation but lacks detailed failure-mode analysis.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Rated Medium zero-shot capability in Table 5; no numeric detail.",
            "layer_analysis": "Not reported beyond modular expert design description.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1934.5"
        },
        {
            "name_short": "Pi-0",
            "name_full": "Pi-0",
            "brief_description": "A lightweight 3B-parameter VLA flow model designed for fast, low-latency general robot control with &gt;200 Hz inference and cross-embodiment generalization.",
            "citation_title": "Pi-0: A visionlanguage-action flow model for general robot control.",
            "mention_or_use": "mention",
            "model_name": "Pi-0",
            "model_description": "A low-latency (200+ Hz) vision-language-action flow model optimized for fast control loops; intended to generalize to new tasks and robot embodiments while operating at high control frequencies.",
            "pretraining_type": "foundation VLA pretraining with architecture optimized for runtime efficiency (review does not list precise pretraining corpus).",
            "pretraining_data_description": "Review references Pi-Cross-Embodiment / cross-embodiment datasets as associated training material but does not enumerate data content; emphasis on efficient models rather than data scale.",
            "target_task_name": "real-time robotic control and manipulation across embodiments",
            "target_task_description": "Low-latency control tasks requiring high-frequency inference (&gt;200 Hz) for reactive manipulation across different robot types; real-robot demonstrations indicated in review.",
            "semantic_alignment": "Review emphasizes runtime and embodiment generalization; semantic alignment specifics are not provided.",
            "performance_with_language_pretraining": "Table 5 classifies Pi-0 as Medium success/Medium zero-shot; review highlights &gt;200 Hz low-latency operation and generalization claims, but provides no numerical success rates.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Claims generalization to new embodiments with minimal adaptation, implying alignment of vision-language representations to action flows, but no mechanistic grounding analyses are provided in this review.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Designed for cross-embodiment generalization and low-latency settings; details of what conditions degrade transfer are not given.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Reported as Medium zero-shot in Table 5; no numeric details.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not reported.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1934.6"
        },
        {
            "name_short": "TLA",
            "name_full": "TLA (Tactile-Language-Action)",
            "brief_description": "A multimodal dataset and model family linking tactile signals with language and action to enable contact-rich insertion and assembly tasks with high success.",
            "citation_title": "Tla: Tactile-language-action model for contact-rich manipulation.",
            "mention_or_use": "mention",
            "model_name": "TLA (model / benchmark)",
            "model_description": "Integrates tactile/haptic streams with visual inputs and natural language instructions to train language-conditioned policies for contact-rich manipulation; includes a tactile-language-action dataset and specialized VLA models.",
            "pretraining_type": "multimodal pretraining incorporating tactile data aligned with language and action trajectories (dataset TLA contains tactile-language-action aligned demonstrations).",
            "pretraining_data_description": "TLA dataset: 30K contact-rich peg-in-hole demonstrations with synchronized tactile, vision, and language annotations enabling tactile-language-action alignment; contains fine-grained contact signals relevant to insertion tasks.",
            "target_task_name": "contact-rich assembly and insertion (peg-in-hole, precise insertion)",
            "target_task_description": "Contact-rich, high-precision assembly tasks with continuous action trajectories and haptic feedback; action space is continuous (trajectory/joint control) and data includes high-frequency tactile/force signals; evaluated on real robotic assembly tasks according to the review.",
            "semantic_alignment": "TLA explicitly aligns tactile signals with language instructions, providing strong modality-level alignment between action verbs (e.g., 'insert', 'press') and tactile affordances; the review highlights this alignment as a key contribution.",
            "performance_with_language_pretraining": "The review reports TLA models achieve '85%+ success on contact-rich tasks' (Table 5) when using tactile-language-action modeling; this is the explicit numeric performance noted in the survey.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not reported in this review; no attention maps or haptic-to-visual attention analyses are provided here.",
            "embedding_space_analysis": "Not reported; review does not present quantitative representational analyses for tactile-language embeddings.",
            "action_grounding_evidence": "Yes â€” the TLA benchmark and associated models provide explicit tactile-language-action alignment for grounding contact-rich verbs to haptic signatures and motor patterns; the review cites the 85%+ success as functional evidence of grounding.",
            "hierarchical_features_evidence": "Not reported in detailed representational terms; the dataset design implies both low-level haptic and higher-level task semantics are used.",
            "transfer_conditions": "Designed for contact-rich tasks; review does not quantify how models trained on TLA transfer to other contact tasks without fine-tuning.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Table 5 rates TLA zero-shot capability as High, but the review provides the explicit success rate (85%+) as in-distribution performance; zero-shot numeric details are not provided.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "The review emphasizes the added value of tactile modality versus vision-only datasets for contact-rich tasks but does not provide numeric head-to-head comparisons in this paper.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1934.7"
        },
        {
            "name_short": "PaLM-E",
            "name_full": "PaLM-E",
            "brief_description": "An embodied multimodal language model that conditions language reasoning on explicit hardware/embodiment descriptors to adapt across robot platforms.",
            "citation_title": "Palm-e: An embodied multimodal language model.",
            "mention_or_use": "mention",
            "model_name": "PaLM-E",
            "model_description": "Large multimodal embodied language model that fuses vision and language and encodes hardware/embodiment descriptors (hardware embeddings) to adapt reasoning and instructions to specific robot platforms.",
            "pretraining_type": "multimodal language + vision pretraining (embodied fine-tuning described in cited work); review summarizes PaLM-E's hardware-embedding strategy rather than listing full pretraining corpora.",
            "pretraining_data_description": "Pretraining involves large language corpora and vision-language data; review notes PaLM-E encodes explicit hardware embeddings to adapt to new platforms but does not provide full dataset composition here.",
            "target_task_name": "language-conditioned reasoning and robot instruction across embodiments",
            "target_task_description": "Tasks requiring language reasoning and adaptation to specific hardware descriptors to produce appropriate action plans; applies to embodied manipulation and instruction following.",
            "semantic_alignment": "Review cites PaLM-E's explicit hardware embeddings as a mechanism for aligning language reasoning with embodiment constraints; semantic alignment to objects/actions is implied but not quantified here.",
            "performance_with_language_pretraining": "Not reported in numeric detail in this review; PaLM-E is cited as an approach to improve cross-embodiment adaptation.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not analyzed in this review.",
            "embedding_space_analysis": "Review mentions hardware embeddings conceptually but does not provide representational analyses.",
            "action_grounding_evidence": "Conceptual evidence: hardware embeddings help map high-level language reasoning into embodiment-appropriate actions; no operational grounding metrics are provided in this review.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Designed to improve transfer across robot embodiments by conditioning on hardware descriptors; quantitative conditions of success/failure not provided here.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Not quantified in this review.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported.",
            "comparison_to_vision_only": "Not provided in this review.",
            "temporal_dynamics": "Not reported.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1934.8"
        },
        {
            "name_short": "Diffusion Policy",
            "name_full": "Diffusion Policy (and Diffusion Transformer Policy)",
            "brief_description": "A family of methods that formulate visuomotor control as a conditional denoising (diffusion) process to generate temporally coherent continuous action trajectories for manipulation.",
            "citation_title": "Diffusion policy: Visuomotor policy learning via action diffusion.",
            "mention_or_use": "mention",
            "model_name": "Diffusion Policy / Diffusion Transformer Policy",
            "model_description": "Action decoder approach using diffusion models (or diffusion transformers) that iteratively denoise latent trajectories conditioned on visual/language/state tokens to produce smooth, temporally-consistent control sequences.",
            "pretraining_type": "trained on trajectory datasets; technique complements vision-language encoders rather than being a vision-language pretraining itself.",
            "pretraining_data_description": "Trained on demonstration trajectory corpora (visuomotor trajectories with associated observations and optionally language); review references diffusion-based policies applied at scale (e.g., Octo, DexVLA).",
            "target_task_name": "continuous trajectory generation for robotic manipulation / visuo-lingual action generation",
            "target_task_description": "Generates continuous control trajectories for manipulation tasks (grasping, insertion, assembly, general manipulation) with temporal coherence; used in both simulated and real-robot settings per cited works in review.",
            "semantic_alignment": "Diffusion policies condition on vision-language embeddings to realize semantic alignment of actions; review emphasizes improved motion smoothness and stability but does not report numeric semantic-overlap measures.",
            "performance_with_language_pretraining": "Review reports diffusion-based decoders improve temporal smoothness and generalization (cited works), and that Octo trained with diffusion achieved robust sim-to-real transfer on millions of trajectories; no single-number metric is given here.",
            "performance_without_language_pretraining": null,
            "sample_efficiency_comparison": null,
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not provided in the review for diffusion policies specifically.",
            "embedding_space_analysis": "Not provided in the review.",
            "action_grounding_evidence": "Functional evidence: diffusion decoders produce temporally coherent trajectories conditioned on semantic embeddings, improving practical grounding of language-conditioned commands into motor sequences; mechanistic grounding analyses are not included here.",
            "hierarchical_features_evidence": "Not reported.",
            "transfer_conditions": "Review notes diffusion policies can struggle with real-time latency; also noted that large-scale pretraining improves transfer, but specific conditions are not quantified.",
            "novel_vs_familiar_objects": "Not reported.",
            "zero_shot_or_few_shot": "Used within models that show varying zero-shot abilities (e.g., Octo, DexVLA), but diffusion policy itself is not reported with standalone zero-shot numbers in this review.",
            "layer_analysis": "Not reported.",
            "negative_transfer_evidence": "Not reported explicitly; review mentions latency and runtime constraints as practical limitations.",
            "comparison_to_vision_only": "Not directly compared in this review.",
            "temporal_dynamics": "Review emphasizes diffusion yields temporally coherent action sequences; no quantified temporal-learning dynamics provided.",
            "dimensionality_analysis": "Not reported.",
            "uuid": "e1934.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Rt-1: Robotics transformer for real-world control at scale.",
            "rating": 2
        },
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control.",
            "rating": 2
        },
        {
            "paper_title": "Cliport: What and where pathways for robotic manipulation.",
            "rating": 2
        },
        {
            "paper_title": "VIMA: General robot manipulation with multimodal prompts.",
            "rating": 2
        },
        {
            "paper_title": "Diffusion policy: Visuomotor policy learning via action diffusion.",
            "rating": 2
        },
        {
            "paper_title": "Pi-0: A visionlanguage-action flow model for general robot control.",
            "rating": 1
        },
        {
            "paper_title": "Dexvla: Visionlanguage model with plug-in diffusion expert for general robot control.",
            "rating": 1
        },
        {
            "paper_title": "Tla: Tactile-language-action model for contact-rich manipulation.",
            "rating": 2
        },
        {
            "paper_title": "Palm-e: An embodied multimodal language model.",
            "rating": 2
        },
        {
            "paper_title": "Octo: An open-source generalist robot policy",
            "rating": 1
        }
    ],
    "cost": 0.027493999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Vision Language Action Models in Robotic Manipulation: A Systematic Review
14 Jul 2025</p>
<p>Muhayy Ud Din 
Khalifa University Center for Autonomous Robotic Systems (KUCARS
Khalifa University United Arab Emirates. b Institute of Industrial and Control Engineering (IOC) Universitat Politecnica de Catalunya Spain</p>
<p>Waseem Akram 
Khalifa University Center for Autonomous Robotic Systems (KUCARS
Khalifa University United Arab Emirates. b Institute of Industrial and Control Engineering (IOC) Universitat Politecnica de Catalunya Spain</p>
<p>Lyes Saad Saoud 
Khalifa University Center for Autonomous Robotic Systems (KUCARS
Khalifa University United Arab Emirates. b Institute of Industrial and Control Engineering (IOC) Universitat Politecnica de Catalunya Spain</p>
<p>Jan Rosell 
Khalifa University Center for Autonomous Robotic Systems (KUCARS
Khalifa University United Arab Emirates. b Institute of Industrial and Control Engineering (IOC) Universitat Politecnica de Catalunya Spain</p>
<p>Irfan Hussain irfan.hussain@ku.ac.ae 
Khalifa University Center for Autonomous Robotic Systems (KUCARS
Khalifa University United Arab Emirates. b Institute of Industrial and Control Engineering (IOC) Universitat Politecnica de Catalunya Spain</p>
<p>Vision Language Action Models in Robotic Manipulation: A Systematic Review
14 Jul 202533A68014ACC115E11EE0C4DF66730158arXiv:2507.10672v1[cs.RO]Vision language models Robotic and embodied control Foundation models Language-conditioned manipulation and control
Vision Language Action (VLA) models represent a transformative shift in robotics, with the aim of unifying visual perception, natural language understanding, and embodied control within a single learning framework.This review presents a comprehensive and forward-looking synthesis of the VLA paradigm, with a particular emphasis on robotic manipulation and instruction-driven autonomy.We comprehensively analyze 102 VLA models, 26 foundational datasets, and 12 simulation platforms that collectively shape the development and evaluation of VLAs models.These models are categorized into key architectural paradigms, each reflecting distinct strategies for integrating vision, language, and control in robotic systems.Foundational datasets are evaluated using a novel criterion based on task complexity, variety of modalities, and dataset scale, allowing a comparative analysis of their suitability for generalist policy learning.We introduce a two-dimensional characterization framework that organizes these datasets based on semantic richness and multimodal alignment, showing underexplored regions in the current data landscape.Simulation environments are evaluated for their effectiveness in generating large-scale data, as well as their ability to facilitate transfer from simulation to real-world settings and the variety of supported tasks.Using both academic and industrial contributions, we recognize ongoing challenges and outline strategic directions such as scalable pretraining protocols, modular architectural design, and robust multimodal alignment strategies.This review serves as both a technical reference and a conceptual roadmap for advancing embodiment and robotic control, providing insights that span from dataset generation to real world deployment of generalist robotic agents.For reference, a public repository summarizing VLA models, datasets, and simulators can be found at: https://github.com/Muhayyuddin/VLAs</p>
<p>Introduction</p>
<p>The integration of vision, language, and action into a unified framework has emerged as a paradigm-shifting approach in robotics and embodied artificial intelligence.Traditionally, robotic systems that depend on task-specific programming struggle in dynamic and unstructured environments.In contrast, VLA models aim to utilize the generalization capabilities of large-scale foundation models to enable robotic systems that can understand instructions in natural language, perceive their surroundings, and perform complex tasks autonomously (Brohan et al., 2022;Ahn et al., 2022;Jiang et al., 2022;Team et al., 2024).The fundamental concept of this paradigm is transformer architecture, which has transformed natural language processing and vision with self-attention mechanisms and large-scale pretraining.Models like GPT (Brown et al., 2020), BERT (Devlin et al., 2018), ViT (Dosovitskiy et al., 2020), and CLIP (Radford et al., 2021) demonstrate that massive datasets and parameter scales can produce remarkable generalizability and robustness.These insights have led to new architectures that fuse vision and language into robotics control policies, which step forward towards generalist agents such as RT-1 (Brohan Figure 1: VLA models, datasets, and contributing institutions from 2022 to 2025.The top row presents major VLA models introduced each year, alongside their associated institutions (logos within red boxes).The bottom row displays key datasets used to train and evaluate these models, grouped by release year.The figure highlights the increasing scale and diversity of datasets and institutional involvement, with contributions from academic (e.g., CMU, CNRS, UC, Peking Uni) and industrial labs (e.g., Google, NVIDIA, Microsoft).This timeline highlights the rapid advancements in VLA research.et al., 2022), SayCan (Ahn et al., 2022), VIMA (Jiang et al., 2022), and Octo (Team et al., 2024).The trend of developing such models is continuously growing as depicted in Fig 1.</p>
<p>The development of effective VLA models is fundamentally dependent on the availability of large-scale, diverse, and multi-model datasets, together with realistic simulation platforms.These elements are essential for training models that can robustly understand language instructions, perceive visual environments, and generate meaningful action sequences.For instance, the Open X-Embodiment (Collaboration et al., 2025) dataset unifies data from 22 robot embodiments and more than 500 tasks using a shared action space.It enables the pre-training of foundation models like RT-1-X, significantly enhancing cross-robot generalization.Similarly, the DROID dataset (Khazatsky, 2024) uses internet-scale data, combining human-annotated language with robotic video demonstrations for scenes with complex manipulations.These datasets significantly advance the data ecosystem for training and benchmarking VLAs.Comprehensive datasets enable the learning of diverse tasks in both household and industrial contexts.These datasets offer rich annotations, including demonstration trajectories, object state transitions, and diverse natural language prompts.However, real-world data collection is labor-intensive, expensive, and limited in diversity, which highlights the importance of simulation.</p>
<p>Simulation environments allow data generation to be scaled across a wide range of settings, object types, lighting conditions, and agent embodiments.Platforms such as Habitat (Savva et al., 2019), Isaac Gym (Makoviychuk et al., 2021), and RoboSuite (Zhu et al., 2020) offer programmable and photorealistic environments with physics-based interactions, facilitating both imitation and reinforcement learning paradigms.More recently, tools such as iGibson (Xia et al., 2020) and AI2-THOR (Kolve et al., 2017) have added support for human-centric indoor environments with naturalistic object arrangements, enhancing semantic realism.Simulation also enables automatic generation of multimodal annotations, such as action trajectories, object states, and natural language instructions that are crucial to align visual, linguistic, and motor modalities.Recent efforts also emphasize the importance of synthetic language generation aligned with task semantics (e.g., VLN-CE (Krantz et al., 2020), ALFRED (Shridhar et al., 2020)) to ensure linguistic diversity and instruction complexity.The integration of simulation and large-scale synthetic datasets is therefore crucial for building VLA systems that are robust, scalable, and applicable to real-world deployment.</p>
<p>Since the field matures at a rapid pace, numerous architectures, datasets, and frameworks are being proposed in a fast order.Despite the growing body of research, there remains a gap in the literature for a comprehensive and systematic synthesis that organizes and categorizes the architectural foundations, benchmark datasets, simulation platforms, and evaluation protocols that collectively shape the current VLA landscape.It is critically needed to contextualize the state-of-the-art and identify emerging patterns, limitations, and opportunities.This study will serve as both a technical reference and a conceptual roadmap to accelerate research in embodied foundation models and generalist robotic intelligence.</p>
<p>Contributions:</p>
<p>This systematic review makes the following key contributions to the field of VLA models in robotics:</p>
<p>â€¢ Structured taxonomy of VLA architectures: We present a detailed classification of VLA model architectures, organizing them into key categories according to their distinct methods for integrating perception, natural language understanding, and embodied control.</p>
<p>â€¢ A novel quantitative benchmarking for VLA datasets:</p>
<p>We introduce a novel quantitative framework designed to systematically benchmark VLA datasets, using metrics that are empirically adjusted for task complexity (îˆ¯ task ) and modality richness (îˆ¯ mod ).This benchmarking approach generates a two-dimensional graph (Fig. 10) that efficiently represents current datasets, allowing us to pinpoint significant gaps, particularly the lack of datasets that combine very high levels of task complexity with comprehensive multimodelities.</p>
<p>â€¢ In-depth review of simulation platforms: We review the most critical simulation platforms used in VLA research, focusing on their distinct sensory modalities, applications and their essential role in facilitating large-scale and reproducible VLA experiments and data generation.</p>
<p>â€¢ Identification of challenges and future roadmap: We identify persistent challenges facing VLA model development and articulate a clear roadmap for future research directions, emphasizing key areas such as architectural modularity, scalable data generation strategies, advancing dynamic simulation through differentiable contact modeling, and unified languagegrounding APIs to enable robust and scalable realworld deployment.</p>
<p>The rest of the paper is organized as illustrated in Fig. 2. Sec. 2 outlines the literature search and selection criteria used to collect the most relevant and representative VLA systems.Sec. 3 introduces core background concepts including transformers, vision transformers, large-language models, and vision-language models, which collectively form the foundation of modern VLA architectures.Sec. 4 presents an in-depth review of VLA model architectures, highlights state-of-the-art systems, and identifies common architectural trends.Sec. 5 lists and analyzes the main training datasets, discussing dataset formats, benchmarking protocols, and evaluation metrics.Sec.6 reviews the simulation platforms and tools used for training and testing VLA agents.Sec.7 outlines practical application domains, describes the criteria used for selecting representative VLA models for comparison, and details evaluation protocols and key findings.Sec. 8 discusses key challenges and future directions, categorizing them into architectural, dataset, and simulation challenges, and outlines open problems and opportunities for future research.Finally, Sec. 9 summarizes the conclusions and offers a roadmap for advancing VLA-driven robotic autonomy.</p>
<p>Literature Search and Selection Criteria</p>
<p>We conducted an extensive search through IEEE Xplore, Elsevier, Springer Nature, MDPI, Wiley, and arXiv to identify work on VLA models, VLA datasets, and simulation tools for robotic simulation and data generation.To capture each aspect, we developed sets of keywords; for VLA models: "vision language action" OR VLA OR (vision AND language AND action) OR Vision Language Models for Robotic manipulation" "Multimodal Robotic Control", "Vision-Language Grounding", "Visuomotor Transformer", "End-to-End Robot Learning".The keywords for Training datasets are "VLA dataset" OR "manipulation dataset" OR "embodied AI dataset for manipulation".Finally for Simulation tools: "simulator for robotic manipulation" OR "embodied AI data-generation simulator" OR "robotic manipulation simulator" OR "realistic dynamic simulation for robotic manipulation" We applied the following inclusion criteria to identify original research: 1) Proposes or evaluates a VLA model, a VLA dataset, or a simulator for robotic manipulation or manipulation data generation.2) Presents a novel model, dataset, or a new simulator.</p>
<p>To ensure comprehensive coverage, we complement traditional database searches with conversational queries in a large-language model (e.g.GPT), using targeted prompts per thematic area.For VLA models, we asked GPT to "List vision-language-action models published between 2022 and 2025," "List end-to-end transformer-based VLA architectures", "List VLA methods that use diffusion-based action decoders", "List modular fusion framework VLA models".For datasets, prompts included "List of VLA datasets released between 2022 and 2025", "List manipulation datasets used for VLM training" and "List embodied AI datasets for robotics" and "List of well-known manipulation datasets".For simulation tools, we queried "List simulators for robotic manipulation data generation," "List simulation environments for embodied AI dataset creation," "List robotic manipulation dataset generation simulators" and "List robotic dynamic simulators".We then merged these lists with our database results, removed duplicates, and performed manual validation to arrive at the final set of models, datasets, and simulators included in this review.</p>
<p>We also included arXiv e-prints (https://arxiv.org/) in our search because the field of VLAs has recently begun to mature, and most breakthroughs and novel architectures have appeared as preprints in recent times.Fig. 3 shows the VLA (in green) and dataset (in purple) counts per year used in this work.We thoroughly examined the preprints and included only those that make substantial contributions to the field.The integration of preprints ensures that we capture the very latest models, datasets, and simulation tools as soon as they emerge, giving a more accurate and up-todate picture of this rapidly evolving field.</p>
<p>Background Concepts</p>
<p>This section outlines the core architectures behind VLA models.We start with the transformer, a model that has transformed both language and vision tasks.We then cover Vision Transformers (ViTs), which apply self-attention to image patches for visual feature extraction.The subsequent category comprises Large Language Models (LLMs), which are transformer-based models trained on large text datasets that perform reasoning, instruction following, and zeroshot tasks.Finally, we will provide an overview of vision language models (VLMs), which fuse visual and textual data through cross-modal attention to ground instructions in robotic actions.These components form the backbone of modern VLA architectures, detailed in the following subsections.</p>
<p>Transformer</p>
<p>Transformers are a class of deep learning architectures introduced by Vaswani et al. (Vaswani et al., 2017), these models have revolutionized the fields of natural language processing and computer vision by enabling greater parallelization and scalability in sequence modeling, i.e. in the processing and learning of patterns from sequences of data.</p>
<p>The core of Transformers lie in the self-attention mechanism, detailed below, which let each token in a sequence weigh and combine information from all other tokens to build a contextaware representation.</p>
<p>The Transformer architecture (depicted in Fig 4) comprises three differentiated parts: the embedding layer, the encoder stack, and the decoder stack, which includes the final output projection and softmax layer.They are detailed in the following subsections, after presenting the self-attention mechanism.</p>
<p>Self-Attention</p>
<p>The self-attention mechanism allows each token in a sequence to attend to all other tokens when computing its representation.To achieve this, the model first transforms each input token into three distinct vector representations: queries (), keys (), and values ( ).These vectors are obtained through learned linear projections of the input embeddings and serve different roles in the attention computation: the query represents what the current token is looking for, the key indicates what information each token provides, and the value contains the actual content to be shared.By comparing queries to keys, the model determines attention weights, which are then used to aggregate the values into a new context-aware representation for each token.</p>
<p>This process is formalized by the scaled dot-product attention mechanism, which computes the dot product between queries and keys, scales the result by âˆš   (where   is the dimension of the keys), applies a softmax function to obtain attention weights, and finally combines these weights with the values to produce the output:
Attention(ð‘„, ð¾, ð‘‰ ) = softmax ( ð‘„ð¾ âŠ¤ âˆš ð‘‘ ð‘˜ ) ð‘‰ (1)
Building upon this mechanism, multi-head attention extends the model's capacity by enabling it to attend to information from multiple representation subspaces simultaneously.Instead of computing attention once with a single set of projection matrices, the model uses â„Ž parallel attention layers, or heads, each with its own learned weight matrices    ,    , and    .These matrices independently project the input into â„Ž different subspaces, allowing each head to focus on different aspects of the input relationships:
MultiHead(ð‘„, ð¾, ð‘‰ ) = Concat(head 1 , â€¦ , head â„Ž )ð‘Š ð‘‚ (2)
where head  = Attention(   ,    ,     ) (3)</p>
<p>The outputs from all heads are concatenated and passed through a final linear projection using   to produce the result of the multi-head attention module.</p>
<p>Embeddings</p>
<p>Embedding refers to continuous vector representations that map discrete inputs such as words, image patches, or action tokens into a dense latent space.In Transformer architectures, embeddings serve as the foundational interface between symbolic input (for example, textual instructions) and differentiable computation.Input embeddings enable the The encoder processes input embeddings through layers of multi-head attention, normalization, and feedforward networks.The decoder mirrors this with additional masked attention layers and incorporates encoder outputs for contextual decoding.The magnified view illustrates the scaled dot-product attention and how multiple attention heads are concatenated and linearly transformed to form the final multi-head attention output.The image is adapted from (Vaswani et al., 2017) model to process structured data in a unified format, while output embeddings decode latent representations into action or token spaces.Since the Transformer architecture lacks recurrence, a set of sinusoidal positional encodings is added to the input embeddings to provide information about the order of elements in a sequence.These learned representations capture semantic, spatial, or temporal relationships that are essential for generalization and reasoning in multimodal tasks (Vaswani et al., 2017;Dosovitskiy et al., 2021;Press and Wolf, 2017).</p>
<p>Encoder</p>
<p>The encoder stack consists of  identical layers, where each layer has a multi-head self-attention mechanism followed by a position-wise feedforward network.Both sublayers are equipped with residual connections (which add the input of a sub-layer to its output to help preserve information and ease optimization) and layer normalization (which normalizes activations to improve training stability).In the first layer, the input embeddings provide the queries, keys, and values.In deeper layers, these are derived from the output of the preceding layer.</p>
<p>Decoder</p>
<p>The decoder stack also consists of  identical layers, but with a slightly different architecture.Each layer contains three sub-layers: a masked multi-head self-attention layer, a multi-head encoder-decoder attention layer, and a feedforward network.The masked self-attention prevents a position from attending to subsequent tokens, ensuring that the model generates output in an autoregressive manner.The encoderdecoder attention enables the decoder to query the encoder's outputs, integrating source information into the generation process.Like the encoder, each sub-layer in the decoder is followed by residual connections and layer normalization.</p>
<p>The final output of the decoder is passed through a linear transformation and a softmax function to produce a probability distribution over all possible output tokens, enabling the model to predict the next token in the target sequence one step at a time.</p>
<p>Vision Transformers</p>
<p>The Vision Transformer extends the Transformer architecture to visual domains by treating image patches as input tokens (Dosovitskiy et al., 2020).As shown in Fig 5, an image is split into a sequence of non-overlapping patches, each of which is linearly projected into a fixed-dimensional embedding.These embeddings are then augmented with learnable positional encodings and passed through a standard Transformer encoder.A classification token is appended, and its final representation is used for prediction through a multi-layer perceptron head (MLP).ViTs achieve competitive performance on benchmarks like ImageNet (Deng et al., 2009), highlighting the power of attention in learning global visual representations.</p>
<p>Large Language Models</p>
<p>LLMs are transformer architectures (Fig. 4) trained on large-scale text data.These models are typically classified The model outputs a natural language caption that describes the semantic content of the image, enabling tasks such as captioning, question answering, and visual grounding.the image is adapted from (Xiao et al., 2024) into three architectural types: encoder-only, decoder-only, and encoder-decoder.Each structure is optimized for different types of task in natural language processing and robotics applications.</p>
<p>Encoder-only models, such as BERT, RoBERTa, and DeBERTa, utilize bidirectional self-attention mechanisms to learn deep contextual relationships from both the left and right of a token (Ghojogh et al., 2024;Zayyanu et al., 2024).These models are trained using masked language modeling (MLM) objectives and are well-suited for text classification, semantic similarity, and question answering.Their strength lies in representing sentence-level meaning rather than in generating sequential text.</p>
<p>Decoder-only models, such as GPT-3, GPT-4, PaLM, and LLaMA, operate sequentially by predicting the next token in a sequence based only on previous tokens (Ghojogh et al., 2024).This unidirectional architecture is optimized for text generation, dialogue, summarization, and other openended generative tasks.</p>
<p>Encoder-decoder models, also known as sequence-tosequence architectures, such as T5, BART, and the original Transformer, include an encoder to represent the input and a decoder to produce the output (Liu et al., 2021;BrandiÅ¡auskas et al., 2023;Ghojogh et al., 2024).These are ideal for tasks where full input processing is required before output generation, including machine translation, summarization, and code generation.Recent applications in robotics also use encoder-decoder models for instruction grounding and language-conditioned action planning.</p>
<p>Vision Language Models</p>
<p>VLMs use the synergy between computer vision and natural language processing to perform tasks such as image captioning, visual question answering, cross-modal retrieval, and instruction grounding.Like LLMs, they are built on the Transformer architecture, using either dual encoders (e.g., CLIP), unified encoder-decoder frameworks (e.g., BLIP, Flamingo), or sequence-to-sequence stacks.During training, they typically align visual and textual inputs using objectives such as contrastive learning (matching image-text pairs), masked modeling (predicting masked tokens or regions), or learning to generate captions from images.</p>
<p>Fig. 6 illustrates a typical VLM designed for image captioning using a transformer-based encoder-decoder architecture.The process begins with an input image, which is divided into patches and embedded via a visual encoder, such as a (ViT) (Mishra et al., 2024;Lam et al., 2023).A language prompt, e.g., "Describe the image", is also tokenized and embedded, either directly or via lightweight text guidance.The visual tokens and prompt embeddings are passed through a Transformer encoder-decoder stack.The encoder captures spatial and semantic relationships from the image, while the decoder generates output tokens autoregressively.This allows the model to produce a fluent and contextually accurate caption, as shown in Fig. 6.This architecture enables semantic-level reasoning over visual input and has shown strong performance on benchmark datasets such as Flickr8k and Flickr30k (Mishra et al., 2024;Abdel-Hamid et al., 2024).</p>
<p>Recent studies have demonstrated improved caption quality through the use of advanced modules or architectural variants within the encoder-decoder stack, such as Swin Transformers, T5-based decoders, or GPT-based language models, depending on the design (Lam et al., 2023;Mishra et al., 2024;Fouad et al., 2024).These approaches are widely used in robotic perception systems where visual scene understanding must be paired with natural language generation or instruction.</p>
<p>Vision Language Action Models</p>
<p>VLA models represent a new frontier in robotic intelligence, enabling robots to perceive visual environments, understand natural language instructions, and execute grounded actions accordingly.These models bridge the semantic gap between multimodal inputs, such as images, sensor data, human commands, and low-level robotic control.VLA architectures are particularly relevant for unstructured and dynamic environments, where traditional rule-based programming is infeasible.They empower robots to generalize tasks such as object manipulation, navigation, and interaction by using deep learning, representation alignment, and sequential decision making (Wang et al., 2024a;Guruprasad et al., 2024;Wang et al., 2024d).</p>
<p>VLA Architecture</p>
<p>The VLA architecture illustrated in Fig. 7 represents an end-to-end framework that is a representative of the leading VLA systems such as; RT-2 (Zitkovich et al., 2023), Open-VLA (Driess et al., 2024), CLIP-RT (Kang et al., 2025), Octo (Team et al., 2024), and RT-1 (Brohan et al., 2022), all of which employ transformer-based vision and language backbones fused through cross-modal attention.These are encoded respectively through visual, text, and state encoders.The resulting embeddings are passed to an LLM that fuses multimodal information and generates a semantic representation of the intended task.This representation, along with robot state features, is processed by an Action Decoder implemented as a diffusion transformer to generate a trajectory that accomplishes the commanded task.</p>
<p>The architecture unifies three parallel encoder streams: visual, linguistic, and proprioceptive, into a single transmission diffusion backbone that directly generates control commands.First, a transformer-based Visual Encoder (e.g.ViT (Dosovitskiy et al., 2020), DINOv2 (Caron et al., 2021)) processes raw RGB (depth/semantic) images of the workspace and produces a sequence of fixed-length feature tokens.In parallel, a pre-trained Language Encoder (for example, PaLM (Chowdhery et al., 2022) or LLaMA (Touvron et al., 2023)) tokenizes and embeds natural language instructions, whether high-level goals (e.g., "Put bowl, apple, and banana on plate.")or detailed stepwise instructions, in the same -dimensional space.Similarly, State Encoder embeds the proprioceptive and kinematic state of the robot (joint angles, pose of the end effector, gripper status) through an MLP or small transformer into additional tokens, allowing the model to reason about reachability, collision avoidance, and feedback correction.</p>
<p>All tokens are concatenated and passed into a transformerbased model that produces action embeddings.This model may implement either a diffusion policy, using a Diffusion Transformer that iteratively denoises a noisy latent trajectory (e.g., as in Diffusion Policy (Chi et al., 2023a) or VLAFlow (Black et al., 2024b)), or a direct policy, which predicts the embeddings in a single pass without diffusion.At inference time, the action embeddings are converted into continuous control signals, such as end-effector velocities or joint torques, either through a lightweight output head or by completing the full diffusion sampling process.In some implementations, the embeddings can also be decoded into imagined next-frame images, enabling an "imagineand-verify" loop for closed-loop execution.</p>
<p>Models such as OpenVLA and Octo further incorporate proprioceptive tokens, while several systems (e.g., Per-Act (Shridhar et al., 2022b), Helix (Team, 2025a)) support real-time feedback loops for continual correction.The rapid evolution and plug-and-play modularity of these architectures, where one can swap in a stronger ViT, a larger language model, or a more expressive diffusion sampler are driving to a new direction of instruction-driven autonomy for generalist robotic systems.</p>
<p>State-of-the-Art VLA Models</p>
<p>Table 1 is organized to provide a brief yet thorough overview of over a hundred VLA models related to robotic manipulation and instruction-driven autonomy.The first two columns detail the name and year of publication of each model.The next two columns End-to-End and Component Focused flag whether a model learns a direct mapping from raw visual and language input to control commands or instead concentrates on developing individual building blocks (for example, a better vision backbone or a more effective action sampler).The Main Contributions column then summarizes each work's core innovation, whether it introduced a novel fusion architecture, demonstrated a new training paradigm, or achieved state-of-the-art performance on a benchmark.</p>
<p>Each VLA model is based on four essential components: the training dataset, which provides foundational real-world task demonstrations or simulated episodes; the vision encoder, responsible for converting raw images or depth data into detailed feature maps; the language encoder, which maps instructions or annotations into a shared latent space; and the action decoder, which integrates these multimodal embeddings to produce the actual robot instructions, whether they are joint trajectories, discrete tokens, or overarching motion primitives.In Table 1, the final column specifies which dataset each model was trained on, which vision backbone it uses (e.g.CLIP-ViT, ResNet, Efficient-Net), which text encoder it employs (e.g.T5, LLaMA, CLIP text), and what kind of action decoder it relies upon (e.g.Transformer head, diffusion policy, CVAE sampler), making it straightforward to compare how different architectures assemble these building blocks.</p>
<p>Architectural Trends</p>
<p>Fig. 8 presents a comprehensive taxonomy of VLA model components, structured around three interconnected modules: vision encoders, language encoders, and action decoders.Within the vision encoder family, several prominent approaches exist.CLIP and SigLIP-based encoders are popular for their strong visual-text alignment through contrastive learning and are utilized in models such as CLIPort, RevLA, and Edge VLA.Other ViT variants such as DINOv2 and Qwen2 VIT, are used in models like Gato, Octo, HybridVLA, and Chain-of-Affordance for their ability to model long-range spatial dependencies and high-level visual semantics.CNN-based encoders such as ResNet and EfficientNet appear in models like CLIPort, ACT, RT-1, and QUAR-VLA.</p>
<p>Table 1: Comprehensive survey of VLA models developed for robotic manipulation and instruction-driven autonomy.The table lists each model's name, publication year, indicates whether it operates in an end-to-end fashion (mapping raw visual and language inputs directly to actions) or focuses on individual components (vision, language, or action modules), and summarizes its main technical contributions.The final column details the primary training datasets and core model components-including vision encoders, language encoders, and action decoders used by each method.Language encoders show similar architectural diversity.LLaMA and Vicuna-based language encoders are widely used in models such as RevLA, OpenVLA, and HybridVLA for instruction understanding and zero-shot reasoning.T5style models appear in VIMA, Octo, and FLARE, offering flexible encoder-decoder structures for sequence generation.GPT-based and Qwen-based encoders, such as those used in VoxPoser, Edge VLA, and DexVLA, balance generalization and compact deployment.Gemma-2B language encoders are found in Pi-0 and FAST, while specialized solutions like CLIP Text encoders are used in CLIPort and PerAct for minimal alignment tasks.</p>
<p>In action decoders, diffusion-based transformers are a leading choice for models like Octo, HybridVLA, and Dex-GraspVLA, as they offer fine-grained, temporally smooth control via iterative denoising.Autoregressive Transformer heads, such as those in Gato, OpenDrive VLA, and GRAPE, generate action sequences step-by-step, optimizing real-time responsiveness.Several models including VoxPoser, LMM Planner Integration, and FLARE embed Model Predictive Control or specialized planning heads to support decisionmaking in dynamic tasks.MLP or token predictor heads are used in OpenVLA, TraceVLA, and RoboMamba for efficient low-level control.Our evaluation of VLA architectures shows a wide variety of algorithms in each core component.For the vision encoder, the majority of models used CLIP and SigLIPbased ViT backbones to extract rich, semantically aligned visual features.In the language domain, the LLaMA family (including LLaMA-2 and its larger variants) is most commonly used, appearing in several models as the primary text encoder.When it comes to action decoding, diffusion-based Transformer heads are the most popular choice with their ability to model complex, multimodal action distributions.On the data side, while many teams rely on privately collected manipulation demonstrations, the publicly available Open X-Embodiment dataset is the most commonly used.</p>
<p>VLA Training Datasets</p>
<p>The foundation of VLA models lies in high quality, diverse training datasets.These datasets are crucial as they expose models in the complete range of real and simulated environments, ensuring a tight alignment of visual elements, natural language instructions, and control (James et al., 2020).These datasets allow VLAs to learn complex crossmodal correlations, such as how language complexities (e.g., 'gently place') affect motion smoothness without relying on manually prepared heuristics.We first introduce the unified dataset schema that underlies the VLA training pipelines, then survey the most influential public datasets, and finally apply a comprehensive benchmarking strategy to assess the scale, modality coverage, and complexity of each dataset.</p>
<p>Dataset Format</p>
<p>Structured overview of the general dataset format commonly used in VLA training pipelines.is illustrated in Fig. 9.It highlights the systematic organization of multimodal data into three primary streams: visual, language, and action/control, which collectively facilitate the training and evaluation of VLA models.</p>
<p>The Visual Streams comprise raw RGB frames, video snippets, and optionally, depth maps and segmentation masks.These visual inputs provide essential spatial and contextual data to perception modules in VLA architectures.Typically, the data in this stream is stored in standard image or video formats like JPEG or PNG for individual frames and MP4 or similar formats for video snippets.</p>
<p>The Language Streams incorporate natural-language instructions or dialogues alongside tokenization metadata (such as token offset).These textual annotations are essential for instructing and conditioning robotic actions and are commonly stored in lightweight, structured formats such as JSON or plain text files.The presence of tokenization metadata facilitates efficient textual processing, enabling direct integration with transformer-based language models.</p>
<p>The Action/Control Labels include discrete action tokens (e.g.categorical commands like "move forward" or "turn left") and continuous control vectors representing joint positions or end-effector trajectories.These action labels provide explicit supervision signals for model output and are typically stored as NumPy arrays or encoded within structured data containers.</p>
<p>All three modality streams are systematically integrated into standardized episode-level directories (e.g., episode/), where visual data reside in subdirectories such as rgb/ and depth/, accompanied by lang.json,actions.npy,and states.npy.Each episode folder can then be serialized in formats like: JSON for lightweight, human-readable metadata; TFRecord/TF-Example for high-throughput, sharded training; or HDF5 for efficient random access to synchronized frames, actions, and state arrays, enabling balance readability, I/O performance, and scalability in their training pipelines.</p>
<p>Major VLA Datasets</p>
<p>Table 2 summarizes the progress of VLA datasets, highlighting how each dataset advances autonomy by varying in scale, modality, and task complexity.Early collections such as EmbodiedQA and R2R focus on discrete decision making in constrained environments, offering simple state-action mappings suitable for evaluating baseline policy architectures (e.g., PACMAN, Speaker-Follower).As we move into 2020-2022, datasets like ALFRED, RLBench, and CALVIN introduce longer-horizon tasks and richer sensory streams combining RGB, depth, proprioception, and natural language instructions to stress test hierarchical planning and subgoal decomposition methods (e.g., C2F-ARM, VIMA, RT-2).These mid-generation datasets bridge the gap between symbolic planners and end-to-end learning, enabling comparative analyses of model-based control versus learned policies under simulated dynamics.(Mees et al., 2022) 5,000+ demonstrations Long-horizon, language-conditioned robotic manipulation tasks HDF5 archives with synchronized RGB-D frames, proprioception, action sequences, and natural language instructions.DROID (Khazatsky, 2024) 76k demonstrations; 564 scenes, 86 tasks High-diversity language-conditioned robot manipulation RLDS-formatted RGB-D data, stereo video, camera calibrations, language annotations, and robot state/action logs.Open X-Embodiment (Collaboration et al., 2025) 1M+ trajectories, 500+ skills, 22 robot types Large-scale, multi-embodiment, multiskill manipulation dataset Sharded TFRecords with RGB/depth frames, language instructions, action vectors; YAML metadata and RLDS format.RoboSpatial (Song et al., 2025a) 1M images, 5K 3D scans, 3M spatial QA pairs 2D-3D paired spatial reasoning dataset RGB images, 3D scans, and relational graph annotations in support of spatial understanding benchmarks.CoVLA (Arai et al., 2024) 83.3 h real-world driving video, 6M frames</p>
<p>Time-aligned vision-language-action dataset</p>
<p>Synchronized RGB video, GPS/IMU-based trajectories, and auto-generated captions using rule-based and VLM methods.TLA (Hao et al., 2025) 30K contact-rich peg-in-hole demonstrations</p>
<p>Tactile-language-action alignment for precise insertion and assembly ROS bag files with synchronized camera/, tactile/, lang.json, and trajectory.csvrecordings.BridgeData V2 (Walke et al., 2023) 60,096 trajectories (50,365 teleoperated; 9,731 scripted)</p>
<p>Multi-skill goal-and languageconditioned manipulation dataset TFRecords with RGB images, natural language instructions, and continuous 7-DoF action vectors; includes both human and scripted demonstrations.LIBERO (Liu et al., 2023) 130 tasks: 10 spatial, 10 object, 10 goal, 100 lifelong</p>
<p>Lifelong VLA benchmark for procedural and declarative knowledge transfer JSON and Parquet files with RGB images, language instructions, action trajectories, and structured metadata.Kaiwu (Jiang et al., 2025) 1M multimodal robotic episodes Real-world, multi-embodiment dataset for dexterous manipulation with natural language commands Per-episode HDF5 files with synchronized RGB, depth, 3D skeletons, tactile, EMG, gaze, IMU, audio, language, and motion capture data.PLAICraft (He et al., 2025) 10,000 + hours of multiplayer Minecraft gameplay across 5 modalities</p>
<p>Open-ended multiplayer interaction with emergent task structures and voice-aligned social play JSON-encoded multimodal streams (RGB, audio, keyboard, mouse) with millisecond alignment AgiBot World (Bu et al., 2025) 1M+ multimodal dual-arm trajectories</p>
<p>Open-source platform for long-horizon generalist policy learning ROS-based: RGB-D, fisheye, tactile, proprioception, language, error annotations, and dexterous control logs.Robo360 (Liang et al., 2023) 2K+ Synchronized multistream recordings from RGB cameras, event camera, microphones, force/torque sensors, and proprioceptive signals, collected during haptically teleoperated assembly and disassembly tasks based on NIST benchmark boards.RoboCerebra (Han et al., 2025) 100K long-horizon trajectories across 1K+ tasks</p>
<p>System-2-level reasoning and generalization in real-world-scale settings Structured plan logs, visual transitions, failure annotations, and dense subtask labels from human-verified demonstrations and multi-stage task generation.IRef-VLA (Zhang et al., 2025c) 11.5K rooms, 7.6M spatial relations, 4.7M instructions Imperfect referential grounding in 3D indoor scenes</p>
<p>Per-room scene graphs, free-space maps, and affordance annotations with synthetic and imperfect language queries.Interleave-VLA (Fan et al., 2025) 210K episodes (13M frames) Interleaved vision-language instruction execution Mixed-format episodes with images, sketch overlays, and text prompts aligned with action sequences.RoboMM (Yan et al., 2024) 30K simulated episodes + 5K real-world trials From 2023 onward, the field shifts to truly multimodal control challenges.Datasets such as DROID and Open X-Embodiment embed synchronized RGBD, language, and multi-skill trajectories, facilitating evaluation of sensor fusion strategies and real-time feedback controllers.Largescale egocentric corpora like Ego4D and CoVLA offer real-world visual streams that drive research on robust perception-action loops under unpredictable dynamics.Recent contact-rich datasets such as ARIO, TLA, RoboMM, and REASSEMBLE integrate high-frequency haptic and force/torque feedback alongside vision and language, enabling fine-grained impedance control and hybrid modelpredictive schemes for deformable-object manipulation.Highly multimodal and large-scale datasets such as Kaiwu, PLAICraft, AgiBot World, and Robo360 support openended, long-horizon, and real-world tasks with diverse sensor suites including tactile, audio, proprioceptive, and multiview data.By standardizing annotation formats (HDF5 bundles, ROS bags, TFRecords) and pairing each collection with representative baselines (e.g., SayCan, HapticBERT, MM-FusionNet), Table 2 provides a detailed overview of these datasets across a continuum of task complexity, modality richness, and real-world fidelity.</p>
<p>Benchamrk VLA Datasets</p>
<p>In order to benchmark, we map each major VLA dataset onto a two-dimensional plane spanned by task complexity and modality richness, illustrated in Fig. 10.The x-axis captures how challenging each dataset's manipulation tasks are, ranging from simple single-step actions to long-horizon, multiskill sequences.The y-axis shows the modality richness, from minimal (dual modalities: text and image) to comprehensive (up to seven modalities including audio, video, robot proprioception, control, depth, haptics, and language).</p>
<p>To quantify these dimensions systematically, we assign scalar scores to each dataset reflecting their task complexity and modality richness.Task complexity, denoted as îˆ¯ task , incorporates:</p>
<p>â€¢ Average number of low-level actions per episode ( ).</p>
<p>This captures how many primitive control commands are grouped together in a typical task (e.g., grasp, lift, move).</p>
<p>â€¢ Number of distinct high-level skills ().This enumerates different semantic subtasks (e.g., open drawer, pick object).</p>
<p>â€¢ Degree of sequential task dependency ().This denotes the fraction of tasks that require strict ordering of subtasks;  âˆˆ [0, 1].</p>
<p>â€¢ Linguistic abstraction level ().Quantifies the average linguistic complexity (e.g., vocabulary size or syntactic depth) of the instruction set;  âˆˆ â„ + .</p>
<p>These attributes are integrated via the following weighted model:
îˆ¯ task (îˆ°) = ð›¼ 1 log(1 + ð‘‡ ) + ð›¼ 2 ð‘† + ð›¼ 3 ð· + ð›¼ 4 ð¿, (4)
where   &gt; 0 for  = 1, â€¦ , 4 are weights that normalize each term to commensurate scales and can be tuned to reflect the emphasis on action length, skill diversity, sequential structure, or language complexity.For our benchmark, we set all weights equal to one.Modality richness, captured by the score îˆ¯ mod , integrates four factors reflecting the scope and quality of sensory input:</p>
<p>â€¢ Number of distinct modalities (), Such as vision, depth, haptics, and language.</p>
<p>â€¢ Mean quality  = 1  âˆ‘  =1    , Where    for  = 1, â€¦ ,  are the modality-specific quality scores.   can be determined by expert annotation, automated signal-to-noise ratio analysis, or based on dataset documentation and previous benchmark studies.For this work, we use a mixture of empirical review and published specifications to assign scores in the range [0.6, 0.95], reflecting the typical range of public datasets.</p>
<p>â€¢ Fidelity of temporal alignment across modalities (),</p>
<p>Measures how tightly modalities are synchronized (e.g., frame-accurate vision-language pairing), with  âˆˆ [0, 1].</p>
<p>â€¢ Presence of reasoning-critical modalities (): Such as object masks or scene graphs that enable higher-level reasoning,  âˆˆ {0, 1}.</p>
<p>This scoring mechanism is formalized as:
îˆ¯ mod = ð›½ 1 ð‘€ + ð›½ 2 ð‘„ + ð›½ 3 ð´ + ð›½ 4 ð‘…,(5)
where modality sensitivity weights   &gt; 0 for  = 1, â€¦ , 4 tune the relative importance of modality count, signal quality, temporal alignment, and reasoning-enabled annotations.</p>
<p>For our benchmark, we set all weights equal to one.Finally, to allow direct comparison across heterogeneous benchmarks, both raw scores are normalized.Task complexity to a standardized [1, 5] scale and modality richness to a [2, 5] scale.This mapping ensures interpretability: datasets with the lowest complexity or modality richness receive a score of 1 or 2 ("Very Low"/"Minimal") and the highest receive 5 ("Very High"/"Comprehensive"), with intermediate values reflecting proportional positioning.The bubble size then encodes the relative dataset scale (e.g., number of episodes or hours), providing an at-a-glance summary of both range and comprehensiveness across the leading VLA benchmarks.</p>
<p>The resulting visualization effectively categorizes datasets, while it also highlights critical gaps in current benchmark, offering notably the underrepresented region combining highly complex tasks with extensive multimodal integration.This gap underscores a promising direction for future dataset development aimed at advancing truly generalist robotic agents capable of complex, real-world perception and planning.Each bubble represents a VLA dataset, positioned according to its normalized task-complexity score (x-axis) and its modality-richness score (y-axis).The bubble area is proportional to the dataset scale that is number of annotated episodes or interactions.The script to generate the plot can be found here:https://github.com/Muhayyuddin/VLAs</p>
<p>Benchmarking Analysis</p>
<p>Fig. 10 shows that most current VLA benchmarks are concentrated within a task complexity range from very low to high on the x-axis and span from minimal to rich modality along the y-axis.Early navigation and QA datasets like EmbodiedQA, R2R, and RoboSpatial are characterized by their very low complexity and minimal modality, reflecting simple, discrete decision-making in constrained environments.In contrast, mid-generation collections such as RL-Bench, TEACh, Ego4D, CVDN, and RoboCerebra, tend to feature low to moderate complexity with moderate modality richness, often focused on navigation, imitation, or basic manipulation tasks involving a limited number of modalities.</p>
<p>As the field evolves, datasets such as ALFRED, Di-alFRED, CoVLA, Interleave-VLA, RoboData, and RE-ASSEMBLE have moved into the medium-complexity, richmodality region by incorporating additional sensory streams like depth, language, and proprioceptive signals, enabling more sophisticated evaluation of policy learning and multistep planning.In particular, a small subset of datasets, including Iref-VLA, Robo360, TLA, CALVIN, and Open X-Embodiment, simultaneously achieve high task complexity and rich modality, each with a particular focus.Robo360 on multiview real-robot visual fidelity, Iref-VLA on referential grounding in 3D scenes, TLA on tactile-language-action alignment for contact-rich assembly, CALVIN on longhorizon language-conditioned robotic manipulation, and Open X-Embodiment on multirobot, multiskill demonstrations at scale.</p>
<p>The only dataset positioned at the extreme of both axes is Kaiwu, which achieves very high task complexity alongside the most comprehensive modality richness, integrating vision, depth, language, proprioception, haptics, and additional streams.Meanwhile, AgiBot World stands out in the very high complexity quadrant while exhibiting just moderate modality diversity, emphasizing large-scale, longhorizon dual-arm tasks rather than maximal sensor integration.This disparity highlights a critical gap: current VLA benchmarks do not yet fully integrate the challenges of longhorizon, multi-skill control with exhaustive multimodal input (vision, depth, language, proprioception, haptics, audio, and scene graphs).Without such datasets, the development of robust and generalist robotic agents remains limited.Future efforts should therefore focus on the upper right quadrant of the landscape, creating new VLA benchmarks that maximize both task difficulty and multimodal diversity to accelerate progress toward general-purpose embodied intelligence.</p>
<p>Simulation Tools</p>
<p>Simulation environments have become essential for VLA research, offering scalable, repeatable, and extensively annotated data at orders of magnitude greater than what is feasible in the physical world.Modern platforms such as AI2-THOR, Habitat, and NVIDIA Isaac Sim provide high-precision physics, realistic rendering, and customizable multimodal sensors ranging from RGBD cameras, force/torque and tactile probes, to proprioceptive encoders Table 3 Overview of simulation platforms commonly used for generating and evaluating VLA datasets.The table summarizes each simulator's supported sensory modalities, primary use cases, core capabilities, and the datasets that rely on them.These tools span diverse domains such as photorealistic indoor navigation, dexterous manipulation, and large-scale reinforcement learning, with varying degrees of physics realism.</p>
<p>Simulator</p>
<p>Modalities Use Cases Capabilities Datasets AI2-THOR (Kolve et al., 2017) [link] RGB, depth, semantic/instance segmentation, object states Embodied navigation, object manipulation Photorealistic indoor scenes; procedural scene generation; physics-based object/agent interaction; built-in interaction APIs; language and task integration ALFRED (Shridhar et al., 2020), TEACh (Padmakumar et al., 2021), DialFRED (Gao et al., 2022b) Habitat (Savva et al., 2019) [link] RGB, depth, semantic segmentation, agent pose Vision-language navigation, embodied QA, point-goal navigation Photorealistic, highperformance rendering; large-scale 3D scene support; modular sensor and agent APIs R2R (Anderson et al., 2018), CVDN (Thomason et al., 2019) (Yu et al., 2020), RoboSuite (Zhu et al., 2020), custom RL benchmarks iGibson (Xia et al., 2020)  Python API; large-scale articulated object library DexGraspNet (Wang et al., 2023), TLA (Hao et al., 2025) and language interfaces all configurable at fine temporal resolutions.Using procedural scene generation, randomized object properties, and scripted agent behaviors, simulators enable the automated synthesis of hundreds of thousands of trajectories, complete with ground truth annotations for object poses, semantic maps, action sequences, and natural language instructions.Crucially, built-in toolkits for scenario scripting and domain randomization facilitate systematic studies of generalization under varied lighting, object geometries, and task orders, while lightweight GPU accelerated backends support rapid iteration of new dataset designs.Together, this ecosystem of VLA simulators accelerates the co-development of control algorithms and benchmark datasets, ensuring that advances in multimodal perception, language grounding, and closed-loop planning can be evaluated and refined in a controlled, reproducible framework before deployment on real robotic platforms.</p>
<p>Table 3 summarizes the current state-of-the-art simulation platforms used for the generation of VLA datasets.The table lists four essential aspects for each simulator: the Modalities of sensors it offers, the main Use Cases it supports, its fundamental technical Capabilities, and the representative Datasets that are based on it.This unified view allows researchers to directly compare engines based on their multi-modal sensor suite, physics accuracy, scalability, and integration with language and control APIs.</p>
<p>In the first column, platforms such as AI2-THOR and Habitat provide photorealistic RGB, depth, and semantic streams, making them ideal for embodied navigation and visual question answering benchmarks (e.g., ALFRED, R2R, EmbodiedQA, CVDN).Middle entries like NVIDIA Isaac Sim and Gazebo deliver advanced LiDAR, IMU, force/torque, and multi-robot support crucial for large-scale reinforcement learning, sim-to-real transfer, and multi-agent coordination, as in Open X-Embodiment and RoboSpatial.Contact-rich simulators including PyBullet, CoppeliaSim, MuJoCo, and SAPIEN enable precise force, torque, and haptic feedback, powering dexterous manipulation datasets such as DexGraspNet, CALVIN, and TLA.Emerging platforms (Unity ML-Agents, RoboSuite, IsaacGym, UniSim) highlight capabilities such as GPU-parallel rollout, cloudnative simulation, and unified multi-sensor APIs, enabling the creation of next-generation VLA datasets with millions of diverse trajectories spanning vision, language, touch, and audio.The table provides an essential reference by mapping these four aspects across fifteen simulators: it assists in choosing the optimal backend for dataset generation, clarifies the trade-offs between rendering quality and processing speed, and identifies gaps where enhancements in simulator features could facilitate more detailed VLA benchmarks.</p>
<p>Applications and Evaluation of VLAs</p>
<p>Application Domains</p>
<p>The Table 4 organizes VLA models into six broad application domains.These application domains are explained below.</p>
<p>The Manipulation and Task Generalization domain covers models that unify visual perception and language instructions into a single control policy for diverse objectlevel tasks from simple grasping and placement to complex assembly with a focus on maintaining adaptability to novel objects, configurations, and robot models with limited retraining.In Autonomous Mobility domain, models convert high-level language goals into safe, efficient navigation plans for wheeled, legged, or aerial platforms.They combine scene understanding (identifying landmarks, obstacles, and waypoints) with motion planning to follow verbal or written navigation instructions.</p>
<p>In Human Assistance and Interaction application area, agents interpret human commands and contexts to perform collaborative tasks, handling tools, manipulating domestic objects on request, or automating GUI workflows via multi-turn dialogues, prioritizing responsiveness and safety when working alongside people.The Robotic Platforms category focuses on models for controllers specialized to specific hardware (quadrupeds, humanoids, custom arms), integrating platform-aware perception and action modules that respect each robot's kinematics, dynamics, and sensor capabilities.Virtual Environments includes purely softwarebased agents that automate GUIs, play video games, or act as benchmarking frameworks.This domain highlights how VLA techniques are generalized beyond physical robots in simulated or desktop settings.Edge and Low-Power Deployment focuses on lightweight architectures optimized for immediate inference on CPUs or embedded processors, demonstrating that successful VLA integration can function within limited computational and energy constraints.</p>
<p>VLA Model Selection and Evaluation</p>
<p>Since manipulation and task generalization remain the dominant challenges in VLA research, we selected ten models that best exemplify high manipulation skill and broad task generalization.These models were chosen according to: (1) the breadth of task coverage, including the ability to handle unseen problems, (2) zero-shot or few-shot generalization to new embodiments and environments, (3) robust real-robot validation, (4) architectural novelty in fusion or decoding mechanisms, and (5) computational practicality regarding inference speed and resource usage.</p>
<p>Our evaluation framework employs three standardized metrics that are success rate, zero-shot generalization, and real-robot validation to enable direct comparisons across VLA architectures.Table 5 summarizes ten representative models, listing each model's name and its primary benchmark dataset.The Success Rate column categorizes average task completion as High (â‰¥ 90 %), Medium (70-90 %), or Low (&lt; 70 %).Zero-Shot Capability is estimated High for â‰¥ 80 % success on unseen tasks, Medium for 50-80 %, and Low for &lt; 50 %.Finally, Real-Robot Deployment indicates whether a model has been validated on physical hardware (Yes) or remains simulation only (No).This uniform metric framework allows for direct comparison between architectures and datasets.</p>
<p>RT-2 demonstrates this balance by co-finetuning on internet-scale VQA data and robot trajectories to achieve zero-shot transfer across dozens of tasks on multiple robots.Pi-0 shows that a lightweight 3B-parameter model can operate at over 200 Hz while generalizing to new tasks and robots.CLIPort introduced dense semantic grounding using CLIP-enhanced transport maps, setting state-of-the-art results on diverse tabletop manipulation tasks.VIMA demonstrated that a single, prompt-based, multimodal policy could perform six grounding tasks, including pushing, grasping,  Together, these models illustrate two main trajectories in the VLA field.On the one hand are large generalist architectures such as RT-2, Octo, Gato, and OpenVLA which use massive transformer-based backbones and diffusion decoders trained on millions of diverse trajectories.These excel in broad zero-shot generalization and are effective in many tasks and robots.On the other hand, modular and task-specialized systems, such as DexVLA, CLIPort, TLA, and RoboAgent, use targeted modules (e.g., objectcentric ViTs, tactile encoders, LoRA adapters, and semantic augmentation) to enhance robustness and data efficiency for specific manipulation skills.This division demonstrates that while scaling and pretraining bring broad generalization, specialized pipelines remain critical for bridging sim-to-real gaps and achieving high-precision real-world performance.</p>
<p>Challenges and Future Directions</p>
<p>This section presents open challenges and future directions essential to advancing VLA models.We categorize these into three interconnected domains: Architectural Challenges, Dataset Challenges, and Simulation Challenges.Addressing these challenges will be essential to develop robust and generalizable robotic autonomy.</p>
<p>Architectural Challenges</p>
<p>VLA models rely on a unified Transformer backbone to process high-resolution images or video frames alongside natural-language instructions and output platform-specific action commands.This end-to-end approach exposes several core architectural challenges that arise from the heterogeneity, scale, and physical diversity inherent to robotic control.</p>
<p>Tokenization and Vocabulary Alignment:</p>
<p>VLA models must process heterogeneous inputs including natural language, image patches, and continuous robot states, however standard techniques such as byte-pair encoding (BPE) for text and fixed patch embeddings for vision often fail to capture the complexities of visual and proprioceptive signals.This misalignment results in inconsistent token distributions and degraded cross-modal attention.To address this, recent approaches have introduced unified tokenization schemes.Perceiver IO uses shared latent arrays for multimodal fusion (Jaegle et al., 2022), BLIP-2 introduces a Q-former to dynamically select vision tokens compatible with language models (Li et al., 2023), and adapter-based quantization layers allow flexible discretization within each modality stream (Pfeiffer et al., 2020).Despite these advances, several key challenges remain, such as efficiently encoding highdimensional sensor streams without information loss, dynamically adapting vocabularies in the presence of noise or novel configurations, achieving low-latency token generation on resource-constrained platforms, and designing interpretable token spaces to support transparent and reliable cross-modal reasoning.</p>
<p>Modality Fusion:</p>
<p>Simply concatenating visual and linguistic features or applying basic cross-attention often fails to align the distinct statistical properties of pixellevel and word-level representations, resulting in weak visual grounding.Recent advances adopt an "align-thenfuse" paradigm to strengthen cross-modal representations.For instance, align-before-fusing employs momentum-based contrastive learning to pre-align vision and language modalities (Li et al., 2022), and VLMo introduces multimodal expert layers within Transformer blocks to adaptively balance contributions from each stream (Wang et al., 2022).Despite these gains, key challenges remain: effectively fusing asynchronous sensory streams like haptics or audio; incorporating additional modalities such as force/torque signals; dynamically reweighting modality importance under domain shift (e.g., lighting changes or ambiguous language); improving interpretability of cross-attention layers for debugging; and enabling low-latency, resource efficient fusion for deployment on embedded robotic platforms.</p>
<p>Generalization Across Embodiments:</p>
<p>Fixed action vocabularies and rigid kinematic bindings severely limit the ability of VLA models to transfer across different robot models.Recent approaches address this by conditioning action generation on robot-specific descriptors or learned affordance models.For example, PaLM-E encodes explicit hardware embeddings to adapt vision-language reasoning to new platforms (Driess et al., 2023), while RT-2 freezes its visionlanguage planning module and delegates embodiment/modelspecific control to a lightweight action adapter.More recent efforts, such as DexVLA, go further by enabling plug-andplay cross-embodiment adaptation using diffusion-based expert modules trained across diverse kinematic structures.Despite these advances, zero-shot generalization to entirely novel robot models, payload distributions, or joint limits continues to degrade without fine-tuning.Moreover, simto-real transfer remains unstable under noisy sensor readings and unexpected dynamics, and generating smooth, compliant trajectories that adapt to varying torque, speed, and stiffness profiles across platforms remains an open and critical challenge.</p>
<p>Manipulator Motion Smoothness: Although many</p>
<p>VLA models emphasize the prediction of discrete action tokens, they often neglect the quality of continuous motion trajectories, which are essential for smooth, safe and precise manipulation.Recent approaches such as Diffusion Policy (Chi et al., 2023b) reformulate visuomotor control as a conditional denoising process, enabling the generation of temporally coherent action sequences.Based on this, the diffusion transformer policy (Hou et al., 2024b) integrates large transformer architectures directly into the diffusion framework, achieving improved stability and generalization across diverse robotic platforms.However, several challenges remain unresolved: achieving real-time inference with latencysensitive diffusion models, ensuring robust collision avoidance under sensor noise and dynamic uncertainty, maintaining a balance between trajectory smoothness and fast reactivity to changing goals, coupling diffusion-based controllers with high-level language planners.</p>
<p>Dataset Challenges</p>
<p>Comprehensive, varied, and well-organized datasets form the basis for developing VLA models.However, current data sets exhibit several significant limitations that obstruct the path toward robust, general-purpose VLA models.</p>
<ol>
<li>Task Diversity: Current datasets are highly specialized, focusing on narrow, short-horizon tasks.For instance, AL-FRED and CALVIN emphasize pick-and-place operations, while R2R focuses on navigation and finding pathways guided by language.However, few datasets integrate longhorizon task planning that combines spatial reasoning, navigation, and fine-grained object manipulation in open-ended, multi-scene environments.This fragmentation restrains the training of agents capable of seamlessly switching between locomotion and manipulation tasks in realistic household or industrial scenarios.</li>
</ol>
<p>Modality Imbalance:</p>
<p>Most VLA datasets primarily offer RGB images and textual annotations, often excluding critical sensor modalities such as depth maps, force/torque signals, tactile feedback, or proprioceptive data.When these streams are present, they are frequently captured at inconsistent sampling rates or resolutions.This lack of highquality, synchronized multimodal data significantly limits the development of models that can perform robust sensor fusion despite environmental uncertainty.</p>
<p>Annotation Quality and Cost:</p>
<p>Obtaining accurate labels such as 6-DoF object poses, frame-aligned multi-sensor data, or detailed natural language explanations is resource intensive and time-consuming, requiring either detailed manual annotation or unreliable semi-automated pipelines.Although simulated environments can provide perfect annotations at scale, domain gaps in appearance, physics, and interaction fidelity often degrade sim-to-real transfer.Meanwhile, current self-supervised and auto-labeling methods remain unreliable across diverse task domains.</p>
<p>Realism and Scale:</p>
<p>Real-world datasets like Open X-Embodiment offer high fidelity data with authentic sensor noise and physical interactions, but are constrained by the cost and time of robot data collection, typically producing only hundreds of hours of recordings.In contrast, simulation platforms can generate millions of trajectories efficiently but struggle to replicate complex real-world dynamics, such as material deformation, lighting variability, or occlusion effects.This trade-off between realism and scalability remains a fundamental bottleneck in the development of models that generalize beyond laboratory conditions.</p>
<p>Addressing these limitations will require coordinated efforts to build long-horizon, cross-domain benchmarks; gather richly synchronized multimodal datasets; reduce annotation costs through self-supervision and automation; and bridge the realism-scale divide via hybrid simulation-real data pipelines.These advances are essential to equip future VLA models with the robustness and adaptability needed for deployment in real-world environments.</p>
<p>Simulation Challenges</p>
<p>Simulators provide scalable, controllable environments for generating training data for VLA models.However, several critical limitations must be addressed to ensure that simulated performance reliably transfers to real-world deployment.</p>
<p>Physics Accuracy and Contact Modeling:</p>
<p>Popular physics engines such as MuJoCo, PyBullet, and NVIDIA Isaac Sim simplify physical interactions by relying on basic Coulomb friction models and point-contact approximations.Although this enables stable and fast simulation, it fails to capture essential dynamics such as soft-body deformation, variable surface friction, and joint compliance.As a result, policies trained in simulation often perform poorly in the real world, leading to issues like object slip, unexpected torque spikes, or unstable contact behavior.</p>
<p>Visual Realism and Throughput</p>
<p>Trade-offs: Highfidelity simulation platforms such as AI2-THOR, Habitat, and Unity ML-Agents provide photorealistic rendering and diverse assets, making them ideal for vision-heavy tasks.However, this comes at the cost of low frame rates and high GPU demand, limiting their suitability for large-scale reinforcement learning or self-supervised pretraining.In contrast, lightweight renderers support high-throughput simulation but suffer from domain gaps in texture, lighting, and occlusion realism, reducing the effectiveness of domainrandomized policies during real-world deployment.</p>
<ol>
<li>
<p>Lack of Built-in Language Grounding APIs: Most simulators do not provide native support for grounding natural language commands into agent behaviors.This forces to create custom annotation pipelines such as those used in ALFRED or TEACh that align textual instructions with actions and scene representations.These efforts introduce significant development overhead, restrict reproducibility, and lead to fragmented and non-standardized data formats.</p>
</li>
<li>
<p>Multi-Robot and Agent Support Capabilities: Support for multiple robots varies widely across simulators.Some platforms like Isaac Sim and Gazebo offer flexible import of arbitrary robot descriptions via URDF or SDF formats, facilitating multi-robot coordination and benchmarking.Others, like Webots and RoboSuite, are optimized for specific robot families, limiting generalization and reusability.This inconsistency complicates cross-platform pretraining and impairs reproducibility across hardware setups.</p>
</li>
</ol>
<p>Overcoming these challenges requires advancing contactrich physics modeling, optimizing rendering pipelines for both fidelity and throughput, developing standardized language grounding interfaces, and unifying multi-agent simulation support.These improvements are essential to create simulation platforms that can produce realistic, scalable, and transferable datasets for training generalizable VLA models.</p>
<p>Future Directions</p>
<p>To advance the next generation of VLA models, future systems should incorporate learnable, modality-aware tokenizers-such as vector-quantized VAEs or neural dictionaries to jointly discretize continuous sensor streams like proprioception and force/torque alongside visual and textual inputs.Dynamic fusion blocks (e.g., gating networks, mixture-of-experts, or conditional attention) can reweight each modality based on task demands, improving flexibility and robustness.For scaling long video or text sequences, hierarchical architectures are recommended, where lightweight CNN or RNN frontends downsample highframe-rate inputs before passing them to sparse Transformer layers for efficient long-range modeling.Additionally, integrating diffusion-based trajectory generators with differentiable safety and collision-avoidance filters can produce smooth, compliant motions that align tightly with high-level task planning.</p>
<p>On the dataset side, procedural task grammars embedded in simulators can automatically generate long-horizon, open-ended scenarios that interleave navigation and finegrained manipulation.To support sensor fusion, standardized multimodal capture pipelines should be adopted to synchronize RGB-D, tactile, force/torque, audio, and language streams at compatible sampling rates, with missing modalities augmented through cross-modal synthesis (e.g., monocular depth estimation).Annotation burdens can be reduced through self-supervised or weakly supervised techniques, including unsupervised segmentation, visionlanguage co-training, and active learning, to automatically extract object masks, 6-DoF trajectories, and language explanations.Hybrid synthetic-real pipelines, using neural rendering and physics-aware domain randomization, can bridge the realism-scale gap, ensuring that large-scale simulated data generalize better to physical environments.</p>
<p>In simulation platforms, physics fidelity should be improved through differentiable, multi-scale contact models that blend classical solvers with data-driven calibration to better handle soft-body deformation, friction variability, and compliance.Hybrid rendering pipelines that combine highthroughput rasterization for general frames with neural or ray-traced rendering for key scenes can deliver realism without compromising speed.A simulator-agnostic language grounding API should be established to map natural language instructions directly to scene graphs and agent behaviors.Finally, to enable broad generalization, simulators must support multi-robot and multi-agent scenarios, with autoimport of URDF/SDF models and shared simulation protocols, allowing for consistent policy pretraining across heterogeneous robot platforms.</p>
<p>Conclusion</p>
<p>VLA models are transforming the operational scope of robotic agents by tightly coupling perception, language, and action through integrated learning strategies.This review has systematically organized the current VLA landscape across four key dimensions: architectural design, dataset ecosystems, simulation platforms, and evaluation methodologies.The analysis reveals a rapid evolution toward multimodal, instruction conditioned agents capable of generalizing across tasks, embodiments, and environments.While significant progress has been made, the field continues to face critical challenges in achieving scalable pretraining, reliable transfer from simulation to physical environments, and transparent decision making in safety critical contexts.Addressing these limitations will require advancements in composable learning systems, data efficient adaptation strategies, and standardized procedures for benchmarking and deployment.The future of VLA research lies in bridging foundational models with real world applications, enabling agents that can reason, act, and adapt in complex, open ended settings.This review contributes to that trajectory by synthesizing core architectural principles, learning paradigms, and deployment infrastructures, while outlining open problems that can shape the development of the next generation of vision language grounded robotic intelligence.</p>
<p>Declaration</p>
<p>Declaration of generative AI and AI-assisted technologies in the writing process.</p>
<p>During the preparation of this work the author(s) used ChatGPT in order to refine the English and to search the names of the VLA models.After using this tool/service, the author(s) reviewed and edited the content as needed and take(s) full responsibility for the content of the publication.</p>
<p>Figure 2 :
2
Figure 2: Overview of the skeleton of the paper, highlighting the main sections and their interrelated subtopics.</p>
<p>Figure 3 :
3
Figure 3: Annual VLA models and foundational VLA datasets count from 2022 to 2025.Green bars indicate the number of new VLA model introduced each year, while purple bars represent the number of novel dataset releases.The data illustrate a rapid acceleration in model development, particularly in 2025, alongside steady growth in dataset creation to support training and evaluation of these models.</p>
<p>Figure 4 :
4
Figure4: An overview of the Transformer architecture highlighting the encoder-decoder structure and the internal mechanism of multi-head attention.The encoder processes input embeddings through layers of multi-head attention, normalization, and feedforward networks.The decoder mirrors this with additional masked attention layers and incorporates encoder outputs for contextual decoding.The magnified view illustrates the scaled dot-product attention and how multiple attention heads are concatenated and linearly transformed to form the final multi-head attention output.The image is adapted from(Vaswani et al., 2017)</p>
<p>Figure 5 :
5
Figure 5: Architecture of the ViT.The input image is divided into fixed-size non-overlapping patches which are flattened and linearly projected into embedding vectors.A learnable classification (CLS) token is prepended to the sequence of patch embeddings (shown in darker blue).Positional embeddings are added to retain spatial information before feeding the sequence into a standard Transformer encoder.The output of the CLS token is passed through an MLP head to produce the final class prediction.The image is adpated from(Dosovitskiy et al., 2021)</p>
<p>Figure 6 :
6
Figure 6: Architecture of a VLM for image captioning and semantic understanding.Visual input is processed by a visual encoder to extract patch-based embeddings.In parallel, a text prompt (e.g., "Describe the image") is tokenized and embedded.These embeddings are fused and jointly processed through a Transformer-based encoder-decoder architecture.The model outputs a natural language caption that describes the semantic content of the image, enabling tasks such as captioning, question answering, and visual grounding.the image is adapted from(Xiao et al., 2024)</p>
<p>Figure 7 :
7
Figure7: Architecture of a VLA system for robotic manipulation.The model processes three inputs: an image of the scene, a natural language instruction, and the robot's internal state.These are encoded respectively through visual, text, and state encoders.The resulting embeddings are passed to an LLM that fuses multimodal information and generates a semantic representation of the intended task.This representation, along with robot state features, is processed by an Action Decoder implemented as a diffusion transformer to generate a trajectory that accomplishes the commanded task.</p>
<p>Figure 8 :
8
Figure8: This mind map presents the principal classes of vision encoders, language encoders, and action decoders employed in state-of-the-art VLA models.Only those encoder and decoder classes that are utilized by at least three different models are visualized, highlighting prevailing architectural trends across the VLA literature.The taxonomy categorizes representative models under each component family based on their dominant backbone; for example, ViT variants (such as; CLIP, SigLIP, DINOv2) and CNNs for vision encoding, LLaMA/Vicuna, T5-base, Qwen, and GPT-based models for language encoding, and diffusion or autoregressive transformers, MLP, and general Token predictors for action decoding.It should be noted that some models are listed under multiple encoder categories due to hybrid or fused architecture designs.For instance, models such as HybridVLA, OpenVLA, and DexGraspVLA appear under both SigLIP and DINOv2, as they integrate features from both backbones to enhance visual grounding and downstream task performance.This fusion-based design supports improved generalization, multi-view robustness, and more flexible multimodal alignment.</p>
<p>Figure 9 :
9
Figure 9: Schematic of the unified VLA training data format.Visual observations, language instructions, and action/control signals are grouped into episode directories and serialized into standardized storage formats (JSON, TFRecord, and HDF5), enabling efficient and scalable data loading for end-to-end model training.</p>
<p>Figure 10 :
10
Figure 10: Benchmarking VLA Datasets by Task Complexity and Modality Richness.Each bubble represents a VLA dataset, positioned according to its normalized task-complexity score (x-axis) and its modality-richness score (y-axis).The bubble area is proportional to the dataset scale that is number of annotated episodes or interactions.The script to generate the plot can be found here:https://github.com/Muhayyuddin/VLAs</p>
<p>Table 2 :
2
Overview of main VLA datasets used in robotic manipulation and embodied AI research.For each dataset, we list the release year, dataset size, distinctive characteristics, and the data storage format.
DatasetSizeDistinctive CharacteristicsData FormatEmbodiedQA (Das et al., 2018)5,000+ QA episodes across 750+ 3D scenesGoal-directed visual question answering in House3D with object and room diver-JSON-formatted question-answer pairs, egocentric RGB frame sequences, and agent trajectoriessityR2R (Anderson et al., 2018)7,189 unique paths with 21,567Real-world vision-language navigation us-JSON files with instructions and navigation paths;natural language instructionsing Matterport3D with path diversity andpanoramic JPEG frames and viewpoint graph metadatacrowd-sourced instructionsALFRED (Shridhar et al., 2020)8,055 expert demonstrations with 25,743 language directivesLanguage-conditioned household manip-ulation tasks in AI2-THOR 2.0 across 120Per-demonstration folders with egocentric RGB frames, ground-truth interaction masks, and language annotations;indoor scenesmetadata and action/state sequences in JSON formatRLBench (James et al., 2020)Expert demonstrations avail-Large-scale few-shot and imitation learn-Pickled demos include joint_positions, camera_images,able for 100 vision-based ma-ing benchmark in PyRep simulationtask_description, and proprioceptive states.nipulation tasksCVDN (NDH) (Thomason et al., 2019)7,415 navigation-from-dialog-history instances from 2,050 di-Vision-and-Dialog Navigation benchmark requiring agents to act based on dialogJSON annotations with dialog turns, navigation paths, image features, and speaker rolesalogshistoryTEACh (Padmakumar et al., 2021)3,047 successful two-agent gameplay sessionsMultiturn dialog-driven household task completion in AI2-THORJSON transcripts aligned to visual frames, with egocentric RGB, object masks, action logs, and benchmark CSVsplitsDialFRED (Gao et al., 2022a)53,000+ human-annotated QADialogue-enabled embodied instructionPer-task dialogue.json with human and oracle QAs, actionpairs across 34,000+ tasksfollowing on augmented ALFRED sub-traces, subgoal templates, and frame alignmentgoalsEgo4D (Grauman et al., 2022)3,670 h of first-person videoLarge-scale, real-world egocentric datasetMP4 video clips; JSON-based narrations and annotations;with diverse scenarios and modalitiesHDF5/LMDB indices; multilingual narration files.CALVIN</p>
<p>Table 4
4
Categorization of VLA Models by Application.</p>
<p>Table 5
5
Table summarizes the evaluation of ten leading VLA models selected based on manipulation capabilities and task generalization.
ModelBenchmark DatasetsSuccessZero-ShotReal-Notable ResultsRateRobotRT-2Open X-Embodiment, BridgeDataHighHighâœ”Outperforms RT-1 and SayCan on gener-V2alization and zero-shot; strong multi-robot,internet-pretrained transfer.OctoRLBench, Open X-EmbodimentMediumMediumâœ”First diffusion-based generalist trained on4M+ trajectories across 22 robots; robust sim-to-real.OpenVLAOpen X-Embodiment, DROIDMediumMediumâœ”LoRA fine-tuned, open-source VLA; competi-tive success with minimal tuning.GatoInternal multi-task datasetMediumMediumâœ”Unified policy for vision, language, androbotics; real-robot zero-shot transfer.Pi-0Pi-Cross-EmbodimentMediumMediumâœ”200Hz+ low-latency control; generalizes tonew embodiments and setups.DexVLART-X, RLBenchMediumMediumâœ”Plug-in diffusion experts; cross-embodimentadaptation without fine-tuning.CLIPortRavens pick-and-place suiteMediumLowâœ”CLIP-based dense transport achieves state-of-the-art on tabletop tasks.RoboAgentRoboSetHighHighâœ”CVAE action-chunking, semantic augmenta-tion; strong real-world generalization.VIMAVIMA datasetMediumMediumâœ”Prompt-based multimodal transformer; com-positional generalization, real-robot demos.TLATLA benchmarkMediumHighâœ”First language-tactile VLA; achieves 85%+success on contact-rich tasks.and stacking, all within a unified model and with real-robotthat spans vision, language, and robotic control, with strongdemos. RoboAgent advances action decomposition and se-zero-shot transfer on real robots.mantic augmentation for real-world manipulation, achievinghigh success in real kitchen settings. OpenVLA offers aLoRA-tuned, open-source alternative to RT-2, matchingcompetitive performance with lower tuning overhead andreal-robot support. Octo introduced the use of diffusion-based policies at scale, training on over 4 million trajectoriesacross 22 robot platforms and achieving robust sim-to-realtransfer. DexVLA uses plug-in diffusion experts to rapidlyadapt across different embodiments without task-specificfine-tuning. TLA introduces the first language-tactile model,reaching over 85% success in challenging tasks that involve alot of contact. Gato established a unified token-based policy</p>
<p>VLA Models Manipulation and Task Generalization CLIPort; RT-1; Gato; VIMA; RoboAgent; RT-Trajectory; RT-2; Pi-0; OpenVLA; OpenVLA-OFT; Tiny VLA; DexVLA; OneTwoVLA; RDT-1B; Shake-VLA; DexGraspVLA; Bi-VLA; MoLe-VLA; TLA; ACT; GeoManip; LMM Planner Integration; Online RL VLA; SAM2Act; VoxPoser; Diffusion Policy. Octo</p>
<p>Chain-Of-Affordance, CogACT; ECoT; OTTER; Gemini Robotics; V-JEPA 2; Knowledge Insulating VLA Models; AgiBot World Colosseo; Fine-Tuning VLA Models; Hi Robot; EnerVerse; FLaRe; Beyond Sight. </p>
<p>Vla; Grape; Forethought, Vla Hamster; Temporep, ; Conrft, Robobert, Diffusion Transformer Policy; GEVRM; SoFar; ARM4R; Magma; An Atomic Skill Library; VLAS; SafeVLA; iRe-VLA; TraceVLA; RevLA; RoboMamba; A3VLM; SVLR; 3D-VLA; HybridVLA; SpatialVLA; OE-VLA; EF-VLA; RoboBrain; PerAct; SayCan; CLIP-RT; VLATest; HiRT; A3VLM; SVLR. </p>
<p>. - Bi, Vla, </p>
<p>. Quar-Vla; 3d-Vla; Robomm, ; Shake, -Vla; More; Dexgraspvla, ; Dexvla, ; Humanoid, - Vla, </p>
<p>Gemini Objectvla, Robotics; ECoT; OTTER; ðœ‹-0.5; OneTwoVLA; Helix; OE-VLA; SmolVLA; EF-VLA. </p>
<p>. Pd-Vla; Leverb, ; Tla; Interleave, - Vla; Ire-Vla; Tracevla, ; Vlatest, ; Openvla, - Oft, </p>
<p>Vla Edge, ; Covla, ; Robomm, Robobert, Diffusion Transformer Policy; GEVRM; SoFar; ARM4R; Magma; An Atomic Skill Library; VLAS; ChatVLA; RoboBrain; SafeVLA; Diffusion-VLA Autonomous Mobility NaVILA; Mobility VLA; Uni-NaVid; COVLA; OpenDrive VLA; ORION; UAV-VLA; CognitiveDrone Human Assistance and Interaction RoboNurse-VLA; ObjectVLA; RoboMM; Interleave-VLA. </p>
<p>ChatVLA Robotic Platforms QUAR-VLA; MORE; LeVERB; Humanoid-VLA; GR00T N1; Helix Virtual Environments ShowUI-2B. </p>
<p>. Vla Combat, Jarvis-Vla; , </p>
<p>Image captioning transformers: A comprehensive review. Vlatest Edge, Low-Power Deployment Edge, Vla Smolvla, ; Gemini, ; , Nora; Pd-Vla References Abdel-Hamid, A Mahmoud, K , 10.1007/s10462-024-10560-wRobotics On-Device; FAST (Pi-0 Fast). 2024</p>
<p>M Ahn, A Brohan, N Brown, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N SÃ£Ä³nderhauf, I Reid, S Gould, A Van Den Hengel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2018</p>
<p>Covla: Comprehensive vision-language-action dataset for autonomous driving. H Arai, K Miwa, K Sasaki, K Watanabe, Y Yamaguchi, S Aoki, I Yamamoto, IEEE/CVF Winter Conference on Applications of Computer Vision (WACV). IEEE2025. 2025</p>
<p>H Arai, K Miwa, K Sasaki, Y Yamaguchi, K Watanabe, S Aoki, I Yamamoto, arXiv:2408.10845Covla: Comprehensive vision-languageaction dataset for autonomous driving URL. 2024</p>
<p>Ef-vla: Vision-language-action models with aligned vision language features for better generalization. M Assran, A Bardes, D Fan, Q Garrido, R Howes, M Muckley, A Rizvi, C Roberts, K Sinha, A Zholus, arXiv:2506.09985Under review at ICLR 2025 URL. 2025. 2025arXiv preprintV-jepa 2: Selfsupervised video models enable understanding, prediction and planning</p>
<p>Roboagent: Generalist robot agent with semantic and temporal understanding. H Bharadhwaj, N Pore, J Liang, J Singh, K Rao, A Zeng, K Gopalakrishnan, arXiv:2310.085602023arXiv preprint</p>
<p>J Bjorck, F CastaÃ±eda, N Cherniadev, X Da, R Ding, L Fan, Y Fang, D Fox, F Hu, S Huang, arXiv:2503.14734Gr00t n1: An open foundation model for generalist humanoid robots. 2025arXiv preprint</p>
<p>K Black, N Brown, J Darpinian, K Dhabalia, D Driess, A Esmail, M Equi, arXiv:2504.16054ðœ‹-0.5:: A vision-language-action model with open-world generalization. 2025arXiv preprint</p>
<p>K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, arXiv:2410.24164Pi-0: A visionlanguage-action flow model for general robot control. 2024aarXiv preprint</p>
<p>K Black, N Brown, D Driess, A Esmail, M Equi, C Finn, N Fusai, L Groom, K Hausman, B Ichter, arXiv:2410.24164ð‘ð‘–_0: A visionlanguage-action flow model for general robot control. 2024barXiv preprint</p>
<p>Seq2code: Encoder-decoder model for program synthesis. M BrandiÅ¡auskas, M Ã…Â¡ukauskas, A Krizhanovsky, 10.1016/j.procs.2023.11.306Procedia Computer Science. 2222023</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Language models are fewshot learners. T B Brown, B Mann, N Ryder, Advances in Neural Information Processing Systems. 332020</p>
<p>Q Bu, J Cai, L Chen, X Cui, Y Ding, S Feng, S Gao, X He, X Hu, X Huang, arXiv:2503.06669Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. 2025arXiv preprint</p>
<p>Emerging properties in self-supervised vision transformers. P Budzianowski, W Maa, M Freed, J Mo, A Xie, V Tipnis, B Bolte, M Caron, H Touvron, M Cord, M Douze, F Massa, A Sablayrolles, H JÃ©gou, 2024. 2021ICCV20Edgevla: Efficient vision-language-action models</p>
<p>Combatvla: An efficient visionlanguage-action model for combat tasks in 3d action role-playing games. P Chen, P Bu, Y Wang, X Wang, Z Wang, J Guo, Y Zhao, Q Zhu, J Song, S Yang, arXiv:2503.095272025aarXiv preprint</p>
<p>Quar-vla: A vision-language-action model for quadruped robots. X Chen, arXiv:2310.085322024arXiv preprint</p>
<p>Y Chen, S Tian, S Liu, Y Zhou, H Li, D Zhao, arXiv:2502.05450Conrft: A reinforced fine-tuning method for vla models via consistency policy. 2025barXiv preprint</p>
<p>Robohorizon: An llm-assisted multi-view world model for long-horizon robotic manipulation. Z Chen, J Huo, Y Chen, Y Gao, arXiv:2501.066052025carXiv preprint</p>
<p>A Cheng, Y Ji, Z Yang, Z Gongye, X Zou, J Kautz, E BÃ¤Å›yÃ¤Å›k, H Yin, S Liu, X Wang, arXiv:2412.04453Navila: Legged robot vision-languageaction model for navigation. 2024arXiv preprint</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. C Chi, Z Xu, S Feng, E Cousineau, Y Du, B Burchfiel, R Tedrake, S Song, The International Journal of Robotics Research. 2023a. 02783649241273668</p>
<p>C Chi, Z Xu, S Feng, E Cousineau, Y Du, B Burchfiel, R Tedrake, S Song, arXiv:2303.04137Diffusion policy: Visuomotor policy learning via action diffusion. 2023barXiv preprint</p>
<p>H Chiang, Z Xu, Z Fu, M Jacob, T Zhang, T Lee, W Yu, C Schenck, D Rendleman, D Shah, arXiv:2407.07775Mobility vla: Multimodal instruction navigation with long-context vims and topological graphs. 2024arXiv preprint</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, R Sepassi, S Gehrmann, E Elsen, D Patrick, P Mishkin, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models. E Collaboration, A O'neill, A Rehman, A Gupta, A Maddukuri, A Gupta, A Padalkar, A Lee, arXiv:2310.088642025</p>
<p>Pybullet, a python module for physics simulation for robotics. E Coumans, Y Bai, 2016</p>
<p>Embodied question answering. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2018</p>
<p>Imagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L J Li, K Li, L Fei-Fei, 10.1109/CVPR.2009.5206848doi:10. 1109/CVPR.2009.5206848IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2009. 2009</p>
<p>J Devlin, M W Chang, K Lee, K Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>Revla: Reverting visual domain limitation of robotic foundation models. S Dey, J Zaech, N Nikolov, L Van Gool, D Paudel, arXiv:2409.152502024arXiv preprint</p>
<p>P Ding, J Ma, X Tong, B Zou, X Luo, Y Fan, T Wang, H Lu, P Mo, J Liu, arXiv:2502.14795Humanoid-vla: Towards universal humanoid control with visual integration. 2025arXiv preprint</p>
<p>Quar-vla: Vision-language-action model for quadruped robots. P Ding, H Zhao, W Zhang, W Song, M Zhang, S Huang, N Yang, D Wang, European Conference on Computer Vision. Springer2024</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, A Dosovitskiy, L Beyer, A Kolesnikov, arXiv:2010.11929International Conference on Learning Representations (ICLR). 2021. 2020arXiv preprintAn image is worth 16x16 words: Transformers for image recognition at scale</p>
<p>D Driess, N Ruiz, K Goyal, Y Chebotar, A Irpan, X Ailon, S Levine, C Finn, arXiv:2303.03378Palm-e: An embodied multimodal language model. 2023arXiv preprint</p>
<p>Knowledge insulating vision-language-action models: Train fast, run fast, generalize better. D Driess, J T Springenberg, B Ichter, L Yu, A Li-Bell, K Pertsch, A Z Ren, H Walke, Q Vuong, L X Shi, arXiv:2505.237052025arXiv preprint</p>
<p>Openvla: Open-source vision-language-action models for robotics. D Driess, arXiv:2406.092462024arXiv preprint</p>
<p>C Fan, X Jia, Y Sun, Y Wang, J Wei, Z Gong, X Zhao, M Tomizuka, X Yang, J Yan, M Ding, arXiv:2406.07000Interleave-vla: Enhancing robot manipulation with interleaved image-text instructions. 2025arXiv preprint</p>
<p>Sam2act: Integrating visual foundation model with a memory architecture for robotic manipulation. H Fang, M Grotz, W Pumacay, Y R Wang, D Fox, R Krishna, J Duan, arXiv:2501.185642025arXiv preprint</p>
<p>Image captioning using deep learning: A comprehensive review and future perspectives. Multimedia Tools and Applications. S Fouad, 10.1007/s11042-024-17234-32024</p>
<p>Orion: A holistic end-to-end autonomous driving framework by vision-language instructed action generation. H Fu, D Zhang, Z Zhao, J Cui, D Liang, C Zhang, D Zhang, H Xie, B Wang, X Bai, arXiv:2503.197552025arXiv preprint</p>
<p>DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following. X Gao, Q Gao, R Gong, K Lin, G Thattai, G S Sukhatme, 10.1109/LRA.2022.3193254IEEE Robotics and Automation Letters. 72022a</p>
<p>Dialfred: Dialogue-enabled agents for embodied instruction following. X Gao, Q Gao, R Gong, K Lin, G Thattai, G S Sukhatme, 10.1109/lra.2022.3193254doi:10.1109/lra.2022.3193254IEEE Robotics and Automation Letters. 72022b</p>
<p>Bi-vla: Vision-language-action model-based system for bimanual robotic dexterous manipulations. K F Gbagbe, M A Cabrera, A Alabbas, O Alyunes, A Lykov, D Tsetserukou, arXiv:2405.060392024arXiv preprint</p>
<p>Attention mechanism in machine learning: A survey. B Ghojogh, A Ghodsi, F Karray, M Crowley, arXiv:2402.053102024arXiv preprint</p>
<p>K Grauman, A Westbury, E Byrne, Z Chavis, A Furnari, arXiv:2110.07058Ego4d: Around the world in 3,000 hours of egocentric video URL. 2022</p>
<p>J Gu, S Kirmani, P Wohlhart, Y Lu, M G Arenas, K Rao, W Yu, C Fu, K Gopalakrishnan, Z Xu, arXiv:2311.01977Robotic task generalization via hindsight trajectory sketches. 2023arXiv preprint</p>
<p>Y Guo, J Zhang, X Chen, X Ji, Y J Wang, Y Hu, J Chen, arXiv:2501.16664irevla: Improving vision-language-action model with online reinforcement learning. 2025arXiv preprint</p>
<p>Benchmarking vision, language, &amp; action models on robotic learning tasks. P Guruprasad, H Sikka, J Song, Y Wang, P P Liang, arXiv:2411.058212024arXiv preprint</p>
<p>S Han, B Qiu, Y Liao, S Huang, C Gao, S Yan, S Liu, arXiv:2506.06677Robocerebra: A large-scale benchmark for long-horizon robotic manipulation evaluation. 2025arXiv preprint</p>
<p>P Hao, C Zhang, D Li, X Cao, X Hao, S Cui, S Wang, arXiv:2503.08548Tla: Tactile-language-action model for contact-rich manipulation. 2025arXiv preprint</p>
<p>Plaicraft: Large-scale time-aligned vision-speech-action dataset for embodied ai. Y He, C D Weilbach, M E Wojciechowska, Y Zhang, F Wood, arXiv:2505.127072025</p>
<p>Z Hou, T Zhang, Y Xiong, H Pu, C Zhao, R Tong, Y Qiao, J Dai, Y Chen, arXiv:2410.15959Diffusion transformer policy. 2024aarXiv preprint</p>
<p>Diffusion transformer policy: Scaling diffusion transformer for generalist visionÃ¢Ä‚ÅžlanguageÃ¢Ä‚Åžaction learning. Z Hou, T Zhang, Y Xiong, H Pu, C Zhao, R Tong, Y Qiao, J Dai, Y Chen, arXiv:2410.159592024barXiv preprint</p>
<p>J Hu, R Hendrix, A Farhadi, A Kembhavi, R MartÃ­n-MartÃ­n, P Stone, K H Zeng, K Ehsani, arXiv:2409.16578Flare: Achieving masterful and adaptive robot policies with large-scale reinforcement learning fine-tuning. 2024arXiv preprint</p>
<p>Otter: A vision-language-action model with textaware visual feature extraction. H Huang, F Liu, L Fu, T Wu, M Mukadam, J Malik, K Goldberg, P Abbeel, arXiv:2503.037342025aarXiv preprint</p>
<p>A3vlm: Actionable articulation-aware vision language model. S Huang, H Chang, Y Liu, Y Zhu, H Dong, A Boularias, P Gao, H Li, Proceedings of the 8th Conference on Robot Learning (CoRL). the 8th Conference on Robot Learning (CoRL)2024</p>
<p>S Huang, L Chen, P Zhou, S Chen, Z Jiang, Y Hu, Y Liao, P Gao, H Li, M Yao, arXiv:2501.01895Enerverse: Envisioning embodied future space for robotics manipulation. 2025barXiv preprint</p>
<p>W Huang, C Wang, R Zhang, Y Li, J Wu, L Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023arXiv preprint</p>
<p>Nora: A small open-sourced generalist vision language action model for embodied tasks. C Hung, Q Sun, P Hong, A Zadeh, C Li, U Tan, N Majumder, S Poria, arXiv:2504.198542025arXiv preprint</p>
<p>Perceiver IO: A general architecture for structured inputs &amp; outputs. A Jaegle, N Gimeno, A Brock, A Zisserman, J Carreira, O Vinyals, R Verdegaal, P Pessoa, S Nowozin, International Conference on Learning Representations (ICLR). 2022</p>
<p>Rlbench: The robot learning benchmark &amp; learning environment. S James, Z Ma, D R Arrojo, A J Davison, 10.1109/LRA.2020.2972831IEEE Robotics and Automation Letters. 52020</p>
<p>Robobrain: A unified brain model for robotic manipulation from abstract to concrete. Y Ji, H Tan, J Shi, X Hao, Y Zhang, H Zhang, P Wang, M Zhao, Y Mu, P An, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>Kaiwu: A multimodal manipulation dataset and framework for robot learning and human-robot interaction. S Jiang, H Li, R Ren, Y Zhou, Z Wang, B He, arXiv:2503.052312025</p>
<p>Vima: General robot manipulation with multimodal prompts. Y Jiang, A Gupta, Z Zhang, arXiv:2210.030942022arXiv preprint</p>
<p>Beyond sight: Finetuning generalist robot policies with heterogeneous sensors via language grounding. J Jones, O Mees, C Sferrazza, K Stachowicz, P Abbeel, S Levine, A Juliani, V Berges, E Teng, Y Gao, H Henry, M Mattar, D Lange, arXiv:2501.04693Proceedings of the 1st Annual Conference on Robot Learning (CoRL). the 1st Annual Conference on Robot Learning (CoRL)2025. 2018arXiv preprintUnity: A general platform for intelligent agents</p>
<p>G C Kang, J Kim, K Shim, J K Lee, B T Zhang, arXiv:2411.00508Clip-rt: Learning language-conditioned robotic policies from natural language supervision. 2024arXiv preprint</p>
<p>Clip-rt: Learning language-conditioned robotic policies from natural language supervision. G C Kang, RSSJ Kim, RSSK Shim, RSSJ K Lee, RSSB T Zhang, RSSProceedings of Robotics: Science and Systems. Robotics: Science and Systems2025</p>
<p>M Khan, S Asfaw, D Iarchuk, M Cabrera, L Moreno, I Tokmurziyev, D Tsetserukou, arXiv:2501.06919Shake-vla: Vision-language-action model-based system for bimanual robotic manipulations and liquid mixing. 2025arXiv preprint</p>
<p>Droid: A large-scale in-the-wild robot manipulation dataset. A E Khazatsky, Robotics: Science and Systems (RSS). 2024</p>
<p>Fine-tuning vision-languageaction models: Optimizing speed and success. M Kim, C Finn, P Liang, arXiv:2502.196452025arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. M Kim, K Pertsch, S Karamcheti, T Xiao, A Balakrishna, S Nair, R Rafailov, E Foster, G Lam, P Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Design and use paradigms for gazebo, an open-source multi-robot simulator. N Koenig, A Howard, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2004. 2004</p>
<p>AI2-THOR: An interactive 3d environment for visual ai. E Kolve, R Mottaghi, W Han, T Randhavane, X Zheng, Y Li, A Gupta, A Farhadi, Proceedings of the 1st Annual Conference on Robot Learning (CoRL). the 1st Annual Conference on Robot Learning (CoRL)2017</p>
<p>Beyond the nav-graph: Vision-and-language navigation in continuous environments. J Krantz, E Wijmans, A Mukhopadhyay, S Lee, S Chernova, D Batra, 10.1007/978-3-030-58568-6_7Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Springer2020</p>
<p>Deep learning with vision transformers: A survey. C Lam, X Wang, X Lu, Y Yao, M H Yang, 10.1109/TPAMI.2023.3241477IEEE Transactions on Pattern Analysis and Machine Intelligence. 452023</p>
<p>An atomic skill library construction method for data-efficient embodied manipulation. D Li, B Peng, C Li, N Qiao, Q Zheng, L Sun, Y Qin, B Li, Y Luan, B Wu, arXiv:2501.150682025aarXiv preprint</p>
<p>Align before fuse: Vision and language representation learning with momentum distillation. J Li, X Li, X Li, J Huang, J Zhang, L Wang, Q Dou, H Ling, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2022</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. J Li, E A Peng, C Wang, J Liu, C Feichtenhofer, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2023</p>
<p>Improving vision-language-action models via chainof-affordance. J Li, Y Zhu, Z Tang, J Wen, M Zhu, X Liu, C Li, R Cheng, Y Peng, F Feng, arXiv:2412.204512024aarXiv preprint</p>
<p>M Li, Z Wang, K He, X Ma, Y Liang, arXiv:2503.16365Jarvis-vla: Post-training large-scale vision language models to play visual games with keyboards and mouse. 2025barXiv preprint</p>
<p>Cogact: A foundational visionlanguage-action model for synergizing cognition and action in robotic manipulation. Q Li, Y Liang, Z Wang, L Luo, X Chen, M Liao, F Wei, Y Deng, S Xu, Y Zhang, arXiv:2411.196502024barXiv preprint</p>
<p>S Li, J Wang, R Dai, W Ma, W Ng, Y Hu, Z Li, arXiv:2409.19590Robonursevla: Robotic scrub nurse system based on vision-language-action model. 2024carXiv preprint</p>
<p>Hamster: Hierarchical action models for open-world robot manipulation. Y Li, Y Deng, J Zhang, J Jang, M Memmel, R Yu, C R Garrett, F Ramos, D Fox, A Li, arXiv:2502.054852025carXiv preprint</p>
<p>Y Li, G Yan, A Macaluso, M Ji, X Zou, X Wang, arXiv:2501.18733Integrating lmm planners and 3d skill policies for generalizable manipulation. 2025darXiv preprint</p>
<p>L Liang, L Bian, C Xiao, arXiv:2312.06686Robo360: A 3d omnispective multi-material robotic manipulation dataset. 2023arXiv preprint</p>
<p>F Lin, R Nai, Y Hu, J You, J Zhao, Y Gao, arXiv:2505.11917Onetwovla: A unified vision-language-action model with adaptive reasoning. 2025arXiv preprint</p>
<p>Showui: One vision-language-action model for gui visual agent. K Lin, L Li, D Gao, Z Yang, S Wu, Z Bai, W Lei, L Wang, M Shou, arXiv:2411.174652024arXiv preprint</p>
<p>Libero: Benchmarking knowledge transfer for lifelong robot learning. B Liu, Y Zhu, C Gao, Y Feng, Q Liu, Y Zhu, P Stone, Advances in Neural Information Processing Systems. 362023</p>
<p>J Liu, H Chen, P An, Z Liu, R Zhang, C Gu, X Li, Z Guo, S Chen, M Liu, arXiv:2503.10631Hybridvla: Collaborative diffusion and autoregression in a unified vision-language-action model. 2025arXiv preprint</p>
<p>Robomamba: Efficient vision-language-action model for robotic reasoning and manipulation. J Liu, M Liu, Z Wang, P An, X Li, K Zhou, S Yang, R Zhang, Y Guo, S Zhang, Advances in Neural Information Processing Systems. 372024a</p>
<p>S Liu, L Wu, B Li, H Tan, H Chen, Z Wang, K Xu, H Su, J Zhu, arXiv:2410.07864Rdt-1b: a diffusion foundation model for bimanual manipulation. 2024barXiv preprint</p>
<p>Pre-train or prompt? the encoder-decoder framework for zero-shot learning. X Liu, W Chen, Y Chen, Y S Chen, W Y Wang, arXiv:2104.086912021arXiv preprint</p>
<p>A Lykov, V Serpiva, M H Khan, O Sautenkov, A Myshlyaev, G Tadevosyan, Y Yaqoot, D Tsetserukou, arXiv:2503.01378Cognitivedrone: A vla model and evaluation benchmark for real-time cognitive task solving and reasoning in uavs. 2025arXiv preprint</p>
<p>V Makoviychuk, L Wawrzyniak, Y Rathod, A Allshire, A Handa, J MÃ¼ller, F Widmaier, L Leal-TaixÃ©, A Makadia, S Leutenegger, arXiv:2108.10470Isaac gym: High performance gpu based physics simulation for robot learning. 2021arXiv preprint</p>
<p>Calvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. O Mees, L Hermann, E Rosete-Beas, W Burgard, arXiv:2112.032272022</p>
<p>O Michel, IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE. 2004. 2004Webots: Professional mobile robot simulation</p>
<p>Image transformers: A survey. A Mishra, A Mishra, G Singh, 2024ACM Computing Surveys In press</p>
<p>V Myers, B C Zheng, A Dragan, K Fang, S Levine, arXiv:2502.05454Temporal representation alignment: Successor features enable emergent compositionality in robot instruction following temporal representation alignment. 2025arXiv preprint</p>
<p>D Niu, Y Sharma, H Xue, G Biamby, J Zhang, Z Ji, T Darrell, R Herzig, arXiv:2502.13142Pre-training auto-regressive robotic models with 4d representations. 2025arXiv preprint</p>
<p>. , Nvidia Corporation, Nvidia Isaac, Sim, </p>
<p>Teach: Task-driven embodied agents that chat. A Padmakumar, J Thomason, A Shrivastava, P Lange, A Narayan-Chen, S Gella, R Piramuthu, G Tur, D Hakkani-Tur, arXiv:2110.005342021</p>
<p>Gemini robotics on-device brings ai to local robotic devices. C Parada, G R Team, 2025</p>
<p>Fast: Efficient action tokenization for vision-language-action models. K Pertsch, K Stachowicz, B Ichter, D Driess, S Nair, Q Vuong, O Mees, C Finn, S Levine, arXiv:2501.097472025arXiv preprint</p>
<p>Adapterfusion: Non-destructive task composition for transfer learning. J Pfeiffer, I Vulic, I Gurevych, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2020</p>
<p>Using the output embedding to improve language models. O Press, L Wolf, Proceedings of the 15th Conference of the European Chapter. the 15th Conference of the European Chapterthe Association for Computational Linguistics2017</p>
<p>Z Qi, W Zhang, Y Ding, R Dong, X Yu, J Li, L Xu, B Li, X He, G Fan, arXiv:2502.13143Sofar: Language-grounded orientation bridges spatial reasoning and object manipulation. 2025arXiv preprint</p>
<p>D Qu, H Song, Q Chen, Y Yao, X Ye, Y Ding, Z Wang, J Gu, B Zhao, D Wang, arXiv:2501.15830Spatialvla: Exploring spatial representations for visual-language-action model. 2025arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, arXiv:2103.000202021arXiv preprint</p>
<p>S Reed, K Zolna, E Parisotto, S Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J Springenberg, arXiv:2205.06175A generalist agent. 2022arXiv preprint</p>
<p>V-rep: A versatile and scalable robot simulation framework. E Rohmer, S P Singh, M Freese, IEEE/RSJ international conference on intelligent robots and systems. 2013. 2013</p>
<p>Scalable, trainingfree visual language robotics: A modular multi-model framework for consumer-grade gpus. M Samson, B Muraccioli, F Kanehiro, arXiv:2502.010712024arXiv preprint</p>
<p>Uav-vla: Vision-language-action system for large scale aerial mission generation. O Sautenkov, Y Yaqoot, A Lykov, M Mustafa, G Tadevosyan, A Akhmetkazy, M Cabrera, M Martynov, S Karaf, D Tsetserukou, arXiv:2501.050142025arXiv preprint</p>
<p>Habitat: A platform for embodied ai research. M Savva, A X Chang, A Dosovitskiy, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>Hi robot: Open-ended instruction following with hierarchical vision-language-action models. L X Shi, B Ichter, M Equi, L Ke, K Pertsch, Q Vuong, J Tanner, A Walling, H Wang, N Fusai, arXiv:2502.194172025arXiv preprint</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on robot learning. 2022aPMLR</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on Robot Learning (CoRL). 2022b</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on Robot Learning, PMLR. 2023</p>
<p>ALFRED: a benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)2020</p>
<p>M Shukor, D Aubakirova, F Capuano, P Kooijmans, S Palma, A Zouitine, arXiv:2506.01844Smolvla: A vision-language-action model for affordable and efficient robotics. 2025arXiv preprint</p>
<p>D Sliwowski, S Jadav, S Stanovcic, J Orbik, J Heidersberger, D Lee, arXiv:2502.05086Reassemble: A multimodal dataset for contact-rich robotic assembly and disassembly. 2025arXiv preprint</p>
<p>Robospatial: Teaching spatial understanding to 2d and 3d visionlanguage models for robotics. C H Song, V Blukis, J Tremblay, S Tyree, Y Su, S Birchfield, arXiv:2411.165372025a</p>
<p>Accelerating vision-language-action model integrated with action chunking via parallel decoding. W Song, J Chen, P Ding, H Zhao, W Zhao, Z Zhong, Z Ge, J Ma, H Li, arXiv:2503.023102025barXiv preprint</p>
<p>W Tang, J H Pan, Y H Liu, M Tomizuka, L E Li, C W Fu, M Ding, arXiv:2501.09783Geomanip: Geometric constraints as general interfaces for robot manipulation. 2025arXiv preprint</p>
<p>Helix: A vision-language-action model for generalist humanoid control. F A Team, 2025a</p>
<p>G R Team, M Ghosh, D Walke, H Pertsch, K Black, K Mees, O Dasari, S Hejna, J Kreiman, T Xu, C , arXiv:2503.20020arXiv:2405.12213Gemini robotics: Bringing ai into the physical world. 2025b. 2024arXiv preprintOcto: An open-source generalist robot policy</p>
<p>Visionand-dialog navigation. J Thomason, M Murray, M Cakmak, L Zettlemoyer, arXiv:1907.049572019</p>
<p>Mujoco: A physics engine for modelbased control. E Todorov, T Erez, Y Tassa, IEEE. 2012. 2012</p>
<p>H Touvron, T Martin, L Stone, A Albert, A Almahairi, I Laradji, Y Aqaj, A Baratin, S Lee, Z Verde, A Kaplanyan, M Azar, S Gelly, A Joulin, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Ã… Kaiser, I Polosukhin, Advances in Neural Information Processing Systems (NeurIPS). 2017</p>
<p>H R Walke, K Black, T Z Zhao, Q Vuong, C Zheng, P Hansen-Estruch, A W He, V Myers, M J Kim, M Du, Bridgedata v2: A dataset for robot learning at scale, in: Conference on Robot Learning, PMLR. 2023</p>
<p>Vlmo: Unified vision-language pre-training with mixture-of-modality-experts. L Wang, H Zhang, Y Zhao, Z Liu, J Bian, H Yu, C Xu, R Lau, S Wang, ACM International Conference on Multimedia (MM). 2022</p>
<p>Dexgraspnet: A large-scale robotic dexterous grasp dataset for general objects based on simulation. R Wang, J Zhang, J Chen, Y Xu, P Li, T Liu, H Wang, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>S Wang, J Shan, J Zhang, H Gao, H Han, Y Chen, K Wei, C Zhang, K Wong, J Zhao, arXiv:2502.07837Robobert: An end-to-end multimodal robotic manipulation model. 2025arXiv preprint</p>
<p>Exploring the adversarial vulnerabilities of vision-language-action models in robotics. T Wang, C Han, J C Liang, W Yang, D Liu, L X Zhang, Q Wang, J Luo, R Tang, arXiv:2411.135872024aarXiv preprint</p>
<p>Z Wang, H Zheng, Y Nie, W Xu, Q Wang, H Ye, Z Li, K Zhang, X Cheng, W Dong, C Cai, L Lin, F Zheng, X Liang, arXiv:2408.10899All robots in one: A new standard and unified dataset for versatile, generalÃ¢Ä‚Åšpurpose embodied agents. 2024barXiv preprint</p>
<p>Towards testing and evaluating vision-language-action models for robotic manipulation: An empirical study. Z Wang, Z Zhou, J Song, Y Huang, Z Shu, L Ma, arXiv:2409.128942024carXiv preprint</p>
<p>Ladev: A language-driven testing and evaluation platform for vision-language-action models in robotic manipulation. Z Wang, Z Zhou, arXiv:2410.051912024darXiv preprint</p>
<p>Diffusion-vla: Generalizable and interpretable robot foundation model via self-generated reasoning. J Wen, M Zhu, Y Zhu, Z Tang, J Li, Z Zhou, C Li, X Liu, Y Peng, C Shen, F Feng, arXiv:2412.032932024arXiv preprintAccepted by ICML 2025</p>
<p>Dexvla: Visionlanguage model with plug-in diffusion expert for general robot control. J Wen, Y Zhu, J Li, Z Tang, C Shen, F Feng, arXiv:2502.058552025arXiv preprint</p>
<p>Y Wu, R Tian, G Swamy, A Bajcsy, arXiv:2502.01828From foresight to forethought: Vlm-in-the-loop policy steering via latent alignment. 2025arXiv preprint</p>
<p>Interactive gibson benchmark: A benchmark for interactive navigation in cluttered environments. F Xia, C Li, R MartÃ­n-MartÃ­n, O Litany, A R Zamir, S Savarese, 10.1109/IROS45743.2020.9341201Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)2020</p>
<p>Sapien: A simulated part-based interactive environment. F Xiang, Y Qin, K Mo, Y Xia, H Zhu, F Liu, M Liu, H Jiang, Y Yuan, H Wang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Florence-2: Advancing a unified representation for a variety of vision tasks. B Xiao, H Wu, W Xu, X Dai, H Hu, Y Lu, M Zeng, C Liu, L Yuan, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2024</p>
<p>Vla-cache: Towards efficient vision-language-action model via adaptive token caching in robotic manipulation. S Xu, Y Wang, C Xia, D Zhu, T Huang, C Xu, arXiv:2502.021752025arXiv preprint</p>
<p>Leverb: Humanoid whole-body control with latent vision-language instruction. H Xue, X Huang, D Niu, Q Liao, T Kragerud, J T Gravdahl, X B Peng, G Shi, T Darrell, K Sreenath, S Sastry, arXiv:2506.137512025arXiv preprint</p>
<p>F Yan, F Liu, L Zheng, Y Zhong, Y Huang, Z Guan, C Feng, L Ma, arXiv:2412.07215Robomm: All-in-one multimodal large model for robotic manipulation. 2024arXiv preprint</p>
<p>Magma: A foundation model for multimodal ai agents. J Yang, R Tan, Q Wu, R Zheng, B Peng, Y Liang, Y Gu, M Cai, S Ye, J Jang, Proceedings of the Computer Vision and Pattern Recognition Conference. the Computer Vision and Pattern Recognition Conference2025</p>
<p>M Yang, Y Du, K Ghasemipour, J Tompson, D Schuurmans, P Abbeel, arXiv:2310.061141Learning interactive real-world simulators. 20236arXiv preprint</p>
<p>T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. 2020Conference on robot learning, PMLR</p>
<p>Robotic control via embodied chain-of-thought reasoning. M Zawalski, W Chen, K Pertsch, O Mees, C Finn, S Levine, arXiv:2407.086932025arXiv preprint</p>
<p>Revolutionising natural language processing with transformers: A survey. M Zayyanu, B T Usman, Z Muda, N A Salim, A Mohamed, A Y Abubakar, F Al-Obeidat, M A Malik, 10.1016/j.ipm.2023.103528Information Processing &amp; Management. 611035282024</p>
<p>Safevla: Towards safety alignment of vision-language-action model via safe reinforcement learning. B Zhang, Y Zhang, J Ji, Y Lei, J Dai, Y Chen, Y Yang, arXiv-25032025aarXiv e-prints</p>
<p>Gevrm: Goalexpressive video generation model for robust visual manipulation. H Zhang, P Ding, S Lyu, Y Peng, D Wang, arXiv:2502.092682025barXiv preprint</p>
<p>Iref-vla: A benchmark for interactive referential grounding with imperfect language in 3d scenes. H Zhang, N Zantout, P Kachana, J Zhang, W Wang, arXiv:2503.174062025carXiv preprint</p>
<p>Hirt: Enhancing robotic control with hierarchical robot transformers. J Zhang, Y Guo, X Chen, Y J Wang, Y Hu, C Shi, J Chen, Proceedings of the 8th Conference on Robot Learning (CoRL). the 8th Conference on Robot Learning (CoRL)2024a</p>
<p>Up-vla: A unified understanding and prediction model for embodied agent. J Zhang, Y Guo, Y Hu, X Chen, X Zhu, J Chen, arXiv:2501.188672025darXiv preprint</p>
<p>Uni-navid: A video-based vision-languageaction model for unifying embodied navigation tasks. J Zhang, K Wang, S Wang, M Li, H Liu, S Wei, Z Wang, Z Zhang, H Wang, arXiv:2412.062242024barXiv preprint</p>
<p>Mole-vla: Dynamic layer-skipping vision language action model via mixture-of-layers for efficient robot manipulation. R Zhang, M Dong, Y Zhang, L Heng, X Chi, G Dai, L Du, D Wang, Y Du, S Zhang, arXiv:2503.203842025earXiv preprint</p>
<p>Grape: Generalizing robot policy via preference alignment. Z Zhang, K Zheng, Z Chen, J Jang, Y Li, S Han, C Wang, M Ding, D Fox, H Yao, arXiv:2411.193092024carXiv preprint</p>
<p>More: Unlocking scalability in reinforcement learning for quadruped vision-language-action models. H Zhao, W Song, D Wang, X Tong, P Ding, X Cheng, Z Ge, arXiv:2503.080072025aarXiv preprint</p>
<p>Learning finegrained bimanual manipulation with low-cost hardware. T Zhao, V Kumar, S Levine, C Finn, arXiv:2304.137052023arXiv preprint</p>
<p>Vlas: Vision-language-action model with speech instructions for customized robot manipulation. W Zhao, P Ding, M Zhang, Z Gong, S Bai, H Zhao, D Wang, arXiv:2502.135082025barXiv preprint</p>
<p>Unveiling the potential of vision-language-action models with open-ended multimodal instructions. W Zhao, G Li, Z Gong, P Ding, H Zhao, D Wang, arXiv:2505.112142025carXiv preprint</p>
<p>H Zhen, X Qiu, P Chen, J Yang, X Yan, Y Du, Y Hong, C Gan, arXiv:2403.096313d-vla: A 3d vision-language-action generative world model. 2024arXiv preprint</p>
<p>Universal actions for enhanced embodied foundation models. J Zheng, J Li, D Liu, Y Zheng, Z Wang, Z Ou, Y Liu, J Liu, Y Q Zhang, X Zhan, arXiv:2501.101052025aarXiv preprint</p>
<p>Tracevla: Visual trace prompting enhances spatialtemporal awareness for generalist robotic policies. R Zheng, Y Liang, S Huang, J Gao, H D Iii, A Kolobov, F Huang, J Yang, International Conference on Learning Representations (ICLR). 2025b</p>
<p>Dexgraspvla: A vision-language-action framework towards general dexterous grasping. Y Zhong, X Huang, R Li, C Zhang, Y Liang, Y Yang, Y Chen, arXiv:2502.209002025arXiv preprint</p>
<p>Z Zhou, Y Zhu, M Zhu, J Wen, N Liu, Z Xu, W Meng, R Cheng, Y Peng, C Shen, arXiv:2502.14420Chatvla: Unified multimodal understanding and robot control with vision-language-action model. 2025arXiv preprint</p>
<p>M Zhu, Y Zhu, J Li, Z Zhou, J Wen, X Liu, C Shen, Y Peng, F Feng, arXiv:2502.19250Objectvla: End-to-end open-world object manipulation without demonstration. 2025aarXiv preprint</p>
<p>Opendrive-vla: Generalist vision-language-action agent for autonomous driving. T Zhu, H Zhang, Y Zhou, W Yu, Y Wang, L Ma, H Xu, arXiv:2505.018712025barXiv preprint</p>
<p>robosuite: A modular simulation framework and benchmark for robot learning. Y Zhu, A Gupta, F Ebert, arXiv:2009.122932020arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. B Zitkovich, T Yu, S Xu, P Xu, T Xiao, F Xia, J Wu, P Wohlhart, S Welker, A Wahid, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7328 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7328</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7328</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-268819369</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.01014v1.pdf" target="_blank">Harnessing Large Language Models for Training-free Video Anomaly Detection</a></p>
                <p><strong>Paper Abstract:</strong> Video anomaly detection (VAD) aims to temporally locate abnormal events in a video. Existing works mostly rely on training deep models to learn the distribution of normality with either video-level supervision, one-class supervision, or in an unsupervised setting. Training-based methods are prone to be domain-specific, thus being costly for practical deployment as any domain change will involve data collection and model training. In this paper, we radically depart from previous efforts and propose LAnguage-based VAD (LAVAD), a method tackling VAD in a novel, training-free paradigm, exploiting the capabilities of pre-trained large language models (LLMs) and existing vision-language models (VLMs). We leverage VLM-based captioning models to generate textual descriptions for each frame of any test video. With the textual scene description, we then devise a prompting mechanism to unlock the capability of LLMs in terms of temporal aggregation and anomaly score estimation, turning LLMs into an effective video anomaly detector. We further leverage modality-aligned VLMs and propose effective techniques based on cross-modal similarity for cleaning noisy captions and refining the LLM-based anomaly scores. We evaluate LAVAD on two large datasets featuring real-world surveillance scenarios (UCF-Crime and XD-Violence), showing that it outperforms both unsupervised and one-class methods without requiring any training or data collection.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7328",
    "paper_id": "paper-268819369",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00467275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Harnessing Large Language Models for Training-free Video Anomaly Detection
1 Apr 2024</p>
<p>Luca Zanella 
University of Trento</p>
<p>Willi Menapace 
University of Trento</p>
<p>Massimiliano Mancini 
University of Trento</p>
<p>Yiming Wang 
University of Trento</p>
<p>Elisa Ricci 
University of Trento</p>
<p>Fondazione Bruno Kessler 
University of Trento</p>
<p>Harnessing Large Language Models for Training-free Video Anomaly Detection
1 Apr 2024DCF92FA77275D4FBF5F6CF5CCA48C1E7arXiv:2404.01014v1[cs.CV]
Video anomaly detection (VAD) aims to temporally locate abnormal events in a video.Existing works mostly rely on training deep models to learn the distribution of normality with either video-level supervision, one-class supervision, or in an unsupervised setting.Training-based methods are prone to be domain-specific, thus being costly for practical deployment as any domain change will involve data collection and model training.In this paper, we radically depart from previous efforts and propose LAnguage-based VAD (LAVAD), a method tackling VAD in a novel, trainingfree paradigm, exploiting the capabilities of pre-trained large language models (LLMs) and existing vision-language models (VLMs).We leverage VLM-based captioning models to generate textual descriptions for each frame of any test video.With the textual scene description, we then devise a prompting mechanism to unlock the capability of LLMs in terms of temporal aggregation and anomaly score estimation, turning LLMs into an effective video anomaly detector.We further leverage modality-aligned VLMs and propose effective techniques based on cross-modal similarity for cleaning noisy captions and refining the LLM-based anomaly scores.We evaluate LAVAD on two large datasets featuring realworld surveillance scenarios (UCF-Crime and XD-Violence), showing that it outperforms both unsupervised and one-class methods without requiring any training or data collection.</p>
<p>Introduction</p>
<p>Video anomaly detection (VAD) aims to temporally localize events that deviate significantly from the normal pattern in a given video, i.e. the anomalies.VAD is challenging as anomalies are often undefined and context-dependent, and they rarely occur in the real world.The literature [10] often casts VAD as an out-of-distribution detection problem and learns the normal distribution using training data with different levels of supervision (see Fig. 1), including fullysupervised (i.e.frame-level supervision of both normal and abnormal videos) [1,32]  Our proposal, LAVAD, leverages modality-aligned vision-language models (VLMs) to query and enhance the anomaly scores generated by large language models (LLMs).</p>
<p>supervision of both normal and abnormal videos) [11,13,15,24,28,35], one-class (i.e.only normal videos) [18,20,21,25,37,38], and unsupervised (i.e.unlabeled videos) [30,31,40].While more supervision leads to better results, the cost of manual annotation is prohibitive.On the other hand, unsupervised methods assume abnormal videos to constitute a certain portion of the training data, a fragile assumption in practice without human intervention.Crucially, every existing method necessitates a training procedure to establish an accurate VAD system, and this entails some limitations.One primary concern is generalization: a VAD model trained on a specific dataset tends to underperform in videos recorded in different settings (e.g., daylight versus night scenes).Another aspect, particularly relevant to VAD, is the challenge of data collection, especially in certain application domains (e.g.video surveillance) where privacy issues can hinder data acquisition.These considerations led us to explore a novel research question: Can we develop a training-free VAD method?</p>
<p>In this paper, we aim to answer this challenging question.Developing a training-free VAD model is hard due to the lack of explicit visual priors on the target setting.However, such priors might be drawn using large foundation models, renowned for their generalization capability and wide knowledge encapsulation.Thus, we investigate the potential of combining existing vision-language models (VLMs) with large language models (LLMs) in addressing training-free VAD.On top of our preliminary findings, we propose the first training-free LAnguage-based VAD method (LAVAD), that jointly leverages pre-trained VLMs and LLMs for VAD.LAVAD first exploits an off-the-shelf captioning model to generate a textual description for each video frame.We address potential noise in the captions by introducing a cleaning process based on the cross-modal similarity between captions and frames in the video.To capture the dynamics of the scene, we use an LLM to summarize captions within a temporal window.This summary is used to prompt the LLM to provide an anomaly score for each frame, which is further refined by aggregating the anomaly scores among frames with semantically similar temporal summaries.We evaluate LAVAD on two benchmark datasets: UCF-Crime [24] and XD-Violence [36], and empirically show that our trainingfree proposal outperforms unsupervised and one-class VAD methods on both datasets, demonstrating that it is possible to address VAD with no training and no data collection.</p>
<p>Contributions.In summary, our contributions are: • We investigate, for the first time, the problem of trainingfree VAD, advocating its importance for the deployment of VAD systems in real settings where data collection may not be possible.• We propose LAVAD, the first language-based method for training-free VAD using LLMs to detect anomalies exclusively from a scene description.</p>
<p>Related Work</p>
<p>Video Anomaly Detection.Existing literature on trainingbased VAD methods can be categorized into four groups, depending on the level of supervision: supervised, weaklysupervised, one-class classification, and unsupervised.Supervised VAD relies on frame-level labels to distinguish normal from abnormal frames [1,32].However, this scenario has received little attention due to its prohibitive annotation effort.Weakly-supervised VAD methods have access to video-level labels (the entire video is labeled as abnormal if at least one frame is abnormal, otherwise is regarded as normal) [11,13,15,24,28,35].Most of these methods utilize 3D convolutional neural networks for feature learning and employ a multiple instance learning (MIL) loss for training.One-class VAD methods train only on normal videos, although manual verification is necessary to ensure the normality of the collected data.Several methods [18,20,21,25,37,38] have been proposed, e.g.considering generative models [37] or pseudo-supervised methods, where pseudo-anomalous instances are synthesized from normal training data [38].Finally, Unsupervised VAD methods do not rely on predefined labels, leveraging both normal and abnormal videos with the assumption that most videos contain normal events [26,27,30,31,40].Most methods in this category exploit generative models to capture normal data patterns in videos.In particular, generative cooperative learning (GCL) [40] employs alternating training: an autoencoder reconstructs input features, and pseudo-labels from reconstruction errors guide a discriminator.Tur et al. [30,31] use a diffusion model to reconstruct the original data distribution from noisy features, calculating anomaly scores based on the reconstruction error between denoised and original samples.Other approaches [26,27] train a regressor network from a set of pseudo-labels generated using OneClassSVM and iForest [16].Instead, we completely sidestep the need for collecting data and training the model by exploiting existing largescale foundation models to design a training-free pipeline for VAD.</p>
<p>LLMs for VAD.Recently, LLMs have been explored in detecting visual anomalies across diverse application domains.Kim et al. [12] propose an unsupervised method that mainly leverages VLMs for detecting anomalies, where ChatGPT is only utilized to produce textual descriptions that characterize normal and anomalous elements.However, the method involves human-in-the-loop to refine the LLM's outputs according to specific application contexts and requires further training to adapt the VLM.Other examples include exploiting LLMs for spatial anomaly detection in images addressing specific applications in robotics [4] or industry [7].</p>
<p>Differently, we leverage LLMs together with VLMs to address temporal anomaly detection on videos and propose the first training-free method for VAD, requiring no training and no data collection.</p>
<p>Training-Free VAD</p>
<p>In this section, we first formalize the VAD problem and the proposed training-free setting (Sec.3.1).We then analyze the capabilities of LLMs in scoring anomalies in video frames (Sec.3.2).Finally, we describe LAVAD, our proposed VAD method (Sec.3.3).[9,29].For reference, we also plot the performance of the best-performing unsupervised method [27] in a red dashed line, and that of a random classifier in a gray dashed line.</p>
<p>a man is being chased by a group of people in a car a man is seen walking in front of a car with a child in his arms Figure 3.The anomaly score predicted by Llama [29] over time for video Shooting033 from UCF-Crime.We highlight some sample frames with their associated BLIP-2 captions to demonstrate that the caption can be semantically noisy or incorrect (red bounding boxes are for abnormal predictions while blue bounding boxes are for normal predictions).Ground-truth anomalies are highlighted.In particular, the caption of the frame enclosed by a blue bounding box within the ground truth anomaly fails to accurately represent the visual content, leading to a wrong classification due to the low anomaly score given by the LLM.</p>
<p>Problem formulation</p>
<p>Given a test video V = [I 1 , . . ., I M ] of M frames, traditional VAD methods aim to learn a model f , which can classify each frame I ∈ V as either normal (score 0) or anomalous (score 1), i.e. f :
I M → [0, 1]
M with I being the image space.f is usually trained on a dataset D that consists of tuples in the form (V, y).Depending on the supervision level, y can be either a binary vector with frame-level labels (fully-supervised), a binary video-level label (weakly-supervised), a default one (one-class), or ab-sent (unsupervised).However, in practice, it can be costly to collect y as anomalies are rare, and V itself due to potential privacy concerns.Moreover, both label and video data may need regular updates due to evolving application contexts.Differently, in this paper, we introduce a novel setup for VAD, termed as training-free VAD.Under this setting, we aim to estimate the anomaly score of each I ∈ V using only pre-trained models at inference time, i.e. without any training or fine-tuning involving a training dataset D.</p>
<p>Are LLMs good for VAD?</p>
<p>We propose to address training-free VAD by exploiting recent advances in LLMs.As the use of LLMs in VAD is still in its infancy [12], we first analyze the capabilities of LLMs in producing an anomaly score based on a textual description of a video frame.</p>
<p>To achieve this, we first exploit a state-of-the-art captioning model Φ C , i.e.BLIP-2 [14], to generate a textual description for each frame I ∈ V. We then treat anomaly score estimation as a classification task, asking an LLM Φ LLM to select only one score from a list of 11 uniformly sampled values in the interval [0, 1], where 0 means normal and 1 anomalous.We get the anomaly score as:
Φ LLM (P C • P F • Φ C (I))(1)
where P C is a context prompt that provides priors to the LLM regarding VAD, P F instructs the LLM on the desired output format to facilitate automated text parsing1 , and • is the text concatenation operation.We devise P C to simulate a potential end user of a VAD system, e.g. law enforcement agency, as we empirically observe that impersonation can be an effective way of guiding the output generation of the LLM.For example, we can form P C as: "If you were a law enforcement agency, how would you rate the scene described on a scale from 0 to 1, with 0 representing a standard scene and 1 denoting a scene with suspicious activities?".Note that P C does not encode any prior on the type of anomalies itself, but just on the context.Finally, with the estimated anomaly score from Eq. ( 1), we measure the VAD performance using the standard area under the curve of the receiver operating characteristic (AUC ROC).Fig. 2 reports the results obtained on the test set of the UCF-Crime dataset [24] with different variants of BLIP-2 for obtaining the frame captions, and with different LLMs including Llama [29] and Mistral [9] for computing the frame-level anomaly scores.For reference, we also provide the state-of-the-art performance under the unsupervised setting (the closest setting to ours) [27], and the random scoring as lower-bound.The plot demonstrates that state-of-the-art LLMs possess anomaly detection capabilities, largely outperforming random scoring.However, this performance is much lower w.r.t.trained state-of-the-art methods, even in an unsupervised setting.We observe that two aspects might be the limiting factors in LLMs' performance.Firstly, the frame-level captions can be very noisy: the captions might be broken or may not fully reflect the visual content (see Fig. 3).Despite the use of BLIP-2 [14], the best off-the-shelf captioning model, some captions appear corrupted, thus leading to unreliable anomaly scores.Secondly, the frame-level caption lacks details about the global context and the dynamics of the scene, which are key elements when modeling videos.In the following, we address these two limitations and propose LAVAD, the first training-free method for VAD that leverages LLMs for anomaly scoring together with modality-aligned VLMs.</p>
<p>LAVAD: LAnguage-based VAD</p>
<p>LAVAD decomposes the VAD function f into five elements (see Fig. 4).As in the preliminary study, the first two are the captioning module Φ C mapping images to textual descriptions in the language space T , i.e.Φ C : I → T , and the LLM Φ LLM generating text from language queries, i.e.Φ LLM : T → T .The other elements involve three en-coders mapping input representations to a shared latent space Z. Specifically we have the image encoder E I : I → Z, the textual encoder E T : T → Z, and the video encoder E V : V → Z for videos.Note that all five elements involve only off-the-shelf frozen models.</p>
<p>Following the positive findings of the preliminary analysis, LAVAD leverages Φ LLM and Φ C to estimate the anomaly score for each frame.We design LAVAD to address the limitations related to noise and lack of scene dynamics in frame-level captions by introducing three components: i) Image-Text Caption Cleaning through the vision-language representations of E I and E T , ii) LLM-based Anomaly Scoring, encoding temporal information via Φ LLM and iii) Video-Text Score Refinement of the anomaly scores via video-text similarity, using E V and E T .In the following, we describe each component in detail.</p>
<p>Image-Text Caption Cleaning.For each test video V, we first employ Φ C to generate a caption C i for each frame I i ∈ V. Specifically, we denote as C = [C 1 , . . ., C M ] the sequence of captions, where C i = Φ C (I i ).However, as shown in Sec.3.2, the raw captions can be noisy, with broken sentences or incorrect descriptions.To mitigate this issue, we rely on the captions of the whole video C assuming that in this set there exist captions that are unbroken and better capture the content of their respective frames, an assumption often verified in practice as the video features a scene captured by static cameras at a high frame rate.Thus, semantic content among frames can overlap regardless of their temporal distances.From this perspective, we treat caption cleaning as finding the semantically closest caption to a target frame I i within C.</p>
<p>Formally, we make use of vision-language encoders and form a set of caption embeddings by encoding each caption in C via E T , i.e. {E T (C 1 ), . . ., E T (C M )}.For each frame I i ∈ V, we compute its closest semantic caption as:
Ĉi = arg max C∈C ⟨E I (I i ) • E T (C)⟩,(2)
where ⟨•, •⟩ is the cosine similarity, and E I the image encoder of the VLM.We then build the cleaned set of captions as Ĉ = [ Ĉ1 , . . ., ĈM ], replacing each initial caption C i with its counterpart Ĉi retrieved from C. By performing the caption cleaning process, we can propagate the captions of frames that are semantically more aligned to the visual content, regardless of their temporal positioning, to improve or correct noisy descriptions.</p>
<p>LLM-based Anomaly Scoring.The obtained caption sequence Ĉ, while being cleaner than the initial set, lacks temporal information.To overcome this, we leverage the LLM to provide temporal summaries.Specifically, we define a temporal window of T seconds, centered around I i .Within this window, we uniformly sample N frames, forming a video snippet V i , and a caption sub-sequence Ĉi = { Ĉn } N n=1 .We can then query the LLM with Ĉi and a prompt P S to get the temporal summary S i centered on frame I i :
S i = Φ LLM (P S • Ĉi )(3)
where the prompt P S is formed as "Please summarize what happened in few sentences, based on the following temporal description of a scene.Do not include any unnecessary details or descriptions." 2 .Coupling Eq. ( 3) with the refinement process of Eq. ( 2), we obtain a textual description of the frame (S i ) which is semantically and temporally richer than C i .With S i , we can then query the LLM for estimating an anomaly score.Following the same prompting strategy described in Sec.3.2, we ask Φ LLM to assign to each temporal summary S i a score a i in the interval [0, 1].We get the score as:
a i = Φ LLM (P C • P F • S i )(4)
where, as in Sec.3.2, P C is a context prompt containing VAD contextual priors, and P F provides information on the desired output format. 2 Ĉi is represented as an ordered list, with items separated by \n.</p>
<p>Video-Text Score Refinement.By querying the LLM for each frame in the video with Eq. ( 4), we obtain the initial anomaly scores of the video a = [a 1 , . . ., a M ].However, a is purely based on the language information encoded in their summaries, without taking into account the whole set of scores.Thus, we further refine them by leveraging the visual information to aggregate scores from semantically similar frames.Specifically, we encode the video snippet V i centered around I i using E V and all the temporal summaries using E T .Let us define K i as the set of indices of the Kclosest temporal summaries to V i in {S 1 , . . ., S M }, where the similarity between V i and a caption S j is the cosine similarity, i.e. ⟨E V (V i ), E T (S j )⟩.We obtain the refined anomaly score ãi :
ãi = k∈Ki a k • e ⟨E V (Vi),E T (S k )⟩ k∈Ki e ⟨E V (Vi),E T (S k )⟩(5)
where ⟨•, •⟩ is the cosine similarity.Note that Eq. ( 5) exploits the same principles of Eq. ( 2), refining frame-level estimations (i.e.score/captions) using their visual-language similarity (i.e.video/image) with other frames in the video.Finally, with the refined anomaly scores for the test video ã = [ã 1 , . . ., ãM ], we identify the anomalous temporal windows via thresholding.</p>
<p>Experiments</p>
<p>We validate our training-free proposal LAVAD on two datasets in comparison with state-of-the-art VAD methods that are trained with different levels of supervision, as well as training-free baselines.We conduct an extensive ablation study to justify our main design choices regarding the proposed components, prompt design, and score refinement.In the following, we first describe our experimental setup in terms of datasets and performance metrics.We then present and discuss the results in Sec.4.1, followed by the ablation study in Sec.4.2.We show more qualitative results and ablation on minor designs in the Supp.Mat.Datasets.We evaluate our method using two commonly used VAD datasets featuring real-world surveillance scenarios, i.e.UCF-Crime [24] and XD-Violence [36].I3D-RGB 77.92 IBL [41] C3D-RGB 78.66 GCL [40] ResNext 79.84 GCN [42] TSN-RGB 82.12 MIST [5] I3D-RGB 82.30WU et al. [36] I3D-RGB 82.44 CLAWS [39] C3D-RGB 83.03 RTFM [28] VideoSwin-RGB 83.31 RTFM [28] I3D-RGB 84.03 WU &amp; LIU [35] I3D-RGB 84.89 MSL [15] I3D-RGB 85.30 MSL [15] VideoSwin-RGB 85.62 S3R [34] I3D-RGB 85.99 MGFN [2] VideoSwin-RGB 86.67 MGFN [2] I3D-RGB 86.98 SSRL [13] I3D-RGB 87.43 CLIP-TSA [11] ViT 87.58 SVM [24] -50.00 SSV [23] -58.50 BODS [33] I3D-RGB 68.26 GODS [33] I3D Performance Metrics.We measure the VAD performance using the area under the curve (AUC) of the frame-level receiver operating characteristics (ROC) as it is agnostic to thresholding for the detection task.For the XD-Violence dataset, we also report the average precision (AP), i.e. the area under the frame-level precision-recall curve, following the established evaluation protocol in [36].
UCF
Implementation Details.We sample each video every 16 frames for computational efficiency.We employ BLIP-2 [14] as the captioning module Φ C .Particularly, we consider an ensemble of BLIP-2 model variants in our Image-Text Caption Cleaning technique.Please refer to Supp.Mat. for a detailed analysis of these variants.We use Llama-2-13b-chat [29] as our LLM module Φ LLM .We use multimodal encoders provided by ImageBind [6].Specifically, the temporal window is T = 10 seconds, in line with the pre-trained video encoder of ImageBind.We employ K = 10 in Video-Text Score Refinement.</p>
<p>METHOD BACKBONE AP(%) AUC(%)</p>
<p>WU et al. [36] C3D-RGB 67.19 -WU et al. [36] I3D-RGB 73.20 -MSL [15] C3D-RGB 75.53 -WU AND LIU [35] I3D-RGB 75.90 -RTFM [28] I3D-RGB 77.81 -MSL [15] I3D-RGB 78.28 -MSL [15] VideoSwin-RGB 78.58 -S3R [34] I3D-RGB 80.26 -MGFN [2] I3D-RGB 79.19 -MGFN [2] VideoSwin-RGB 80.</p>
<p>Comparison with state of the art</p>
<p>We compare LAVAD against state-of-the-art approaches, including unsupervised methods [26,27,30,31,40], one-class methods [8,19,23,24,33], and weakly-supervised methods [2,5,11,13,15,15,24,28,[34][35][36][39][40][41][42].In addition, as none of the above methods specifically address VAD in a training-free setup, we further introduce a few training-free baselines with VLMs, i.e.CLIP [22], ImageBind [6], and LLaVa [17].Specifically, we introduce Zero-shot CLIP [22] (ZS CLIP) and Zero-shot ImageBind [6] (ZS IMAGEBIND).For both baselines, we exploit their pre-trained encoders to compute the cosine similarities of each frame embedding against the textual embeddings of two prompts: a standard scene and a scene with suspicious or potentially criminal activities.We then apply a softmax function to the cosine similarities to obtain the anomaly score for each frame.Since ImageBind also supports the video modality, we include ZS IMAGEBIND (VIDEO) using the cosine similarities of the video embeddings against the two prompts.We choose ViT-B/32 [3] as the visual encoder for ZS-CLIP, ViT-H/14 [3] as the visual encoders for ZS-IMAGEBIND (IMAGE, VIDEO), and both utilize CLIP's text encoder [22].Finally, we introduce a baseline based on LLAVA-1.5, where we directly query LLaVa [17] to generate an anomaly score for each frame, using the same context prompt as in ours.LLAVA-1.5 uses CLIP ViT-L/14 [22] as the visual encoder and Vicuna-13B as the LLM.</p>
<p>XD-Violence</p>
<p>People are working at their desks in a bank.The camera captures them for several seconds.</p>
<p>The camera captures people working at desks in a bank.The scene is repetitive UCF-Crime a man is laying down on his back with his head on the floor A camera captures a view of a patio, then a man is seen in the driveway, and finally the camera captures a view of the patio again.</p>
<p>A woman is being attacked by a man in a driveway.The attack is captured on camera.The woman is fighting back.</p>
<p>A player from the Philadelphia Flyers is fighting with an official during a game against the Pittsburgh Penguins.</p>
<p>The Philadelphia Flyers are playing the Pittsburgh Penguins in a hockey game, and a player is trying to score a goal on the television screen.</p>
<p>Figure 5.We showcase qualitative results obtained by LAVAD on four test videos, including two videos (top row) from UCF-Crime and two videos from XD-Violence (bottom row).For each video, we plot the anomaly score over frames computed by our method.We display some keyframes alongside their most aligned temporal summary (blue bounding boxes for normal frame predictions and red bounding boxes for abnormal frame predictions), illustrating the relevance among the predicted anomaly score, visual content, and description.Ground-truth anomalies are highlighted.</p>
<p>Tab. 1 presents the results of the full comparison against the state-of-the-art methods, as well as our introduced training-free baselines, on the UCF-Crime dataset [24].Notably, our method without any training demonstrates superior performance compared to both the one-class and unsupervised baselines, achieving a higher AUC ROC, with a significant improvement of +6.08% when compared to GCL [40] and a minor improvement of +0.52% against the current state of the art obtained by DyAnNet [27].</p>
<p>Moreover, it is evident that training-free VAD is a challenging task as a naive application of VLMs to VAD, such as ZS CLIP, ZS IMAGEBIND (IMAGE) and ZS IMAGEBIND (VIDEO), leads to poor VAD performance.VLMs are mostly trained to attend to foreground objects, rather than actions or the background information in an image that contributes to the judgment of anomalies.This might be the main reason for the poor generalization of VLMs on the VAD task.The baseline LLAVA-1.5, which directly prompts for the anomaly score for each frame, achieves a much higher VAD performance than directly exploiting VLMs in a zero-shot manner.Yet, its performance is still inferior to ours, where we leverage a richer temporal scene description for anomaly estimation, instead of a single-frame basis.The similar effect of the temporal summary is also confirmed by our ablation study as presented in Tab. 3. We also report the comparison against state-of-the-art methods and our baselines evaluated on XD-Violence in Tab. 2. Ours achieves superior performance compared to all one-class and unsupervised methods.In particular, LAVAD outperforms RareAnom [26], the bestscoring unsupervised method, by a substantial margin of +17.03% in terms of AUC ROC.</p>
<p>Qualitative Results.Fig. 5 shows qualitative results of LAVAD with sample videos from UCF-Crime and XD-Violence, where we highlight some keyframes with their temporal summaries.In the three abnormal videos (Row 1, Column 1, and Row 2), we can see that the temporal summaries of the keyframes during the anomalies accurately portray the visual content regarding the anomalous situations, which in turn benefits LAVAD to correctly identify the anomalies.In the case of Normal Videos 722 (row 1, column 2), we can see that LAVAD consistently predicts a low anomaly score throughout the video.For more qualitative results on the test videos, please refer to the Supp.Mat.</p>
<p>Ablation study</p>
<p>In this section, we present the ablation study conducted with the UCF-Crime dataset.We first ablate the effectiveness of each proposed component of LAVAD.Then, we demonstrate the impact of task-related priors in the context prompt P C when prompting the LLM for estimating the anomaly scores.Finally, we show the effect of K when aggregating the K semantically closest frames in the Video-Text Score Refinement component.</p>
<p>Effectiveness of each proposed component.We ablate different variants of our proposed method LAVAD to prove the effectiveness of the three proposed components, including Image-Text Caption Cleaning, LLM-based Anomaly Score, and Video-Text Score Refinement.Tab. 3 shows the results of all ablated variants of LAVAD.When the Image-Text Caption Cleaning component is omitted (Row 1), i.e. the LLM only exploits the raw captions to perform temporal summary and obtain the anomaly scores with refinement, the VAD performance degrades by −3.8% compared to LAVAD in terms of AUC ROC (Row 4).If we do not perform temporal summary, and only rely on the cleaned captions with refinement (Row 2), we observe a significant performance drop of −7.58% compared to LAVAD in AUC ROC, indicating that the temporal summary is an effective booster for LLM-based anomaly scoring.Finally, if we only use the anomaly scores obtained with the temporal summary on cleaned captions, without the final aggregation of semantically similar frames (Row 3), we can see that the AUC ROC decreases with a significant margin of −7.49% compared to LAVAD, proving that Video-Text Score Refinement also plays an important role in improving the VAD performance.</p>
<p>Task priors in the context prompt.We investigate the impact of different priors in the context prompt P C and present the results in Tab. 4. In particular, we experimented on two aspects, i.e. impersonation and anomaly prior, which we believe can potentially benefit the estimation of LLM.Impersonation may help the LLM to process the input from the perspective of potential end users of a VAD system, while anomaly prior, e.g.anomalies are criminal activities, may provide the LLM with a more relevant semantic context.Specifically, we ablate LAVAD with various context prompts P C .We begin with a base context prompt: "How would you rate the scene described on a scale from 0 to 1, with 0 representing a standard scene and 1 denoting a scene with suspicious activities?" (Row 1).We inject only the anomaly prior by appending "suspicious activities" with "or potentially criminal activities" (Row 2).We incorporate only impersonation by adding "If you were a law enforcement agency," at the beginning of the base prompt (Row 3).Finally, we integrate both priors into the base context prompt (Row 4).As shown in Tab. 4, for videos within UCF-Crime, the anomaly prior appears to have a negligible effect on the LLM's assessment for anomalies, while impersonation improves the AUC ROC by +0.96% compared to the one obtained with only the base context prompt.Interestingly, incorporating both priors does not further boost the AUC ROC.We hypothesize that a more stringent context might limit the detection of a wider range of anomalies.</p>
<p>Effect of K on refining anomaly score.In this experiment, we investigate how the VAD performance changes in relation to the number of semantically similar temporal summaries, i.e.K, used for refining the anomaly score of each frame.As depicted in Fig. 6, the AUC ROC metric consistently increases as K increases, and saturates when K approaches 9.The plot confirms the contribution of accounting semantically similar frames in obtaining more reliable anomaly scores of the video.</p>
<p>Conclusions</p>
<p>In this work, we introduced LAVAD, a pioneering method to address training-free VAD.LAVAD follows a languagedriven pathway for estimating the anomaly scores, leveraging off-the-shelf LLMs and VLMs.LAVAD has three main components, where the first uses image-text similarities to clean the noisy captions provided by a captioning model; the second leverages an LLM to aggregate scene dynamics over time and estimate anomaly scores; and the final component refines the latter by aggregating scores from semantically close frames according to video-text similarity.We evaluated LAVAD on both UCF-Crime and XD-Violence, demonstrating superior performance compared to trainingbased methods in the unsupervised and one-class setting, without the need for training and additional data collection.personation and anomaly priors, on XD-Violence [36].This follows the same ablation design as presented in Tab. 4 in the main manuscript for UCF-Crime, with the priors added in the same way for both datasets.As shown in Tab. 5, for videos within XD-Violence, incorporating the anomaly prior (Row 2) improves the average precision (AP) by +1.67% compared to using only the base context prompt (Row 1).Conversely, introducing impersonation (Row 3) degrades the AP by −1.51% compared to not using it (Row 1).Videos in XD-Violence originate from various sources, including CCTV cameras, movies, sports, and games.The effectiveness of the impersonation prior might be limited to CCTV camera videos, given that the surveillance domain is more closely associated with the concept of "law enforcement agency" which is utilized for impersonation.Finally, combining both priors (Row 4) leads to improved performance compared to not utilizing any of them, primarily due to the positive impact of the anomaly prior.</p>
<p>Impact of different BLIP-2 models.As captioners, we consider different BLIP-2 [14] models and their ensemble for both UCF-Crime [24] and XD-Violence [36], and we present the results in Tables 6 and 7, respectively.In Tab. 6, the most effective strategy for UCF-Crime videos is employing an ensemble of all BLIP-2 models (Row 6).This involves generating captions for all frames in a video using all BLIP-2 models and relying on the visionlanguage model (VLM) to identify the semantically closest captions for each frame.The effectiveness of the ensemble might be attributed to the challenges posed by UCF-Crime videos.These videos, characterized by low-resolution CCTV footage, often lead captioning models to hallucinate scene descriptions.For instance, it is common to encounter captions, such as "a person riding a skateboard down a road" when the image only depicts a road in the absence of any specific event.The ensemble approach, by allowing the selection from a larger set of candidates, increases the likelihood of choosing more correct captions and filtering incorrect ones.</p>
<p>For XD-Violence, as shown in Tab. 7, utilizing the captions generated by flan-t5-xxl (Row 3) yields the best average A man is seen stealing a car from a parking garage multiple times, with the same black SUV parked in the lot.</p>
<p>XD-Violence</p>
<p>A small plane flies over a snowy mountain, then suddenly an explosion occurs in the snow below.</p>
<p>The scene is a car driving simulator game with a view of the dashboard.The car is driving on the road, and the game is shown from the driver's perspective For each video, we plot the anomaly score over frames computed by our method.We display some keyframes alongside their most aligned temporal summary (blue bounding boxes for normal frame predictions and red bounding boxes for abnormal frame predictions), illustrating the relevance among the predicted anomaly score, visual content, and description.Ground-truth anomalies are highlighted.sampled frames (N ), which is used to query the LLM for the temporal summary S i .Specifically, the temporal window duration T determines the time interval, while the number of sampled frames N determines the number of captions.First, we conduct experiments by adjusting the duration T to 2.5, 5, 10, and 20 seconds, while maintaining N = 10.The 10-second temporal window yields the highest AUC score (Row 3).This is in line with the fact that ImageBind [6] is trained with video clips of 10 seconds.</p>
<p>Subsequently, we maintain the temporal window's duration T at 10 seconds and vary the number of frames from 5 to 10 and 20.Notably, using 10 frames (Row 3), i.e. 1 frame every second, is the optimal choice within this experiment.Balancing the number of captions per snippet presents a trade-off with the quality of the summary.Too many captions may overwhelm with excessive and non-diverse content, while too few captions may result in limited coverage of the content.</p>
<p>C. Qualitative results</p>
<p>In Fig. 7, we present additional qualitative results demonstrating the effectiveness of our proposed LAVAD in detecting anomalies within a set of UCF-Crime [24] and XD-Violence [36] test videos.The figure showcases keyframes along with the most semantically similar temporal summaries.For example, in the video Shooting047 (Row 1, Column 1), LAVAD assigns a high anomaly score when the video is labeled abnormal.However, it also assigns a high anomaly score during the initial and final segments, despite these parts being labeled as normal.This discrepancy arises because the video begins with text describing the subsequent content, leading the LLM to attribute a high anomaly score.In the final part, our method correctly identifies abnormality as the frame depicts a person on the ground who has been shot.In the video Burglary079 (Row 1, Column 2), there is a false abnormal instance.This occurs because the temporal summary associated with that frame incorrectly suggests the presence of a man stealing a car.In reality, the video depicts a man behaving suspiciously near the car, leading to a wrong interpretation by the captioning module.In the XD-Violence videos (Row 2), an anomaly caused by an explosion is correctly detected (Row 2, Column 1), while a normal video is consistently predicted as normal for more than 17, 500 frames (Row 2, Column 2).</p>
<p>D. Limitations</p>
<p>We identify two main limitations of our work.Firstly, our method fully relies on pre-trained models from VLMs and LLMs, thus its performance greatly depends on i) how well the captioning model describes the visual content, ii) how reliable the LLM is when generating the anomaly scores, and iii) how aligned the multi-modal encoders are when processing videos from various domains.Secondly, our anomaly scores are primarily obtained via prompting LLMs.Although we conducted experiments investigating different prompting strategies, a systematic understanding of LLM prompting for VAD requires a community effort.</p>
<p>E. Broader Societal Impacts</p>
<p>While our work pioneers the technical aspect of leveraging LLMs for detecting anomalies in videos, there exist open ethical challenges for a broader concern.VAD systems are mostly applied to safety-related contexts, for private use or public interests.Prior to any deployment, it is crucial to first investigate the behaviors of LLM-based methods, mitigating any potential bias in LLMs and improving explainability.Our work serves as the first technical exploration of leveraging LLMs for training-free VAD, proving it as a competitive alternative.This is a necessary step to increase the awareness of the community on these important topics.</p>
<p>, weakly-supervised (i.e.video-level State-of-the-art methods</p>
<p>Figure 1 .
1
Figure 1.We introduce the first training-free method for video anomaly detection (VAD), diverging from state-of-the-art methods that are ALL training-based with different degrees of supervision.Our proposal, LAVAD, leverages modality-aligned vision-language models (VLMs) to query and enhance the anomaly scores generated by large language models (LLMs).</p>
<p>b l i p 2 -
2
o p t -6 .7 b -c o c o b l i p 2 -o p t -6 .7 b b l i p 2 -fl a n -t 5 -x x l b l i p 2 -fl a n -t 5 -x l b l i p 2 -fl a n -t 5 -x l -c o c o</p>
<p>Figure 4 .
4
Figure 4.The architecture of our proposed LAVAD for addressing training-free VAD.For each test video V, we first employ a captioning model to generate a caption Ci for each frame Ii ∈ V, forming a caption sequence C. Our Image-Text Caption Cleaning component addresses noisy and incorrect raw captions based on cross-modal similarity.We replace the raw caption with a caption Ĉi ∈ C whose textual embedding ET ( Ĉi) is most aligned to the image embedding EI (Ii), resulting in a cleaned caption sequence Ĉ.To account for scene context and dynamics, our LLM-based Anomaly Scoring component further aggregates the cleaned captions within a temporal window centered around each Ii by prompting the LLM to produce a temporal summary Si, forming a summary sequence S. The LLM is then queried to provide an anomaly score for each frame based on its Si, obtaining the initial anomaly scores a for all frames.Finally, our Video-Text Score Refinement component refines each ai by aggregating the initial anomaly scores of frames whose textual embeddings of the summaries are mostly aligned to the representation EV (Vi) of the video snippet Vi centered around Ii, leading to the final anomaly scores ã for detecting the anomalies ( anomalous frames are highlighted) within the video.</p>
<p>Figure 6 .
6
Figure 6.Results of LAVAD on UCF-Crime over the number of K semantically similar frames used for anomaly score refinement.</p>
<p>police officer John Sav was shot.Surveillance video shows the shooting.A man steals a car from a driveway, and Miami-Dade police officer John Sav is shot.Surveillance videos show the shooting and the theft of the car.A group of men are breaking into a car.They are wearing hoodies and carrying baseball bats.The scene is captured by a surveillance camera.</p>
<p>Figure 7 .
7
Figure 7.We showcase qualitative results obtained by LAVAD on four test videos, including two videos (top row) from UCF-Crime and two videos from XD-Violence (bottom row).For each video, we plot the anomaly score over frames computed by our method.We display some keyframes alongside their most aligned temporal summary (blue bounding boxes for normal frame predictions and red bounding boxes for abnormal frame predictions), illustrating the relevance among the predicted anomaly score, visual content, and description.Ground-truth anomalies are highlighted.</p>
<p>Figure 2. Bar plot of the VAD performance (AUC ROC) by querying LLMs with textual descriptions of video frames from various captioning models on the UCF-Crime test set.Different bars correspond to different variants of the captioning model BLIP-2 [14], while different colors indicate two different LLMs
RandomDyAnNetLlama-2-13B-chatMistral-7B-Instruct62.9262.5164.0861.9261.0660.1363.2860.2758.7459.28</p>
<p>-Crime is a large-scale dataset that is composed of 1900 long untrimmed real-world surveillance videos, covering 13 real-world anomalies.The training set consists of 800 normal and 810 anomalous videos, while the test set includes 150 normal and 140 anomalous videos.XD-Violence is another large-scale dataset for violence detection, comprising 4754 untrimmed videos with audio signals and weak labels that are collected from both movies and YouTube.XD-Violence captures 6 categories of anomalies and it is divided into a training set of 3954 videos and a test set of 800 videos.
METHODBACKBONEAUC(%)SULTANI et al. [24]C3D-RGB75.41SULTANI et al. [24]</p>
<p>Table 1
1-RGB70.46GCL [40]ResNext74.20TUR et al. [30]ResNet65.22TUR et al. [31]ResNet66.85DYANNET [27]I3D79.76ZS CLIP [22]ViT53.16ZS IMAGEBIND (IMAGE) [6]ViT53.65ZS IMAGEBIND (VIDEO) [6]ViT55.78LLAVA-1.5 [17]ViT72.84LAVADViT80.28
. Comparison with state-of-the-art weakly-supervised , one-class , unsupervised and training-free methods on the UCF-Crime dataset.The best results among training-free methods are highlighted in bold.</p>
<p>Table 2
211-
[26]mparison with state-of-the-art weakly-supervised , one-class , unsupervised and training-free methods on the XD-Violence dataset.*denotesresults reported in[26].The best results among training-free methods are highlighted in bold.</p>
<p>Table 3 .
3
Results of LAVAD variants w/o each proposed component on the UCF-Crime Dataset.
ANOMALY PRIOR IMPERSONATION AUC (%)✗✗79.32✓✗79.38✗✓80.28✓✓79.77</p>
<p>Table 4 .
4
Results of LAVAD on UCF-Crime with different priors in the context prompt when querying the LLM for anomaly scores.</p>
<p>Table 5 .
5
Results of LAVAD on XD-Violence with different priors in the context prompt when querying the LLM for anomaly scores.
ANOMALY PRIOR IMPERSONATION AP (%) AUC (%)✗✗60.3484.42✓✗62.0185.36✗✓58.8384.50✓✓60.7885.26</p>
<p>Table 6 .
6BLIP-2AUCFLAN-T5-XLFLAN-T5-XL-COCOFLAN-T5-XXLOPT-6.7BOPT-6.7B-COCO(%)✓✗✗✗✗74.19✗✓✗✗✗74.49✗✗✓✗✗74.38✗✗✗✓✗75.50✗✗✗✗✓73.94✓✓✓✓✓80.28
Results of LAVAD on UCF-Crime with different BLIP-2 model variants in our Image-Text Caption Cleaning technique.</p>
<p>Table 7 .
7
Results of LAVAD on XD-Violence with different BLIP-2 model variants in our Image-Text Caption Cleaning technique.
BLIP-2APAUCFLAN-T5-XLFLAN-T5-XL-COCOFLAN-T5-XXLOPT-6.7BOPT-6.7B-COCO(%)(%)✓✗✗✗✗61.09 85.16✗✓✗✗✗57.41 82.78✗✗✓✗✗62.01 85.36✗✗✗✓✗56.55 82.42✗✗✗✗✓54.71 82.93✓✓✓✓✓59.62 84.90precision (AP). Other BLIP-2 variants for XD-Violence mayprovide captions that prioritize foreground objects, poten-tially overlooking background elements constituting anoma-lies (e.g. a vehicle enveloped in smoke on a busy street),yet better aligning with the VLM's representation of thevideo frames. Hence, when employing the ensemble ofBLIP-2 models (Row 6), captions that specifically highlightelements constituting anomalies are not chosen as the seman-tically closest captions to video frames in the cleaning step,
with a negative impact on the anomaly scoring phase.Temporal window's duration and number of sampled frames.In Tab. 8, we evaluate the impact of varying the duration of the temporal window (T ) and the number of</p>
<p>Table 8 .
8T(S) N AUC (%)2.51079.3351078.10101080.28201079.2410577.48102074.45
Results of LAVAD on UCF-Crime with different combinations of temporal window duration (T ) and number of sampled frames per window (N ).</p>
<p>The exact form of P F can be found in the Supp. Mat.
Acknowledgments.This work is supported by MUR PNRR project FAIR -Future AI Research (PE00000013), funded by NextGeneration EU and by PRECRISIS, funded by EU Internal Security Fund (ISFP-2022-TFI-AG-PROTECT-02-101100539).We acknowledge the CINECA award under the ISCRA initiative, for the availability of high-performance computing resources and support.Harnessing Large Language Models for Training-free Video Anomaly DetectionSupplementary MaterialIn this supplementary material, we first provide the exact form of the prompts employed in our method and then we present additional experimental analyses.Specifically, we first present the impact of the task-related priors in prompting the anomaly scores on XD-Violence[36].We then present the impact of captioning models, i.e. different variants of BLIP-2 models, for the VAD performance of our method on both XD-Violence[36]and UCF-Crime[24]datasets.Finally, we ablate the hyperparameters in constructing temporal windows to justify our design choice.Moreover, we describe the limitations and broader social impacts of our work, and we showcase additional qualitative results that demonstrate temporal summaries and the detection results.More qualitative results in the form of videos can be conveniently accessed on the project website at https://lucazanella.github.io/lavad/.A. PromptsThe prompts utilized in our approach serve distinct purposes.The contextual prompt P C provides priors to the LLM for VAD.In line with the findings of our ablation studies presented in Tab. 4 and in Tab. 5, this prompt differs for UCF-Crime[24]and XD-Violence[36].For UCF-Crime, the prompt is structured as: "If you were a law enforcement agency, how would you rate the scene described on a scale from 0 to 1, with 0 representing a standard scene and 1 denoting a scene with suspicious activities?".In contrast, for XD-Violence, the prompt has the form: "How would you rate the scene described on a scale from 0 to 1, with 0 representing a standard scene and 1 denoting a scene with suspicious or potentially criminal activities?".The prompt P F provides guidance to the LLM for the desired output format, aimed at facilitating automated text parsing.This prompt remains consistent across both datasets and is defined as follows: "Please provide the response in the form of a Python list and respond with only one number in the provided list below [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] without any textual explanation.It should begin with '[' and end with ']'.".Lastly, the prompt P S is employed to obtain a temporal summary S i for each frame I i .The prompt is formulated as follows: "Please summarize what happened in few sentences, based on the following temporal description of a scene.Do not include any unnecessary details or descriptions.".B. Additional analysesTask priors in the context prompt.In Tab. 5 we present the impact of different priors in the context prompt P C , i.e. im-
Traffic anomaly detection via perspective map based on spatial-temporal information matrix. Shuai Bai, Zhiqun He, Yu Lei, Wei Wu, Chengkai Zhu, Ming Sun, Junjie Yan, CVPRW. 20191</p>
<p>Mgfn: Magnitudecontrastive glance-and-focus network for weakly-supervised video anomaly detection. Yingxian Chen, Zhengzhe Liu, Baoheng Zhang, Wilton Fok, Xiaojuan Qi, Yik-Chung Wu, AAAI. 2023</p>
<p>Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, ICLR2021</p>
<p>Semantic anomaly detection with large language models. Amine Elhafsi, Rohan Sinha, Christopher Agia, Edward Schmerling, Marco Issa Ad Nesnas, Pavone, Autonomous Robots. 22023</p>
<p>Mist: Multiple instance self-training framework for video anomaly detection. Jia-Chang Feng, Fa-Ting Hong, Wei-Shi Zheng, CVPR. 2021</p>
<p>Imagebind: One embedding space to bind them all. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra, CVPR. 20236</p>
<p>Zhaopeng Gu, Bingke Zhu, Guibo Zhu, Yingying Chen, Ming Tang, Jinqiao Wang, Anomalygpt: Detecting industrial anomalies using large vision-language models. 2023</p>
<p>Learning temporal regularity in video sequences. Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Larry S Amit K Roy-Chowdhury, Davis, CVPR. 2016</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv, 2023. 3Mistral 7b. </p>
<p>Survey on video anomaly detection in dynamic scenes with moving cameras. Runyu Jiao, Yi Wan, Fabio Poiesi, Yiming Wang, Artificial Intelligence Review. 12023</p>
<p>Clip-tsa: Clip-assisted temporal self-attention for weaklysupervised video anomaly detection. Kevin Hyekang, Khoa Joo, Kashu Vo, Ngan Yamazaki, Le, ICIP. 202316</p>
<p>Unsupervised video anomaly detection based on similarity with predefined text descriptions. Jaehyun Kim, Seongwook Yoon, Taehyeon Choi, Sanghoon Sull, Sensors. 232023</p>
<p>Scale-aware spatio-temporal relation learning for video anomaly detection. Guoqiu Li, Guanxiong Cai, Xingyu Zeng, Rui Zhao, ECCV. 202216</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, ICML, 2023. 3, 4. 61</p>
<p>Self-training multisequence learning with transformer for weakly supervised video anomaly detection. Shuo Li, Fang Liu, Licheng Jiao, AAAI. 202216</p>
<p>Isolationbased anomaly detection. Tony Fei, Kai Ming Liu, Zhi-Hua Ting, Zhou, 2012ACM TKDD</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, arXiv, 2023. 6</p>
<p>A hybrid video anomaly detection framework via memory-augmented flow reconstruction and flow-guided frame prediction. Zhian Liu, Yongwei Nie, Chengjiang Long, Qing Zhang, Guiqing Li, ICCV. 20211</p>
<p>Abnormal event detection at 150 fps in matlab. Cewu Lu, Jianping Shi, Jiaya Jia, ICCV. 2013</p>
<p>Learning normal dynamics in videos with meta prototype network. Hui Lv, Chen Chen, Zhen Cui, Chunyan Xu, Yong Li, Jian Yang, CVPR. 20211</p>
<p>Learning memory-guided normality for anomaly detection. Hyunjong Park, Jongyoun Noh, Bumsub Ham, CVPR. 20201</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, ICML. 2021</p>
<p>Moncef Gabbouj, and Alexandros Iosifidis. Fahad Sohrab, Jenni Raitoharju, ICPR. 2018Subspace support vector data description</p>
<p>Real-world anomaly detection in surveillance videos. Waqas Sultani, Chen Chen, Mubarak Shah, CVPR. 2018. 1, 2, 3, 5, 6, 7</p>
<p>Hierarchical semantic contrast for scene-aware video anomaly detection. Shengyang Sun, Xiaojin Gong, CVPR. 20231</p>
<p>Rareanom: A benchmark video dataset for rare type anomalies. Kamalakar Vijay Thakare, Debi Prosad Dogra, Heeseung Choi, Haksub Kim, Ig-Jae Kim, Pattern Recognition. 6272023</p>
<p>Dyannet: A scene dynamicity guided self-trained video anomaly detection network. Kamalakar Vijay Thakare, Yash Raghuwanshi, Debi Prosad Dogra, Heeseung Choi, Ig-Jae Kim, WACV. 2023. 2, 3, 6, 7</p>
<p>Weakly-supervised video anomaly detection with robust temporal feature magnitude learning. Yu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan W Verjans, Gustavo Carneiro, ICCV. 202116</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv, 2023. 3Faisal Azhar, et al. Llama: Open and efficient foundation language models. 6</p>
<p>Exploring diffusion models for unsupervised video anomaly detection. Anil Osman Tur, Nicola Dall'asen, Cigdem Beyan, Elisa Ricci, ICIP. 202316</p>
<p>Unsupervised video anomaly detection with diffusion models conditioned on compact motion representations. Anil Osman Tur, Nicola Dall'asen, Cigdem Beyan, Elisa Ricci, ICIAP. 202316</p>
<p>Anomaly candidate identification and starting time estimation of vehicles from traffic videos. Gaoang Wang, Xinyu Yuan, Aotian Zheng, Hung-Min Hsu, Jenq-Neng Hwang, CVPRW. 20191</p>
<p>Gods: Generalized one-class discriminative subspaces for anomaly detection. Jue Wang, Anoop Cherian, ICCV. 2019</p>
<p>Self-supervised sparse representation for video anomaly detection. Jhih-Ciang Wu, He-Yen Hsieh, Ding-Jie Chen, Chiou-Shann, Tyng-Luh Fuh, Liu, ECCV. 2022</p>
<p>Learning causal temporal relation and feature discrimination for anomaly detection. Peng Wu, Jing Liu, IEEE TIP. 162021</p>
<p>Not only look, but also listen: Learning multimodal violence detection under weak supervision. Peng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, Zhiwei Yang, ECCV. 2020. 2, 5, 6, 1, 3</p>
<p>Feature prediction diffusion model for video anomaly detection. Cheng Yan, Shiyu Zhang, Yang Liu, Guansong Pang, Wenjun Wang, ICCV. 20231</p>
<p>Old is gold: Redefining the adversarially learned one-class classifier training paradigm. Muhammad Zaigham Zaheer, Jin-Ha Lee, Marcella Astrid, Seung-Ik Lee, CVPR. 20201</p>
<p>Claws: Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection. Muhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, Seung-Ik Lee, ECCV. 2020</p>
<p>Generative cooperative learning for unsupervised video anomaly detection. Arif Zaigham Zaheer, Mahmood, Mattia Haris Khan, Fisher Segu, Seung-Ik Yu, Lee, CVPR. 2022. 1, 2, 6, 7</p>
<p>Temporal convolutional network with complementary inner bag loss for weakly supervised anomaly detection. Jiangong Zhang, Laiyun Qing, Jun Miao, ICIP. 2019</p>
<p>Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection. Jia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H Li, Ge Li, CVPR. 2019</p>            </div>
        </div>

    </div>
</body>
</html>
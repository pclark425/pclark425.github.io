<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8774 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8774</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8774</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-251bdaff7521e60fe81fc375acfd34951c7f13ea</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/251bdaff7521e60fe81fc375acfd34951c7f13ea" target="_blank">ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This paper presents ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning to improve performance, and presents an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets.</p>
                <p><strong>Paper Abstract:</strong> Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks. More details at https://github.com/IBM/regen.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8774.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8774.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TripleLinearization<S/P/O></td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triple Linearization with <S>, <P>, <O> Boundary Markers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Graphs (RDF triples) are serialized into token sequences by emitting each triple as a sequence with explicit subject/predicate/object boundary marker tokens (<S>, <P>, <O>); these special tokens are added to the model vocabulary and are kept indivisible by the tokenizer so the encoder-decoder PLM can learn positional roles within the linearized triple sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization (token-marker based)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each triple (s, p, o) is converted to a contiguous token subsequence by inserting dedicated marker tokens: <S> followed by subject tokens, <P> followed by predicate tokens, and <O> followed by object tokens. The full graph is represented as a sequence of such triple subsequences (in some list order). The model vocabulary is expanded with these special indivisible tokens so they are not split by subword tokenizers.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF-style triple sets (DBpedia, Wikidata-derived triples)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize list-of-triples into a flat token sequence by concatenating per-triple segments of the form [<S>, subject_tokens, <P>, predicate_tokens, <O>, object_tokens]; in practice the authors used the order provided by the dataset for the triple list, but treat order as ambiguous (see augmentation). For TEKGEN they reconstruct s/p/o boundaries first (via lookup) then apply same linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-Text (G2T: RDF-to-text / data-to-text) and Text-to-Graph (T2G: text-to-RDF / semantic parsing) sequence-to-sequence training with pretrained encoder-decoder LMs (T5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used with T5 PLM; WebNLG+ 2020 (v3.0) G2T (t5-large): ReGen G2T.CE (linearization + CE) BLEU=0.553, BLEU_NLTK=0.549, METEOR=0.418, chrF++=0.694; ReGen G2T.RL (SCST starting from CE; same linearization) BLEU=0.563, BLEU_NLTK=0.559, METEOR=0.425, chrF++=0.706 (Table 1). WebNLG+ T2G (t5 models): ReGen T2G.CE Exact F1=0.723 (Precision=0.714, Recall=0.738); ReGen T2G.RL Exact F1=0.720 (Table 2). TEKGEN G2T (t5-large): ReGen-CE Test BLEU=0.241, ReGen-SCST Test BLEU=0.262; TEKGEN T2G: ReGen-CE Test F1=0.619, ReGen-SCST Test F1=0.623 (Tables 3 and 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>ReGen's linearization-based approach outperformed prior top systems on WebNLG+ 2020: e.g., Amazon AI (top challenge) reported BLEU=0.540; OSU Neural NLG BLEU=0.535; FBConvAI BLEU=0.527; bt5 BLEU=0.517 (Table 1). Other teams used alternative input modeling (e.g., R-GCN + T5 + canonicalization, various RDF modeling strategies), but this paper shows that a straightforward linearization with S/P/O markers plus PLM fine-tuning (and RL) achieves superior metrics on the evaluated splits.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables direct framing of G2T and T2G as Seq2Seq tasks so pretrained encoder-decoder LMs (T5/BART) can be fine-tuned; marker tokens explicitly expose role boundaries (subject/predicate/object) which aids model learning; simple to implement; supports data augmentation via reordering of triples; expands vocabulary to keep role markers indivisible, avoiding tokenization artifacts; compatible with RL (SCST) sequence training.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Linearization is not unique (many possible graph traversals), producing ambiguity; serialized sequences can be long (authors used max length 192) which increases decoding cost and memory; evaluation of generated graphs is combinatorially expensive since scoring considers all permutations (factorial growth) making large triple sets costly to score; linearization may underrepresent graph structure (loss of explicit graph connectivity beyond triple order).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>RL fine-tuning did not reliably improve T2G (text-to-graph) over CE in WebNLG+: SCST sometimes failed to improve Exact F1 (T2G.RL slightly worse than T2G.CE), possibly because Exact F1 reward is too rigid; when sampled sequence equals greedy sequence (no difference between sample and baseline), SCST produces no learning signal for that sample; unseen categories (test-only categories) produce substantially lower scores than seen categories even after RL; TEKGEN paired data sometimes have linearized graphs that do not cover all concepts in text, causing underestimation by n-gram metrics when using linearized sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8774.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8774.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TripleOrderShuffleAug</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Triple Order Permutation Data Augmentation (random shuffling of triple order)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>To mitigate the ambiguity of triple ordering in linearized sequences and to prevent the model from memorizing a specific triples order, the authors augment training data by including multiple random permutations of the triple list for the same graph.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization with permutation augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Generate additional training examples by reordering the concatenated triple subsequences (random permutations of the triple list) while keeping the per-triple <S>/<P>/<O> internal structure intact. The core representation remains the marker-based linearization, but multiple orderings are provided as distinct input sequences for the same graph.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triple sets</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Start from dataset-provided list of triples; create several permuted orders of that list and linearize each permutation as described above, yielding multiple linearized sequences per graph for training.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>G2T and T2G Seq2Seq training (improves robustness during PLM fine-tuning and RL training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No single-number ablation isolating only permutation augmentation is reported; authors state qualitatively that augmentation is used to avoid the model memorizing triple order and to improve generalization of T2G (Section 2 and 4 Experimental Setup). Downstream results reported (with augmentation in the pipeline) are the G2T/T2G metrics in Tables 1-4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Unlike some other teams which applied canonicalization or heavier RDF preprocessing, the authors rely on minimal preprocessing plus permutation augmentation; they report stronger overall results than the top challenge systems without heavy canonicalization pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces model sensitivity to arbitrary triple sequence order and helps the model learn graph content rather than memorized order; simple data-augmentation approach that leverages the inherent non-uniqueness of linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increases dataset size and training time due to multiple permutations; does not change the core limitation that linearization loses explicit graph connectivity beyond triple-local structure.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No isolated failure numbers provided; authors note that despite augmentation, RL still struggles on T2G for WebNLG+ which suggests augmentation alone cannot overcome challenges in rigid graph-reconstruction reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8774.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8774.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEKGEN-BoundaryReconstruction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TEKGEN Subject/Relation/Object Boundary Reconstruction via Wikidata Lookup</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Because the raw TEKGEN corpus lacked explicit KG (subject/predicate/object) boundaries, the authors processed TEKGEN by using Wikidata property lookups to recover and mark KG boundaries (s/p/o), enabling conventional linearization and evaluation for T2G and G2T tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>boundary reconstruction + linearization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Map TEKGEN sentence–triple text pairs to explicit triples by matching relation/property mentions against Wikidata properties (lookup) and then marking subject, predicate, and object boundaries so that triples can be serialized with <S>/<P>/<O> markers for model input/output like other datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Wikidata-derived knowledge graph / synthetic KG-text pairs (TEKGEN)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Use Wikidata properties lookup to convert sentence-triple text into list-of-triples with explicit token boundaries; then linearize each triple with <S>, <P>, <O> markers as done for WebNLG. The authors also limited validation/test subsets for processing (validation 5K, test 50K) and created a training split of 6.3M sentence-triple pairs after processing.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>G2T (graph-to-text) and T2G (text-to-graph) training/evaluation at large scale on TEKGEN.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>TEKGEN G2T (t5-large): ReGen-CE Test BLEU=0.241, ReGen-SCST Test BLEU=0.262, METEOR improved similarly (Table 3). TEKGEN T2G (t5-large): ReGen-CE Test F1=0.619, ReGen-SCST Test F1=0.623 (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The authors claim their processed TEKGEN (with added boundaries) enables conventional evaluation; no head-to-head comparison to alternate TEKGEN processing pipelines is provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables use of a large-scale dataset (TEKGEN) for both T2G and G2T tasks by adding necessary KG boundaries; scales training data greatly (6.3M pairs reported), giving a large training set for PLM fine-tuning and RL.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Processing relies on heuristic/lookup matching against Wikidata which can be noisy; the TEKGEN sentence-graph alignments often do not fully cover all concepts in the text leading to weaker n-gram metric scores and underestimation of model quality.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Authors note that TEKGEN's linearized graphs often do not cover all concepts in text, so n-gram based evaluation underestimates generation quality; this structural mismatch causes lower measured scores and complicates interpretation of improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8774.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8774.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HybridTaskPrefixing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid Input Encoding with Task Prefix Strings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single PLM is trained to handle both directions (T2G and G2T) by prefixing each input with a task-specific string ('Text to Graph:' or 'Graph to Text:') and concatenating the modal inputs so the same encoder-decoder can learn both transformation directions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>task-prefix hybrid encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Create a single input by prefixing modality-specific samples with an explicit textual token indicating the task (e.g., 'Text to Graph:' for text inputs, 'Graph to Text:' for graph inputs) and provide the appropriate target (graph or text) during training. For graph inputs the graphs are linearized with <S>/<P>/<O> markers before prefixing.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triples (and textual modality)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Same linearization for graphs; prepend a task-specific string to the input so the model learns which direction to generate. The combined model is fine-tuned on mixed batches of T2G and G2T examples.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Bi-directional generation: Graph-to-text and Text-to-graph within a single model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors report early in experiments hybrid models achieved G2T BLEU ≈ 0.547 (outperforming Challenge winning team early), but final specialized models performed better when batch sizes increased (specialized G or T models achieved higher scores). Final published bests are from specialized models (Tables 1 and 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Hybrid model initially outperformed top-challenge systems but later specialized models (trained only on one direction) surpassed hybrid performance when training configuration (batch size) was adjusted.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Single model can perform both directions, potentially allowing parameter sharing and dual-learning benefits; simple to implement via textual prefixing.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Hybrid models can be outperformed by specialized models under certain training regimes (larger batch sizes favored specialized models in authors' experiments); mixing modalities may complicate optimization and require careful curriculum/hyperparameter tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Hybrid model performed worse than specialized models once batch size increased (authors report specialized models took lead when batch size became larger (20-24 samples)). No other explicit failure cases beyond optimization sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8774.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8774.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Canonicalization (related)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Canonicalization of Graph Surface Forms (as used by other teams)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned in related work: canonicalization refers to mapping surface tokens in triples/text to normalized placeholders or abstracted forms (e.g., entity rewriting) to simplify generation and reduce sparsity; some prior top teams combined canonicalization with neural modules (e.g., R-GCN + T5).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>i^2 : A plan-and-pretrain approach for knowledge graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>canonicalization / abstraction</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Replace concrete entity mentions and/or literals with normalized placeholders or canonical forms prior to model input/output (often combined with graph encoders). This reduces lexical sparsity and can allow the model to rely on surface templates that are re-filled post-generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs / RDF triples (DBpedia-style)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Preprocessing step outside linearization: normalize/replace entity and relation surface forms using rules or learned mappings, then feed canonicalized triples (possibly with graph encoder features) into the generation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-Text (especially in WebNLG-style challenges) and related data-to-text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Referenced team (Amazon AI) that used canonicalization reported G2T BLEU=0.540 (Table 1) — canonicalization was part of their pipeline but this paper does not provide an isolated ablation for canonicalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Canonicalization combined with graph encoders (R-GCN) is a stronger pipeline used by some top teams; the present paper demonstrates that simple linearization + PLM fine-tuning (without heavy canonicalization) can match or exceed those prior challenge numbers when combined with RL.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces lexical sparsity and helps models generalize across entity surface forms; can simplify learning for template-like generation.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires extra preprocessing rules/mappings and de-canonicalization postprocessing; may lose natural surface variation and require alignment to reinsert original surface forms.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not directly evaluated in this paper; included as related-work mention only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models', 'publication_date_yy_mm': '2021-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>i^2 : A plan-and-pretrain approach for knowledge graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Leveraging large pretrained models for WebNLG 2020 <em>(Rating: 2)</em></li>
                <li>Improving text-to-text pretrained models for the graph-to-text task <em>(Rating: 2)</em></li>
                <li>Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training <em>(Rating: 2)</em></li>
                <li>Reinforcement learning based graph-to-sequence model for natural question generation <em>(Rating: 2)</em></li>
                <li>Investigating pretrained language models for graph-to-text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8774",
    "paper_id": "paper-251bdaff7521e60fe81fc375acfd34951c7f13ea",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "TripleLinearization&lt;S/P/O&gt;",
            "name_full": "Triple Linearization with &lt;S&gt;, &lt;P&gt;, &lt;O&gt; Boundary Markers",
            "brief_description": "Graphs (RDF triples) are serialized into token sequences by emitting each triple as a sequence with explicit subject/predicate/object boundary marker tokens (&lt;S&gt;, &lt;P&gt;, &lt;O&gt;); these special tokens are added to the model vocabulary and are kept indivisible by the tokenizer so the encoder-decoder PLM can learn positional roles within the linearized triple sequence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "linearization (token-marker based)",
            "representation_description": "Each triple (s, p, o) is converted to a contiguous token subsequence by inserting dedicated marker tokens: &lt;S&gt; followed by subject tokens, &lt;P&gt; followed by predicate tokens, and &lt;O&gt; followed by object tokens. The full graph is represented as a sequence of such triple subsequences (in some list order). The model vocabulary is expanded with these special indivisible tokens so they are not split by subword tokenizers.",
            "graph_type": "Knowledge graphs / RDF-style triple sets (DBpedia, Wikidata-derived triples)",
            "conversion_method": "Serialize list-of-triples into a flat token sequence by concatenating per-triple segments of the form [&lt;S&gt;, subject_tokens, &lt;P&gt;, predicate_tokens, &lt;O&gt;, object_tokens]; in practice the authors used the order provided by the dataset for the triple list, but treat order as ambiguous (see augmentation). For TEKGEN they reconstruct s/p/o boundaries first (via lookup) then apply same linearization.",
            "downstream_task": "Graph-to-Text (G2T: RDF-to-text / data-to-text) and Text-to-Graph (T2G: text-to-RDF / semantic parsing) sequence-to-sequence training with pretrained encoder-decoder LMs (T5).",
            "performance_metrics": "Used with T5 PLM; WebNLG+ 2020 (v3.0) G2T (t5-large): ReGen G2T.CE (linearization + CE) BLEU=0.553, BLEU_NLTK=0.549, METEOR=0.418, chrF++=0.694; ReGen G2T.RL (SCST starting from CE; same linearization) BLEU=0.563, BLEU_NLTK=0.559, METEOR=0.425, chrF++=0.706 (Table 1). WebNLG+ T2G (t5 models): ReGen T2G.CE Exact F1=0.723 (Precision=0.714, Recall=0.738); ReGen T2G.RL Exact F1=0.720 (Table 2). TEKGEN G2T (t5-large): ReGen-CE Test BLEU=0.241, ReGen-SCST Test BLEU=0.262; TEKGEN T2G: ReGen-CE Test F1=0.619, ReGen-SCST Test F1=0.623 (Tables 3 and 4).",
            "comparison_to_others": "ReGen's linearization-based approach outperformed prior top systems on WebNLG+ 2020: e.g., Amazon AI (top challenge) reported BLEU=0.540; OSU Neural NLG BLEU=0.535; FBConvAI BLEU=0.527; bt5 BLEU=0.517 (Table 1). Other teams used alternative input modeling (e.g., R-GCN + T5 + canonicalization, various RDF modeling strategies), but this paper shows that a straightforward linearization with S/P/O markers plus PLM fine-tuning (and RL) achieves superior metrics on the evaluated splits.",
            "advantages": "Enables direct framing of G2T and T2G as Seq2Seq tasks so pretrained encoder-decoder LMs (T5/BART) can be fine-tuned; marker tokens explicitly expose role boundaries (subject/predicate/object) which aids model learning; simple to implement; supports data augmentation via reordering of triples; expands vocabulary to keep role markers indivisible, avoiding tokenization artifacts; compatible with RL (SCST) sequence training.",
            "disadvantages": "Linearization is not unique (many possible graph traversals), producing ambiguity; serialized sequences can be long (authors used max length 192) which increases decoding cost and memory; evaluation of generated graphs is combinatorially expensive since scoring considers all permutations (factorial growth) making large triple sets costly to score; linearization may underrepresent graph structure (loss of explicit graph connectivity beyond triple order).",
            "failure_cases": "RL fine-tuning did not reliably improve T2G (text-to-graph) over CE in WebNLG+: SCST sometimes failed to improve Exact F1 (T2G.RL slightly worse than T2G.CE), possibly because Exact F1 reward is too rigid; when sampled sequence equals greedy sequence (no difference between sample and baseline), SCST produces no learning signal for that sample; unseen categories (test-only categories) produce substantially lower scores than seen categories even after RL; TEKGEN paired data sometimes have linearized graphs that do not cover all concepts in text, causing underestimation by n-gram metrics when using linearized sequences.",
            "uuid": "e8774.0",
            "source_info": {
                "paper_title": "ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "TripleOrderShuffleAug",
            "name_full": "Triple Order Permutation Data Augmentation (random shuffling of triple order)",
            "brief_description": "To mitigate the ambiguity of triple ordering in linearized sequences and to prevent the model from memorizing a specific triples order, the authors augment training data by including multiple random permutations of the triple list for the same graph.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "linearization with permutation augmentation",
            "representation_description": "Generate additional training examples by reordering the concatenated triple subsequences (random permutations of the triple list) while keeping the per-triple &lt;S&gt;/&lt;P&gt;/&lt;O&gt; internal structure intact. The core representation remains the marker-based linearization, but multiple orderings are provided as distinct input sequences for the same graph.",
            "graph_type": "Knowledge graphs / RDF triple sets",
            "conversion_method": "Start from dataset-provided list of triples; create several permuted orders of that list and linearize each permutation as described above, yielding multiple linearized sequences per graph for training.",
            "downstream_task": "G2T and T2G Seq2Seq training (improves robustness during PLM fine-tuning and RL training).",
            "performance_metrics": "No single-number ablation isolating only permutation augmentation is reported; authors state qualitatively that augmentation is used to avoid the model memorizing triple order and to improve generalization of T2G (Section 2 and 4 Experimental Setup). Downstream results reported (with augmentation in the pipeline) are the G2T/T2G metrics in Tables 1-4.",
            "comparison_to_others": "Unlike some other teams which applied canonicalization or heavier RDF preprocessing, the authors rely on minimal preprocessing plus permutation augmentation; they report stronger overall results than the top challenge systems without heavy canonicalization pipelines.",
            "advantages": "Reduces model sensitivity to arbitrary triple sequence order and helps the model learn graph content rather than memorized order; simple data-augmentation approach that leverages the inherent non-uniqueness of linearization.",
            "disadvantages": "Increases dataset size and training time due to multiple permutations; does not change the core limitation that linearization loses explicit graph connectivity beyond triple-local structure.",
            "failure_cases": "No isolated failure numbers provided; authors note that despite augmentation, RL still struggles on T2G for WebNLG+ which suggests augmentation alone cannot overcome challenges in rigid graph-reconstruction reward signals.",
            "uuid": "e8774.1",
            "source_info": {
                "paper_title": "ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "TEKGEN-BoundaryReconstruction",
            "name_full": "TEKGEN Subject/Relation/Object Boundary Reconstruction via Wikidata Lookup",
            "brief_description": "Because the raw TEKGEN corpus lacked explicit KG (subject/predicate/object) boundaries, the authors processed TEKGEN by using Wikidata property lookups to recover and mark KG boundaries (s/p/o), enabling conventional linearization and evaluation for T2G and G2T tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "boundary reconstruction + linearization",
            "representation_description": "Map TEKGEN sentence–triple text pairs to explicit triples by matching relation/property mentions against Wikidata properties (lookup) and then marking subject, predicate, and object boundaries so that triples can be serialized with &lt;S&gt;/&lt;P&gt;/&lt;O&gt; markers for model input/output like other datasets.",
            "graph_type": "Wikidata-derived knowledge graph / synthetic KG-text pairs (TEKGEN)",
            "conversion_method": "Use Wikidata properties lookup to convert sentence-triple text into list-of-triples with explicit token boundaries; then linearize each triple with &lt;S&gt;, &lt;P&gt;, &lt;O&gt; markers as done for WebNLG. The authors also limited validation/test subsets for processing (validation 5K, test 50K) and created a training split of 6.3M sentence-triple pairs after processing.",
            "downstream_task": "G2T (graph-to-text) and T2G (text-to-graph) training/evaluation at large scale on TEKGEN.",
            "performance_metrics": "TEKGEN G2T (t5-large): ReGen-CE Test BLEU=0.241, ReGen-SCST Test BLEU=0.262, METEOR improved similarly (Table 3). TEKGEN T2G (t5-large): ReGen-CE Test F1=0.619, ReGen-SCST Test F1=0.623 (Table 4).",
            "comparison_to_others": "The authors claim their processed TEKGEN (with added boundaries) enables conventional evaluation; no head-to-head comparison to alternate TEKGEN processing pipelines is provided in the paper.",
            "advantages": "Enables use of a large-scale dataset (TEKGEN) for both T2G and G2T tasks by adding necessary KG boundaries; scales training data greatly (6.3M pairs reported), giving a large training set for PLM fine-tuning and RL.",
            "disadvantages": "Processing relies on heuristic/lookup matching against Wikidata which can be noisy; the TEKGEN sentence-graph alignments often do not fully cover all concepts in the text leading to weaker n-gram metric scores and underestimation of model quality.",
            "failure_cases": "Authors note that TEKGEN's linearized graphs often do not cover all concepts in text, so n-gram based evaluation underestimates generation quality; this structural mismatch causes lower measured scores and complicates interpretation of improvements.",
            "uuid": "e8774.2",
            "source_info": {
                "paper_title": "ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "HybridTaskPrefixing",
            "name_full": "Hybrid Input Encoding with Task Prefix Strings",
            "brief_description": "A single PLM is trained to handle both directions (T2G and G2T) by prefixing each input with a task-specific string ('Text to Graph:' or 'Graph to Text:') and concatenating the modal inputs so the same encoder-decoder can learn both transformation directions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "task-prefix hybrid encoding",
            "representation_description": "Create a single input by prefixing modality-specific samples with an explicit textual token indicating the task (e.g., 'Text to Graph:' for text inputs, 'Graph to Text:' for graph inputs) and provide the appropriate target (graph or text) during training. For graph inputs the graphs are linearized with &lt;S&gt;/&lt;P&gt;/&lt;O&gt; markers before prefixing.",
            "graph_type": "Knowledge graphs / RDF triples (and textual modality)",
            "conversion_method": "Same linearization for graphs; prepend a task-specific string to the input so the model learns which direction to generate. The combined model is fine-tuned on mixed batches of T2G and G2T examples.",
            "downstream_task": "Bi-directional generation: Graph-to-text and Text-to-graph within a single model.",
            "performance_metrics": "Authors report early in experiments hybrid models achieved G2T BLEU ≈ 0.547 (outperforming Challenge winning team early), but final specialized models performed better when batch sizes increased (specialized G or T models achieved higher scores). Final published bests are from specialized models (Tables 1 and 2).",
            "comparison_to_others": "Hybrid model initially outperformed top-challenge systems but later specialized models (trained only on one direction) surpassed hybrid performance when training configuration (batch size) was adjusted.",
            "advantages": "Single model can perform both directions, potentially allowing parameter sharing and dual-learning benefits; simple to implement via textual prefixing.",
            "disadvantages": "Hybrid models can be outperformed by specialized models under certain training regimes (larger batch sizes favored specialized models in authors' experiments); mixing modalities may complicate optimization and require careful curriculum/hyperparameter tuning.",
            "failure_cases": "Hybrid model performed worse than specialized models once batch size increased (authors report specialized models took lead when batch size became larger (20-24 samples)). No other explicit failure cases beyond optimization sensitivity.",
            "uuid": "e8774.3",
            "source_info": {
                "paper_title": "ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models",
                "publication_date_yy_mm": "2021-08"
            }
        },
        {
            "name_short": "Canonicalization (related)",
            "name_full": "Canonicalization of Graph Surface Forms (as used by other teams)",
            "brief_description": "Mentioned in related work: canonicalization refers to mapping surface tokens in triples/text to normalized placeholders or abstracted forms (e.g., entity rewriting) to simplify generation and reduce sparsity; some prior top teams combined canonicalization with neural modules (e.g., R-GCN + T5).",
            "citation_title": "i^2 : A plan-and-pretrain approach for knowledge graph-to-text generation",
            "mention_or_use": "mention",
            "representation_name": "canonicalization / abstraction",
            "representation_description": "Replace concrete entity mentions and/or literals with normalized placeholders or canonical forms prior to model input/output (often combined with graph encoders). This reduces lexical sparsity and can allow the model to rely on surface templates that are re-filled post-generation.",
            "graph_type": "Knowledge graphs / RDF triples (DBpedia-style)",
            "conversion_method": "Preprocessing step outside linearization: normalize/replace entity and relation surface forms using rules or learned mappings, then feed canonicalized triples (possibly with graph encoder features) into the generation pipeline.",
            "downstream_task": "Graph-to-Text (especially in WebNLG-style challenges) and related data-to-text generation.",
            "performance_metrics": "Referenced team (Amazon AI) that used canonicalization reported G2T BLEU=0.540 (Table 1) — canonicalization was part of their pipeline but this paper does not provide an isolated ablation for canonicalization.",
            "comparison_to_others": "Canonicalization combined with graph encoders (R-GCN) is a stronger pipeline used by some top teams; the present paper demonstrates that simple linearization + PLM fine-tuning (without heavy canonicalization) can match or exceed those prior challenge numbers when combined with RL.",
            "advantages": "Reduces lexical sparsity and helps models generalize across entity surface forms; can simplify learning for template-like generation.",
            "disadvantages": "Requires extra preprocessing rules/mappings and de-canonicalization postprocessing; may lose natural surface variation and require alignment to reinsert original surface forms.",
            "failure_cases": "Not directly evaluated in this paper; included as related-work mention only.",
            "uuid": "e8774.4",
            "source_info": {
                "paper_title": "ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models",
                "publication_date_yy_mm": "2021-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "i^2 : A plan-and-pretrain approach for knowledge graph-to-text generation",
            "rating": 2,
            "sanitized_title": "i2_a_planandpretrain_approach_for_knowledge_graphtotext_generation"
        },
        {
            "paper_title": "Leveraging large pretrained models for WebNLG 2020",
            "rating": 2,
            "sanitized_title": "leveraging_large_pretrained_models_for_webnlg_2020"
        },
        {
            "paper_title": "Improving text-to-text pretrained models for the graph-to-text task",
            "rating": 2,
            "sanitized_title": "improving_texttotext_pretrained_models_for_the_graphtotext_task"
        },
        {
            "paper_title": "Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
            "rating": 2,
            "sanitized_title": "knowledge_graph_based_synthetic_corpus_generation_for_knowledgeenhanced_language_model_pretraining"
        },
        {
            "paper_title": "Reinforcement learning based graph-to-sequence model for natural question generation",
            "rating": 2,
            "sanitized_title": "reinforcement_learning_based_graphtosequence_model_for_natural_question_generation"
        },
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation",
            "rating": 1,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        }
    ],
    "cost": 0.015621,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models</h1>
<p>Pierre L. Dognin<br>IBM Research<br>pdognin@us.ibm.com<br>Igor Melnyk<br>IBM Research<br>igor.melnyk@ibm.com</p>
<h4>Abstract</h4>
<p>Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning (RL) to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TeKGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks. More details in https://github.com/IBM/regen.</p>
<h2>1 Introduction</h2>
<p>Graph representation of knowledge is a powerful tool to capture real-world information where complex relationships between node entities can be efficiently encoded. Automatic generation of Knowledge Bases (KBs) from free-form text and its counterpart of generating semantically relevant text from KBs are both active and challenging research topics.</p>
<p>Recently, there has been an increased interest in leveraging Pretrained Language Models (PLMs) to improve performance for text generation from graph, or graph-to-text (G2T) task (Ribeiro et al., 2020). Indeed, large PLMs like T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) that have been pretrained on vast amount of diverse and variedly structured data, are particularly good candidates for generating natural looking text from graph data.</p>
<p>Inkit Padhi<br>IBM Research<br>inkpad@ibm.com<br>Payel Das<br>IBM Research<br>daspa@us.ibm.com</p>
<p>BART- and T5-related models have been employed by top performers in public challenges such as the WebNLG+ 2020 Challenge (Castro Ferreira et al., 2020b) where both graph-to-text and text-to-graph (T2G) tasks are offered, under the names RDF-to-Text and Text-to-RDF (semantic parsing) respectively; RDF stands for Resource Description Framework, a standard for describing web resources. One can notice that more teams entered the competition for the G2T task than for T2G as the latter is a much harder task. Best models generally use PLMs and fine-tune them for the target modality at hand (either graph or text). This is possible by re-framing the T2G and G2T generations as a sequence to sequence (Seq2Seq) generation problem, which suits fine-tuning PLMs well. One can therefore hope to leverage the large pretraining of PLMs to improve the overall generation quality.</p>
<p>The Seq2Seq formulation requires any input graph to be linearized as a sequence, which is not unique. This creates an opportunity for data augmentation where multiple linearizations are provided to the model at training time so the model learns the content represented by the graph, not the order of its sequential representation.</p>
<p>In this work, we are interested in leveraging the power of PLMs for both G2T and T2G generation tasks, and will demonstrate the strength of our approach by improving upon the best results of the WebNLG+ 2020 Challenge (rev 3.0) as reported by Castro Ferreira et al. (2020a) for both T2G (Semantic Parsing) and G2T (Data-to-Text) tasks. We will also present results for the TeKGen Corpus (Agarwal et al., 2021) to show performance on a different, much larger dataset. To illustrate the task of generation, Fig. 1 provides examples of G2T and T2G outputs obtained using the proposed generation framework. The first two sentences of the abstract of this paper were used as input for T2G using our best model. The model generates a graph from the input text by simultaneously extracting</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Actual examples of generation for Text-to-Graph and Graph-to-Text tasks using our best RL models. The first two sentences of the abstract were processed through our best models. First, a graph was created capturing the facts from the input sentences. Then, this graph was used as input to generate text. Despite a strong domain mismatch between input data and models, the generated paragraph is capturing most of the original sentences content. Both models were trained using RL, specifically Self-Critical Sequence Training (SCST).
relevant nodes and linking them coherently. For the G2T task, another model starts from the generated graph and generates semantically relevant text from it. As one can appreciate, the final text is quite readable and captures most facts from the original abstract sentences despite a strong domain mismatch between input data and training data, which both models were built on.</p>
<p>Since both T2G and G2T generative tasks can be formulated as a Seq2Seq problem, we propose to use Reinforcement Learning (RL) as part of the PLMs fine-tuning on the target domain data. For both G2T and T2G tasks, a differentiable function such as the cross-entropy (CE) loss function is often used, since minimizing it results in maximizing the probability of generating the correct token/word. However, when it comes to evaluating a model's performance, benchmarks often use BLEU (Pa Pa Aung et al., 2020), METEOR (Lavie and Agarwal, 2007), and chrF++ (Popović, 2017) for G2T, or simply F1, Precision, and Recall scores for T2G, none of which being differentiable. During training, one hopes that by minimizing the CE loss, the model will tend towards better prediction of the target tokens, hence improving on evaluation metrics as a beneficial by-product. Thankfully, RL provides a framework where we can update our model parameters so to improve evaluation metrics directly. Mixed Incremental Cross-Entropy Reinforce from Ranzato et al. (2016) introduced using REINFORCE (Williams, 1992) for sequence training. We propose to use one of its variant known as Self-Critical Sequence Training (SCST) (Rennie et al., 2017) for both T2G and G2T training.</p>
<p>In summary, our main contributions are:</p>
<ul>
<li>We propose to use RL-based sequence training, specifically SCST, for both G2T and T2G tasks. This is the first time that RL based training is proposed to the bi-directional generation of text and
graph. To the best of our knowledge, the present work is the first time it is introduced for a T2G task. - We demonstrate that our approach provides better performance than the best systems reported for the WebNLG 2020+ Challenge.</li>
<li>We provide a thorough investigation of SCSTbased training for both T2G and G2T tasks, including best rewards combination.</li>
<li>We constructed subject and relation-object boundaries from TEXGEN sentence-triples pairs and showed performance of our approach for both T2G and G2T tasks.</li>
<li>We adapted the large-scale TEXGEN corpus (Agarwal et al., 2021) for T2G and G2T tasks and confirmed the benefit of SCST-based fine-tuning approach over CE-trained baselines.</li>
</ul>
<h2>2 Related work</h2>
<p>In the WebNLG+ 2020 Challenge, most top performing models relied on fine-tuning of PLMs. Interestingly, all four top teams in this Challenge proposed quite different approaches while leveraging PLMs. $1^{\text {st }}$ place Amazon AI (Guo et al., 2020a) pipelined a relational graph convolutional network (R-GCN) and a T5 PLM with some canonicalization rules. $2^{\text {nd }}$ place OSU Neural NLG (Li et al., 2020), the closest to our approach in spirit, used T5 and mBART PLMs to fine-tune after special data preprocessing. $3^{\text {rd }}$ place FBConvAI (Yang et al., 2020) used BART PLM and multiple strategies to model input RDFs. $4^{\text {th }}$ place bt5 employed a T5 PLM trained in a bi-lingual approach on English and Russian, even using WMT English/Russian parallel corpus.</p>
<p>Recently, Dognin et al. (2020); Guo et al. (2020b, 2021) proposed models trained to generate in both T2G and G2T directions, with consistency cycles created to enable the use of unsupervised datasets.</p>
<p>In contrast, our approach of fine-tuning a T5 PLM is fully supervised but can produce either the specialized models for T2G and G2T tasks alone, or a hybrid model that can handle both T/G inputs simultaneously to generate the corresponding translated G/T outputs.</p>
<p>Note that in contrast to many WebNLG+ 2020 Challenge participants, e.g. Li et al. (2020), no preprocessing of the data is performed for text, while for graph triples, we add tokens to mark subject, predicate, and object positions in their linearized sequence representation. Moreover, data augmentation is performed by allowing random shuffling of triples order in graph linearization to avoid a model to learn the exact order of triples, especially for the T2G task.</p>
<p>While the use of RL training in PLM has been explored in many works, the approach of Chen et al. (2020) is closest to ours. However, their work focuses on the improved text generation in the context of natural question generation, while in our algorithm we use it for graph-to-text and text-to-graph generations.</p>
<h2>3 Models</h2>
<p>Models are trained on a dataset $\mathcal{D}$ composed of a set of $\left(x_{\mathrm{T}}, x_{\mathrm{G}}\right)^{i}$ samples, where superscript $i$ denotes the $i$-th sample in $\mathcal{D}, x_{\mathrm{T}}$ is made of text (one or more sentences), and $x_{\mathrm{G}}$ is a corresponding graph represented as a list of triples $x_{\mathrm{G}}=\left[\left(s^{1}, p^{1}, o^{1}\right), \ldots,\left(s^{K}, p^{K}, o^{K}\right)\right]$, where the $k$-th triple is composed of a subject $s^{k}$, predicate (relationship) $p^{k}$, and object $o^{k}$. For G2T, the model is given $x_{\mathrm{G}}$ as input and must generate $\hat{x}_{\mathrm{T}}$. A cross-entropy loss is computed as an expectation:</p>
<p>$$
\mathcal{L}<em _mathrm_T="\mathrm{T">{\mathrm{CE}}^{\mathrm{T}}=\underset{x</em>\right)\right]
$$}} \sim \mathcal{D}}{\mathbb{E}}\left[-\log p_{\theta}^{\mathrm{G} 2 \mathrm{~T}}\left(x_{\mathrm{T}</p>
<p>where $p_{\theta}^{\mathrm{G} 2 \mathrm{~T}}\left(x_{\mathrm{T}}\right)$ is the distribution of the generated sequence $\hat{x}<em _mathrm_G="\mathrm{G">{\mathrm{T}}=T</em>} 2 \mathrm{~T}}\left(x_{\mathrm{G}}\right), T_{\mathrm{G} 2 \mathrm{~T}}($.$) being the trans-$ formation from graph to text. Our model is parameterized by $\theta$, and $x_{\mathrm{T}}$ is effectively sampled from the marginal distribution of text samples from $\mathcal{D}$. $\hat{x<em 1="1">{\mathrm{T}}=\left[\hat{w}</em>}, \hat{w<em T="T">{2}, \ldots, \hat{w}</em>\right]$ is a sequence of generated tokens/words. Similarly, for training a T2G model, the cross-entropy loss used in training is simply</p>
<p>$$
\mathcal{L}<em _mathrm_G="\mathrm{G">{\mathrm{CE}}^{\mathrm{G}}=\underset{x</em>\right)\right]
$$}} \sim \mathcal{D}}{\mathbb{E}}\left[-\log p_{\theta}^{\mathrm{T} 2 \mathrm{G}}\left(x_{\mathrm{G}</p>
<p>where $p_{\theta}^{\mathrm{T} 2 \mathrm{G}}\left(x_{\mathrm{G}}\right)$ is the distribution of the generated graph $\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{G}}=T</em>$ is drawn
from the marginal distribution of graph samples from $\mathcal{D}$.} 2 \mathrm{G}}\left(x_{\mathrm{T}}\right), T_{\mathrm{T} 2 \mathrm{G}}($.$) being the transfor-$ mation from text to graph, and where $x_{\mathrm{G}</p>
<p>In both Eq. (1) and Eq. (2), $x_{\mathrm{G}}$ must be expressed as a sequence of tokens $t_{j}$ such that a list of triples $x_{\mathrm{G}}$ turns into a list of tokens $\left[t_{1}, t_{2}, \cdots, t_{M}\right]$. This is simply done by adding tokens marking the subject, predicate, and object boundaries in the sequence such that each triple $\left(s^{k}, p^{k}, o^{k}\right)$ is turned into a sequence such as $\left[&lt;\mathrm{S}&gt;, w_{1}^{s},&lt;\mathrm{P}&gt;, w_{1}^{\mathrm{p}}, w_{2}^{\mathrm{p}},&lt;\mathrm{O}&gt;, w_{1}^{o}, w_{2}^{o}, w_{3}^{o}\right]$, assuming our subject is made of 1 token, our predicate of 2 tokens, and our object of 3 tokens in this example. $&lt;\mathrm{S}&gt;,&lt;\mathrm{P}&gt;$, and $&lt;\mathrm{O}&gt;$ are just special marker tokens to help the model know where subject, predicate and objects are located in the sequence.</p>
<p>We start from a pretrained encoder-decoder $\mathcal{M}$ model that we fine-tune on either T2G to get $\mathcal{M}<em _mathrm_G="\mathrm{G">{\mathrm{T}}$, or G2T task to get $\mathcal{M}</em>}}$. We also propose a third kind of model $\mathcal{M<em _mathrm_T="\mathrm{T">{\mathrm{T}+\mathrm{G}}$ to be fine-tuned on both T2G and G2T samples, i.e. the model will learn to generate in any direction, by supplying an input sample $x=\left[x</em>}} ; x_{\mathrm{G}}\right]^{\top}$ and corresponding target for it. Input from each modality is prefixed by a task specific string to distinguish transfer directions ("Text to Graph:" for $x_{\mathrm{T}}$ and "Graph to Text:" for $x_{\mathrm{G}}$ ). For $\mathcal{M<em _mathrm_CE="\mathrm{CE">{\mathrm{T}+\mathrm{G}}$ models, the cross-entropy loss is similarly defined as for Eq. (1) and Eq. (2) such that $\mathcal{L}</em>(x)\right]$. All models are shown in Fig. 2. By convention, we refer to models in this paper by their input modality T, G, or T+G.}}^{\mathrm{T}+\mathrm{G}}=\underset{x \sim \mathcal{D}}{\mathbb{E}}\left[-\log p_{\theta</p>
<h3>3.1 Reinforcement Learning</h3>
<p>Sequence generation can be seen as an agent making sequential decisions of picking words from a given vocabulary. The agent reacts to its environment by accounting for past predictions and getting rewarded along the way, while its state is defined by the partial sequence generated so far. This interpretation enables the reformulation of Seq2Seq generation within the Reinforcement Learning (RL) framework (Sutton and Barto, 2018; Silver, 2015). More precisely, a sequence generation task can be recast as a Markov Decision Process (MDP) where the agent behavior follows a policy $\pi\left(a_{t} \mid s_{t}\right)$. Action $a_{t}$ corresponds to picking a particular word $w_{t}$ at time $t$ from a vocabulary $\mathcal{V}$, conditioned on state $s_{t}$ expressed as the partial sequence generation $s_{t}=\hat{x}<em 1="1">{1: t}=\left[\hat{w}</em>}, \ldots, \hat{w<em t="t">{t}\right]$, that is sequence of words/tokens already picked. $\pi\left(a</em>$ is taken,} \mid s_{t}\right)$ is a stochastic policy that defines a probability distribution of $a_{t}$. Once the action $a_{t</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Specialized and hybrid models rely on the same losses for fine-tuning. However, specialized models are dedicated to a particular generation task while hybrid models can handle both generation directions.
the agent receives a reward $r_{t}=r\left(s_{t}, a_{t}\right)$ before it transitions to the next state $s_{t+1}$. A sequence of actions $a_{1: T}=\left[a_{1}, \ldots, a_{T}\right]$ is selected until the end of generation is reached. The agent aims at maximizing the expectation of cumulative reward</p>
<p>$$
J(\pi)=\mathbb{E}<em t="1">{\tau}\left[\sum</em>[R(\tau)]
$$}^{T} \gamma^{t} r_{t}\right]=\mathbb{E}_{\tau</p>
<p>where $\gamma$ is a discounting factor used to control the horizon of the cumulative reward, $\gamma \in[0,1]$. The expectation is taken over trajectories $\tau$, sequences made of $\left{s_{1}, a_{1}, r_{1}, \ldots, s_{T}, a_{T}, r_{T}\right}$, where $a_{t}$ was chosen from policy $\pi\left(a_{t} \mid s_{t}\right)$. RL provides both on-policy and off-policy approaches to maximize $J(\pi)$ in Eq. (3). We are particularly interested in on-policy techniques that rely on data samples generated from the model to train, especially since our models start from large fine-tuned PLMs that can already generate good samples. This helps avoid the common drawback of on-policy techniques of generating poor samples at first when trained from scratch. These policy-based (Williams, 1992; Zaremba and Sutskever, 2016) and actor-critic based techniques (Bahdanau et al., 2017; Rennie et al., 2017) have been studied for text generation and often update the underlying model with policy gradient (Ranzato et al., 2016; Li et al., 2016; Tan et al., 2019; Paulus et al., 2017). Policy-based methods focus on a parameterized policy $\pi_{\theta}$ where $\theta$ is optimized to maximize $J\left(\pi_{\theta}\right)$. The policy $\pi_{\theta}\left(a_{t} \mid s_{t}\right)$ is the PLM generative model $p_{\theta}$, CE fine-tuned as described at the beginning of Section 3.</p>
<p>REINFORCE, presented by Williams (1992), allows the optimization of a model's parameters $\theta$ by maximizing the expected value of the wordbased reward $R_{w}\left(\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}\right)$ of generated sequence $\hat{x}</em>}}=$ $\left[\hat{w<em T="T">{1}, \ldots, \hat{w}</em>}\right]$. For notation convenience, note that $R_{w}\left(\hat{x<em _mathrm_T="\mathrm{T">{\mathrm{T}}\right)=R(\tau)$ since we are now dealing with sequence of words/tokens $\hat{x}</em>\right)$
notation for simplicity. In order to match common Deep Learning conventions, we can minimize a loss expressed as the negative value of the expected cumulative reward:}}$ selected by the actions in trajectory $\tau$. We will also use the $R\left(\hat{x}_{\mathrm{T}</p>
<p>$$
\begin{aligned}
\mathcal{L}<em _left_hat_w="\left[\hat{w">{\mathrm{RL}} &amp; =-\sum</em><em T="T">{1}, \ldots, \hat{w}</em>}\right]} p_{\theta}\left(\hat{w<em T="T">{1}, \ldots, \hat{w}</em>}\right) R_{w}\left(\hat{w<em T="T">{1}, \ldots, \hat{w}</em>\right) \
&amp; =-\mathbb{E}<em 1="1">{\left[\hat{w}</em>}, \ldots, \hat{w<em _theta="\theta">{T}\right] \sim p</em>}} R_{w}\left(\hat{w<em T="T">{1}, \ldots, \hat{w}</em>\right) \
&amp; =-\mathbb{E}<em _mathrm_T="\mathrm{T">{\hat{x}</em>\right)
\end{aligned}
$$}} \sim p_{\theta}} R_{w}\left(\hat{x}_{\mathrm{T}</p>
<p>$R_{w}\left(\hat{x}_{\mathrm{T}}\right)$ is the reward for the generated text which is often associated with non-differentiable metrics such as BLEU, METEOR, chrF, etc. Note that in sequence generation, these metrics-based rewards are available only once a whole sequence is generated, trading sparsity/delay of reward for quality (i.e. we use the full sequence reward, not an estimation of partial future reward). We circumvent the non-differentiability issue by using the REINFORCE policy gradient method:</p>
<p>$$
\nabla_{\theta} \mathcal{L}<em _mathrm_T="\mathrm{T">{\mathrm{RL}} \propto-\left(R\left(\hat{x}</em>\right)
$$}}\right)-b\right) \nabla_{\theta} \log p_{\theta}\left(\hat{x}_{\mathrm{T}</p>
<p>where $b$ is a baseline used to reduce the variance of our gradient estimate. $b$ can be any function, even a random variable, as long as it is independent of the actions taken to generate $\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}$, as described in Chapter 13.4 from Sutton and Barto (2018). In SelfCritical Sequence Training (SCST) (Rennie et al., 2017), $b$ is chosen to be the reward of $x</em>$, the output generated by the model by greedy max generation, hence the model serving as its own critic:}}^{*</p>
<p>$$
\nabla_{\theta} \mathcal{L}<em _mathrm_T="\mathrm{T">{\mathrm{SCST}} \propto-\left(R\left(\hat{x}</em>\right)
$$}}\right)-R\left(x_{\mathrm{T}}^{*}\right)\right) \nabla_{\theta} \log p_{\theta}\left(\hat{x}_{\mathrm{T}</p>
<p>where $\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}$ is sampled from our model and $x</em>^{}<em>}$ is generated by greedy max. An interesting property of the baseline is that if $R\left(\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}\right)&gt;R\left(x</em>^{}</em>}\right)$, sampled $\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}$ has higher reward than $x</em>^{}<em>}$, then the model is updated to reinforce the choices made by this generation. In the opposite case where $R\left(\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}\right)&lt;R\left(x</em>^{}</em>}\right)$,</p>
<p>the model update will take the negative gradient to subdue such generation. When $R\left(\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}\right)=R\left(x</em>^{}<em>}\right)$, no update is performed on the model since the gradient is effectively zeroed out, regardless of the individual values $R\left(\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}\right)$ and $R\left(x</em>^{}</em>}\right)$. This happens when $\hat{x}<em _mathrm_T="\mathrm{T">{\mathrm{T}}$ and $x</em>\right)$ compares to baseline $b$, the role of $b$ being to reduce the variance of the gradient estimate. Variations around REINFORCE exist on how to apply the gradients, such as MIXER from Ranzato et al. (2016), or on how to evaluate the baseline (Luo, 2020) to minimize the gradient variance.}}^{*}$ are identical (greedy-max and sampled sequences are the same). In that case the sample is lost for RL as no update to the model will result from this sample. Basically, REINFORCE is a Monte Carlo method of learning where a gradient update is applied in the direction decided by how $R\left(\hat{x}_{\mathrm{T}</p>
<p>In our training, PLMs are first fine-tuned using $\mathcal{L}<em _SCST="{SCST" _text="\text">{\text {CE }}$ loss. Once they reach a good generation quality, the training is switched to RL fine-tuning by minimizing $\mathcal{L}</em>$.}</p>
<h2>4 Experimental Setup</h2>
<p>In this Section, we present the experimental setup used for all the results reported in this paper.
Models We used T5 PLMs from Wolf et al. (2020) for our experiments for two distinct models, $t 5$ large ( 770 M parameters) and $t 5$-base ( 220 M parameters), with a special focus on t5-large as it is the best performing of the two on various NLP tasks. Models were fine-tuned to be either specialized on T2G $\left(\mathcal{M}<em _mathrm_G="\mathrm{G">{\mathrm{T}}\right)$ or G2T $\left(\mathcal{M}</em>\right)$.
Data processing Graphs are often represented as list of triples. However our model expects a sequence of input words/tokens to work on. The linearization of graph triples is obviously ambiguous as there are many ways to traverse a graph (Breadth First Search, Depth First Search, random walk, etc.). In practice, we linearize the triples in the order of the list provided by the dataset, but use this inherent linearization ambiguity as an opportunity to do data-augmentation. Indeed, models are first fine-tuned using cross-entropy loss that strongly penalizes generation if it is in any different order than the ground truth order. To avoid the model to overfit to our data and memorize observed triples order, we augment the data by including a few permutations of the graph triples.}}\right)$ task, or to accommodate both directions of generation $\left(\mathcal{M}_{\mathrm{T}+\mathrm{G}</p>
<p>During graph linearization, we encode the subject, predicate, and object positions by using
$&lt;\mathrm{S}\rangle,&lt;\mathrm{P}\rangle,&lt;\mathrm{O}\rangle$ tokens. In practice, we expand the model vocabulary with these special indivisible tokens that are not split during tokenization. No other preprocessing is done on the data for training. We explored masked and span-masked LM fine-tuning to match T5 pretraining (Raffel et al., 2020) which did not lead to any noticeable improvements.</p>
<h3>4.1 Datasets</h3>
<p>WebNLG+ 2020 We report results on WebNLG+ 2020 (v3.0) used in the WebNLG 2020 Challenge (Castro Ferreira et al., 2020b). The Challenge comprises of two tasks: RDF-to-text generation (G2T), and Text-to-RDF semantic parsing (T2G). The Resource Description Framework (RDF) language is used to encode DBpedia and is commonly used in linked data framework. WebNLG+ uses RDF to encode graphs as sets of triples which are associated to one or more lexicalizations of one or more sentences each. Data for English and Russian are provided, but we only worked on the English subset made of 13,211 train, 1,667 dev, 2,155 testA (semantic parsing), and 1,779 testB (data-to-text) samples (triples sets w/ lexicalizations). The data is clustered semantically into 16 categories seen in train and dev sets (Airport, Astronaut, Building, etc.), while 3 categories (Film, Scientist, and Musical-Work) were introduced in test and are unseen, i.e. not present in training; see Castro Ferreira et al. (2020a) for more details. Results are aggregated for all, seen, and unseen categories during evaluation. Note that in the literature, prior works sometimes report 'WebNLG' results on previous dataset version, with completely different performance ranges. We compare all our results to WebNLG+ 2020 (v3.0) numbers reported by Castro Ferreira et al. (2020a) in their Table 6 for G2T, and Table 10 for T2G tasks, using the provided official scoring scripts.
TEKGEN To further study the robustness of our system, we also provide experiments using TEKGEN dataset recently introduced in Agarwal et al. (2021). The graph-sentence alignments are curated using Wikipedia and Wikidata. This serves as a perfect large scale test-bed for both G2T and T2G tasks. Unfortunately, this dataset lacks in entity/relation/object boundaries, which makes it difficult to evaluate systems for T2G tasks. In order to address this issue, we further process the triple-text (with no triple boundaries) to create list of triples using Wikidata properties lookup, via Wikidata</p>
<table>
<thead>
<tr>
<th style="text-align: left;">WebNLG G2T</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Team/model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">NLTK</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Amazon AI (Shanghai) (Guo et al., 2020a)</td>
<td style="text-align: center;">0.540</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.417</td>
<td style="text-align: center;">0.690</td>
</tr>
<tr>
<td style="text-align: left;">OSU Neural NLG (Li et al., 2020)</td>
<td style="text-align: center;">0.535</td>
<td style="text-align: center;">0.532</td>
<td style="text-align: center;">0.414</td>
<td style="text-align: center;">0.688</td>
</tr>
<tr>
<td style="text-align: left;">FBConvAI (Yang et al., 2020)</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.686</td>
</tr>
<tr>
<td style="text-align: left;">bt5 (Agarwal et al., 2020)</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.517</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">0.679</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) G2T.CE t5-large</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.694</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) G2T.RL t5-large</td>
<td style="text-align: center;">$\mathbf{0 . 5 6 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 6}$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) G2T.CE.ES t5-base (early CE)</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.518</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.675</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) G2T.RL.ES t5-base (early CE)</td>
<td style="text-align: center;">0.531</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.410</td>
<td style="text-align: center;">0.686</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) G2T.CE.best t5-base (best CE)</td>
<td style="text-align: center;">0.524</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.677</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) G2T.RL.best t5-base (best CE)</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.408</td>
<td style="text-align: center;">0.681</td>
</tr>
</tbody>
</table>
<p>Table 1: G2T Best results on WebNLG 2020 Challenge (v3.0) dataset. The first four rows were the top performers of the Challenge. Results for CE and RL models are presented for our ReGen systems so to show gains from using SCST. Our G2T.RL is the best system overall, fine-tuning a t5-large model using METEOR reward. G2T.RL.ES and G2T.RL.best show the impact of using early stopping (ES) or best CE selection for starting SCST fine-tuning on a t5-base smaller model while using BLEU_NLTK reward.</p>
<p>Query Service. Additionally, we limit the validation set and test set to 5 K and 50 K sentence-triples pairs respectively. Our training split after processing contains 6.3 million sentence-triples pairs. As a contribution to the work, we will present the steps to augment TEKGen dataset with appropriate subject, object and relation boundaries, which enables conventional evaluation of research systems. An example of the processed TEKGen is shown in Fig. 3 in Appendix.
Metrics WebNLG+ 2020 provides automatic metrics to evaluate models. For G2T, we used BLEU, BLEU_NLTK, METEOR, and chrF++ that are provided by the challenge. For T2G, F1, Precision, and Recall scores are utilized and computed for 4 levels of match: Exact, Ent_Type, Partial and Strict as described in Castro Ferreira et al. (2020a), which loosely correspond to different levels of relaxation of how close a match of an entity must be to the ground truth in content and position in a triple. Note that when generating graphs/RDFs, scoring metrics explore all possible permutations of a graph edges. For TEKGen, we use the same metrics as for WebNLG+ 2020.</p>
<h2>5 Results</h2>
<p>For all experiments, PLMs were first exposed to the target datasets (WebNLG+, TEKGEN) by finetuning using $\mathcal{L}<em _SCST="{SCST" _text="\text">{\text {CE }}$ loss. They were then switched to RL training by optimizing the $\mathcal{L}</em>$ loss. Although no exact recipe has been established for}</p>
<p>Seq2Seq RL-training, starting from a good CE model helps RL training performance in practice (Ranzato et al., 2016; Rennie et al., 2017). Therefore, we followed the subsequent simple approach: During fine-tuning, the evaluations are conducted on the validation set. From the CE phase, the best performing model iteration is selected based on the METEOR and F1 score for the G2T and T2G tasks, respectively, to pursue RL fine-tuning. In case of G2T, potential ties in METEOR scores among candidate models, are resolved by using BLEU_NLTK, followed by the chrF++ metric. Note that early stopping selection of CE models led to good performance for t5-base models as well. During the SCST phase, the best model iteration on the validation set is selected and its performance numbers on the test set are reported in our tables.
WebNLG+ 2020 G2T For the WebNLG+ 2020 Challenge, the results of the top four systems for RDF-to-text task can be found in Tab. 1 for all categories (results for seen and unseen categories are given in Tab. 5 in the Appendix), while descriptions the top teams' systems were given in Section 2. We report our G2T results for both t5-large and t5base models as well. For t5-large, ReGen G2T.CE is the best model from CE fine-tuning. ReGen G2T.RL is best model performance for SCST training while using METEOR as reward when starting from G2T.CE model. Tab. 1 shows that our CE model is better than models from all top teams, and the SCST results further improve significantly in</p>
<table>
<thead>
<tr>
<th style="text-align: left;">WebNLG T2G <br> Team/model</th>
<th style="text-align: left;">Match</th>
<th style="text-align: left;">F1 $\uparrow$</th>
<th style="text-align: left;">Precision $\uparrow$</th>
<th style="text-align: left;">Recall $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Amazon AI (Shanghai) (Guo et al., 2020a)</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: left;">0.689</td>
<td style="text-align: left;">0.689</td>
<td style="text-align: left;">0.690</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: left;">0.700</td>
<td style="text-align: left;">0.699</td>
<td style="text-align: left;">0.701</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: left;">0.696</td>
<td style="text-align: left;">0.696</td>
<td style="text-align: left;">0.698</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: left;">0.686</td>
<td style="text-align: left;">0.686</td>
<td style="text-align: left;">0.687</td>
</tr>
<tr>
<td style="text-align: left;">bt5 (Agarwal et al., 2020)</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: left;">0.682</td>
<td style="text-align: left;">0.670</td>
<td style="text-align: left;">0.701</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: left;">0.737</td>
<td style="text-align: left;">0.721</td>
<td style="text-align: left;">0.762</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: left;">0.713</td>
<td style="text-align: left;">0.700</td>
<td style="text-align: left;">0.736</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: left;">0.675</td>
<td style="text-align: left;">0.663</td>
<td style="text-align: left;">0.695</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) T2G.CE</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: left;">$\mathbf{0 . 7 2 3}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 1 4}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 3 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: left;">$\mathbf{0 . 8 0 7}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 9 1}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 3 5}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: left;">$\mathbf{0 . 7 6 7}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 5 5}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 8 8}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: left;">$\mathbf{0 . 7 2 0}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 1 3}$</td>
<td style="text-align: left;">$\mathbf{0 . 7 3 5}$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen (Ours) T2G.RL</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: left;">0.720</td>
<td style="text-align: left;">0.712</td>
<td style="text-align: left;">0.734</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: left;">0.804</td>
<td style="text-align: left;">0.789</td>
<td style="text-align: left;">0.829</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: left;">0.764</td>
<td style="text-align: left;">0.752</td>
<td style="text-align: left;">0.784</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: left;">0.717</td>
<td style="text-align: left;">0.709</td>
<td style="text-align: left;">0.731</td>
</tr>
</tbody>
</table>
<p>Table 2: T2G Best results on WebNLG+ 2020 (v3.0) dataset. The top two teams were the first and second place winner of the Challeneg. Our T2G.CE model improves upon all metrics for all matching schemes, providing a new state-of-the-art results for this Challenge task. T2G.RL models, while still better than previous best results, does not improve upon its CE counterpart.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">TEKGEN G2T <br> Model</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$ <br> NLTK</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ReGen-CE</td>
<td style="text-align: center;">Val</td>
<td style="text-align: center;">0.240</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.400</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.242</td>
<td style="text-align: center;">0.405</td>
</tr>
<tr>
<td style="text-align: left;">ReGen-SCST</td>
<td style="text-align: center;">Val</td>
<td style="text-align: center;">0.258</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.418</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">$\mathbf{0 . 2 6 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 2 6 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 2}$</td>
</tr>
</tbody>
</table>
<p>Table 3: G2T Results for TeKGen dataset. ReGen-CE establishes a baseline on this dataset. ReGen-SCST consistently improve on the baseline on all metrics, for validation and test sets.
all metrics achieving state-of-the-art results to our knowledge. The gain obtained by SCST alone is quite significant and demonstrates the benefits of RL fine-tuning for this task. We report our best model results in Tab. 1, as well as mean and standard deviation results for multiple random number generator seeds in Tab. 10 in Appendix. When averaging results for few seeded models, sustained gains from SCST are seen for all metrics.</p>
<p>Multiple reward candidates were investigated (BLEU, BLEU_NLTK, METEOR, chrF) as well as some linear combinations of pairs of them, as can be seen in Tab. 7 in Appendix. In Tab. 7, for t5-large, METEOR is consistently the best SCST reward, and improves all the other metrics scores as well. However, for 'smaller' models such as
t5-base, BLEU_NLTK is revealed to be the best reward for improving BLEU performance as expected. Again, SCST brings significant gains across all the metrics in that case. Note that for t5-base model, selecting a METEOR reward improves METEOR results significantly as reported in Tab. 9 in Appendix.</p>
<p>Another interesting fact is that early stopping of CE model G2T.CE.ES (at 5 epochs) leads to the best SCST model G2T.RL.ES for t5-base, while selecting the best CE model G2T.CE.best (at 11 epochs) still showed some gains from SCST model G2T.RL.best. SCST needs a good starting point, but a better CE model that has seen a lot more epochs of our dataset maybe harder for SCST to stir in a better solution in the parameter space.</p>
<p>Moreover, the test split contains unseen categories not present in the validation dataset which render choices based on validation sub-optimal for the test dataset. The best models we report in this work are specialized models $\mathcal{M}_{\mathrm{G}}$. Early in our investigation, hybrid models were the best performing model for G2T reaching 0.547 BLEU, 0.543 BLEU_NLTK and 0.417 METEOR, and first to beat the Challenge winning team. However, when batch size became larger (20-24 samples), the specialized models took the lead and retain it still.</p>
<p>For training, we optimized all our models using AdamW (Loshchilov and Hutter, 2017), variant of the Adam optimizer with default values of $\beta=[0.9,0.999]$ and weight decay of $10^{-2}$. For learning rate, we used $5.10^{-6}$ for all our experiments as it was better than $10^{-5}$ and $10^{-6}$ as seen in Tab. 8 in Appendix. All our models were trained with 20-24 minibatch size on WebNLG. Further details on our experimental setup are provided in the Appendix in Section A.
WebNLG+ 2020 T2G Results for the Text-to-RDF task are reported in Tab. 2 for all categories. Results for our best model on seen and unseen categories are given in Tab. 6 in Appendix. Amazon AI and bt5 are the top performing teams. Again, the proposed ReGen T2G.CE model shows strong results that are better in term of all metrics, for all matching categories. In themselves, these numbers are a de-facto new state-of-the-art for this dataset, as far as we know. SCST model T2G.RL fails to improve on this model though. The exact F1 metric was used as reward, but the model could never pull ahead of the CE model in our experiments. The exact F1 metric may not be a strong enough reward to really capture the dynamics of graph generation properly for WebNLG+ as it is very rigid in its measure (one must have an exact match), although the same reward gave good results on our second dataset TEXGEN. A more sensitive metric could possibly help. We even tried to use n-gram based metrics (like BLEU) but to no avail. We further address this issue at the end on this Section.
TEKGen G2T For the TEXGEN dataset, we present our results on Graph-to-Text generation in Tab. 3. Similar to the experiments in WebNLG+, we pick the best model during the CE fine-tuning based on the METEOR score and proceed with the RL fine-tuning. We observe that the RL fine-tuning step helps boost the test split scores on all metrics. It is worth noting that the scores are slightly under-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">T2G <br> Model</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">$\mathrm{F} 1 \uparrow$</th>
<th style="text-align: left;">$\mathrm{P} \uparrow$</th>
<th style="text-align: left;">$\mathrm{R} \uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Val</td>
<td style="text-align: left;">0.622</td>
<td style="text-align: left;">0.608</td>
<td style="text-align: left;">0.647</td>
</tr>
<tr>
<td style="text-align: left;">ReGen-CE</td>
<td style="text-align: left;">Test</td>
<td style="text-align: left;">0.619</td>
<td style="text-align: left;">0.605</td>
<td style="text-align: left;">0.643</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Val</td>
<td style="text-align: left;">0.615</td>
<td style="text-align: left;">0.600</td>
<td style="text-align: left;">0.640</td>
</tr>
<tr>
<td style="text-align: left;">ReGen-SCST</td>
<td style="text-align: left;">Test</td>
<td style="text-align: left;">$\mathbf{0 . 6 2 3}$</td>
<td style="text-align: left;">$\mathbf{0 . 6 1 0}$</td>
<td style="text-align: left;">$\mathbf{0 . 6 4 7}$</td>
</tr>
</tbody>
</table>
<p>Table 4: T2G TEXGEN Results: ReGen-CE establishes a baseline of the dataset. ReGen-SCST improves results on the test set compared to ReGen-CE.
estimating the potential of our system because of the nature of the sentences in the TEXGEN dataset. Unlike WebNLG+, in a paired text-graph sample in TEXGEN, the linearized graph does not usually cover all the concepts described in the corresponding text. This leads to underestimating when the hypothesis is scored against the reference using n-gram metrics.
TEKGen T2G Results for the Text-to-Graph for TEXGEN are reported in Tab. 4. Once the CE finetuning is done, we continue with the RL fine-tuning using exact F1 as reward. The performance is consistent with what we observe in G2T task for TEXGEN, where SCST step boosts the performance of the model. Since, we reformulate this dataset (refer Section 4.1) to offer as T2G and G2T tasks, our approach is the first attempt in understanding the nature of TEXGEN dataset and our methods provide a baseline for future research. Please note that for both T2G and G2T tasks in TEXGEN, we only start a t5-large PLM.
Summary Results on WebNLG+ 2020 and TEXGEN demonstrated that RL fine-tuning of models leads to significant improvements of results for T2G and G2T, establishing new state-of-the-art results for both tasks. For WebNLG+, T2G was a challenging task for RL fine-tuning. In further work, we plan to address this issue by investigating two points: First, look into a more sensible graphdependent sampling for graph structures, rather than the current multinomial sampling of the best tokens at each generation step. Second, try a different reward schemes where the reward is more attuned to the challenges of graph generation as well as graph structure, allowing for some curriculum learning, or increasing the harshness of rewards gradually during training. Results on TEXGEN showed that RL fine-tuning is a viable option even on large-scale datasets. To enrich this quantitative</p>
<p>study of ReGen, we provide a few qualitative cherry picked results in Tab. 11 and Tab. 12 in Appendix.</p>
<h2>6 Conclusions</h2>
<p>In this paper, we proposed to use RL for improving upon current generation for text-to-graph and graph-to-text tasks for the WebNLG+ 2020 Challenge dataset using pre-trained LMs. We not only defined a novel Seq2Seq training of models in T2G and G2T generation tasks, but we established state-of-the-art results for WebNLG+ for both tasks, significantly improving on the previously published results. We provided extensive analyses of our results and of the steps taken to reach these improvements. We then expanded our approach to large scale training by means of TEXGEN where we demonstrated that RL fine-tuning provides a robust way to improve upon regular model finetuning within a dataset that is orders of magnitude larger than the WebNLG+ starting point. We established gains despite a weaker content overlap in text-graph data pairs for TEXGEN. Along the way, we constructed subject, and relation-object boundaries from TEXGEN sentence-triples pairs that we plan on releasing to benefit the research community.</p>
<p>Future work will focus on developing a variant of SCST that leverages the unique structure of graph by either performing of more sensible graphdependent sampling, or by investigating different reward schemes more attuned to integrating the content and structure of graphs.</p>
<h2>7 Broader Impact Statement</h2>
<p>The techniques proposed in this paper are inherently dependent on the training data and the PLMs used for fine-tuning on this data. The models do benefit from the large amount of data seen by the PLM they are derived from, however it is fair to assume that any detectable bias in the original data or PLMs would most likely be transferred to the text-to-graph and graph-to-text generative models. This is something to keep in mind when building these generative models. Public datasets were used for all experiments. The TEXGEN with recreated boundaries does not change the underlying data and should not add any further noise nor bias to the original data.</p>
<h2>References</h2>
<p>Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. 2021. Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3554-3565, Online. Association for Computational Linguistics.</p>
<p>Oshin Agarwal, Mihir Kale, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. 2020. Machine translation aided bilingual data-to-text generation and semantic parsing. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 125-130, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C. Courville, and Yoshua Bengio. 2017. An actor-critic algorithm for sequence prediction. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.</p>
<p>Thiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris van der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. 2020a. The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+ 2020). In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 55-76, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Thiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris van der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina, editors. 2020b. Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+). Association for Computational Linguistics, Dublin, Ireland (Virtual).</p>
<p>Yu Chen, Lingfei Wu, and Mohammed J. Zaki. 2020. Reinforcement learning based graph-to-sequence model for natural question generation. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Pierre Dognin, Igor Melnyk, Inkit Padhi, Cicero Nogueira dos Santos, and Payel Das. 2020. DualTKB: A Dual Learning Bridge between Text and Knowledge Base. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8605-8616, Online. Association for Computational Linguistics.</p>
<p>Qipeng Guo, Zhijing Jin, Ning Dai, Xipeng Qiu, Xiangyang Xue, David Wipf, and Zheng Zhang. 2020a. $i^{2}$ : A plan-and-pretrain approach for knowledge</p>
<p>graph-to-text generation. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 100-106, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, and Zheng Zhang. 2020b. CycleGT: Unsupervised graph-to-text and text-to-graph generation via cycle training. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 7788, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Qipeng Guo, Zhijing Jin, Ziyu Wang, Xipeng Qiu, Weinan Zhang, Jun Zhu, Zheng Zhang, and David Wipf. 2021. Fork or fail: Cycle-consistent training with many-to-one mappings. In The 24th International Conference on Artificial Intelligence and Statistics, AISTATS 2021, April 13-15, 2021, Virtual Event, volume 130 of Proceedings of Machine Learning Research, pages 1828-1836. PMLR.</p>
<p>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228-231, Prague, Czech Republic. Association for Computational Linguistics.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. 2016. Deep reinforcement learning for dialogue generation.</p>
<p>Xintong Li, Aleksandre Maskharashvili, Symon Jory Stevens-Guille, and Michael White. 2020. Leveraging large pretrained models for WebNLG 2020. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 117-124, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Ilya Loshchilov and Frank Hutter. 2017. Fixing weight decay regularization in adam. CoRR, abs/1711.05101.</p>
<p>Ruotian Luo. 2020. A better variant of self-critical sequence training.</p>
<p>San Pa Pa Aung, Win Pa Pa, and Tin Lay Nwe. 2020. Automatic Myanmar image captioning using CNN and LSTM-based language model. In Proceedings of the 1st Joint Workshop on Spoken</p>
<p>Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL), pages 139-143, Marseille, France. European Language Resources association.</p>
<p>Romain Paulus, Caiming Xiong, and Richard Socher. 2017. A deep reinforced model for abstractive summarization.</p>
<p>Maja Popović. 2017. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, pages 612-618, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.</p>
<p>Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level training with recurrent neural networks. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Steven J. Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. Self-critical sequence training for image captioning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1179-1195. IEEE Computer Society.</p>
<p>Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, and Iryna Gurevych. 2020. Investigating pretrained language models for graph-to-text generation.</p>
<p>David Silver. 2015. Lectures on reinforcement learning. URL: https://www.davidsilver.uk/ teaching/.</p>
<p>Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. A Bradford Book, Cambridge, MA, USA.</p>
<p>Bowen Tan, Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, and Eric Xing. 2019. Connecting the dots between mle and rl for sequence prediction.</p>
<p>Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning, 8(3-4):229-256.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame,</p>
<p>Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Zixiaofan Yang, Arash Einolghozati, Hakan Inan, Keith Diedrick, Angela Fan, Pinar Donmez, and Sonal Gupta. 2020. Improving text-to-text pretrained models for the graph-to-text task. In Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 107-116, Dublin, Ireland (Virtual). Association for Computational Linguistics.</p>
<p>Wojciech Zaremba and Ilya Sutskever. 2016. Reinforcement learning neural turing machines - revised.</p>
<h2>A Training Setup</h2>
<p>All our experiments were run using NVIDIA V100 GPUs for training and validation, some trainings were done on A100. We distributed our training to 2-4 GPUs depending on availability. Each training epoch for CE ranged from 30 minutes to 1 hour depending on number of GPUs utilized.</p>
<p>Validation and testing (1,779 and 2,155 samples for testA and testB of WebNLG+ 2020) lasted from 40 minutes to 1 hour depending on machines. Computation was dominated by beam search generation as we used beam search with beam size of 5 and a max sequence length of 192 (since linearized graph sequence can be quite long). We used the official scoring scripts released by WebNLG+ 2020 Challenge to score all our experiments. The evaluation of graph being the most computationally expensive as all possible matching combinations are tested in what looks like a factorial complexity, taking scoring of set of triples larger than 8 from impractical to not feasible.</p>
<p>All our models were built using PyTorch. Total effective batch sizes were set to either 20 or 24 samples for our distributed training. We adjusted the batch size on each worker to ensure consistent global batch size of 20 or 24.</p>
<p>We did some search on learning rates for t5large training and SCST rewards, see discussion and results in Section C.</p>
<p>All our trainings have a seeded random number generator for reproducibility. We also report results on WebNLG+ 2020 G2T tasks for each training setup by showing results for 3 models from different seeds, and provide means and standard deviations of these results in Tab. 10.</p>
<h2>B WebNLG+ 2020 Results per Categories for Best G2T and T2G Models</h2>
<p>In Tab. 5, we are reporting results for all WebNLG+ 2020 categories for our best CE and RL models. While results for unseen categories are much worse than for seen categories, RL fine-tuning manages to improve on both seen and unseen categories.</p>
<p>Tab. 6 provides results for seen, unseen and all categories for our best CE model ReGen T2G.CE which established state-of-the-art results on T2G task of WebNLG+ 2020 Challenge dataset.</p>
<h2>C Ablation Studies</h2>
<p>In Tables 7 and 8 we present ablation studies of different optimized metrics and learning rates for SCST training. As can be seen from Table 7, when METEOR is used as a reward, we get the best performance across all the metrics. We also tried using a combination of multiple rewards with different scaling but did not get any gain over the single metric rewards. In Table 8. we also show the effect of learning rate on SCST performance. Using $l r=5 \cdot 10^{-6}$ gave us the best performance, while higher rates, such as $10^{-4}$, led to unstable training and collapse of SCST.</p>
<h2>D G2T Results t5-base models for SCST with METEOR Reward</h2>
<p>Results for SCST fine-tuning of t5-base models using a METEOR reward are compiled in Tab. 9. Clearly, these models achieve better METEOR results as expected since they are RL optimized on this metric.</p>
<h2>E G2T Results for Models from Multiple Random Seeds</h2>
<p>All our training have a seeded random number generator for reproducibility. We also report the mean and standard deviations for all our G2T models. Each model setup was run 3 times using three independent and distinct seeds, following the same exact process. This is to ensure that our results are not just the product of a lucky system configuration or otherwise advantageous random shuffling of our training dataset. All results are reported in Tab. 10.</p>
<p>The gain reported between CE and RL for our t5large models are clearly still showing after average of all 3 models from distinct random seeds. For t5-base, gains between CE and RL are still present, albeit smaller than for our best systems.</p>
<h2>F Generation Examples for G2T and T2G</h2>
<p>We present some cherry-picked examples for G2T in Tab. 12 and for T2G in Tab. 11 for both WebNLG and TekGen datasets.</p>
<h2>G Processed TekGen Dataset</h2>
<p>In Fig. 3 we show an example of our processing of TEXGen dataset in establishing subject, relation, object boundaries. This enables both training and evaluating systems for T2G and G2T tasks.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">WebNLG G2T Best Models</th>
<th style="text-align: center;">Category</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$ <br> NLTK</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ours t5-large ReGen-CE</td>
<td style="text-align: center;">unseen</td>
<td style="text-align: center;">48.76</td>
<td style="text-align: center;">0.489</td>
<td style="text-align: center;">0.397</td>
<td style="text-align: center;">0.653</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">seen</td>
<td style="text-align: center;">59.73</td>
<td style="text-align: center;">0.592</td>
<td style="text-align: center;">0.433</td>
<td style="text-align: center;">0.722</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">55.26</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">0.694</td>
</tr>
<tr>
<td style="text-align: left;">Ours t5-large ReGen-SCST</td>
<td style="text-align: center;">unseen</td>
<td style="text-align: center;">49.06</td>
<td style="text-align: center;">0.493</td>
<td style="text-align: center;">0.404</td>
<td style="text-align: center;">0.665</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">seen</td>
<td style="text-align: center;">61.22</td>
<td style="text-align: center;">0.605</td>
<td style="text-align: center;">0.440</td>
<td style="text-align: center;">0.734</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">all</td>
<td style="text-align: center;">56.25</td>
<td style="text-align: center;">0.559</td>
<td style="text-align: center;">0.425</td>
<td style="text-align: center;">0.706</td>
</tr>
</tbody>
</table>
<p>Table 5: G2T: Results for seen, unseen, and all categories subsets in WebNLG+ 2020 Challenge Test dataset. As expected, unseen categories much worse results than for seen categories. RL fine-tuning manages to improve on both seen and unseen categories.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">WebNLG T2G <br> ReGen T2G.CE</th>
<th style="text-align: left;">Match</th>
<th style="text-align: center;">F1 $\uparrow$</th>
<th style="text-align: center;">Precision $\uparrow$</th>
<th style="text-align: center;">Recall $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">unseen</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: center;">0.5809</td>
<td style="text-align: center;">0.5662</td>
<td style="text-align: center;">0.6069</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: center;">0.7014</td>
<td style="text-align: center;">0.6741</td>
<td style="text-align: center;">0.7497</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: center;">0.6453</td>
<td style="text-align: center;">0.6241</td>
<td style="text-align: center;">0.6826</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: center;">0.5754</td>
<td style="text-align: center;">0.5608</td>
<td style="text-align: center;">0.6012</td>
</tr>
<tr>
<td style="text-align: left;">seen</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: center;">0.8322</td>
<td style="text-align: center;">0.8286</td>
<td style="text-align: center;">0.8384</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: center;">0.8878</td>
<td style="text-align: center;">0.8811</td>
<td style="text-align: center;">0.8998</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: center;">0.8604</td>
<td style="text-align: center;">0.8553</td>
<td style="text-align: center;">0.8696</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: center;">0.8317</td>
<td style="text-align: center;">0.8282</td>
<td style="text-align: center;">0.8379</td>
</tr>
<tr>
<td style="text-align: left;">all</td>
<td style="text-align: left;">Exact</td>
<td style="text-align: center;">0.7229</td>
<td style="text-align: center;">0.7144</td>
<td style="text-align: center;">0.7376</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Ent_Type</td>
<td style="text-align: center;">0.8067</td>
<td style="text-align: center;">0.7910</td>
<td style="text-align: center;">0.8345</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Partial</td>
<td style="text-align: center;">0.7668</td>
<td style="text-align: center;">0.7547</td>
<td style="text-align: center;">0.7882</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Strict</td>
<td style="text-align: center;">0.7202</td>
<td style="text-align: center;">0.7118</td>
<td style="text-align: center;">0.7349</td>
</tr>
</tbody>
</table>
<p>Table 6: T2G: Results for seen, unseen, and all categories subsets in WebNLG+ 2020 Challenge Test dataset. As expected the performance drops significantly for unseen categories and are the best for seen categories.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">SCST Reward</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$ <br> NLTK</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BLEU</td>
<td style="text-align: center;">0.556</td>
<td style="text-align: center;">0.552</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.698</td>
</tr>
<tr>
<td style="text-align: left;">BLEU NLTK</td>
<td style="text-align: center;">0.558</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.422</td>
<td style="text-align: center;">0.700</td>
</tr>
<tr>
<td style="text-align: left;">METEOR</td>
<td style="text-align: center;">$\mathbf{0 . 5 6 3}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 9}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 6}$</td>
</tr>
<tr>
<td style="text-align: left;">chrF++</td>
<td style="text-align: center;">0.554</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.423</td>
<td style="text-align: center;">0.701</td>
</tr>
<tr>
<td style="text-align: left;">$1 / 2$-METEOR+ $1 / 2$-BLEU NLTK</td>
<td style="text-align: center;">0.555</td>
<td style="text-align: center;">0.551</td>
<td style="text-align: center;">0.421</td>
<td style="text-align: center;">0.699</td>
</tr>
<tr>
<td style="text-align: left;">$2 / 3$-METEOR+ $1 / 3$-BLEU NLTK</td>
<td style="text-align: center;">0.547</td>
<td style="text-align: center;">0.543</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.697</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablation study of metrics used as rewards in SCST for t5-large models. The results shown are on the test split.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Learning Rate</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$ <br> NLTK</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$10^{-6}$</td>
<td style="text-align: center;">0.553</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">0.698</td>
</tr>
<tr>
<td style="text-align: center;">$5 \cdot 10^{-6}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 5 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 4 2 2}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 0}$</td>
</tr>
<tr>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">0.544</td>
<td style="text-align: center;">0.542</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">0.696</td>
</tr>
</tbody>
</table>
<p>Table 8: Ablation study on learning rates in SCST (using BLEU NLTK as the optimized metric)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">WebNLG G2T</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Team/model</td>
<td style="text-align: center;">NLTK</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL.ES.meteor t5-base (early CE)</td>
<td style="text-align: center;">0.527</td>
<td style="text-align: center;">0.523</td>
<td style="text-align: center;">0.413</td>
<td style="text-align: center;">0.689</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL.best.meteor t5-base (best CE)</td>
<td style="text-align: center;">0.528</td>
<td style="text-align: center;">0.526</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">0.681</td>
</tr>
</tbody>
</table>
<p>Table 9: G2T: Best results for t5-base fine-tuned with SCST using METEOR as reward.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Team Name</th>
<th style="text-align: center;">BLEU $\uparrow$</th>
<th style="text-align: center;">BLEU $\uparrow$ <br> NLTK</th>
<th style="text-align: center;">METEOR $\uparrow$</th>
<th style="text-align: center;">chrF++ $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ReGen G2T.CE t5-large</td>
<td style="text-align: center;">$0.543 \pm 0.007$</td>
<td style="text-align: center;">$0.540 \pm 0.007$</td>
<td style="text-align: center;">$0.416 \pm 0.002$</td>
<td style="text-align: center;">$0.691 \pm 0.002$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL t5-large</td>
<td style="text-align: center;">$0.553 \pm 0.007$</td>
<td style="text-align: center;">$0.550 \pm 0.007$</td>
<td style="text-align: center;">$0.422 \pm 0.002$</td>
<td style="text-align: center;">$0.702 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.CE.ES t5-base (early CE)</td>
<td style="text-align: center;">$0.521 \pm 0.004$</td>
<td style="text-align: center;">$0.517 \pm 0.004$</td>
<td style="text-align: center;">$0.404 \pm 0.001$</td>
<td style="text-align: center;">$0.675 \pm 0.002$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL.ES t5-base (early CE)</td>
<td style="text-align: center;">$0.528 \pm 0.007$</td>
<td style="text-align: center;">$0.523 \pm 0.007$</td>
<td style="text-align: center;">$0.408 \pm 0.002$</td>
<td style="text-align: center;">$0.682 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.CE.best t5-base (best CE)</td>
<td style="text-align: center;">$0.524 \pm 0.000$</td>
<td style="text-align: center;">$0.520 \pm 0.001$</td>
<td style="text-align: center;">$0.404 \pm 0.000$</td>
<td style="text-align: center;">$0.670 \pm 0.000$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL.best t5-base (best CE)</td>
<td style="text-align: center;">$0.525 \pm 0.007$</td>
<td style="text-align: center;">$0.522 \pm 0.007$</td>
<td style="text-align: center;">$0.407 \pm 0.002$</td>
<td style="text-align: center;">$0.681 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL.ES.meteor t5-base (early CE)</td>
<td style="text-align: center;">$0.525 \pm 0.007$</td>
<td style="text-align: center;">$0.521 \pm 0.007$</td>
<td style="text-align: center;">$0.412 \pm 0.002$</td>
<td style="text-align: center;">$0.687 \pm 0.003$</td>
</tr>
<tr>
<td style="text-align: left;">ReGen G2T.RL.best.meteor t5-base (best CE)</td>
<td style="text-align: center;">$0.527 \pm 0.007$</td>
<td style="text-align: center;">$0.524 \pm 0.007$</td>
<td style="text-align: center;">$0.410 \pm 0.002$</td>
<td style="text-align: center;">$0.686 \pm 0.003$</td>
</tr>
</tbody>
</table>
<p>Table 10: Results means and standard deviations (SD), shown as mean $\pm \mathrm{SD}$, for CE and SCST trained models (including our best results model) for a total of 3 different random number generator seeds used in training.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An example from the processed TEXGEN dataset. The original dataset lacks KG boundaries, which makes it difficult to evaluate T2G systems efficiently.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Sentence / Graph</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">The Pontiac Rageous began and ended its production in 1997 on an assembly line in Detroit, a city in Michigan.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Pontiac_Rageous $\diamond$ productionStartYear $\diamond 1997 \diamond$ Pontiac_Rageous $\diamond$ assembly $\diamond$ Michigan $\diamond$ Pontiac_Rageous $\diamond$ assembly $\diamond$ Detroit $\diamond$ Pontiac_Rageous $\diamond$ productionEndYear $\diamond 1997 \diamond$ Detroit $\diamond$ type $\diamond$ City_(Michigan)</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">Pontiac_Rageous $\diamond$ assembly $\diamond$ Detroit $\diamond$ Pontiac_Rageous $\diamond$ modelYears $\diamond 1997 \diamond$ Pontiac_Rageous $\diamond$ modelYears $\diamond 1997 \diamond$ Detroit $\diamond$ isPartOf $\diamond$ Michigan</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">Pontiac_Rageous $\diamond$ assembly $\diamond$ Detroit $\diamond$ Pontiac_Rageous $\diamond$ modelYears $\diamond 1997 \diamond$ Pontiac_Rageous $\diamond$ assembly $\diamond$ Michigan</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">In the United States, where Abraham A, Ribicoff was born, African Americans are one of the ethnic groups. Abraham A. Ribicoff was married to Ruth Ribicoff.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Abraham_A._Ribicoff $\diamond$ spouse $\diamond$ "Ruth Ribicoff" $\diamond$ Abraham_A._Ribicoff $\diamond$ birthPlace $\diamond$ United_States $\diamond$ United_States $\diamond$ ethnicGroup $\diamond$ African_Americans $\diamond$ Abraham_A._Ribicoff $\diamond$ nationality $\diamond$ United_States</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">Abraham_A._Ribicoff $\diamond$ birthPlace $\diamond$ United_States $\diamond$ Abraham_A._Ribicoff $\diamond$ spouse $\diamond$ "Ruth Ribicoff" $\diamond$ United_States $\diamond$ ethnicGroup $\diamond$ African_Americans</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">Abraham_A._Ribicoff $\diamond$ birthPlace $\diamond$ United_States $\diamond$ Abraham_A._Ribicoff $\diamond$ spouse $\diamond$ "Ruth Ribicoff" $\diamond$ Abraham_A._Ribicoff $\diamond$ nationality $\diamond$ American $\diamond$ United_States $\diamond$ ethnicGroup $\diamond$ African_Americans</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Super Capers, edited by Stacy Katzman, is a 98 minute film starring Michael Rooker and Tom Sizemore.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Super_Capers $\diamond$ editing $\diamond$ Stacy_Katzman $\diamond$ Super_Capers $\diamond$ starring $\diamond$ Michael_Rooker $\diamond$ Super_Capers $\diamond$ starring $\diamond$ Tom_Sizemore $\diamond$ Super_Capers $\diamond$ runtime 198.0</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">Super_Capers $\diamond$ starring $\diamond$ Tom_Sizemore $\diamond$ Super_Capers $\diamond$ timeOut $\diamond$ "980.0"(minutes) $\diamond$ Super_Capers $\diamond$ starring $\diamond$ Michael_Rooker $\diamond$ Super_Capers $\diamond$ editor $\diamond$ Stacy_Katzman</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">Super_Capers $\diamond$ starring $\diamond$ Tom_Sizemore $\diamond$ Super_Capers $\diamond$ length $\diamond 98.0$ (minutes) $\diamond$ Super_Capers $\diamond$ starring $\diamond$ Michael_Rooker $\diamond$ Super_Capers $\diamond$ editor $\diamond$ Stacy_Katzman</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Doctor George Cary (1611-1680), Professor of Sacred Theology, lord of the manor of Clovelly, Devon, was Dean of Exeter between 1663 and 1680 (amongst other duties responsible for the maintenance and decoration of Exeter Cathedral).</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">George Cary (1611-1680) $\diamond$ position held $\diamond$ Dean of Exeter $\diamond$ start time $\diamond 01$ January 1663 $\diamond$ date of birth $\diamond 001611 \diamond$ date of death $\diamond 001680$</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">George Cary (priest) $\diamond$ date of birth $\diamond 01$ January 1611 $\diamond$ date of death $\diamond 01$ January 1680</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">George Cary (priest) $\diamond$ position held $\diamond$ Dean of Exeter $\diamond$ date of birth $\diamond 01$ January 1611 $\diamond$ date of death $\diamond 01$ January 1680</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Early general elections were held in the Bahamas on 10 April 1968.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">1968 Bahamian general election $\diamond$ point in time $\diamond 10$ April 1968 $\diamond$ country $\diamond$ The Bahamas $\diamond$ applies to jurisdiction $\diamond$ The Bahamas</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">1968 Bahamian general election $\diamond$ point in time $\diamond 10$ April 1968</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">1968 Bahamian general election $\diamond$ point in time $\diamond 10$ April 1968 $\diamond$ country $\diamond$ The Bahamas</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">The school was established on 6 January 1930, by former education minister, CWW Kannangara, who additionally founded two other colleges located in central Ceylon.</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Kattankudy Central College $\diamond$ instance of $\diamond$ School</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">Government Polytechnic, Colombo $\diamond$ inception $\diamond 001930$</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">Government Polytechnic, Colombo $\diamond$ inception $\diamond 001930 \diamond$ instance of $\diamond$ School</td>
</tr>
</tbody>
</table>
<p>Table 11: Few cherry-picked generation for T2G task for WebNLG+ 2020 (top three) and TEXGEN (bottom three). For each source (Text), we show the ground truth (Gold) and system generated hypothesis from the best CE (Hyp-CE) and SCST models (Hyp-SCST). Note that the set of triples in WebNLG+ takes the form $x_{0}=$ $\left[\left(s^{1} \diamond p^{1} \diamond o^{1}\right), \ldots,\left(s^{K} \diamond p^{K} \diamond o^{K}\right)\right]$, whereas the same for TEXGEN is of form $x_{0}=\left[s \diamond\left(p^{1} \diamond o^{1}\right), \ldots,\left(p^{K} \diamond o^{K}\right)\right]$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Graph / Sentence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">McVeagh_of_the_South_Seas $\diamond$ starring $\diamond$ Harry_Carey_(actor_born_1878) $\diamond$ McVeagh_of_the_South_Seas $\diamond$ writer $\diamond$ Harry_Carey_(actor_born_1878)</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Born in 1878, Harry Carey later grew up to write and star in the movie McVeagh of the South Seas. Harry Carey, born in 1878, wrote and appeared in the movie McVeagh of the South Seas. Harry Carey, who was born in 1878, wrote and starred the film of McVeagh of the South Seas.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">McVeagh of the South Seas was written by Harry Carey, who was born in 1878.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">McVeagh of the South Seas was written by Harry Carey and starred the actor Harry Carey who was born in 1878 .</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Aleksandr_Prudnikov $\diamond$ height $\diamond 185.0$ (centimetres) $\diamond$ Aleksandr_Prudnikov $\diamond$ youthclub $\diamond$</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">FC_Spartak_Moscow $\diamond$ FC_Spartak_Moscow $\diamond$ ground $\diamond$ Otkrytiye_Arena <br> Aleksandr Prudnikov, 185 cm tall played for FC Spartak Moscow's youth team. FC Spartak Moscow is based in the Otkrytiye Arena. Aleksandr Prudnikov who is 185 cm tall is a member of the youth side of FC Spartak Moscow. The home ground of FC Spartak Moscow is Otkrytiye Arena. Aleksandr Prudnikov is 185.0 cm tall and played for the FC Spartak Moscow at the Otkrytiye Arena.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">Aleksandr Prudnikov is 185 cm tall and played for FC Spartak Moscow's youth team at the Otkrytiye Arena.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">Aleksandr Prudnikov is 185 cm tall and played for the youth team of FC Spartak Moscow whose home ground is the Otkrytiye Arena.</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Baku_Turkish_Martyrs'_Memorial $\diamond$ nativeName $\diamond$ "Türk Şehitleri Anitı" $\diamond$</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Baku_Turkish_Martyrs'_Memorial $\diamond$ location $\diamond$ Azerbaijan <br> The Native name of the Baku Turkish Martyrs' Memorial is "Türk Şehitleri Anıtı" which is located in Azerbaijan. The native name of the Baku Turkish Martyrs' Memorial is Türk Şehitleri Anıtı located in Azerbaijan. The native name for the Baku Turkish Martyrs' Memorial is Türk Şehitleri Anıtı, which is located in Baku, Azerbaijan.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">The native name of the Baku Turkish Martyrs' Memorial in Azerbaijan is Türk Şehitleri Anıtı.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">The Baku Turkish Martyrs' Memorial is located in Azerbaijan and is known locally as Türk Şehitleri Anıtı.</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">John Banister (anatomist) $\diamond$ occupation $\diamond$ Surgeon $\diamond$ date of birth $\diamond 01$ January $1533 \diamond$ date of death $\diamond 01$ January 1610</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">John Banister (1533-1610) was an English anatomist, surgeon and teacher.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">John Banister (1533-1610) was an English surgeon.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">John Banister (1533-1610) was an English surgeon and anatomist.</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">WNPT (TV) $\diamond$ country $\diamond$ United States $\diamond$ instance of $\diamond$ Television station</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">WNPT, virtual channel 8 (VHF digital channel 7), is a PBS member television station licensed to Nashville, Tennessee, United States.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">WNPT, virtual channel 3 (UHF digital channel 15), is a Fox-affiliated television station licensed to Portland, Oregon, United States.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">WNPT, virtual channel 4 (UHF digital channel 16), is a Public Broadcasting Service (PBS) member television station licensed to Portland, Oregon, United States.</td>
</tr>
<tr>
<td style="text-align: center;">Source</td>
<td style="text-align: center;">Our Lady of the Presentation Cathedral, Natal $\diamond$ inception $\diamond 21$ November 1988</td>
</tr>
<tr>
<td style="text-align: center;">Gold</td>
<td style="text-align: center;">Our Lady of the Presentation Cathedral, Natal was inaugurated on November 21, 1988, and is located in the district of Cidade Alta in Natal, capital of the Brazilian state of Rio Grande do Norte.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-CE</td>
<td style="text-align: center;">Our Lady of the Presentation Cathedral, Natal was built in 1988.</td>
</tr>
<tr>
<td style="text-align: center;">Hyp-SCST</td>
<td style="text-align: center;">Our Lady of the Presentation Cathedral, Natal was consecrated on 21 November 1988.</td>
</tr>
</tbody>
</table>
<p>Table 12: Few cherry-picked generation for G2T task for WebNLG+ 2020 (top three) and TeKGEN (bottom three). For each source (Graph), we show the ground truth (Gold) and system generated hypothesis from the best CE (Hyp-CE) and SCST models (Hyp-SCST). Note that the set of triples in WebNLG+ 2020 takes the form $x_{0}=$ $\left[\left(s^{1} \diamond p^{1} \diamond o^{1}\right), \ldots,\left(s^{K} \diamond p^{K} \diamond o^{K}\right)\right]$, whereas the same for TeKGEN is of form $x_{0}=\left[s \diamond\left(p^{1} \diamond o^{1}\right), \ldots,\left(p^{K} \diamond o^{K}\right)\right]$</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6817 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6817</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6817</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-b4013141cceb937c46ea5f84f8c06f6bf1215106</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b4013141cceb937c46ea5f84f8c06f6bf1215106" target="_blank">PRover: Proof Generation for Interpretable Reasoning over Rules</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work proposes PROVER, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs, and learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm.</p>
                <p><strong>Paper Abstract:</strong> Recent work by Clark et al. (2020) shows that transformers can act as 'soft theorem provers' by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PROVER, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance. First, PROVER generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers (up to 6% improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement). Third, PROVER obtains near perfect QA accuracy of 98% using only 40% of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for 'depth 5', indicating significant scope for future work. Our code and models are publicly available at this https URL</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6817.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6817.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRoVer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRoVer: Proof Generation for Interpretable Reasoning over Rules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-based joint model that answers binary questions over natural-language rule-bases and generates corresponding proof graphs (nodes = facts/rules/NAF, edges = rule applications); built on RoBERTa-large with separate QA, node and edge modules and ILP-based constrained inference for valid proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PRoVer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa-large backbone with three task heads (QA head, Node head, Edge head). Node embeddings obtained by mean-pooling token embeddings; edge embeddings are [node_i, node_j, node_j - node_i]. Joint cross-entropy training over answer, node presence, and edge presence. During training some edge labels are masked by semantic constraints; during inference an ILP (PuLP) with max-flow connectivity constraints selects final edges.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈355M parameters (RoBERTa-large backbone)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (RoBERTa-large) + task-specific node/edge heads + ILP-based constrained graph inference (max-flow connectivity)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained and evaluated on RuleTakers-style datasets: DU0-DU5 (synthetic rule theories with depths 0..5), ParaRules (paraphrased natural-language theories), Birds-Electricity (test-only robustness set).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Explicit proof generation as directed proof graphs (node selection + edge prediction) learned end-to-end; proof constraints enforced via masked labels at training and ILP optimization at inference; NAF (negation-as-failure) represented as a learned node embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>ILP solver (PuLP) is used at inference to optimize binary edge selection under global linear constraints including flow-based connectivity (max-flow reduction).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>DU0-DU5 (Rule reasoning datasets), ParaRules, Birds-Electricity</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>DU0-DU5: synthetic rule-bases in natural language with reasoning depth up to 5. ParaRules: crowdsourced paraphrases of rule theories. Birds-Electricity: out-of-distribution test-only contexts for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Binary QA over rule-bases (entailment) and proof graph generation (node+edge prediction; exact proof match)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>QA accuracy; Node Accuracy (exact node set match); Edge Accuracy (exact edge set match); Proof Accuracy (exact proof graph match); Full Accuracy (answer+proof correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On DU5 test (All): QA 99.3% accuracy; Node Accuracy 89.2%; Edge Accuracy 87.5%; Proof Accuracy 87.1%; Full Accuracy 87.1%. Depth 5 proof accuracy drops to 65.1%. On ParaRules (combined training) proof accuracy ~95% for paraphrased data (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to RuleTakers (fine-tuned RoBERTa), PRoVer matches or modestly improves QA (overall DU5 QA: RT 99.2% vs PRoVer 99.3%) and uniquely provides proof graphs (87.1% exact). Paper reports up to +6% QA improvement in zero-shot scenarios and up to +15% absolute QA improvement when trained on lower-depth data and tested on higher-depth questions (generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Joint training with explicit proof supervision yields interpretable proofs with high exact-match rates (87% overall) while retaining or slightly improving QA accuracy; constrained training + ILP inference improves proof quality; PRoVer generalizes better to higher reasoning depths for QA than a baseline that only predicts answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Proof generation degrades with proof depth (edge prediction becomes harder; depth-5 PA 65.1%); proof supervision is required (expensive to obtain); ILP inference adds computational overhead; datasets lack some logic constructs (e.g., disjunctions) so extensions are needed for other logical fragments; model tends to underestimate required nodes (predicted node set is a subset in ~42% of node-error cases).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRover: Proof Generation for Interpretable Reasoning over Rules', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6817.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6817.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RuleTakers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RuleTakers (Transformers as soft reasoners over language)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach that fine-tunes a transformer (RoBERTa) to predict binary truth-values of statements given natural-language facts and rules; characterized as a 'soft theorem prover' because it predicts answers without producing formal proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers as soft reasoners over language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RuleTakers (RoBERTa fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Fine-tuned RoBERTa model (classification head on [CLS]) trained to map (context + question) to binary True/False answers over natural-language rule-bases; explanations produced post-hoc via leave-one-out critical sentence identification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Uses RoBERTa-large (~355M) in the referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (RoBERTa-large) fine-tuned for binary classification</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Trained on DU0-DU5 style synthetic rule reasoning datasets (Clark et al., 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>End-to-end answer prediction by fine-tuning pretrained transformer; no explicit proof generation. Explanations produced by leave-one-out (remove each sentence and see if answer flips) to find critical sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>DU0-DU5 (Rule reasoning datasets), ParaRules, Birds-Electricity (in comparative evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same as PRoVer datasets; used as baseline for QA over rule-bases.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Binary QA / entailment over natural-language rule-bases; critical-sentence identification (leave-one-out) as a post-hoc explanation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>QA accuracy; critical-sentence identification metrics (accuracy/precision/recall/F1 reported for no-negation subset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On DU5 test (All) reported QA ~99.2% (Table 1 RT QA). Critical sentence identification (no-negation subset): accuracy 74.5%, precision 98.7%, recall 86.9%, F1 92.4% (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used as baseline for PRoVer; PRoVer matches or slightly surpasses RuleTakers on QA and substantially adds proof-generation capability. PRoVer showed improved critical-sentence identification (78.1% accuracy vs RuleTakers 74.5%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained transformers fine-tuned on synthetic rule datasets can achieve very high QA accuracy (can function as 'soft' theorem provers), but they do not produce structured proofs and their post-hoc leave-one-out explanations have limitations (fail on negation, only determine presence/absence of sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not generate explicit proofs; leave-one-out critical sentence method is brittle (breaks with negation, does not capture reasoning chain/directionality); less effective at providing interpretable, compositional justifications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRover: Proof Generation for Interpretable Reasoning over Rules', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6817.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6817.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa: A robustly optimized BERT pretraining approach (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pretrained transformer language model (an optimized BERT variant) used as the encoder backbone in PRoVer and as the base model for RuleTakers; provides contextual token embeddings used by downstream QA/node/edge heads.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: A robustly optimized bert pretraining approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder pretrained with optimized masked language-modeling and training recipes; commonly used as a backbone for fine-tuning on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈355M parameters (large)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer encoder (masked LM pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on large unlabeled corpora (original RoBERTa pretraining datasets); fine-tuned on DU datasets / ParaRules for reasoning tasks in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Pretraining provides linguistic and some reasoning priors; strict logical reasoning achieved by fine-tuning (RuleTakers) or by augmenting with explicit proof heads and constrained inference (PRoVer).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used as backbone on DU datasets, ParaRules, Birds-Electricity in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Not a benchmark; the model is fine-tuned/evaluated on rule-reasoning benchmarks described above.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Backbone encoder for binary QA and proof graph prediction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>N/A (backbone), contributes to downstream QA and proof metrics reported under PRoVer and RuleTakers</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>RoBERTa-large is the common backbone for both PRoVer and RuleTakers; PRoVer augments it with node/edge heads and ILP to gain interpretability without QA loss.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A large pretrained transformer encoder can be adapted for strong performance on structured logical tasks when augmented with task-specific modules/constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>By itself (only fine-tuned classifier) it yields answers but not interpretable structured proofs; must be extended (as PRoVer does) for explicit proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRover: Proof Generation for Interpretable Reasoning over Rules', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6817.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6817.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ILP + max-flow constraints</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integer Linear Programming inference with max-flow connectivity constraints</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Inference-time constrained optimization that selects a binary edge set maximizing agreement with the model's edge probabilities while enforcing semantic rules (no fact→fact, no rule→fact, edges only between present nodes) and ensuring proof graph connectivity via a max-flow formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ILP + max-flow connectivity</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Formulate edge selection as an ILP maximizing per-edge agreement scores (φ) subject to binary edge variables and flow variables; connectivity enforced by requiring a max-flow of |N| in an augmented source-sink graph; implemented with PuLP.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Constrained optimization (ILP) + graph flow constraints</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Enforces global structural constraints on predicted proofs (connectivity, semantic edge masks) at inference, turning local edge logits into a globally consistent proof graph.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>PuLP ILP solver is used to solve the binary optimization with flow variables; max-flow semantics ensure connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Used for inference on DU0-DU5 / ParaRules experiments in PRoVer</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Not a benchmark; part of PRoVer inference pipeline applied to those datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof graph consistency and selection (post-processing/inference-level reasoning)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Impact on Edge/Proof Accuracy and Full Accuracy in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation: removing constrained training but adding ILP at inference ('UT + ILP') yields slight proof improvements compared to no ILP; removing ILP at inference ('UT + No ILP') causes ~5-6% drop in edge/proof accuracy versus PRoVer; connectivity constraint itself produced marginal gains (few disconnected cases).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Applying ILP constraints at inference improves final proof validity relative to unconstrained selection of edges; constrained training + ILP performs best.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ILP-based constrained inference can effectively enforce global proof properties (connectivity, semantic edge constraints) and improves proof exact-match rates when combined with constrained training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Adds computational overhead at inference (solving ILPs); benefits limited if model already predicts connected proofs in most cases (connectivity constraint gave marginal gains); scalability could be an issue with larger graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRover: Proof Generation for Interpretable Reasoning over Rules', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6817.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6817.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Neural Theorem Proving (related works)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Theorem Proving / neuro-symbolic theorem-proving approaches (e.g., NLProlog)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of approaches combining symbolic reasoning techniques and differentiable neural models to learn or emulate theorem proving and logical inference; cited as conceptual background and contrast to PRoVer's approach on natural-language inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NLProlog: Reasoning with weak unification for question answering in natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Theorem Proving family (e.g., NLProlog, neural SAT learners)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neuro-symbolic systems that integrate symbolic proof/search mechanisms (unification, rule application, SAT solving) with learned components (embeddings, differentiable modules) to perform logical inference or assist theorem-proving tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Neuro-symbolic hybrids (neural components + symbolic solvers / differentiable unification / SAT learners)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Varies by work (synthetic theorems, program traces, SAT instances); not directly used in PRoVer experiments but mentioned in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Combines symbolic rule application/search with neural scoring or differentiable approximations (weak unification, neural-guided search, learned SAT heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Prior works evaluate on theorem proving, SAT, program induction, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Includes tasks like SAT solving, formula embedding for ATP, neural-program induction benchmarks; mentioned to situate PRoVer relative to neuro-symbolic literature.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>First-order/Propositional theorem proving, SAT solving, neural program induction (varies by paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Varies by work (solver success, accuracy, proof completeness); not reported as a single number here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>PRoVer differs by operating on free-form natural language rule-bases and producing graph-like proofs learned end-to-end, rather than requiring formal logical representations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Neuro-symbolic theorem-proving work shows promise combining symbolic algorithms with neural learning; PRoVer draws conceptual similarity but focuses on natural-language inputs and interpretable proof graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Prior neuro-symbolic systems often require formal representations or domain-specific engineering; PRoVer intentionally bypasses semantic parsing into formal logic but still inherits challenges of representing richer logical constructs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRover: Proof Generation for Interpretable Reasoning over Rules', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6817.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6817.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Critical-sentence identification (leave-one-out)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leave-one-out critical sentence identification (post-hoc explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-hoc explanatory method that identifies sentences (facts/rules) critical to a model's answer by removing each sentence and checking whether the predicted answer flips; used in prior work (RuleTakers) and compared against PRoVer's learned proof nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers as soft reasoners over language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Leave-one-out critical sentence identification</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Remove each sentence in the context and recompute model's prediction; sentences whose removal flips the answer are labeled 'critical'.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Post-hoc perturbation/explainability method</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Perturbation-based explanation (ablation): treat sentences whose removal changes model output as contributing to the inference.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Evaluated on DU5 No-negation subset for direct comparison in the paper</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>No-negation subset omits examples with negation to make leave-one-out sensible.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Critical sentence identification (explainability)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy, Precision, Recall, F1 for identifying exact critical sentences</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On DU5 no-negation subset: RuleTakers accuracy 74.5%, precision 98.7%, recall 86.9%, F1 92.4%; PRoVer accuracy 78.1%, precision 98.7%, recall 87.2%, F1 92.6% (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>PRoVer (which produces explicit proof node sets) outperforms the leave-one-out method in exact critical-sentence identification (78.1% vs 74.5% accuracy in the evaluated subset).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Leave-one-out can identify critical sentences in many cases but fails with negation and cannot capture edges/directionality; PRoVer's learned node/edge proof representation gives more complete, compositional explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not applicable when theories include negation; only identifies presence/absence of sentences and not the full reasoning chain or edge directions; computationally expensive (re-run model for each removed sentence).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PRover: Proof Generation for Interpretable Reasoning over Rules', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>NLProlog: Reasoning with weak unification for question answering in natural language <em>(Rating: 2)</em></li>
                <li>Learning a SAT solver from single-bit supervision <em>(Rating: 1)</em></li>
                <li>RoBERTa: A robustly optimized bert pretraining approach <em>(Rating: 2)</em></li>
                <li>Neural programmer-interpreters <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6817",
    "paper_id": "paper-b4013141cceb937c46ea5f84f8c06f6bf1215106",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "PRoVer",
            "name_full": "PRoVer: Proof Generation for Interpretable Reasoning over Rules",
            "brief_description": "A transformer-based joint model that answers binary questions over natural-language rule-bases and generates corresponding proof graphs (nodes = facts/rules/NAF, edges = rule applications); built on RoBERTa-large with separate QA, node and edge modules and ILP-based constrained inference for valid proofs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PRoVer",
            "model_description": "RoBERTa-large backbone with three task heads (QA head, Node head, Edge head). Node embeddings obtained by mean-pooling token embeddings; edge embeddings are [node_i, node_j, node_j - node_i]. Joint cross-entropy training over answer, node presence, and edge presence. During training some edge labels are masked by semantic constraints; during inference an ILP (PuLP) with max-flow connectivity constraints selects final edges.",
            "model_size": "≈355M parameters (RoBERTa-large backbone)",
            "architecture_type": "Transformer (RoBERTa-large) + task-specific node/edge heads + ILP-based constrained graph inference (max-flow connectivity)",
            "training_data": "Trained and evaluated on RuleTakers-style datasets: DU0-DU5 (synthetic rule theories with depths 0..5), ParaRules (paraphrased natural-language theories), Birds-Electricity (test-only robustness set).",
            "reasoning_method": "Explicit proof generation as directed proof graphs (node selection + edge prediction) learned end-to-end; proof constraints enforced via masked labels at training and ILP optimization at inference; NAF (negation-as-failure) represented as a learned node embedding.",
            "external_tool_used": true,
            "external_tool_description": "ILP solver (PuLP) is used at inference to optimize binary edge selection under global linear constraints including flow-based connectivity (max-flow reduction).",
            "benchmark_name": "DU0-DU5 (Rule reasoning datasets), ParaRules, Birds-Electricity",
            "benchmark_description": "DU0-DU5: synthetic rule-bases in natural language with reasoning depth up to 5. ParaRules: crowdsourced paraphrases of rule theories. Birds-Electricity: out-of-distribution test-only contexts for robustness.",
            "task_type": "Binary QA over rule-bases (entailment) and proof graph generation (node+edge prediction; exact proof match)",
            "performance_metric": "QA accuracy; Node Accuracy (exact node set match); Edge Accuracy (exact edge set match); Proof Accuracy (exact proof graph match); Full Accuracy (answer+proof correct)",
            "performance_value": "On DU5 test (All): QA 99.3% accuracy; Node Accuracy 89.2%; Edge Accuracy 87.5%; Proof Accuracy 87.1%; Full Accuracy 87.1%. Depth 5 proof accuracy drops to 65.1%. On ParaRules (combined training) proof accuracy ~95% for paraphrased data (reported).",
            "comparison_with_baseline": "Compared to RuleTakers (fine-tuned RoBERTa), PRoVer matches or modestly improves QA (overall DU5 QA: RT 99.2% vs PRoVer 99.3%) and uniquely provides proof graphs (87.1% exact). Paper reports up to +6% QA improvement in zero-shot scenarios and up to +15% absolute QA improvement when trained on lower-depth data and tested on higher-depth questions (generalization).",
            "key_findings": "Joint training with explicit proof supervision yields interpretable proofs with high exact-match rates (87% overall) while retaining or slightly improving QA accuracy; constrained training + ILP inference improves proof quality; PRoVer generalizes better to higher reasoning depths for QA than a baseline that only predicts answers.",
            "limitations": "Proof generation degrades with proof depth (edge prediction becomes harder; depth-5 PA 65.1%); proof supervision is required (expensive to obtain); ILP inference adds computational overhead; datasets lack some logic constructs (e.g., disjunctions) so extensions are needed for other logical fragments; model tends to underestimate required nodes (predicted node set is a subset in ~42% of node-error cases).",
            "uuid": "e6817.0",
            "source_info": {
                "paper_title": "PRover: Proof Generation for Interpretable Reasoning over Rules",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "RuleTakers",
            "name_full": "RuleTakers (Transformers as soft reasoners over language)",
            "brief_description": "A baseline approach that fine-tunes a transformer (RoBERTa) to predict binary truth-values of statements given natural-language facts and rules; characterized as a 'soft theorem prover' because it predicts answers without producing formal proofs.",
            "citation_title": "Transformers as soft reasoners over language",
            "mention_or_use": "use",
            "model_name": "RuleTakers (RoBERTa fine-tuned)",
            "model_description": "Fine-tuned RoBERTa model (classification head on [CLS]) trained to map (context + question) to binary True/False answers over natural-language rule-bases; explanations produced post-hoc via leave-one-out critical sentence identification.",
            "model_size": "Uses RoBERTa-large (~355M) in the referenced work",
            "architecture_type": "Transformer (RoBERTa-large) fine-tuned for binary classification",
            "training_data": "Trained on DU0-DU5 style synthetic rule reasoning datasets (Clark et al., 2020)",
            "reasoning_method": "End-to-end answer prediction by fine-tuning pretrained transformer; no explicit proof generation. Explanations produced by leave-one-out (remove each sentence and see if answer flips) to find critical sentences.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "DU0-DU5 (Rule reasoning datasets), ParaRules, Birds-Electricity (in comparative evaluations)",
            "benchmark_description": "Same as PRoVer datasets; used as baseline for QA over rule-bases.",
            "task_type": "Binary QA / entailment over natural-language rule-bases; critical-sentence identification (leave-one-out) as a post-hoc explanation",
            "performance_metric": "QA accuracy; critical-sentence identification metrics (accuracy/precision/recall/F1 reported for no-negation subset)",
            "performance_value": "On DU5 test (All) reported QA ~99.2% (Table 1 RT QA). Critical sentence identification (no-negation subset): accuracy 74.5%, precision 98.7%, recall 86.9%, F1 92.4% (Table 9).",
            "comparison_with_baseline": "Used as baseline for PRoVer; PRoVer matches or slightly surpasses RuleTakers on QA and substantially adds proof-generation capability. PRoVer showed improved critical-sentence identification (78.1% accuracy vs RuleTakers 74.5%).",
            "key_findings": "Pretrained transformers fine-tuned on synthetic rule datasets can achieve very high QA accuracy (can function as 'soft' theorem provers), but they do not produce structured proofs and their post-hoc leave-one-out explanations have limitations (fail on negation, only determine presence/absence of sentences).",
            "limitations": "Does not generate explicit proofs; leave-one-out critical sentence method is brittle (breaks with negation, does not capture reasoning chain/directionality); less effective at providing interpretable, compositional justifications.",
            "uuid": "e6817.1",
            "source_info": {
                "paper_title": "PRover: Proof Generation for Interpretable Reasoning over Rules",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "RoBERTa-large",
            "name_full": "RoBERTa: A robustly optimized BERT pretraining approach (large)",
            "brief_description": "A large pretrained transformer language model (an optimized BERT variant) used as the encoder backbone in PRoVer and as the base model for RuleTakers; provides contextual token embeddings used by downstream QA/node/edge heads.",
            "citation_title": "RoBERTa: A robustly optimized bert pretraining approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large",
            "model_description": "Transformer encoder pretrained with optimized masked language-modeling and training recipes; commonly used as a backbone for fine-tuning on downstream tasks.",
            "model_size": "≈355M parameters (large)",
            "architecture_type": "Transformer encoder (masked LM pretraining)",
            "training_data": "Pretrained on large unlabeled corpora (original RoBERTa pretraining datasets); fine-tuned on DU datasets / ParaRules for reasoning tasks in these experiments.",
            "reasoning_method": "Pretraining provides linguistic and some reasoning priors; strict logical reasoning achieved by fine-tuning (RuleTakers) or by augmenting with explicit proof heads and constrained inference (PRoVer).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Used as backbone on DU datasets, ParaRules, Birds-Electricity in experiments",
            "benchmark_description": "Not a benchmark; the model is fine-tuned/evaluated on rule-reasoning benchmarks described above.",
            "task_type": "Backbone encoder for binary QA and proof graph prediction tasks",
            "performance_metric": "N/A (backbone), contributes to downstream QA and proof metrics reported under PRoVer and RuleTakers",
            "performance_value": null,
            "comparison_with_baseline": "RoBERTa-large is the common backbone for both PRoVer and RuleTakers; PRoVer augments it with node/edge heads and ILP to gain interpretability without QA loss.",
            "key_findings": "A large pretrained transformer encoder can be adapted for strong performance on structured logical tasks when augmented with task-specific modules/constraints.",
            "limitations": "By itself (only fine-tuned classifier) it yields answers but not interpretable structured proofs; must be extended (as PRoVer does) for explicit proof generation.",
            "uuid": "e6817.2",
            "source_info": {
                "paper_title": "PRover: Proof Generation for Interpretable Reasoning over Rules",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "ILP + max-flow constraints",
            "name_full": "Integer Linear Programming inference with max-flow connectivity constraints",
            "brief_description": "Inference-time constrained optimization that selects a binary edge set maximizing agreement with the model's edge probabilities while enforcing semantic rules (no fact→fact, no rule→fact, edges only between present nodes) and ensuring proof graph connectivity via a max-flow formulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ILP + max-flow connectivity",
            "model_description": "Formulate edge selection as an ILP maximizing per-edge agreement scores (φ) subject to binary edge variables and flow variables; connectivity enforced by requiring a max-flow of |N| in an augmented source-sink graph; implemented with PuLP.",
            "model_size": null,
            "architecture_type": "Constrained optimization (ILP) + graph flow constraints",
            "training_data": null,
            "reasoning_method": "Enforces global structural constraints on predicted proofs (connectivity, semantic edge masks) at inference, turning local edge logits into a globally consistent proof graph.",
            "external_tool_used": true,
            "external_tool_description": "PuLP ILP solver is used to solve the binary optimization with flow variables; max-flow semantics ensure connectivity.",
            "benchmark_name": "Used for inference on DU0-DU5 / ParaRules experiments in PRoVer",
            "benchmark_description": "Not a benchmark; part of PRoVer inference pipeline applied to those datasets.",
            "task_type": "Proof graph consistency and selection (post-processing/inference-level reasoning)",
            "performance_metric": "Impact on Edge/Proof Accuracy and Full Accuracy in ablations",
            "performance_value": "Ablation: removing constrained training but adding ILP at inference ('UT + ILP') yields slight proof improvements compared to no ILP; removing ILP at inference ('UT + No ILP') causes ~5-6% drop in edge/proof accuracy versus PRoVer; connectivity constraint itself produced marginal gains (few disconnected cases).",
            "comparison_with_baseline": "Applying ILP constraints at inference improves final proof validity relative to unconstrained selection of edges; constrained training + ILP performs best.",
            "key_findings": "ILP-based constrained inference can effectively enforce global proof properties (connectivity, semantic edge constraints) and improves proof exact-match rates when combined with constrained training.",
            "limitations": "Adds computational overhead at inference (solving ILPs); benefits limited if model already predicts connected proofs in most cases (connectivity constraint gave marginal gains); scalability could be an issue with larger graphs.",
            "uuid": "e6817.3",
            "source_info": {
                "paper_title": "PRover: Proof Generation for Interpretable Reasoning over Rules",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Neural Theorem Proving (related works)",
            "name_full": "Neural Theorem Proving / neuro-symbolic theorem-proving approaches (e.g., NLProlog)",
            "brief_description": "A family of approaches combining symbolic reasoning techniques and differentiable neural models to learn or emulate theorem proving and logical inference; cited as conceptual background and contrast to PRoVer's approach on natural-language inputs.",
            "citation_title": "NLProlog: Reasoning with weak unification for question answering in natural language",
            "mention_or_use": "mention",
            "model_name": "Neural Theorem Proving family (e.g., NLProlog, neural SAT learners)",
            "model_description": "Neuro-symbolic systems that integrate symbolic proof/search mechanisms (unification, rule application, SAT solving) with learned components (embeddings, differentiable modules) to perform logical inference or assist theorem-proving tasks.",
            "model_size": null,
            "architecture_type": "Neuro-symbolic hybrids (neural components + symbolic solvers / differentiable unification / SAT learners)",
            "training_data": "Varies by work (synthetic theorems, program traces, SAT instances); not directly used in PRoVer experiments but mentioned in related work.",
            "reasoning_method": "Combines symbolic rule application/search with neural scoring or differentiable approximations (weak unification, neural-guided search, learned SAT heuristics).",
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "Prior works evaluate on theorem proving, SAT, program induction, etc.",
            "benchmark_description": "Includes tasks like SAT solving, formula embedding for ATP, neural-program induction benchmarks; mentioned to situate PRoVer relative to neuro-symbolic literature.",
            "task_type": "First-order/Propositional theorem proving, SAT solving, neural program induction (varies by paper)",
            "performance_metric": "Varies by work (solver success, accuracy, proof completeness); not reported as a single number here.",
            "performance_value": null,
            "comparison_with_baseline": "PRoVer differs by operating on free-form natural language rule-bases and producing graph-like proofs learned end-to-end, rather than requiring formal logical representations.",
            "key_findings": "Neuro-symbolic theorem-proving work shows promise combining symbolic algorithms with neural learning; PRoVer draws conceptual similarity but focuses on natural-language inputs and interpretable proof graphs.",
            "limitations": "Prior neuro-symbolic systems often require formal representations or domain-specific engineering; PRoVer intentionally bypasses semantic parsing into formal logic but still inherits challenges of representing richer logical constructs.",
            "uuid": "e6817.4",
            "source_info": {
                "paper_title": "PRover: Proof Generation for Interpretable Reasoning over Rules",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Critical-sentence identification (leave-one-out)",
            "name_full": "Leave-one-out critical sentence identification (post-hoc explanation)",
            "brief_description": "A post-hoc explanatory method that identifies sentences (facts/rules) critical to a model's answer by removing each sentence and checking whether the predicted answer flips; used in prior work (RuleTakers) and compared against PRoVer's learned proof nodes.",
            "citation_title": "Transformers as soft reasoners over language",
            "mention_or_use": "mention",
            "model_name": "Leave-one-out critical sentence identification",
            "model_description": "Remove each sentence in the context and recompute model's prediction; sentences whose removal flips the answer are labeled 'critical'.",
            "model_size": null,
            "architecture_type": "Post-hoc perturbation/explainability method",
            "training_data": null,
            "reasoning_method": "Perturbation-based explanation (ablation): treat sentences whose removal changes model output as contributing to the inference.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Evaluated on DU5 No-negation subset for direct comparison in the paper",
            "benchmark_description": "No-negation subset omits examples with negation to make leave-one-out sensible.",
            "task_type": "Critical sentence identification (explainability)",
            "performance_metric": "Accuracy, Precision, Recall, F1 for identifying exact critical sentences",
            "performance_value": "On DU5 no-negation subset: RuleTakers accuracy 74.5%, precision 98.7%, recall 86.9%, F1 92.4%; PRoVer accuracy 78.1%, precision 98.7%, recall 87.2%, F1 92.6% (Table 9).",
            "comparison_with_baseline": "PRoVer (which produces explicit proof node sets) outperforms the leave-one-out method in exact critical-sentence identification (78.1% vs 74.5% accuracy in the evaluated subset).",
            "key_findings": "Leave-one-out can identify critical sentences in many cases but fails with negation and cannot capture edges/directionality; PRoVer's learned node/edge proof representation gives more complete, compositional explanations.",
            "limitations": "Not applicable when theories include negation; only identifies presence/absence of sentences and not the full reasoning chain or edge directions; computationally expensive (re-run model for each removed sentence).",
            "uuid": "e6817.5",
            "source_info": {
                "paper_title": "PRover: Proof Generation for Interpretable Reasoning over Rules",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "NLProlog: Reasoning with weak unification for question answering in natural language",
            "rating": 2,
            "sanitized_title": "nlprolog_reasoning_with_weak_unification_for_question_answering_in_natural_language"
        },
        {
            "paper_title": "Learning a SAT solver from single-bit supervision",
            "rating": 1,
            "sanitized_title": "learning_a_sat_solver_from_singlebit_supervision"
        },
        {
            "paper_title": "RoBERTa: A robustly optimized bert pretraining approach",
            "rating": 2,
            "sanitized_title": "roberta_a_robustly_optimized_bert_pretraining_approach"
        },
        {
            "paper_title": "Neural programmer-interpreters",
            "rating": 1,
            "sanitized_title": "neural_programmerinterpreters"
        }
    ],
    "cost": 0.0189625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PRoVer: Proof Generation for Interpretable Reasoning over Rules</h1>
<p>Swarnadeep Saha Sayan Ghosh Shashank Srivastava Mohit Bansal<br>UNC Chapel Hill<br>{swarna, sayghosh, ssrivastava, mbansal}@cs.unc.edu</p>
<h4>Abstract</h4>
<p>Recent work by Clark et al. (2020) shows that transformers can act as "soft theorem provers" by answering questions over explicitly provided knowledge in natural language. In our work, we take a step closer to emulating formal theorem provers, by proposing PROVER, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs. Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm. During inference, a valid proof, satisfying a set of global constraints is generated. We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance. First, PRoVer generates proofs with an accuracy of $87 \%$, while retaining or improving performance on the QA task, compared to RuleTakers (up to $6 \%$ improvement on zero-shot evaluation). Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to $15 \%$ improvement). Third, PRoVer obtains near perfect QA accuracy of $98 \%$ using only $40 \%$ of the training data. However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to $65 \%$ for "depth 5", indicating significant scope for future work. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Developing systems that can understand and reason over explicitly provided knowledge has been a fundamental goal of AI (Newell and Simon, 1956). Owing to the challenges posed in reasoning over formal representations (Musen and Van Der Lei,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Block diagram showing that PRoVer is a closer linguistic analog of formal reasoning.</p>
<p>1988), and backed by the recent successes of transformers (Vaswani et al., 2017) in NLP, Clark et al. (2020) propose a new version of the problem by replacing the formal representations of rule-bases with natural language (English). Specifically, their task requires predicting the truth value of a statement by reasoning over a set of facts and rules, all expressed in natural language. Figure 2 shows some examples of the task. Clark et al. (2020) propose RuleTakers, a fine-tuned RoBERTa model (Liu et al., 2019b) to show that transformers can act as "soft theorem provers" by predicting the final answer in such reasoning-based problems with high accuracy.</p>
<p>We argue that to use transformers for natural language reasoning reliably, they should be able to generate proofs that provide rationales for the predicted answer. Proof generation is vital for emulating formal reasoning but also for moving towards human-interpretable models that alleviate concerns about the black-box nature of deep architectures (Rudin, 2019). Towards this, we present PRoVer, a transformer-based model that jointly answers questions over natural language rule-bases and generates corresponding proofs. Figure 1 illustrates our method as a closer linguistic analog</p>
<p>of formal reasoning, as it generates proofs along with answers. However, unlike formal reasoners, PRoVER can operate on natural language text that provides the underlying theory, rather than rely on formal logical representations. Such methods that combine interpretability and flexibility in reasoning can have wide applications across domains.</p>
<p>PROVER's architecture consists of three modules that together generate answers along with proofs. In this work, proofs are represented as directed graphs consisting of the relevant rules and facts needed to prove or disprove the question statement. Section 3.1 contains details of this representation. A QA module predicts a binary answer for the question, a node module chooses which rules and facts are part of the proof, and an edge module predicts the presence and the direction of the edges between the chosen nodes. Model training minimizes a joint cross-entropy loss over the three modules. To guide the model to predict edges between valid nodes only, we enforce global constraints on the structure of the proof during training, by masking out labels for impossible edges, resulting in a more efficient learning problem. PRoVER generates valid proofs during inference by solving an ILP over the edge potentials, subject to multiple semantic constraints, such as ensuring proof graph connectivity. Our contributions are:</p>
<ul>
<li>We present PRoVER, an interpretable joint model that learns to reason over natural language rule-bases and generate corresponding proofs.</li>
<li>PRoVER performs similarly or improves upon state-of-the-art QA accuracy for the task, with up to $6 \%$ improvement on zero-shot evaluation, and generates exact proofs at $87 \%$ accuracy. Unlike RuleTakers, it does not require additional finetuning on the RACE (Lai et al., 2017) dataset.</li>
<li>PRoVER demonstrates significantly better generalization. When trained on lower depth questions, it shows better QA accuracy (up to 15\%) on higher depth ones.</li>
</ul>
<h2>2 Related Work</h2>
<p>Our work is related to multiple bodies of previous work in NLP and formal reasoning.</p>
<p>QA and NLI: The rule reasoning task is related to reasoning tasks that have been proposed recently. These include tasks in the bAbI dataset (Weston et al., 2015), synthetically generated probe tasks (Richardson et al., 2020) or reading comprehension tasks in datasets such as QuaRTz (Tafjord et al.,
2019) and ROPES (Lin et al., 2019). Unlike our task, most of these require reasoning over implicit rules, the focus being on language understanding and one step of rule application. Multi-hop QA datasets like HotpotQA (Yang et al., 2018) require multiple reasoning steps, but the inference rules needed are again implicitly inferred, rather than explicitly provided. Our task also bears similarity with Natural Language Inference (MacCartney and Manning, 2014), but NLI also allows unsupported inferences by filling gaps in explicitly stated knowledge (Dagan et al., 2013).</p>
<p>Formal Reasoning and Neural Theorem Proving: Semantic parsing (Zettlemoyer and Collins, 2005; Berant et al., 2013; Berant and Liang, 2014) of multi-sentence texts into logical forms has proved to be challenging, restricting the application of semantic parsers to formal reasoning systems (Kamath and Das, 2019). PRoVER bypasses this expensive and error-prone process and attempts to solve the problem in an end-to-end manner, without any intermediate logical representations.</p>
<p>Our approach is conceptually similar to a body of work on Neural Theorem Proving (Weber et al., 2019) that has focused on developing theorem provers by combining reasoning from symbolic techniques with the possibility of differentiable learning from neural networks. These include neuro-symbolic methods for table comprehension (Neelakantan et al., 2016), executing basic compositional programs (Reed and de Freitas, 2016), SAT solving (Selsam et al., 2019), formula embedding (Abdelaziz et al., 2020), approximate (DNF) model counting (Abboud et al., 2020), etc. However, PRoVER diverges from these in working with free-form natural language input to generate proofs similar to formal reasoners.</p>
<p>Model Interpretability: PRoVER follows a significant body of previous work on developing interpretable neural models for NLP tasks to foster explainability. Several approaches have focused on formalizing the notion of interpretability (Rudin, 2019; Doshi-Velez and Kim, 2017; Hase and Bansal, 2020), tweaking features for local model interpretability (Ribeiro et al., 2016, 2018) and exploring interpretability in latent spaces (Joshi et al., 2018; Samangouei et al., 2018). Our work can be seen as generating explanations in the form of proofs for an NLP task. While there has been prior work on generating natural language explana-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Diagram showing two rule-bases with rules, facts, questions, answers and proofs. PROVER answers all the questions correctly and also generates all the corresponding proofs accurately in the above scenarios.
tions for multiple NLP tasks, including NLI (Camburu et al., 2018), commonsense reasoning (Rajani et al., 2019; Zhang et al., 2020) and generic text classification tasks (Liu et al., 2019a), our novelty lies in generating compositional explanations consisting of proof graphs that detail the chain of reasoning, starting from language. We use a maxflow ILP formulation for checking proof graph connectivity (Even and Tarjan, 1975). Multiple approaches for NLP tasks such as sentiment analysis and content selection (Pang and Lee, 2004; Barzilay and Lapata, 2005; Bansal et al., 2008) have been framed as optimal flow problems on graphs.</p>
<p>Program Synthesis with Transformers: Existing works show that transformers already capture some knowledge from pre-training for algorithm emulation (Talmor et al., 2019) or can be fine-tuned for tasks like semantic parsing (He and Choi, 2020), translation (Wang et al., 2019), symbolic integration (Lample and Charton, 2020) and mathematics (Saxton et al., 2019). In our work, we also employ a transformer-based pre-trained language model (RoBERTa (Liu et al., 2019b)) but for the downstream task of rule-based reasoning.</p>
<h2>3 Method</h2>
<p>Each input to PROVER is a context $C$ (consisting of facts $F$ and rules $R$ ) and a question $Q$, about the context. PROVER predicts the answer $A \in$
{True, False $}$ and generates a proof $\mathcal{P}$.</p>
<h3>3.1 Proof Representation</h3>
<p>A proof, $\mathcal{P}=(\mathcal{N}, \mathcal{E})$, is a directed graph with nodes $n \in \mathcal{N}$ and edges $e \in \mathcal{E}$. Each node is either a fact $f \in F$, a rule $r \in R$ or a special $N A F$ node (Negation As Failure, as described below). Edges in the proof are directed either from a fact (or $N A F$ ) to a rule or from a rule to another rule. These indicate that a fact (or $N A F$ ) is consumed by a rule, or the output of a rule (a new fact) is consumed by another rule, respectively. We use these constraints both during PROVER's training and inference, as described later in the paper. Formally, we have:</p>
<p>$$
\begin{gathered}
\mathcal{P}=(\mathcal{N}, \mathcal{E}) \
\mathcal{N} \subseteq R \cup F \cup N A F \
\mathcal{E} \subseteq \mathcal{N} \times \mathcal{N}
\end{gathered}
$$</p>
<p>Figure 2 shows examples of two contexts (consisting of facts and rules), five questions about the contexts, along with their answers and proofs. Each proof has a depth ( $Q_{1}$ 's proof has a depth of 1 ). The maximum proof depth in all the datasets considered in this work (Clark et al., 2020) is 5. Proofs in the datasets are of three types:</p>
<p>Successful proof with NAF: The proof of $Q_{1}$ in Figure 2 is one such such example. $F_{2}$ acts on $R_{4}$ to prove that "The wire is conducting." and hence the answer is false.</p>
<p>Successful proof with NAF: Given a statement $s, N A F$ in logic programming is a non-monotonic inference rule used to derive "not $s$ " (negation of the statement) from failure to derive $s$. Hence, a proof may contain $N A F$ node(s), representing the truthfulness of the negation of statement(s) that cannot be proved using the set of rules. Consider the proofs for $Q_{4}$ and $Q_{5}$ where the $N A F$ node in $Q_{4}$ represents "The bald eagle is not kind.".</p>
<p>Failed proof: This happens when a statement cannot be derived using the given rule-base and the shallowest branch of the proof tree that fails is shown. $Q_{3}$ 's proof in Figure 2 is an example as "The radio is playing." cannot be proved.</p>
<p>Note that a proof can have edges between two rules in both directions. E.g., consider the edges $R_{4} \rightarrow R_{5}$ and $R_{5} \rightarrow R_{4}$ in $Q_{5}$ 's proof in Figure 2. A node can have more than two incoming edges the node $R_{6}$ in $Q_{2}$ has three incoming edges from $R_{1}, R_{3}$, and $R_{4}$.</p>
<h3>3.2 Task Description</h3>
<p>Each training example is a tuple $\left(C_{i}::=\right.$ $\left.\left{F_{i}, R_{i}\right}, Q_{i}, A_{i}, \mathcal{P}_{i}\right)$ consisting of a context (set of rules and facts), a question, the corresponding answer, and a proof. Generating a proof graph requires (1) identifying the nodes (set of relevant facts, $N A F$ and rules) that are part of the proof, (2) identifying the edges connecting these nodes, and (3) verifying a set of global constraints such as proof connectivity that ensure a valid proof.</p>
<p>For the first, we predict a binary label over each rule, fact and $N A F$ denoting their presence or absence in the proof. For the second, we also predict binary labels denoting the presence or absence of each edge. For the third, we enforce constraints during both training and inference (Section 3.4). During training, we mask out the edge labels ${ }^{2}$ corresponding to (1) self-loops, (2) edges between absent nodes, and (3) edges between facts to facts and rules to facts. This enforces a semantic constraint that the set of candidate edges in the ensuing proof is consistent with the chosen set of nodes, and also simplifies the learning problem, since a smaller number of edges need to be labeled.</p>
<h3>3.3 PROVER: Joint QA and Proof Generation Model</h3>
<p>Figure 3 shows the architecture of PROVER, built on top of RoBERTa (Liu et al., 2019b). Our model</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>consists of three modules: (1) QA module, (2) Node module, and (3) Edge module. The QA module is exactly the same as the RuleTakers model (Clark et al., 2020), thus allowing us to directly evaluate the effectiveness of our node and edge modules. The input to RoBERTa is the concatenation of the context and the question, separated by the $[S E P]$ tokens. The context is represented by concatenating the text consisting of facts and rules. Formally, if the rules and facts are denoted by $\left{R F_{i}\right}_{i=1}^{k}$ and the question by $Q$, the input is</p>
<p>$$
[C L S]\left{R F_{i}\right}_{i=1}^{k}[S E P][S E P] Q[S E P]
$$</p>
<p>QA Module: The output of RoBERTa contains an embedding for each token in the context and a global embedding corresponding to the $[C L S]$ token. The QA classification head $H_{Q A}$ is a sequence of two linear layers with dropout probability of $p$. Formally, if $t_{[C L S]}$ denotes the $[C L S]$ token embedding, we obtain the class-wise probability values $P_{Q A}$ using the softmax function $\sigma$.</p>
<p>$$
P_{Q A}=\sigma\left(H_{Q A}\left(t_{[C L S]}\right)\right)
$$</p>
<p>Node Module: Let $\left{w_{j}^{(i)}\right}<em i="i">{j=1}^{m}$ denotes the $m$ tokens corresponding to $R F</em>\right}}$. Assuming the corresponding RoBERTa embeddings are denoted by $\left{t_{w_{j}^{(i)}<em F__i="F_{i" R="R">{j=1}^{m}$, we learn a representation $t</em>$, by performing a mean pooling $M P$ of the constituent token embeddings.}}$ for each $R F_{i</p>
<p>$$
t_{R F_{i}}=M P\left(\left{t_{w_{j}^{(i)}}\right}_{j=1}^{m}\right)
$$</p>
<p>We also learn a representation of the $N A F$ node $t_{N A F}$ as a linear transformation on $t_{[C L S]}$. Note that due to the self-attention layers of RoBERTa, $t_{[C L S]}$ summarizes the set of all derivable facts given the context and the question. We want the $N A F$ node to encode information about all facts containing negation (e.g.,"The bald eagle is not kind" in $Q_{4}$ 's proof of Figure 2) in the context. These are taken as true as their positive counterparts ("The bald eagle is kind") are non-derivable given the context. Thus, if a statement $s$ cannot be derived from the facts and the rules in a context, the $N A F$ node should infer that "not $s$ " is true. We model this notion of the negation of all unprovable statements (given a context) by learning $N A F$ as a function of everything provable in the context, encoded by the $t_{[C L S]}$ embedding. ${ }^{3}$</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Architecture diagram of PROVER. The presence and absence of nodes/edges are labeled by 1 and 0 respectively while -100 represents masked out edges.</p>
<p>The node classifier $H_{N o d e}$ has a similar architecture to the QA classifier and predicts a presence and absence probability score for each node.</p>
<p>$$
P_{N o d e}=\sigma\left(H_{N o d e}\left(\left{t_{R F_{i}}\right}<em A="A" F="F" N="N">{i=1}^{k}, t</em>\right)\right)
$$</p>
<p>Edge Module: Now, given the representations of each fact, rule and $N A F$, we learn a representation for each edge between these. Formally, we define the edge embedding $t_{\left(R F_{i}, R F_{j}\right)}$ from node $R F_{i}$ to node $R F_{j}$ by concatenating their individual embeddings $t_{R F_{i}}$ and $t_{R F_{j}}$ with their element-wise difference (which gives the directionality vector).</p>
<p>$$
t_{\left(R F_{i}, R F_{j}\right)}=\left[t_{R F_{i}}, t_{R F_{j}},\left(t_{R F_{j}}-t_{R F_{i}}\right)\right]
$$</p>
<p>The above formulation also helps learn separate representations for edges $R F_{i} \rightarrow R F_{j}$ and $R F_{j} \rightarrow$ $R F_{i}$. This is essential for our task as a proof can have edges between two rules in both directions. In Section 4.7, we see that this formulation leads to a near perfect empirical performance in predicting the directionality of edges. The edge classifier $H_{E d g e}$ outputs probability scores representing the presence and absence of each edge.</p>
<p>$$
\begin{array}{r}
P_{E d g e}=\sigma\left(H_{E d g e}\left(\left{t_{\left(R F_{i}, R F_{j}\right)}\right}<em F__k_1="F_{k+1" R="R">{i, j=1}^{k+1}\right)\right) \
t</em>
\end{array}
$$}}=t_{N A F</p>
<p>We train our model by using binary cross-entropy loss for each of the three modules. Formally, if</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>$L_{Q A}, L_{N o d e}$ and $L_{E d g e}$ denote the three losses, the overall loss $L$ is given by:</p>
<p>$$
L=L_{Q A}+L_{N o d e}+L_{E d g e}
$$</p>
<h3>3.4 ILP Inference for Global Constraints</h3>
<p>As mentioned previously, during inference, we enforce additional constraints on the structure of the predicted proof graph. For this, we frame inference as Integer Linear Program (ILP) optimization, which we describe next. We follow the generative process of a graph wherein the nodes are defined first, followed by the edges on that set of nodes. Thus, we fix the nodes first based on the predictions by the node module of our model and maximize a global score over the set of edges only. This reduces the large search space and ensures that all constraints can be expressed as linear expressions.
Proof Connectivity Formulation: An important constraint is to ensure that the predicted proof graphs are connected. ${ }^{4}$ To check if a proof graph $\mathcal{P}$ is connected, we define an augmented graph $\mathcal{P}<em _aug="{aug" _text="\text">{\text {aug }}=\left(\mathcal{N}</em>$ to the sink.}}, \mathcal{E}_{\text {aug }}\right)$ with two added nodes "source" and "sink". We add an edge from the source to any one of the nodes $x$ in $\mathcal{P}$. We also define edges from all nodes in $\mathcal{P</p>
<p>$$
\mathcal{N}_{\text {aug }}=\mathcal{N} \cup{\text { source }, \text { sink }}
$$</p>
<p>$\mathcal{E}<em _aug="{aug" _text="\text">{\text {aug }}=\mathcal{E} \cup{($ source, $x)} \cup{(n, \operatorname{sink}) \forall n \in \mathcal{N}}$
Having defined $\mathcal{P}</em>$ to a maximum flow problem}}$, we can reduce the graph connectivity in $\mathcal{P</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>(Leighton and Rao, 1999) in $\mathcal{P}<em _m_="(m," n_="n)">{\text {aug }}$ (Even and Tarjan, 1975). For this, we define the capacity variable $c</em>$, as follows.}$ for each edge, $m \rightarrow n$ in $P_{\text {aug }</p>
<p>$$
\begin{gathered}
c_{(\text {source }, x)}=|\mathcal{N}| \text { and } c_{(x, \text { source })}=0 \
\forall n \in \mathcal{N}, c_{(n, \text { sink })}=1 \text { and } c_{(\text {sink }, n)}=0 \
\forall m, n \in \mathcal{N}, c_{(m, n)}=|\mathcal{N}| \
c_{(m, n)}=0 \text { if } m \notin \mathcal{N} \text { or } n \notin \mathcal{N}
\end{gathered}
$$</p>
<p>Now, there can be a maximum total flow of $|\mathcal{N}|$ from the source to the sink, if and only if the graph is connected. We use this flow formulation to provide additional constraints for our ILP inference procedure that ensure connectivity of proof graphs.</p>
<p>Final Optimization Problem: Our maximization objective, subject to the connectivity constraint and all other constraints (that ensure a valid proof) is as follows. Let $\phi_{(m, n)}$ represent the probability that an edge $m \rightarrow n$ is present, as predicted by PROVER. We want to infer $0 / 1$ assignments for our optimization variables $e_{(m, n)}$ (a value of 1 means the edge is part of the proof, while 0 means it is not) such that the following objective is maximized:</p>
<p>$$
\begin{gathered}
\underset{e_{(m, n)}, f_{(m, n)}}{\operatorname{argmax}} \sum_{m, n, m \neq n}\left(\phi_{(m, n)} e_{(m, n)}+\right. \
\left.\left(1-\phi_{(m, n)}\right)\left(1-e_{(m, n)}\right)\right)
\end{gathered}
$$</p>
<p>subject to constraints:</p>
<p>$$
\begin{gathered}
\forall m, n \in F \cup R \cup N A F, e_{(m, n)} \in{0,1} \
e_{(m, n)}=0, \text { if } m \notin \mathcal{N} \text { or } n \notin \mathcal{N} \
e_{(m, n)}=0, \text { if } m \in F \text { and } n \in F \
e_{(m, n)}=0, \text { if } m \in R \text { and } n \in F \
\forall m, n \in \mathcal{N}<em _m_="(m," n_="n)">{\text {aug }}, 0 \leq f</em> \
\forall n \in \mathcal{N}} \leq c_{(m, n)<em _in="\in" _mathcal_E="\mathcal{E" m:_m_="m:(m," n_="n)">{\text {aug }}, \sum</em><em _m_="(m," n_="n)">{\text {aug }}} f</em> \
=\sum_{o:(n, o) \in \mathcal{E}<em _n_="(n," o_="o)">{\text {aug }}} f</em> \
f_{(\text {source }, x)}=|\mathcal{N}| \
\forall m, n \in \mathcal{N}<em _m_="(m," n_="n)">{\text {aug }}, e</em> \
-\left(f_{(m, n)} /|\mathcal{N}|\right) \geq 0
\end{gathered}
$$}+e_{(n, m)</p>
<p>Note that $\mathcal{N}, F$, and $R$ refer to the set of predicted nodes (from the model), the set of facts, and the set of rules, respectively. Equations 2, 3 and 4 ensure that edges are present only when the corresponding nodes are present and that there are no edges between two facts and from a rule to a
fact. Next, to ensure proof connectivity, we first define the flow constraints in Equations 5 and 6 constrained by the flow variables $f_{(m, n)}$ for each edge $m \rightarrow n$. These maintain the capacity constraints (the flow at each edge should be less than its capacity) and the flow conservation constraints (the total flow through the incoming edges at a node is equal to the total flow through the outgoing edges). Equation 7 ensures connectivity in the proof graph, by enforcing the total flow to be $|\mathcal{N}|$. Finally, we ensure that the proof connectivity is checked on the valid edges only (which are part of the proof) through the last constraint, since a max-flow of $|\mathcal{N}|$ is achievable for any connected graph.</p>
<h2>4 Experiments</h2>
<p>Our experiments evaluate the effectiveness of PROVER (PR), our joint QA, and proof model against RuleTakers (RT). Details of our experimental setup are in the appendix.</p>
<h3>4.1 Datasets and Evaluation Metrics</h3>
<p>We conduct experiments on all the three sets of datasets introduced in Clark et al. (2020) and consisting of gold answers and proofs. Further details of the datasets can be found in the appendix.
DU0-DU5: The first set consists of five datasets, each containing 100 k questions with theories in synthetic language and requiring reasoning paths up to depth $D(D=0,1,2,3,5)$. We refer to these datasets as DU0, DU1, DU2, DU3 and DU5, where DU stands for "Depth Upto".
Birds-Electricity: It consists of two test-only datasets of 5 k samples used to evaluate the out-of-distribution performance of the models.
ParaRules: ParaRules consists of 40k questions against 2 k theories expressed in paraphrased natural language, obtained through crowdsourcing.</p>
<p>We evaluate QA performance through accuracy. For proofs, we introduce three metrics: (1) Node Accuracy (NA): Fraction of examples where the predicted node set matches exactly with the gold node set, (2) Edge Accuracy (EA): Fraction of examples where the predicted edge set match exactly with the gold set, and (3) Proof Accuracy (PA): Fraction of examples where the generated proof matches exactly with the gold proof. For examples with multiple gold proofs, we give credit if the prediction matches exactly with any one of the proofs. We also evaluate Full Accuracy (FA), denoting the fraction of samples where both the answer and the</p>
<table>
<thead>
<tr>
<th>D</th>
<th>Cnt</th>
<th>QA</th>
<th></th>
<th>NA</th>
<th>EA</th>
<th>PA</th>
<th>FA</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>RT</td>
<td>PR</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>0</td>
<td>6299</td>
<td>$\mathbf{1 0 0}$</td>
<td>$\mathbf{1 0 0}$</td>
<td>98.6</td>
<td>98.5</td>
<td>98.4</td>
<td>98.4</td>
</tr>
<tr>
<td>1</td>
<td>4434</td>
<td>98.4</td>
<td>$\mathbf{9 9 . 0}$</td>
<td>93.3</td>
<td>95.1</td>
<td>93.2</td>
<td>93.1</td>
</tr>
<tr>
<td>2</td>
<td>2915</td>
<td>98.4</td>
<td>$\mathbf{9 8 . 8}$</td>
<td>85.9</td>
<td>84.8</td>
<td>84.8</td>
<td>84.8</td>
</tr>
<tr>
<td>3</td>
<td>2396</td>
<td>98.8</td>
<td>$\mathbf{9 9 . 1}$</td>
<td>82.3</td>
<td>80.5</td>
<td>80.5</td>
<td>80.5</td>
</tr>
<tr>
<td>4</td>
<td>2134</td>
<td>$\mathbf{9 9 . 2}$</td>
<td>98.8</td>
<td>77.7</td>
<td>72.5</td>
<td>72.5</td>
<td>72.4</td>
</tr>
<tr>
<td>5</td>
<td>2003</td>
<td>$\mathbf{9 9 . 8}$</td>
<td>99.3</td>
<td>76.0</td>
<td>65.1</td>
<td>65.1</td>
<td>65.1</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>All</td>
<td>20192</td>
<td>99.2</td>
<td>$\mathbf{9 9 . 3}$</td>
<td>89.2</td>
<td>87.5</td>
<td>87.1</td>
<td>87.1</td>
</tr>
</tbody>
</table>
<p>Table 1: QA comparison between RT and PR for varying depths along with node, edge, proof and full accuracy for PRoVER on DU5. Cnt $=$ Sample Count. proof are exactly correct.</p>
<h3>4.2 QA and Proof Results for Varying Depths</h3>
<p>We first train and evaluate PRoVER on the train and test splits of the DU5 dataset, and compare its QA performance with RuleTakers for questions of varying depths (D). Table 1 shows these results and the proof-related metrics for PRoVER. The corresponding validation set results can be found in the appendix. Overall, and at each depth, PRoVER matches the QA performance of RuleTakers. PRoVER is also able to generate exact proofs fairly accurately at $87 \%$. Perhaps unsurprisingly, we find that edge prediction is a harder task than node prediction, and performance worsens with increasing depth due to an increasingly large number of edges to be labeled. The proof accuracy matches the edge accuracy at each depth, suggesting that proofs are almost always correct if the edges are correct. Similarly, the full accuracy matches the proof accuracy, showing that the predicted answer is almost always correct when the corresponding proof is correct. This points to an interesting observation - QA is easier than node prediction, which in turn is easier than edge prediction. All the datasets experimented with exhibit this behavior, as we also describe later. Proof generation becomes harder with increasing depth (and hence, more nodes and edges), as the exact proof generation accuracy drops to $65 \%$ for depth 5 . On analyzing further, we find that on average, PRoVER correctly predicts 6 out of 7 edges present in a depth 5 proof. Overall, PRoVER is interpretable yet efficient, as it generates proofs fairly accurately without any loss in QA performance.</p>
<h3>4.3 Zero-Shot Evaluation</h3>
<p>Following previous work (Clark et al., 2020), we now test the out-of-distribution performance of</p>
<p>|  | Cnt | QA |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---</p>
<table>
<thead>
<tr>
<th style="text-align: center;">D</th>
<th style="text-align: center;">Cnt</th>
<th style="text-align: center;">QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">NA</th>
<th style="text-align: center;">EA</th>
<th style="text-align: center;">PA</th>
<th style="text-align: center;">FA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">RT</td>
<td style="text-align: center;">PR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2968</td>
<td style="text-align: center;">$\mathbf{9 9 . 8}$</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.4</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2406</td>
<td style="text-align: center;">$\mathbf{9 9 . 3}$</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">97.3</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1443</td>
<td style="text-align: center;">$\mathbf{9 8 . 2}$</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">88.7</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1036</td>
<td style="text-align: center;">$\mathbf{9 6 . 7}$</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">89.9</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">142</td>
<td style="text-align: center;">$\mathbf{9 0 . 1}$</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">76.1</td>
</tr>
<tr>
<td style="text-align: center;">All</td>
<td style="text-align: center;">8008</td>
<td style="text-align: center;">$\mathbf{9 8 . 8}$</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">95.1</td>
</tr>
</tbody>
</table>
<p>Table 3: Comparison of models trained on DU3 and ParaRules training sets and tested on ParaRules test set.
and $6 \%$, respectively. On DU3, both models show high and comparable performance. PRoVER's superior generalization ability can be attributed to the extra training supervision incorporated in the form of proofs and an inductive bias for making proof-based predictions. While proof construction for supervised training is expensive, PRoVER's superior QA results on out-of-distribution data (Table 2) and higher depth questions is a potential first step to showing that limited proof supervision can still lead to effective generalization.</p>
<h3>4.5 Varying Training Data Size</h3>
<p>We explore varying the amount of training data from 10 k to 30 k to all the examples ( 70 k ) in DU5. As shown in Table 4, when trained with only $40 \%$ of the data, PRoVER obtains a near-perfect QA accuracy of $97.8 \%$. Thus, for QA, PRoVER's joint training with proofs can compensate for the lack of training data. Proof generation, however, is much harder and with increased training data, the rate of increase in proof accuracy is much more gradual.</p>
<h3>4.6 Evaluation on Complex Language</h3>
<p>We also test PRoVER's ability to generate proofs for more human-like natural language theories. More details on the ParaRules dataset can be found in the appendix. Following Clark et al. (2020), we train a model by combining the DU3 and ParaRules training partitions and test on the ParaRules test partition. Table 3 again shows that PRoVER matches the QA performance of RuleTakers, and also generates proofs with a high accuracy of $95 \%$. Following previous trends, the proof accuracy drops as the depth increases, and QA performance is higher than for node prediction, which in turn is higher than for edge prediction.</p>
<h3>4.7 Ablation and Error Analysis</h3>
<p>Table 5 analyzes the effectiveness of the individual components of PRoVER through an ablation</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Count</th>
<th style="text-align: left;">QA</th>
<th style="text-align: left;">NA</th>
<th style="text-align: left;">EA</th>
<th style="text-align: left;">PA</th>
<th style="text-align: left;">FA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">10 k</td>
<td style="text-align: left;">87.1</td>
<td style="text-align: left;">48.1</td>
<td style="text-align: left;">44.7</td>
<td style="text-align: left;">44.0</td>
<td style="text-align: left;">42.7</td>
</tr>
<tr>
<td style="text-align: left;">30 k</td>
<td style="text-align: left;">97.8</td>
<td style="text-align: left;">77.9</td>
<td style="text-align: left;">73.2</td>
<td style="text-align: left;">72.5</td>
<td style="text-align: left;">72.4</td>
</tr>
<tr>
<td style="text-align: left;">70 k</td>
<td style="text-align: left;">99.3</td>
<td style="text-align: left;">89.2</td>
<td style="text-align: left;">87.5</td>
<td style="text-align: left;">87.1</td>
<td style="text-align: left;">87.1</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of PRoVER models trained with varying amount of training data on DU5. Count $=$ Number of training examples.
study. These ablated variants also provide natural baselines for our proof-related results. Specifically, we train and test the following models on DU5: (1) QA+Node: We train a model consisting of only the QA and Node modules; (2) No NAF: We train a model using random NAF embeddings; (3) Unconstrained Train (UT) + No ILP: We remove constraints both during training and inference; (4) Unconstrained Train (UT) + ILP: We remove constraints only during training; (5) No Connectivity: Finally, we train a model where we only remove the connectivity constraint during inference. More details about these models in appendix.</p>
<p>The QA accuracy is mostly unaffected in all our models and all but "No NAF" have similar node accuracy. The "No NAF" model does not learn a representation for NAF, leading to 5-6\% drop in both node and edge accuracy. The 5-6\% drop in edge and proof accuracy for the "Unconstrained Train + No ILP" model, compared to PRoVER, shows that removing constraints results in a harder learning problem and the model fails to automatically learn all the constraints. The proof accuracy improves slightly when we add constraints only during inference ("Unconstrained Train + ILP"). The connectivity constraint provides only marginal improvement as our model mostly predicts connected proofs without any explicit supervision. Specifically, only 57 examples have disconnected proofs without this constraint. The overall PRoVER model outperforms all variants in full accuracy.</p>
<p>To better understand the loss of accuracy for higher depth proofs, we perform error analysis of PRoVER for the depth 5 subset of DU5. We find that our NAF learning module is highly accurate PRoVER correctly predicts NAF in a proof $95 \%$ of the time. Among all examples with incorrectly predicted node sets, $42 \%$ are such that the predicted set is a subset of the gold set while for $25 \%$ examples, it is a superset, demonstrating that our model tends to underestimate the number of essential rules and facts. PRoVER almost perfectly identifies the direction of edges. We find only 1 example where the proof is incorrect solely due to the incorrect</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">QA</th>
<th style="text-align: left;">NA</th>
<th style="text-align: center;">EA</th>
<th style="text-align: center;">PA</th>
<th style="text-align: center;">FA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">QA+N+E (PR)</td>
<td style="text-align: left;">99.3</td>
<td style="text-align: left;">89.2</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">$\mathbf{8 7 . 1}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">QA+N</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">88.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">QA (RT)</td>
<td style="text-align: left;">99.2</td>
<td style="text-align: left;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">No NAF</td>
<td style="text-align: left;">$\mathbf{9 9 . 5}$</td>
<td style="text-align: left;">83.1</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">81.7</td>
</tr>
<tr>
<td style="text-align: left;">UT + No ILP</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">$\mathbf{9 0 . 1}$</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">81.9</td>
</tr>
<tr>
<td style="text-align: left;">UT + ILP</td>
<td style="text-align: left;">99.4</td>
<td style="text-align: left;">$\mathbf{9 0 . 1}$</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">82.8</td>
</tr>
<tr>
<td style="text-align: left;">No Connectivity</td>
<td style="text-align: left;">99.3</td>
<td style="text-align: left;">89.2</td>
<td style="text-align: center;">$\mathbf{8 7 . 8}$</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">87.0</td>
</tr>
</tbody>
</table>
<p>Table 5: Ablation studies of PROVER showing the importance of each component and constraints.
identification of directionality. Further, $21 \%$ of the incorrectly predicted edges are subsets of the gold sets, while $35 \%$ are supersets.</p>
<h2>5 Discussion and Future Work</h2>
<p>Graph-based Explanations: While we have presented PROVER as a model that can emulate formal reasoning, it has further potential use as an explanation generation system. PROVER generates compositional explanations in the form of graphs and QA systems, in general, can potentially benefit from generating such graphical explanations. For example, in multi-hop QA tasks, the node module can choose all the relevant sentences in the context and the edge module can identify the flow of information between these to arrive at the answer (in the presence of task-specific constraints). Graphical explanations, in contrast to natural language ones, are more structured and can allow explicit modeling of causality (and are easier to evaluate, as opposed to free-form natural language generation). We hope that PROVER will encourage further work towards developing interpretable NLP models with structured explanations.</p>
<p>QA and Proof Consistency: Currently, PROVER predicts the answer and generates the proof by jointly optimizing the QA, node and edge modules using a shared RoBERTa model. Another modeling choice could explicitly condition the QA module on the node and edge modules so that the answer is predicted from the proof. We empirically verify the consistency between the predicted answer and the generated proof by showing that the full accuracy matches the proof accuracy. However, in scenarios where questions have open-ended answers, generating answer from a 'proof' in a consistent manner needs more exploration. PROVER's constraints like ensuring connectivity are necessary constraints for generating valid proofs for any graph-based
explanation generation system. However, other tasks may require imposing additional constraints to ensure valid explanations.PROVER's inference mechanism can be extended to incorporate these.</p>
<h2>Broader Implications in Formal Logic:</h2>
<p>PROVER's framework is not conceptually constrained to a particular logic fragment. PROVER uses the idea that applying a rule to fact(s) can produce new fact(s). All logic fragments from formal logic fit this idea and may only differ in the nature of the graphs generated. For a fact "Robin is a bird" and a rule with universal quantification "All birds can fly", PROVER's graph will have an edge from the fact to the rule to generate "Robin can fly". We experiment with datasets which already contain negations in facts. While these datasets currently do not contain disjunctions, our graphical representations of proofs allow an easy extension in such scenarios. E.g., if there is a disjunction rule "If X or Y then Z" instead of a conjunction rule "If X and Y then Z", only the shape of the graph changes. In the former, Z is proved by either an edge from X or from Y to the rule, while in the latter, both edges have to be necessarily present. Inferences over modals like "might" and disjunction rules like "If X then Y or Z" will mean that both the answer and the proof will be probabilistic. In such scenarios, PROVER's unweighted proof graphs can be extended to weighted ones to represent this probabilistic nature.</p>
<h2>6 Conclusion</h2>
<p>We introduce PROVER, an interpretable joint model that answers binary questions over natural language rule-bases and generates corresponding proofs. The proofs are generated through the node and edge modules of the model in the presence of multiple global constraints during training and ILP inference. Our model improves state-of-theart QA accuracy in the zero-shot scenario by $6 \%$ and generates proofs accurately. PROVER also generalizes much better to higher depth questions with up to $15 \%$ absolute improvement in QA performance over RuleTakers. PROVER's modeling is relatively generic, and similar proof generation methods can be explored in traditional multi-hop QA tasks. PROVER can also be a helpful aid to formal reasoners in scenarios where rules are fuzzy and creating rule-bases in a formal language is tedious or infeasible.</p>
<h2>Acknowledgements</h2>
<p>We thank the reviewers for their helpful feedback. This work was supported by DARPA MCS Grant N66001-19-2-4031, NSF-CAREER Award 1846185, DARPA YFA17-D17AP00022, ONR Grant N00014-18-1-2871, Microsoft Investigator Fellowship, and Munroe \&amp; Rebecca Cobey Fellowship. The views in this article are those of the authors and not the funding agency.</p>
<h2>References</h2>
<p>Ralph Abboud, Ismail Ilkan Ceylan, and Thomas Lukasiewicz. 2020. Learning to reason: Leveraging neural networks for approximate DNF counting. In Proceedings of the 34th AAAI Conference on Artificial Intelligence.</p>
<p>Ibrahim Abdelaziz, Veronika Thost, Maxwell Crouse, and Achille Fokoue. 2020. An experimental study of formula embeddings for automated theorem proving in first-order logic. arXiv preprint arXiv:2002.00423.</p>
<p>Mohit Bansal, Claire Cardie, and Lillian Lee. 2008. The power of negative thinking: Exploiting label disagreement in the min-cut classification framework. In COLING 2008: Companion Volume: Posters, pages $15-18$.</p>
<p>Regina Barzilay and Mirella Lapata. 2005. Collective content selection for concept-to-text generation. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 331-338. Association for Computational Linguistics.</p>
<p>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1533-1544.</p>
<p>Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14151425 .</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural language inference with natural language explanations. In Advances in Neural Information Processing Systems, pages 9539-9549.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In International Joint Conference on Artificial Intelligence.</p>
<p>Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment: Models and applications. Synthesis Lectures on Human Language Technologies, 6(4):1-220.</p>
<p>Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608.</p>
<p>Shimon Even and R Endre Tarjan. 1975. Network flow and testing graph connectivity. SIAM journal on computing, 4(4):507-518.</p>
<p>Peter Hase and Mohit Bansal. 2020. Evaluating explainable AI: Which algorithmic explanations help users predict model behavior? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Han He and Jinho Choi. 2020. Establishing strong baselines for the new decade: Sequence tagging, syntactic and semantic parsing with bert. In The Thirty-Third International Flairs Conference.</p>
<p>Shalmali Joshi, Oluwasanmi Koyejo, Been Kim, and Joydeep Ghosh. 2018. xGEMs: Generating examplars to explain black-box models. arXiv preprint arXiv:1806.08867.</p>
<p>Aishwarya Kamath and Rajarshi Das. 2019. A survey on semantic parsing. In Automated Knowledge Base Construction (AKBC).</p>
<p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. RACE: Large-scale reading comprehension dataset from examinations. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785-794.</p>
<p>Guillaume Lample and François Charton. 2020. Deep learning for symbolic mathematics. In 8th International Conference on Learning Representations, ICLR 2020. OpenReview.net.</p>
<p>Tom Leighton and Satish Rao. 1999. Multicommodity max-flow min-cut theorems and their use in designing approximation algorithms. Journal of the ACM (JACM), 46(6):787-832.</p>
<p>Kevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gardner. 2019. Reasoning over paragraph effects in situations. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 5862, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Hui Liu, Qingyu Yin, and William Yang Wang. 2019a. Towards explainable NLP: A generative explanation framework for text classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5570-5581.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b.</p>
<p>RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Bill MacCartney and Christopher D Manning. 2014. Natural logic and natural language inference. In Computing meaning, pages 129-147. Springer.</p>
<p>Mark A Musen and Johan Van Der Lei. 1988. Of brittleness and bottlenecks: Challenges in the creation of pattern-recognition and expert-system models. In Machine Intelligence and Pattern Recognition, volume 7, pages 335-352. Elsevier.</p>
<p>Arvind Neelakantan, Quoc V. Le, and Ilya Sutskever. 2016. Neural programmer: Inducing latent programs with gradient descent. In 4th International Conference on Learning Representations, ICLR 2016, Conference Track Proceedings.</p>
<p>Allen Newell and Herbert Simon. 1956. The logic theory machine-a complex information processing system. IRE Transactions on information theory, 2(3):61-79.</p>
<p>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932-4942.</p>
<p>Scott E. Reed and Nando de Freitas. 2016. Neural programmer-interpreters. In 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. "Why should I trust you?": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135-1144.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-precision modelagnostic explanations. In Thirty-Second AAAI Conference on Artificial Intelligence.</p>
<p>Kyle Richardson, Hai Hu, Lawrence S Moss, and Ashish Sabharwal. 2020. Probing natural language inference models through semantic fragments. In Thirty-Fourth AAAI Conference on Artificial Intelligence.</p>
<p>Cynthia Rudin. 2019. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1(5):206-215.</p>
<p>Pouya Samangouei, Ardavan Saeedi, Liam Nakagawa, and Nathan Silberman. 2018. ExplainGAN: Model explanation via decision boundary crossing transformations. In Proceedings of the European Conference on Computer Vision (ECCV), pages 666-681.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. 2019. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations.</p>
<p>Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, and David L. Dill. 2019. Learning a SAT solver from single-bit supervision. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.</p>
<p>Oyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. 2019. QuaRTz: An open-domain dataset of qualitative relationship questions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5941-5946, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2019. oLMpics-on what language model pre-training captures. arXiv preprint arXiv:1912.13283.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.</p>
<p>Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019. Learning deep transformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810-1822, Florence, Italy. Association for Computational Linguistics.</p>
<p>Leon Weber, Pasquale Minervini, Jannes Münchmeyer, Ulf Leser, and Tim Rocktäschel. 2019. NLProlog: Reasoning with weak unification for question answering in natural language. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6151-6161, Florence, Italy. Association for Computational Linguistics.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015. Towards AI-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence, UAI'05, page 658-666. AUAI Press.</p>
<p>Hongming Zhang, Xinran Zhao, and Yangqiu Song. 2020. WinoWhy: A deep diagnosis of essential commonsense knowledge for answering Winograd schema challenge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL).</p>
<h2>A Appendix</h2>
<h2>A. 1 Experimental Setup</h2>
<p>We build our model on top of the Hugging Face Transformers library (Wolf et al., 2019). ${ }^{5}$ All hyperparamters are chosen based on the best validation set performance (Full Accuracy) of the corresponding dataset. We use RoBERTa-large (Liu et al., 2019b) as the pre-trained Language Model and all our models are trained using a batch size of 8 and a maximum sequence length of 300 . We train the models for a maximum of 5 epochs using an initial learning rate of $10^{-5}$, with linear decay and a weight decay of 0.1 . The dropout probability is chosen to be 0.1 . The random seed used in all the experiments is 42 . Each epoch of PROVER takes 2.5 hours to run on one V100 Volta GPU. The total number of parameters of PROVER is similar to that of RoBERTa-large ( 355 M ). Batch size and learning rate are manually tuned in the range ${8,16}$ and $\left{10^{-5}, 2 * 10^{-5}\right}$ respectively. The ILP is modeled using PuLP. ${ }^{6}$ Proofs in the datasets are represented as bracketed strings, which are pre-processed into graph representations consisting of unique nodes and edges. The maximum number of facts and rules corresponding to a context is $25 .{ }^{7}$</p>
<h2>A. 2 Dataset Details</h2>
<p>Below we briefly describe the three sets of datasets we conduct experiments on. ${ }^{8}$ Each dataset has a</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>train, validation and test split, except for the zeroshot test-only one. Further details about these can be found in Clark et al. (2020).</p>
<p>DU0-DU5: The first set consists of five datasets, each containing 100k questions with theories in synthetic language and requiring reasoning paths up to depth $D(D=0,1,2,3,5)$. For example, $D=0$ means the true facts can be proved by simple lookup in the context. The samples are randomly split 70/10/20 into train/dev/test partitions such that there is no overlap of theories between the partitions.</p>
<p>Birds-Electricity: The second set consists of two test-only datasets used to evaluate robustness and out-of-distribution performance of the models. The contexts are about birds and an electric circuit, and consist of 5 k samples in total. The vocabulary of entities, attributes and predicates, apart from is () are all new at test time.</p>
<p>ParaRules: The final dataset, ParaRules consists of 40k questions against 2 k theories expressed in paraphrased natural language, obtained through crowdsourcing. While the previous datasets contain synthetic language, ParaRules tests the models' ability to reason over more human-like paraphrased language.</p>
<h2>A. 3 QA and Proof Results for Varying Depths</h2>
<p>Table 7 shows the DU5 validation set performance of PROVER trained on the training split of DU5. PROVER obtains a near perfect QA accuracy and a proof accuracy of $88 \%$. While the QA accuracy remains equally high at all depths, the proof accuracy drops with increasing depth. Full accuracy matches the proof accuracy, demonstrating consistency between the predicted answers and generated proofs.</p>
<h2>A. 4 Generalization to Higher Depths</h2>
<p>In Table 6, we provide detailed results of PROVER's generalization ability to higher depth questions. Specifically, we evaluate four models, trained on the training splits of DU0, DU1, DU2 and DU3 and tested on the validation and test splits of DU5. We have shown previously that PROVER does significantly better than RuleTakers (Clark et al., 2020) on QA generalization. The proofs, however, do not generalize that well. Note that depth 0 proofs are rather simple (consisting of a</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">QA</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">NA</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">EA</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">PA</th>
<th style="text-align: right;"></th>
<th style="text-align: right;">FA</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: right;">Dev</td>
<td style="text-align: right;">Test</td>
<td style="text-align: right;">Dev</td>
<td style="text-align: right;">Test</td>
<td style="text-align: right;">Dev</td>
<td style="text-align: right;">Test</td>
<td style="text-align: right;">Dev</td>
<td style="text-align: right;">Test</td>
<td style="text-align: right;">Dev</td>
<td style="text-align: right;">Test</td>
</tr>
<tr>
<td style="text-align: left;">DU0</td>
<td style="text-align: right;">68.3</td>
<td style="text-align: right;">68.7</td>
<td style="text-align: right;">45.3</td>
<td style="text-align: right;">46.0</td>
<td style="text-align: right;">49.3</td>
<td style="text-align: right;">49.5</td>
<td style="text-align: right;">43.8</td>
<td style="text-align: right;">44.4</td>
<td style="text-align: right;">42.3</td>
<td style="text-align: right;">42.8</td>
</tr>
<tr>
<td style="text-align: left;">DU1</td>
<td style="text-align: right;">73.2</td>
<td style="text-align: right;">73.7</td>
<td style="text-align: right;">66.4</td>
<td style="text-align: right;">66.3</td>
<td style="text-align: right;">64.5</td>
<td style="text-align: right;">64.3</td>
<td style="text-align: right;">63.9</td>
<td style="text-align: right;">63.8</td>
<td style="text-align: right;">61.8</td>
<td style="text-align: right;">61.9</td>
</tr>
<tr>
<td style="text-align: left;">DU2</td>
<td style="text-align: right;">89.3</td>
<td style="text-align: right;">89.6</td>
<td style="text-align: right;">76.6</td>
<td style="text-align: right;">76.4</td>
<td style="text-align: right;">73.1</td>
<td style="text-align: right;">73.1</td>
<td style="text-align: right;">72.6</td>
<td style="text-align: right;">72.6</td>
<td style="text-align: right;">72.3</td>
<td style="text-align: right;">72.3</td>
</tr>
<tr>
<td style="text-align: left;">DU3</td>
<td style="text-align: right;">98.3</td>
<td style="text-align: right;">98.6</td>
<td style="text-align: right;">85.5</td>
<td style="text-align: right;">85.0</td>
<td style="text-align: right;">79.9</td>
<td style="text-align: right;">79.5</td>
<td style="text-align: right;">79.4</td>
<td style="text-align: right;">79.1</td>
<td style="text-align: right;">79.4</td>
<td style="text-align: right;">79.1</td>
</tr>
</tbody>
</table>
<p>Table 6: Performance of PROVER trained on the training splits of DU0, DU1, DU2 and DU3 and tested on the validation and test splits of DU5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">D</th>
<th style="text-align: right;">Cnt</th>
<th style="text-align: right;">QA</th>
<th style="text-align: right;">NA</th>
<th style="text-align: right;">EA</th>
<th style="text-align: right;">PA</th>
<th style="text-align: right;">FA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: right;">3116</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">98.7</td>
<td style="text-align: right;">98.6</td>
<td style="text-align: right;">98.5</td>
<td style="text-align: right;">98.5</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: right;">2304</td>
<td style="text-align: right;">98.8</td>
<td style="text-align: right;">92.5</td>
<td style="text-align: right;">94.9</td>
<td style="text-align: right;">92.2</td>
<td style="text-align: right;">92.2</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: right;">1436</td>
<td style="text-align: right;">99.2</td>
<td style="text-align: right;">86.1</td>
<td style="text-align: right;">85.6</td>
<td style="text-align: right;">85.6</td>
<td style="text-align: right;">85.6</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: right;">1165</td>
<td style="text-align: right;">98.7</td>
<td style="text-align: right;">85.1</td>
<td style="text-align: right;">82.8</td>
<td style="text-align: right;">82.8</td>
<td style="text-align: right;">82.8</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: right;">1041</td>
<td style="text-align: right;">98.8</td>
<td style="text-align: right;">81.2</td>
<td style="text-align: right;">76.9</td>
<td style="text-align: right;">76.9</td>
<td style="text-align: right;">76.9</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: right;">990</td>
<td style="text-align: right;">99.3</td>
<td style="text-align: right;">78.3</td>
<td style="text-align: right;">67.4</td>
<td style="text-align: right;">67.4</td>
<td style="text-align: right;">67.4</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: right;">10068</td>
<td style="text-align: right;">99.3</td>
<td style="text-align: right;">90.0</td>
<td style="text-align: right;">88.6</td>
<td style="text-align: right;">88.0</td>
<td style="text-align: right;">88.0</td>
</tr>
</tbody>
</table>
<p>Table 7: Performance of PROVER trained on the training split of DU5 and tested on the validation split of DU5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">D</th>
<th style="text-align: right;">Cnt</th>
<th style="text-align: right;">QA</th>
<th style="text-align: right;">NA</th>
<th style="text-align: right;">EA</th>
<th style="text-align: right;">PA</th>
<th style="text-align: right;">FA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: right;">1485</td>
<td style="text-align: right;">99.9</td>
<td style="text-align: right;">99.6</td>
<td style="text-align: right;">99.7</td>
<td style="text-align: right;">99.5</td>
<td style="text-align: right;">99.5</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: right;">1180</td>
<td style="text-align: right;">99.7</td>
<td style="text-align: right;">99.3</td>
<td style="text-align: right;">99.5</td>
<td style="text-align: right;">99.3</td>
<td style="text-align: right;">99.3</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: right;">727</td>
<td style="text-align: right;">99.4</td>
<td style="text-align: right;">91.5</td>
<td style="text-align: right;">91.5</td>
<td style="text-align: right;">91.5</td>
<td style="text-align: right;">91.3</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: right;">524</td>
<td style="text-align: right;">98.5</td>
<td style="text-align: right;">92.0</td>
<td style="text-align: right;">90.3</td>
<td style="text-align: right;">90.3</td>
<td style="text-align: right;">90.3</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: right;">81</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">87.6</td>
<td style="text-align: right;">72.8</td>
<td style="text-align: right;">72.8</td>
<td style="text-align: right;">72.8</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: right;">7</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: right;">4004</td>
<td style="text-align: right;">99.6</td>
<td style="text-align: right;">96.8</td>
<td style="text-align: right;">96.2</td>
<td style="text-align: right;">96.1</td>
<td style="text-align: right;">96.0</td>
</tr>
</tbody>
</table>
<p>Table 8: PROVER results on the ParaRules validation set after training on DU3+ParaRules training splits.
single fact) and a model trained on only such proofs, unsuprisingly, fails to generate proofs for higher depth questions. However, the proof results start improving as the model gets trained on more complex proofs and reaches an accuracy of $79 \%$, when trained on DU3 questions.</p>
<h2>A. 5 Evaluation on Complex Language</h2>
<p>In Table 8, we report the ParaRules validation set results of PROVER trained on the combination of DU3 and ParaRules training splits (following previous work (Clark et al., 2020)). ParaRules is created by first separating the fact groups (a fact group is the set of all facts in the theory concerning a particular person) and the rules from a theory and then asking crowdworkers to paraphrase these in their own words. For example, a fact group "Alan is blue. Alan is rough. Alan is young.", may be reworded into "Alan is on the young side, but rough.</p>
<p>He often feels rather blue.". Thus, unlike the previous datasets where the proof graphs are composed of facts and rules, ParaRules proofs are composed of fact groups and rules. ${ }^{9}$ PROVER obtains high QA and proof accuracy on complex humanparapharsed rule-bases, showing good generalization on such language. However, the proof accuracy again drops as the depth of the questions increases.</p>
<h2>A. 6 Ablation Models and Simpler Baselines</h2>
<p>We provide brief descriptions of our ablation models. These are (1) QA+Node: We model PROVER consisting of only the QA and Node modules. Since there is no edge module, this model does not require any constrained training or inference; (2) No NAF: We train a model using random NAF embeddings with no learning. This helps us understand the effectiveness of our NAF learning; (3) Unconstrained Train + No ILP: Through this model, we study the effectiveness of our global constraints. Specifically, no edges are masked for training and during inference, the edge labels are predicted based on the model's probability scores only; (4) Unconstrained Train + ILP: Here the constraints are employed only during inference. Note that the reverse configuration, constrained training without ILP inference, is not included as the edge logits for the masked out labels would be random (since they are not learned). (5) No Connectivity: Finally, we train a model where we only remove the connectivity constraint during ILP optimization, keeping everything else same.</p>
<p>We also experiment with simpler baselines for edge prediction like training a Random Forest with lexical features (BLEU scores, length difference,</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>word overlap, etc.) and this obtains a much lower edge accuracy of $47 \%$. This fails primarily because (1) proof graphs can contain NAF which account for $9 \%$ of the data and edges from it cannot be learned without learning a latent representation; (2) overlap features are mostly symmetric and hence are not enough for learning directionality; (3) there is lack of overall context information.</p>
<h1>A. 7 Critical Sentence Identification</h1>
<p>Clark et al. (2020) provide an initial solution towards generating explanations for the predicted answers by using a post-hoc method - they remove each fact or rule from the theory and check if the predicted answer changes with the new theory. They define all such rules and facts which flip the answer as critical sentences. If an example has multiple gold proofs, a critical sentence is one which is present in all the proofs. We argue that this leave-one-out analysis is not ideal for multiple reasons - (1) This does not work if the theory has negations, (2) This only predicts the presence or absence of rules and facts, and does not look at the entire chain of reasoning, which our model achieves through the edge module. In our final ex-</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;">Precision</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">RuleTakers</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">$\mathbf{9 8 . 7}$</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">92.4</td>
</tr>
<tr>
<td style="text-align: left;">PROVER</td>
<td style="text-align: center;">$\mathbf{7 8 . 1}$</td>
<td style="text-align: center;">$\mathbf{9 8 . 7}$</td>
<td style="text-align: center;">$\mathbf{8 7 . 2}$</td>
<td style="text-align: center;">$\mathbf{9 2 . 6}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Comparison of critical sentence identification on the No Negation subset of DU5 test set.
periment, we still apply the leave-one-out-strategy on the no-negation subset of the DU5 test set for a direct comparison with RuleTakers. As shown in Table 9, our model identifies the exact critical sentences in an example in $78 \%$ of the cases, a $4 \%$ improvement over RuleTakers.</p>
<h2>A. 8 Proofs Generated by PROVER</h2>
<p>In Figure 5, we show two rule-bases, one about electric circuits and another about birds from the Birds-Electricity dataset. PROVER not only answers the questions correctly but also generates the proofs accurately. These proofs are complex because of the presence of NAF and also the long chains of reasoning needed in the inference process. Figure 6 shows three more accurate proofs generated by PROVER for three questions from the DU5 datset.</p>
<h2>Facts :</h2>
<p>$\mathbf{F}<em 2="2">{1}$ : The circuit has the battery. $\mathbf{F}</em>$ : The circuit has the bell.}$ : The switch is on. $\mathbf{F}_{3</p>
<h2>Rules :</h2>
<p>$\mathbf{R}<em 2="2">{1}$ : If the circuit has the battery then the circuit is powered.
$\mathbf{R}</em>$ : If the circuit does not have the battery then the circuit is dead.
$\mathbf{R}<em 4="4">{3}$ : If the circuit is dead then the bell is not ringing.
$\mathbf{R}</em>$ : If the circuit is dead then the radio is not playing.
$\mathbf{R}<em 1="1">{5}$ : If the circuit is dead then the light bulb is not glowing.
$\mathbf{Q}</em>$ : The current runs through the circuit. [ Answer : T]
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<h2>Facts :</h2>
<p>$\mathbf{F}<em 2="2">{1}$ : Arthur is a bird. $\mathbf{F}</em>$ : Arthur is not wounded.
$\mathbf{F}<em 4="4">{3}$ : Bill is an ostrich. $\mathbf{F}</em>$ : Colin is a bird.
$\mathbf{F}<em 6="6">{5}$ : Colin is wounded. $\mathbf{F}</em>$ : Dave is not an ostrich.
$\mathbf{F}<em 3="3">{7}$ : Dave is wounded.
<img alt="img-4.jpeg" src="img-4.jpeg" />
$\mathbf{Q}</em>$ : Colin is not abnormal. [ Answer : F]</p>
<h2>Rules :</h2>
<p>$\mathbf{R}<em 7="7">{6}$ : If the circuit has the switch and the switch is on then the circuit is complete.
$\mathbf{R}</em>$ : If the circuit does not have the switch then the circuit is complete.
$\mathbf{R}<em 9="9">{8}$ : If the circuit is powered and the circuit is complete then the current runs through the circuit.
$\mathbf{R}</em>$ : If the current runs through the circuit and the circuit has the light bulb then the light bulb is glowing.
$\mathbf{R}<em 11="11">{10}$ : If the current runs through the circuit and the circuit has the bell then the bell is ringing.
$\mathbf{R}</em>$ : If the current runs through the circuit and the circuit has the radio then the radio is playing.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<h2>Rules :</h2>
<p>$\mathbf{R}<em 2="2">{1}$ : If someone is an ostrich then they are a bird.
$\mathbf{R}</em>$ : If someone is an ostrich then they are abnormal.
$\mathbf{R}<em 4="4">{3}$ : If someone is an ostrich then they cannot fly.
$\mathbf{R}</em>$ : If someone is a bird and wounded then they are abnormal.
$\mathbf{R}<em 6="6">{5}$ : If someone is wounded then they cannot fly.
$\mathbf{R}</em>$ : If someone is a bird and not abnormal then they can fly.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 5: Examples of proofs generated by PROVER for four questions on two rule-bases about electric circuits and birds from the Birds-Electricity dataset. PROVER not only answers the questions correctly but also accurately predicts the long reasoning chains with multiple branches.</p>
<h2>Facts :</h2>
<p>$\mathbf{F}<em 2="2">{1}$ : The bear visits the tiger. $\mathbf{F}</em>$ : The cat is kind.
$\mathbf{F}<em 4="4">{3}$ : The mouse is green. $\mathbf{F}</em>$ : The mouse is kind.
$\mathbf{F}<em 6="6">{5}$ : The mouse sees the tiger. $\mathbf{F}</em>$ : The tiger is rough.
$\mathbf{F}_{7}$ : The tiger visits the cat.</p>
<h2>Rules :</h2>
<p>$\mathbf{R}<em 2="2">{1}$ : If something visits the bear then it sees the bear.
$\mathbf{R}</em>$ : If something sees the bear then the bear likes the cat.
$\mathbf{R}<em 4="4">{3}$ : If something visits the cat then the cat visits the bear.
$\mathbf{R}</em>$ : If something sees the bear and the bear likes the cat then it is cold.
$\mathbf{R}<em 6="6">{5}$ : Cold things are rough.
$\mathbf{R}</em>$ : If something is green and it likes the tiger then the tiger visits the mouse.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 6: Examples of proofs generated by PROVER for three questions on a rule-base from the DU5 dataset. The proof corresponding to the last question is a failed case.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ The original ParaRules dataset released by Clark et al. (2020) has proofs for the unparaphrased theories (consisting of facts and rules). Using the mapping from a fact to the corresponding fact group, we replace the fact nodes in the proof graph with the corresponding fact group nodes. Note that this is done to report proof accuracy for this dataset as well.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ Proofs are directed graphs. We check connectivity in the equivalent undirected graphs.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6236 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6236</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6236</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-4c8cc2383cec93bd9ea0758692f01b98a035215b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4c8cc2383cec93bd9ea0758692f01b98a035215b" target="_blank">UltraFeedback: Boosting Language Models with High-quality Feedback</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> This work validates the effectiveness of scaled AI feedback data in constructing strong open-source chat language models, serving as a solid foundation for future feedback learning research.</p>
                <p><strong>Paper Abstract:</strong> Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability, resulting in small sizes or limited topics of current datasets. This further hinders feedback learning as well as alignment research within the open-source community. To address this issue, we explore how to go beyond human feedback and collect high-quality \textit{AI feedback} automatically for a scalable alternative. Specifically, we identify \textbf{scale and diversity} as the key factors for feedback data to take effect. Accordingly, we first broaden instructions and responses in both amount and breadth to encompass a wider range of user-assistant interactions. Then, we meticulously apply a series of techniques to mitigate annotation biases for more reliable AI feedback. We finally present \textsc{UltraFeedback}, a large-scale, high-quality, and diversified AI feedback dataset, which contains over 1 million GPT-4 feedback for 250k user-assistant conversations from various aspects. Built upon \textsc{UltraFeedback}, we align a LLaMA-based model by best-of-$n$ sampling and reinforcement learning, demonstrating its exceptional performance on chat benchmarks. Our work validates the effectiveness of scaled AI feedback data in constructing strong open-source chat language models, serving as a solid foundation for future feedback learning research. Our data and models are available at https://github.com/thunlp/UltraFeedback.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6236.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6236.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI) used as an automatic judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was used throughout the paper as an AI annotator/judge to produce fine-grained scalar scores and textual critiques and to perform pairwise model-response comparisons; its judgments were directly compared to human annotators to quantify agreement and systematic differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-domain instruction following / dialogue and multiple benchmarks (AlpacaEval, Evol-Instruct, UltraChat, UltraFeedback)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Randomly sampled 400 comparison pairs (100 each from UltraFeedback, AlpacaEval, Evol-Instruct, UltraChat); 3 independent human annotators (undergraduate/graduate students) judged each pair with win/tie/lose options; majority vote used as a reference; a filtered subset of 266 pairs (excluding all-different votes) was also used for additional comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Agreement rate (percentage agreement between GPT-4 and each human annotator and with majority vote), win/tie/lose rates (model-level win percentages reported per benchmark), and qualitative category breakdowns (reasoning, writing, QA).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Quantitatively, GPT-4 agreed with individual annotators on average 59.7% and with the majority vote 68.6% on the 400-sample set (random agreement baseline 33%). On the 266-sample filtered set GPT-4's average win rate was 67.3% vs humans' average win 64.3% (i.e., GPT-4 awarded more wins overall). Qualitatively, GPT-4 produced fewer ties than humans, especially on reasoning tasks where it gave more losses and fewer ties compared to human annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Reported limitations include imperfect modeling of human preferences in all situations (cannot precisely replicate human judgment everywhere), tendency to produce fewer ties (lower tie rate), potential biases toward features like answer position, response length, and style, and possible underestimation of model reasoning when judged against ground-truth answers.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>On reasoning tasks (coding, math, logic), GPT-4 gave fewer ties and more losses compared to human judges; when ground-truth correctness was enforced via an expert labeler, GPT-4 appeared to underestimate the evaluated model's reasoning performance (i.e., judged correct/incorrect differently than human/expert ground truth).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Authors applied several mitigation strategies when using GPT-4: (1) decompose evaluation into fine-grained aspects (instruction-following, truthfulness, honesty, helpfulness); (2) provide objective scoring documentation and reference responses to calibrate scores; (3) ask GPT-4 to score multiple completions for the same instruction simultaneously to reduce randomness and inconsistent standards; (4) require GPT-4 to output rationales (chain-of-thought) before numeric scores; (5) use majority human votes and expert labelers for ground-truth checks on reasoning tasks; and (6) supplement GPT-4 judgments with other AI judges (e.g., Claude-3) and human evaluation for cross-checking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_context_snippets</strong></td>
                            <td>['On average, GPT-4 judge exhibits 59.7% agreement rate with human labelers ... the agreement between GPT-4 and the majority votes of three annotators raises to 68.6%.', 'Human evaluations are mostly consistent with GPT-4, giving a 64.3% against 67.3% average winning rate. We notice that human labelers tend to assign more ties than GPT-4, leading to slightly lower winning rates.', 'To reduce ambiguity ... we decompose the overall quality assessment into four fine-grained assessments ... provide GPT-4 with detailed documentation of scores ... ask GPT-4 to score four completions simultaneously ... require GPT-4 to generate a rationale on how the response should be scored.']</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UltraFeedback: Boosting Language Models with High-quality Feedback', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6236.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6236.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3 judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude-3 Sonnet (Anthropic) used as an alternate AI judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude-3 Sonnet was used as a supplementary AI judge to assess agreement trends across AI evaluators and to compare how different AI judges correlate with human majority votes; its outputs were compared with GPT-4 and human judgments on the same set of samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-domain instruction following / dialogue benchmarks (same sets used for GPT-4 comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Claude-3 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same prompt and dataset subset as GPT-4 comparison: the authors compared Claude-3 judgments on the 266 filtered pairs and contrasted them with GPT-4 and majority human votes (human majority used as reference).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Win/tie/lose rates per benchmark and average win percentage; comparison with human majority votes and GPT-4 outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Claude-3 produced higher average win rates (76.1% avg win) than GPT-4 (67.3%) and humans (64.3%) on the 266-pair set; Claude-3 tended to give far fewer ties (e.g., Evol-Instruct tie 1.4% vs GPT-4 tie 8.8% and human tie 17.6%), thereby increasing model win rates relative to human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Claude-3 showed a tendency to increase winning rates and assign very few ties, indicating possible systematic bias or stricter/stronger preference behavior relative to humans; differences across AI evaluators suggest evaluator-family-specific biases.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Extremely low tie rates (e.g., 1.4% ties on Evol-Instruct) contrast with human annotators who used ties significantly more, implying Claude-3 may over-attribute wins/losses where humans consider responses comparable.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Authors recommend cross-checking multiple AI evaluators against human judgments, using majority human votes as an anchor, and treating single-AI judgments cautiously; they use ensemble checks (GPT-4 + Claude-3) and human annotations to detect evaluator-specific biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_context_snippets</strong></td>
                            <td>['We first supplement another AI evaluation using Claude-3 Sonnet ... Claude-3 shares the same trend as GPT-4 and further increases our models’ win rates.', 'Claude-3 | ... | Avg. Win (%) 76.1  (compared to GPT-4 67.3 and Human 64.3).']</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UltraFeedback: Boosting Language Models with High-quality Feedback', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6236.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6236.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Annotation mitigation techniques</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Techniques to mitigate annotation biases when using LLMs as judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of procedural and prompt-engineering practices the authors used to improve reliability of LLM-as-judge annotations, including decomposition of aspects, scoring standards, multi-completion scoring, rationale requirements, scoring calibration with references, and data decontamination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Annotation/evaluation of model responses across dialogue, QA, reasoning and writing tasks</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Primarily GPT-4 (used to produce scalar scores and textual critiques); methods intended to generalize to other LLM judges</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Not a human-evaluation protocol per se but a hybrid pipeline: the authors compared LLM annotations with human annotators (3 judges) and used human majority votes and expert labelers to validate and calibrate the LLM-based annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Annotation consistency/agreement (agreement rates with humans), stability/reproducibility (reduced randomness across runs), and downstream reward-model performance (preference-prediction accuracy on human-annotated benchmarks).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Fine-grained decomposition and scoring multiple completions simultaneously produced better alignment with human preferences and improved downstream reward-model generalization; aggregated overall-score annotations (single-sample scoring) were less consistent and led to weaker generalization on some benchmarks (e.g., WebGPT) than fine-grained, multi-completion scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Despite mitigations, LLM judges still exhibit biases (position, length, style), can be subjective across runs, and cannot perfectly model human preferences in all situations; chain-of-thought rationales can help but do not fully eliminate inconsistency.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Position bias and ordering effects observed in some reward-model formats (cited example: SteamSHP ordering led to large differences, Table 7); single-sample overall scoring by GPT-4 led to more randomness and worse transfer on WebGPT than fine-grained multi-completion scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Concrete methods applied in this paper: (1) decompose judgments into four aspects (instruction-following, truthfulness, honesty, helpfulness); (2) provide detailed scoring documentation (1–5) and reference answers for calibration; (3) score multiple completions jointly to enable cross-reference and reduce randomness; (4) require textual rationales before numeric scores; (5) diversify sampled completions (multiple model families, sizes, and prompting principles) to avoid spurious style-quality correlations; (6) decontaminate datasets against evaluation sets; (7) supplement LLM judgments with other AI judges and human majority/expert labels for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_context_snippets</strong></td>
                            <td>['Regarding the potential subjectivity and randomness of GPT-4 annotation, we apply four techniques to improve the annotation quality: (1) Decomposition ... (2) Standard ... (3) Reference ... (4) Rationale.', 'A noteworthy finding is that ... the reward model trained with overall scores discernably lags behind the other two variants on WebGPT. ... fine-grained annotation ... provides a more precise assessment ... scoring all four completions at the same time ... may provide GPT-4 with cross-references and prevent it from applying inconsistent standards, reducing the impact of randomness.']</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UltraFeedback: Boosting Language Models with High-quality Feedback', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Alpacafarm: A simulation framework for methods that learn from human feedback <em>(Rating: 1)</em></li>
                <li>RLAIF: Scaling reinforcement learning from human feedback with ai feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6236",
    "paper_id": "paper-4c8cc2383cec93bd9ea0758692f01b98a035215b",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "GPT-4 judge",
            "name_full": "GPT-4 (OpenAI) used as an automatic judge",
            "brief_description": "GPT-4 was used throughout the paper as an AI annotator/judge to produce fine-grained scalar scores and textual critiques and to perform pairwise model-response comparisons; its judgments were directly compared to human annotators to quantify agreement and systematic differences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-domain instruction following / dialogue and multiple benchmarks (AlpacaEval, Evol-Instruct, UltraChat, UltraFeedback)",
            "llm_judge_model": "GPT-4",
            "human_evaluation_setup": "Randomly sampled 400 comparison pairs (100 each from UltraFeedback, AlpacaEval, Evol-Instruct, UltraChat); 3 independent human annotators (undergraduate/graduate students) judged each pair with win/tie/lose options; majority vote used as a reference; a filtered subset of 266 pairs (excluding all-different votes) was also used for additional comparison.",
            "metrics_compared": "Agreement rate (percentage agreement between GPT-4 and each human annotator and with majority vote), win/tie/lose rates (model-level win percentages reported per benchmark), and qualitative category breakdowns (reasoning, writing, QA).",
            "reported_differences": "Quantitatively, GPT-4 agreed with individual annotators on average 59.7% and with the majority vote 68.6% on the 400-sample set (random agreement baseline 33%). On the 266-sample filtered set GPT-4's average win rate was 67.3% vs humans' average win 64.3% (i.e., GPT-4 awarded more wins overall). Qualitatively, GPT-4 produced fewer ties than humans, especially on reasoning tasks where it gave more losses and fewer ties compared to human annotators.",
            "llm_specific_limitations": "Reported limitations include imperfect modeling of human preferences in all situations (cannot precisely replicate human judgment everywhere), tendency to produce fewer ties (lower tie rate), potential biases toward features like answer position, response length, and style, and possible underestimation of model reasoning when judged against ground-truth answers.",
            "notable_failure_cases": "On reasoning tasks (coding, math, logic), GPT-4 gave fewer ties and more losses compared to human judges; when ground-truth correctness was enforced via an expert labeler, GPT-4 appeared to underestimate the evaluated model's reasoning performance (i.e., judged correct/incorrect differently than human/expert ground truth).",
            "mitigation_strategies": "Authors applied several mitigation strategies when using GPT-4: (1) decompose evaluation into fine-grained aspects (instruction-following, truthfulness, honesty, helpfulness); (2) provide objective scoring documentation and reference responses to calibrate scores; (3) ask GPT-4 to score multiple completions for the same instruction simultaneously to reduce randomness and inconsistent standards; (4) require GPT-4 to output rationales (chain-of-thought) before numeric scores; (5) use majority human votes and expert labelers for ground-truth checks on reasoning tasks; and (6) supplement GPT-4 judgments with other AI judges (e.g., Claude-3) and human evaluation for cross-checking.",
            "citation_context_snippets": [
                "On average, GPT-4 judge exhibits 59.7% agreement rate with human labelers ... the agreement between GPT-4 and the majority votes of three annotators raises to 68.6%.",
                "Human evaluations are mostly consistent with GPT-4, giving a 64.3% against 67.3% average winning rate. We notice that human labelers tend to assign more ties than GPT-4, leading to slightly lower winning rates.",
                "To reduce ambiguity ... we decompose the overall quality assessment into four fine-grained assessments ... provide GPT-4 with detailed documentation of scores ... ask GPT-4 to score four completions simultaneously ... require GPT-4 to generate a rationale on how the response should be scored."
            ],
            "uuid": "e6236.0",
            "source_info": {
                "paper_title": "UltraFeedback: Boosting Language Models with High-quality Feedback",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Claude-3 judge",
            "name_full": "Claude-3 Sonnet (Anthropic) used as an alternate AI judge",
            "brief_description": "Claude-3 Sonnet was used as a supplementary AI judge to assess agreement trends across AI evaluators and to compare how different AI judges correlate with human majority votes; its outputs were compared with GPT-4 and human judgments on the same set of samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-domain instruction following / dialogue benchmarks (same sets used for GPT-4 comparisons)",
            "llm_judge_model": "Claude-3 Sonnet",
            "human_evaluation_setup": "Same prompt and dataset subset as GPT-4 comparison: the authors compared Claude-3 judgments on the 266 filtered pairs and contrasted them with GPT-4 and majority human votes (human majority used as reference).",
            "metrics_compared": "Win/tie/lose rates per benchmark and average win percentage; comparison with human majority votes and GPT-4 outputs.",
            "reported_differences": "Claude-3 produced higher average win rates (76.1% avg win) than GPT-4 (67.3%) and humans (64.3%) on the 266-pair set; Claude-3 tended to give far fewer ties (e.g., Evol-Instruct tie 1.4% vs GPT-4 tie 8.8% and human tie 17.6%), thereby increasing model win rates relative to human judgments.",
            "llm_specific_limitations": "Claude-3 showed a tendency to increase winning rates and assign very few ties, indicating possible systematic bias or stricter/stronger preference behavior relative to humans; differences across AI evaluators suggest evaluator-family-specific biases.",
            "notable_failure_cases": "Extremely low tie rates (e.g., 1.4% ties on Evol-Instruct) contrast with human annotators who used ties significantly more, implying Claude-3 may over-attribute wins/losses where humans consider responses comparable.",
            "mitigation_strategies": "Authors recommend cross-checking multiple AI evaluators against human judgments, using majority human votes as an anchor, and treating single-AI judgments cautiously; they use ensemble checks (GPT-4 + Claude-3) and human annotations to detect evaluator-specific biases.",
            "citation_context_snippets": [
                "We first supplement another AI evaluation using Claude-3 Sonnet ... Claude-3 shares the same trend as GPT-4 and further increases our models’ win rates.",
                "Claude-3 | ... | Avg. Win (%) 76.1  (compared to GPT-4 67.3 and Human 64.3)."
            ],
            "uuid": "e6236.1",
            "source_info": {
                "paper_title": "UltraFeedback: Boosting Language Models with High-quality Feedback",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Annotation mitigation techniques",
            "name_full": "Techniques to mitigate annotation biases when using LLMs as judges",
            "brief_description": "A set of procedural and prompt-engineering practices the authors used to improve reliability of LLM-as-judge annotations, including decomposition of aspects, scoring standards, multi-completion scoring, rationale requirements, scoring calibration with references, and data decontamination.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Annotation/evaluation of model responses across dialogue, QA, reasoning and writing tasks",
            "llm_judge_model": "Primarily GPT-4 (used to produce scalar scores and textual critiques); methods intended to generalize to other LLM judges",
            "human_evaluation_setup": "Not a human-evaluation protocol per se but a hybrid pipeline: the authors compared LLM annotations with human annotators (3 judges) and used human majority votes and expert labelers to validate and calibrate the LLM-based annotations.",
            "metrics_compared": "Annotation consistency/agreement (agreement rates with humans), stability/reproducibility (reduced randomness across runs), and downstream reward-model performance (preference-prediction accuracy on human-annotated benchmarks).",
            "reported_differences": "Fine-grained decomposition and scoring multiple completions simultaneously produced better alignment with human preferences and improved downstream reward-model generalization; aggregated overall-score annotations (single-sample scoring) were less consistent and led to weaker generalization on some benchmarks (e.g., WebGPT) than fine-grained, multi-completion scoring.",
            "llm_specific_limitations": "Despite mitigations, LLM judges still exhibit biases (position, length, style), can be subjective across runs, and cannot perfectly model human preferences in all situations; chain-of-thought rationales can help but do not fully eliminate inconsistency.",
            "notable_failure_cases": "Position bias and ordering effects observed in some reward-model formats (cited example: SteamSHP ordering led to large differences, Table 7); single-sample overall scoring by GPT-4 led to more randomness and worse transfer on WebGPT than fine-grained multi-completion scoring.",
            "mitigation_strategies": "Concrete methods applied in this paper: (1) decompose judgments into four aspects (instruction-following, truthfulness, honesty, helpfulness); (2) provide detailed scoring documentation (1–5) and reference answers for calibration; (3) score multiple completions jointly to enable cross-reference and reduce randomness; (4) require textual rationales before numeric scores; (5) diversify sampled completions (multiple model families, sizes, and prompting principles) to avoid spurious style-quality correlations; (6) decontaminate datasets against evaluation sets; (7) supplement LLM judgments with other AI judges and human majority/expert labels for validation.",
            "citation_context_snippets": [
                "Regarding the potential subjectivity and randomness of GPT-4 annotation, we apply four techniques to improve the annotation quality: (1) Decomposition ... (2) Standard ... (3) Reference ... (4) Rationale.",
                "A noteworthy finding is that ... the reward model trained with overall scores discernably lags behind the other two variants on WebGPT. ... fine-grained annotation ... provides a more precise assessment ... scoring all four completions at the same time ... may provide GPT-4 with cross-references and prevent it from applying inconsistent standards, reducing the impact of randomness."
            ],
            "uuid": "e6236.2",
            "source_info": {
                "paper_title": "UltraFeedback: Boosting Language Models with High-quality Feedback",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2
        },
        {
            "paper_title": "Alpacafarm: A simulation framework for methods that learn from human feedback",
            "rating": 1
        },
        {
            "paper_title": "RLAIF: Scaling reinforcement learning from human feedback with ai feedback",
            "rating": 1
        }
    ],
    "cost": 0.0154195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>UltraFeedback: Boosting Language Models with Scaled AI Feedback</h1>
<p>Ganqu Cui ${ }^{<em> 1}$ Lifan Yuan ${ }^{</em> 12}$ Ning Ding ${ }^{1}$ Guanming Yao ${ }^{13}$ Bingxiang He ${ }^{1}$ Wei Zhu ${ }^{4}$ Yuan Ni ${ }^{4}$<br>Guotong Xie ${ }^{4}$ Ruobing Xie ${ }^{5}$ Yankai Lin ${ }^{6}$ Zhiyuan Liu ${ }^{1}$ Maosong Sun ${ }^{17}$</p>
<h4>Abstract</h4>
<p>Learning from human feedback has become a pivot technique in aligning large language models (LLMs) with human preferences. However, acquiring vast and premium human feedback is bottlenecked by time, labor, and human capability, resulting in small sizes or limited topics of current datasets. This further hinders feedback learning as well as alignment research within the open-source community. To address this issue, we explore how to go beyond human feedback and collect high-quality AI feedback automatically for a scalable alternative. Specifically, we identify scale and diversity as the key factors for feedback data to take effect. Accordingly, we first broaden instructions and responses in both amount and breadth to encompass a wider range of user-assistant interactions. Then, we meticulously apply a series of techniques to mitigate annotation biases for more reliable AI feedback. We finally present UlTRAFEEDBACK, a large-scale, high-quality, and diversified AI feedback dataset, which contains over 1 million GPT-4 feedback for 250k user-assistant conversations from various aspects. Built upon UltraFeedback, we align a LLaMA-based model by best-of- $n$ sampling and reinforcement learning, demonstrating its exceptional performance on chat benchmarks. Our work validates the effectiveness of scaled AI feedback data in constructing strong open-source chat language models, serving as a solid foundation for future feedback learning research.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>1. Introduction</h2>
<p>Large language models (LLMs) (OpenAI, 2022; 2023) have demonstrated proficiency in generating fluent text as well as solving various language-oriented tasks. Trained on massive corpora through likelihood maximization techniques, these LLMs have equipped the ability to execute diverse tasks in response to user directives (Ouyang et al., 2022; Wei et al., 2022a; Sanh et al., 2022). Unfortunately, relying solely on imitation learning during training leads to well-known issues - LLMs may generate convincing but incorrect or unsafe content that deviates from human preferences (Stiennon et al., 2020; Perez et al., 2022). To further align LLMs with human preferences, learning from human feedback (Ouyang et al., 2022; Askell et al., 2021; Bai et al., 2022a; Touvron et al., 2023b) has been introduced and widely adopted by leading corporations. Over a period, feedback learning has been widely applied to closed-source models but scarcely used in open-source models.</p>
<p>Many factors hinder the implementation of feedback learning in the open-source community, but the first and primary issue is data. Preference data, which rates and compares different responses given the same prompt, is central to feedback learning. When scaled sufficiently, preference data reflects the intrinsic values of the annotators. Such annotators are often assumed, by default, to be human beings who can provide the most flexible and accurate supervision signals, yet the data they generate is severely bounded by factors like financial resources, time, and knowledge. As a result, existing preference datasets are either small in scale (Wu et al., 2023) or limited on specific tasks (Stiennon et al., 2020; Nakano et al., 2021). To this end, more efficient and principled methods to scale preference data are on the horizon.</p>
<p>This study aims to scale feedback data in an efficient manner. Specifically, we explore AI feedback (Bai et al., 2022b; Lee et al., 2023), which substitutes human annotators with advanced LLMs. Compared with human feedback, AI feedback is more scalable, which means (1) it is easier to collect and expand with lower cost; (2) its quality improves as the LLM annotators become more capable. In previous research, it is shown that advanced AI systems are capable of conducting chatbot evaluations (Dubois et al., 2023; Zheng et al., 2023a), giving textual critiques (Ye et al., 2023; Wang</p>
<p>et al., 2023c), or assisting human annotators (Saunders et al., 2022). However, open-source LLMs have not yet benefited from AI feedback through the lens of feedback learning.</p>
<p>This paper establishes a comprehensive AI feedback collection pipeline. Besides scalability, we prioritize diversity of both instructions and responses for holistic language model alignment. Particularly, we compile a diverse array of over 60,000 instructions and 17 models from multiple sources to produce comparative conversations in broad topics and quality. Then, we adopt a bunch of techniques to alleviate annotation biases and improve feedback quality to the greatest extent. These include (1) decomposing annotation documents into four different aspects, namely instruction-following, truthfulness, honesty, and helpfulness, to reduce ambiguity; (2) providing objective grading criteria and reference responses for score calibration; (3) asking GPT-4 for detailed textual critique before scores as chain-ofthought (Wei et al., 2022b) rationales. Comprehending all above, we finally build UlTRAFEEDBACK, a million-scale AI feedback dataset for aligning open-source LLMs.</p>
<p>We comprehensively validate the advantage of AI feedback in boosting open-source models with UlTRAFEEDBACK. By fine-tuning a LLaMA2-13B model (Touvron et al., 2023b), we build a state-of-the-art reward model UltraRM, which significantly outperforms existing opensource reward models. Based on UltraRM, we enhance a powerful open-source model UltraLM (Ding et al., 2023; Touvron et al., 2023a) with best-of- $n$ sampling and PPO. Experiments show that both strategies improve the model dramatically. Moreover, we fine-tune a critique model that could criticize and judge model responses. We also conduct a detailed analysis of the consistency and inconsistency between AI and human feedback.</p>
<p>To summarize, our contributions are three-fold: (1) To the best of our knowledge, we for the first time demonstrate the beneficial effect of scaled AI feedback on open-source chat LLMs. (2) We establish a systematic and sizable pipeline to collect high-quality and diversified AI feedback. (3) We release a suite of resources for feedback learning research, including a dataset, reward model, and critique model.</p>
<h2>2. UlTRAFEEDBACK</h2>
<h3>2.1. Overview</h3>
<p>Inspired by the data engineering principles in supervised fine-tuning (Ding et al., 2023; Chiang et al., 2023; Xu et al., 2023), we identify scalability and diversity as pivot factors of the overall generalizability of preference data. We argue that existing preference data suffer from satisfying either one of the two factors. To be specific, human feedback collection usually relies on human annotators to compare a pair of completions (Stiennon et al., 2020; Nakano et al., 2021; Ouyang et al., 2022; Bai et al., 2022a). Thus, the data is hard
to scale up due to time and budget constraints, especially for open-source researchers. On the other hand, existing AI feedback approaches (Bai et al., 2022b; Lee et al., 2023) reduce human involvement and enjoy scalability via capable LLMs, but they are limited to specific domains (Bai et al., 2022b; Lee et al., 2023) or forms (Ye et al., 2023) and hence lack the necessary diversity to boost LM performance under broader contexts.</p>
<p>To this end, we take into account scalability and diversity in all three stages of the preference data collection process: collecting instructions, sampling completions, and annotating comparison pairs. The overview of the data collection pipeline is shown in Figure 1. Firstly, we collect a largescale and diversified instruction set to enhance LLMs' capabilities from four aspects: (1) Follow Instructions: LLMs should respond to humans without deviating from the requirements. (2) Helpful and Informative: LLMs should provide useful and correct answers to address the given problems. (3) Truthful: LLMs' output should be grounded in the instructions and real-world knowledge, and avoid introducing any self-contradiction. (4) Honesty: LLMs should know what they (don't) know and express uncertainty towards the given problem. For the second stage, to avoid the sameness of comparison responses, we build a pool of distinct models at different capability levels to sample completions. Finally, to overcome the issues concerning scalability (Nakano et al., 2021; Stiennon et al., 2020) and quality (Ethayarajh et al., 2022), we seek scalable AI feedback from GPT-4, and explore several techniques to improve the reliability. Next, we will introduce our data construction pipeline in detail.</p>
<h3>2.2. Instruction Collection</h3>
<p>We select instructions that target four distinct but allimportant abilities of language models, namely instructionfollowing, truthfulness, honesty, and helpfulness. Specifically, we include all instructions from TruthfulQA (Lin et al., 2022) and FalseQA (Hu et al., 2023) training set for truthfulness. For instruction-following and helpfulness, we randomly sample 10k instructions from Evol-Instruct (Xu et al., 2023) and UltraChat (Ding et al., 2023) respectively, and sample 20k from ShareGPT (Chiang et al., 2023). We finally include FLAN (Longpre et al., 2023) to improve LLMs' helpfulness in various NLP tasks due to the task diversity within FLAN. We adopt a stratified sampling strategy following (Mukherjee et al., 2023), randomly picking 3k instructions from the "CoT" subset and sampling 10 instructions per task for the other three subsets, while excluding those with overly long instructions. In particular, honesty will be assessed by TruthfulQA and FLAN as they both contain reference answers, based on which it is easier for the annotator to judge if the uncertainty expressed in LLMs' responses calibrates with the accuracy. We then conduct</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. UlTRAFEEDBACK construction process. We sample instructions and models from large pools to guarantee diversity, then query GPT-4 with detailed illustrations for fine-grained and high-quality annotations in both textual and numerical formats.
a data contamination detection (see Appendix B). Finally, we obtain 63, 967 instructions of various types from the six publicly available high-quality datasets.</p>
<h3>2.3. Completion Sampling</h3>
<p>To guarantee that the collected responses are dissimilar and well-distributed, we include different models to generate completions for each instruction. To alleviate the potential spurious correlation between text styles and response quality within the dataset, we introduce intervention by selecting not only different series of models at different levels, but also models with different model sizes, architectures, and training data within the same model series. This strategy enables one type of text style to present responses of different quality levels, namely the response of one series of models may be better or worse than another depending on model sizes, thus avoiding the establishment of spurious correlations.</p>
<p>Specifically, we set up a pool of 17 models: (1) For commercial models, we choose GPT-4, gpt-3.5-turbo (ChatGPT), and Bard ${ }^{1}$; (2) For LLaMA-series, we choose UltraLM-13B/65B (Ding et al., 2023), WizardLM-7Bv1.1/13B-v1.2/70B-v1.1 (Xu et al., 2023), Vicuna-33B-v1.3 (Chiang et al., 2023), LLaMA2-7B/13B/70B-Chat (Touvron et al., 2023b), and Alpaca-7B (Taori et al., 2023); (3) For Non-LLaMA series, we choose MPT-30B-Chat (MosaicML, 2023), Falcon-40B-Instruct (Almazrouei et al., 2023), StarChat (Tunstall et al., 2023), and Pythia-12B (Biderman et al., 2023). We randomly sample four different models from the pool to complete each instruction.</p>
<p>To further improve diversity in model responses, we elicit distinct model behaviors by adding different principles before completing each instruction. Following Sun et al.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>(2023) and Mukherjee et al. (2023), we first hand-craft one principle for each aspect and then automize the procedure by invoking GPT-4 to curate another ten based on the humanwritten example. According to dataset characteristics, each data source is assigned with different principle prompts. We randomly sample a corresponding principle for each completion and add it to the system prompt to induce model behaviors. The principles can be found in Appendix G.1, and the effects of different principles are plotted in Figure 6.</p>
<h3>2.4. AI Feedback Annotation</h3>
<p>After generating 255, 864 model completions based on the 63, 967 instructions, we employ GPT-4 to provide two types of feedback for each completion: (1) scalar scores that indicate the fine-grained quality regarding multiple aspects, and (2) textual critique that gives detailed guidance on how to improve the completion. These lead to over 1 million feedback data in total.</p>
<p>Preference Annotation. Regarding the potential subjectivity and randomness of GPT-4 annotation, we apply four techniques to improve the annotation quality: (1) Decomposition. To reduce ambiguity and the difficulty of annotation, we decompose the overall quality assessment into four finegrained assessments, namely instruction-following, truthfulness, honesty, and helpfulness. (2) Standard. For each aspect, we provide GPT-4 with detailed documentation of scores from 1 to 5 for reference, thus avoiding variable and subjective standards. See Appendix G. 2 for an example. (3) Reference. To prevent inconsistency ratings across different runs, we wrap one instruction and all its completions into the prompt and ask GPT-4 to score four completions simultaneously to reduce randomness. (4) Rationale. Besides scoring each response, GPT-4 is required to generate a rationale on how the response should be scored according to the documentation. Combining all the techniques, we</p>
<p>Table 1. Statistics of existing preference and critique datasets. The average length refers to the number of tokens.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;"># Convs</th>
<th style="text-align: center;">Prompt <br> Length</th>
<th style="text-align: center;">Response <br> Length</th>
<th style="text-align: center;">Critique <br> Length</th>
<th style="text-align: center;">Fine- <br> Grained?</th>
<th style="text-align: center;">Feedback <br> Format</th>
<th style="text-align: center;"># Pairs</th>
<th style="text-align: center;"># Critique</th>
<th style="text-align: center;">Annotator</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Preference Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">OASST1</td>
<td style="text-align: center;">35,905</td>
<td style="text-align: center;">167.6</td>
<td style="text-align: center;">221.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Scalar</td>
<td style="text-align: center;">17,966</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI WebGPT</td>
<td style="text-align: center;">38,925</td>
<td style="text-align: center;">50.9</td>
<td style="text-align: center;">188.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Scalar</td>
<td style="text-align: center;">19,578</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: left;">Anthropic Helpful</td>
<td style="text-align: center;">118,263</td>
<td style="text-align: center;">185.7</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">Ranking</td>
<td style="text-align: center;">118,263</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: left;">OpenAI Summ.</td>
<td style="text-align: center;">60,674</td>
<td style="text-align: center;">$\mathbf{3 2 6 . 4}$</td>
<td style="text-align: center;">36.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Scalar</td>
<td style="text-align: center;">92,858</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: left;">QA Feedback</td>
<td style="text-align: center;">11,378</td>
<td style="text-align: center;">155.8</td>
<td style="text-align: center;">107.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Scalar</td>
<td style="text-align: center;">17,118</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: left;">Critique Dataset</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">SelFee</td>
<td style="text-align: center;">178,331</td>
<td style="text-align: center;">100.3</td>
<td style="text-align: center;">243.9</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{3 1 6 , 0 2 6}$</td>
<td style="text-align: center;">AI</td>
</tr>
<tr>
<td style="text-align: left;">Shepherd</td>
<td style="text-align: center;">1,316</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">67.2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Text</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">1,317</td>
<td style="text-align: center;">Human</td>
</tr>
<tr>
<td style="text-align: left;">UltraFeedback</td>
<td style="text-align: center;">$\mathbf{2 5 5 , 8 6 4}$</td>
<td style="text-align: center;">185.1</td>
<td style="text-align: center;">$\mathbf{3 0 5 . 3}$</td>
<td style="text-align: center;">$\mathbf{1 4 3 . 1}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Scalar \&amp; Text</td>
<td style="text-align: center;">$\mathbf{3 4 0 , 0 2 5}$</td>
<td style="text-align: center;">255,864</td>
<td style="text-align: center;">AI</td>
</tr>
</tbody>
</table>
<p>finally have four fine-grained scalar scores and rationales for each response.</p>
<p>Critique Generation. Besides scalar reward, we also seek textual critique from GPT-4. We prompt GPT-4 to act as a tutor and provide detailed suggestions specified for each completion to help models improve rather than propose answers directly. Different from the above comparison-oriented annotations, critique prompts are generated separately from an overall perspective for each completion. The prompts can be found in Appendix G.2.</p>
<h3>2.5. Dataset Statistics</h3>
<p>We compare UltraFeedback with current open-source datasets in Table 1. UltraFeedback stands out to be the largest one among all preference and critique datasets, which is at least twice as large as other datasets. Also, its completions and critiques are the longest. Moreover, we highlight that UltraFeedback is the only dataset that provides both scalar preferences and textual feedback, enabling it to serve as a preference and critique dataset simultaneously. Overall, UltraFeedback outperforms previous datasets in both scale and diversity, and we also validate its high quality by experiment in Section 3.</p>
<h3>2.6. UltraFeedback-Powered Models</h3>
<p>Based on UltraFeedback, we develop UltraRM, an advanced open-source reward model that provides preferences for AI responses given user instructions. Additionally, we train a critique model UltraCM from the textual feedback in UltraFeedback. UltraCM could interact with human and AI assistants more flexibly in text.</p>
<p>UltraRM. For reward modeling, we train UltraRM based on LLaMA2-13B (Touvron et al., 2023b). Specifically, we train three versions of UltraRM. We mix several open-source datasets with UltraFeedback to train</p>
<p>UltraRM. The open-source datasets include Stanford SHP (Ethayarajh et al., 2022), OpenAI Summarization (Stiennon et al., 2020), and Anthropic Helpful (Bai et al., 2022a). To validate the quality of UltraFeedback, we also train one model with merely the fine-grained scores of this dataset, i.e. averaging the preference scores in each aspect to get a final reward score. Further, to compare the effectiveness of the fine-grained scores and overall scores, we replace the fine-grained scores in UltraRM with the assessment ratings in critique generation, while remaining the open-source datasets. The details for dataset processing can be found in Appendix E.1. We keep the training strategy, including loss objective and training hyperparameters, exactly the same as Touvron et al. (2023b).</p>
<p>UltraCM. We also train a critique model stemming from UltraFeedback to boost future research in learning from textual feedback (Wang et al., 2023d). UltraCM has the same initialization as UltraRM but is trained solely on UlTRAFEEDBACK critique data, i.e. 255,864 textual feedback in total. Given a response, we fine-tune the model to give a corresponding critique that judges the response, figures out flaws, and provides suggestions for improvement.</p>
<h2>3. Experiments</h2>
<p>To validate the effect of AI feedback, we first evaluate UltraRM on human preference benchmarks in Section 3.1. Next, we test UltraRM in enhancing chat language models with two strategies, namely best-of- $n$ sampling (Section 3.2) and reinforcement learning (Section 3.3). Finally, we evaluate the feedback quality of UltraCM in Appendix E.3.</p>
<h3>3.1. Reward Modeling</h3>
<p>Setup. To evaluate how UltraRM aligns with human preference, we conduct experiments on four human annotated preference datasets, OpenAI WebGPT (Nakano et al., 2021), OpenAI Summarization (Stiennon et al., 2020), Anthropic HH-RLHF (Bai et al., 2022a), and Standford SHP. On</p>
<p>Table 2. Reward modeling accuracy (%) results. We compare our UltraRM with baseline open-source reward models. LLaMA2 results are taken from <em>(Touvron et al., 2023b)</em>. The highest results are in bold and the second highest scores are underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Backbone Model</th>
<th style="text-align: center;">Open?</th>
<th style="text-align: center;">Anthropic <br> Helpful</th>
<th style="text-align: center;">OpenAI <br> WebGPT</th>
<th style="text-align: center;">OpenAI <br> Summ.</th>
<th style="text-align: center;">Stanford <br> SHP</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Moss</td>
<td style="text-align: left;">LLaMA-7B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">54.6</td>
<td style="text-align: center;">58.3</td>
</tr>
<tr>
<td style="text-align: left;">Ziya</td>
<td style="text-align: left;">LLaMA-7B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">61.8</td>
<td style="text-align: center;">60.3</td>
<td style="text-align: center;">57.0</td>
<td style="text-align: center;">60.1</td>
</tr>
<tr>
<td style="text-align: left;">OASST</td>
<td style="text-align: left;">DeBERTa-v3-large</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">71.8</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">SteamSHP</td>
<td style="text-align: left;">FLAN-T5-XL</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">51.6</td>
<td style="text-align: center;">54.5</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA2 Helpfulness</td>
<td style="text-align: left;">LLaMA2-70B</td>
<td style="text-align: center;">$\boldsymbol{x}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 0}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\mathbf{7 5 . 5}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 0}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">UltraRM</td>
<td style="text-align: left;">LLaMA2-13B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\underline{71.0}$</td>
<td style="text-align: center;">$\mathbf{6 5 . 2}$</td>
<td style="text-align: center;">$\underline{74.0}$</td>
<td style="text-align: center;">$\underline{73.7}$</td>
<td style="text-align: center;">$\mathbf{7 1 . 0}$</td>
</tr>
<tr>
<td style="text-align: left;">w/ Only UltraFeEDBACK</td>
<td style="text-align: left;">LLaMA2-13B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">68.4</td>
<td style="text-align: center;">66.8</td>
</tr>
<tr>
<td style="text-align: left;">w/ Overall Score</td>
<td style="text-align: left;">LLaMA2-13B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\underline{71.0}$</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">$\underline{69.9}$</td>
</tr>
</tbody>
</table>
<p>each dataset, we calculate the rewards of two responses for one prompt and predict which one is more preferred. We compare our UltraRM-UF, UltraRM-Overall, and UltraRM with open-source baselines, including Moss <em>(Zheng et al., 2023b)</em>, Ziya <em>(IDEA-CCNL, 2021)</em>, OASST , and SteamSHP <em>(Ethayarajh et al., 2022)</em>. We also report the results in LLaMA2 <em>(Touvron et al., 2023b)</em>, although their reward models are not released.</p>
<p>Results. The preference prediction accuracy results are reported in Table 2. As we can see, the UltraRM series outperforms baseline reward models except for the closed LLaMA2 reward model (much larger) by a large margin, indicating that UltraRM series are the best open-source reward models. Notably, our reward model can still surpass all other baselines even without mixing open-source datasets. These results reveal that, UltraFeedback is highly consistent with human preference, and its high quality as well as diversity enable strong out-of-distribution generalization. On average, the model trained with only UltraFeedback outperforms open-source baseline models by over 6.3 percent in accuracy, while mixing open-source datasets with overall scores and fine-grained scores of UltraFeedback achieves 3.1 and 4.2 percent more improvement respectively.</p>
<p>We highlight that the OpenAI WebGPT dataset has no training and test splits, and neither most baselines nor we train reward models on this dataset, making it a fair benchmark to evaluate the generalization ability of reward models. Obviously, UltraRM series are significantly better, reaching $2.6 \%$ absolute points improvement over baselines. Another intriguing finding is that adding open-source datasets has a minor effect on the WebGPT dataset, which again proves the transferability advantage of UltraFeedback. On another benchmark Stanford SHP, UltraRM also achieves</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>remarkable performance.
A noteworthy finding is that, despite exhibiting comparably on the other three datasets, the reward model trained with overall scores discernably lags behind the other two variants on WebGPT. There can be two potential explanations for this observation. First, fine-grained annotation, which scores model outputs from different aspects respectively, provides a more precise assessment for each completion than aggregating evaluation into an overall number. Second, in the overall quality annotation process, each sample is sent to GPT-4 separately whereas, in fine-grained rating, all four completions are scored at the same time, which may provide GPT-4 with cross-references and prevent it from applying inconsistent standards, reducing the impact of randomness. These superiorities demonstrate the high quality of our finegrained preference data, and we advocate future work to adopt the fine-grained annotation schema and rate multiple completions at one time.</p>
<h3>3.2 Best-of- $n$ Experiments</h3>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Win rate against text-davinci-003 on AlpacaEval benchmark. We sample $n$ responses and choose the one with the highest reward.
Setup. To verify that UltraRM could serve as a good indicator of response quality, we conduct best-of- $n$ experiments.</p>
<p>Table 3. Head-to-head comparison results on three public benchmarks. The baseline is text-davinci-003 in AlpacaEval and gpt-3.5-turbo in Evol-Instruct and UltraChat. The judge is GPT-4. The highest win rates are in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">AlpacaEval Win (\%)</th>
<th style="text-align: center;">Evol-Instruct Win / Tie / Lose (\%)</th>
<th style="text-align: center;">UltraChat Win / Tie / Lose (\%)</th>
<th style="text-align: center;">Average <br> Win (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B-v1.5</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.0 / 23.9 / 43.1</td>
<td style="text-align: center;">34.5 / 38.2 / 27.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-13B-Chat</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">44.1 / 11.9 / 44.0</td>
<td style="text-align: center;">53.5 / 21.3 / 25.2</td>
<td style="text-align: center;">59.5</td>
</tr>
<tr>
<td style="text-align: center;">WizardLM-13B-v1.2</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">55.5 / 17.4 / 27.1</td>
<td style="text-align: center;">59.7 / 25.5 / 14.8</td>
<td style="text-align: center;">68.1</td>
</tr>
<tr>
<td style="text-align: center;">OpenChat-13B-v3.2super</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">55.5 / 11.0 / 33.5</td>
<td style="text-align: center;">58.7 / 26.7 / 14.5</td>
<td style="text-align: center;">67.9</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-70B-Chat</td>
<td style="text-align: center;">70B</td>
<td style="text-align: center;">92.7</td>
<td style="text-align: center;">56.4 / 13.8 / 29.8</td>
<td style="text-align: center;">54.0 / 28.6 / 17.4</td>
<td style="text-align: center;">67.7</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">UltraLM-13B</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">80.7</td>
<td style="text-align: center;">39.9 / 14.7 / 45.4</td>
<td style="text-align: center;">38.2 / 34.8 / 27.0</td>
<td style="text-align: center;">52.9</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B-v1.3</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">82.1</td>
<td style="text-align: center;">36.7 / 17.4 / 45.9</td>
<td style="text-align: center;">41.3 / 33.2 / 25.5</td>
<td style="text-align: center;">53.4</td>
</tr>
<tr>
<td style="text-align: center;">WizardLM-13B-v1.1</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">54.1 / 14.7 / 31.2</td>
<td style="text-align: center;">56.1 / 26.0 / 17.9</td>
<td style="text-align: center;">65.5</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-33B-v1.3</td>
<td style="text-align: center;">33B</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">50.0 / 17.0 / 33.0</td>
<td style="text-align: center;">57.7 / 25.7 / 16.6</td>
<td style="text-align: center;">65.6</td>
</tr>
<tr>
<td style="text-align: center;">UltraLM-13B-PPO</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">57.8 / 10.1 / 32.1</td>
<td style="text-align: center;">64.9 / 15.6 / 19.5</td>
<td style="text-align: center;">69.7</td>
</tr>
</tbody>
</table>
<p>On the AlpacaEval benchmark, we randomly sample 16 completions from the original UltraLM-13B and calculate their corresponding rewards. We then select the best-of${1,2,4,8,16}$ completions as the final response. The sampling parameters are set to temperature $=1$ and top- $p=1$.</p>
<p>Results. We present results in Figure 2. Apparently, we can see the win rate on AlpacaEval increases proportionally with rewards. This validates that our UltraRM gives rigorous rewards that reflect the overall response quality. Notably, the best-of- $n$ sampling strategy is surprisingly effective. The initial UltraLM-13B model achieves a $76.53 \%$ win rate for a single sampling, and a simple best-of-2 sample increases the win rate to $84.64 \%$. With more samples, we can get even more high-quality responses, and the final best-of-16 win rate hits $91.54 \%$. The best-of- $n$ sampling is universally applicable across models and tasks, which enhances models without training. Please refer to Appendix F. 2 for cases.</p>
<h3>3.3. PPO Experiments</h3>
<p>Setup. Given the state-of-the-art UltraRM, we aim to push the upper bound of open-source chat language models with RLAIF. Specifically, we perform PPO over UltraLM13B (Ding et al., 2023) to get its PPO version, UltraLM-13B-PPO. We tune UltraLM for 80 iterations on the ULTRAFEEDBACK prompts. In each iteration, we collect 512 samples and update the policy model with a mini-batch size of 64. The learning rate is fixed at 1e-6.</p>
<p>Baselines. We compare UltraLM-13B-PPO with leading open-source models and proprietary models, including LLaMA2-Chat (Touvron et al., 2023b), Vicuna (Chiang et al., 2023), WizardLM (Xu et al., 2023), OpenChat (Wang et al., 2023a), and ChatGPT (OpenAI, 2022).</p>
<p>Benchmarks. We conduct experiments on three public benchmarks, namely AlpacaEval (Li et al., 2023), EvolInstruct (Xu et al., 2023), and UltraChat (Ding et al., 2023). On each benchmark, we ask GPT-4 to judge which response is better given the same instruction. AlpacaEval adopts text-davinci-003 as the competitor model, while we compete with gpt-3.5-turbo on Evol-Instruct and UltraChat. To avoid position bias, we randomly switch the comparing responses. For all models, we use the same decoding parameter with temperature $=0.7$ and top- $p=1$.</p>
<p>Results. We report experiment results in Table 3. We take the official results on the AlpacaEval leaderboard for baseline models and conduct evaluations by ourselves for other results. Overall, our UltraLM-13B-PPO achieves the highest average win rate on the three benchmarks, outperforming all other open-source models. Among LLaMA-based models, UltraLM-13B-PPO overtakes other models by at least 3.6 percent on average. Even when compared with the much larger LLaMA2-70B-Chat model, our model still holds the advantage, illustrating the huge benefit of RLAIF alignment. Our model also reaches the highest win rate on two of the benchmarks, Evol-Instruct and UltraChat, against the more powerful gpt-3.5-turbo. It is worth noting that, compared with the original UltraLM-13B, the PPO process benefits the model greatly, leading to a 16.8 percent enhancement. We provide cases in Appendix F.3.</p>
<h2>4. Agreement with Human Preferences</h2>
<p>The inclusivity of human preferences is known to be hard to capture (Dubois et al., 2023). Heavily relying on AI feedback, it is essential to measure and monitor the agreement between AI and human preferences. In this section, we conduct experiments to see (1) to what extent AI annotations</p>
<p>are consistent with human preferences (Section 4.1) and (2) how reliable AI evaluations are (Section 4.2).</p>
<h3>4.1. Annotation Consistency</h3>
<p>In Section 3.1, we show that the reward models trained on UltraFeedback could predict human preference accurately. To further analyze to what extent AI feedback could capture human preference, we randomly sample 400 comparison pair from UltraFeedback, AlpacaEval, Evol-Instruct and UltraChat test sets (100 each) and ask 3 independent annotators to compare those pairs (win/tie/lose). The annotators are undergraduate and graduate students. We present the agreement ratio between GPT-4 and annotators, as well as annotators themselves in Table 4. On average, GPT-4 judge exhibits 59.7% agreement rate with human labelers, which matches previous human evaluation on MT-Bench (Zheng et al., 2023a). We also observe similar agreement rates among annotators. Notably, the agreement between GPT-4 and the majority votes of three annotators raises to 68.6%, meaning that GPT-4 better reflects the collective human preferences.</p>
<p>Table 4. Agreement between judges on 400 samples from UltraFeedback, AlpacaEval, Evol-Instruct, and UltraChat test sets . A-1, A-2, A-3 are three human judges. “Majority” stands for the agreement between each judge and other three judges’s majority votes. We include tie votes and the random agreement is 33%.</p>
<table>
<thead>
<tr>
<th>Judge</th>
<th>A-1</th>
<th>A-2</th>
<th>A-3</th>
<th>Average</th>
<th>Majority</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4</td>
<td>59.2%</td>
<td>60.8%</td>
<td>59.1%</td>
<td>59.7%</td>
<td>68.6%</td>
</tr>
<tr>
<td>A-1</td>
<td>-</td>
<td>58.1%</td>
<td>54.7%</td>
<td>57.3%</td>
<td>60.3%</td>
</tr>
<tr>
<td>A-2</td>
<td>58.1%</td>
<td>-</td>
<td>55.4%</td>
<td>58.1%</td>
<td>63.3%</td>
</tr>
<tr>
<td>A-3</td>
<td>54.7%</td>
<td>55.4%</td>
<td>-</td>
<td>56.4%</td>
<td>62.0%</td>
</tr>
</tbody>
</table>
<h3>4.2. Reliability of AI Evaluation</h3>
<p>We first supplement another AI evaluation using Claude-3 Sonnet (Anthropic, 2024) to investigate the agreement among different series of AI models. The prompts are the same as the GPT-4 evaluation. Then, we compare both of our AI evaluation results with human annotations to examine if AI evaluations reliably correlate with humans. Particularly, we use the majority votes of the three annotators and filter out samples with all different votes.</p>
<p>We present GPT-4, Claude-3, and human evaluation results on the remaining 266 pairs in Table 5. Overall, Claude-3 shares the same trend as GPT-4 and further increases our models’ win rates. Human evaluations are mostly consistent with GPT-4, giving a 64.3% against 67.3% average winning rate. We notice that human labelers tend to assign more ties than GPT-4, leading to slightly lower winning rates.</p>
<p>For fine-grained analysis, we categorize the evaluated samples into reasoning, writing, and QA tasks. The categorical comparison results are presented in Figure 3. It is shown that human evaluations are mostly consistent with GPT-4, where they both prefer our models on writing and QA tasks. On reasoning tasks including coding, math, and logic, human and GPT-4 judgments diverge on ties and losses, where GPT-4 gives fewer ties but more losses. To delve into the discrepancy deeper, we ask another expert labeler to determine the ground truth answer for each question. In this sense, a model wins when it gives the correct answer while the other does not, and vice versa. The two models tie when they both successfully or unsuccessfully answer the question. The final win/tie/lose rate comes at $42.1\%/26.3\%/31.6\%$ and closely matches human evaluations. The GPT-4 judge, in this case, potentially underestimated our model’s reasoning performance and still has space to improve.</p>
<p>Table 5. Human evaluation results. We use majority votes from three human judges and compare GPT-4, Claude-3, and human evaluations on the same 266 samples.</p>
<table>
<thead>
<tr>
<th>Judge</th>
<th>AlpacaEval Win (%)</th>
<th>Evol-Instruct Win / Tie / Lose (%)</th>
<th>UltraChat</th>
<th>Avg. Win (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4</td>
<td>83.9</td>
<td>57.1 / 8.8 / 34.1</td>
<td>61.0 / 17.1 / 21.9</td>
<td>67.3</td>
</tr>
<tr>
<td>Claude-3</td>
<td>95.1</td>
<td>59.6 / 1.4 / 39.0</td>
<td>73.5 / 6.8 / 19.7</td>
<td>76.1</td>
</tr>
<tr>
<td>Human</td>
<td>78.5</td>
<td>68.1 / 17.6 / 14.3</td>
<td>46.3 / 19.5 / 34.1</td>
<td>64.3</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Catrgorical comparison of human and GPT-4 judgments. Human judgments are majority votes from three annotators. Sample numbers of each category are in parentness.</p>
<h2>5. Analysis</h2>
<p>In this section, we further analyze how UltraFeedback enhances language models on different subjects (Section 5.1) and tasks (Section 5.2).</p>
<h3>5.1. Question Type Breakdown</h3>
<p>Figure 4 reports the UltraLM-13B-PPO and UltraLM-13B scores on different question types versus gpt-3.5-turbo on the Evol-Instruct test set. We observe that UltraLM-13B-PPO overtakes ChatGPT on 22/29 subjects, especially on writing-related tasks such as academic writing. Our model is also well-aligned with human values, getting higher scores on toxicity, ethics, and TruthfulQA. On some difficult subjects like roleplay, reasoning, and counterfactual, our model is still on par with ChatGPT, indicating the strong advanced model capability. Compared with the original UltraLM-13B, PPO boosts the model in multiple aspects, including</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Comparison results between UltraLM-13B-PPO, UltraLM-13B, and gpt-3.5-turbo on Evol-Instruct test set, where gpt-3.5-turbo scores are $100 \%$.</p>
<p>Table 6. Exact match scores (\%) for UltraLM-13B and UltraLM-13B-PPO on model capability benchmarks.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>BoolQ</th>
<th>HellaSwag</th>
<th>RACE-h</th>
<th>RACE-m</th>
<th>MultiRC</th>
<th>TriviaQA</th>
<th>NQ</th>
<th>PIQA</th>
<th>OBQA</th>
<th>ARC-E</th>
<th>ARC-C</th>
<th>Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td>UltraLM-13B</td>
<td>85.0</td>
<td>59.8</td>
<td>66.1</td>
<td>73.5</td>
<td>83.2</td>
<td>50.8</td>
<td>19.4</td>
<td>73.5</td>
<td>57.0</td>
<td>76.1</td>
<td>51.5</td>
<td>63.3</td>
</tr>
<tr>
<td>UltraLM-13B-PPO</td>
<td>83.5</td>
<td>62.6</td>
<td>66.8</td>
<td>74.2</td>
<td>83.7</td>
<td>52.5</td>
<td>22.1</td>
<td>74.9</td>
<td>57.0</td>
<td>76.1</td>
<td>53.9</td>
<td>64.3</td>
</tr>
</tbody>
</table>
<p>professional knowledge (economy, chemistry, music, literature) and reasoning ability (reasoning, complex format, code generation, math). Meanwhile, our model falls behind gpt-3.5-turbo on math and code-related tasks, which might be attributed to the limitation of base model ability and the lack of relevant data in ULTRAFEEDBACK. Table 9 in Appendix E. 5 provides additional results on the UltraChat test set and reaches the same conclusion. We leave this as our future work.</p>
<h3>5.2. Does RLAIF Benefit Model Capability?</h3>
<p>To test whether RLAIF impacts base model capability, we conduct experiments on nine commonly used benchmarks including question answering and multiple-choice questions (See Appendix E. 4 for details). We compare UltraLM-13B before and after PPO. The results in Table 6 demonstrate marginal improvements over these benchmarks with about 1 absolute point. We note that this is in line with established conclusions (OpenAI, 2023) regarding RLHF, which state that RLHF could produce more preferable responses, but has a minor effect on model capability.</p>
<h2>6. Related Work</h2>
<p>Feedback Learning for LLMs. Incorporating human feedback with imitation learning or reinforcement learning (Schulman et al., 2017; Rafailov et al., 2023) has been the mainstream approach to align LLMs with human preferences in leading cooperations (Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022a; Glaese et al., 2022; OpenAI, 2022; 2023; Touvron et al., 2023a). However, human feedback relies on human capabilities, which makes it hard to scale up and apply to superhuman tasks. Accordingly, some researchers proposed scalable oversight, which aims to supervise potent AI models by models themselves (Irving et al., 2018; Leike et al., 2018; Christiano et al., 2018). Empirically for LLMs, Bai et al. (2022b) first presented</p>
<p>Constitutional AI to let LLMs refine their responses given a set of regulations. Lee et al. (2023) and Burns et al. (2023) further validated that learning from AI feedback could surpass human feedback on some specific tasks. More broadly, our work verified that scaled AI feedback could enhance the general ability of open-source chat models. Data for LLM Alignment. The importance of data scalability and quality has been widely recognized in the literature on instruction tuning (also known as SFT). Early works collected various NLP tasks or real user conversations to conduct instruction tuning and observed that LLMs could generalize well across different tasks (Wei et al., 2022a; Sanh et al., 2022; Wang et al., 2022; Ouyang et al., 2022). After the release of ChatGPT, most recent research on SFT emphasized the importance of data construction and reached conclusions that scalability, diversity, as well as quality, are vital for the final performance (Ding et al., 2023; Taori et al., 2023; Chiang et al., 2023; Xu et al., 2023). However, when it goes to the feedback learning stage, the importance of data engineering has not been well illustrated. Among current preference datasets, some of them focus on specific tasks (e.g. summarization (Stiennon et al., 2020), search-based question answering (Nakano et al., 2021), safety-oriented scenarios (Ji et al., 2023), and math problems (Lightman et al., 2023)), thus cannot boost general chat models. Some datasets are small in scale (Wu et al., 2023; Wang et al., 2023c) or provide only community votes as coarse-grained preferences (Ethayarajh et al., 2022; Askell et al., 2021). Therefore, a large general-purpose preference dataset with diverse instructions and fine-grained annotations in the opensource community is urgently in need, which motivates us to construct ULTRAFEEDBACK.</p>
<h2>7. Conclusion</h2>
<p>In this paper, we proposed to enhance open-source LLMs with scaled AI feedback. Through meticulous designation, we constructed UlTRAFEEDBACK, a large-scale and di-</p>
<p>verse AI feedback dataset. With the data, we embarked on a thorough exploration of AI feedback's multifaceted utilities, including modeling human preferences, improving chat language models, and training critique models. Our analysis further delved deep into human agreement and model capability evaluations, revealing some nuanced insights. We believe that AI feedback would become a scalable and reliable source for future AI oversight. We hope our work could serve as an early exploration and data support in this area, facilitating researchers in the open-source community. In future work, we will continue exploring diverse, high-quality, and scalable preference data construction, expanding AI feedback in multi-turn dialogues, complex reasoning, coding, and safety scenarios.</p>
<h2>Impact Statement</h2>
<p>Aligning AI systems, especially advanced LLMs, is important for the safety and trustworthiness in their applications. We manage to enhance open LLMs with scaled AI feedback which is an underexplored research direction. With high efficiency and low cost, leveraging AI feedback could significantly reduce the consumation of human labors, leading to more scalable alignment. We should also raise attention to the limitations of AI feedback, LLMs could be biased towards certain features, such as answer positions (Zheng et al., 2023a), response lengths, and certain styles. In this way, such biases might lead to inaccurate or unfair annotations and evaluations. By overcoming these biases, more precise and helpful AI feedback can be obtained. In terms of UltraFeedback, we could expect it to improve a considerable amount of open-source LLMs and narrow their gaps with close-sourced models. We did not add safety-oriented conversations intentionally, so there could still be toxicity and unethical behaviors in the aligned models if prompted adversarially. We believe our paradigm is still useful for enhancing model safety, and are extensively working on it. Alongside data, we also release a series of models for feedback learning research. The reward model and critique model can be directly used to align LLMs for more prefered behaviors. On the other hand, although our models are potent in solving tasks and giving feedback, they may also generate hallucinations and falsehoods. The risk of misuse is a severe threat to open LLMs, which calls for appropriate regulation and supervision.</p>
<h2>Acknowledgement</h2>
<p>This work is supported by the National Key R\&amp;D Program of China (No.2022ZD0116312), National Natural Science Foundation of China (No. 62236004).</p>
<h2>References</h2>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art performance. 2023.</p>
<p>Anthropic. Introducing the next generation of claude. 2024.
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022b.</p>
<p>Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, Proceedings of Machine Learning Research, 2023.</p>
<p>Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, 2020.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,</p>
<p>Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, Jeff Wu, and OpenAI. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. ArXiv, abs/2312.09390, 2023. URL https://api.semanticscholar. org/CorpusID:266312608.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.</p>
<p>Paul Francis Christiano, Buck Shlegeris, and Dario Amodei. Supervising strong learners by amplifying weak experts. ArXiv, abs/1810.08575, 2018. URL https://api. semanticscholar.org/CorpusID:53041432.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233, 2023.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy</p>
<p>Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.</p>
<p>Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with $\mathcal{V}$-usable information. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 5988-6008. PMLR, 17-23 Jul 2022.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021. URL https : //doi.org/10.5281/zenodo.5371628.</p>
<p>Amelia Glaese, Nat McAleese, Maja Trkebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Shengding Hu, Yifan Luo, Huadong Wang, Xingyi Cheng, Zhiyuan Liu, and Maosong Sun. Won't get fooled again: Answering questions with false premises. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, 2023.</p>
<p>IDEA-CCNL. Fengshenbang-lm. https://github. com/IDEA-CCNL/Fengshenbang-LM, 2021.</p>
<p>Geoffrey Irving, Paul Francis Christiano, and Dario Amodei. Ai safety via debate. ArXiv, abs/1805.00899, 2018. URL https://api.semanticscholar. org/CorpusID:22050710.</p>
<p>Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. arXiv preprint arXiv:2307.04657, 2023.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017.</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252-262, 2018.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466, 2019.</p>
<p>Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683, 2017.</p>
<p>Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.</p>
<p>Jan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable agent alignment via reward modeling: a research direction. ArXiv, abs/1811.07871, 2018. URL https://api. semanticscholar.org/CorpusID:53745764.</p>
<p>Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github. com/tatsu-lab/alpaca_eval, 2023.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. arXiv preprint arXiv:2305.20050, 2023.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, 2022.</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. The flan collection: Designing data and methods for effective instruction tuning. CoRR, abs/2301.13688, 2023.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018.</p>
<p>MosaicML. Introducing mpt-30b: Raising the bar for opensource foundation models, 2023. URL www.mosaicml . com/blog/mpt-30b. Accessed: 2023-06-22.</p>
<p>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Hassan Awadallah. Orca: Progressive learning from complex explanation traces of GPT-4. CoRR, abs/2306.02707, 2023.</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.</p>
<p>OpenAI. Chatgpt: Optimizing language models for dialogue, 2022.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744, 2022.</p>
<p>Joon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.</p>
<p>Ethan Perez, Saffron Huang, H. Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 3419-3448. Association for Computational Linguistics, 2022. doi: 10.18653/v1/ 2022.emnlp-main.225. URL https://doi.org/10. 18653/v1/2022.emnlp-main. 225.</p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023.</p>
<p>Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv preprint arXiv:2304.08354, 2023.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.</p>
<p>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. URL https://openreview.net/forum? id=9Vrb9D0WI4.</p>
<p>William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. CoRR, abs/2206.05802, 2022. doi: 10.48550/ARXIV. 2206.05802. URL https://doi.org/10.48550/ arXiv.2206.05802.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing Systems, 33:3008-3021, 2020.</p>
<p>Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David D. Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. ArXiv, abs/2305.03047, 2023.</p>
<p>Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Stanford alpaca: An instructionfollowing llama model, 2023.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Lewis Tunstall, Nathan Lambert, Nazneen Rajani, Edward Beeching, Teven Le Scao, Leandro von Werra, Sheon Han, Philipp Schmid, and Alexander Rush. Creating a coding assistant with starcoder. Hugging Face Blog, 2023. https://huggingface.co/blog/starchat.</p>
<p>Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235, 2023a.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators, 2023b.</p>
<p>Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O’Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: A critic for language model generation. arXiv preprint arXiv:2308.04592, 2023c.</p>
<p>Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691, 2023d.</p>
<p>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza,</p>
<p>Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-naturalinstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 50855109, 2022. URL https://doi.org/10.18653/ v1/2022.emnlp-main. 340.</p>
<p>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022a. URL https://openreview. net/forum?id=gEZrGCozdqR.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022b.</p>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. arXiv preprint arXiv:2306.01693, 2023.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.</p>
<p>Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective large language agents with policy gradient optimization. arXiv preprint arXiv:2308.02151, 2023.</p>
<p>Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. Selfee: Iterative self-revising llm empowered by self-feedback generation. Blog post, May 2023. URL https://kaistai. github.io/SelFee/.</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge
with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023a.</p>
<p>Rui Zheng, Shihan Dou, Songyang Gao, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Limao Xiong, Lu Chen, et al. Secrets of rlhf in large language models part i: Ppo. arXiv preprint arXiv:2307.04964, 2023b.</p>
<h1>A. Limitations</h1>
<p>In constructing UlTRAFeEDBACK, we made an assumption that powerful LLMs like GPT-4 are capable of imitating human annotators and fair evaluators. Although more and more works accepted this assumption and demonstrated high agreement between human and LLM feedbacks (Dubois et al., 2023; Lee et al., 2023; Bai et al., 2022b), LLMs still cannot model human preference precisely under all situations. How to efficiently and accurately collect preference data and conduct rigorous evaluation are still challenging. We leave this as future work for further investigation. Another limitation is that UlTRAFeEDBACK only provides single-turn dialogues to improve the utility of LLMs due to time and budget restrictions. We will also expand UlTRAFeEDBACK to cover more tasks and scenarios.</p>
<h2>B. Data Contamination</h2>
<p>To avoid data contamination which could result in unfair even wrong evaluations, we did careful decontamination for UlTRAFeEDBACK. Following GPT-3 (Brown et al., 2020) and evaluation-harness (Gao et al., 2021), we search for 13-gram matches between AlpacaEval, Evol-Instruct, and UltraChat test set. We found in total 48 contamination samples and filtered out them. However, we did not conduct a thorough examination of contamination over other evaluation datasets because of the huge amount of datasets. Therefore, we suggest researchers decontaminate UlTRAFeEDBACK with their evaluation datasets before using it.</p>
<h2>C. UltraFeedback Statistics</h2>
<p>We summarize the scores for each model over different aspects in Figure 5. Overall, the rankings are consistent with model capabilities. For example, the GPT series is the best in all aspects, and larger models are generally better than smaller ones. The distinction among different aspects also exists. For instance, the LLaMA2-Chat models received higher scores on honesty, since they are aligned with human values with RLHF (Touvron et al., 2023b).</p>
<p>We also showcase how different principles stimulate diverse model behaviors. We average the score of each aspect when applying different principles to models, and plot them in Figure 6.</p>
<h2>D. Training Details</h2>
<h2>D.1. UltraRM</h2>
<p>We construct each comparison pair as a binary selection, with one completion being chosen and the other rejected. We optimize the reward model to select preferred completion by minimizing the binary ranking loss:</p>
<p>$$
\mathcal{L}<em _theta="\theta">{\text {ranking }}=-\log \left(\sigma\left(r</em>\right)-m(r)\right)\right)
$$}\left(x, y_{c}\right)-r_{\theta}\left(x, y_{r</p>
<p>where $\theta$ represents the reward model, $r_{\theta}\left(x, y_{c}\right)$ is its scalar reward prediction towards the chosen text, $r_{\theta}\left(x, y_{r}\right)$ is that towards the rejected text, and $m(r)$ is the absolute difference between the annotated reward of two texts. We set the $m(r)=0$ for datasets with only preference rankings and normalize the margins to $(0,1]$ to avoid training instability due to a mismatch among the score scales of the datasets. Following Touvron et al. (2023b), we train the 13B reward model for one epoch with the batch size being 512 pairs (i.e., 1024 completions) and the learning rate being 1e-5. We adopt the cosine learning rate decay strategy with a warm-up ratio of $3 \%$ and a final learning rate of 1e-6.</p>
<h2>D.2. UltraCM</h2>
<p>We train LLaMA2-13B for two epochs with a batch size of 256 and a learning rate of 2e-5. We adopt the same learning rate scheduler as reward modeling.</p>
<h2>E. Experiment Details</h2>
<h2>E.1. Dataset Details for UltraRM Training</h2>
<p>We mix UlTRAFeEDBACK with other open-source preference datasets for reward modeling. Stanford SHP is a communitybased preference dataset collected from 18 different topics, adopting a strict filtering strategy to ensure text quality and reliability of preferences. We follow the guidelines in the official repository to further filter the dataset, only retaining</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Average scores for each model over the four aspects.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Different principles stimulate diverse model behaviors.
preferences with a score ratio greater than 2 and using at most 5 comparison pairs for each post via random sampling. OpenAI Summarize consists of human-written completions and human-annotated preferences, with the instructions being much longer than UlTRAFEEDBACK. Hence, we include the high-quality dataset to enhance the subsequent reward model for long-text scenarios. We adopt the same comparison pair filtering method to avoid the reward model overfitting certain instructions. Anthropic Helpful is another human-annotated dataset. We incorporate all its samples into our training dataset to supplement multiturn dialogs data. For UlTRAFEEDBACK, we directly adopt the overall score obtained in critique annotation as the preference score for UltraRM-Overall, while for fine-grained versions, we average the scores of all aspects for each sample as the final preference score. Finally, the training dataset for our reward model contains a total of 749,702 comparison pairs, with 340,025 from UlTRAFEEDBACK, 198, 556 from Stanford SHP, 92, 858 from OpenAI Summarize, and 118,263 from Anthropic Helpful.</p>
<h1>E.2. Additional Reward Modeling Experiments</h1>
<p>We observed that the SteamSHP model is different from other reward models in the input format, for it accepts two responses simultaneously and outputs which one is better (text-to-text format). During the experiment, we found that there is a position bias issue for this approach, where the reward model tends to prefer the first responses. To eliminate this issue, we average the scores from two runs exchanging response orders to get the final scores. We report the detailed results in Table 7.</p>
<h2>E.3. Critique Modeling</h2>
<p>Setup. To assess the ability of UltraCM to provide reliable critique, we employ GPT-4 to score the quality of critique based on detailed documentation. we follow (Wang et al., 2023c) to randomly sample 50 instructions from PIQA (Bisk et al., 2020), OpenBookQA (OBQA) (Mihaylov et al., 2018), CommonsenseQA (Talmor et al., 2018), AlpacaFarm (Dubois et al., 2023), and FairEval (Wang et al., 2023b). We also supplement HumanEval (Chen et al., 2021), MBPP (Austin et al.,</p>
<p>Table 7. Reward modeling results for SteamSHP with different sample orders.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;">Anthropic <br> Helpful</th>
<th style="text-align: center;">OpenAI <br> WebGPT</th>
<th style="text-align: center;">OpenAI <br> Summ.</th>
<th style="text-align: center;">Stanford <br> SHP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Chosen first</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">72.4</td>
<td style="text-align: center;">52.8</td>
<td style="text-align: center;">71.8</td>
</tr>
<tr>
<td style="text-align: left;">Rejected first</td>
<td style="text-align: center;">38.8</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">44.0</td>
<td style="text-align: center;">31.4</td>
</tr>
<tr>
<td style="text-align: left;">Avg.</td>
<td style="text-align: center;">55.4</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">48.4</td>
<td style="text-align: center;">51.6</td>
</tr>
</tbody>
</table>
<p>Table 8. Feedback quality of each model on different datasets rated by GPT-4. The best performance on each dataset is marked in bold, and the second has been underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">PIQA</th>
<th style="text-align: center;">OBQA</th>
<th style="text-align: center;">Common- <br> senseQA</th>
<th style="text-align: center;">Alpaca- <br> Farm</th>
<th style="text-align: center;">Fair- <br> Eval</th>
<th style="text-align: center;">Human- <br> Eval</th>
<th style="text-align: center;">MBPP</th>
<th style="text-align: center;">MATH</th>
<th style="text-align: center;">GSM8K</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">gpt-3.5-turbo</td>
<td style="text-align: center;">$\mathbf{6 . 0 8}$</td>
<td style="text-align: center;">$\mathbf{6 . 1 2}$</td>
<td style="text-align: center;">$\mathbf{6 . 0 4}$</td>
<td style="text-align: center;">$\mathbf{6 . 4 4}$</td>
<td style="text-align: center;">$\mathbf{6 . 3 2}$</td>
<td style="text-align: center;">$\mathbf{6 . 1 4}$</td>
<td style="text-align: center;">$\mathbf{6 . 4 8}$</td>
<td style="text-align: center;">$\mathbf{5 . 9 8}$</td>
<td style="text-align: center;">$\mathbf{5 . 9 4}$</td>
<td style="text-align: center;">$\mathbf{6 . 1 7}$</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA2-13B-Chat</td>
<td style="text-align: center;">5.92</td>
<td style="text-align: center;">5.04</td>
<td style="text-align: center;">5.66</td>
<td style="text-align: center;">5.26</td>
<td style="text-align: center;">5.74</td>
<td style="text-align: center;">4.64</td>
<td style="text-align: center;">4.82</td>
<td style="text-align: center;">3.88</td>
<td style="text-align: center;">4.30</td>
<td style="text-align: center;">5.03</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-13B-v1.5</td>
<td style="text-align: center;">5.66</td>
<td style="text-align: center;">5.58</td>
<td style="text-align: center;">5.42</td>
<td style="text-align: center;">5.58</td>
<td style="text-align: center;">5.82</td>
<td style="text-align: center;">4.86</td>
<td style="text-align: center;">5.20</td>
<td style="text-align: center;">4.56</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">5.28</td>
</tr>
<tr>
<td style="text-align: left;">WizardLM-13B-v1.2</td>
<td style="text-align: center;">5.90</td>
<td style="text-align: center;">5.52</td>
<td style="text-align: center;">5.82</td>
<td style="text-align: center;">5.66</td>
<td style="text-align: center;">5.88</td>
<td style="text-align: center;">5.28</td>
<td style="text-align: center;">5.34</td>
<td style="text-align: center;">4.30</td>
<td style="text-align: center;">4.90</td>
<td style="text-align: center;">5.40</td>
</tr>
<tr>
<td style="text-align: left;">Shepherd-13B</td>
<td style="text-align: center;">3.48</td>
<td style="text-align: center;">3.64</td>
<td style="text-align: center;">3.48</td>
<td style="text-align: center;">3.04</td>
<td style="text-align: center;">3.30</td>
<td style="text-align: center;">3.08</td>
<td style="text-align: center;">3.20</td>
<td style="text-align: center;">3.10</td>
<td style="text-align: center;">2.76</td>
<td style="text-align: center;">3.23</td>
</tr>
<tr>
<td style="text-align: left;">SelFee-13B</td>
<td style="text-align: center;">$\underline{6.00}$</td>
<td style="text-align: center;">5.32</td>
<td style="text-align: center;">5.74</td>
<td style="text-align: center;">5.88</td>
<td style="text-align: center;">5.94</td>
<td style="text-align: center;">4.84</td>
<td style="text-align: center;">5.12</td>
<td style="text-align: center;">4.46</td>
<td style="text-align: center;">5.40</td>
<td style="text-align: center;">5.41</td>
</tr>
<tr>
<td style="text-align: left;">UltraCM-13B</td>
<td style="text-align: center;">$\underline{6.00}$</td>
<td style="text-align: center;">$\mathbf{6 . 1 2}$</td>
<td style="text-align: center;">$\underline{6.02}$</td>
<td style="text-align: center;">$\underline{5.98}$</td>
<td style="text-align: center;">$\underline{6.18}$</td>
<td style="text-align: center;">$\underline{5.74}$</td>
<td style="text-align: center;">$\underline{5.56}$</td>
<td style="text-align: center;">$\underline{5.84}$</td>
<td style="text-align: center;">$\underline{5.88}$</td>
<td style="text-align: center;">$\underline{5.92}$</td>
</tr>
</tbody>
</table>
<p>2021), MATH (Hendrycks et al., 2021), and GSM8K (Cobbe et al., 2021) to evaluate critique quality on coding and math tasks. We then generate model completions for the instructions in the same way as Section 2.2. We adopt two categories of models for comparison. First, we compare with four general-purpose models, gpt-3.5-turbo, LLaMA2-13B-Chat, Vicuna-13B-v1.5, and WizardLM-13B-v1.2. Then, we adopt two specifically trained critique models, SelFee ${ }^{4}$ and Shepherd (Wang et al., 2023c) ${ }^{5}$. We apply the baseline models and UltraCM to provide feedback on model completions respectively. Finally, we rate the quality of the critique from 1 to 7 using GPT-4, 1 being the worst and 7 being the best. The prompt is adapted from (Wang et al., 2023c).</p>
<p>Results. The scores of feedback quality are presented in Table 8. Overall, the performances of UltraCM almost approach gpt-3.5-turbo and dramatically surpass other models of both categories. To be specific, UltraCM achieves comparable performance with gpt-3.5-turbo on commonsense reasoning and mathematics reasoning. However, on AlpacaFarm and code datasets, UltraCM still exhibits deficiencies. Compared with two critique models, we find that (the communitytrained) Shepherd almost always fails to provide high-quality feedback. SelFee achieves the highest average scores after gpt-3.5-turbo and UltraCM, but it dramatically falls short on HumanEval and MATH. We highlight the comparison between UltraCM and the other three general-purpose models. All four models are trained from LLaMA2-13B, but UltraCM is the only one trained to provide textual critique rather than enhancing knowledge or reasoning capability. However, the feedback of UltraCM consistently gains higher scores than other models across all tasks and datasets, indicating that criticizing is a learnable task and employing an expert critic is more effective than an expert for downstream tasks in providing feedback. With more powerful backbone models, we believe UlTraFeEDBACK will greatly benefit autonomous agents (Park et al., 2023; Qin et al., 2023; Qian et al., 2023) and feedback learning (Yao et al., 2023; Shinn et al., 2023) research.</p>
<h1>E.4. Capability Experiments</h1>
<p>We use nine datasets in Section 5.2 to test the model capability. For world knowledge, we adopt NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). For commonsense reasoning, we use PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), OpenBookQA (Mihaylov et al., 2018), and ARC (Clark et al., 2018). For reading comprehension, we use BoolQ (Clark et al., 2019), RACE (Lai et al., 2017) and MultiRC (Khashabi et al., 2018).</p>
<p>For evaluation, we simply ask models to answer the questions directly with answers (e.g. with options A, B, C, D or Yes/No).</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 9. Relative scores (\%) versus gpt-3.5-turbo across different question types on UltraChat evaluation set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Vicuna <br> Set</th>
<th style="text-align: center;">Commonsense <br> Easy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">World Knowledge <br> Easy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Professional Knowledge <br> Physics</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Ability <br> Biology</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Writing</th>
<th style="text-align: center;">Overall</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">UltraLM-13B</td>
<td style="text-align: center;">95.6</td>
<td style="text-align: center;">113.7</td>
<td style="text-align: center;">106.8</td>
<td style="text-align: center;">111.7</td>
<td style="text-align: center;">103.3</td>
<td style="text-align: center;">102.1</td>
<td style="text-align: center;">105.1</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">98.8</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B-v1.3</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">113.4</td>
<td style="text-align: center;">106.4</td>
<td style="text-align: center;">109.6</td>
<td style="text-align: center;">107.1</td>
<td style="text-align: center;">106.0</td>
<td style="text-align: center;">108.9</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">98.8</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13B-v1.5</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">115.8</td>
<td style="text-align: center;">106.6</td>
<td style="text-align: center;">104.9</td>
<td style="text-align: center;">105.0</td>
<td style="text-align: center;">100.1</td>
<td style="text-align: center;">101.2</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">99.0</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-13B-Chat</td>
<td style="text-align: center;">97.1</td>
<td style="text-align: center;">114.6</td>
<td style="text-align: center;">108.5</td>
<td style="text-align: center;">109.3</td>
<td style="text-align: center;">107.7</td>
<td style="text-align: center;">105.9</td>
<td style="text-align: center;">108.0</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">100.2</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-33B-v1.3</td>
<td style="text-align: center;">98.5</td>
<td style="text-align: center;">113.4</td>
<td style="text-align: center;">114.0</td>
<td style="text-align: center;">105.1</td>
<td style="text-align: center;">109.0</td>
<td style="text-align: center;">109.9</td>
<td style="text-align: center;">112.8</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">86.7</td>
<td style="text-align: center;">103.0</td>
<td style="text-align: center;">102.4</td>
</tr>
<tr>
<td style="text-align: center;">WizardLM13B-v1.1</td>
<td style="text-align: center;">100.7</td>
<td style="text-align: center;">113.9</td>
<td style="text-align: center;">112.1</td>
<td style="text-align: center;">106.9</td>
<td style="text-align: center;">113.0</td>
<td style="text-align: center;">108.1</td>
<td style="text-align: center;">110.7</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">102.6</td>
<td style="text-align: center;">102.6</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA2-70B-Chat</td>
<td style="text-align: center;">100.5</td>
<td style="text-align: center;">116.5</td>
<td style="text-align: center;">106.7</td>
<td style="text-align: center;">111.5</td>
<td style="text-align: center;">109.0</td>
<td style="text-align: center;">106.6</td>
<td style="text-align: center;">109.4</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">103.6</td>
<td style="text-align: center;">103.2</td>
</tr>
<tr>
<td style="text-align: center;">OpenChat-13B-v3.2super</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">121.2</td>
<td style="text-align: center;">112.6</td>
<td style="text-align: center;">116.1</td>
<td style="text-align: center;">110.1</td>
<td style="text-align: center;">106.0</td>
<td style="text-align: center;">110.0</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">104.7</td>
<td style="text-align: center;">103.9</td>
</tr>
<tr>
<td style="text-align: center;">WizardLM13B-v1.2</td>
<td style="text-align: center;">102.5</td>
<td style="text-align: center;">122.0</td>
<td style="text-align: center;">110.3</td>
<td style="text-align: center;">114.3</td>
<td style="text-align: center;">111.7</td>
<td style="text-align: center;">108.6</td>
<td style="text-align: center;">109.0</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">79.7</td>
<td style="text-align: center;">103.8</td>
<td style="text-align: center;">104.9</td>
</tr>
<tr>
<td style="text-align: center;">UltraLM-13B-PPO</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">123.5</td>
<td style="text-align: center;">113.6</td>
<td style="text-align: center;">131.1</td>
<td style="text-align: center;">118.4</td>
<td style="text-align: center;">113.2</td>
<td style="text-align: center;">120.2</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">101.7</td>
<td style="text-align: center;">105.7</td>
</tr>
</tbody>
</table>
<p>We then match the output with the ground truth and calculate the exact match scores.</p>
<h1>E.5. Question Type Breakdown</h1>
<p>Table 9 reports the type-specific performance of our model and baselines compared with gpt-3.5-turbo. As is shown, our UltraLM-13B-PPO gets the highest average score, especially excels on the commonsense, world knowledge as well as professional knowledge questions. In the meantime, our model does not show advantages in math and reasoning tasks, which is consistent with the above results.</p>
<h2>F. Case Study</h2>
<h2>F.1. UltraFeedback</h2>
<p>We present a case sampled from ShareGPT in Table 10. The four assistants sampled to complete this instruction are Falcon-40B-Instruct, GPT-4, LLaMA2-70B-Chat, and UltraLM-13B. We also present GPT-4 annotations on the aspect of instruction following. GPT-4 accurately points out the lack of details in Falcon-40B-Instruct and GPT-4 responses.</p>
<h2>F.2. Best-of-11 Sampling</h2>
<p>We present a case comparing best-of-1, best-of-2, and best-of-16 sampling in Table 11. The best-of-1 sample is incorrect at all, so it received a low reward of -0.73 . To compare, the best-of-2 sample is partially correct, as it only addressed the @ symbol in decorating staticmethod. The example it provided is also problematic. The best-of-16 sample, however, correctly illustrated the usage of the @ symbol in Python with a valid example. Thus it received a much higher reward from the reward model. Another worth-noting point is that the best-of-2 sample is longer than the best-of-16 sample, which verified that our UltraRM does not depend on length shortcut to evaluate the response quality.</p>
<h2>F.3. UltraLM-13B-PPO</h2>
<p>We compare UltraLM-13B before and after PPO from an AlpacaEval case in Table 12. As we can see, the original UltraLM-13B made mistakes in describing Lady Gaga's characters in American Horror Story. The UltraLM-13B-PPO, on the other hand, correctly answered the question.</p>
<h2>F.4. UltraCM-13B</h2>
<p>To comprehensively evaluate and analyze the critiques our model generates, we conducted case study in Table 13. On GSM8K benchmark, we find that UltraCM-13B could give meaningful critiques on incorrect answers, pointing out flaws and provide suggestions. However, the suggestions are sometimes unnecessarily long, which indicates the imperfection of the model. We will continue to improve the critique quality in future work.</p>
<p>Table 10. An example of UltraFeedback comparing generations from 4 models from the aspect of instruction-following.
User: what's the general consensus best time to take vitamin D supplements.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Response</th>
<th style="text-align: left;">Annotation <br> (Instruction-Following)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Falcon-40B-Instruct: A good rule of thumb is to take vitamin D supplements in the <br> morning or afternoon so that they can be properly absorbed by the body. However, it is best to <br> consult with a healthcare professional to determine a specific dosing regimen for your individual <br> needs.</td>
<td style="text-align: left;">Rating: 3 <br> Rationale: The text partially complies <br> with the instruction, providing a general <br> rule of thumb for taking vitamin D sup- <br> plements. However, it lacks detail and <br> does not address the various factors that <br> may influence the best time to take the <br> supplements.</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4: The general consensus on the best time to take vitamin D supplements varies, as <br> individual needs and absorption rates can differ. However, many experts recommend taking <br> vitamin D supplements with a meal containing healthy fats for better absorption. <br> Some studies suggest that taking vitamin D supplements in the morning may be preferable to <br> nighttime due to its potential effects on sleep. Keep in mind that it is crucial to follow the dosage <br> recommended by a healthcare professional, as individual needs may vary. As always, it's best to <br> consult with a healthcare provider for personalized advice on supplement timing and dosage.</td>
<td style="text-align: left;">Rating: 4 <br> Rationale: The text almost fully aligns <br> with the instruction, providing a general <br> consensus on the best time to take vi- <br> tamin D supplements and mentioning <br> individual factors. However, it could <br> provide more detail on the various fac- <br> tors that may influence the best time to <br> take the supplements.</td>
</tr>
</tbody>
</table>
<p>LLaMA2-70B-Chat: Hello! I'm happy to help you with your question.
The general consensus among health professionals is that the best time to take vitamin D supplements depends on various factors, including your personal schedule, sun exposure, and health status. Here are some general guidelines to consider:</p>
<ol>
<li>Morning: Taking vitamin D supplements in the morning can help you start your day with adequate levels of this essential nutrient. This may be particularly helpful if you have a busy schedule and may not have time to take them later in the day.</li>
<li>After sun exposure: Vitamin D is produced in the skin when it's exposed to sunlight. Taking a vitamin D supplement after spending time outdoors can help boost your levels and ensure you're getting enough.</li>
<li>With a meal: Vitamin D is a fat-soluble vitamin, which means it's best absorbed when taken with a meal that contains healthy fats. Having a meal with a source of fat, such as avocado, nuts, or seeds, can enhance the absorption of vitamin D.</li>
<li>Consistency: The most important thing is to establish a consistent routine and take your vitamin D supplements at the same time every day. This will help maintain a stable level of vitamin D in your body.
It's important to note that excessive intake of vitamin D can be harmful, so it's essential to consult with a healthcare professional to determine the appropriate dosage for your individual needs. They can help you identify the best time to take vitamin D supplements based on your specific circumstances.
Remember, it's always a good idea to consult with a healthcare professional before starting any new supplement regimen. They can assess your individual needs and provide personalized recommendations for your health and well-being.
UltraLM-13B: According to current research and guidelines, the general consensus is that the best time to take vitamin D supplements is either in the morning or early afternoon, as our bodies produce more vitamin D when exposed to sunlight during this time. However, it's important to note that the optimal time to take vitamin D supplements can vary depending on individual factors such as sun exposure, geographical location, and overall health.
For people living in areas with limited sunlight or those who spend most of their time indoors, taking vitamin D supplements with a meal can be an effective way to ensure adequate intake. Some research also suggests that taking larger doses of vitamin D, such as 2,000-5,000 IU (international units), as a single dose a week or every few weeks, may be more effective for increasing vitamin D levels and maintaining them over time.
Keep in mind that the optimal time to take vitamin D supplements may also be influenced by medications or supplements you are currently taking, so it's best to consult with a healthcare professional for personalized advice based on your individual needs and circumstances.</li>
</ol>
<h2>Rating: 5</h2>
<p>Rationale: The text fully aligns with the instruction, providing a general consensus on the best time to take vitamin D supplements and considering individual factors. It also emphasizes the importance of consulting with a healthcare professional for personalized advice.</p>
<p>Table 11. Comparative cases of best-of-1, best-of-2, and best-of-16 sampling.
User: what does the @ in python do
Best-of-1: In Python, the @ symbol is used as a way to reference a class within its own namespace. This is often referred to as a class object. When you create an instance of a class, you are creating a new object that is stored in the namespace of the class. Using the @ symbol before a class declaration allows you to bypass. Reward: $\sim 0.73$
Best-of-2: The @ symbol in Python is used in front of a class to indicate that the class is decorated with the @staticmethod decorator. This decorator is sandwiched between the class definition and the name of the class. A static method in Python is a method that can be called directly on a class, like a regular method, but does not have access to the instance data (self). It usually takes no parameters and does not modify the object's state directly. Instead, it operates on the class itself. As a result, if you define a method as staticmethod, it will become a static method and you can call it on an instance of the class without having to create an instance of the class first. For example, a Rectangle class could define the perimeter method as a static method:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">@staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">perimeter</span><span class="p">(</span><span class="n">width</span><span class="p">,</span><span class="w"> </span><span class="n">height</span><span class="p">)</span><span class="err">:</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="mi">2</span><span class="o">*</span><span class="n">width</span><span class="w"> </span><span class="o">*</span><span class="mi">2</span><span class="o">*</span><span class="n">height</span>
</code></pre></div>

<p>You can then call this perimeter method on a Rectangle instance as follows:</p>
<div class="codehilite"><pre><span></span><code>rect -Rectangle(10, 5)
print(rect.perimeter()) # Output: 40
</code></pre></div>

<p>In this example, the Rectangle class defines a perimeter method, which is static by nature. The @staticmethod decorator simply serves to inform the Python interpreter that the method you are defining is indeed intended to be a static method. Moreover, you could also use this decorator with classmethod or lamdba function syntax. The '@staticmethod' decorator is by default aligned with classmethod, but the other formats are used for cases where you need more control over the function's self context, or more specific function memory. In summary, the @ symbol is a Python decorator that converts a method into a static method, which is able to be called on an instance of the class without requiring the creation of an actual instance. This functionality makes your code cleaner, more readable, and easier to maintain. Please note that Python has several other symbols and decorators that can modify the behavior of your code or enrich the functionality of your objects, such as @staticmethod, @classmethod, @property, and @newmethod. However, these will generally require some level of understanding and configuration before they can be effectively adapted for your applications. Remember that these decorators should not be confused with the general syntax to create a static method. If you don't see the @ symbol in front of a class method, then it is by default considered as a method. Adding static to the method implies decorating the method with the staticmethod decorator. To keep the code interesting, decorators are an elegant and expressive way to integrate various functional programming concepts into Python, improving encapsulation and abstraction across scripts and classes. Reward: $\sim 0.10$
Best-of-16: In Python, the @ symbol is used to define a decorator, which is a way to add functionality to or modify the behavior of functions or objects (not class methods). A decorator is essentially a decoration around the original function, executing some arbitrary code or modifying the function's properties before the function is executed. When a function is correctly decorated using the @ symbol, a decorator function is applied to it. The decorator function can then modify the behavior of the original function, by, for example, adding arguments, modifying attributes of the function, or executing additional code before or after the function is executed. Here's an example of how to use a simple decorator to add some additional functionality to a function:</p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">wraps</span>
<span class="k">def</span><span class="w"> </span><span class="nf">login_required</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
    <span class="nd">@wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">wrapper</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">username</span> <span class="o">!=</span><span class="s2">&quot;secret&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Please enter the correct username and password.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">func</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">wrapper</span>
<span class="nd">@login_required</span>
<span class="k">def</span><span class="w"> </span><span class="nf">secret_page</span><span class="p">(</span><span class="n">username</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Hello, </span><span class="si">{</span><span class="n">username</span><span class="si">}</span><span class="s2">! You are now on the secret page.&quot;</span><span class="p">)</span>
</code></pre></div>

<p>In this example, we are defining a decorator login_required to handle the authentication of our user. It will raise an exception if the provided 'username' argument is incorrect. And then we use this decorator to wrapper the interesting function secret_page with this restriction. This approach allows you to modify the behavior of a function at runtime, making it a very flexible and powerful tool for Python development. Reward: 0.42</p>
<p>Table 12. Comparative cases of UltraLM-13B and UltraLM-13B-PPO.
User: who does lady gaga play in american horror story
UltraLM-13B: The question is asking for the character that Lady Gaga has played in the television series American Horror Story. Lady Gaga has appeared in multiple seasons of the show and has played different characters in each one. The following are the characters Lady Gaga has played in the series:</p>
<ol>
<li>Season 1: American Horror Story (2011) - Gaga played the role of The Countess or Elizabeth Johnson, a wealthy and sophisticated owner of a Los Angeles apartment building who resides with her ventriloquist dummy.</li>
<li>Season 6: American Horror Story: Hotel (2016) - In this season, Lady Gaga portrayed German singer-songwriter Elizabeth along with her twin brother Dennis. They have gained fame in the 1970s with their horror rock collaboration, The Countess.
Gaga's portrayal in American Horror Story has added a unique dimension to the series, showcasing her acting talent, and has become a fan-favorite character.
UltraLM-13B-PPO: Lady Gaga played the role of The Countess in American Horror Story: Hotel. The Countess is a mysterious and enigmatic character who runs the Hotel Cortez, a luxurious and decadent hotel that serves as a front for a series of gruesome murders. The Countess is a seductive and charismatic figure who preys on her victims, using her beauty and charm to lure them into her web. Throughout the season, viewers are taken on a twisted and terrifying journey as they uncover the dark secrets of the Hotel Cortez and the sinister forces that lurk within its walls. Lady Gaga's performance in American Horror Story: Hotel was widely praised for its depth, complexity, and sheer intensity, cementing her status as one of the most talented and versatile actresses of her generation.</li>
</ol>
<h1>Table 13. Case of UltraCM-13B on GSM8K.</h1>
<p>User: A hotel in the Philippines charges 1000 pesos for a 12 -hour stay or 1600 pesos for a 24 -hour stay. After 12 hours, visitors have the option to add 70 pesos for every additional hour. Cameron arrives at 5 pm at the hotel and wants to leave at 10 am the next morning. He decides to go with the option of adding on 70 pesos for every hour after the 12 -hour mark instead of paying for 24 hours. How much money would Cameron save? Answer: 250
Vicuna-33B: First, let's determine the cost of Cameron's stay if he were to stay for 12 hours. Since he arrives at 5 pm , his stay would actually be 13 hours (from 5 pm to 6 am ). The cost for a 12 -hour stay is 1000 pesos.
Next, let's calculate the cost of a 24 -hour stay. The cost for a 24 -hour stay is 1600 pesos.
Now, we need to calculate the cost of Cameron's stay if he were to stay for 13 hours and pay the additional 70 pesos for every hour after the 12 -hour mark. For the first 12 hours, the cost is 1000 pesos. For the additional 1-hour period (from 12 hours to 13 hours), the cost is 70 pesos. Therefore, the total cost for Cameron's 13 -hour stay would be 1000 pesos +70 pesos $=1070$ pesos.
Now, we can calculate the amount Cameron would save by choosing the 13-hour option over the 24-hour option. The savings would be the difference between the two costs:
Savings $=1600$ pesos ( 24 -hour cost) - 1070 pesos ( 13 -hour cost)
avings $=530$ pesos
So, Cameron would save 530 pesos by choosing the option of adding 70 pesos for every additional hour after the 12 -hour mark instead of paying for 24 hours.
UltraCM-13B Critique: Your calculation of the hotel stay cost is incorrect. Cameron's stay from 5 pm to 10 am is 17 hours, not 13 hours. You should calculate the cost for a 17-hour stay instead. Also, you didn't need to calculate the savings by staying for 13 hours. Just focus on the correct calculation for the 17-hour stay. The rest of your answer is detailed and understandable, but the numbers are wrong due to the incorrect calculation of the hotel stay hours. Please revise this.
For improvement, always double-check your calculations to avoid such mistakes. Make sure to understand the problem clearly before you start solving it. Focus on what the question is asking you to find and ignore unnecessary details that might confuse you. As an AI, accuracy is key. A small mistake can lead to a wrong conclusion and misinformation, which is not our goal as AI assistants. Keep practicing and focus on your accuracy, try to avoid rush decisions and take your time to understand the questions fully before answering them. Overall Score: 6</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://huggingface.co/kaist-ai/selfee-13b
${ }^{5}$ Note that Wang et al. (2023c) did not open source their model weights, so we use the model from the community that has been trained on their data: https://huggingface.co/reciprocate/shepherd-13b&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
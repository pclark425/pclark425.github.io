<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8941 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8941</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8941</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-876eb375cb7b365475040046df669c039ad54202</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/876eb375cb7b365475040046df669c039ad54202" target="_blank">CodeT: Code Generation with Generated Tests</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes a novel method, CodeT, that leverages the same pre-trained language models to automatically generate test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios.</p>
                <p><strong>Paper Abstract:</strong> The task of generating code solutions for a given programming problem can benefit from the use of pre-trained language models such as Codex, which can produce multiple diverse samples. However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pre-trained language models. A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases, but the manual creation of such test cases is often costly and time-consuming. In this paper, we propose a novel method, CodeT, that leverages the same pre-trained language models to automatically generate test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios. CodeT then executes the code samples using the generated test cases, and performs a dual execution agreement, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples. We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS and CodeContests, using five different pre-trained language models with varying sizes and capabilities. Our results show that CodeT can significantly improve the performance of code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks. For instance, CodeT improves the pass@1 metric on HumanEval to 65.8%, which represents an absolute improvement of 18.8% over the code-davinci-002 model, and an absolute improvement of more than 20% over the previous state-of-the-art results.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8941.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8941.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CODET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CODE generation with generated Test-driven dual execution agreement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses a pre-trained code language model to generate both candidate solutions and synthetic test cases, then selects solutions via a test-driven dual execution agreement (consensus sets scored by number of solutions × number of passed tests).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex / InCoder / CodeGen (multiple models evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluated with OpenAI Codex family (code-cushman-001, code-davinci-001, code-davinci-002), InCoder-6B, and CodeGen-Mono-16B; Codex models are descendants of GPT-3 trained on large code corpora, InCoder/CodeGen are large encoder-decoder/decoder-only models specialized for code (sizes: Codex models unspecified, InCoder 6.7B, CodeGen 16B).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Dual execution agreement (test-driven consensus)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate many code samples X and many test cases Y from the same language model via prompting; execute each x∈X on each y∈Y; group solutions that pass the same subset of tests into consensus sets S (S_x = solutions, S_y = tests passed); score consensus sets by f(S)=|S_x|·|S_y| (practically using sqrt(|S_x|) to reduce code-solution weight) and pick top-ranked solution(s). Implemented either via random pair sampling (RANSAC-like) or exhaustive pairing when feasible.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code generation benchmarks (HumanEval, MBPP, APPS, CodeContests)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a function implementation from a natural-language/code prompt; correctness measured by pass@k against ground-truth unit tests (HumanEval, MBPP) or benchmark-specific evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>HumanEval (code-davinci-002) pass@1 = 65.8% (CODET); MBPP (code-davinci-002) pass@1 = 67.7%; APPS Introductory (code-davinci-002) pass@1 = 34.6%; CodeContests (code-davinci-002) pass@1 = 2.1%. (Settings: CODET uses generated test cases; typical sampling: 100 code samples, 100 test-case samples (extract up to 5 assertions per sample) for HumanEval/MBPP.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baselines (no CODET selection) reported: HumanEval (code-davinci-002) pass@1 = 47.0%; MBPP (code-davinci-002) pass@1 = 58.1%; APPS Introductory baseline pass@1 = 27.2%; CodeContests baseline pass@1 = 0.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Prompt-engineered generation of synthetic test cases from the same LM; execution of generated code on generated tests; grouping by functional agreement; scoring consensus sets combining number of agreeing solutions and number of passed tests. No additional trained verifier or human labels required.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative gains across models/benchmarks: e.g., code-davinci-002 HumanEval pass@1 improved from 47.0% to 65.8% (absolute +18.8%); improvements observed for multiple models and datasets; ablations show combining both solution-agreement and test-count (f=|S_x||S_y| with sqrt weighting) outperforms scoring by solutions-only or tests-only.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires executable problems (CODET only applies when candidate code can be executed). Extra computation cost for generating and running synthetic tests. Failure cases include: generated test cases that are 'toxic' (pass incorrect solutions but not canonical correct solution), model misunderstandings or ambiguous problem statements, uncovered corner cases, lack of imports or formatting issues; authors found examples where correct solution exists but is not top-1 due to these issues (e.g., 53/164 problems for code-cushman-001 on HumanEval had a correct solution not ranked top-1; ~20% of these due to ambiguity/uncovered corner cases, rest due to model misunderstanding).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared experimentally to: a replication of AlphaCode clustering (AlphaCode-C) and baseline random selection (pass@k). CODET outperforms AlphaCode-C and baseline by significant margins. Ablation compared CODET to variants that score only by number of solutions (self-consistency-style) or only by number of test cases; CODET's combined scoring is superior. Also related to generate-&-rank and verifier-based ranking in prior work (cited) but CODET requires no additional trained ranker or labeled data.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablations: (1) scoring variants: solution-only (f' = |S_x|) and test-only (f'' = |S_y|) both perform worse than CODET; example (HumanEval, code-davinci-002): f' pass@1 = 55.9%, f'' pass@1 = 58.4%, CODET pass@1 = 65.8% (baseline 47.0%). (2) De-duplication of solutions/tests has minor and inconsistent effects. (3) Weighting: using sqrt(|S_x|) to downweight cluster size vs test count improves performance. (4) Test-case quantity: more generated tests generally help, but marginal gains diminish beyond ~50 samples and limit≥3; even with only 10 tests CODET still improved pass@1 (e.g., code-davinci-002 +10 tests gave +9.5% over baseline). (5) Temperature sensitivity: higher sampling temperature (0.8) improved diversity and CODET performance.)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeT: Code Generation with Generated Tests', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8941.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8941.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistency (ablation f')</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (as operationalized by CODET ablation f')</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The idea of aggregating multiple independent model outputs (e.g., by majority / cluster size) to improve reliability; in this paper implemented as ranking consensus sets by number of solutions only (f' = |S_x|).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Not a single LM (method applied to model outputs); ablation evaluated with Codex variants (code-cushman-001, code-davinci-001, code-davinci-002).</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method that samples multiple outputs from an LM (via high-temperature sampling) and aggregates via majority vote / cluster size; in ablation applied to code generation outputs produced by Codex family.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-consistency / majority-by-cluster-size</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Group multiple generated solutions by functional equivalence (here: passing the same generated tests) and rank clusters by size; select top solutions from largest cluster (no test-count weighting).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HumanEval (code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See CODET entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Example: HumanEval pass@1 (ablation f') — code-davinci-002 f' pass@1 = 55.9% (from Table 13).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Baseline pass@1 for code-davinci-002 = 47.0% (random selection from samples).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Aggregation of multiple sampled outputs and selecting the most common solution behavior (implemented by grouping outputs by passed synthetic tests in this paper's ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>f' improves over baseline in some settings (e.g., baseline 47.0% → f' 55.9% for code-davinci-002 pass@1), but is still substantially worse than CODET combined scoring (65.8%). The paper reports that scoring by solutions-only performs consistently worse than the combined test+solution scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Clustering by solution agreement alone can be misled by large clusters of incorrect or trivial solutions; AlphaCode-C-like clustering can cluster trivial incorrect solutions together producing large incorrect clusters and hurting selection.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly (ablation) to full CODET scoring (combined test+solution) and test-only scoring; full CODET outperforms self-consistency-style scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Table 13 reports f' results: for code-davinci-002 pass@1 = 55.9% (worse than CODET 65.8%); other Codex variants similarly show reduced performance relative to CODET.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeT: Code Generation with Generated Tests', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8941.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8941.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generate-&-Rank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generate & Rank (multi-task framework for math word problems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior approach that jointly generates candidate solutions and ranks them (often via a trained ranker or verifier), referenced as related work for iterative generate-and-evaluate ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generate & rank: A multi-task framework for math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Generate & Rank</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate multiple candidate outputs then rank them (often using a trained verifier or joint training) to choose the best output; cited as prior art in ranking multiple generated answers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Math word problem solving (original work cited)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve math word problems by generating candidate solutions and using a ranker to select the best.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Mentioned as involving training a verifier or jointly training generator and ranker (cite Shen et al., 2021) — external learned ranker rather than purely self-generated tests.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not evaluated in this paper; mentioned as requiring training a ranker (contrast with CODET which needs no extra training).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned as prior work that requires training; CODET contrasted as zero-shot and not needing labeled data or separate rankers.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeT: Code Generation with Generated Tests', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8941.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8941.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaCode-C (clustering)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaCode clustering method (replicated as AlphaCode-C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method inspired by AlphaCode that clusters solutions by functional outputs on generated inputs, then ranks clusters by size to select solutions; authors implement a replication (AlphaCode-C) for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Competition-level code generation with alphacode</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (selection method)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Clustering selection method; original AlphaCode uses a trained test-input generator and clustering to find consensus solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Clustering by output agreement (AlphaCode-C)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate test inputs, run candidate programs to collect outputs, group programs by outputs (functional equivalence) and rank clusters by size to select solutions; replication in this paper uses CODET-generated tests as inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code generation benchmarks (HumanEval, MBPP) as baseline comparison</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See CODET entry.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>AlphaCode-C replication results are reported in Table 2; generally inferior to CODET across models/benchmarks (no single-number summarization in text).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External clustering based on outputs on generated inputs; relies on separate test-input generation in original AlphaCode (replicated here using CODET tests).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Not an improvement in this paper: AlphaCode-C is consistently inferior to CODET on HumanEval and MBPP using different models (authors attribute difference to use of test-case information in CODET and issues with trivial solution clustering in AlphaCode-C).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>AlphaCode-C can cluster many trivial incorrect solutions together (e.g., constant or identity-returning solutions) producing large incorrect clusters and hurting selection. Original AlphaCode requires a separately trained test-input generator which is unavailable; replication used CODET tests but still underperformed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared experimentally to CODET and baseline; CODET outperforms AlphaCode-C.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeT: Code Generation with Generated Tests', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8941.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8941.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fault-aware neural code ranker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fault-aware neural code rankers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A trained neural ranker that is aware of faults to rank generated code solutions; mentioned as related work for learned ranking approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fault-aware neural code rankers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Fault-aware ranking</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Train a ranker to predict correctness/faults of generated code and use it to select top candidates; contrasted with CODET's zero-shot test-driven selection.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Code generation ranking</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Rank generated code by predicted correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Learned model (trained ranker) rather than prompt-only or execution-driven self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires labeled data and ranker training; contrasted by authors as not zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Positioned as alternative to CODET; CODET does not require additional training or labels.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CodeT: Code Generation with Generated Tests', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Generate & rank: A multi-task framework for math word problems <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Fault-aware neural code rankers <em>(Rating: 2)</em></li>
                <li>Competition-level code generation with alphacode <em>(Rating: 2)</em></li>
                <li>On the advance of making language models better reasoners <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8941",
    "paper_id": "paper-876eb375cb7b365475040046df669c039ad54202",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "CODET",
            "name_full": "CODE generation with generated Test-driven dual execution agreement",
            "brief_description": "A method that uses a pre-trained code language model to generate both candidate solutions and synthetic test cases, then selects solutions via a test-driven dual execution agreement (consensus sets scored by number of solutions × number of passed tests).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codex / InCoder / CodeGen (multiple models evaluated)",
            "model_description": "Evaluated with OpenAI Codex family (code-cushman-001, code-davinci-001, code-davinci-002), InCoder-6B, and CodeGen-Mono-16B; Codex models are descendants of GPT-3 trained on large code corpora, InCoder/CodeGen are large encoder-decoder/decoder-only models specialized for code (sizes: Codex models unspecified, InCoder 6.7B, CodeGen 16B).",
            "reflection_method_name": "Dual execution agreement (test-driven consensus)",
            "reflection_method_description": "Generate many code samples X and many test cases Y from the same language model via prompting; execute each x∈X on each y∈Y; group solutions that pass the same subset of tests into consensus sets S (S_x = solutions, S_y = tests passed); score consensus sets by f(S)=|S_x|·|S_y| (practically using sqrt(|S_x|) to reduce code-solution weight) and pick top-ranked solution(s). Implemented either via random pair sampling (RANSAC-like) or exhaustive pairing when feasible.",
            "task_name": "Code generation benchmarks (HumanEval, MBPP, APPS, CodeContests)",
            "task_description": "Generate a function implementation from a natural-language/code prompt; correctness measured by pass@k against ground-truth unit tests (HumanEval, MBPP) or benchmark-specific evaluation.",
            "performance_with_reflection": "HumanEval (code-davinci-002) pass@1 = 65.8% (CODET); MBPP (code-davinci-002) pass@1 = 67.7%; APPS Introductory (code-davinci-002) pass@1 = 34.6%; CodeContests (code-davinci-002) pass@1 = 2.1%. (Settings: CODET uses generated test cases; typical sampling: 100 code samples, 100 test-case samples (extract up to 5 assertions per sample) for HumanEval/MBPP.)",
            "performance_without_reflection": "Baselines (no CODET selection) reported: HumanEval (code-davinci-002) pass@1 = 47.0%; MBPP (code-davinci-002) pass@1 = 58.1%; APPS Introductory baseline pass@1 = 27.2%; CodeContests baseline pass@1 = 0.7%.",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Prompt-engineered generation of synthetic test cases from the same LM; execution of generated code on generated tests; grouping by functional agreement; scoring consensus sets combining number of agreeing solutions and number of passed tests. No additional trained verifier or human labels required.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative gains across models/benchmarks: e.g., code-davinci-002 HumanEval pass@1 improved from 47.0% to 65.8% (absolute +18.8%); improvements observed for multiple models and datasets; ablations show combining both solution-agreement and test-count (f=|S_x||S_y| with sqrt weighting) outperforms scoring by solutions-only or tests-only.",
            "limitations_or_failure_cases": "Requires executable problems (CODET only applies when candidate code can be executed). Extra computation cost for generating and running synthetic tests. Failure cases include: generated test cases that are 'toxic' (pass incorrect solutions but not canonical correct solution), model misunderstandings or ambiguous problem statements, uncovered corner cases, lack of imports or formatting issues; authors found examples where correct solution exists but is not top-1 due to these issues (e.g., 53/164 problems for code-cushman-001 on HumanEval had a correct solution not ranked top-1; ~20% of these due to ambiguity/uncovered corner cases, rest due to model misunderstanding).",
            "comparison_to_other_methods": "Compared experimentally to: a replication of AlphaCode clustering (AlphaCode-C) and baseline random selection (pass@k). CODET outperforms AlphaCode-C and baseline by significant margins. Ablation compared CODET to variants that score only by number of solutions (self-consistency-style) or only by number of test cases; CODET's combined scoring is superior. Also related to generate-&-rank and verifier-based ranking in prior work (cited) but CODET requires no additional trained ranker or labeled data.",
            "ablation_study_results": "Ablations: (1) scoring variants: solution-only (f' = |S_x|) and test-only (f'' = |S_y|) both perform worse than CODET; example (HumanEval, code-davinci-002): f' pass@1 = 55.9%, f'' pass@1 = 58.4%, CODET pass@1 = 65.8% (baseline 47.0%). (2) De-duplication of solutions/tests has minor and inconsistent effects. (3) Weighting: using sqrt(|S_x|) to downweight cluster size vs test count improves performance. (4) Test-case quantity: more generated tests generally help, but marginal gains diminish beyond ~50 samples and limit≥3; even with only 10 tests CODET still improved pass@1 (e.g., code-davinci-002 +10 tests gave +9.5% over baseline). (5) Temperature sensitivity: higher sampling temperature (0.8) improved diversity and CODET performance.)",
            "uuid": "e8941.0",
            "source_info": {
                "paper_title": "CodeT: Code Generation with Generated Tests",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Self-consistency (ablation f')",
            "name_full": "Self-consistency (as operationalized by CODET ablation f')",
            "brief_description": "The idea of aggregating multiple independent model outputs (e.g., by majority / cluster size) to improve reliability; in this paper implemented as ranking consensus sets by number of solutions only (f' = |S_x|).",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "use",
            "model_name": "Not a single LM (method applied to model outputs); ablation evaluated with Codex variants (code-cushman-001, code-davinci-001, code-davinci-002).",
            "model_description": "Method that samples multiple outputs from an LM (via high-temperature sampling) and aggregates via majority vote / cluster size; in ablation applied to code generation outputs produced by Codex family.",
            "reflection_method_name": "Self-consistency / majority-by-cluster-size",
            "reflection_method_description": "Group multiple generated solutions by functional equivalence (here: passing the same generated tests) and rank clusters by size; select top solutions from largest cluster (no test-count weighting).",
            "task_name": "HumanEval (code generation)",
            "task_description": "See CODET entry.",
            "performance_with_reflection": "Example: HumanEval pass@1 (ablation f') — code-davinci-002 f' pass@1 = 55.9% (from Table 13).",
            "performance_without_reflection": "Baseline pass@1 for code-davinci-002 = 47.0% (random selection from samples).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Aggregation of multiple sampled outputs and selecting the most common solution behavior (implemented by grouping outputs by passed synthetic tests in this paper's ablation).",
            "number_of_iterations": null,
            "evidence_for_improvement": "f' improves over baseline in some settings (e.g., baseline 47.0% → f' 55.9% for code-davinci-002 pass@1), but is still substantially worse than CODET combined scoring (65.8%). The paper reports that scoring by solutions-only performs consistently worse than the combined test+solution scoring.",
            "limitations_or_failure_cases": "Clustering by solution agreement alone can be misled by large clusters of incorrect or trivial solutions; AlphaCode-C-like clustering can cluster trivial incorrect solutions together producing large incorrect clusters and hurting selection.",
            "comparison_to_other_methods": "Compared directly (ablation) to full CODET scoring (combined test+solution) and test-only scoring; full CODET outperforms self-consistency-style scoring.",
            "ablation_study_results": "Table 13 reports f' results: for code-davinci-002 pass@1 = 55.9% (worse than CODET 65.8%); other Codex variants similarly show reduced performance relative to CODET.",
            "uuid": "e8941.1",
            "source_info": {
                "paper_title": "CodeT: Code Generation with Generated Tests",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Generate-&-Rank",
            "name_full": "Generate & Rank (multi-task framework for math word problems)",
            "brief_description": "A prior approach that jointly generates candidate solutions and ranks them (often via a trained ranker or verifier), referenced as related work for iterative generate-and-evaluate ideas.",
            "citation_title": "Generate & rank: A multi-task framework for math word problems",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Generate & Rank",
            "reflection_method_description": "Generate multiple candidate outputs then rank them (often using a trained verifier or joint training) to choose the best output; cited as prior art in ranking multiple generated answers.",
            "task_name": "Math word problem solving (original work cited)",
            "task_description": "Solve math word problems by generating candidate solutions and using a ranker to select the best.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Mentioned as involving training a verifier or jointly training generator and ranker (cite Shen et al., 2021) — external learned ranker rather than purely self-generated tests.",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": "Not evaluated in this paper; mentioned as requiring training a ranker (contrast with CODET which needs no extra training).",
            "comparison_to_other_methods": "Positioned as prior work that requires training; CODET contrasted as zero-shot and not needing labeled data or separate rankers.",
            "ablation_study_results": null,
            "uuid": "e8941.2",
            "source_info": {
                "paper_title": "CodeT: Code Generation with Generated Tests",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "AlphaCode-C (clustering)",
            "name_full": "AlphaCode clustering method (replicated as AlphaCode-C)",
            "brief_description": "A method inspired by AlphaCode that clusters solutions by functional outputs on generated inputs, then ranks clusters by size to select solutions; authors implement a replication (AlphaCode-C) for comparison.",
            "citation_title": "Competition-level code generation with alphacode",
            "mention_or_use": "use",
            "model_name": "N/A (selection method)",
            "model_description": "Clustering selection method; original AlphaCode uses a trained test-input generator and clustering to find consensus solutions.",
            "reflection_method_name": "Clustering by output agreement (AlphaCode-C)",
            "reflection_method_description": "Generate test inputs, run candidate programs to collect outputs, group programs by outputs (functional equivalence) and rank clusters by size to select solutions; replication in this paper uses CODET-generated tests as inputs.",
            "task_name": "Code generation benchmarks (HumanEval, MBPP) as baseline comparison",
            "task_description": "See CODET entry.",
            "performance_with_reflection": "AlphaCode-C replication results are reported in Table 2; generally inferior to CODET across models/benchmarks (no single-number summarization in text).",
            "performance_without_reflection": null,
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External clustering based on outputs on generated inputs; relies on separate test-input generation in original AlphaCode (replicated here using CODET tests).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Not an improvement in this paper: AlphaCode-C is consistently inferior to CODET on HumanEval and MBPP using different models (authors attribute difference to use of test-case information in CODET and issues with trivial solution clustering in AlphaCode-C).",
            "limitations_or_failure_cases": "AlphaCode-C can cluster many trivial incorrect solutions together (e.g., constant or identity-returning solutions) producing large incorrect clusters and hurting selection. Original AlphaCode requires a separately trained test-input generator which is unavailable; replication used CODET tests but still underperformed.",
            "comparison_to_other_methods": "Compared experimentally to CODET and baseline; CODET outperforms AlphaCode-C.",
            "ablation_study_results": null,
            "uuid": "e8941.3",
            "source_info": {
                "paper_title": "CodeT: Code Generation with Generated Tests",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Fault-aware neural code ranker",
            "name_full": "Fault-aware neural code rankers",
            "brief_description": "A trained neural ranker that is aware of faults to rank generated code solutions; mentioned as related work for learned ranking approaches.",
            "citation_title": "Fault-aware neural code rankers",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "reflection_method_name": "Fault-aware ranking",
            "reflection_method_description": "Train a ranker to predict correctness/faults of generated code and use it to select top candidates; contrasted with CODET's zero-shot test-driven selection.",
            "task_name": "Code generation ranking",
            "task_description": "Rank generated code by predicted correctness.",
            "performance_with_reflection": null,
            "performance_without_reflection": null,
            "has_performance_comparison": false,
            "mechanism_of_reflection": "Learned model (trained ranker) rather than prompt-only or execution-driven self-reflection.",
            "number_of_iterations": null,
            "evidence_for_improvement": null,
            "limitations_or_failure_cases": "Requires labeled data and ranker training; contrasted by authors as not zero-shot.",
            "comparison_to_other_methods": "Positioned as alternative to CODET; CODET does not require additional training or labels.",
            "ablation_study_results": null,
            "uuid": "e8941.4",
            "source_info": {
                "paper_title": "CodeT: Code Generation with Generated Tests",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Generate & rank: A multi-task framework for math word problems",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Fault-aware neural code rankers",
            "rating": 2
        },
        {
            "paper_title": "Competition-level code generation with alphacode",
            "rating": 2
        },
        {
            "paper_title": "On the advance of making language models better reasoners",
            "rating": 2
        }
    ],
    "cost": 0.016784,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CODET: CODE GENERATION WITH GENERATED TESTS</h1>
<p>Bei Chen<em>, Fengji Zhang</em>, Anh Nguyen*, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, Weizhu Chen<br>Microsoft Corporation<br>{beichen, v-fengjzhang, anhnguyen, v-dazan, zeqi.lin, jlou, wzchen}@microsoft.com</p>
<h4>Abstract</h4>
<p>The task of generating code solutions for a given programming problem can benefit from the use of pre-trained language models such as Codex, which can produce multiple diverse samples. However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pretrained language models. A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases, but the manual creation of such test cases is often costly and time-consuming. In this paper, we propose a novel method, CODET, that leverages the same pre-trained language models to automatically generate test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios. CODET then executes the code samples using the generated test cases and performs a dual execution agreement, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples. We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS, and CodeContests, using five different pre-trained language models with varying sizes and capabilities. Our results show that CODET can significantly improve the performance of code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks. For instance, CODET improves the pass@1 metric on HumanEval to $65.8 \%$, which represents an absolute improvement of $18.8 \%$ over the code-davinci-002 model, and an absolute improvement of more than $20 \%$ over the previous state-of-the-art results.</p>
<h2>1 INTRODUCTION</h2>
<p>Despite the remarkable progress in pre-training techniques for code generation, selecting a single correct solution from multiple candidates generated by large language models remains a hard problem. For instance, Codex (Chen et al., 2021), a state-of-the-art pre-trained language model for code generation, can achieve a pass@100 (pass if one or more among 100 generated solutions for a given problem can pass the corresponding test cases) of $77.4 \%$, but a pass@1 (correct rate of a single solution) of only $33.5 \%$ on the HumanEval benchmark (Chen et al., 2021) ${ }^{1}$. This huge gap limits the practical usefulness of code generation models and motivates us to explore how to pick the correct or best solution from multiple candidates.</p>
<p>A straightforward way to verify the correctness of a solution is to execute it and check if it passes all corresponding test cases. This execution-guided approach has been widely adopted in various code-related tasks, such as code generation (Chen et al., 2021; Li et al., 2022b; Shi et al., 2022), code translation (Roziere et al., 2021), and program synthesis (Chen et al., 2018; Ellis et al., 2019). However, this approach relies heavily on the quality and quantity of test cases, which are often costly and time-consuming to create and maintain. Moreover, in real-world applications like Copilot ${ }^{2}$, a code generation tool that assists developers in writing code, it is unrealistic to expect users to provide test cases for every problem they want to solve. Therefore, we propose to automatically generate test cases for arbitrary programming problems and use them to quickly verify any solution.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The illustration of CODET. Both the code solutions and the test cases are generated by the pre-trained language model. The best code solution is then selected by a dual execution agreement.</p>
<p>In this paper, we propose CODET: CODE generation with generated Test-driven dual execution agreement, as illustrated in Figure 1. First, we leverage the same pre-trained language model that generates code solutions, such as Codex, to generate a large number of test cases for each programming problem by providing an elaborate instruction as prompt. Next, we use a dual execution agreement approach inspired by the classical RANSAC algorithm (Fischler \&amp; Bolles, 1981). We execute each generated code solution on each generated test case, and iteratively find multiple groups of code solution and test case pairs. Each group, or consensus set, has solutions that pass the same test cases, indicating that they have the same functionality, even if they are different in implementation. We expect that a solution that passes more test cases is more correct, and that a solution that has more similar solutions, i.e., solutions in the same consensus set, is more consistent with the problem specification. So, we rank each consensus set by both the number of test cases and solutions in it, and choose the best solution from the highest-ranked consensus set.</p>
<p>Our method is simple and efficient, as it does not require any labelled data or additional rankers, but it achieves surprisingly exceptional performance. We evaluate our method on five different pre-trained language models for code generation: three OpenAI Codex models (Chen et al., 2021), InCoder (Fried et al., 2022b), and CodeGen (Nijkamp et al., 2022), as well as four established benchmarks for code generation: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), APPS (Hendrycks et al., 2021), and CodeContests (Li et al., 2022b). The experimental results show that our method can effectively select the correct solution from multiple candidates, improving the pass@1 score significantly on all benchmarks in the zero-shot setting. For instance, CODET achieves improvements using code-davinci-002: HumanEval ( $47.0 \% \rightarrow 65.8 \%$ ), MBPP ( $58.1 \% \rightarrow 67.7 \%$ ), APPS IntroDUCTORY $(27.2 \% \rightarrow 34.6 \%)$, and CodeContests $(0.7 \% \rightarrow 2.1 \%)$. Moreover, when we combine code-davinci-002, the most powerful pre-trained model, and CODET, we outperform previous state-of-the-art methods by a large margin, e.g., HumanEval: $42.7 \%$ (Inala et al., 2022) $\rightarrow 65.8 \%$. We also conduct a thorough analysis to provide more insights. Our work is publicly available at https://github.com/microsoft/CodeT.</p>
<h1>2 Methodology</h1>
<p>The task of code generation is to solve a programming problem: generate code solution $x$ based on context $c$. As shown in Figure 2, context $c$ contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header. A code solution is a code snippet that solves the programming problem described in the context. Generally, we sample a set of code solutions, denoted as $\mathbf{X}=\left{x_{1}, x_{2}, \cdots, x_{N}\right}$, based on the context $c$ using a pre-trained language model $\mathcal{M}$, which can be formulated as $\mathbf{X}=\mathcal{M}(c)$. Our goal is to select the best code solution $\hat{x}$ from the set of generated code solutions $\mathbf{X}$, where $\hat{x}$ is the most likely solution to correctly solve the given programming problem. To this end, we propose CODET in the hope of unleashing the inherent power of the pre-trained language model $\mathcal{M}$. Specifically, we use $\mathcal{M}$ to generate test cases for the programming problem (Section 2.1), and then select the best code solution $\hat{x}$ based on a dual execution agreement (Section 2.2).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Code generation and test case generation: an example from the HumanEval benchmark. Example input-output cases are removed from the context.</p>
<h1>2.1 Test Case Generation</h1>
<p>Besides generating code solutions, we also need to generate test cases to evaluate the correctness of the code solutions. A test case is a pair of input and expected output for the function defined in the context. For example, in Figure 2, a test case for the programming problem of checking whether there exist close elements in a list less than a threshold. To generate test cases, we use the same pre-trained language model $\mathcal{M}$ that we use for generating code solutions, but we add an instruction $p$ to the context $c$ as a prompt to indicate that we want test cases instead of code solutions. As shown in Figure 2, the instruction $p$ consists of three parts: (1) a "pass" statement as a placeholder of the function body, which signals that we do not need to generate code for the function, (2) a comment "check the correctness of [entry point]" to clarify the intention of generating test cases, where "[entry point]" is the name of the function, and (3) an "assert" statement to start the test case generation, which specifies the format of the test cases as input-output pairs.
We then feed the concatenated context and instruction, concat $(c, p)$, to the language model $\mathcal{M}$, and sample a set of test cases, denoted as $\mathbf{Y}=\left{y_{1}, y_{2}, \cdots, y_{M}\right}$, from the model output. The process of test case generation can be formulated as $\mathbf{Y}=\mathcal{M}(\operatorname{concat}(c, p))$. The language model will try to complete the instruction by generating plausible input-output pairs for the function. Note that we remove all example input-output cases from the context $c$ before generating code solutions and test cases, to avoid exposing real test cases to the language model and to increase the diversity and difficulty of the generated test cases.</p>
<h3>2.2 Dual Execution Agreement</h3>
<p>In this subsection, we explain how we select the best code solution $\hat{x}$ from the set of generated code solutions $\mathbf{X}=\left{x_{1}, x_{2}, \cdots, x_{N}\right}$, using the set of generated test cases $\mathbf{Y}=\left{y_{1}, y_{2}, \cdots, y_{M}\right}$ as a criterion. We can execute a code solution $x$ on a test case $y$, which means running the function defined by $x$ on the input part of $y$ and comparing the output with the output part of $y$. If the code solution $x$ can be executed without errors and the output matches the expected output, then we say the code solution $x$ can pass the test case $y$. Furthermore, we say there is a functionality agreement between two code solutions $x_{i}$ and $x_{j}$ if they can pass the same set of test cases in $\mathbf{Y}$. Our approach is based on the following assumptions: (1) the code solutions and the test cases are independently and randomly sampled from the pre-trained language model $\mathcal{M}$ given a certain programming problem, and (2) incorrect code solutions are often diverse, and the probability of having a functionality agreement between two incorrect code solutions by chance is very low. These assumptions are similar to those of the classical RANSAC algorithm (Fischler \&amp; Bolles, 1981), which is a robust method for finding consensus among noisy data. Inspired by RANSAC, we propose our approach CODET to perform dual execution agreement, which is an iterative approach as follows:</p>
<ul>
<li>We randomly select a pair $(x, y)$ from the set of all possible pairs $\mathcal{D}={(x, y) \mid x \in \mathbf{X}, y \in$ $\mathbf{Y}}$. We then try to execute the code solution $x$ on the test case $y$. If $x$ can pass $y$, then we say that the pair $(x, y)$ is a hypothetical inlier, because it hypothetically describes the correct functionality for the programming problem. Otherwise, we say that $(x, y)$ is an outlier, because it fails to describe the correct functionality. Figure 3 shows a simple example of the programming problem "return the square of a number". $\left(x_{1}, y_{1}\right)$ and $\left(x_{3}, y_{2}\right)$ are two of the hypothetical inliers, while $\left(x_{1}, y_{4}\right)$ and $\left(x_{3}, y_{1}\right)$ are two of the outliers.</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A simple example of the programming problem "return the square of a number". The gray line between $x$ and $y$ indicates that $x$ can pass $y$, i.e., $(x, y)$ is a hypothetical inlier. The green or purple box indicates a consensus set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">Problems</th>
<th style="text-align: center;">GT Tests</th>
<th style="text-align: center;">$n$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;">164</td>
<td style="text-align: center;">7.77</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;">427</td>
<td style="text-align: center;">3.1</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">APPS</td>
<td style="text-align: center;">IntroDUCTORY</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INTERVIEW</td>
<td style="text-align: center;">3,000</td>
<td style="text-align: center;">20.99</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPETITION</td>
<td style="text-align: center;">1,000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">CodeContests</td>
<td style="text-align: center;">165</td>
<td style="text-align: center;">203.7</td>
<td style="text-align: center;">1,000</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of benchmarks: the total number of problems in the benchmark (Problems), the average number of ground-truth test cases per problem (GT Tests), and the number of sampling code solutions for each problem $(n)$.</p>
<ul>
<li>If $(x, y)$ is a hypothetical inlier, we collect all other pairs from $\mathcal{D}$ that agree with this hypothetical inlier, forming a set $\mathcal{S}$ called consensus set. To find the pairs that agree with $(x, y)$, we first find all test cases that $x$ can pass, denoted as $\mathcal{S}<em x="x">{y}$. Then, we find all code solutions that can pass exactly the same test cases as $x$, denoted as $\mathcal{S}</em>}$. Finally, the consensus set is the set of all pairs that consist of a code solution from $\mathcal{S<em y="y">{x}$ and a test case from $\mathcal{S}</em>}$, i.e., $\mathcal{S}=\left{(x, y) \mid x \in \mathcal{S<em y="y">{x}, y \in \mathcal{S}</em>}\right}$. For example in Figure 3, we can get $\mathcal{S<em 1="1">{x}=\left{x</em>}, x_{2}\right}, \mathcal{S<em 1="1">{y}=\left{y</em>}, y_{2}, y_{3}\right}$ from the hypothetical inlier $\left(x_{1}, y_{1}\right)$ (shown in green box), and $\mathcal{S<em 3="3">{x}=\left{x</em>}\right}, \mathcal{S<em 2="2">{y}=\left{y</em>\right)$ (shown in purple box).}, y_{3}, y_{4}, y_{5}\right}$ from $\left(x_{3}, y_{2</li>
<li>We score the consensus set as $f(\mathcal{S})=\left|\mathcal{S}<em y="y">{x}\right|\left|\mathcal{S}</em>}\right|$, where $\left|\mathcal{S<em x="x">{x}\right|$ is the number of code solutions in $\mathcal{S}</em>}$ and $\left|\mathcal{S<em y="y">{y}\right|$ is the number of test cases in $\mathcal{S}</em>\right)$, respectively.}$. This score is equal to the number of pairs in the consensus set. The intuition is that the more pairs that agree with the hypothetical functionality, the more likely this functionality is correct, according to our assumptions. Following the example in Figure 3, the consensus set scores are 6 and 4 for the hypothetical inliers $\left(x_{1}, y_{1}\right)$ and $\left(x_{3}, y_{2</li>
</ul>
<p>We repeat the above procedure for a fixed number of times, each time producing a consensus set with its score. Finally, we get the best code solution $\hat{x}$ by selecting any code solution from the consensus set with the highest score. If we want to obtain $k$ code solutions, we can select the top $k$ consensus sets with the highest scores, and one code solution is picked up from each of the $k$ consensus sets.</p>
<p>In practice, when the number of code solutions in $\mathbf{D}$ is not large, we can simplify the above method by examining all possible pairs in $\mathcal{D}$, instead of sampling pairs from $\mathcal{D}$. Specially, for each code solution $x \in \mathbf{X}$, we run it with every test case in $\mathbf{Y}$ and keep track of which test cases it passes. We group together code solutions that pass the same test cases, because they have the same functionality. This way, we divide all code solutions in $\mathbf{X}$ into groups based on their functionality, which we write as $\mathbf{X}=\left{\mathcal{S}<em x="x">{x}^{1}, \mathcal{S}</em>}^{2}, \cdots, \mathcal{S<em x="x">{x}^{K}\right}$, where $K$ is the number of code solution groups. Each group $\mathcal{S}</em>}$ has a set of test cases that it passes, which we write as $\mathcal{S<em x="x">{y}$. Then, we get $K$ consensus sets, each of which has the form $\mathcal{S}=\left{(x, y) \mid x \in \mathcal{S}</em>}, y \in \mathcal{S<em x="x">{y}\right}$. We can score each consensus set by $f(\mathcal{S})=\left|\mathcal{S}</em>\right|$, as before. This naive version captures the same underline intuition, but it finds all consensus sets right away, without sampling pairs repeatedly.}\right|\left|\mathcal{S}_{y</p>
<h1>3 EXPERIMENTAL SETUP</h1>
<p>Models Our experiments are based on Codex (Chen et al., 2021), InCODER (Fried et al., 2022a) and CODEGEN (Nijkamp et al., 2022). Codex is a descendant of GPT-3 (Brown et al., 2020) and proficient in understanding the provided context and generating functional programs. We use three Codex models with different capabilities provided by OpenAI: code-cushman-001, code-davinci001, and code-davinci-002. InCODER is a unified generative model that can perform left-to-right code generation and code infilling, while CODEGEN is a family of large-scale language models to perform conversational program synthesis. We take use of the INCODER 6.7B version (INCODER6B) and the CODEGEN 16B Python mono-lingual version (CODEGEN-MONO-16B).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Baseline</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">AlphaCode-C</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CODET</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">k</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">HumanEval</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">code-cushman-001</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">39.6</td>
<td style="text-align: center;">46.4</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">44.511 .0</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">65.711 .4</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-001</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">60.6</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">50.211 .2</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">75.815 .2</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-002</td>
<td style="text-align: center;">47.0</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">65.818 .8</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">86.611 .7</td>
</tr>
<tr>
<td style="text-align: center;">INCODER-6B</td>
<td style="text-align: center;">16.415 .2</td>
<td style="text-align: center;">28.327 .8</td>
<td style="text-align: center;">47.547 .0</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">20.64 .2</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">37.18 .8</td>
</tr>
<tr>
<td style="text-align: center;">CODEGen-Mono-16B</td>
<td style="text-align: center;">29.729 .5</td>
<td style="text-align: center;">50.349 .9</td>
<td style="text-align: center;">73.775 .0</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">64.4</td>
<td style="text-align: center;">36.77 .0</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">59.38 .0</td>
</tr>
<tr>
<td style="text-align: center;">MBPP</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">code-cushman-001</td>
<td style="text-align: center;">45.9</td>
<td style="text-align: center;">66.9</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">51.5</td>
<td style="text-align: center;">59.0</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">55.49 .5</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">72.75 .8</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-001</td>
<td style="text-align: center;">51.8</td>
<td style="text-align: center;">72.8</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">64.7</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">61.910 .1</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">79.38 .5</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-002</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;">76.7</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">70.7</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">67.79 .6</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">81.54 .8</td>
</tr>
<tr>
<td style="text-align: center;">INCODER-6B</td>
<td style="text-align: center;">21.319 .4</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">66.2</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">34.413 .1</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">58.211 .7</td>
</tr>
<tr>
<td style="text-align: center;">CODEGen-Mono-16B</td>
<td style="text-align: center;">42.4</td>
<td style="text-align: center;">65.8</td>
<td style="text-align: center;">79.1</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">49.57 .1</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">68.52 .7</td>
</tr>
</tbody>
</table>
<p>Table 2: Pass@ $k(\%)$ on the HumanEval and MBPP benchmarks. AlphaCode-C is our replication of the clustering method in Li et al. (2022b). The numbers in red indicate the absolute improvements of CODET over baseline on pass@1 and pass@10. We also list the baseline results from Fried et al. (2022a) and Nijkamp et al. (2022) for reference in gray, where the settings of context are not exactly the same as ours. For CODET, temperature is set to 0.8 and sampling number is set to 100 . We do not show CODET pass@100, since it is the same as the baseline pass@100.</p>
<p>Metrics and Baseline We use the metric pass@ $k$ (with $n$ samples) for performance evaluation and take advantage of ground truth test cases to determine the functional correctness of code solutions. For each problem, we sample $n$ code solutions and then select $k$ of them for evaluation. If any of the $k$ code solutions passes all ground truth test cases, the problem is considered solved. Then pass@ $k$ is the percentage of solved problems. We use the unbiased definition of pass@ $k$ as our baseline (Chen et al., 2021), where $k$ solutions are randomly picked from $n$ samples. Our CodeT uses a dual execution agreement mechanism to select $k$ solutions from $n$ samples, as mentioned in 2.2. In addition, we include a clustering method from Li et al. (2022b) for comparison, denoted as AlphaCode-C. Our replication is to use the test inputs generated by CODET, run the solutions on the test inputs, group the solutions by test outputs, and rank the clusters by size (details in Appendix I).</p>
<p>Benchmarks We conduct experiments on four public code generation benchmarks in the zeroshot setting. The statistics of benchmarks are shown in Table 1. (1) HumanEval (Chen et al., 2021) consists of hand-written Python programming problems. The original contexts include example input-output cases, which are removed in our experiments to avoid exposing real test cases. The experiment in Appendix B shows that this removal operation is reasonable and indispensable. (2) MBPP (Austin et al., 2021) (sanitized version) contains crowd-sourced Python programming problems, and we follow HumanEval to construct the context for it. (3) APPS (Hendrycks et al., 2021) consists of coding problems collected from open-access coding websites, which have different difficulty levels. (4) CodeContests (Li et al., 2022b) includes competitive programming problems scraped from the Codeforces platform. To enable zero-shot inference, we construct the context for APPS and CodeContests as follows: the original problem description is treated as a comment where input-output examples are removed, and a simple function header "def solution(stdin: str) $\rightarrow$ str :" is placed after the comment to accommodate the input/output data format. More implementation details can be found in Appendix A.</p>
<h1>4 EXPERIMENTAL ReSULTS</h1>
<p>In this section, we evaluate CODET on five different pre-trained models and four benchmarks to verify its effectiveness, followed by test case analysis and case studies to provide more insights.</p>
<h3>4.1 ReSults on HumanEval and MBPP</h3>
<p>The experimental results of various models on the HumanEval and MBPP benchmarks are summarized in Table 2. If we compare the pass@100 to pass@1 on the Baseline column, it is clear that the</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th></th>
<th>Baseline</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>CODET</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>$k$</td>
<td></td>
<td>1</td>
<td>10</td>
<td>50</td>
<td>100</td>
<td>1000</td>
<td>1</td>
<td>2</td>
<td>10</td>
<td>100</td>
</tr>
<tr>
<td>APPS</td>
<td>INTRODUCTORY</td>
<td>27.2</td>
<td>46.6</td>
<td>59.4</td>
<td>-</td>
<td>-</td>
<td>34.67 .4</td>
<td>41.2</td>
<td>53.26 .6</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>INTERVIEW</td>
<td>5.1</td>
<td>12.8</td>
<td>23.0</td>
<td>-</td>
<td>-</td>
<td>8.13 .0</td>
<td>11.2</td>
<td>18.15 .3</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>COMPETITION</td>
<td>1.8</td>
<td>4.9</td>
<td>12.1</td>
<td>-</td>
<td>-</td>
<td>2.20 .4</td>
<td>4.1</td>
<td>8.63 .7</td>
<td>-</td>
</tr>
<tr>
<td>CodeContests</td>
<td></td>
<td>0.7</td>
<td>3.0</td>
<td>5.7</td>
<td>7.5</td>
<td>13.9</td>
<td>2.11 .4</td>
<td>2.3</td>
<td>5.32 .3</td>
<td>9.92 .4</td>
</tr>
</tbody>
</table>
<p>Table 3: Pass@ $k$ (\%) results on the APPS and CodeContests benchmarks using code-davinci-002 in the zero-shot setting. The numbers in red indicate the absolute improvements of CODET over baseline on pass@1, pass@10 and pass@100. For CODET, temperature is set to 0.8 and sampling number is set to 50 for APPS and 1,000 for CodeContests.
former is significantly better than the latter, indicating the potential to select the best code solution from the 100 generated samples.</p>
<p>For three Codex models, when we compare the CODET column with the Baseline column, CODET pass@1 achieves an absolute improvement of about $10 \%$ over the baseline pass@1. The improvements are consistently above $10 \%$ on HumanEval. Surprisingly, even for the strongest baseline, code-davinci-002, the improvement is $18.8 \%$, boosting the pass@1 to $65.8 \%$, which is a $20+\%$ absolute improvement over the best previously reported results (Inala et al., 2022). We attribute this larger improvement to the higher quality of test cases generated by code-davinci-002, providing a deeper analysis in Section 4.3. CODET also achieves exceptional performance on the MBPP benchmark, although the magnitude of the improvements is slightly less than that of HumanEval. Using the code-davinci-002 as an example, the pass@1 improves by $9.6 \%$. We also report pass@2 and pass@10 of CODET to further show its superiority. The pass@2 results of CODET are close to the baseline pass@10 results. Meanwhile, the improvements on pass@10 are also consistently over $10 \%$ on the HumanEval benchmark.</p>
<p>The experimental results of InCODER-6B and CODEGen-MONO-16B further verify the effectiveness of CODET. It is obvious CODET can significantly improve the pass@1, with absolute improvements in the range of $4.2 \%$ to $13.1 \%$. InCODER-6B achieves the greatest improvement with a gain of $13.1 \%$ on the MBPP benchmark. Similar to the experimental results of Codex, the pass@2 results are close to the baseline pass@10. All the results demonstrate that CODET can boost the performance of various pre-trained language models consistently.</p>
<p>As for AlphaCode-C, it is consistently inferior to CODET on both benchmarks using different models, demonstrating the superiority of our dual execution agreement that takes test case information into consideration. In addition, we notice that duplication exists in the generated code solutions and test cases. We perform an ablation study in Appendix D to show that de-duplication has little influence on the results of CODET. Moreover, we discuss the sensitivity of CODET to the temperature in Appendix E, showing the rationality of choosing a rather high temperature at 0.8 .</p>
<h1>4.2 ReSults on APPS and CodeContests</h1>
<p>We also conduct experiments on two more challenging benchmarks, APPS and CodeContests. We build the zero-shot versions of APPS and CodeContests to be in line with our setting of HumanEval and MBPP by removing the example input-output cases in the problem descriptions. We employ code-davinci-002 for code solution and test case generation. The sampling number is set to 50 for APPS to save computation cost on the 5,000 testing problems, while for CodeContests, following Li et al. (2022b), the sampling number is set to 1,000 to solve especially hard problems. From the results summarized in Table 3, we can clearly observe the consistent performance improvements on both benchmarks using CODET. The absolute pass@1 improvement is $7.4 \%$ for introductory problems in APPS, while the improvements are not significant for competition level problems in APPS and CodeContest, indicating their difficulties. In addition, we notice that code-davinci-002 may generate many trivial code solutions for the problems in APPS and CodeContests due to the superior difficulty of these two benchmarks. We perform a comprehensive study in Appendix F to demonstrate the robustness of CODET to this issue. Inspired by Chen et al. (2021) and Li et al. (2022b), we also conduct experiments in the one-shot setting, which is detailed in Appendix G.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The distributions of (a) test case accuracy and (b) toxicity rate for each problem on HumanEval. Test cases are of better quality if they have higher accuracy and lower toxicity rate.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmarks</th>
<th style="text-align: center;">HumanEval</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">MBPP</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$k$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">code-cushman-001</td>
<td style="text-align: center;">47.12 .6</td>
<td style="text-align: center;">58.6 8.5</td>
<td style="text-align: center;">71.2 5.5</td>
<td style="text-align: center;">59.74 .3</td>
<td style="text-align: center;">64.8 3.1</td>
<td style="text-align: center;">75.52 .8</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-001</td>
<td style="text-align: center;">52.01 .8</td>
<td style="text-align: center;">62.94 .0</td>
<td style="text-align: center;">78.12 .3</td>
<td style="text-align: center;">64.32 .4</td>
<td style="text-align: center;">71.72 .6</td>
<td style="text-align: center;">80.51 .2</td>
</tr>
<tr>
<td style="text-align: center;">INCODER-6B</td>
<td style="text-align: center;">26.8 6.2</td>
<td style="text-align: center;">30.42 .8</td>
<td style="text-align: center;">40.83 .7</td>
<td style="text-align: center;">50.315 .9</td>
<td style="text-align: center;">55.411 .5</td>
<td style="text-align: center;">64.56 .3</td>
</tr>
<tr>
<td style="text-align: center;">CODEGEN-MONO-16B</td>
<td style="text-align: center;">47.711 .0</td>
<td style="text-align: center;">54.910 .2</td>
<td style="text-align: center;">71.011 .7</td>
<td style="text-align: center;">60.010 .5</td>
<td style="text-align: center;">67.611 .0</td>
<td style="text-align: center;">76.58 .0</td>
</tr>
</tbody>
</table>
<p>Table 4: Pass@ $k$ (\%) on the HumanEval and MBPP benchmarks with code-cushman-001, code-davinci-001, INCODER, and CODEGEN using the test cases generated by code-davinci-002. The numbers in orange indicate the absolute improvements of pass@ $k$ using code-davinci-002 test cases over that using their own generated test cases.</p>
<h1>4.3 ANALYSIS ON TEST CASES</h1>
<p>The test cases are vital to CODET since the core idea is based on test-driven execution agreement. Hence, in this subsection, we analyze the test cases by answering the following research questions.</p>
<h2>Q1. What is the quality of the generated test cases?</h2>
<p>We evaluate the correctness of the generated test cases using the canonical solutions. A test case is considered correct if the canonical solution can pass it. Figure 4a summarizes the distributions of test case accuracy on HumanEval, where the horizontal axis represents the accuracy value for each problem and the vertical axis represents the probability density of problems with the corresponding accuracy value. We can see that the test cases generated by Codex models are of much higher accuracy than CODEGEN/INCODER. Besides accuracy, we also introduce the test case toxicity rate as a measurement of quality. We consider a test case to be "toxic" if any generated code solution can pass it while the canonical solution cannot. Toxic test cases may hinder the scoring of consensus sets and lead to the failure of CODET. As shown in Figure 4b, we can find that the toxicity rate highly correlates to the test case accuracy with respect to different models, where the proportions of toxic test cases for Codex models are smaller than CODEGEN/INCODER. We also evaluate the code coverage of generated test cases using two coverage criterias in Appendix H.2, where Codex models still outperform CODEGEN/INCODER with an average coverage of over $95 \%$. Comparing the test case quality and the performance of CODET shown in Table 2, we can find that the quality of test cases strongly correlates to the performance gain using CODET concerning different models.</p>
<h2>Q2. Can better test cases further boost the performance of mediocre models?</h2>
<p>From the above discussion with Figure 4, we can find that code-davinci-002 is the most capable model for generating high-quality test cases. Hence, we conduct an experiment to boost the performance of the other four models (code-cushman-001, code-davinci-001, INCODER, and CODEGEN) using test cases generated by code-davinci-002. Table 4 summarizes the performance gain with respect to different models on the HumanEval and MBPP benchmarks. In general, using the test cases generated by code-davinci-002 can significantly improve the performance of using the test cases generated by the less capable models themselves. For code-cushman-001 and code-davinci-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Two real cases from the HumanEval benchmark with CODET and code-cushman-001.</p>
<p>001, the absolute improvements are in the range of $1.8 \%$ to $4.3 \%$ on pass@1, while for INCODER and CODEGEN, the range is from $6.2 \%$ to $15.9 \%$. The above results indicate that the correct code solutions generated by mediocre models can be further exploited by adopting better test cases.</p>
<h1>Q3. How effective is CODET when there are fewer test cases?</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">Limit</th>
<th style="text-align: center;">Sampling Number</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">63.6</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">65.0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">65.0</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">65.8</td>
</tr>
</tbody>
</table>
<p>Table 5: Pass@1 (\%) on HumanEval using CODET and code-davinci-002 with different numbers of test cases. Sampling Number denotes the number of samples generated by model, and Limit denotes the test cases extracted per sample.</p>
<p>When generating test cases for the HumanEval benchmark, we sample 100 times for each problem and each sample may include multiple assertion statements (i.e., test cases), denoted as Sampling Number $=100$. Then we extract the first 5 syntactically correct test cases from each sample, denoted as Limit $=5$. This means each problem is equipped with 500 test cases at most. The actual numbers of extracted test cases are summarized in Appendix H.1. We perform an ablation study on the number of test cases by decreasing Sampling Number and Limit. As shown in Table 5, we can conclude that using more test cases in CODET could generally lead to better performance, while the performance gap narrows when Sampling Number $\geq 50$ and Limit $\geq 3$. Moreover, CODET improves the pass@1 by $9.5 \%$ with only 10 test cases using code-davinci-002, suggesting the high test case efficiency. We can use a smaller Sampling Number in real-world application to balance the performance and computation cost. More results can be found in Appendix H.3.</p>
<h3>4.4 CASE Study</h3>
<p>In CODET, we design the dual execution agreement based on the idea that a good code solution can pass the most test cases and agree with the most solutions of the same functionality. We use "dual" because both the code solutions and the test cases are critical. Figure 5a shows a case from the HumanEval benchmark using code-cushman-001. The highest scoring consensus set has the correct functionality that returns true if all numbers in the list are below threshold $t$, while the consensus set ranked 2 does not understand the boundary condition exactly. The solutions in the second consensus set can pass more test cases (i.e., 226) than that in the first consensus set (i.e., 218). However, considering both code solutions and test cases, CODET can successfully rank the consensus sets and find the correct solutions. Such cases are not rare, suggesting that our design of the dual execution agreement is reasonable. For further statistical demonstration, we conduct an ablation study to score the consensus set by considering only the number of code solutions or test cases. The results again support our claim, as detailed in Appendix I.</p>
<p>CODET is empowered by the pre-trained language models, but is also limited by them. Therefore, the second assumption made in Section 2.2 does not always hold, leading to error cases where</p>
<p>the correct code solution is generated, but not in the top 1 consensus set. For CODET with code-cushman-001 on the HumanEval benchmark, we find 53 out of 164 programming problems that belong to this situation. We manually investigated these problems and found that $20 \%$ of them can be blamed on issues such as ambiguous problem descriptions, uncovered corner cases, and lack of import statements, while the remaining problems are attributed to the failure of the model to understand the problem descriptions. Figure 5b shows an error case caused by ambiguity. The correct understanding of the description "sum(first index value, last index value)" is to add the first and last values, while the code solutions that sum all values from the first to the last are ranked top 1. More real cases can be found in Appendix J. And hope the error analysis can provide inspiration for future studies on improving code generation for more difficult programming problems.</p>
<h1>5 Related Work</h1>
<p>Code Generation with Large Models Recently, a number of large pre-trained language models have been proposed for code generation. Benefiting from billions of trainable parameters and massive publicly available source code, models could achieve surprisingly good performance. For instance, AlphaCode (Li et al., 2022b) claimed to have outperformed half of the human competitors in real-world programming competitions, and Codex (Chen et al., 2021) is empowering Copilot to provide real-time coding suggestions. Other open-source code generation models include GPTNeo (Black et al., 2021), GPT-J (Wang \&amp; Komatsuzaki, 2021), CodeParrot (Tunstall et al., 2022), PolyCoder (Xu et al., 2022), CODEGEN (Nijkamp et al., 2022), and INCODER (Fried et al., 2022a). In our study, we take advantage of the Codex inference API provided by OpenAI as well as the two competitive open-source models CODEGEN and INCODER to perform zero-shot code generation.</p>
<p>Automatic Test Case Generation Automated test case generation for programming problems can reduce the effort of writing test cases manually by developers. Early works including Randoop (Pacheco et al., 2007), EvoSuite (Fraser \&amp; Arcuri, 2011), MOSA (Panichella et al., 2015), DynaMOSA (Panichella et al., 2017), and MIO (Arcuri, 2017), were proposed to automatically generate test cases for statically typed programming languages like Java. The later proposed Pynguin (Lukasczyk \&amp; Fraser, 2022) could handle dynamically typed language like Python. Nevertheless, they are all search-based heuristics methods, which have limitations to the diversity and quantity of generated test cases. To combat these limitations, recently proposed approaches (Tufano et al., 2020; Li et al., 2022b) leveraged pre-trained language models like BART (Lewis et al., 2019) and T5 (Raffel et al., 2020) fine-tuned on labelled data for test case generation. Unlike previous works that require heuristic rules or model training, we directly sample test cases from powerful code generation models like Codex in the zero-shot setting with elaborate prompts.</p>
<p>Code Selection from Multiple Samples Despite large models have achieved great performance in code generation, the models need to sample many times to find the correct answer. Recently, several approaches were proposed to tackle this issue. In the domain of solving math word problems, Cobbe et al. (2021) chose the one with highest rank by a trained verifier, and Shen et al. (2021) proposed to jointly train the generator and ranker through a multi-task framework. In the domain of general purpose code generation, Inala et al. (2022) trained a fault-aware ranker. Moreover, some work has been proposed to leverage the execution information (Shi et al., 2022; Li et al., 2022b; Le et al., 2022; Lahiri et al., 2022). Unlike previous works that require model training or pre-existing test cases or user interactions, we let the large models generate test cases for themselves and automatically rank the solutions based on the test-driven dual execution agreement. The idea of ranking based on agreement also appears in the domain of reasoning (Wang et al., 2022; Li et al., 2022a).</p>
<h2>6 CONCLUSION AND Future Work</h2>
<p>In this paper, we propose a simple yet effective approach, called CODET, leveraging pre-trained language models to generate both the code solutions and the test cases. CODET executes the code solutions using the test cases and chooses the best solution based on the dual execution agreement. We demonstrate the dual agreement with both the test cases and other solutions is critical to the success of CODET, perform a thorough analysis on the quality of generated test cases and their impact on CODET, and study cases to provide more insights. Experimental results clearly demonstrate the</p>
<p>superiority of CODET, improving the pass@1 numbers significantly on various benchmarks. While there remain challenges that CODET only works for executable code generation and it introduces extra computation cost for test case generation. In future work, we will explore the ways to tackle these challenges and improve CODET to solve more difficult programming problems.</p>
<h1>REFERENCES</h1>
<p>Andrea Arcuri. Many independent objective (mio) algorithm for test suite generation. In International symposium on search based software engineering, pp. 3-17. Springer, 2017.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow. If you use this software, please cite it using these metadata, 58, 2021.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Xinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In International Conference on Learning Representations, 2018.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-Lezama. Write, execute, assess: Program synthesis with a repl. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24 (6):381-395, 1981.</p>
<p>Gordon Fraser and Andrea Arcuri. EvoSuite: automatic test suite generation for object-oriented software. In Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering, pp. 416-419, 2011.</p>
<p>Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. arXiv preprint, 2022a.</p>
<p>Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999, 2022b.</p>
<p>Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021.</p>
<p>Jeevana Priya Inala, Chenglong Wang, Mei Yang, Andres Codas, Mark Encarnación, Shuvendu K Lahiri, Madanlal Musuvathi, and Jianfeng Gao. Fault-aware neural code rankers. arXiv preprint arXiv:2206.03865, 2022.</p>
<p>Shuvendu K. Lahiri, Aaditya Naik, Georgios Sakkas, Piali Choudhury, Curtis von Veh, Madanlal Musuvathi, Jeevana Priya Inala, Chenglong Wang, and Jianfeng Gao. Interactive code generation via test-driven user-intent formalization, 2022.</p>
<p>Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven CH Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. arXiv preprint arXiv:2207.01780, 2022.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.</p>
<p>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022a.</p>
<p>Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. arXiv preprint arXiv:2203.07814, 2022b.</p>
<p>Stephan Lukasczyk and Gordon Fraser. Pynguin: Automated unit test generation for python. arXiv preprint arXiv:2202.05218, 2022.</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. A conversational paradigm for program synthesis. arXiv preprint, 2022.</p>
<p>Carlos Pacheco, Shuvendu K Lahiri, Michael D Ernst, and Thomas Ball. Feedback-directed random test generation. In 29th International Conference on Software Engineering (ICSE'07), pp. 75-84. IEEE, 2007.</p>
<p>Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. Reformulating branch coverage as a many-objective optimization problem. In 2015 IEEE 8th international conference on software testing, verification and validation (ICST), pp. 1-10. IEEE, 2015.</p>
<p>Annibale Panichella, Fitsum Meshesha Kifetew, and Paolo Tonella. Automated test case generation as a many-objective optimisation problem with dynamic selection of the targets. IEEE Transactions on Software Engineering, 44(2):122-158, 2017.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.</p>
<p>Baptiste Roziere, Jie M Zhang, Francois Charton, Mark Harman, Gabriel Synnaeve, and Guillaume Lample. Leveraging automated unit tests for unsupervised code translation. arXiv preprint arXiv:2110.06773, 2021.</p>
<p>Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate \&amp; rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034, 2021.</p>
<p>Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I Wang. Natural language to code translation with execution. arXiv preprint arXiv:2204.11454, 2022.</p>
<p>Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. Unit test case generation with transformers and focal context. arXiv preprint arXiv:2009.05617, 2020.</p>
<p>Lewis Tunstall, Leandro von Werra, and Thomas Wolf. Natural language processing with transformers. " O'Reilly Media, Inc.", 2022.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface's transformers: State-of-the-art natural language processing. ArXiv, 2019.</p>
<p>Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evaluation of large language models of code. In Deep Learning for Code Workshop, 2022.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th>Baseline</th>
<th></th>
<th></th>
<th>CODET</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>$k$</td>
<td>1</td>
<td>10</td>
<td>100</td>
<td>1</td>
<td>2</td>
<td>10</td>
</tr>
<tr>
<td>code-cushman-001</td>
<td>$31.7-1.8$</td>
<td>56.42 .1</td>
<td>84.16 .7</td>
<td>58.614 .1</td>
<td>65.715 .6</td>
<td>80.114 .4</td>
</tr>
<tr>
<td>code-davinci-001</td>
<td>$34.8-4.2$</td>
<td>63.02 .4</td>
<td>87.23 .1</td>
<td>60.410 .2</td>
<td>69.110 .2</td>
<td>82.46 .6</td>
</tr>
<tr>
<td>code-davinci-002</td>
<td>47.60 .6</td>
<td>78.83 .9</td>
<td>92.70 .6</td>
<td>74.89 .0</td>
<td>82.97 .8</td>
<td>89.02 .4</td>
</tr>
</tbody>
</table>
<p>Table 6: Pass@ $k$ (\%) on the original HumanEval benchmark with Codex models. The numbers in orange indicate the absolute improvements of pass@ $k$ on the original benchmark over our modified benchmark in Table 2.</p>
<h1>A MORE IMPLEMENTATION DETAILS</h1>
<p>We set the temperature to 0.8 , the top $p$ to 0.95 , the max generation length to 300 , and the timeout of executing a test case to 0.1 seconds. Specially, for baseline pass@1, we use the greedy search setting with temperature 0 . The number of sampling test cases for each problem is set to 100 for the HumanEval and MBPP benchmarks, and 50 for the APPS and CodeContests benchmarks. When scoring consensus sets in CODET, we use the square root of $\left|\mathcal{S}_{x}\right|$ to reduce the impact caused by code solutions. A supporting experiment can be found in Appendix C. For code solution post-processing, we follow Chen et al. (2021) to truncate the generated content by five stop sequences: " $\backslash$ nclass", " $\backslash$ ndef", " $\backslash$ n#", " $\backslash$ nif", and " $\backslash$ nprint". For the implementation of InCODER and CODEGen, we use the HuggingFace transformers library (Wolf et al., 2019) and run both models with half precision. In addition, when the number of consensus sets in CODET is smaller than $k$, the selection is done from the highest scoring consensus set to the lowest. When reaching the set with the lowest score, it repeats from the highest scoring consensus set. In most cases, the number of consensus sets is larger than $k$, as shown in Figure 6.</p>
<h2>B ReSults on Original HumanEval</h2>
<p>As mentioned in Section 3, for all benchmarks, we remove the example input-output cases from the original contexts to avoid exposing real test cases. To study the influence of such modification, we take HumanEval as an example and perform an additional experiment with its original contexts. The results are summarized in Table 6. On the one hand, the baseline pass@10 and pass@100 results on the original HumanEval benchmark outperform the modified version, which is reasonable because the example input-output cases may provide useful information for code generation. Nevertheless, the pass@1 results on the original benchmark are basically the same or even worse than the modified version, suggesting that the Codex models have not fully understood the semantics of the example input-output cases provided in the contexts. On the other hand, the performance of CODET is significantly improved using the original benchmark. This is as expected because the original contexts used for test case generation include real test cases, which could be borrowed by the models during the generation. Such real test cases will greatly empower CODET to distinguish correct code solutions. Hence, in our experiments, it is indispensable to remove the example input-output cases to avoid exposing the real test cases. In this way, the effectiveness of CODET can be fairly verified.</p>
<h2>C ANALYSIS ON CODE SOLUTIONS</h2>
<p>In CODET, code solutions that can pass exactly the same test cases are considered consistent in functionality and are grouped into the same consensus set. Since we employ top $p$ sampling with a rather high temperature of 0.8 , the functionality of the code solutions may vary significantly, which results in more consensus sets. We draw a histogram in Figure 6 to show the number of consensus sets produced by code-cushman-001 and CODET for each problem in the HumanEval benchmark. The average and median numbers are 26.8 and 25.5 , respectively. We can find that most problems have less than 50 consensus sets, but the numbers have a high variance among different problems. We also draw the distribution of the numbers of code solutions for the top-ranked consensus sets in Figure 7. The consensus sets ranked top 1 tend to have more code solutions with an average value of 9.8 , and the numbers also have a high variance.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The numbers of consensus sets that are produced by code-cushman-001 and CODET on the HumanEval benchmark.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: The CODET results of three Codex models with and without constraint on the number of code solutions.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: The distribution of the code solution numbers for the top 5 consensus sets. The long tail distribution with number $\geq 20$ is truncated.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: The baseline pass@100 and CODET pass@1 with code-cushman-001 at different temperature settings.</p>
<p>As mentioned in Appendix A, we use the square root of $\left|\mathcal{S}_{x}\right|$ to reduce the impact caused by code solutions, because we believe passing more test cases is more important than having more code solutions with the same functionality. For example, there may be one code solution that can pass five test cases, whereas another five code solutions in a consensus set can pass only one test case. We intuitively consider that the former may be more likely correct. For validation, we perform an experiment by comparing the performance of CODET with the "sqrt", "log" functions, and without any constraint (i.e., "linear") on the number of code solutions. Figure 8 shows the results of three Codex models on the HumanEval benchmark. We can find that reducing the importance of code solutions can consistently improve the performance of CODET. Similar observations have been found in other models and benchmarks, where the performance of employing "sqrt" is always better than or competitive to "linear", indicating the rationality of our design.</p>
<h1>D INFLUENCE OF DE-DUPLICATION</h1>
<p>Since we sample multiple times during generation, there is the chance that many of the generated code solutions and test cases are exactly the same. On the one hand, the number of duplicates may indicate the importance of a sample. On the other hand, duplicates may hinder the scoring of consensus sets in CODET when the quality of generation is unsatisfactory. Hence, we perform an ablation study to investigate the effects of removing duplicate code solutions and test cases. Specifically, we first format the generated Python code to conform to the PEP 8 style guide ${ }^{3}$, and then remove duplicate code solutions and test cases before performing CODET. The de-duplication results on the HumanEval and MBPP benchmarks using CODET and code-cushman-001 are shown in Table 7, where we can choose to de-duplicate the code solutions, or the test cases, or both. We can</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>| De-duplication | | HumanEval | | | MBPP | | |
| Solution | Test | 1 | 2 | 10 | 1 | 2 | 10 |
| --- | --- | --- | --- | --- | --- | --- | --- |
| No | No | 44.5 | 50.1 | 65.7 | $\mathbf{5 5 . 4}$ | 61.7 | 72.7 |
| No | Yes | 42.2 | 48.8 | $\mathbf{6 6 . 7}$ | 54.5 | $\mathbf{6 2 . 3}$ | $\mathbf{7 3 . 4}$ |
| Yes | No | $\mathbf{4 6 . 9}$ | $\mathbf{5 2 . 5}$ | 65.6 | 54.7 | 61.7 | 73.2 |
| Yes | Yes | 42.7 | 51.2 | 66.4 | 54.7 | 62.1 | 73.2 |</p>
<p>Table 7: Pass@ $k(\%)$ on the HumanEval and MBPP benchmarks using CODET and code-cushman001 with different de-duplication settings. The setting "No No" in the first line means that neither the code solutions nor the test cases are de-duplicated, which is used in our main experiments.</p>
<table>
<thead>
<tr>
<th>Methods</th>
<th></th>
<th>CODET</th>
<th></th>
<th></th>
<th>CODET (Remove Trivial)</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>$k$</td>
<td></td>
<td>1</td>
<td>10</td>
<td>100</td>
<td>1</td>
<td>10</td>
<td>100</td>
</tr>
<tr>
<td>APPS</td>
<td>INTRODUCTORY</td>
<td>34.6</td>
<td>53.2</td>
<td>-</td>
<td>34.90 .3</td>
<td>53.40 .2</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>INTERVIEW</td>
<td>8.1</td>
<td>18.1</td>
<td>-</td>
<td>8.30 .2</td>
<td>18.20 .1</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>COMPETITION</td>
<td>2.2</td>
<td>8.6</td>
<td>-</td>
<td>2.50 .3</td>
<td>8.70 .1</td>
<td>-</td>
</tr>
<tr>
<td>CodeContests</td>
<td></td>
<td>2.1</td>
<td>5.3</td>
<td>9.9</td>
<td>2.70 .6</td>
<td>5.30 .0</td>
<td>10.00 .1</td>
</tr>
</tbody>
</table>
<p>Table 8: Pass@ $k(\%)$ results on the zero-shot APPS and CodeContests benchmarks using code-davinci-002 and CODET with/without the trivial code solutions filtered. The numbers in red indicate the absolute improvements after filtering the trivial solutions.
find that de-duplication has slight and inconsistent influence on the performance of CODET. For the HumanEval benchmark, the pass@1 results using code solution de-duplication alone are better than other settings. Nonetheless, for the MBPP benchmark, the best pass@1 results are achieved without de-duplication. Therefore, in our main experiments, we reserve all the generated code solutions and test cases when performing CODET and leave the study of more advanced de-duplication methods for future work.</p>
<h1>E SENSITIVITY TO THE TEMPERATURE</h1>
<p>The hyper-parameter temperature has a great impact on the quality of generated code solutions and test cases when using top $p$ sampling. We use a high temperature of 0.8 in our main experiments since CODET could benefit from a larger number of diverse samples. To investigate the sensitivity of CODET to the temperature, we perform an ablation study by using a range of temperatures to report the results of baseline pass@100 and CODET pass@1. Figure 9 shows the results of code-cushman-001 on the HumanEval benchmark at different temperature settings. We can find that a higher temperature does improve the baseline pass@100 and CODET pass@1, and CODET achieves a good performance when temperature is set to 0.8 .</p>
<h2>F Removing Trivial Code Solutions</h2>
<p>The problems in the APPS COMPETITION and CodeContests benchmarks are of great difficulty compared to HumanEval and MBPP, leading to the poor performance of the most capable code-davinci-002 model. After checking the incorrect code solutions generated by code-davinci-002, we identify many trivial solutions that just return the input argument or a constant value. Such solutions may hinder the ranking process of CODET if they can pass any generated test case. A trivial solution can be easily identified by its input arguments and returned values. If a solution always returns the same output value for different inputs, or its returned values are always the same as the inputs, it must be a trivial solution. To investigate the impact of trivial code solutions, we use code-davinci002 on the zero-shot APPS and CodeContests benchmarks, and perform CODET after filtering out all the trivial solutions. As a result, we can remove an average of 4.5 (91.6) trivial solutions from the $50(1,000)$ generated solutions per problem for the APPS (CodeContests) benchmark. How-</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$k$</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">50</th>
<th style="text-align: center;">100</th>
<th style="text-align: center;">1000</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">10</th>
<th style="text-align: center;">100</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Baseline</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CODET</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">APPS</td>
<td style="text-align: center;">IntroDUCTORY</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">47.316 .0</td>
<td style="text-align: center;">52.7</td>
<td style="text-align: center;">58.49 .9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INTERVIEW</td>
<td style="text-align: center;">6.4</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.37 .9</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">23.38 .7</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPETITION</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">6.23 .7</td>
<td style="text-align: center;">9.8</td>
<td style="text-align: center;">13.67 .3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CodeContests</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">7.1</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">3.22 .2</td>
<td style="text-align: center;">5.6</td>
<td style="text-align: center;">9.35 .2</td>
<td style="text-align: center;">12.33 .5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Baseline Filter</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CODET Filter</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">APPS</td>
<td style="text-align: center;">IntroDUCTORY</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">58.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">49.66 .0</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">59.40 .8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">INTERVIEW</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">16.12 .8</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">24.01 .2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMPETITION</td>
<td style="text-align: center;">7.0</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">7.96 .9</td>
<td style="text-align: center;">10.5</td>
<td style="text-align: center;">14.10 .8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">CodeContests</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">14.5</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">15.2</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">9.6 -0.3</td>
<td style="text-align: center;">11.5</td>
<td style="text-align: center;">13.7 -0.8</td>
<td style="text-align: center;">14.5 -0.2</td>
</tr>
</tbody>
</table>
<p>Table 9: Pass@ $k(\%)$ results on the APPS and CodeContests benchmarks using code-davinci-002 and the one-shot setting. The numbers in red indicate the absolute improvements of CODET (Filter) over Baseline (Filter) on pass@1, pass@10 and pass@100. For CODET (Filter), temperature is set to 0.8 and sampling number is set to 50 for APPS and 1,000 for CodeContests. We do not report pass@1000 for "Baseline Filter" because the numbers of code solutions after filtering are less than the sampling numbers.
ever, as shown in Table 8, after removing a prominent percentage of trivial solutions, there is little performance gain, which could exactly demonstrate the robustness of CODET.</p>
<h1>G Results on APPS and CodeContests in the One-shot Setting</h1>
<p>Inspired by Chen et al. (2021) and Li et al. (2022b), we build one-shot versions of APPS and CodeContests by appending a single input-output example to the problem description as a formatting hint. After generation, we filter out the generated solutions that cannot pass the given example input-output cases, which we call the "Baseline Filter" method. After filtering, we can still perform CODET using the rest of code solutions, called the "CODET Filter" method. Following the zeroshot experiments on APPS and CodeContests, we employ code-davinci-002 for generation and set the sampling number to 50 for APPS and 1,000 for CodeContests.</p>
<p>We summarize the experimental results in Table 9, where we can find the one-shot performance using CODET is much better than that reported in Table 3 in the zero-shot setting. The performance of the baselines can be significantly improved by filtering the solutions with the given example test cases. Moreover, "CODET Filter" can further outperform "Baseline Filter" on the APPS benchmark, especially for the introductory and interview problems. Nonetheless, for CodeContests and the competition level problems in APPS, "CODET Filter" has little performance improvement or even performs slightly worse than "Baseline Filter". After manual investigation, we blame such issue to the generated low-quality test cases, which hinder the scoring of consensus sets. This suggests the interest of future study on test case generation for more challenging programming problems.</p>
<h2>H More Analysis on Test Cases</h2>
<h2>H. 1 Statistics on Test Cases</h2>
<p>How many valid test cases do the models generate for CODET? Taking the HumanEval benchmark as an example, we sample 100 times for each problem when generating test cases. As illustrated in Figure 2, at each time of sampling, we feed the context $c$ along with an instruction $p$ to the model and get the generated content that may contain multiple test cases. Then, as mentioned in Section 4.3, we further post-process the generated samples to get individual test cases that are syntactically correct. Finally, we only keep the first five valid test cases for each sample, which means a problem can be equipped with 500 test cases at most. Table 10 summarizes the average and median numbers of the extracted test cases for each problem. We can find that almost all the models could generate a considerable number of syntactically correct test cases, while CODEGEN generates plenty of unexpected noise.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Test Case Number</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;">Median</td>
</tr>
<tr>
<td style="text-align: center;">code-cushman-001</td>
<td style="text-align: center;">410.7</td>
<td style="text-align: center;">429.0</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-001</td>
<td style="text-align: center;">381.9</td>
<td style="text-align: center;">388.0</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-002</td>
<td style="text-align: center;">391.1</td>
<td style="text-align: center;">402.0</td>
</tr>
<tr>
<td style="text-align: center;">InCOder</td>
<td style="text-align: center;">390.1</td>
<td style="text-align: center;">400.0</td>
</tr>
<tr>
<td style="text-align: center;">CODEGen</td>
<td style="text-align: center;">55.6</td>
<td style="text-align: center;">42.0</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Code Coverage</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Statement</td>
<td style="text-align: center;">Branch</td>
</tr>
<tr>
<td style="text-align: center;">code-cushman-001</td>
<td style="text-align: center;">95.3</td>
<td style="text-align: center;">98.1</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-001</td>
<td style="text-align: center;">94.9</td>
<td style="text-align: center;">97.6</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-002</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">98.5</td>
</tr>
<tr>
<td style="text-align: center;">InCOder</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">96.3</td>
</tr>
<tr>
<td style="text-align: center;">CODEGen</td>
<td style="text-align: center;">78.2</td>
<td style="text-align: center;">78.6</td>
</tr>
</tbody>
</table>
<p>Table 10: The numbers of extracted test cases for each problem generated by five models on the HumanEval benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Limit</th>
<th style="text-align: center;">Sampling Number</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">code-cushman-001</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">38.7</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">41.8</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">41.8</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">42.5</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">41.2</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">43.3</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">45.4</td>
<td style="text-align: center;">44.5</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">60.7</td>
<td style="text-align: center;">62.4</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">62.2</td>
<td style="text-align: center;">62.8</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">63.6</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">63.2</td>
<td style="text-align: center;">65.5</td>
<td style="text-align: center;">65.0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">64.1</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">65.7</td>
<td style="text-align: center;">65.0</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">65.8</td>
</tr>
</tbody>
</table>
<p>(a) pass@1</p>
<p>Table 11: The Code Coverage (\%) statistics of test cases generated by five models on the Hu manEval benchmark.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Limit</th>
<th style="text-align: center;">Sampling Number</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">code-cushman-001</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">43.3</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">48.2</td>
<td style="text-align: center;">49.1</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">48.1</td>
<td style="text-align: center;">49.5</td>
<td style="text-align: center;">49.8</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">48.7</td>
<td style="text-align: center;">48.7</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">49.2</td>
<td style="text-align: center;">47.9</td>
<td style="text-align: center;">49.4</td>
<td style="text-align: center;">49.1</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">48.3</td>
<td style="text-align: center;">48.5</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">50.1</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">67.8</td>
<td style="text-align: center;">71.9</td>
<td style="text-align: center;">71.5</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">71.7</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">74.1</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">75.0</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">74.1</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">74.3</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">74.3</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">75.1</td>
</tr>
</tbody>
</table>
<p>(b) pass@2</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Limit</th>
<th style="text-align: center;">Sampling Number</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: center;">code-cushman-001</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">55.1</td>
<td style="text-align: center;">56.6</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;">62.9</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">64.5</td>
<td style="text-align: center;">65.8</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">60.9</td>
<td style="text-align: center;">62.5</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">65.3</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">61.4</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">65.8</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">63.1</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">65.7</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-002</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">79.6</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">84.3</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">86.5</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">87.1</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">86.9</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">86.6</td>
</tr>
</tbody>
</table>
<p>(c) pass@10</p>
<p>Table 12: Pass@ $k(\%)$ on the HumanEval benchmark using CODET with different test case numbers. Sampling Number is the number of test case samples we generate for each problem. Each sample may contain multiple assertion statements. These assertion statements are potential test cases, but we do not use all of them. Instead, we extract a Limit number of syntactically correct assertion statements from each sample, and discard the rest.</p>
<h1>H. 2 Code Coverage of Test Cases</h1>
<p>To further inspect the quality of generated test cases, we utilize the code coverage measurement and report two coverage criterias - the statement coverage and the branch coverage. The statement coverage can be calculated as the percentage of statements in a code solution that are executed by test cases. The branch coverage is the percentage of executed branches for the control structure (e.g. the if statement). We execute the canonical solution for each HumanEval problem on the test cases generated by five models, then collect the coverage results using Coverage.py ${ }^{4}$. As a result, the average numbers of statements and branches in the canonical solution of a problem are 6.30 and 4.42 , respectively. As shown in Table 11, all the models except CODEGen have good performance on both statement and branch coverage, reaching an average of over $94 \%$ coverage. Such results may be attributed to the relatively short canonical solutions and the massive sampling number of test cases. Nevertheless, there are still corner cases that the models cannot cover, which calls for future improvements.</p>
<h2>H. 3 Results of Reducing the Number of Test Cases</h2>
<p>To investigate the performance of CODET using fewer test cases, we perform an ablation study on the number of test cases that participate in the dual execution agreement. As shown in Table 12, we report the results on the HumanEval benchmark using code-cushman-001 and code-davinci-002 with a range of test case numbers. The number of test cases is related to two hyper-parameters. One is the number of test case samples, which is set to 100 for HumanEval in our main experiments. The</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Methods</th>
<th style="text-align: center;">Code Solution Only $f^{\prime}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test Case Only $f^{\prime \prime}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$k$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">code-cushman-001</td>
<td style="text-align: center;">$41.2_{-7.1}^{-3.3}$</td>
<td style="text-align: center;">$49.2^{-0.9}$</td>
<td style="text-align: center;">$61.9_{-7.6}^{-3.8}$</td>
<td style="text-align: center;">$29.9_{-3.6}^{-14.6}$</td>
<td style="text-align: center;">$36.6^{-13.5}$</td>
<td style="text-align: center;">$59.5_{-5.2}^{-6.2}$</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-001</td>
<td style="text-align: center;">$44.4_{-5.4}^{-5.8}$</td>
<td style="text-align: center;">$54.7^{-4.2}$</td>
<td style="text-align: center;">$69.0_{-6.8}^{-6.8}$</td>
<td style="text-align: center;">$35.0_{-15.2}^{-15.2}$</td>
<td style="text-align: center;">$46.0^{-12.9}$</td>
<td style="text-align: center;">$70.2_{-3.6}^{-5.6}$</td>
</tr>
<tr>
<td style="text-align: center;">code-davinci-002</td>
<td style="text-align: center;">$55.9_{-8.9}^{-9.9}$</td>
<td style="text-align: center;">$67.0^{-8.1}$</td>
<td style="text-align: center;">$82.7_{-7.8}^{-5.3}$</td>
<td style="text-align: center;">$58.4_{-11.4}^{-4.4}$</td>
<td style="text-align: center;">$65.1^{-10.0}$</td>
<td style="text-align: center;">$86.1_{-11.2}^{-0.5}$</td>
</tr>
</tbody>
</table>
<p>Table 13: Pass@k (\%) on the HumanEval benchmark with ranking only on the number of code solutions $\left(f^{\prime}(\mathcal{S})=\left|\mathcal{S}<em y="y">{x}\right|\right)$ or test cases $\left(f^{\prime \prime}(\mathcal{S})=\left|\mathcal{S}</em>\right|\right)$ in a consensus set. The numbers in red and green indicate the absolute improvements over baseline and CODET, respectively.
other one is Limit that controls the amount of syntactically correct test cases we extract from each sample, which is set to 5 for all benchmarks in our main experiments. Note that Limit multiplied by the Sampling Number is the maximum number of test cases for a problem, not the exact number, because not every sample contains the Limit number of valid test cases. A valid test case (i.e., assertion statement) should start with "assert" and contain the name of the corresponding entry point function. We can conclude from the results that using more test cases in CODET could generally lead to better performance. While the performance gap narrows when Limit $\geq 3$ and the sampling number $\geq 50$. Moreover, using only 10 test cases per problem for CODET can still improve the baseline pass@1 performance of code-cushman-001 by absolute $4.3 \%$ and code-davinci-002 by absolute $9.5 \%$. It demonstrates that CODET has high test case efficiency and we can use a smaller Sampling Number in real-world application to balance the performance and computation cost.</p>
<h1>I Ablation Study on the Score of Consensus Set</h1>
<p>In CODET, the score of a consensus set is calculated as $f(\mathcal{S})=\left|\mathcal{S}<em y="y">{x}\right|\left|\mathcal{S}</em>}\right|$, where $\mathcal{S<em y="y">{x}$ and $\mathcal{S}</em>}$ are the code solutions and test cases in the consensus set, respectively. We can naturally derive two variants of scoring. One is $f^{\prime}(\mathcal{S})=\left|\mathcal{S<em y="y">{x}\right|$, in line with the idea of self-consistency (Wang et al., 2022), which only considers the number of code solutions with the same functionality. The other one is $f^{\prime \prime}(\mathcal{S})=\left|\mathcal{S}</em>\right|$, which corresponds to simply counting the test cases that each code solution can pass. To evaluate the performance of these two variants, we perform an ablation study on the HumanEval benchmark using three Codex models. The experimental results are summarized in Table 13, from which we can observe that only considering the number of code solutions or test cases for consensus set scoring performs consistently worse than CODET, and even worse than the baseline. Therefore, it is essential to consider the importance of both code solutions and test cases, suggesting the reasonable design of our dual execution agreement.
As mentioned in Section 3, AlphaCode (Li et al., 2022b) also includes a clustering method (denoted as AlphaCode-C) to select the generated code solutions, which shares a similar goal with our ablation method $f^{\prime}$ : clustering code solutions based on code functionality, and then scoring each cluster by size. AlphaCode-C requires a number of additional test inputs to produce outputs from code solutions, which are then used to determine the functional equivalence. AlphaCode-C relies on a separate test input generation model, which needs extra training and annotation. The model is unavailable and hard to replicate, as the paper does not provide sufficient details. We replicate AlphaCode-C by extracting test inputs from the test cases generated by CODET. We run all code solutions on the test inputs, and group them by outputs. The clusters are ranked by size and then we select the code solutions from each cluster in order. From Table 2 and Table 13, we can find that AlphaCode-C is inferior to $f^{\prime}$, though they share the similar idea. The reason is that AlphaCode-C will group the trivial code solutions (e.g., solutions that always output "None", "0", or an empty string with whatever inputs) together, leading to a large cluster of incorrect solutions that significantly affects performance. While such trivial code solutions are hard to pass the generated test cases in CODET, thus having lower consensus scores for ranking. This confirms the effectiveness of considering test case information.</p>
<div class="codehilite"><pre><span></span><code>def remove_vowels(text):
    remove_vowels is a function that takes string and returns string without vowels.
    ***
    Rank #1: The consensus set has 31 solutions and 170
    test cases. An example solution: Correct
        vowels = &#39;aeiouAEIOU&#39;
        text_without_vowels = &#39;&#39;
        for character in text:
            if character not in vowels:
                text_without_vowels += character
            return text_without_vowels
</code></pre></div>

<p>Rank #2: The consensus set has 41 solutions and 147 test cases. An example solution: Incorrect
vowels = set("aeiou")
characters = set(text)
if not (characters - vowels):
return ""
else:
return "",join(c for c in text
if c not in vowels)
(a) The first consensus set has fewer code solutions.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">below_zero</span><span class="p">(</span><span class="nl">operations</span><span class="p">:</span><span class="w"> </span><span class="n">List</span><span class="o">[</span><span class="n">int</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nl">bool</span><span class="p">:</span>
<span class="w">    </span><span class="n">You</span><span class="err">&#39;</span><span class="n">re</span><span class="w"> </span><span class="n">given</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">list</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">deposit</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">withdrawal</span><span class="w"> </span><span class="n">operations</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bank</span><span class="w"> </span><span class="n">account</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">starts</span><span class="w"> </span><span class="k">with</span>
<span class="w">    </span><span class="n">zero</span><span class="w"> </span><span class="n">balance</span><span class="p">.</span><span class="w"> </span><span class="n">Your</span><span class="w"> </span><span class="n">task</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">detect</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="ow">any</span><span class="w"> </span><span class="n">point</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">balance</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">account</span><span class="w"> </span><span class="n">falls</span><span class="w"> </span><span class="n">below</span>
<span class="w">    </span><span class="n">zero</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">point</span><span class="w"> </span><span class="k">function</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">True</span><span class="p">.</span><span class="w"> </span><span class="n">Otherwise</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">False</span><span class="p">.</span>
<span class="w">    </span><span class="o">***</span>
<span class="w">    </span><span class="nf">Rank</span><span class="w"> </span><span class="n">#1</span><span class="err">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">consensus</span><span class="w"> </span><span class="k">set</span><span class="w"> </span><span class="n">has</span><span class="w"> </span><span class="mi">79</span><span class="w"> </span><span class="n">solutions</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="mi">248</span>
<span class="w">    </span><span class="n">test</span><span class="w"> </span><span class="n">cases</span><span class="p">.</span><span class="w"> </span><span class="n">An</span><span class="w"> </span><span class="n">example</span><span class="w"> </span><span class="nl">solution</span><span class="p">:</span><span class="w"> </span><span class="n">Correct</span>
<span class="w">        </span><span class="n">zero_balance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="k">operation</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="nl">operations</span><span class="p">:</span>
<span class="w">            </span><span class="n">zero_balance</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="k">operation</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="n">zero_balance</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">0</span><span class="err">:</span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="k">True</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">False</span>
</code></pre></div>

<p>(b) The first consensus set has fewer test cases.</p>
<p>Figure 10: Two cases from the HumanEval benchmark, where CODET can find the correct consensus sets though they have (a) fewer code solutions, or (b) fewer test cases.</p>
<h1>J MORE EXAMPLES FOR CASE STUDY</h1>
<p>Figure 10 illustrates two cases that CODET can successfully find the correct consensus sets. Specifically, the case in Figure 10a requires to remove the vowels in the input text. There are 41 incorrect solutions and 147 test cases in the consensus set ranked 2 , which forget to remove the upper-case vowels. Though the correct solutions in the top 1 consensus set are fewer (i.e., 31), they can pass more test cases (i.e., 170) and thus have a higher score. The case in Figure 10b is to decide when the balance of account will fall below zero. The functionality of the incorrect solutions in the second consensus set is to tell whether there are withdrawing operations. Nevertheless, the incorrect solutions can pass more test cases (i.e., 255) than the correct solutions (i.e., 248) in the top 1 consensus set. Fortunately, there are 79 correct solutions and only 6 incorrect solutions, making it possible for CODET to rank the correct consensus ahead. Both cases demonstrate the plausibility of using the dual execution agreement instead of solely considering the functional agreement between code solutions or the number of passed test cases.</p>
<p>Figure 11 illustrates the cases that CODET fails to find the correct consensus sets. Specifically, Figure 11a demonstrates the situation that there are partially correct solutions that may fail at certain corner cases. In the example, there are 20 incorrect solutions in the top 1 consensus set that can pass 205 test cases, which will fail if the input is a string of length 1 . The correct consensus set ranked 3 has more test cases (i.e., 222), while it has a lower consensus score due to the small number of code solutions (i.e., 9). The second example in Figure 11b shows the most common situation where CODET fails because the model cannot fully understand the problem. We can find that the incorrect solutions in the top 1 consensus set are totally missing the points of the given problem. While the model still tends to generate more incorrect solutions and test cases based on its wrong understanding. All the bad cases call for future improvements on the quality of generated code solutions and test cases.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" />
(a) Uncovered corner cases.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">def minSubArraySum(nums):</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Given an array of integers nums, find the minimum sum of any non-empty sub-array of nums.</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Rank #1: The consensus set has 16 solutions and 102 test cases. An example solution:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Rank #2: The consensus set has 7 solutions and 96 test cases. An example solution: Correct</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">if not nums:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">return 0</td>
<td style="text-align: center;">if not nums:</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">total = nums[0]</td>
<td style="text-align: center;">return 0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">min_sum = total</td>
<td style="text-align: center;">min_sum = float('inf')</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">for 1 in range(1, len(nums)):</td>
<td style="text-align: center;">$n=$ len(nums)</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">if total &lt; 0:</td>
<td style="text-align: center;">for 1 in range $(n)$ :</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">total = nums[i]</td>
<td style="text-align: center;">curr_sum $=0$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">else:</td>
<td style="text-align: center;">for j in range(1, m):</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">total += nums[i]</td>
<td style="text-align: center;">curr_sum += nums[j]</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">min_sum = min(min_sum, total)</td>
<td style="text-align: center;">min_sum = min(min_sum, curr_sum)</td>
</tr>
<tr>
<td style="text-align: center;">return min_sum</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">return min_sum</td>
</tr>
</tbody>
</table>
<p>(b) Failure of Problem Understanding.</p>
<p>Figure 11: Three incorrect cases from the HumanEval benchmark, where CODET cannot find the correct consensus sets due to (a) uncovered corner cases, or (b) failure of problem understanding.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://coverage.readthedocs.io/en/6.4.2&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
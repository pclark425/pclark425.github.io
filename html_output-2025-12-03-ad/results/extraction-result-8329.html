<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8329 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8329</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8329</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-153.html">extraction-schema-153</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-271710997</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.10276v7.pdf" target="_blank">Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we first investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act or reason correspondingly in text. By evaluating on classic spatial planning scenarios, we found that current LLMs still lack abilities to handle spatial relationships in texts. This arises a question: Is the natural language the best way to represent complex spatial environments for LLMs, or are other alternatives such as symbolic representations more efficient and effective for LLMs? To this end, we propose a novel method called C O S ( C hain-o f-S ymbol Prompting) that represents the spatial relationships with condensed symbols during the chained intermediate thinking steps. C O S is easy to use and does not need additional training on LLMs. Extensive experiments indicate that C O S clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting described in natural language in all three spatial reasoning and planning tasks with even fewer tokens used in the inputs compared with CoT. The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World for GPT-3.5-Turbo. C O S also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World. Interestingly, we also observed emergent ability of abstract symbols understanding when the size of models scales up. 1</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8329.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8329.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo (ChatGPT) - Brick World</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (ChatGPT) evaluated on Brick World</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source conversational LLM (ChatGPT / gpt-3.5-turbo) evaluated in the paper on synthetic text-only spatial planning tasks (Brick World 1D and 2D) using zero-shot CoT, few-shot CoT and the proposed Chain-of-Symbol (COS) prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's conversational transformer model (gpt-3.5-turbo) used via API with temperature=0 for deterministic few-shot evaluation; treated as a representative closed-source LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Brick World (1D and 2D stacking/task planning)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>text-described stacking and spatial planning task (1D vertical stacks and 2D with horizontal 'in front of' relations); requires reasoning about occlusion/ordering and multi-step removal actions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Models received natural-language descriptions of stacked/bricked environments and a goal (e.g., 'How to get brick X?'). Three prompting variants were used: zero-shot CoT (prompt 'Let's think step by step'), few-shot CoT (5 human-corrected CoT demonstrations), and few-shot Chain-of-Symbol (COS) where intermediate CoT steps are converted to compact symbolic relations (symbols like '/', '=', triplets) and redundant natural language removed; temperature set to 0; evaluated on 4,000 generated instances (500 per setting). Outputs were sequences of removals (ordered lists of bricks).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Compared standard Chain-of-Thought (natural-language intermediate steps) vs Chain-of-Symbol (COS) where spatial relations are represented by concise symbols (e.g., '/' for 'on top of', triplets '(size,shape,color)'). COS demonstrations were hand-created by converting correct CoT traces into symbol sequences; models generate COS/CoT during inference in a few-shot manner.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>COS produced large gains for GPT-3.5-Turbo on Brick World: e.g., on 1D Shuffle-Label setting accuracy rose from 31.8% (CoT) to 92.6% (COS) — a 60.8 percentage-point improvement reported by the paper; Table 1 reports many values: for 1D No-Shuffle average acc: zs-CoT 61.0%, few-shot CoT 81.0% (±11.0), COS 96.6% (±1.9); 1D Shuffle-Both acc: zs-CoT 9.x% (very low), CoT 43.0% (±4.4), COS 69.7% (±5.1); 2D scenarios show lower absolute performance but COS > CoT (e.g., 2D No-Shuffle COS 60.7% vs zs-CoT ~32.7% / CoT 25.0% reported). The paper also reports token savings: intermediate-step tokens reduced from 407 (CoT demonstrations) to 139 (COS) for Brick World 1D.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Quantitative: improved goal-achievement accuracy and precision/recall when using COS vs CoT/zero-shot CoT on tasks that require tracking stacking and front/behind relations indicates models can leverage symbolic intermediate representations to better perform spatial planning. Qualitative examples and few-shot exemplars show the models produce correct multi-step removal sequences after COS prompting. The authors also measured Longest Common Subsequence (LCS)-based precision/recall to compare sequences against ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct comparisons in the paper: zero-shot CoT vs few-shot CoT vs few-shot COS on the same datasets with same number of demonstrations (N_s=5). COS consistently outperformed CoT across Brick World settings for GPT-3.5-Turbo, with much higher accuracy and lower variance. No human-baseline performance numbers are given in the paper for Brick World.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CoT and zero-shot CoT perform poorly on harder settings (shuffled labels/descriptions and 2D scenarios). Some settings remain challenging even with COS (2D harder than 1D). The paper notes results may vary over time for closed-source APIs and that only a limited set of models were evaluated. Specific failure modes include incorrect handling when natural-language descriptions are shuffled or more complex relations exist (CoT often confused); COS reduces but does not fully eliminate failures in harder settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8329.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8329.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo - NLVR-based Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (ChatGPT) evaluated on NLVR-based Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>gpt-3.5-turbo evaluated on an NLVR-derived textual planning task where models must identify objects meeting a property (color/shape) across three 'boxes' and output the set to move to a target box.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>See previous entry; tested with few-shot CoT and few-shot COS; temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>NLVR-based Manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>synthetic box-based object manipulation planning derived from Natural Language Visual Reasoning (NLVR) — set selection and planning involving spatial relations inside boxes (properties: color, shape, size).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Synthetic NLVR descriptions (1,000 instances) were generated in text (three boxes, objects described by triplets). Question: move all objects satisfying a condition to a target box; ground truth is set of objects to be moved. Evaluated with zero-shot CoT / few-shot CoT / few-shot COS with 5 demonstrations; precision and recall computed as set-based metrics (not sequence-LCS).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>COS represents each object as triplets (size,shape,color) and uses symbols to encode relations; CoT uses natural-language reasoning steps. Model produces a set (or sequence) of selected objects / actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paper reports COS outperforms CoT and zero-shot CoT on accuracy/precision/recall across NLVR-based Manipulation (exact table entries are in the paper's Table 2). The paper also reports token reduction for intermediate steps: e.g., for NLVR-based Manipulation COS reduced tokens from 653 to 534 for the intermediate steps (reported). Exact numeric metrics per run are in Table 2 (averages and standard deviations from three runs).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Improved set-precision/recall when using COS indicates better object-selection according to spatially-specified properties; COS triplet representation more directly encodes object attributes and relations, which the authors interpret as evidence that symbolic condensed representations help LLMs perform spatially-grounded selection/planning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared few-shot CoT and zero-shot CoT with few-shot COS (all with same number of demonstrations). COS had higher precision/recall. No explicit human baselines provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>NLVR-based Manipulation remains challenging for some cases with many objects / complex relations; the paper reports COS reduces but does not eliminate errors. The token-reduction claim and some numeric reductions are approximate and the authors note measurement choices in Appendix. The COS advantage was smaller on some harder 2D-like instances.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8329.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8329.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo - Natural Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (ChatGPT) evaluated on Natural Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>gpt-3.5-turbo evaluated on a text-only navigation planning task over landmarks connected in a binary-tree-like map described via natural language; the task asks for the route to the nearest landmark of a given type.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>As above; used with zero-shot CoT, few-shot CoT, few-shot COS; deterministic temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Natural Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>2D-like tree-structured navigation planning described in text (landmarks and road distances); requires path-finding and shortest-distance reasoning across textual map.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>500 generated instances: textual description of landmarks (7–10 per instance) arranged in a binary-tree graph with edges of 100m/200m lengths; question asks how to reach the nearest specific kind of landmark from a start point. Models were prompted with few-shot CoT or COS demonstrations and asked to output ordered path (landmarks separated by commas). Evaluation metric: accuracy (success rate to reach nearest target).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>COS represents landmark sequences with symbols (e.g., '/' to connect ordered landmarks) and condenses the path reasoning; CoT uses step-by-step natural-language reasoning. The model outputs path strings derived from the condensed symbolic intermediate reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>COS outperformed CoT and zero-shot CoT on reported accuracy (Table 2), with average and standard deviation reported across three runs; specific example: the paper gives route examples and summarises that COS gives higher performance for GPT-3.5-Turbo. Exact numeric values are in Table 2 of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Improved accuracy on shortest-path/nearest landmark queries when using COS suggests the model is leveraging the condensed representation to compute distances and compare alternatives — empirical evidence comes from higher success rates and qualitative examples.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared against few-shot CoT and zero-shot CoT with N_s=5 demonstrations. COS had higher accuracy and used fewer tokens for intermediate steps compared with CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Some navigation instances with multiple similar-distance paths or deeper tree nodes remained failure cases; overall 2D/tree tasks were more difficult than simple 1D stacking. The paper notes limited evaluation budget and not exhaustive coverage of failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8329.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8329.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-Turbo - SPARTUN (Spatial QA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-Turbo (ChatGPT) evaluated on SPARTUN spatial question answering benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>gpt-3.5-turbo evaluated (few-shot CoT and few-shot COS) on SPARTUN, a human-annotated spatial QA dataset with many spatial-relation types and natural questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-3.5-turbo (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>As above; used in experiments applying both COS and CoT (5-shot) on SPARTUN; temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SPARTUN (spatial question answering dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>textual spatial question answering with diverse spatial relations and expressions (which/what/where style queries over described scenes).</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Applied few-shot CoT and few-shot COS (5-shot) to SPARTUN and measured accuracy as the metric. Authors manually corrected CoT demonstrations and converted them to COS symbols for few-shot prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>COS uses symbols (e.g., '=', '∼', '/', triplets) to encode relations and object attributes in concise chains; CoT uses natural-language intermediates. For SPARTUN the COS demonstrations included many symbols because the dataset contains many relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 3 reports results: for GPT-3.5-Turbo, CoT accuracy reported around 54.7% and CoS around 54.9% as listed (paper's table formatting is compact); for GPT-4, CoT ~69.8% and CoS ~72.6% (numbers reported in the paper's Table 3). COS used fewer intermediate tokens (paper reports CoT token counts ~198 vs CoS ~174.3 in that table).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>COS provided improved accuracy for GPT-4 and slight improvements for GPT-3.5-Turbo on SPARTUN per the reported table, suggesting that condensed symbolic representation helps with complex spatial relation reasoning; qualitative COS/CoT exemplar pairs are provided showing clearer symbolic reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared CoT vs COS for both GPT-3.5-Turbo and GPT-4. COS improved GPT-4 by a few percentage points and slightly improved or matched GPT-3.5-Turbo according to reported numbers. No direct human performance baselines provided in this paper for SPARTUN.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SPARTUN includes many relation types, and COS requires mapping relations to symbols (more symbols required). The authors note COS still faces limits when many distinct relations must be symbolized and corrections of generated CoT traces were needed to produce reliable COS demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8329.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8329.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2 series - Spatial tasks (scaling analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2 (open-source) models evaluated at multiple sizes (7B, 13B, 70B) on the three spatial planning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source Llama-2 family (multiple parameter sizes) was evaluated with CoT and COS prompting to study scaling effects and emergent symbol-understanding for spatial tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2 (multiple sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source transformer LLM family (Llama-2) evaluated at 7B, 13B and 70B parameter sizes; used with deterministic decoding (temperature=0) and same prompting methods (zero-shot CoT / few-shot CoT / few-shot COS).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 13B, 70B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Brick World, NLVR-based Manipulation, Natural Language Navigation</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Text-described spatial planning tasks (stacking, box-object manipulation, navigation) requiring relational and sequential spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Same tasks/datasets as for GPT-3.5-Turbo; experiments compared few-shot CoT vs few-shot COS across model sizes to observe scaling effects. N_s=5 demonstrations, temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>Same prompting strategies: CoT natural-language chains vs COS symbolic chains. Authors observe whether COS representation aids different model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Authors report that for smaller Llama-2 sizes (7B and 13B) COS does not consistently outperform CoT in many cases, but at 70B COS yields clear better performance across the three tasks. The paper shows a scaling curve (Figure 4) indicating the performance of COS improves more steeply with model size than CoT. Exact per-size numbers are reported in the figure and described qualitatively in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Scaling behavior: emergent ability of abstract-symbol understanding as model size grows (70B shows marked COS advantage), interpreted as evidence that larger LLMs can better leverage condensed symbolic intermediate representations to perform spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Ablation across model sizes (7B, 13B, 70B) comparing CoT vs COS; COS advantage increases with model size. The paper explicitly contrasts COS and CoT performance curves.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>On smaller Llama-2 sizes (7B, 13B) COS did not consistently outperform CoT; authors note that COS seems to require sufficiently large models to realize its full benefits (emergent phenomenon). Limited compute/resources meant only a small set of sizes were tested and no extensive hyperparameter sweep was performed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8329.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8329.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs) being evaluated on puzzle games that require spatial knowledge (such as Sudoku, Rubik's Cube, Minesweeper, etc.), including details of the models, the puzzles, the evaluation setup, the mechanisms or strategies used by the models, performance metrics, evidence of spatial reasoning, comparisons to other models or humans, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 - SPARTUN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on SPARTUN spatial question answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 was evaluated (few-shot CoT vs few-shot COS) on the SPARTUN spatial QA benchmark and shown to benefit from COS prompting in the paper's reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's larger generative pre-trained transformer (GPT-4) used as a closed-source baseline for spatial QA; run with few-shot CoT and few-shot COS (N_s=5) and temperature=0.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>SPARTUN (spatial question answering)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>Textual spatial QA with diverse relation types requiring multi-relation reasoning and comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>task_setup</strong></td>
                            <td>Few-shot CoT and few-shot COS prompts (5 demonstrations) applied to SPARTUN; accuracy reported as the primary metric; intermediate-step token counts recorded for CoT vs COS demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanisms_or_strategies</strong></td>
                            <td>COS uses compact symbolic relation encodings and triplets for objects; GPT-4 processes these symbolic intermediate steps to derive answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 3 shows GPT-4 CoT accuracy reported ~69.8% and COS accuracy ~72.6% (numbers as reported in the paper's table), with COS using fewer intermediate tokens (CoT ~198 tokens vs COS ~174.3 tokens in demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>GPT-4's higher absolute accuracy on SPARTUN and further improvement under COS suggests that larger models can better exploit symbolic intermediate representations to perform complex spatial reasoning across many relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Direct comparison in the paper between GPT-4 CoT and GPT-4 COS; COS gave an improvement of a few percentage points and reduced token usage for intermediate steps. Also compared to GPT-3.5-Turbo results on the same benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No per-question failure breakdown is provided; the paper notes many relation types in SPARTUN increase symbol complexity for COS, and manual correction was necessary to create reliable COS demonstrations. The marginal COS gain for smaller models suggests dependency on model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SPARTQA: A textual question answering benchmark for spatial reasoning <em>(Rating: 2)</em></li>
                <li>Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions <em>(Rating: 2)</em></li>
                <li>Representation learning for grounded spatial reasoning <em>(Rating: 1)</em></li>
                <li>Vision-and-language navigation: A survey of tasks, methods, and future directions <em>(Rating: 1)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Symbol tuning improves in-context learning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8329",
    "paper_id": "paper-271710997",
    "extraction_schema_id": "extraction-schema-153",
    "extracted_data": [
        {
            "name_short": "GPT-3.5-Turbo (ChatGPT) - Brick World",
            "name_full": "GPT-3.5-Turbo (ChatGPT) evaluated on Brick World",
            "brief_description": "Closed-source conversational LLM (ChatGPT / gpt-3.5-turbo) evaluated in the paper on synthetic text-only spatial planning tasks (Brick World 1D and 2D) using zero-shot CoT, few-shot CoT and the proposed Chain-of-Symbol (COS) prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (ChatGPT)",
            "model_description": "OpenAI's conversational transformer model (gpt-3.5-turbo) used via API with temperature=0 for deterministic few-shot evaluation; treated as a representative closed-source LLM.",
            "model_size": null,
            "puzzle_name": "Brick World (1D and 2D stacking/task planning)",
            "puzzle_type": "text-described stacking and spatial planning task (1D vertical stacks and 2D with horizontal 'in front of' relations); requires reasoning about occlusion/ordering and multi-step removal actions.",
            "task_setup": "Models received natural-language descriptions of stacked/bricked environments and a goal (e.g., 'How to get brick X?'). Three prompting variants were used: zero-shot CoT (prompt 'Let's think step by step'), few-shot CoT (5 human-corrected CoT demonstrations), and few-shot Chain-of-Symbol (COS) where intermediate CoT steps are converted to compact symbolic relations (symbols like '/', '=', triplets) and redundant natural language removed; temperature set to 0; evaluated on 4,000 generated instances (500 per setting). Outputs were sequences of removals (ordered lists of bricks).",
            "mechanisms_or_strategies": "Compared standard Chain-of-Thought (natural-language intermediate steps) vs Chain-of-Symbol (COS) where spatial relations are represented by concise symbols (e.g., '/' for 'on top of', triplets '(size,shape,color)'). COS demonstrations were hand-created by converting correct CoT traces into symbol sequences; models generate COS/CoT during inference in a few-shot manner.",
            "performance_metrics": "COS produced large gains for GPT-3.5-Turbo on Brick World: e.g., on 1D Shuffle-Label setting accuracy rose from 31.8% (CoT) to 92.6% (COS) — a 60.8 percentage-point improvement reported by the paper; Table 1 reports many values: for 1D No-Shuffle average acc: zs-CoT 61.0%, few-shot CoT 81.0% (±11.0), COS 96.6% (±1.9); 1D Shuffle-Both acc: zs-CoT 9.x% (very low), CoT 43.0% (±4.4), COS 69.7% (±5.1); 2D scenarios show lower absolute performance but COS &gt; CoT (e.g., 2D No-Shuffle COS 60.7% vs zs-CoT ~32.7% / CoT 25.0% reported). The paper also reports token savings: intermediate-step tokens reduced from 407 (CoT demonstrations) to 139 (COS) for Brick World 1D.",
            "evidence_of_spatial_reasoning": "Quantitative: improved goal-achievement accuracy and precision/recall when using COS vs CoT/zero-shot CoT on tasks that require tracking stacking and front/behind relations indicates models can leverage symbolic intermediate representations to better perform spatial planning. Qualitative examples and few-shot exemplars show the models produce correct multi-step removal sequences after COS prompting. The authors also measured Longest Common Subsequence (LCS)-based precision/recall to compare sequences against ground truth.",
            "comparisons": "Direct comparisons in the paper: zero-shot CoT vs few-shot CoT vs few-shot COS on the same datasets with same number of demonstrations (N_s=5). COS consistently outperformed CoT across Brick World settings for GPT-3.5-Turbo, with much higher accuracy and lower variance. No human-baseline performance numbers are given in the paper for Brick World.",
            "limitations_or_failure_cases": "CoT and zero-shot CoT perform poorly on harder settings (shuffled labels/descriptions and 2D scenarios). Some settings remain challenging even with COS (2D harder than 1D). The paper notes results may vary over time for closed-source APIs and that only a limited set of models were evaluated. Specific failure modes include incorrect handling when natural-language descriptions are shuffled or more complex relations exist (CoT often confused); COS reduces but does not fully eliminate failures in harder settings.",
            "uuid": "e8329.0",
            "source_info": {
                "paper_title": "Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo - NLVR-based Manipulation",
            "name_full": "GPT-3.5-Turbo (ChatGPT) evaluated on NLVR-based Manipulation",
            "brief_description": "gpt-3.5-turbo evaluated on an NLVR-derived textual planning task where models must identify objects meeting a property (color/shape) across three 'boxes' and output the set to move to a target box.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (ChatGPT)",
            "model_description": "See previous entry; tested with few-shot CoT and few-shot COS; temperature=0.",
            "model_size": null,
            "puzzle_name": "NLVR-based Manipulation",
            "puzzle_type": "synthetic box-based object manipulation planning derived from Natural Language Visual Reasoning (NLVR) — set selection and planning involving spatial relations inside boxes (properties: color, shape, size).",
            "task_setup": "Synthetic NLVR descriptions (1,000 instances) were generated in text (three boxes, objects described by triplets). Question: move all objects satisfying a condition to a target box; ground truth is set of objects to be moved. Evaluated with zero-shot CoT / few-shot CoT / few-shot COS with 5 demonstrations; precision and recall computed as set-based metrics (not sequence-LCS).",
            "mechanisms_or_strategies": "COS represents each object as triplets (size,shape,color) and uses symbols to encode relations; CoT uses natural-language reasoning steps. Model produces a set (or sequence) of selected objects / actions.",
            "performance_metrics": "Paper reports COS outperforms CoT and zero-shot CoT on accuracy/precision/recall across NLVR-based Manipulation (exact table entries are in the paper's Table 2). The paper also reports token reduction for intermediate steps: e.g., for NLVR-based Manipulation COS reduced tokens from 653 to 534 for the intermediate steps (reported). Exact numeric metrics per run are in Table 2 (averages and standard deviations from three runs).",
            "evidence_of_spatial_reasoning": "Improved set-precision/recall when using COS indicates better object-selection according to spatially-specified properties; COS triplet representation more directly encodes object attributes and relations, which the authors interpret as evidence that symbolic condensed representations help LLMs perform spatially-grounded selection/planning.",
            "comparisons": "Compared few-shot CoT and zero-shot CoT with few-shot COS (all with same number of demonstrations). COS had higher precision/recall. No explicit human baselines provided.",
            "limitations_or_failure_cases": "NLVR-based Manipulation remains challenging for some cases with many objects / complex relations; the paper reports COS reduces but does not eliminate errors. The token-reduction claim and some numeric reductions are approximate and the authors note measurement choices in Appendix. The COS advantage was smaller on some harder 2D-like instances.",
            "uuid": "e8329.1",
            "source_info": {
                "paper_title": "Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo - Natural Language Navigation",
            "name_full": "GPT-3.5-Turbo (ChatGPT) evaluated on Natural Language Navigation",
            "brief_description": "gpt-3.5-turbo evaluated on a text-only navigation planning task over landmarks connected in a binary-tree-like map described via natural language; the task asks for the route to the nearest landmark of a given type.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (ChatGPT)",
            "model_description": "As above; used with zero-shot CoT, few-shot CoT, few-shot COS; deterministic temperature.",
            "model_size": null,
            "puzzle_name": "Natural Language Navigation",
            "puzzle_type": "2D-like tree-structured navigation planning described in text (landmarks and road distances); requires path-finding and shortest-distance reasoning across textual map.",
            "task_setup": "500 generated instances: textual description of landmarks (7–10 per instance) arranged in a binary-tree graph with edges of 100m/200m lengths; question asks how to reach the nearest specific kind of landmark from a start point. Models were prompted with few-shot CoT or COS demonstrations and asked to output ordered path (landmarks separated by commas). Evaluation metric: accuracy (success rate to reach nearest target).",
            "mechanisms_or_strategies": "COS represents landmark sequences with symbols (e.g., '/' to connect ordered landmarks) and condenses the path reasoning; CoT uses step-by-step natural-language reasoning. The model outputs path strings derived from the condensed symbolic intermediate reasoning.",
            "performance_metrics": "COS outperformed CoT and zero-shot CoT on reported accuracy (Table 2), with average and standard deviation reported across three runs; specific example: the paper gives route examples and summarises that COS gives higher performance for GPT-3.5-Turbo. Exact numeric values are in Table 2 of the paper.",
            "evidence_of_spatial_reasoning": "Improved accuracy on shortest-path/nearest landmark queries when using COS suggests the model is leveraging the condensed representation to compute distances and compare alternatives — empirical evidence comes from higher success rates and qualitative examples.",
            "comparisons": "Compared against few-shot CoT and zero-shot CoT with N_s=5 demonstrations. COS had higher accuracy and used fewer tokens for intermediate steps compared with CoT.",
            "limitations_or_failure_cases": "Some navigation instances with multiple similar-distance paths or deeper tree nodes remained failure cases; overall 2D/tree tasks were more difficult than simple 1D stacking. The paper notes limited evaluation budget and not exhaustive coverage of failure modes.",
            "uuid": "e8329.2",
            "source_info": {
                "paper_title": "Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-3.5-Turbo - SPARTUN (Spatial QA)",
            "name_full": "GPT-3.5-Turbo (ChatGPT) evaluated on SPARTUN spatial question answering benchmark",
            "brief_description": "gpt-3.5-turbo evaluated (few-shot CoT and few-shot COS) on SPARTUN, a human-annotated spatial QA dataset with many spatial-relation types and natural questions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "gpt-3.5-turbo (ChatGPT)",
            "model_description": "As above; used in experiments applying both COS and CoT (5-shot) on SPARTUN; temperature=0.",
            "model_size": null,
            "puzzle_name": "SPARTUN (spatial question answering dataset)",
            "puzzle_type": "textual spatial question answering with diverse spatial relations and expressions (which/what/where style queries over described scenes).",
            "task_setup": "Applied few-shot CoT and few-shot COS (5-shot) to SPARTUN and measured accuracy as the metric. Authors manually corrected CoT demonstrations and converted them to COS symbols for few-shot prompts.",
            "mechanisms_or_strategies": "COS uses symbols (e.g., '=', '∼', '/', triplets) to encode relations and object attributes in concise chains; CoT uses natural-language intermediates. For SPARTUN the COS demonstrations included many symbols because the dataset contains many relation types.",
            "performance_metrics": "Table 3 reports results: for GPT-3.5-Turbo, CoT accuracy reported around 54.7% and CoS around 54.9% as listed (paper's table formatting is compact); for GPT-4, CoT ~69.8% and CoS ~72.6% (numbers reported in the paper's Table 3). COS used fewer intermediate tokens (paper reports CoT token counts ~198 vs CoS ~174.3 in that table).",
            "evidence_of_spatial_reasoning": "COS provided improved accuracy for GPT-4 and slight improvements for GPT-3.5-Turbo on SPARTUN per the reported table, suggesting that condensed symbolic representation helps with complex spatial relation reasoning; qualitative COS/CoT exemplar pairs are provided showing clearer symbolic reasoning traces.",
            "comparisons": "Compared CoT vs COS for both GPT-3.5-Turbo and GPT-4. COS improved GPT-4 by a few percentage points and slightly improved or matched GPT-3.5-Turbo according to reported numbers. No direct human performance baselines provided in this paper for SPARTUN.",
            "limitations_or_failure_cases": "SPARTUN includes many relation types, and COS requires mapping relations to symbols (more symbols required). The authors note COS still faces limits when many distinct relations must be symbolized and corrections of generated CoT traces were needed to produce reliable COS demonstrations.",
            "uuid": "e8329.3",
            "source_info": {
                "paper_title": "Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Llama-2 series - Spatial tasks (scaling analysis)",
            "name_full": "Llama-2 (open-source) models evaluated at multiple sizes (7B, 13B, 70B) on the three spatial planning tasks",
            "brief_description": "Open-source Llama-2 family (multiple parameter sizes) was evaluated with CoT and COS prompting to study scaling effects and emergent symbol-understanding for spatial tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2 (multiple sizes)",
            "model_description": "Open-source transformer LLM family (Llama-2) evaluated at 7B, 13B and 70B parameter sizes; used with deterministic decoding (temperature=0) and same prompting methods (zero-shot CoT / few-shot CoT / few-shot COS).",
            "model_size": "7B, 13B, 70B",
            "puzzle_name": "Brick World, NLVR-based Manipulation, Natural Language Navigation",
            "puzzle_type": "Text-described spatial planning tasks (stacking, box-object manipulation, navigation) requiring relational and sequential spatial reasoning.",
            "task_setup": "Same tasks/datasets as for GPT-3.5-Turbo; experiments compared few-shot CoT vs few-shot COS across model sizes to observe scaling effects. N_s=5 demonstrations, temperature=0.",
            "mechanisms_or_strategies": "Same prompting strategies: CoT natural-language chains vs COS symbolic chains. Authors observe whether COS representation aids different model sizes.",
            "performance_metrics": "Authors report that for smaller Llama-2 sizes (7B and 13B) COS does not consistently outperform CoT in many cases, but at 70B COS yields clear better performance across the three tasks. The paper shows a scaling curve (Figure 4) indicating the performance of COS improves more steeply with model size than CoT. Exact per-size numbers are reported in the figure and described qualitatively in the text.",
            "evidence_of_spatial_reasoning": "Scaling behavior: emergent ability of abstract-symbol understanding as model size grows (70B shows marked COS advantage), interpreted as evidence that larger LLMs can better leverage condensed symbolic intermediate representations to perform spatial reasoning.",
            "comparisons": "Ablation across model sizes (7B, 13B, 70B) comparing CoT vs COS; COS advantage increases with model size. The paper explicitly contrasts COS and CoT performance curves.",
            "limitations_or_failure_cases": "On smaller Llama-2 sizes (7B, 13B) COS did not consistently outperform CoT; authors note that COS seems to require sufficiently large models to realize its full benefits (emergent phenomenon). Limited compute/resources meant only a small set of sizes were tested and no extensive hyperparameter sweep was performed.",
            "uuid": "e8329.4",
            "source_info": {
                "paper_title": "Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 - SPARTUN",
            "name_full": "GPT-4 evaluated on SPARTUN spatial question answering",
            "brief_description": "GPT-4 was evaluated (few-shot CoT vs few-shot COS) on the SPARTUN spatial QA benchmark and shown to benefit from COS prompting in the paper's reported results.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's larger generative pre-trained transformer (GPT-4) used as a closed-source baseline for spatial QA; run with few-shot CoT and few-shot COS (N_s=5) and temperature=0.",
            "model_size": null,
            "puzzle_name": "SPARTUN (spatial question answering)",
            "puzzle_type": "Textual spatial QA with diverse relation types requiring multi-relation reasoning and comparison.",
            "task_setup": "Few-shot CoT and few-shot COS prompts (5 demonstrations) applied to SPARTUN; accuracy reported as the primary metric; intermediate-step token counts recorded for CoT vs COS demonstrations.",
            "mechanisms_or_strategies": "COS uses compact symbolic relation encodings and triplets for objects; GPT-4 processes these symbolic intermediate steps to derive answers.",
            "performance_metrics": "Table 3 shows GPT-4 CoT accuracy reported ~69.8% and COS accuracy ~72.6% (numbers as reported in the paper's table), with COS using fewer intermediate tokens (CoT ~198 tokens vs COS ~174.3 tokens in demonstrations).",
            "evidence_of_spatial_reasoning": "GPT-4's higher absolute accuracy on SPARTUN and further improvement under COS suggests that larger models can better exploit symbolic intermediate representations to perform complex spatial reasoning across many relation types.",
            "comparisons": "Direct comparison in the paper between GPT-4 CoT and GPT-4 COS; COS gave an improvement of a few percentage points and reduced token usage for intermediate steps. Also compared to GPT-3.5-Turbo results on the same benchmark.",
            "limitations_or_failure_cases": "No per-question failure breakdown is provided; the paper notes many relation types in SPARTUN increase symbol complexity for COS, and manual correction was necessary to create reliable COS demonstrations. The marginal COS gain for smaller models suggests dependency on model capacity.",
            "uuid": "e8329.5",
            "source_info": {
                "paper_title": "Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SPARTQA: A textual question answering benchmark for spatial reasoning",
            "rating": 2,
            "sanitized_title": "spartqa_a_textual_question_answering_benchmark_for_spatial_reasoning"
        },
        {
            "paper_title": "Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions",
            "rating": 2,
            "sanitized_title": "can_large_language_models_play_text_games_well_current_stateoftheart_and_open_questions"
        },
        {
            "paper_title": "Representation learning for grounded spatial reasoning",
            "rating": 1,
            "sanitized_title": "representation_learning_for_grounded_spatial_reasoning"
        },
        {
            "paper_title": "Vision-and-language navigation: A survey of tasks, methods, and future directions",
            "rating": 1,
            "sanitized_title": "visionandlanguage_navigation_a_survey_of_tasks_methods_and_future_directions"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Symbol tuning improves in-context learning in language models",
            "rating": 1,
            "sanitized_title": "symbol_tuning_improves_incontext_learning_in_language_models"
        }
    ],
    "cost": 0.018885,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models</p>
<p>Hanxu Hu 
Westlake University</p>
<p>University of Edinburgh</p>
<p>Hongyuan Lu hylu@se.cuhk.edu 
The Chinese University of Hong Kong</p>
<p>Huajian Zhang 
University of Edinburgh</p>
<p>Yun-Ze Song 
Westlake University</p>
<p>Wai Lam wlam@se.cuhk.edu 
The Chinese University of Hong Kong</p>
<p>Yue Zhang zhangyue@westlake.edu.cn 
Westlake University</p>
<p>Chain-of-Symbol Prompting for Spatial Reasoning in Large Language Models
0F5D5A521451BA61438AC1CEDDC23472
In this paper, we first investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act or reason correspondingly in text.By evaluating on classic spatial planning scenarios, we found that current LLMs still lack abilities to handle spatial relationships in texts.This arises a question: Is the natural language the best way to represent complex spatial environments for LLMs, or are other alternatives such as symbolic representations more efficient and effective for LLMs?To this end, we propose a novel method called COS (Chain-of-Symbol Prompting) that represents the spatial relationships with condensed symbols during the chained intermediate thinking steps.COS is easy to use and does not need additional training on LLMs.Extensive experiments indicate that COS clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting described in natural language in all three spatial reasoning and planning tasks with even fewer tokens used in the inputs compared with CoT.The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World for GPT-3.5-Turbo.COS also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World.Interestingly, we also observed emergent ability of abstract symbols understanding when the size of models scales up. 1 * Equal Contribution † Corresponding Author 1 Our code are available at https://github.com/hanxuhu/chain-of-symbol-planning.• We evaluate LLMs on both existing classic spatial understanding tasks and our proposed synthetic spatial planning tasks.We spot that there is still room for performance improvements on current LLMs even with CoT.• We propose a novel method called COS, which prompts LLMs to convert the complex environment described with natural language into condensed symbolic</p>
<p>Introduction</p>
<p>Given a set of target behaviour examples, large language models (LLMs) demonstrate exceptional abilities to accomplish a wide range of tasks, frequently exhibiting performance that surpasses that of humans (Brown et al., 2020;Srivastava et al., 2022).Specifically, LLMs exhibit impressive sequential textual reasoning ability during inference, resulting in a significant boost in their performance when encountered with reasoning questions described in natural languages (Nye et al., 2021;Wei et al., 2022).This phenomenon can be clearly observed with a multi-step chain of intermediate thinking procedure, i.e., a "Chain of Thought" (CoT, Wei et al. 2022).</p>
<p>Conventional CoT usually leverages natural languages as intermediate thinking steps in prompting.Although CoT can enhance LLMs' ability in many cases, redundant natural languages and irrelevant information also can hamper the performance of LLMs (Shi et al., 2023) in some cases.For example, spatial languages and descriptions can be hard for language models to understand Mirzaee et al. 2021;Mirzaee &amp; Kordjamshidi 2022  There are a set of bricks.The yellow brick C is on top of the brick E .The yellow brick D is on top of the brick A .The yellow brick E is on top of the brick D .The white brick A is on top of the brick B .For the brick B, the color is white.Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick D?</p>
<p>Shared Model Input</p>
<p>The bricks from bottom to top is B, A, D, E, C 1. Remove brick A from the top of brick B. 2. Remove brick E from the top of brick D. 3. Now brick D is the topmost yellow brick and can be grabbed.</p>
<p>Chain-of-Thought Prompting</p>
<p>Model Output</p>
<p>So we get the result as C, E, D.</p>
<p>Model Output</p>
<p>Figure 1: An example for comparison between Chain-of-Thought (CoT) and Chain-of-Symbol (COS) that elicits large language models in tackling complex planning tasks with higher performance and fewer input tokens.We let the model generate CoT/COS during inference in a few-shot manner.Results were taken in May 2023 with ChatGPT and can be subject to change.</p>
<p>complex spatial relationships.Aligning symbols and representing spatial relationships by symbols in word sequences can be a neater representation and thus can be potentially easier to understand by LLMs.We thus explore the use of symbols for LLM prompting, which is still an understudied topic.This is important to study which implies understanding abilities beyond language models for language understanding per se.</p>
<p>To explore the role of symbolic representations in prompting, we take the complex spatial understanding and planning as the evaluation scenarios, which require LLMs to understand the virtual spatial environments described through natural language as well as planning and achieving certain goals in such environments.Inspired by existing classic planning competitions and spatial reasoning datasets, we present three domains: (i) Brick World (ii) NLVR-based Manipulation and (iii) Natural Language Navigation. Figure 1 illustrates an example for Brick World 1D, and all these three tasks are described in detail in Section 2.1.These three tasks are all described in natural language.And we also evaluate one existing spatial question answering dataset SPARTUN (Mirzaee &amp; Kordjamshidi, 2022) which uses human-generated questions thus closer to realistic situations.For these tasks, LLMs need to understand a virtual environment in natural language, with the spatial relationship between the objects to be operated on and the restrictions on the operation, which is easy for real humans.However, we found that there are still places for improvement in the performance of LLMs on the tasks.</p>
<p>As a major contribution to this study, we investigate the symbolic representations for spatial relationships, and propose a novel method called Chain-of-Symbol (COS) prompting to elicit spatial understanding and planning abilities on LLMs.As in Figure 1, instead of using intermediate thinking steps described in natural language in CoT prompts shown on the left-hand side, the CoS prompts remove the redundant text description but only using a set of symbols to represent spatial relationships between objects in complex environments.COS achieves noticeable improvement in both performance and efficiency (by up to 60.8% improvements in accuracy and 65.8% for the number of input tokens).We speculate that such an improvement is benefited by the more efficient symbolic representation produced by COS.Our main contributions are three-fold:</p>
<p>There is a set of bricks.The white brick F is on top of brick D .</p>
<p>Brick World 1D</p>
<p>Story: There are three boxes.In the left box, there are one large round in black, one small square in blue, one small square in blue, one small triangle in yellow, one middle square in yellow.In the middle box, there are one large square in yellow, one middle triangle in blue, one large round in black.In the right box, there are one large square in blue, one large triangle in black, one middle triangle in black.Question: How to move all black objects to the left box?Answer:</p>
<p>To move all black objects to the left box, we need to identify all the black objects in the three boxes.(Mirzaee &amp; Kordjamshidi, 2022).Chains of Symbols are highlighted.</p>
<p>representations.COS drastically improves LLMs on the spatial tasks.The accuracy gain of COS is large, also with a good reduction in the token consumption for LLMs.• We conduct an in-depth analysis on COS to explore the effect of using different symbols, on different LLMs, and different languages to show the robustness of our method.</p>
<p>2 Spatial Planning and Understanding tasks</p>
<p>Natural Language Spatial Planning</p>
<p>Inspired by classic planning domains and tasks described in Liu et al. (2023) and existing spatial reasoning dataset Suhr et al. (2017), we explore the performance of LLMs in three natural language spatial planning tasks.For all three tasks, we can formulate the problem as given a virtual scenario described by natural language, and a planning question.LLMs should take both the scenario and the question as the input and output correspondingly to solve the question.Such a solution usually contains a series of steps to achieve a final goal.</p>
<p>The final test tasks consist of 5,500 evaluation instances, with 4,000 from Brick World, 1,000 from NLVR-based Manipulation, and the remaining 500 from Natural Language Navigation.We use code to generate these instances based on definition of each task.</p>
<p>Brick World</p>
<p>Figure 2 demonstrates an instance for Brick World (top), which requires the LLMs to acquire certain bricks by grabbing the bricks sequentially.We explore 1D and 2D scenarios for the Brick Worlds task.Specifically, in the 1D scenario, the relationship between bricks is only vertical.In the 2D scenario, in addition to the vertical relationship, there is also a horizontal relationship, which we express as "in the front of".To explore the characteristics of language understanding from LLMs, we investigate different levels of difficulty in the way of describing virtual scenarios.We describe them in increasing levels of difficulty as below.</p>
<p>• Firstly, we explore labelling bricks from A to Z according to the order of spatial stacking from bottom to top, and the corresponding texts are also described in order from bottom to top, we call this setting "No shuffle".• Secondly, we shuffle the order of the corresponding natural language description while maintaining the labelling rules in alphabetic order called "Shuffle description".• Thirdly, we shuffled the order of labelling so that the spatial relationships do not correspond to the alphabetic order anymore, but are still described in the order from bottom to top in the text description, called "Shuffle label".• Finally, we shuffled both the order of labelling and description.We call it "Shuffle both".</p>
<p>We use colors to represent the bricks, which enriches the information and increases the difficulty of the tasks.For each setting with 1D and 2D, we create 500 evaluation instances.</p>
<p>The final evaluation set consists of 4,000 instances.</p>
<p>NLVR-based Manipulation</p>
<p>Figure 2 demonstrates an instance for NLVR-based Manipulation (middle).We convert the format of Natural Language Visual Reasoning (NLVR, Suhr et al. (2017)) tasks into a text-based planning task.Based on the creation rules of synthetic images of NLVR, we create 1,000 natural language descriptions for the virtual spatial environments using Python code.Specifically, for each description, we set three boxes just like NLVR, in the left, middle, and right, and in each box, and there are several objects.Each object has three properties: color, shape, and size.Each description has one related question, the question is about how to move all objects that satisfy a certain condition of one property (such as "all objects in black" or "all rounds") to a specific target box.The ground truth is the set of all objects satisfied with this condition which needs to be moved (not in the target boxes).</p>
<p>Natural Language Navigation</p>
<p>Figure 2 demonstrates an instance for Natural Language Navigation (bottom).Inspired by Vision-and-Language navigation (Gu et al., 2022), we create a virtual spatial environment that is similar to a 2D map of navigation tasks but using natural language description only.Specifically, we define a set of landmarks: ′ store ′ , ′ bank ′ , ′ house ′ , ′ cinema ′ , ′ garden ′ , ′ school ′ .For each description, there are 7 to 10 landmarks.We create 500 evaluation instances using Python code: the relationship between landmarks is a binary tree structure, with a root node which indicates the start point in the virtual scenario, and each node other than the leaf nodes has one or two child nodes, with a distance of 100 meters or 200 meters between them.Each description has one related question which is about how to reach the nearest one specific kind of landmark from the starting point.</p>
<p>Spatial QA</p>
<p>We also evaluate COS on manually annotated existing spatial question answering task, SPARTUN (Mirzaee &amp; Kordjamshidi, 2022), which contains a larger variety of spatial relation types and spatial expressions compared with previous Spatial QA datasets and our three synthetic spatial planning tasks.And the questions in this dataset are manually annotated, which is closer to real-world scenes.The scenarios in this dataset are described in natural languages based on NLVR (Suhr et al., 2017) and SPARTQA (Mirzaee et al., 2021).</p>
<p>Chain-of-Symbol Prompting</p>
<p>We propose Chain-of-Symbol (COS) prompting for LLMs, which converts the simulated environment with natural language into a condensed symbolic representation that considers spatial relationship.In order to make our constructing method of COS generalizable and reliable, we adopt a three-step procedure in creating the demonstrations of our COS which can be used in any related tasks:</p>
<p>• (i) Automatically prompt the LLMs to generate a CoT demonstration in a zero-shot manner • (ii) Correct the generated CoT demonstration if there existing errors.• (iii) Replace the spatial relationships described in natural languages in CoT with random symbols, and only keep objects and symbols, remove other descriptions.</p>
<p>We then use the COS demonstrations to guide the language model in a few-shot manner for prompting LLMs just like CoT (Wei et al., 2022).Figure 1 depicts an example of a demonstration of CoS produced by models.In this example, we see that both CoT and COS receive the same shared simulated spatial environment in natural language texts.COS depicts a different intermediate thinking process than CoT.The latter represents the environments in a natural language only, while the former use a condensed symbolic representation that considers spatial relationship.Specifically, we use the symbol "/" to represent the spatial relationship "from the top of" here.By doing such a conversion, and removing redundant descriptions, COS effectively improves the model performance as well as reduces the inference costs with LLMs.</p>
<p>Figure 2 depicts examples of CoS demonstration for all three planning tasks we proposed.</p>
<p>For NLVR-based Manipulation, we convert natural language descriptions for objects to the format of a triplet such as "(large, round, black)".For Natural Language Navigation, we represent the order of landmarks by using symbol "/" to connect them.For Spatial QA task, we use a set of symbols such as "=", "∼" to represent different spatial relationships, and use triplet with "( , , )" to represent objects and their attributes.</p>
<p>CoS prompting has multiple properties that are attractive as a prompting approach for LLMs:</p>
<p>• First, COS effectively allows a neater, shorter, and condensed intermediate procedure than CoT.It is more structured than natural languages, hence easier for human annotators to analyze, check and correct the intermediate thinking process for LLMs.• Second, COS improves important planning tasks that current LLMs do not tackle well.It provides a better representing method for spatial environments which is easier for LLMs to learn compared with natural language.• Finally, COS clearly reduces the amount of text input into the LLMs and output from LLMs.This makes it much cheaper to access LLMs with API/GPU.</p>
<p>Experiments</p>
<p>In this section, we first introduce our experimental setup in Section 4.1 about the settings of different methods we use, the language models, and the evaluation metrics.Then, in Section 4.2, we report the results of the three spatial planning tasks we proposed.In Section 4.3, we report the results on the SPARTUN dataset.</p>
<p>Experimental Setup</p>
<p>We evaluate CoS and CoT on our proposed three spatial planning tasks and the existing SQA dataset, based on representative LLMs like ChatGPT(gpt-3.5-turbo) and LLAMA-2 series.There are three prompts: zero-shot CoT, few-shot CoT, and few-shot CoS (Ours).</p>
<p>Zero-shot Chain-of-Thought Prompting</p>
<p>We consider zero-shot CoT as our baseline.The reason is that we have found that our choices of LLMs naturally give their intermediate steps (CoT) in their answers, even without specifically asking them to do so.We also found that asking them to remove the thinking steps obviously degrades the results.Therefore, we allow the LLMs to generate CoT, while we do not put any demonstration to the prompt but give prompts like "Let's think step by step" just as Kojima et al. (2022).For an easier evaluation, we ask the LLMs to output the final results by separating the landmarks with commas.(Wei et al., 2022), we manually crafted five demonstrations for each task to guarantee their correctness.To guarantee the consistency and reliability of the prompts, we follow the format of CoT generated by zeroshot-CoT prompting.We use these fixed five demonstrations for evaluations on each task.</p>
<p>Chain-of-Symbol Prompting</p>
<p>As described in Section 3, COS augments the standard CoT prompting with condensed symbolic representation.While CoT has been shown to give large improvements to LLMs on various tasks (Wei et al., 2022), we argue that using condensed symbolic representations can be an alternative to describing using natural language texts.We manually converted from CoT demonstrations to CoS using the procedure described in Section 3. Five CoS demonstrations of the same examples with CoT are created for each task of Natural Language Planning.</p>
<p>Language Models</p>
<p>We use both Opensource LLMs (Llama-2) and Closed Source LLMs ChatGPT(Gpt-3.5-turbo) for the evaluation of all tasks.We set the temperature to 0 for all the experiments throughout this paper.</p>
<p>Evaluation Metrics</p>
<p>For planning tasks, we use three evaluation metrics, namely accuracy, precision, and recall.We define accuracy as the success rate in achieving the final goal.</p>
<p>We then compute the Longest Common Sequence (LCS) between the ground truth and LLM output sequence to measure their similarity.We compute precision as the ratio of LCS against the length of the LLM output, and we compute recall as the ratio of LCS against the length of the ground truth.For spatial QA task, we only compute accuracy.</p>
<p>Results of</p>
<p>CoS on the Different Language</p>
<p>In addition to the tasks described in English, we also tested COS on Brick World in Chinese.CoT reports 22.9% accuracy, and COS reports 39.1% in 1D scenario of Brick World, which demonstrates the robustness of COS to a language other than English.Robustness to Different Symbols Figure 3 demonstrates the robustness of using different symbols for COS.As we can see, using different symbols brings consistent performance, while not having any symbol drastically impacts the results.Among all the symbols, the comma gives the best results.We conclude that COS is robust to the selection of symbol.Results on Different Language Models Figure 4 reports the results on LLAMA-2 under the 1D scenario.The experimental results align with our previous conclusion that COS outperforms CoT obviously.Saving Tokens for Prompting One advantage featured by COS is that it reduces the number of input tokens to be fed into LLMs.For Brick World (1D scenario), COS reduces the number of tokens for the intermediate steps from 407 to 139 (Table 1, the numbers are reported from OpenAI Playground2 ).This subsequently saves the costs of accessing LLMs via API/GPUs.</p>
<p>NLVR-based Manipulation and Natural Language Navigation</p>
<p>For both of these two tasks, we adopt almost the same experimental settings as the ones for Brick World.The only difference is the evaluation metrics we report, we compute precision and recall based on the set rather than the Longest Common Sequence.</p>
<p>Main Results</p>
<p>Table 2 reports the results of NLVR-based Manipulation and Natural Language Navigation with GPT-3.5-turbo.For both of these two tasks, COS reports a higher performance than COT and zero-shot CoT prompting on all of the metrics.</p>
<p>Saving Tokens for Prompting One advantage of COS is that it can reduces the number of input tokens.Table 2 reported that for NLVR-based Manipulation, COS reduces the number of tokens for the intermediate steps from 653 to 534, nearly by half of the original intermediate steps (we separate the tokens by space).This subsequently saves the costs of accessing LLMs via API/GPU, which enables easier access to the models.</p>
<p>Table 2: The automatic evaluation results with gpt-3.5-turbo on Natural Language Navigation and NLVR-based Manipulation.We set N s = 5, where N s represents the number of demonstrations for prompting with COS and CoT.The best results are bolded.We report the average and the standard deviation from three runs with different demonstrations.Acc.represents accuracy, Pre.represents precision, and Rec.represents recall (precision and recall are computed with sets in this case).We also explore the effectiveness of COS in a more real-world scenario, by using existing human annotated spatial QA dataset SPARTUN (Mirzaee &amp; Kordjamshidi, 2022).Specifically, we applied both COS and CoT on GPT-3.5-Turbo and GPT-4.COS gains better performance and uses fewer tokens compared with CoT.In table 3, we report the results of performance, and both CoT and CoS have 5 shots.It should be noticed that there are far more types of spatial relationships in SPARTUN dataset than our proposed planning tasks, so the results indicate CoS can gain promising performance even when there are a lot of symbols to represent different spatial relationships.</p>
<p>Results of different size LLAMA-2</p>
<p>We also evaluate CoS on current open-source representative models like LLAMA-2 Touvron et al. (2023) series to further validate the effectiveness and generality of our method.We use LLAMA-2 with different size (7B, 13B and 13B).As shown in Figure 4, when the model size is small (7B and 13B), CoS cannot outperform CoT in many cases, but in 70B, CoS gain a clear better performance in all three tasks compared with using CoT.It can be seen that as the model size increases, the ratio of model performance to parameters for CoS has a larger slope compared to CoT.This indicate that large language models might can have emergent ability of abstract symbols understanding.</p>
<p>Related Work</p>
<p>In-Context Learning Large language models (LLMs) have demonstrated remarkable fewshot learning abilities across various domains (Brown et al., 2020;Srivastava et al., 2022), which is also called as in-context learning, leading to a paradigm shift in AI to use LLMs as foundational models for language-related tasks, either directly or through fine-tuning (Bommasani et al., 2021;Hu et al., 2022).While less relevant to COS, a concurrent work converts natural language into executable actions for robots with ChatGPT (Wake et al., 2023).Another very recent concurrent work uses Symbol Tuning that replaces natural language labels with arbitrary symbols to improve in-context learning (Wei et al., 2023).</p>
<p>Chain-of-Thought Reasoning</p>
<p>The ability of LLMs (Brown et al., 2020;Srivastava et al., 2022) to perform complex reasoning tasks can be significantly enhanced by using a show known as Chain-of-Thought (CoT) prompting, which involves providing them with intermediate reasoning steps (Nye et al., 2021;Wei et al., 2022).Such a phenomenon also generalizes to the multilingual settings (Shi et al., 2023).Despite the fact that CoT is powerful, there are reports that demonstrate that CoT is not always useful and that integrating CoT degrades the performance on the task of Machine Translation in their experiment.And this is possibly due to the word-by-word translation (Peng et al., 2023).</p>
<p>Spatial Reasoning Spatial reasoning over natural language texts has been an important research direction in the community (Janner et al., 2018;Mirzaee et al., 2021).Janner et al. (2018) proposes to leverage representation learning on a navigation task that requires the agent to move a specific location.Rojowiec et al. (2020) proposes a new task on spatial reasoning that requires the language model to generate natural language instructions for 'before' and 'after' image pairs.Mirzaee et al. (2021) proposes a new benchmark for spatial question-answering with 'which' and 'what' questions regarding the environment.In a concurrent work, Tsai et al. (2023) demonstrates that LLMs perform poorly on text-based games with question-answering tasks that require several steps of reasoning.</p>
<p>Navigation and Path Planning Language grounding navigation (Gu et al., 2022) refers to the interdisciplinary task that requires the intelligent agent to perceive the visual environment and guide the user to the goal location through natural language instructions (Nguyen et al., 2019;Chen et al., 2019).Path planning (Panov et al., 2018;Krishna Lakshmanan et al., 2020) refers to the tasks that require the agent to plan its own path to achieve certain goals such as the shortest path or maximizing the cleaning area, typically through the use of reinforcement learning.These areas are highly relevant to the spatial planning tasks we explored and COS, as the spatial environments can be potentially represented by symbolic representations.We leave the investigations of these application areas to future studies.(Sun et al., 2024)</p>
<p>Conclusion</p>
<p>We found that current popular LLMs still lack abilities in complex spatial planning and understanding tasks.To this end, we propose a novel method called COS (Chain-of-Symbol Prompting) that converts spatial relationships described in natural languages to condensed symbolic representations in the chained intermediate thinking steps.COS is easy to use and does not need additional training on LLMs.Extensive experiments indicate that using few-shot COS demonstration clearly surpasses the performance of using CoT described in natural languages on all three spatial planning tasks we proposed and the representative spatial QA benchmark with even fewer tokens (down to about 1/3 tokens of the thinking steps with CoT) used in the inputs compared with CoT prompting.The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World for ChatGPT.</p>
<p>Limitations Refer to the Appendix for the section on Broader Impact.In addition, we only use two models to verify the effectiveness of our method due to the limited time and resources.It would be interesting to apply our method to more models with different sizes to see whether there is an emergent ability of CoS for LLMs.Nevertheless, our choices of foundation models are representative and they are popular LLMs.</p>
<p>Answer:</p>
<p>We can get A//B//C//D.</p>
<p>So we get the result as D, C, B.</p>
<p>Question: There is a set of bricks.For the brick A, the color is blue.The yellow brick B is in front of the brick A. The yellow brick C is in front of the brick B. The white brick D is on top of the brick C .The white brick E is on top of the brick D .The yellow brick F is on top of the brick E .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick C?</p>
<p>Answer:</p>
<p>We can get C-&gt;B-&gt;A, C//D//E//F.</p>
<p>So we get the result as F, E, D, C.</p>
<p>Question: There is a set of bricks.The brick P is on top of the brick R .The brick J is on top of the brick B .The brick D is on top of the brick P .The brick R is on top of the brick H .The brick K is in front of the brick M. The brick B is on top of the brick D .For the brick M, the color is blue.The brick C is on top of the brick J .The brick H is in front of the brick K. Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick M?</p>
<p>Answer:</p>
<p>We can get H//R//P//D//B//J//C, M-&gt; K -&gt; H.</p>
<p>So we get the result as M directly.</p>
<p>Question: There is a set of bricks.The brick K is on top of the brick F .The brick M is in front of the brick F. The brick N is on top of the brick K .</p>
<p>For the brick O, the color is blue.The brick G is on top of the brick A .The brick F is in front of the brick I.The brick I is in front of the brick O.The brick A is on top of the brick N .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick O?</p>
<p>Answer:</p>
<p>We can get F//K//N//A//G, F-&gt;I-&gt;O, So we get the result as O directly.</p>
<p>Table 12: Few-shot exemplars for full COT prompt for SPARTQA.</p>
<p>Example 1:</p>
<p>Background: There are three blocks called A, B, and C. In block A, there is a small blue square far above a small blue circle.There is a small black triangle far to the right of both small blue objects.The black triange is near and above a medium blue square.The medium blue square is touching the right edge of A and is near and above a medium black circle.Near and to the right of the medium black circle is a medium yellow triangle.The yellow triangle is to the left of the small blue circle.In B, which is to the right of A, there is a large blue square near and to the left of a large yellow triangle.There is also a small yellow triangle far below the square.In C, which is to the right of B. there is a small yellow circle near and above a small yellow triangle.The small yellow triangle is touching the top of a medium black triangle.The black triangle is touching the right edge of C.There is a large blue circle far below the black triangle.</p>
<p>Question: Are all yellow objects in C above a blue thing?</p>
<p>Candidate Answers:["Yes", "No", "Do not Know"]</p>
<p>Answer:</p>
<p>Let's analyze the information provided step by step: In block C: 1.There a small yellow circle near and above a small yellow triangle.2. The small yellow triangle is touching the top of a medium black triangle.3.There is a large blue circle far below the black triangle.Based on the provided information, the small yellow circle and the small yellow triangle in Block C are indeed above a blue thing, which is the large blue circle.So, the correct answer is: "Yes".</p>
<p>Example 2:</p>
<p>Background: There are three blocks called A, B, and C. A is to the left of B and B is to the left of C. In A, there is a small black triangle near and ot the left of a large blue circle.The large blue circle is near and to the left of a small blue circle.The small blue square is far below a medium yellow square.In B, there is a small yellow circle.In C, there is a medium black triangle that is near and to the left of a small yellow circle.The small yellow circle is near and to the left of a blue triangle.There is a large yellow square touching the bottom of the small yellow circle.Near and below the large yellow square is a large black square.</p>
<p>Question: What is the relation between the large black thing and the small yellow thing?</p>
<p>Candidate Answers:["left", "right", "above", "below", "near to", "far from", "touching", "Do Not Know"]</p>
<p>Answer:</p>
<p>Let's analyze the information provided step by step: In block C: 1.There is a medium black triangle that is near and to the left of a small yellow circle.2. The small yellow circle is near and to the left of a blue triangle.3.There is a large yellow square touching the bottom of the small yellow circle.4. Near and below the large yellow square is a large black square.Now, let's determine the relation between the "large black thing" (large black square) and the "small yellow thing" (small yellow circle): 1.The large black square is in block C, and it is described as being "near and below" the large yellow square.2. The small yellow circle is also in block C, and it is described as being "near and to the left of" the medium black triangle.Based on the provided information, the large black square is below the large yellow square and the small yellow circle is near to the medium black triangle.So, the correct answer: "below" and "near to".</p>
<p>Example 3:</p>
<p>Background: There are three blocks called A, B, and C. A is to the right of B and B is to the right of C. In A, there is a medium black circe that is far to the right of a medium blue square.The medium blue square is near and to the right of a small yellow circle.Far above the blue square is a medium yellow triangle.Near above the medium triangle is a small blue circle.In B, there is a large blue square that is touching the right edge of B. There is a medium black triangle that is touching the left side of the blue square.Near and above the medium black triangle is a large blue circle.Near and above the large blue circle is a large yellow triangle.In C, there is a small yellow triangle near and to the right of a medium black circle.The yellow triangle is touching the right edge of C. Far to the left of the medium black circle is a small blue square.</p>
<p>Question: What block does not have any objects touching the edge of it?</p>
<p>Candidate Answers: ["A", "B", "C"]</p>
<p>Answer:</p>
<p>Let's analyze the information provided step by step: In block A: 1.There is a medium black circle, which is far to the right of a medium blue square.2. The medium blue square is near and to the right of a small yellow circle.3. Far above the blue square is a medium yellow triangle.4. Near above the medium triangle is a small blue circle.In block B: 1.There is a large blue square that is touching the right edge of B. 2. There is a medium black triangle that is touching the left side of the blue square.3. Near and above the medium black triangle is a large blue circle.4. Near and above the large blue circle is a large yellow triangle.In block C: 1.There is a small yellow triangle near and to the right of a medium black circle.2. The yellow triangle is touching the right edge of C. 3. Far to the left of the medium black circle is a small blue square.Based on the provided information, it's Block A, which does not have any objects touching its edge.So, the correct answer is: "A".</p>
<p>Example 4:</p>
<p>Background: There are three blocks called A, B, and C. A is to the left of B and B is to the left of C. In A, there is a large black triangle far above a medium black triangle.The medium black triangle is near and above a large yellow circle.Near and to the right of the large circle is a small blue circle.In B, there is a medium blue circle near and to the left of a small black square.Far below the blue circle is a small yellow circle.In C, there is a small blue square.</p>
<p>Question: What object is near the large yellow thing, the medium black triangle or the small blue circle the medium black triangle or the smal blue circle?</p>
<p>Candidate Answers:["the medium black triangle", "the smal blue circle", "both of them", "none of them"]</p>
<p>Answer:</p>
<p>Let's analyze the information provided step by step: In lock A: 1.There is a large black triangle far above a medium black triangle.2. The medium black triangle is near and above a large yellow circle.3. Near and to the right of the large circle is a small blue circle.In block B: 1.There is a medium blue circle near and to the left of a small black square.2. Far below the blue circle is a small yellow circle.In block C: 1.There is a small blue square.Now, let's analyze the position of the large yellow circle: 1.The large yellow circle is in Block A and is near and above the medium black triangle.2. The small blue circle is also in Block A and is near and to the right of the large yellow circle.Based on the provided information, both the medium black triangle and the small blue circle are near the large yellow circle in Block A. So, the correct answer is: "both of them".</p>
<p>Example 5:</p>
<p>Background: There are three blue shapes in a block that we called A. Two medium blue squares are below a big blue square.First medium blue square is to the left of the other medium one which is touching the bottom edge of block A. There is another block below the block A we call it B. A big blue triangle is touching the top edge of this block which is above a small black triangle, a big blue circle and a medium blue triangle.The medium triangle is touching the bottom edge of the block.The circle is below and to the left of the small triangle.It is above the medium triangle.</p>
<p>Question: Is the small triangle below the medium triangle?</p>
<p>Candidate Answers:["Yes", "No", "Do not Know"]</p>
<p>Answer:</p>
<p>Let's analyze the information provided step by step: In block B: 1.A big blue triangle is touching the top edge of this block which is above a small black triangle, a big blue circle and a medium blue triangle.2. The medium triangle is touching the bottom edge of the block.3. The circle is below and to the left of the small triangle.It is above the medium triangle.Based on the provided information, the small black triangle is Table 13: Few-shot exemplars for full COS prompt for SPARTQA.</p>
<p>Example 1:</p>
<p>Background: There are three blocks called A, B, and C. In block A, there is a small blue square far above a small blue circle.There is a small black triangle far to the right of both small blue objects.The black triangle is near and above a medium blue square.The medium blue square is touching the right edge of A and is near and above a medium black circle.Near and to the right of the medium black circle is a medium yellow triangle.The yellow triangle is to the left of the small blue circle.In B, which is to the right of A, there is a large blue square near and to the left of a large yellow triangle.There is also a small yellow triangle far below the square.In C, which is to the right of B. There is a small yellow circle near and above a small yellow triangle.The small yellow triangle is touching the top of a medium black triangle.The black triangle is touching the right edge of C.There is a large blue circle far below the black triangle.</p>
<p>Question: Are all yellow objects in C above a blue thing?</p>
<p>Candidate Answers:["Yes", "NO", "Do not Know"]</p>
<p>Answer:</p>
<p>Let's analyze the information provided step by step to determine if all yellow objects in Block C are above a blue thing: C: 1. (small, yellow, circle) ↑ (small, yellow, triangle) 2. (small, yellow, triangle) = top of (medium, black, triangle) 3. (large, blue, circle) ∞↓ (medium, black, triangle).Based on the provided information, C: (small, yellow, circle) ↑ (large, blue, circle) and (small, yellow, triangle) ↑ (large, blue, circle).So, the correct answer is: "Yes".</p>
<p>Example 2:</p>
<p>Background: There are three blocks called A, B, and C. A is to the left of B and B is to the left of C. In A, there is a small black triangle near and to the left of a large blue circle.The large blue circle is near and to the left of a small blue circle.The small blue square is far below a medium yellow square.In B, there is a small yellow circle.In C, there is a medium black triangle that is near and to the left of a small yellow circle.The small yellow circle is near and to the left of a blue triangle.There is a large yellow square touching the bottom of the small yellow circle.Near and below the large yellow square is a large black square.</p>
<p>Question: What is the relation between the large black thing and the small yellow thing?</p>
<p>Candidate Answers:["left", "right", "above", "below", "near to", "far from", "touching", "Do Not Know"] Answer:</p>
<p>Let's break down the relationships between the objects step by step: A: 1. (small, black, triangle) &lt; (large, blue, circle) 2. (large, blue, circle) &lt; (small, blue, circle) 3. (small, blue, square) ∞↓ (medium, yellow, square) B: 1. (small, yellow, circle) C: 1. (medium, black, triangle) &lt; (small, yellow, circle) 2. (small, yellow, circle) &lt; (blue, triangle) 3. (large, yellow, square) =↓ (small, yellow, circle) 4. (large, black, square) ↓ (large, yellow, square) Now, let's determine the relation between the (large, black, square) and the (small, yellow, circle): 1. C: (large, black, square) ↓ (large, yellow, square).Therefore, (large, black, square) ↓ (large, yellow, square) 2. C: (small, yellow, circle) &lt; (medium, black, triangle).Therefore, (small, yellow, circle) (medium, black, triangle) So, the correct answer: "below" and "near to".</p>
<p>Example 3:</p>
<p>Background: There are three blocks called A, B, and C. A is to the right of B and B is to the right of C. In A, there is a medium black circle that is far to the right of a medium blue square.The medium blue square is near and to the right of a small yellow circle.Far above the blue square is a medium yellow triangle.Near the medium triangle is a small blue circle.In B, there is a large blue square that is touching the right edge of B.</p>
<p>There is a medium black triangle that is touching the left side of the blue square.Near and above the medium black triangle is a large blue circle.Near and above the large blue circle is a large yellow triangle.In C, there is a small yellow triangle near and to the right of a medium black circle.The yellow triangle is touching the right edge of C. Far to the left of the medium black circle is a small blue square.</p>
<p>Question: What block does not have any objects touching the edge of it?</p>
<p>Candidate Answers:["A", "B", "C"]</p>
<p>Answer:</p>
<p>Let's analyze each block step by step to determine which one does not have any objects touching its edge: A: 1. (medium, black, circle) ∞&gt; (medium, blue, square).2. (medium, blue, square) &gt; (small, yellow, circle  (medium, black, circle).Based on the provided information" let's determine which block does not have any objects touching its edge.Block B. So, the correct answer is: "A".</p>
<p>Example 4:</p>
<p>Background: There are three blocks called A, B, and C. A is to the left of B and B is to the left of C. In A, there is a large black triangle far above a medium black triangle.The medium black triangle is near and above a large yellow circle.Near and to the right of the large circle is a small blue circle.In B, there is a medium blue circle near and to the left of a small black square.Far below the blue circle is a small yellow circle.In C, there is a small blue square.</p>
<p>Question: What object is near the large yellow thing, the medium black triangle the small blue circle the medium black triangle or the small blue circle?</p>
<p>Candidate Answers:["the medium black triangle", "the small blue circle", "both of them", "none of them"] Answer:</p>
<p>Let's break down the information step by step to determine which object is near the large yellow circle, the medium black triangle, or the small blue circle: A: 1. (large, black, triangle) ∞↑ (medium, black, triangle).2. (medium, black, triangle) ↑ (large, yellow, circle).3. (small, blue, circle) &gt; (large, circle).B: 1. (medium, blue, circle) &lt; (small, black, square).2. (small, yellow, circle) ∞↓ (blue, circle).C: 1. there is a small blue square.Now, let's analyze the position of the (large, yellow, circle): 1. A: (large, yellow, circle) ↑ (medium, black, triangle).2. A: (small, blue, circle) &gt; (large, yellow, circle).Based on the provided information, A: (medium, black, triangle) (large, yellow, circle) and (small, blue, circle) (large, yellow, circle).So, the correct answer is: "both of them".</p>
<p>Example 5:</p>
<p>Background: There are three blue shapes in a block that we call A. Two medium blue squares are below a big blue square.The first medium blue square is to the left of the other medium one which is touching the bottom edge of block A. There is another block below block A we call it B. A big blue triangle is touching the top edge of this block which is above a small black triangle, a big blue circle and a medium blue triangle.The medium triangle is touching the bottom edge of the block.The circle is below and to the left of the small triangle.It is above the medium triangle.</p>
<p>Question: Is the small triangle below the medium triangle?</p>
<p>Candidate Answers:["Yes", "NO", "Do not Know"]</p>
<p>Answer:</p>
<p>Let's analyze the arrangement of objects in block B to determine if the small triangle is below the medium triangle: B: 1. (big, blue, triangle) = top edge of B 2. (big, blue, triangle) ↑ (small, black, triangle), (big, blue, circle) and (medium, blue, triangle) 2. (big, blue, circle) ↓&lt; (small, black, triangle) 3. (big, blue, circle) ↓ (big, blue, triangle) 4. (medium, blue, triangle) = bottom edge B Based on the provided information, the small black triangle is above the medium blue triangle.So, the correct answer is: "No."</p>
<p>due to 1 arXiv:2305.10276v7[cs.CL] 5 Aug 2024</p>
<p>So we get the result as A, E, D.</p>
<p>Figure 4 :
4
Figure 4: Scaling curve of CoS and CoT of Llama-2 on three tasks.</p>
<p>The yellow brick B is on top of brick A. The blue brick D is on top of the brick C. The white brick E is on top of brick G .For brick A, the color is blue.The blue brick C is on top of the brick E. The white brick G is on top of the brick B. Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get the brick G?
A/B/G/E/C/D/FD/FC/DE/CG/E. So we get the result as F, D, C, E, G.</p>
<p>Then we can move the objects not in the left box above to the left box:There is a set of roads and a set of landmarks.The start point is bank A. There is a road which is 200 meters long from bank A to bank C.There is a road which is 100 meters long from bank C to house H.There is a road which is 100 meters long from house H to cinema F. There is a road which is 200 meters long from cinema F to store B.There is a road which is 100 meters long from store B to store G.There is a road which is 200 meters long from bank C to house D. There is a road which is 200 meters long from house D to garden J.There is a road which is 100 meters long from bank A to cinema I.There is a road which is 100 meters long from cinema I to house E. From the start point, how to reach the nearest store?
Background:There are three blue shapes in a block that we callA. Two medium blue squares are below a big bluesquare. The first medium blue square is to the leftof the other medium one which is touching thebottom edge of block A. There is another blockNLVR-based Manipulationbelow block A we call it B. A big blue triangle is touching the top edge of this block which is abovea small black triangle, a big blue circle and amedium blue triangle. The medium triangle istouching the bottom edge of the block. The circleis below and to the left of the small triangle. It isabove the medium triangle.Question:Is the small triangle below the medium triangle?They are: -left box: (large, round, black)Candidate Answers: ["Yes", "NO", "Do not Know"]-middle box: (large, round, black), -right box: (middle, triangle, black), (large, triangle, black)Answer: Let's analyze the arrangement of objects in block B1. (large, round, black): middle -left 2. (middle, triangle, black): right -left 3. (large, triangle, black): right -left Natural Language Navigationto determine if the small triangle is below the medium triangle: B: 1. (big, blue, triangle) = top B 2. (big, blue, triangle) ↑ (small, black, triangle),(big, blue, circle) &amp; (medium, blue, triangle)So the answer is ACHFB
-bank A / bank C / house H / cinema F / store B (200 + 100 + 100 + 200 = 600) -bank A / bank C / house H / cinema F / store B / store G (200 + 100 + 100 + 200 + 100 = 700) -So we get the result as bank A / bank C / house H / cinema F / store B. 2. (big, blue, circle) ↓&lt; (small, black, triangle) 3. (big, blue, circle) ~↓ (big, blue, triangle) 4. (medium, blue, triangle) = bottom B Based on the provided information, the small black triangle is above the medium blue triangle.}So, the correct answer is: "No."Spatial QA Figure 2: <input, Chain of Symbol, output> example triples for our three proposed tasks: Brick World, NLVR-based Manipulation, and Natural Language Navigation, and SPARTUN dataset</p>
<p>Table 1 :
1
The results of ChatGPT(gpt-3.5-turbo) on Brick World.We report the results with four settings as described in Section 2.2, under both 1D and 2D scenarios.We adopt N s = 5, where N s represents the number of demonstrations for COS and CoT.The best results are bolded.For COS and CoT, we report the average and the standard deviation from three runs with different sets of demonstrations.Acc.represents accuracy, Pre.represents precision, and Rec.represents recall.zs-CoT represents zero-shot CoT.We report the average number of tokens in the intermediate steps.
ModelNo ShuffleShuffle DescriptionShuffle LabelShuffle BothAcc.Pre.Rec.Acc.Pre.Rec.Acc.Pre.Rec.Acc.Pre.Rec. Tok.1D Scenariozs-CoT61.077.271.960.477.577.531.863.459.828.258.655.3-CoT81.0±11.0 87.7±4.5 90.1±2.6 71.5±9.2 90.7± 3.6 81.8±7.1 75.1±10.1 88.0± 3.6 90.1±0.9 43.0±4.4 71.4±3.3 75.7±1.6 407COS96.6±1.9 98.0±0.8 97.7±0.8 95.9±1.2 97.9± 0.6 97.5±0.3 92.6±2.0 97.0± 1.3 95.9±1.1 69.7±5.1 86.7±4.2 83.6±1.6 1392D Scenariozs-CoT32.753.860.614.831.946.913.032.042.39.830.438.4-CoT25.0±15.6 49.8±9.8 45.0±10.5 21.5±8.2 45.6±5.4 41.2±6.3 21.8±2.3 44.7±5.9 43.2±4.0 14.9±3.4 38.1±2.9 36.4±3.5 546COS60.7±1.9 67.2±1.1 71.3±1.3 33.7±3.2 46.7±0.8 50.0±1.5 23.5±5.0 45.9±0.8 63.0±12.1 28.9±2.3 46.3±1.0 44.4±2.8 341
Chain-of-Thought PromptingThis baseline uses a few-shot CoT, in which we encourage LLMs to think step by step, and we use five demonstrations to guide the LLMs in the thinking procedure.Note that the intermediate thinking procedure is represented as natural language text, just like the Standard Prompting.Like in</p>
<p>8%accuracy on the most difficult setting Shuffle Both under the 2D scenario.Although CoT brings some improvements, the performance for CoT is still not satisfying, with an accuracy of 43.0% which is just below the 50% bar for setting Shuffle Both under the 1D scenario.In contrast, we see that COS gives very good improvements on this setting (from 28.2% to 69.7%).We found that COS gives consistent improvements to all the settings on Brick World, clearly surpassing CoT.The largest gain is on the setting of Shuffle Label under the 1D scenario, with 60.8% improvements in accuracy (from 31.8% to 92.6%).To investigate the randomness in our experiments, we run multiple trials with three different sets of demonstrations for CoT and COS.Table1reports their means and standard deviations.We see a general trend here that COS usually reports a lower standard deviation than CoT (for example, a standard deviation of 1.9 for Acc. for No Shuffle under the 1D scenario for COS, against 11.0 for CoT).This represents that COS is more stable than CoT on Brick World.
4.2.2 Further Analysis of Brick WorldRandomness in the Experiments GPT-3.5-Turbo 0 20 40 60 80 100 None / -Brick World 1D (shuffle both) success rate (%) // ,Figure 3: Performance of us-ing different symbols for COSon Brick World 1D (ShuffleBoth) in accuracy.
Spatial Planning Tasks 4.2.1 Brick World Table 1 reports the results of COS against the zs-CoT and CoT on the task of Brick World.First of all, we can see that the complexity increases both from the 1D scenario to the 2D scenario and from the setting of No Shuffle to the setting of Shuffle Both, together with a drop in the performance.ChatGPT with zs-CoT does not perform well, with only 9.</p>
<p>Table 3 :
3
The automatic evaluation results with GPT-3.5-Turbo and GPT-4 on SPARTUN dataset.We apply CoT with 5 shots, and CoS with 5 shots (Ours) respectively.We report the number of tokens in the intermediate steps from demonstrations as the last column.
Model GPT-3.5-Turbo GPT-4 TokensCoT-547.169.8198CoS-549.472.61674.3 Spatial Question Answering</p>
<p>Table 4 :
4
Few-shot exemplars for full Chain-of-Symbol prompt for brick 1D.There is a set of bricks.For brick B, the color is yellow.The yellow brick C is on top of the brick B .The yellow brick A is on top of the brick C .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick B There is a set of bricks.The yellow brick A is on top of the brick C .The yellow brick B is on top of the brick A .For the brick C, the color is white.Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick A? There is a set of bricks.The white brick F is on top of the brick D .The yellow brick B is on top of the brick A .The blue brick D is on top of the brick C .The white brick E is on top of the brick G .For the brick A, the color is blue.The blue brick C is on top of the brick E .The white brick G is on top of the brick B .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick G?
Answer:A//CC//BIn sum: A//C//BSo we get the result as A, C, B.Answer:B//AA//CIn sum: B//A//CSo we get the result as B, A.Answer:A//EE//DD//BB//CIn sum: A//E//D//B//CSo we get the result as A, E, D.Answer:E//AA//DD//FF//CC//BIn sum: E//A//D//F//C//BSo we get the result as E, A, D, F.Answer:A//B//G//E//C//D//FD//FC//DE//CG//ESo we get the result as F, D, C, E, G.
Question: Question:Question: There is a set of bricks.The blue brick A is on top of the brick E .For the brick C, the color is blue.The blue brick D is on top of the brick B .The white brick E is on top of the brick D .The blue brick B is on top of the brick C .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick D? Question: There is a set of bricks.The white brick F is on top of the brick C .The white brick C is on top of the brick B .The yellow brick E is on top of the brick A .For the brick B, the color is white.The white brick D is on top of the brick F .The blue brick A is on top of the brick D .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick F? Question:</p>
<p>Table 5 :
5
Few-shot exemplars for full COT prompt for brick 1D.There is a set of bricks.The yellow brick B is on top of the brick D .For the brick D, the color is white.The yellow brick A is on top of the brick C .The yellow brick E is on top of the brick A .The blue brick C is on top of the brick B .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick D?Answer: The bricks from bottom to top is B, D, C, A, E 1. Remove brick E from the top of brick A. 2. Remove brick A from the top of brick C. 3. Remove brick C from the top of brick B. 4. Now brick B is the topmost yellow brick and can be grabbed, but we need to remove it to get to brick D. 5. Remove brick B from the top of brick D. 6.Now brick D is the topmost white brick and can be grabbed.So we get the result as E, A, C, B, D. There is a set of bricks.For the brick A, the color is blue.The white brick B is on top of the brick C .The blue brick C is on top of the brick A .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick B? Answer: The bricks from bottom to top is A, C, B 1. Remove brick C from the top of brick A. 2. Now brick A is the topmost brick and can be grabbed, but we need to remove it to get to brick B. 3. Remove brick A from the top of brick C. 4. Now brick B is the topmost white brick and can be grabbed.So we get the result as C, A, B. The bricks from bottom to top is A, D, C, B, E 1. Remove brick E from the top of brick B. 2. Remove brick B from the top of brick C. 3. Now brick C is the topmost white brick and can be grabbed, but we need to remove it to get to brick D. 4. Remove brick C from the top of brick D. 5. Now brick D is the topmost yellow brick and can be grabbed.So we get the result as E, B, C, D. There is a set of bricks.The yellow brick C is on top of the brick B .The white brick B is on top of the brick A .For the brick A, the color is white.Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick B?</p>
<p>Question: Question:Question: There is a set of bricks.The white brick B is on top of the brick A .The white brick A is on top of the brick G .The white brick G is on top of the brick D .The blue brick D is on top of the brick F .The white brick H is on top of the brick B .For the brick C, the color is yellow.The white brick E is on top of the brick C .The white brick F is on top of the brick E .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick F? Answer: The bricks from bottom to top is C, E, F, D, G, A, B, H 1. Remove brick H from the top of brick B. 2. Remove brick B from the top of brick A. 3. Remove brick A from the top of brick G. 4. Remove brick G from the top of brick D. 5. Remove brick D from the top of brick F. 6.Now brick F is the topmost white brick and can be grabbed.So we get the result as H, B, A, G, D, F.Question: There is a set of bricks.The white brick B is on top of the brick C .The yellow brick D is on top of the brick A .For the brick A, the color is yellow.The yellow brick E is on top of the brick B .The white brick C is on top of the brick D .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick D?Answer:Question:</p>
<p>Table 6 :
6
Few-shot exemplars for full COS prompt for brick 2D.There is a set of bricks.There is a brick A. The brick B is in front of the brick A. The brick C is in front of the brick B. The brick D is on top of the brick B .The brick E is on top of the brick A .The brick F is on top of the brick C .The brick G is on top of the brick D .The brick H is on top of the brick G .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick A? There is a set of bricks.For the brick A. The yellow brick B is on top of the brick A .The blue brick C is on top of the brick B .The blue brick D is on top of the brick C .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick B?
Answer:We can getA//E,C//F,B//D//G//H,So we get the result as E, A.
Question:Question:</p>
<p>). 3. (medium, yellow, triangle) ∞↑ (blue, square).4. (small, blue, circle) ↑ (medium, triangle).B: 1. (large, blue, square) =&gt; edge of B. 2. (medium, black, triangle) =&lt; (blue, square).3. (large, blue, circle) ↑ (medium, black, triangle).4. (large, yellow, triangle) ↑ (large, blue, circle).C: 1. (small, yellow, triangle) &gt; (medium, black, circle).2. (yellow, triangle) =&gt; edge of C. 3. (small, blue, square) ∞&lt;</p>
<p>https://platform.openai.com/playground
https://platform.openai.com/playground
Answer:The bricks from bottom to top is A, B, C 1. Now brick C is the topmost brick and can be grabbed, but we need to remove it to get to brick B. 2. Now brick B is the topmost white brick and can be grabbed. So we get the result as C, B.
. (large, triangle, black) right -left
AcknowledgmentsWe acknowledge funding support from the NSFC Key project 62336006, and Center for Perceptual and Interactive Intelligence (CPII) Ltd. under the Innovation and Technology Commission's innoHK scheme.A AppendixA.1 Broader Impact COS is a prompting technique that is easy to use, which can effectively improve the performance of complex planning with LLMS.It also indicates that future training with LLMs can also be well benefited by employing COS in the training procedure to further improve LLM's planning abilities.A.2 Extended SettingsA.2.1 Number of TokensWe have mentioned that we used white spacing for calculating the number of tokens in the intermediate thinking steps.This was a typo and in fact, we accurately measures the number of tokens using the OpenAI Playground. 3 The numbers we reported are correct and there is no need for modification.A.2.2 Designing the Intermediate StepsThe intermediate steps we use in the demonstrations for CoT are created and modified from the zero-shot CoT from the LLMs by simply adding "Let's think step by step" before the answer.We then manually correct the intermediate steps from the outputs of using zero-shot CoT for further improvements.We attempted our best efforts in tuning the baselines, and we report the best results we achieved.A.3 Few-shot ExemplarsIn the remaining of this section, we demonstrate the few-shot exemplars used in the experiments in our study.We demonstrate the exemplars for both COS and CoT.Question: There are a set of bricks.There is a brick A. The brick B is in front of the brick A. The brick C is in front of the brick B. The brick D is on top of the brick B .The brick E is on top of the brick A .The brick F is on top of the brick C .The brick G is on top of the brick D .The brick H is on top of the brick G .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick A?Answer: Let's think step by step: 1.To get brick A, we find E is on top of A. 2. We find E is on the top.3 We need to remove brick E first, as it is on top of brick A. 4. Brick A is now accessible and can be grabbed.So we get the result as E, A.Question: There are a set of bricks.For the brick A. The yellow brick B is on top of the brick A .The blue brick C is on top of the brick B .The blue brick D is on top of the brick C .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick B?Answer: Let's think step by step: 1.To get brick B, we find C is on top of B 2. We find D is on top of C 3. We find D is on the top 4. We need to remove brick D, as it is on top of brick C. 5. We need to remove brick C, as it is on top of brick B. 6. Brick B is now accessible and can be grabbed.So we get the result as D, C, B.Question: There are a set of bricks.The brick P is on top of the brick R .The brick J is on top of the brick B .The brick D is on top of the brick P .The brick R is on top of the brick H .The brick K is in front of the brick M. The brick B is on top of the brick D .For the brick M, the color is blue.The brick C is on top of the brick J .The brick H is in front of the brick K. Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick M? Answer: Let's think step by step: 1.To get brick M, we find there is no brick on top of brick M, So we get the result as M directly.Question: There are a set of bricks.The brick K is on top of the brick F .The brick M is in front of the brick F. The brick N is on top of the brick K .For the brick O, the color is blue.The brick G is on top of the brick A .The brick F is in front of the brick I.The brick I is in front of the brick O.The brick A is on top of the brick N .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick O?Answer: Let's think step by step: 1.To get brick O, we find there is no brick on top of brick O, So we get the result as O directly.Question: There are a set of bricks.For the brick A, the color is blue.The yellow brick B is in front of the brick A. The yellow brick C is in front of the brick B. The white brick D is on top of the brick C .The white brick E is on top of the brick D .The yellow brick F is on top of the brick E .Now we have to get a specific brick.The bricks must now be grabbed from top to bottom, and if the lower brick is to be grabbed, the upper brick must be removed first.How to get brick C?Answer: Let's think step by step: 1.To get brick C, we find C is in front of B. 2. We find D is on top of C. 3. We find E is on top of D. 4. We find F is on top of E. 5. We need to remove brick F, as it is on top of brick E. 6.We need to remove brick E, as it is on top of brick D. 7. We need to remove brick D, as it is on top of brick C. 8. Brick C is now accessible and can be grabbed.So we get the result as F, E, D, C. Question: There is a set of roads and a set of landmarks.The start point is bank A. There is a road which is 200 meters long from bank A to bank C.There is a road which is 100 meters long from bank C to house H.There is a road which is 100 meters long from house H to cinema F. There is a road which is 200 meters long from cinema F to store B.There is a road which is 100 meters long from store B to store G.There is a road which is 200 meters long from bank C to house D. There is a road which is 200 meters long from house D to garden J.There is a road which is 100 meters long from bank A to cinema I.There is a road which is 100 meters long from cinema I to house E. From the start point, how to reach the nearest store?Answer: There are two stores on the map, store B and store G.-bank A / bank C / house H / cinema F / store B (200 + 100 + 100 + 200 = 600) -bank A / bank C / house H / cinema F / store B / store G (200 + 100 + 100 + 200 + 100 = 700) So we get the result as bank A / bank C / house H / cinema F / store B.So the answer is ACHFBQuestion: There is a set of roads and a set of landmarks.The start point is bank H.There is a road which is 100 meters long from bank H to store E.There is a road which is 200 meters long from store E to bank C.There is a road which is 100 meters long from bank C to house A. There is a road which is 100 meters long from house A to house F. There is a road which is 200 meters long from bank C to garden I.There is a road which is 200 meters long from store E to cinema G.There is a road which is 200 meters long from cinema G to school J.There is a road which is 200 meters long from bank H to school D. There is a road which is 100 meters long from school D to store B. From the start point, how to reach the nearest school?So the answer is HDQuestion: There is a set of roads and a set of landmarks.The start point is garden B. There is a road which is 200 meters long from garden B to cinema D. There is a road which is 200 meters long from cinema D to school J.There is a road which is 100 meters long from garden B to school I.There is a road which is 100 meters long from school I to house C.There is a road which is 100 meters long from house C to bank F.There is a road which is 100 meters long from house C to cinema G.There is a road which is 200 meters long from cinema G to store H.There is a road which is 100 meters long from store H to cinema E. There is a road which is 200 meters long from cinema E to bank A. From the start point, how to reach the nearest school?Answer: There are two schools on the map, school J and school I.-garden B / cinema D / school J (200+200 = 400) -garden B / school I (100) We get the result as garden B / school I (100).So the answer is BI Table9: Few-shot exemplars for full COT prompt for Natural Language Navigation Question: There is a set of roads and a set of landmarks.The start point is bank A. There is a road which is 200 meters long from bank A to bank C.There is a road which is 100 meters long from bank C to house H.There is a road which is 100 meters long from house H to cinema F. There is a road which is 200 meters long from cinema F to store B.There is a road which is 100 meters long from store B to store G.There is a road which is 200 meters long from bank C to house D. There is a road which is 200 meters long from house D to garden J.There is a road which is 100 meters long from bank A to cinema I.There is a road which is 100 meters long from cinema I to house E. From the start point, how to reach the nearest store?Answer: Let's think step by step 1. Start at bank A. 2. There are two stores on the map, store B and store G. 3. The road from bank A to store B is from bank A to bank C to house H to cinema F to store B (200 meters + 100 meters + 100 meters + 200 meters = 600 meters) 4. The road from bank A to store B is shorter than the road from bank A to store G. 5. Therefore, the nearest store from the start point (bank A) is store B. So the answer is ACHFB.Question: There is a set of roads and a set of landmarks.The start point is bank H.There is a road which is 100 meters long from bank H to store E.There is a road which is 200 meters long from store E to bank C.There is a road which is 100 meters long from bank C to house A. There is a road which is 100 meters long from house A to house F. There is a road which is 200 meters long from bank C to garden I.There is a road which is 200 meters long from store E to cinema G.There is a road which is 200 meters long from cinema G to school J.There is a road which is 200 meters long from bank H to school D. There is a road which is 100 meters long from school D to store B. From the start point, how to reach the nearest school?Answer: Let's think step by step 1. Start at bank H. 2. There are two schools on the map, school J and school D. 3. The road from bank H to school J is from bank H to store E to cinema G to school J (200 meters + 200 meters + 200 meters = 600 meters).4. The road from bank H to school D is from bank H to school D (200 meters). 5. Therefore, the nearest school from the start point (bank H) is school D. 6. Follow the road from bank H to school D(200 meters).So the answer is HD Question: There is a set of roads and a set of landmarks.The start point is garden B. There is a road which is 200 meters long from garden B to cinema D. There is a road which is 200 meters long from cinema D to school J.There is a road which is 100 meters long from garden B to school I.There is a road which is 100 meters long from school I to house C.There is a road which is 100 meters long from house C to bank F.There is a road which is 100 meters long from house C to cinema G.There is a road which is 200 meters long from cinema G to store H.There is a road which is 100 meters long from store H to cinema E. There is a road which is 200 meters long from cinema E to bank A. From the start point, how to reach the nearest school?Answer: Let's think step by step 1. Start at garden B. 2. There are two schools on the map, school J and school I. 3. The road from garden B to school J is from garden B to cinema D to school J (200 meters + 200 meters = 400 meters).4. The road from garden B to school I is from garden B to school I (100 meters). 5. Therefore, the nearest school from the start point (garden B) is school I. 6. Follow the road from garden B to school I (100 meters).So the answer is BI Story: There are three boxes.In the left box, there are one middle square in yellow, one middle square in black, one small square in blue, one middle square in blue.In the middle box, there are one large square in blue, one middle square in blue, one middle square in black, one large triangle in black, one middle round in blue, one small square in yellow.In the right box, there are one large round in blue, one small triangle in yellow, one large triangle in blue.Question:How to move all rounds to the middle box?Answer: To move all rounds to the middle box, we need to identify all the round objects in the three boxes.They are:-middle box: (middle, round, blue) -right box: (large, round, blue)Then we can move the objects not in the middle box above to the middle box one by one:-(large, round, blue) middle -right Story: There are three boxes.In the left box, there are one small round in blue, one small round in blue, one large round in black, one large square in blue, one small round in yellow, one small round in yellow.In the middle box, there are one large square in blue, one middle triangle in yellow.In the right box, there are one large round in black, one large round in blue, one middle triangle in black, one middle triangle in black.Question:How to move all squares to the right box?Answer: To move all sqaure objects to the right box, we need to identify all the square objects in the three boxes.They are:-left box: (large, square, blue) -middle box: (large, square, blue)Then we can move the objects not in the right box above to the right box:-(large, square, blue) left -right box -(large, square, blue) middle -right box Story: There are three boxes.In the left box, there are one large square in blue, one middle square in blue, one small round in blue, one middle triangle in blue, one middle round in yellow, one large square in yellow.In the middle box, there are one small round in yellow, one middle square in blue, one small triangle in black, one small square in black, one small triangle in yellow, one large round in black.In the right box, there are one small square in yellow, one small triangle in yellow, one middle triangle in black, one large round in yellow, one middle square in blue, one large square in yellow.Question:How to move all black objects to the right box?Answer: To move all black objects to the right box, we need to identify all the black objects in the three boxes.They are: -middle box: (small, triangle, black), (small, square, black), (large, round, black) Then we can move the objects not in the right box above to the right box:-(small, triangle, black) middle -right -(small, square, black) middle -right -(large, round, black) middle -right Story: There are three boxes.In the left box, there are one middle square in yellow, one middle square in black, one small square in blue, one middle square in blue.In the middle box, there are one large square in blue, one middle square in blue, one middle square in black, one large triangle in black, one middle round in blue, one small square in yellow.In the right box, there are one large round in blue, one small triangle in yellow, one large triangle in blue.Question:How to move all rounds to the middle box?Answer: To move all rounds to the middle box, we need to identify all the round objects in the three boxes.They are: -middle box: (middle, round, blue) -right box: (large, round, blue)Then we can move the objects not in the middle box above to the middle box:-(large, round, blue) right -middle Story: There are three boxes.In the left box, there are one large round in black, one small square in blue, one small square in blue, one small triangle in yellow, one middle square in yellow.In the middle box, there are one large square in yellow, one middle triangle in blue, one large round in black.In the right box, there are one large square in blue, one large triangle in black, one middle triangle in black.Question:How to move all black objects to the left box?Answer: To move all black objects to the left box, we need to identify all the black objects in the three boxes.They are:  Story: There are three boxes.In the left box, there are one middle square in yellow, one middle square in black, one small square in blue, one middle square in blue.In the middle box, there are one large square in blue, one middle square in blue, one middle square in black, one large triangle in black, one middle round in blue, one small square in yellow.In the right box, there are one large round in blue, one small triangle in yellow, one large triangle in blue.Question:How to move all rounds to the middle box?Answer: To move all rounds to the middle box, we need to identify all the round objects in the three boxes.They are: -One middle round in blue in the middle box -One large round in blue in the right box Then we can move the boxes above not in the middle box to the middle box one by one: 1. Move the large round in blue from the right box to the middle box.Now all the rounds are in the middle box.Story: There are three boxes.In the left box, there are one small round in blue, one small round in blue, one large round in black, one large square in blue, one small round in yellow, one small round in yellow.In the middle box, there are one large square in blue, one middle triangle in yellow.In the right box, there are one large round in black, one large round in blue, one middle triangle in black, one middle triangle in black.Question:How to move all squares to the right box?Answer: To move all sqaure objects to the right box, we need to identify all the square objects in the three boxes.They are: -One large square in blue in the left box -One large square in blue in the middle box Then we can move the box above with the square object to the right box: 1. Move the large square in blue from the left box to the right box. 2. Move the large square in blue from the middle box to the right box.Now all the square objects are in the right box.Story: There are three boxes.In the left box, there are one large square in blue, one middle square in blue, one small round in blue, one middle triangle in blue, one middle round in yellow, one large square in yellow.In the middle box, there are one small round in yellow, one middle square in blue, one small triangle in black, one small square in black, one small triangle in yellow, one large round in black.In the right box, there are one small square in yellow, one small triangle in yellow, one middle triangle in black, one large round in yellow, one middle square in blue, one large square in yellow.Question:How to move all black objects to the right box? Story: There are three boxes.In the left box, there are one middle square in yellow, one middle square in black, one small square in blue, one middle square in blue.In the middle box, there are one large square in blue, one middle square in blue, one middle square in black, one large triangle in black, one middle round in blue, one small square in yellow.In the right box, there are one large round in blue, one small triangle in yellow, one large triangle in blue.Question:How to move all rounds to the middle box?Answer: To move all rounds to the middle box, we need to identify all the round objects in the three boxes.They are: -One middle round in blue in the middle box -One large round in blue in the right box Then we can move the boxes above not in the middle box to the middle box one by one: 1. Move the large round in blue from the right box to the middle box.Now all the rounds are in the middle box.Story: There are three boxes.In the left box, there are one large round in black, one small square in blue, one small square in blue, one small triangle in yellow, one middle square in yellow.In the middle box, there are one large square in yellow, one middle triangle in blue, one large round in black.In the right box, there are one large square in blue, one large triangle in black, one middle triangle in black.Question:How to move all black objects to the left box?Answer: To move all black objects to the left box, we need to identify all the black objects in the three boxes.Wake-up time: 1pm.1pm-2pm: free.2pm-4pm: reading at the library.4pm-5pm: watching a movie at the theater.5pm-6pm: waiting at the airport.6pm-7pm: buying clothes at the mall.The museum closure time: 7pm.The only time when Emily could have gone to the museum was 1pm to 2pm.So the answer is (A).
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, Shyamal Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri Castellon, Annie Chatterji, Kathleen Chen, Jared Quincy Creel, Dora Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren Gale, Karan Gillespie, Noah Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, Omar Khani, Pang Wei Khattab, Mark Koh, Ranjay Krass, Rohith Krishna, Ananya Kuditipudi, Faisal Kumar, Mina Ladhak, Tony Lee, Jure Lee, Isabelle Leskovec, Levent, Lisa Xiang, Xuechen Li, Tengyu Li, Ali Ma, Christopher D Malik, Suvir Manning, Eric Mirchandani, Zanele Mitchell, Suraj Munyikwa, Avanika Nair, Deepak Narayan, Ben Narayanan, Allen Newman, Juan Carlos Nie, Hamed Niebles, Julian Nilforoshan, Giray Nyarko, Andy Ogut, Krishnan Shih, Alex Srinivasan, Rohan Tamkin, Armin W Taori, Florian Thomas, Rose E Tramèr, William Wang, Bohan Wang, Jiajun Wu, Yuhuai Wu, Sang Wu, Michihiro Michael Xie, Jiaxuan Yasunaga, Matei You, Michael Zaharia, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zhang, Kaitlyn Zheng, Percy Zhou, Liang, 10.48550/arXiv.2108.07258arXiv:2108.07258Joon Sung Park. Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Laurel Orr, Isabel PapadimitriouAugust 2021On the Opportunities and Risks of Foundation Models. arXiv e-prints, art</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Curran Associates, Inc202033</p>
<p>Touchdown: Natural language navigation and spatial reasoning in visual street environments. Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, Yoav Artzi, 10.1109/CVPR.2019.012822019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019</p>
<p>Vision-and-language navigation: A survey of tasks, methods, and future directions. Jing Gu, Eliana Stefani, Qi Wu, Jesse Thomason, Xin Wang, 10.18653/v1/2022.acl-long.524Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational LinguisticsMay 20221</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Representation learning for grounded spatial reasoning. Michael Janner, Karthik Narasimhan, Regina Barzilay, 10.1162/tacl_a_00004Transactions of the Association for Computational Linguistics. 62018</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Complete coverage path planning using reinforcement learning for tetromino based cleaning and maintenance robot. Automation in Construction. Anirudh Krishna Lakshmanan, Rajesh Elara Mohan, Balakrishnan Ramalingam, Anh Vu Le, Prabahar Veerajagadeshwar, Kamlesh Tiwari, Muhammad Ilyas, 10.1016/j.autcon.2020.103078.URLhttps://www.sciencedirect.com/science/article/pii/S09265805193058132020112103078</p>
<p>Llm+p: Empowering large language models with optimal planning proficiency, 2023. Roshanak Mirzaee and Parisa Kordjamshidi. Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, 10.18653/v1/2022.emnlp-main.413Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsDecember 2022Transfer learning with synthetic corpora for spatial role labeling and reasoning</p>
<p>SPARTQA: A textual question answering benchmark for spatial reasoning. Roshanak Mirzaee, Rajaby Hossein, Qiang Faghihi, Parisa Ning, Kordjamshidi, 10.18653/v1/2021.naacl-main.364Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineJune 2021Association for Computational Linguistics</p>
<p>Vision-based navigation with language-based assistance via imitation learning with indirect intervention. Khanh Nguyen, Debadeepta Dey, Chris Brockett, Bill Dolan, 10.1109/CVPR.2019.012812019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 2019</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, Augustus Odena, 10.48550/arXiv.2112.00114arXiv:2112.00114Show Your Work: Scratchpads for Intermediate Computation with Language Models. arXiv e-prints, art. November 2021</p>
<p>Grid path planning with deep reinforcement learning: Preliminary results. I Aleksandr, Konstantin S Panov, Roman Yakovlev, Suvorov, 10.1016/j.procs.2018.01.054.URLhttps://www.sciencedirect.com/science/article/pii/S1877050918300553.8thAnnual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society. Moscow, Russia2018. August 1-6, 2017123</p>
<p>Keqin Peng, Liang Ding, Qihuang Zhong, Li Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, Dacheng Tao, arXiv:2303.13780Towards Making the Most of ChatGPT for Machine Translation. arXiv e-prints, art. March 2023</p>
<p>From "before" to "after": Generating natural language instructions from image pairs in a simple visual domain. Robin Rojowiec, Jana Götze, Philipp Sadler, Henrik Voigt, Sina Zarrieß, David Schlangen, Proceedings of the 13th International Conference on Natural Language Generation. the 13th International Conference on Natural Language GenerationDublin, IrelandAssociation for Computational LinguisticsDecember 2020</p>
<p>Large Language Models Can Be Easily Distracted by Irrelevant Context. arXiv e-prints. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, Denny Zhou, 10.48550/arXiv.2302.00093arXiv:2302.00093January 2023</p>
<p>Language models are multilingual chain-of-thought reasoners. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard De Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jan Koco Ń, Jana Thompson, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U Balis, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Jones, Joshua B Tenenbaum, Joshua S Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, D Kaustubh, Kevin Dhole, Kevin Gimpel, Kory Omondi, Kristen Mathewson, Ksenia Chiafullo, Kumar Shkaruta, Kyle Shridhar, Kyle Mcdonell, Laria Richardson, Leo Reynolds, Li Gao, Liam Zhang, Lianhui Dugan, Lidia Qin, Louis-Philippe Contreras-Ochando, Luca Morency, Lucas Moschella, Lucy Lam, Ludwig Noble, Luheng Schmidt, Luis He, Luke Oliveros Colón, Metz ; Maheen, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Maru, Jose Ramírez, Marie Quintana, Mario Tolkiehn, Martha Giulianelli, Martin Lewis, Matthew L Potthast, Matthias Leavitt, Mátyás Hagen, Medina Schubert, Melody Orduna Baitemirova, Melvin Arnaud, Michael A Mcelrath, Michael Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michał Strube, Michele Swędrowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Mo Suzgun, Mohit Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T Varma, Nanyun Peng, Nathan Chi, Nayeon Lee, Neta Gur-, Ari Krakover, ; Niveditha, S Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Sam Shleifer, Sam Wiseman, Samuel Gruetter, R Samuel, Samuel S Bowman, Sanghyun Schoenholz, Sanjeev Han, Sarah A Kwatra, Sarik Rous, Sayan Ghazarian, Sean Ghosh, Sebastian Casey, Sebastian Bischoff, Sebastian Gehrmann, Sepideh Schuster, Shadi Sadeghi, Sharon Hamdan, Shashank Zhou, Sherry Srivastava, Shikhar Shi, Shima Singh, Asaadi, Shane Shixiang, Shubh Gu, Shubham Pachchigar, Shyam Toshniwal, Upadhyay, Shyamolima, Siamak Debnath, Simon Shakeri, Simone Thormeyer, Siva Melzi, Reddy, Priscilla Sneha, Soo-Hwan Makini, Spencer Lee, Sriharsha Torene, Stanislas Hatwar, Stefan Dehaene, Stefano Divic, Stella Ermon, Stephanie Biderman, Stephen Lin, Steven T Prasad, Stuart M Piantadosi, Summer Shieber, Svetlana Misherghi, Swaroop Kiritchenko, Mishra, William Vivek Srikumar, William Fedus, William Saunders, Wout Zhang, Xiang Vossen, Xiaoyu Ren, Xinran Tong, Xinyi Zhao, Xudong Wu, Yadollah Shen, Yair Yaghoobzadeh, Yangqiu Lakretz, Yasaman Song, Yejin Bahri, Yichi Choi, Yiding Yang, Yifu Hao, Yonatan Chen, Yu Belinkov, Yufang Hou, Yuntao Hou, Zachary Bai, Zhuoye Seid, Zijian Zhao, Zijie J Wang, Zirui Wang, Ziyi Wang, Wu, 10.48550/arXiv.2206.04615arXiv:2206.04615Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi. Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nikita Nangia, Niklas Deckers, Niklas Muennighoff; Omer Levy, Owain Evans, Pablo Antonio Moreno Casares; Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto; Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Timothy Telleen-Lawton, Titus Tunduny, Tobias Gerstenberg, Trenton ChangTe-Lin WuJune 2022Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar,. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. arXiv e-prints, art</p>
<p>A corpus of natural language for visual reasoning. Alane Suhr, Mike Lewis, James Yeh, Yoav Artzi, 10.18653/v1/P17-2034Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsJuly 20172Short Papers)</p>
<p>Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang, Chengcheng Han, Renyu Zhu, Shuai Yuan, arXiv:2403.14734A survey of neural code intelligence: Paradigms, advances and beyond. 2024arXiv preprint</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Can Large Language Models Play Text Games Well? Current State-of-the-Art and Open Questions. Chen Feng Tsai, Xiaochen Zhou, Sierra S Liu, Jing Li, Mo Yu, Hongyuan Mei, 10.48550/arXiv.2304.02868arXiv:2304.02868April 2023arXiv e-prints, art</p>
<p>Chatgpt empowered long-step robot control in various environments: A case application. Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi, 2023</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Jerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, Quoc V Le, 10.48550/arXiv.2305.08298arXiv:2305.08298Symbol tuning improves incontext learning in language models. arXiv e-prints, art. May 2023</p>            </div>
        </div>

    </div>
</body>
</html>
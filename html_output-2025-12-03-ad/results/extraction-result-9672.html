<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9672 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9672</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9672</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-274965768</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.15249v2.pdf" target="_blank">LitLLMs, LLMs for Literature Review: Are we there yet?</a></p>
                <p><strong>Paper Abstract:</strong> Literature reviews are an essential component of scientific research, but they remain time-intensive and challenging to write, especially due to the recent influx of research papers. This paper explores the zero-shot abilities of recent Large Language Models (LLMs) in assisting with the writing of literature reviews based on an abstract. We decompose the task into two components: 1. Retrieving related works given a query abstract, and 2. Writing a literature review based on the retrieved results. We analyze how effective LLMs are for both components. For retrieval, we introduce a novel two-step search strategy that first uses an LLM to extract meaningful keywords from the abstract of a paper and then retrieves potentially relevant papers by querying an external knowledge base. Additionally, we study a prompting-based re-ranking mechanism with attribution and show that re-ranking doubles the normalized recall compared to naive search methods, while providing insights into the LLM's decision-making process. In the generation phase, we propose a two-step approach that first outlines a plan for the review and then executes steps in the plan to generate the actual review. To evaluate different LLM-based literature review methods, we create test sets from arXiv papers using a protocol designed for rolling use with newly released LLMs to avoid test set contamination in zero-shot evaluations. We release this evaluation protocol to promote additional research and development in this regard. Our empirical results suggest that LLMs show promising potential for writing literature reviews when the task is decomposed into smaller components of retrieval and planning. Our project page including a demonstration system and toolkit can be accessed here: https://litllm.github.io.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9672.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9672.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LitLLM pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LitLLMs: LLM-based pipeline for literature review (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end retrieval + generation pipeline that uses LLMs to (1) extract keywords from a query abstract, (2) retrieve candidate papers via keyword and embedding search, (3) re-rank candidates with LLM prompting (including an attribution step), and (4) generate a related-work section using plan-based, retrieval-augmented generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Ensemble of LLMs (GPT-4, GPT-3.5-turbo, Llama 2 variants, Llama-3.1-70B, Falcon-180B, CodeLlama)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The paper uses a mixture of closed-source models (GPT-3.5-turbo, GPT-4) and open-source models (Llama 2 Chat 7B/13B/70B, Llama-3.1-70B, Falcon-180B, CodeLlama 34B) for different pipeline stages: keyword extraction, re-ranking (permutation or debate-style prompting with attribution), plan generation, and final RAG-conditioned text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4: N/A; GPT-3.5: N/A; Llama 2: 7B/13B/70B; Llama-3.1:70B; Falcon:180B; CodeLlama:34B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Uses extended Multi-XScience corpus (full text added where available), newly created RollingEval datasets from arXiv (August 2023 and December 2023), Semantic Scholar / S2ORC extracts for retrieved candidate abstracts, and an optional SPECTER2 150M-embedding index for embedding-based retrieval. Retrieval candidate pools were constructed by LLM-generated keyword queries to Semantic Scholar and SERP API (site:arxiv.org) and/or top-n nearest neighbors in SPECTER2.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Given a short query abstract (early-stage paper summary), find relevant prior works and synthesize a related-work section grounded in retrieved abstracts; supports interactive sentence plans.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Two-stage approach: (1) Retrieval: LLM generates multiple keyword queries from the query abstract; search engines (Semantic Scholar, SERP) return top-k candidates; optionally combine with SPECTER2 embedding nearest-neighbors. (2) Re-ranking: LLM re-rankers used (permutation prompt, and a debate-style prompt that generates pro/con arguments and a probability of inclusion with explicit extraction of supporting sentences). (3) Generation: Retrieval-augmented generation (RAG) where top-k re-ranked abstracts are provided as context and the LLM produces a sentence-plan-guided or plan-generated related-work paragraph (teacher-forced plan, prompted plan, or learned plan). Variants include per-cite and sentence-by-sentence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Narrative related-work sections (multi-document abstractive summary), plus intermediate sentence plans and ranked candidate lists with extracted attribution sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>"Please generate 5 sentences in 60 words. Cite @cite_1 at line 1, ..." — sample plan-conditioned related-work paragraph generated by GPT-4 Plan that closely follows the specified sentence plan and cites the selected references.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated metrics (ROUGE-1/2/L, BERTScore, Llama-3-Eval/G-Eval style), retrieval metrics (precision@k, NormalizedRecall@k), ablation studies (e.g., remove attribution verification), and human expert evaluation (6 annotators ranking outputs and marking hallucinations). Statistical tests (t-test, McNemar) used to establish significance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Plan-based RAG improved automated metrics relative to zero-shot baselines: e.g., GPT-4 (Plan) ROUGE1 ≈ 37.20, ROUGE2 ≈ 8.86, ROUGEL ≈ 18.77 on Multi-XScience; plan-based prompting reduced hallucinations by ~18–26% and increased coverage (GPT-4 coverage ≈ 98%). Combining keyword- and embedding-based retrieval improved precision by ~10% and normalized recall by ~30% over either method alone. However overall retrieval coverage of ground-truth cited papers was low (~<7% for naive keyword search across months).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>1) Decomposes the task into retrieval + re-ranking + plan-based generation for better controllability and factuality. 2) LLM-generated keyword queries plus SPECTER embeddings complement each other to improve retrieval precision/recall. 3) Debate-style re-ranking with explicit extracted supporting sentences increases transparency and ranking quality. 4) Plan-based generation notably reduces hallucinations and improves citation coverage and human preference.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>1) Retrieval coverage is low when relying on abstracts alone (authors report <7% of ground-truth references retrieved in some settings). 2) Re-ranking via LLMs can be brittle (GPT-4 produced incomplete lists 41% of the time, and occasional repeated/garbage values). 3) Plan-based prompting reduces but does not eliminate hallucinations. 4) Computational and monetary cost is high (debate ranking requires many API calls). 5) Reliance on abstracts misses information present only in other sections; distributional shift/test-set contamination remains a concern.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Examples include GPT-4 re-ranker producing incomplete or malformed rankings (41% incomplete lists), LLMs hallucinating nonexistent citations or invented statements (more frequent in zero-shot than plan-based outputs), and RoPE scaling attempts producing gibberish for longer contexts without extensive tuning; learned-plan generation improves but still underperforms teacher-forced plans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLMs, LLMs for Literature Review: Are we there yet?', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9672.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9672.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Debate Ranking (Attribution)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Debate-style LLM re-ranking with attribution/verification (pro/con arguments + extracted supporting sentences)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting-based, listwise re-ranking strategy where an LLM generates arguments for and against including each candidate paper and outputs an inclusion probability; it also extracts verbatim sentences from candidate abstracts to attribute relevance, with a verification loop to ensure extracted sentences actually exist.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM re-ranker (evaluated with GPT-4 and GPT-3.5 in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompts instruct the LLM to (a) read query abstract + candidate abstract, (b) produce arguments for/against inclusion, (c) extract verbatim supporting sentences from the candidate, and (d) output a probability of inclusion. The pipeline re-prompts if attributed sentences are not actually present (verification).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-4: N/A; GPT-3.5: N/A (used as re-rankers)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Lists of candidate paper abstracts retrieved via keyword or embedding search, typically a candidate pool of up to 100 papers per query (constructed by merging top results from multiple search queries/engines).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>100</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Re-rank a retrieval-produced candidate set by estimated relevance to a specific query abstract, providing reasoned support and verbatim attributions for decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Listwise debate-style prompting: for each candidate, the LLM generates pro/con arguments, extracts supporting sentences from the candidate abstract, and assigns an inclusion probability; candidate probabilities are then used to order the list. An explicit verification step checks that extracted sentences appear verbatim in the candidate abstract and re-prompts the model if not.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Ordered ranked list of candidate papers plus per-candidate pro/con rationales, inclusion probabilities, and quoted supporting sentences (attributions).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>{"candidate_id": X, "prob_include": 0.87, "for": "...", "against": "...", "supporting_sentences": ["...sentence verbatim..."]}</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Precision@k and NormalizedRecall@k for ranked lists; ablation removing attribution verification; t-tests over repeated seeds to measure significance.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Debate ranking significantly outperformed permutation-ranking prompting at small top-k values in precision and normalized recall. Removing attribution verification caused statistically significant drops (precision p = 4.7e-4, normalized recall p = 1.9e-6). However, SPECTER2 embedding-based ranker still outperformed GPT-4 re-ranking on some metrics. Practical brittleness observed: GPT-4 re-ranker produced incomplete lists 41% of the time and repeated values 3.3% of the time (RollingEval-Aug/Dec aggregated).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Provides interpretable, attributable reasons for inclusion/exclusion; verification step materially improves ranking quality; better prioritization of retrieved relevant documents at top-k.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High computational/API cost (debate ranking requires an API call per candidate, so n * k calls overall), brittle outputs from some LLMs (incomplete/repeated/garbage outputs), and embedding-ranker sometimes outperforms prompting in absolute ranking metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>GPT-4 produced incomplete ranked lists in ~41% of runs; rare garbage outputs (e.g., numbers like '2020'); in ablations where attribution verification is removed, precision and normalized recall drop significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLMs, LLMs for Literature Review: Are we there yet?', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9672.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9672.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Plan-based Generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plan-based retrieval-augmented generation for literature reviews</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generation strategy that first produces (or accepts a user-provided) sentence-level plan specifying which papers to cite on which lines, and then conditions the LLM to generate the related-work paragraph according to this plan to improve factuality, citation coverage, and controllability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs used for generation: GPT-4, GPT-3.5-turbo, Llama 2-Chat (7B/13B/70B), Llama-3.1-70B, CodeLlama 34B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Models are prompted in a two-stage fashion: (1) produce or accept a sentence-plan (mapping lines to citations), and (2) autoregressively generate the related-work conditioned on the plan plus retrieved abstracts; variants: teacher-forced (ground-truth) plan, prompted plan (model generates plan), learned plan (model generates plan then text), per-cite generation, sentence-by-sentence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>See individual models: Llama 2: 7B/13B/70B; Llama-3.1:70B; CodeLlama:34B; Falcon:180B; GPT models unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Query abstract plus abstracts of selected retrieved references (top-k chosen from re-ranked list). Evaluations used Multi-XScience extended (full text for many records) and RollingEval-Aug (recent arXiv). Typical examples had ~2 cited papers per case (dataset averages).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Generate a human-quality related-work paragraph for a given query abstract, ensuring cited references are discussed and reducing hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Plan-conditioned RAG: the model receives the query abstract, the abstracts of the selected references, and a sentence plan (either user-provided, teacher-forced from ground truth, or generated) that specifies number of sentences and which citations to mention on which lines; the model then generates the final text. Alternative strategies: per-cite micro-generation followed by aggregation; sentence-by-sentence conditioned on preceding draft.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Plan-constrained abstractive related-work paragraphs (narrative synthesis), along with the plans themselves as intermediate structured artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>"Please generate 5 sentences in 60 words. Cite @cite_1 at line 1, Cite @cite_2 at line 2, ..." → generated 5-sentence paragraph that cites the specified works where indicated.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>ROUGE-1/2/L comparisons, BERTScore, Llama-3-Eval (G-Eval style), coverage metric (percentage of citations covered in output), human expert ranking and hallucination annotation, and comparison to extractive/finetuned baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Teacher-forced plan-based generation outperformed zero-shot baselines: GPT-4 (Plan) gave highest ROUGE/BERTScore and Llama-3-Eval (e.g., ROUGE1 ≈ 37.20 on Multi-XScience). Plan-based prompting reduced hallucinations (e.g., for Llama 2-Chat hallucinations fell from 58.6% to 32.7% in one study; GPT-4 from 29.6% to 11.6% in another) and increased citation coverage (GPT-4 coverage ≈ 98%; Llama 2-Chat 70B coverage rose from 59% to 82% with plan). Learned-plan variants improved over zero-shot but underperformed teacher-forced plans per automatic metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Gives users structural control, substantially reduces hallucinations and increases citation coverage, aligns with human preferences in expert ranking, and improves automatic metrics over zero-shot generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Depends on quality of retrieved references (low retrieval coverage limits end-to-end gains), learned-plan methods are less effective than teacher-forced plans, some models (GPT-3.5) struggle to precisely follow plans, and plan-based prompting does not fully eliminate hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Models sometimes produce fewer sentences than specified in the plan; GPT-3.5 struggles to follow plans precisely; zero-shot Llama variants hallucinate citations (e.g., invented 'XYZ et al.') which plan-based methods reduced but did not always remove entirely.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLMs, LLMs for Literature Review: Are we there yet?', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9672.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9672.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPECTER2 embedding retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SPECTER2: large-scale document embeddings for scientific retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embedding-based retrieval approach using SPECTER2 document embeddings (150M embeddings) for nearest-neighbor retrieval and ranking of candidate scientific papers; used as an alternative and complement to LLM keyword search and prompting-based re-ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Specter: Document-level representation learning using citation-informed transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SPECTER2 (contrastive SciBERT-based document embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SPECTER2 provides dense document embeddings trained with contrastive objectives on scientific text (citation-informed). The authors built a FAISS index over the 150M SPECTER2 embeddings and retrieved nearest neighbors by cosine similarity to the query abstract embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Index of ≈150 million document embeddings (SPECTER2) representing scientific articles; FAISS indexes were constructed for nearest-neighbor retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td>150000000</td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Retrieve semantically closest documents to a query abstract for downstream re-ranking and literature review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Embedding-based nearest-neighbor retrieval (FAISS) using SPECTER2 embeddings; retrieved papers optionally re-ranked by LLMs or used directly for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>List of semantically-similar candidate papers (ranked by embedding cosine similarity) used as retrieval context for subsequent LLM re-ranking/generation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>[top-100 nearest neighbor document IDs from SPECTER2 for query abstract X]</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared precision@k and NormalizedRecall@k to keyword-based search and LLM-based re-rankers; embedding-ranker treated as baseline/alternative.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>SPECTER (embedding) ranking often outperformed prompting-based LLM re-ranking on precision and normalized recall; combining SPECTER embedding retrieval with keyword search improved retrieval performance overall. SPECTER-based search achieved higher precision and normalized recall in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Scalable to very large corpora (150M embeddings), robust semantic retrieval that often beats LLM prompting for ranking, and low run-time cost per query once the index is constructed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Embedding-only retrieval lacks explicit attribution/explanations; embedding indices require substantial storage and pre-processing; coverage still limited if query context (abstract) misses key signals authors used when selecting citations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Embedding retrieval can return high-similarity documents that are not judged by authors as cited (authors cite for varied reasons), so high embedding similarity does not guarantee inclusion in ground-truth related work; merging with keyword queries is necessary to improve coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLMs, LLMs for Literature Review: Are we there yet?', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9672.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9672.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Galactica (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Galactica: a large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior large language model trained on a large scientific corpus designed to store, combine and reason about scientific knowledge; mentioned as an example of LLMs for scientific reasoning, but noted to hallucinate non-existent citations and not be fine-tuned for plan-based writing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Galactica: A large language model for science</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Galactica</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large LLM trained on a scientific corpus (papers, knowledge bases, reference material) that was reported to perform well on scientific tasks; in this paper it is discussed critically for hallucination behavior and lack of instruction fine-tuning for plan-driven literature review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Trained on large-scale scientific texts, reference material and curated knowledge sources (as described in the Galactica paper); here it is cited as prior work demonstrating potential for scientific knowledge synthesis but with limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Mentioned as a model that aims to store/combine/scaffold scientific knowledge across many documents, relevant to the goal of LLMs synthesizing literature.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not used in experiments; referenced as a pretrained model for scientific tasks (no plan-based fine-tuning reported in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Mentioned outputs in original Galactica work include scientific Q&A, organized material; in this paper, used as a cautionary example of hallucinated citations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated in the present experiments; Galactica's behavior noted qualitatively (hallucinations) in the discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Cited prior claims that Galactica outperforms then-existing models on some scientific tasks, but the present paper notes notable hallucination problems (non-existent citations) and lack of plan-following instruction tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Demonstrates promise of large LMs as an interface for scientific knowledge (store/combine/reason).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Hallucinated non-existent citations and results; not instruction fine-tuned for writing plans, making it ill-suited for grounded literature review generation without additional techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Generates imaginary prior work and citations (hallucinated references) when used naively for literature review tasks per the paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLMs, LLMs for Literature Review: Are we there yet?', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9672.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9672.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Emerging literature-review tools (2025 mentions)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepResearch, AI Co-Scientist, ScholarQA (examples of 2025 AI-assisted research tools)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Named recent systems (DeepResearch, AI Co-Scientist, ScholarQA) cited as examples of tools that improve literature review generation, citation accuracy, and retrieval strategies in 2025; referenced as part of the evolving ecosystem rather than evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepResearch; AI Co-Scientist; ScholarQA (unnamed underlying models in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Briefly mentioned 2025 tools said to demonstrate remarkable improvements in literature review generation and retrieval; details on architectures/training are not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper; mentioned as contemporary systems operating on scientific corpora and retrieval+generation techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Examples of deployed/advanced systems for literature review assistance and synthesis of scientific knowledge across many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Not detailed here; cited as examples of improved retrieval and generation systems in the landscape.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Not specified in this paper (implied: improved literature reviews, citation accuracy, retrieval pipelines).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Serves as evidence that the ecosystem is advancing rapidly and that integrated retrieval + generation tools are being developed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>No implementation, methods, or empirical results provided in this paper; referenced only as contextual examples.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not applicable / not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LitLLMs, LLMs for Literature Review: Are we there yet?', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multi-XScience: A Large-Scale Multi-Document Summarization Dataset <em>(Rating: 2)</em></li>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>Specter: Document-level representation learning using citation-informed transformers <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>ContextCite: Attributing model generation to context <em>(Rating: 1)</em></li>
                <li>Zero-shot listwise document reranking with a large language model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9672",
    "paper_id": "paper-274965768",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "LitLLM pipeline",
            "name_full": "LitLLMs: LLM-based pipeline for literature review (this paper)",
            "brief_description": "An end-to-end retrieval + generation pipeline that uses LLMs to (1) extract keywords from a query abstract, (2) retrieve candidate papers via keyword and embedding search, (3) re-rank candidates with LLM prompting (including an attribution step), and (4) generate a related-work section using plan-based, retrieval-augmented generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Ensemble of LLMs (GPT-4, GPT-3.5-turbo, Llama 2 variants, Llama-3.1-70B, Falcon-180B, CodeLlama)",
            "model_description": "The paper uses a mixture of closed-source models (GPT-3.5-turbo, GPT-4) and open-source models (Llama 2 Chat 7B/13B/70B, Llama-3.1-70B, Falcon-180B, CodeLlama 34B) for different pipeline stages: keyword extraction, re-ranking (permutation or debate-style prompting with attribution), plan generation, and final RAG-conditioned text generation.",
            "model_size": "GPT-4: N/A; GPT-3.5: N/A; Llama 2: 7B/13B/70B; Llama-3.1:70B; Falcon:180B; CodeLlama:34B",
            "input_corpus_description": "Uses extended Multi-XScience corpus (full text added where available), newly created RollingEval datasets from arXiv (August 2023 and December 2023), Semantic Scholar / S2ORC extracts for retrieved candidate abstracts, and an optional SPECTER2 150M-embedding index for embedding-based retrieval. Retrieval candidate pools were constructed by LLM-generated keyword queries to Semantic Scholar and SERP API (site:arxiv.org) and/or top-n nearest neighbors in SPECTER2.",
            "input_corpus_size": null,
            "topic_query_description": "Given a short query abstract (early-stage paper summary), find relevant prior works and synthesize a related-work section grounded in retrieved abstracts; supports interactive sentence plans.",
            "distillation_method": "Two-stage approach: (1) Retrieval: LLM generates multiple keyword queries from the query abstract; search engines (Semantic Scholar, SERP) return top-k candidates; optionally combine with SPECTER2 embedding nearest-neighbors. (2) Re-ranking: LLM re-rankers used (permutation prompt, and a debate-style prompt that generates pro/con arguments and a probability of inclusion with explicit extraction of supporting sentences). (3) Generation: Retrieval-augmented generation (RAG) where top-k re-ranked abstracts are provided as context and the LLM produces a sentence-plan-guided or plan-generated related-work paragraph (teacher-forced plan, prompted plan, or learned plan). Variants include per-cite and sentence-by-sentence generation.",
            "output_type": "Narrative related-work sections (multi-document abstractive summary), plus intermediate sentence plans and ranked candidate lists with extracted attribution sentences.",
            "output_example": "\"Please generate 5 sentences in 60 words. Cite @cite_1 at line 1, ...\" — sample plan-conditioned related-work paragraph generated by GPT-4 Plan that closely follows the specified sentence plan and cites the selected references.",
            "evaluation_method": "Automated metrics (ROUGE-1/2/L, BERTScore, Llama-3-Eval/G-Eval style), retrieval metrics (precision@k, NormalizedRecall@k), ablation studies (e.g., remove attribution verification), and human expert evaluation (6 annotators ranking outputs and marking hallucinations). Statistical tests (t-test, McNemar) used to establish significance.",
            "evaluation_results": "Plan-based RAG improved automated metrics relative to zero-shot baselines: e.g., GPT-4 (Plan) ROUGE1 ≈ 37.20, ROUGE2 ≈ 8.86, ROUGEL ≈ 18.77 on Multi-XScience; plan-based prompting reduced hallucinations by ~18–26% and increased coverage (GPT-4 coverage ≈ 98%). Combining keyword- and embedding-based retrieval improved precision by ~10% and normalized recall by ~30% over either method alone. However overall retrieval coverage of ground-truth cited papers was low (~&lt;7% for naive keyword search across months).",
            "strengths": "1) Decomposes the task into retrieval + re-ranking + plan-based generation for better controllability and factuality. 2) LLM-generated keyword queries plus SPECTER embeddings complement each other to improve retrieval precision/recall. 3) Debate-style re-ranking with explicit extracted supporting sentences increases transparency and ranking quality. 4) Plan-based generation notably reduces hallucinations and improves citation coverage and human preference.",
            "limitations": "1) Retrieval coverage is low when relying on abstracts alone (authors report &lt;7% of ground-truth references retrieved in some settings). 2) Re-ranking via LLMs can be brittle (GPT-4 produced incomplete lists 41% of the time, and occasional repeated/garbage values). 3) Plan-based prompting reduces but does not eliminate hallucinations. 4) Computational and monetary cost is high (debate ranking requires many API calls). 5) Reliance on abstracts misses information present only in other sections; distributional shift/test-set contamination remains a concern.",
            "failure_cases": "Examples include GPT-4 re-ranker producing incomplete or malformed rankings (41% incomplete lists), LLMs hallucinating nonexistent citations or invented statements (more frequent in zero-shot than plan-based outputs), and RoPE scaling attempts producing gibberish for longer contexts without extensive tuning; learned-plan generation improves but still underperforms teacher-forced plans.",
            "uuid": "e9672.0",
            "source_info": {
                "paper_title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Debate Ranking (Attribution)",
            "name_full": "Debate-style LLM re-ranking with attribution/verification (pro/con arguments + extracted supporting sentences)",
            "brief_description": "A prompting-based, listwise re-ranking strategy where an LLM generates arguments for and against including each candidate paper and outputs an inclusion probability; it also extracts verbatim sentences from candidate abstracts to attribute relevance, with a verification loop to ensure extracted sentences actually exist.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLM re-ranker (evaluated with GPT-4 and GPT-3.5 in experiments)",
            "model_description": "Prompts instruct the LLM to (a) read query abstract + candidate abstract, (b) produce arguments for/against inclusion, (c) extract verbatim supporting sentences from the candidate, and (d) output a probability of inclusion. The pipeline re-prompts if attributed sentences are not actually present (verification).",
            "model_size": "GPT-4: N/A; GPT-3.5: N/A (used as re-rankers)",
            "input_corpus_description": "Lists of candidate paper abstracts retrieved via keyword or embedding search, typically a candidate pool of up to 100 papers per query (constructed by merging top results from multiple search queries/engines).",
            "input_corpus_size": 100,
            "topic_query_description": "Re-rank a retrieval-produced candidate set by estimated relevance to a specific query abstract, providing reasoned support and verbatim attributions for decisions.",
            "distillation_method": "Listwise debate-style prompting: for each candidate, the LLM generates pro/con arguments, extracts supporting sentences from the candidate abstract, and assigns an inclusion probability; candidate probabilities are then used to order the list. An explicit verification step checks that extracted sentences appear verbatim in the candidate abstract and re-prompts the model if not.",
            "output_type": "Ordered ranked list of candidate papers plus per-candidate pro/con rationales, inclusion probabilities, and quoted supporting sentences (attributions).",
            "output_example": "{\"candidate_id\": X, \"prob_include\": 0.87, \"for\": \"...\", \"against\": \"...\", \"supporting_sentences\": [\"...sentence verbatim...\"]}",
            "evaluation_method": "Precision@k and NormalizedRecall@k for ranked lists; ablation removing attribution verification; t-tests over repeated seeds to measure significance.",
            "evaluation_results": "Debate ranking significantly outperformed permutation-ranking prompting at small top-k values in precision and normalized recall. Removing attribution verification caused statistically significant drops (precision p = 4.7e-4, normalized recall p = 1.9e-6). However, SPECTER2 embedding-based ranker still outperformed GPT-4 re-ranking on some metrics. Practical brittleness observed: GPT-4 re-ranker produced incomplete lists 41% of the time and repeated values 3.3% of the time (RollingEval-Aug/Dec aggregated).",
            "strengths": "Provides interpretable, attributable reasons for inclusion/exclusion; verification step materially improves ranking quality; better prioritization of retrieved relevant documents at top-k.",
            "limitations": "High computational/API cost (debate ranking requires an API call per candidate, so n * k calls overall), brittle outputs from some LLMs (incomplete/repeated/garbage outputs), and embedding-ranker sometimes outperforms prompting in absolute ranking metrics.",
            "failure_cases": "GPT-4 produced incomplete ranked lists in ~41% of runs; rare garbage outputs (e.g., numbers like '2020'); in ablations where attribution verification is removed, precision and normalized recall drop significantly.",
            "uuid": "e9672.1",
            "source_info": {
                "paper_title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Plan-based Generation",
            "name_full": "Plan-based retrieval-augmented generation for literature reviews",
            "brief_description": "A generation strategy that first produces (or accepts a user-provided) sentence-level plan specifying which papers to cite on which lines, and then conditions the LLM to generate the related-work paragraph according to this plan to improve factuality, citation coverage, and controllability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLMs used for generation: GPT-4, GPT-3.5-turbo, Llama 2-Chat (7B/13B/70B), Llama-3.1-70B, CodeLlama 34B",
            "model_description": "Models are prompted in a two-stage fashion: (1) produce or accept a sentence-plan (mapping lines to citations), and (2) autoregressively generate the related-work conditioned on the plan plus retrieved abstracts; variants: teacher-forced (ground-truth) plan, prompted plan (model generates plan), learned plan (model generates plan then text), per-cite generation, sentence-by-sentence generation.",
            "model_size": "See individual models: Llama 2: 7B/13B/70B; Llama-3.1:70B; CodeLlama:34B; Falcon:180B; GPT models unspecified",
            "input_corpus_description": "Query abstract plus abstracts of selected retrieved references (top-k chosen from re-ranked list). Evaluations used Multi-XScience extended (full text for many records) and RollingEval-Aug (recent arXiv). Typical examples had ~2 cited papers per case (dataset averages).",
            "input_corpus_size": null,
            "topic_query_description": "Generate a human-quality related-work paragraph for a given query abstract, ensuring cited references are discussed and reducing hallucinations.",
            "distillation_method": "Plan-conditioned RAG: the model receives the query abstract, the abstracts of the selected references, and a sentence plan (either user-provided, teacher-forced from ground truth, or generated) that specifies number of sentences and which citations to mention on which lines; the model then generates the final text. Alternative strategies: per-cite micro-generation followed by aggregation; sentence-by-sentence conditioned on preceding draft.",
            "output_type": "Plan-constrained abstractive related-work paragraphs (narrative synthesis), along with the plans themselves as intermediate structured artifacts.",
            "output_example": "\"Please generate 5 sentences in 60 words. Cite @cite_1 at line 1, Cite @cite_2 at line 2, ...\" → generated 5-sentence paragraph that cites the specified works where indicated.",
            "evaluation_method": "ROUGE-1/2/L comparisons, BERTScore, Llama-3-Eval (G-Eval style), coverage metric (percentage of citations covered in output), human expert ranking and hallucination annotation, and comparison to extractive/finetuned baselines.",
            "evaluation_results": "Teacher-forced plan-based generation outperformed zero-shot baselines: GPT-4 (Plan) gave highest ROUGE/BERTScore and Llama-3-Eval (e.g., ROUGE1 ≈ 37.20 on Multi-XScience). Plan-based prompting reduced hallucinations (e.g., for Llama 2-Chat hallucinations fell from 58.6% to 32.7% in one study; GPT-4 from 29.6% to 11.6% in another) and increased citation coverage (GPT-4 coverage ≈ 98%; Llama 2-Chat 70B coverage rose from 59% to 82% with plan). Learned-plan variants improved over zero-shot but underperformed teacher-forced plans per automatic metrics.",
            "strengths": "Gives users structural control, substantially reduces hallucinations and increases citation coverage, aligns with human preferences in expert ranking, and improves automatic metrics over zero-shot generation.",
            "limitations": "Depends on quality of retrieved references (low retrieval coverage limits end-to-end gains), learned-plan methods are less effective than teacher-forced plans, some models (GPT-3.5) struggle to precisely follow plans, and plan-based prompting does not fully eliminate hallucinations.",
            "failure_cases": "Models sometimes produce fewer sentences than specified in the plan; GPT-3.5 struggles to follow plans precisely; zero-shot Llama variants hallucinate citations (e.g., invented 'XYZ et al.') which plan-based methods reduced but did not always remove entirely.",
            "uuid": "e9672.2",
            "source_info": {
                "paper_title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "SPECTER2 embedding retrieval",
            "name_full": "SPECTER2: large-scale document embeddings for scientific retrieval",
            "brief_description": "An embedding-based retrieval approach using SPECTER2 document embeddings (150M embeddings) for nearest-neighbor retrieval and ranking of candidate scientific papers; used as an alternative and complement to LLM keyword search and prompting-based re-ranking.",
            "citation_title": "Specter: Document-level representation learning using citation-informed transformers",
            "mention_or_use": "use",
            "model_name": "SPECTER2 (contrastive SciBERT-based document embeddings)",
            "model_description": "SPECTER2 provides dense document embeddings trained with contrastive objectives on scientific text (citation-informed). The authors built a FAISS index over the 150M SPECTER2 embeddings and retrieved nearest neighbors by cosine similarity to the query abstract embedding.",
            "model_size": null,
            "input_corpus_description": "Index of ≈150 million document embeddings (SPECTER2) representing scientific articles; FAISS indexes were constructed for nearest-neighbor retrieval.",
            "input_corpus_size": 150000000,
            "topic_query_description": "Retrieve semantically closest documents to a query abstract for downstream re-ranking and literature review generation.",
            "distillation_method": "Embedding-based nearest-neighbor retrieval (FAISS) using SPECTER2 embeddings; retrieved papers optionally re-ranked by LLMs or used directly for generation.",
            "output_type": "List of semantically-similar candidate papers (ranked by embedding cosine similarity) used as retrieval context for subsequent LLM re-ranking/generation.",
            "output_example": "[top-100 nearest neighbor document IDs from SPECTER2 for query abstract X]",
            "evaluation_method": "Compared precision@k and NormalizedRecall@k to keyword-based search and LLM-based re-rankers; embedding-ranker treated as baseline/alternative.",
            "evaluation_results": "SPECTER (embedding) ranking often outperformed prompting-based LLM re-ranking on precision and normalized recall; combining SPECTER embedding retrieval with keyword search improved retrieval performance overall. SPECTER-based search achieved higher precision and normalized recall in many settings.",
            "strengths": "Scalable to very large corpora (150M embeddings), robust semantic retrieval that often beats LLM prompting for ranking, and low run-time cost per query once the index is constructed.",
            "limitations": "Embedding-only retrieval lacks explicit attribution/explanations; embedding indices require substantial storage and pre-processing; coverage still limited if query context (abstract) misses key signals authors used when selecting citations.",
            "failure_cases": "Embedding retrieval can return high-similarity documents that are not judged by authors as cited (authors cite for varied reasons), so high embedding similarity does not guarantee inclusion in ground-truth related work; merging with keyword queries is necessary to improve coverage.",
            "uuid": "e9672.3",
            "source_info": {
                "paper_title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Galactica (prior work)",
            "name_full": "Galactica: a large language model for science",
            "brief_description": "Prior large language model trained on a large scientific corpus designed to store, combine and reason about scientific knowledge; mentioned as an example of LLMs for scientific reasoning, but noted to hallucinate non-existent citations and not be fine-tuned for plan-based writing.",
            "citation_title": "Galactica: A large language model for science",
            "mention_or_use": "mention",
            "model_name": "Galactica",
            "model_description": "A large LLM trained on a scientific corpus (papers, knowledge bases, reference material) that was reported to perform well on scientific tasks; in this paper it is discussed critically for hallucination behavior and lack of instruction fine-tuning for plan-driven literature review generation.",
            "model_size": null,
            "input_corpus_description": "Trained on large-scale scientific texts, reference material and curated knowledge sources (as described in the Galactica paper); here it is cited as prior work demonstrating potential for scientific knowledge synthesis but with limitations.",
            "input_corpus_size": null,
            "topic_query_description": "Mentioned as a model that aims to store/combine/scaffold scientific knowledge across many documents, relevant to the goal of LLMs synthesizing literature.",
            "distillation_method": "Not used in experiments; referenced as a pretrained model for scientific tasks (no plan-based fine-tuning reported in this paper).",
            "output_type": "Mentioned outputs in original Galactica work include scientific Q&A, organized material; in this paper, used as a cautionary example of hallucinated citations.",
            "output_example": null,
            "evaluation_method": "Not evaluated in the present experiments; Galactica's behavior noted qualitatively (hallucinations) in the discussion.",
            "evaluation_results": "Cited prior claims that Galactica outperforms then-existing models on some scientific tasks, but the present paper notes notable hallucination problems (non-existent citations) and lack of plan-following instruction tuning.",
            "strengths": "Demonstrates promise of large LMs as an interface for scientific knowledge (store/combine/reason).",
            "limitations": "Hallucinated non-existent citations and results; not instruction fine-tuned for writing plans, making it ill-suited for grounded literature review generation without additional techniques.",
            "failure_cases": "Generates imaginary prior work and citations (hallucinated references) when used naively for literature review tasks per the paper's discussion.",
            "uuid": "e9672.4",
            "source_info": {
                "paper_title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Emerging literature-review tools (2025 mentions)",
            "name_full": "DeepResearch, AI Co-Scientist, ScholarQA (examples of 2025 AI-assisted research tools)",
            "brief_description": "Named recent systems (DeepResearch, AI Co-Scientist, ScholarQA) cited as examples of tools that improve literature review generation, citation accuracy, and retrieval strategies in 2025; referenced as part of the evolving ecosystem rather than evaluated here.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "DeepResearch; AI Co-Scientist; ScholarQA (unnamed underlying models in this paper)",
            "model_description": "Briefly mentioned 2025 tools said to demonstrate remarkable improvements in literature review generation and retrieval; details on architectures/training are not provided in this paper.",
            "model_size": null,
            "input_corpus_description": "Not specified in this paper; mentioned as contemporary systems operating on scientific corpora and retrieval+generation techniques.",
            "input_corpus_size": null,
            "topic_query_description": "Examples of deployed/advanced systems for literature review assistance and synthesis of scientific knowledge across many papers.",
            "distillation_method": "Not detailed here; cited as examples of improved retrieval and generation systems in the landscape.",
            "output_type": "Not specified in this paper (implied: improved literature reviews, citation accuracy, retrieval pipelines).",
            "output_example": null,
            "evaluation_method": "Not evaluated in this paper.",
            "evaluation_results": null,
            "strengths": "Serves as evidence that the ecosystem is advancing rapidly and that integrated retrieval + generation tools are being developed.",
            "limitations": "No implementation, methods, or empirical results provided in this paper; referenced only as contextual examples.",
            "failure_cases": "Not applicable / not discussed in this paper.",
            "uuid": "e9672.5",
            "source_info": {
                "paper_title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multi-XScience: A Large-Scale Multi-Document Summarization Dataset",
            "rating": 2,
            "sanitized_title": "multixscience_a_largescale_multidocument_summarization_dataset"
        },
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Specter: Document-level representation learning using citation-informed transformers",
            "rating": 2,
            "sanitized_title": "specter_documentlevel_representation_learning_using_citationinformed_transformers"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "ContextCite: Attributing model generation to context",
            "rating": 1,
            "sanitized_title": "contextcite_attributing_model_generation_to_context"
        },
        {
            "paper_title": "Zero-shot listwise document reranking with a large language model",
            "rating": 1,
            "sanitized_title": "zeroshot_listwise_document_reranking_with_a_large_language_model"
        }
    ],
    "cost": 0.023085,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LitLLMs, LLMs for Literature Review: Are we there yet?
21 Mar 2025</p>
<p>Shubham Agarwal 
Gaurav Sahu 
Abhay Puri 
Issam H Laradji 
Jason Stanley 
Laurent Charlin 
Christopher Pal </p>
<p>ServiceNow Research
Mila -Quebec AI Institute
HEC Montreal</p>
<p>ServiceNow Research
University of Waterloo</p>
<p>ServiceNow Research</p>
<p>ServiceNow Research
University of British Columbia Krishnamurthy DJ Dvijotham ServiceNow Research</p>
<p>ServiceNow Research</p>
<p>Mila -Quebec AI Institute
HEC Montreal
Canada</p>
<p>CIFAR AI Chair</p>
<p>ServiceNow Research
Polytechnique Montreal</p>
<p>Mila -Quebec AI Institute
Canada</p>
<p>CIFAR AI Chair</p>
<p>LitLLMs, LLMs for Literature Review: Are we there yet?
21 Mar 2025FFC7BCA796AD48A713BD1836F19F54B0arXiv:2412.15249v2[cs.CL]Generator Module Generated Literature Review Plan Re-ranking Module
Literature reviews are an essential component of scientific research, but they remain timeintensive and challenging to write, especially due to the recent influx of research papers.This paper explores the zero-shot abilities of recent Large Language Models (LLMs) in assisting with the writing of literature reviews based on an abstract.We decompose the task into two components: (1) Retrieving related works given a query abstract and (2) Writing a literature review based on the retrieved results.We analyze how effective LLMs are for both components.For retrieval, we introduce a novel two-step search strategy that first uses an LLM to extract meaningful keywords from the abstract of a paper and then retrieves potentially relevant papers by querying an external knowledge base.Additionally, we study a prompting-based re-ranking mechanism with attribution and show that re-ranking doubles the normalized recall compared to naive search methods while providing insights into the LLM's decision-making process.In the generation phase, we propose a two-step approach that first outlines a plan for the review and then executes steps in the plan to generate the actual review.To evaluate different LLM-based literature review methods, we create test sets from arXiv papers using a protocol designed for rolling use with newly released LLMs to avoid test set contamination in zero-shot evaluations.We release this evaluation protocol to promote additional research and development in this regard.Our empirical results suggest that LLMs show promising potential for writing literature reviews when the task is decomposed into smaller components of retrieval and planning.Particularly, we find that combining keyword-based and document-embedding-based search improves precision and recall during retrieval by 10% and 30%, respectively, compared to using either of the methods in isolation.</p>
<p>Introduction</p>
<p>Writing a literature review-finding, citing, and contextualizing relevant prior work-is a fundamental scientific writing requirement.When writing manuscripts, scientists must situate their proposed ideas within the existing literature.Writing a good literature review is a complex task which can be broken down into two broad sub-tasks: 1) Finding relevant papers and 2) Generating a related work section to discuss the proposed research given prior works.This challenge is further amplified in fields such as machine learning, where thousands of relevant papers appear every month on arXiv alone. 1We explore the utility and potential of large language models (LLMs), in combination with retrieval mechanisms, to assist in generating comprehensive literature reviews for scientific papers.</p>
<p>Specifically, we investigate using LLMs to generate a paper's related work section based on its abstract.We use the term abstract loosely, not necessarily to refer to the actual abstract of the paper but rather to a textual passage that captures a concise summary of the paper's key contributions and scope.Using the abstract as input allows our system to target the central ideas of the paper without requiring the complete manuscript, which is often continuously evolving in the early stages of writing.While our experiments focus on using the abstract, our framework is designed to be flexible.It can use the entire manuscript as it evolves, albeit at a higher computational cost and the need to use models that support longer context windows.This approach provides valuable early-stage insights for authors seeking preliminary references to shape their work, with the capacity to seamlessly incorporate additional information as the manuscript develops.</p>
<p>The architecture of our framework is illustrated in Figure 1, where we further decompose each of the two above tasks into two subtasks.In this work: 1) We introduce an LLM-based approach to retrieve relevant papers, where we first extract the keywords from an abstract or research idea paragraph using an LLM and then feed these keywords to a keyword-based search tool -we experiment with Google search and Semantic Scholar.We optionally also transform the abstract or idea into an embedding and use an embedding-based search procedure.2) We then employ a prompting-based approach to rank the set of retrieved candidate papers based on their relevance to the query abstract, also requiring the LLM to attribute the relevance to specific excerpts in the candidate papers.We explore multiple re-ranking and aggregation strategies.3) To generate the literature review, we select top-k papers from the ranked list and prompt the LLM to generate the related work section based on the query abstract and the abstracts of the selected papers.4) Additionally, we examine the effectiveness of providing a writing plan to the LLM that specifies which papers to cite at various points in the literature review.These plans can be generated entirely by the LLM, by the user, or a combination of the two.These plans serve as an intermediate representation giving the user more control over the organizational structure of the literature review.</p>
<p>The complete framework involves multiple innovations, where we use LLMs in multiple ways, namely for generating search queries, re-ranking search results, and attribution.We summarize the main contributions of our work as follows:</p>
<p>• To answer the key question that our paper poses, we present a data collection protocol and multiple instances of using it to collect arXiv papers.Critically, our protocol is based on using the most recent month of arXiv papers in a rolling manner with the goal of avoiding test-set contamination when evaluating the most recent LLMs for literature review-related tasks.We then use this protocol to perform extensive retrieval and literature review generation experiments.We release both our datasets and our code to the community.</p>
<p>• We propose a novel LLM-based pipeline for the task of interactive literature review writing, which we decompose into two distinct components: retrieval and generation.This also facilitates more controlled Figure 1: A schematic diagram of our framework, where: 1) Relevant prior work is retrieved using keyword and embedding-based search.2) LLMs re-rank results to find the most relevant prior work.3) Based on these papers and the user abstract or idea summary, an LLM generates a literature review, 4) optionally controlled by a sentence plan.</p>
<p>studies investigating alternative LLM-based approaches for these sub-tasks.Our experiments focus on evaluating fully automated variants of these sub-tasks, but our framing of the problem and our proposed solutions are easily integrated into scenarios where human users interact with systems that assist them.To the best of our knowledge, our decomposition of the problem into sub-tasks, along with our proposed solutions to them, and our framing of this assistive scenario is novel.</p>
<p>• We make multiple contributions that improve the retrieval phase of the two-step process.First, we propose and evaluate a novel strategy that first uses an LLM to extract meaningful keywords from the abstract of a paper and then queries different types of external sources to retrieve potentially relevant papers.Second, we compare and combine the aforementioned LLM-generated keyword search techniques with document embedding-based retrieval methods.Third, we propose and examine a wide variety of search re-ranking techniques.Among these, we propose a novel prompting for attribution approach, which we find to empirically improve the relevance of retrieved literature while also improving reliability, providing insights and improving transparency in the decision-making process of LLMs when used to rerank results.Going even further, we examine debate-prone LLMs for aggregating and re-ranking keyword search and embedding search results.Our experiments show that combining these ideas improves precision and normalized recall by 10% and 30%, respectively, compared to standard retrieval methods.</p>
<p>• For text generation, we propose and examine a plan-based retrieval augmented approach to writing literature reviews.By using a plan and conditioning on retrieved context, we provide a user greater control over generated content, and our experiments show that this approach can improve the quality of the generated literature reviews substantially.We evaluate the approach using automated metrics and human assessments and show that our method generates higher-quality reviews as measured by ROUGE scores and as assessed by human evaluation.Our approach also reduces hallucinations by 18-26%.</p>
<p>Related Work</p>
<p>We decompose the literature review task into two key sub-tasks: identifying relevant papers and generating the final related work section.This decomposition enhances the likelihood that the LLMs can effectively accomplish the task.We now discuss the relevant literature pertinent to both aspects of the process.</p>
<p>Ranking and Retrieval</p>
<p>Traditional methods for information retrieval rely on techniques like TF-IDF and BM25 to identify documents that are semantically similar to a given query.More recently, dense vector representations obtained through models like Sentence-BERT (Reimers &amp; Gurevych, 2019) have been shown to improve retrieval accuracy by encoding both query and documents into an embedding space where semantic similarity can be readily computed.The initial retrieval stage often results in a large set of candidate documents, which then need to be re-ranked to obtain an ordered list based on relevance.</p>
<p>Recent efforts have explored the application of proprietary and open-source LLMs for ranking (Sun et al., 2023;Ma et al., 2023;Pradeep et al., 2023a;b), where the LLM is passed a combined list of passages directly as input and prompted to rank them based on a criteria.Notably, only top-k candidates are passed as input to the LLM for re-ranking (Zhang et al., 2023).In our work, we instruct an LLM to output an ordering of the different candidate papers (e.g.
[3] &gt; [8] &gt; [6]
) in descending order based on the relevance to the user-provided abstract.Although existing re-ranking methods improve the ordering of candidate papers, they do not provide explicit justification for the relative rankings assigned.To improve the reliability of the system and offer a clear explanation for the model's choices, our methodology also incorporates attribution capabilities, allowing us to identify specific textual elements contributing to relevance scores for different candidates.Among works exploring the attribution capabilities in LLMs, Yue et al. (2023) focuses on automatically evaluating whether generated statements are fully supported by cited references.Cohen- Wang et al. (2024) present ContextCite, a method for attributing model generation to context, which can be applied on top of any existing language model to help verify generated statements, improve response quality, and detect poisoning attacks.Gradient-based techniques, such as Integrated Gradients (Sundararajan et al., 2017) and Saliency Maps (Simonyan et al., 2014), measure the contribution of each input token by computing gradients of the output with respect to input features; advances like SmoothGrad (Smilkov et al., 2017) and DeepLIFT (Shrikumar et al., 2017) improve these methods by reducing noise and enhancing accuracy.Perturbation techniques involve modifying or occluding parts of the input and observing changes in the output to infer input significance (Li et al., 2016), utilizing methods like Meaningful Perturbations (Fong &amp; Vedaldi, 2017) and LIME (Ribeiro et al., 2016).Despite advancements, challenges such as attribution leakage (Adebayo et al., 2018), unreliability of saliency methods (Kindermans et al., 2019), and complexities in attributing outputs in large models (Ghorbani et al., 2019) persist.Surveys like Li et al. (2023) discuss current methodologies and inherent challenges, while research by Keeling &amp; Street (2024) examines the theoretical basis for attributing confidence to LLMs, raising concerns about the reliability of experimental assessment techniques.In contrast to the discussed gradient-based attribution methods that are challenging to scale and perturbation-based approaches that need multiple passes through the model, we propose a straightforward prompting-based attribution approach that can be applied to any LLM agent, is readily scalable, and does not require multiple passes through the model.</p>
<p>Literature Review Generation</p>
<p>The concept of literature review generation using large language models (LLMs) is built upon the foundation laid by the Multi-XScience dataset proposed by Lu et al. (2020).This dataset paves the way for the challenging task of multi-document summarization, specifically focusing on generating the related work section of a scientific paper.As underlined by Lu et al. (2020), this approach favors abstractive models, which are well suited for the task.However, unlike the approach suggested by Lu et al. (2020), our work introduces the use of intermediate plans to improve the quality of generated literature reviews.The empirical evidence presented in our study shows that our novel strategy outperforms the vanilla zero-shot generation previously championed by the Multi-XScience dataset (Lu et al., 2020).(Note: This paragraph was entirely generated by GPT-4 following plan-based generation.</p>
<p>2 )</p>
<p>Traditional methods for Natural Language Generation have typically employed a rule-based modular pipeline approach comprising of multiple stages of generation with intermediary steps of content planning (selecting content from input while also determining the structure of the output), sentence planning (planning the structure of sentences) and surface realization (surfacing the text in sentence) (Reiter &amp; Dale, 1997;Stent et al., 2004;Walker et al., 2007).Our proposed plan-based prompting technique draws a parallel between the modern methods of end-to-end neural models for joint data-to-text generation with micro or content planning (Gehrmann et al., 2018;Puduppully et al., 2019;Puduppully &amp; Lapata, 2021) where we use plans to define the sentence structure of the generated output.While some recent works have explored planning in terms of devising actions (Yang et al., 2022;Song et al., 2023;Wang et al., 2023), prompting LLMs based on sentence plans have not been explored, to the best of our knowledge.We show two strategies of using plans 1.)The model generates the sentence plan as an intermediary step and conditions on this generated plan to output the final summary autoregressively.2.) Humans can provide a ground-truth plan which results in an iterative setting, inherently providing controllability to the generated text where LLMs are susceptible to generating additional content.</p>
<p>Closely related to our work, Gao et al. (2023) generates answers for questions based on the citations from Wikipedia.Also related to our work, Pilault et al. (2020) examined LLM-based abstractive summarization of scientific papers in the arxiv dataset of Cohan et al. (2018); however, their work was limited to creating the abstract of a single document.Perhaps the most similar prior prompting-based approach to our work is known as 0-shot chain-of-thought prompting (Kojima et al., 2022;Zhou et al., 2022) where a model is prompted with 'Let's think step-by-step' (and similar prompts).</p>
<p>Additionally, Galactica has been developed to store, combine, and reason about scientific knowledge (Taylor et al., 2022).It outperforms existing models on various scientific tasks and sets new state-of-the-art results on downstream tasks.These findings highlight the potential of language models as a new interface for scientific research.However, the Galactica model was not developed to specifically address the problem of literature review assistance and it was not instruction fine-tuned to follow writing plans, and as such it suffered from the effect of hallucinating non-existent citations and results associated with imaginary prior work.</p>
<p>3 Recent works (Rodriguez et al., 2024a;b;Awadalla et al., 2024) have focused on building datasets multimodal of documents and scientific contents.However, our study focuses on exploring the zero-shot abilities of LLMs for literature review generation and proposes a novel strategy that includes generating an intermediate plan before generating the actual text.Our empirical study shows that these intermediate plans improve the quality of generated literature reviews compared to vanilla zero-shot generation.Furthermore, we ensure the validity of our experiments by using a new test corpus consisting of recent arXiv papers to avoid test set contamination. (Note: GPT-3.5 generated this paragraph with the 4th sentence added by the authors).</p>
<p>Retrieval of Related Work</p>
<p>In this section, we discuss the creation of the corpus of arXiv papers to examine different retrieval strategies for finding related works for a given paper abstract using different academic and generic web search engines, including Semantic Scholar and Google Search.</p>
<p>Dataset Construction</p>
<p>We create two datasets that contain papers posted on arXiv in August and December 2023, respectively, starting with 1,000 papers from each month.We use the arXiv wrapper in Python4 to create RollingEval datasets.We then filter out papers for which we were not able to retrieve 100 relevant paper results using LLM summarized keywords.We query the Semantic Scholar API available through the Semantic Scholar Open Data Platform (Lo et al., 2020;Kinney et al., 2023) to search for the relevant papers.To get Google search results, we use SERP API,5 specifically conditioned to leverage the "site:arxiv.org"parameter.This approach ensures the retrieval of search results are sourced solely from arXiv.org.</p>
<p>To combine results from multiple queries, we take the equal number of top results from each query to get a total of 100 papers.We took caution to avoid duplicate results from different queries.In case we are not able to retrieve a sufficient number of results from a query, we then take an equal number from the rest of the queries.This way we ensure that we always retrieve a candidate pool of 100 possible related work for each query paper.We pass these papers as queries to our literature review generation pipeline.We now describe our two-step retrieval mechanism and provide its pseudo-code in Algorithm 1.</p>
<p>Retrieving Candidate Papers</p>
<p>Algorithm 1 Retrieval algorithm Require: Input abstract a 1: keywords = LLMKeywords(a); // Generate keywords from the abstract using an LLM 2: candidate_papers = SearchEngine(keywords); // Query a search engine to retrieve candidates 3: reranked_papers = LLMRerank(candidate_papers, a); // LLM-based reranking of candidates 4: return reranked_papers</p>
<p>To retrieve related work for a given paper abstract, first, for each query abstract in the dataset, we prompt an LLM to generate keywords that we use as queries for a search API (refer to Figure 14 in the Appendix for the detailed prompt used for this task).Importantly, we add a timestamp filter to the search API to retrieve papers published strictly before the publication date of the query paper.In addition to evaluating multiple search engines, we also experiment with generating multiple queries6 from an LLM and various heuristics to combine the search results from each query (see Appendix D).We evaluate multiple general and academic search engines on the quality of the retrieved papers using coverage, which we define as the percentage of ground-truth papers retrieved by the search engine.</p>
<p>Table 1 shows the coverage for different search engines and query heuristics.We note that using multiple queries achieves the highest coverage with comparable results for Semantic Scholar search and SERP API.However, at best, we retrieve just under 7% of the ground truth papers.The low retrieval percentage of just under 7% can be attributed to several factors.First, the task of finding related work for a given paper is inherently challenging due to the diverse styles and methods authors use in literature reviews.This stylistic variability means that a one-size-fits-all approach, such as generating search keywords and using a search engine, might not capture the nuanced criteria that a human expert would apply.Additionally, our retrieval process operates in a constrained setting, generating search keywords based solely on the paper's abstract.In theory, including more of the paper-such as the introduction or methodology-could provide richer context and lead to higher retrieval rates.However, this would not align with our intended use case: supporting researchers in the early stages of drafting when only limited, unpolished material is available.</p>
<p>We also explore an embedding-based strategy for retrieval using SPECTER embeddings (Cohan et al., 2020).SPECTER uses a contrastive learning approach to train a SciBERT (Beltagy et al., 2019)  Table 1: We created two datasets to measure the efficacy of search using LLM-generated keywords -RollingEval-Aug and RollingEval-Dec.We evaluate the % of the ground truth references covered in the top model (Devlin et al., 2018) trained on scientific text.More recently, Singh et al. (2022) further extended the contrastive learning approach of SPECTER to better deal with multiple tasks of relevance to scientific papers, including citation prediction, leading to SPECTER2.Since SPECTER2 provides a large knowledge base of 150M document embeddings of scientific articles, we explore embedding-based retrieval of candidate papers, where we obtain the top-k papers based on the cosine similarity between the query abstract and the corpus of candidate papers in the SPECTER2 dataset.</p>
<p>Re-ranking Candidate Papers</p>
<p>Next, given a list of retrieved papers, we explore re-ranking the list using an LLM.The retrieved abstracts and the original query abstract are used as input to an LLM Re-ranker, which provides a listwise ranking of the candidate papers based on the relevance to the query abstract.We explore different strategies for reranking the candidates, detailed as follows: a) Instructional permutation generation: we use the approach by Sun et al. (2023), which prompts the model to directly generate a permutation of the different candidate papers, thus producing an ordered list of preferences against providing intermediate scores; b) SPECTER2 embeddings: We use SPECTER2 embeddings as an alternative to prompting-based strategies for reranking, where we rank the candidate papers based on their cosine distances to the SPECTER2 embedding of the query abstract (see Appendix D for more details on SPECTER2 implementation); and c) Debate Ranking with Attribution (Ours): our prompting-based approach that builds on the work of Rahaman et al. (2024), where we pass each candidate paper's abstract along with the query abstract and instruct the LLM to (1) generate arguments for and against including the candidate paper and (2) output a final probability of including the candidate based on the arguments.Crucially, we add an attribution step to this ranking module, where we instruct the LLM to extract verbatim sentences from the candidate abstract that support the arguments, and we re-prompt the LLM if the extracted sentences are not present in the candidate abstract.</p>
<p>Retrieval and Re-ranking Experiments</p>
<p>We use an ensemble of search engines to retrieve candidates based on an abstract.We now describe the search engines used in our approach.Based on Table 1, we select the Semantic Scholar (S2) API as the search engine to retrieve search results using LLM-generated keywords.While the SERP API provides access to a broader set of search results, it is expensive, making it less practical for large-scale retrieval.Additionally, the S2 API is specifically designed for academic literature, offering strong performance comparable to the SERP API.As discussed in Section 3.2, we also explore SPECTER2 embeddings for retrieval and compare it with the different prompting-based strategies.</p>
<p>We experiment with different combinations of search engine and the retriever method, and present our results in Figure 2. We present precision and normalized recall at different values of top-k recommendations, where we calculate normalized recall as the proportion of the number of ground truth papers retrieved (instead of all ground truth papers).Formally, normalized recall and precision follow the definitions:
Normalized Recall@k = |Retrieved@k ∩ Ground Truth| |Retrieved ∩ Ground Truth| ; Precision@k = |Retrieved@k ∩ Ground Truth| k (1)
where "Retrieved" denotes the set of all candidate papers, "Retrieved@k" denotes the set of top k candidate papers, and "Ground Truth" denotes the set of papers cited by the query paper.Unlike standard recall, which measures how many ground truth citations are retrieved, Normalized Recall@k measures how effectively the retrieval method prioritizes the most relevant papers in the top-k.By normalizing over the total relevant papers retrieved at any rank, this metric helps evaluate ranking quality independently of retrieval coverage.We include a working example in Appendix D.1 to further demonstrate the difference between normalized recall and standard recall.</p>
<p>From Figure 2, we note that debate ranking significantly outperforms permutation ranking, as denoted by the higher precision and normalized recall at smaller values of top-k recommendations; however, SPECTER and set k = 100 for these experiments.We evaluate the Precision and Normalized Recall of the re-ranked results with embedding-based ranker (SPECTER2) outperforming GPT-4 based re-ranking.We find a similar pattern for the RollingEval-Aug dataset, as shown in Appendix (Figure 7).Note: The first part in the legend denotes the search database for retrieval, and the second denotes the re-ranking mechanism.</p>
<p>embeddings outperform both prompting-based strategies.Next, the higher precision and normalized recall values for SPECTER and S2+SPECTER settings suggest that SPECTER is also an excellent search engine.</p>
<p>In Table 2, we examine the behaviour of the GPT-4 reranking approach in more detail.Using GPT-4 as a reranker in the manner discussed above produces an incomplete list 41% of the time.In 3% cases, GPT-4 produces a list with repeated values and, in some rare cases, with garbage values or numbers (e.g.2020).We conclude that this strategy of using GPT-4 for reranking is brittle.</p>
<p>Positive Effect of Attribution Verification</p>
<p>We conduct an ablation study on the first 100 papers of the larger set of 500, and focus on the top k = 40 papers, which is representative of the typical number of papers cited in the Machine Learning community.In Figure 3 we show the result of removing the verification step in our debate re-ranking strategy, i.e. in this ablation, we do not check if the sentences extracted by the LLM are indeed present in the candidate paper abstract.We find that removing this kind of attribution verification leads to a drop in the precision and normalized recalls, especially for lower k values.We perform a t-test to test the significance of the drop and Figure 3: The effect of removing the referenced content verification step in our debate ranking strategy.We plot precision and normalized recall for two variants of the debate ranking strategy.For this ablation study, we select a smaller subset of n = 100 query abstracts, set k = 40, and repeat the experiment for three random seeds.We plot the mean and show the standard deviation as the shaded region.We find that the precision and normalized recall drop slightly upon removing the verification step.This difference is significant (as determined by the t-test,) indicating that the verification step is crucial for the success of the debate ranking strategy.</p>
<p>find that for both precision and normalized recall, the drop is significant, with p-values of 4.7 × 10 −4 and 1.9 × 10 −6 for precision and normalized recall curves, respectively.This indicates that proper attribution also allows the LLM to provide a more accurate ranking of candidates.2023) explore LLMs like GPT-4 for 0-shot rank predictions, we find a tendency of GPT-4 to produce an incomplete list or repeated values in the re-ranked order list with some rare cases of garbage values.</p>
<p>Literature Review Generation</p>
<p>Plan Based Generation Approach &amp; Model Variants.We now focus on generating the related work section of a scientific document from a user-supplied list of papers.In a real-world scenario, this list might be obtained through traditional means, from the above-mentioned automated methods, or some combination.We evaluate several dimensions of writing quality in the generated text.Importantly, while modern LLMs can yield seemingly well-written text passages, "hallucinations" remain a problem and can be particularly egregious when LLMs are used in scientific writing (Athaluri et al., 2023).The hallucination of statements not entailed by the contents of cited papers and the hallucinations of imaginary papers that do not exist is a well-known issue of even the most powerful LLMs.We use ideas from retrieval augmented generation (RAG) techniques (Lewis et al., 2020) and instruction prompting to address the key problem of hallucinations.Our work also aims to increase the number of papers from the desired set that are indeed discussed (the coverage).</p>
<p>We present our general framework and problem setup in Figure 4. We use the abstract of a query paperthe one for which we generate a literature review, along with the abstracts of the set of papers to be cited (the retrieved abstracts of reference papers) to generate the related work section of the query paper.For evaluation, our approach relies on prompting LLMs in different ways and measuring the similarity of the generated literature review text to ground truth literature reviews found within a corpus of recent scientific papers -i.e.ones not used in the training set of the underlying LLMs.We use both automated methods and human evaluations in our experiments below.</p>
<p>We propose to further decompose the writing task to increase passage quality, factual accuracy, and coverage.We examine different strategies for generating a writing plan, a line-by-line description including citations of the passage to write.These writing plans also give authors (users) control over the output passages.This is likely essential in practice to meet author preferences and possible publication constraints.These plans are defined and generated in such a way that the user can interact with and edit them if desired, or they can be used as an automatically generated intermediate (but human-understandable) representation.We now describe our proposed methods, their use in practice and their evaluation in more detail.In what we refer to as plan-based generation (Plan), a model is prompted with a known (user-provided) plan to produce X sentences in Y words and cite references on specific lines.These plans are obtained from the ground truth data and serve as a proxy for a user who desires text to be written with specific constraints.This kind of Plan strongly guides generated text to the user's desires (as observed by the final ground truth text).During evaluation, this might be considered a form of teacher-forcing at the structural level.An example of the format of these plans is provided below:</p>
<p>Please generate {num_sentences} sentences in {num_words} words.Cite {cite_x} at line {line_x}.Cite {cite_y} at line {line_y}.</p>
<p>Prompted plan.The previous method replicates the scenario of a detailed user-provided plan.However, one can also generate such plans automatically from an LLM.The model is prompted first to generate a plan of sentences and citations, which it would then condition upon to generate the final related work text.When used as an interactive tool, we envision the user might start with a suggested plan, see the corresponding generated full literature review text, and then iteratively edit the plan and regenerate the result.See our Appendix (Figures 9, 10 and 11) for the differences in the prompts used in our experiments.</p>
<p>We also experiment with two other strategies in which researchers could prompt the model:</p>
<p>Per cite.We first use a two-stage strategy to generate content relevant to each cited paper.In the first stage, the model is prompted to generate related works in 1-2 lines for each individual reference citation.All the outputs for different citations are combined together to form the generated related work.In the second stage, the LLM summarizes and paraphrases the output of the first stage.</p>
<p>Sentence by sentence Based on the Ground truth (GT) related work and the citation on each line, we prompt the model to generate one sentence conditioned on the abstract, the reference cited in that line, and the generated draft so far.In the absence of a citation or at the start, the model is prompted only with the abstract and draft of the generated work till now.</p>
<p>Generation Experiments.</p>
<p>For the following studies on generating related work, we introduce an additional corpus.We extend the Multi-XScience corpus (Lu et al., 2020) to include the full text of research papers.We also reuse the RollingEval-Aug introduced in Section 3. We use HuggingFace Transformers (Wolf et al., 2019) and PyTorch (Paszke et al., 2017) for our experiments7 and calculate ROUGE scores (Lin, 2004) using the Huggingface's evaluate library.</p>
<p>Details on the dataset and the implementation are in Appendix B and D, respectively.Similar to Lu et al. (2020), we extract the ground truth cited references as the relevant papers and evaluate only the generated outputs from different systems.Since ROUGE score only measures token-level similarity and does not account for semantic meaning, we also report BERTScore and Llama-3-Eval in Table 4.We use BERTScore, an embedding-based evaluation metric, to account for the semantic meaning of two texts during evaluation as well.On the other hand, we use Llama-3-Eval, an open-source variant of the widely used G-Eval metric (Liu et al., 2023b), as G-Eval has been shown to correlate better with human preferences.</p>
<p>Generation Baselines</p>
<p>Extractive baselines As in Lu et al. (2020), we report the performance of LexRank (Erkan &amp; Radev, 2004) and TextRank (Mihalcea &amp; Tarau, 2004).We also create a simple one-line extractive baseline which extracts the first line of the abstract and combines all the citations to form the output.</p>
<p>Abstractive finetuned baselines</p>
<p>We use the model outputs of Hiersum (Liu &amp; Lapata, 2019) and Pointer-Generator (See et al., 2017) from Lu et al. (2020) for abstractive finetuned baselines.We also reproduce the finetuned PRIMER (Xiao et al., 2021) model (considered to be the SOTA).</p>
<p>Abstractive zero-shot baselines</p>
<p>We use the zero-shot single-document abstractive summarizers FlanT5 (Chung et al., 2022) and LongT5 (Guo et al., 2022) based on the T5 architecture (Raffel et al., 2020).Since Galactica (Taylor et al., 2022) is trained on documents from a similar domain, we include it along with Falcon-180B (Almazrouei et al., 2023).For closed-source models, we evaluate zero-shot both GPT-3.5-turbo(Brown et al., 2020) and GPT-4 (OpenAI, 2023).Since they perform best in our initial evaluations, we use the closed-source models in combination with the different generation strategies (Per-cite, Sentence by sentence, plan-based, and learned plan) from Section 4.</p>
<p>Open and closed source models</p>
<p>Results and Observations</p>
<p>From Table 3, we first note that unsupervised extractive models provide a strong baseline compared to abstractive 0-shot single document summarization baselines.Fine-tuning these abstractive models on Multi-XScience (released initially with the benchmark) improves performance at least to the level of extractive models.We reproduce the PRIMER model using their open-source code but find lower-than-reported results.</p>
<p>As such, we consider the Pointer-Generator method to be the current state-of-the-art (SOTA).</p>
<p>Single-document summarizers (LongT5, Flan T5) perform poorly in the zero-shot settings with limited ability to cite references.We are limited in the prompt we can provide (because of the training prompts) and resort to "Summarize the text and cite sources."Galactica's performance is encouraging compared to other models in the same group, but inspecting its output reveals that it generates the whole introduction of the paper instead of the related work.The model is very sensitive to the prompts used (mostly as suffixes) and struggles to follow instructions.Falcon 180-B, on the other hand, tends to hallucinate user turns and considers this task as multiple turns of user-system exchange, even though we prompted to generate relevant outputs.</p>
<p>All recent versions (7B, 13B, 70B) of zero-shot Llama 2 models underperform the supervised Pointer-Generator baseline (except for 70B on ROUGE2) and their GPT counterparts.All Llama 2 models tend to produce output in bullet points and also provide references.We find that closed-sourced models like GPT-3.5-turbo and GPT-4 achieve SOTA in the zero-shot setting.However, the proposed sentence-by-sentence and per-citation strategies deteriorate the performance of GPT models which tends to cover all related concepts hierarchically.</p>
<p>9</p>
<p>Our teacher-forced plan-based framework improves the scores over the 0-shot baseline for both closed-sourced (GPT-3.5 and GPT-4) and open-sourced LLMs, with Llama 2 70B achieving similar scores as GPT-3.5 on both the original Multi-XScience and the new RollingEval-Aug dataset (in Table 4).In Table 4, we also notice that BERTScore and Llama-3-Eval exhibit the same trends as ROUGE scores except in the case where GPT-3.5-turbo(Plan) obtains a higher Llama-3-Eval score than Llama 2-Chat 70B (Plan).These values also showcase the weaker discerning power of BERTScore compared to Llama-3-Eval as all the models achieve a high BERTScore between 82-85%.Llama 2 70B gets more uplift with the plan compared to GPT models where manual inspection reveals fewer hallucinations in the outputs (see qualitative results in Table 13 in Appendix using our abstract).In Table 5, we evaluate the controllability of LLM-based generation using sentence plans and find that GPT-4 tends to follow the plan more closely.It follows the exact plan 60% of the time, often producing fewer sentences than provided.Llama 2 70B comes in second place in following the plan instructions and GPT-3.5 struggles to follow the plan precisely.We also experiment with a learned plan strategy where the model first generates a plan and then autoregressively generates the output.Though it Table 5: We show % of responses with the same number of lines as the plan for both datasets.Here we also show the mean and max difference in lines generated by the model vs. the original plan.-ive implies that a model generated fewer lines than the plan.We find GPT-4 to follow the plan more closely compared to Llama 2 and GPT-3.5. Figure 5: Human evaluation study where annotators ranked the generations of 0-shot models with their sentence-plan-based counterparts.On the Y-axis, we show counts from an overall sample size of 58 annotations for Llama 2-Chat and 54 for GPT-4 (where ranking ties are allowed).We see a reduction of 58.6% cases of hallucinations to 32.7% for Llama 2-Chat and 29.6% to 11.6% for GPT-4 using plan-based prompting.</p>
<p>improves the results over 0-shot baseline, it does not outperform the teacher-forced plan generation in terms of automatic metrics.There is a considerable drop in performance on RollingEval-Aug dataset compared to the original Multi-XScience in terms of ROUGE1/2.It gives more credibility to the hypothesis that the Multi-XScience test set is in the training data of these LLMs and/or that there is a shift in the distribution of these more recent papers.Nevertheless, we find similar scores and ranking patterns between models as for Multi-XScience.We provide other experiments related to fine-tuning, longer context, and CodeLLM in Appendix C, and we provide cost estimates for different methods in Appendix D.5.</p>
<p>We also run a human evaluation study using 6 expert annotators.They rank model generated outputs for 160 papers with 3 citations10 where we show the abstract of the query paper, cited references, and the model outputs of 0-shot vs plan counterparts side-by-side.We randomly selected 80 examples each for GPT-4 and Llama 2-Chat comparisons.Experts had the flexibility to choose rank 1 for one or both the models, otherwise they could select rank 2 for each.We also ask the annotators to identify hallucinations, i.e. which model generated content not from the abstracts.The interface is described further and shown in the Appendix (Figure 8).Out of 160, we find agreement among the annotators for 112 examples (54 for GPT4 and 58 for Llama 2-Chat).The results in Figure 5 show that humans rank generated response to be significantly better
11
for Llama 2-Chat Plan based model where it was ranked at the top 32 times compared to 18 times for 0-shot.We can also observe a similar trend for GPT-4, where annotators rank GPT-4 Plan-based output 37 times at Rank 1 compared to 30 for GPT-4 0-shot.In terms of hallucinations, we find significant reductions in the hallucination for plan-based Llama 2-Chat models from 34 to 19 instances, where the 0-shot model often provided a made-up citation (XYZ et al.), possibly from background knowledge.Only 6 cases of hallucination were found for GPT-4 plan compared to 16 instances for 0-shot vanilla GPT-4 model.</p>
<p>Conclusions &amp; Answering: Are</p>
<p>We There Yet?</p>
<p>This work discusses, establishes and evaluates a pipeline to help people write literature reviews.We first identified some challenges of evaluating such systems when LLMs are constantly updated based on training on new data, which may contain recent papers found online.To address these issues, we propose and implement a rolling evaluation procedure that focuses on recent arXiv papers, and we collect several evaluation datasets in this manner.</p>
<p>Our experiments show that LLMs have significant potential for writing literature reviews, especially when the task is decomposed into these smaller and simpler sub-tasks that are within reach of LLMs, namely through the use of LLM-generated keyword search and embedding-based search for relevant prior work.Notably, our experiments indicate that both debate prompting and debate arguments that use attribution based on citing extracted content from source material improve LLM re-ranking results.The most powerful LLMs evaluated in our studies exhibit extremely promising paper re-ranking abilities as well as promising literature review generation results.Importantly, LLM hallucinations can be substantially reduced using our proposed plan-based prompting and retrieval augmented generation techniques.</p>
<p>Our evaluation also reveals clear challenges: 1) retrieving all relevant papers consistent with a given humangenerated literature review will require new querying strategies; 2) hallucinations can be significantly reduced using plan-based prompting, but our approach does not completely eliminate hallucinations.</p>
<p>So, are we there yet?Not quite-but we are getting closer.The landscape of AI-assisted research exploded in 2025, with tools like DeepResearch (OpenAI, 2025), AI Co-Scientist (Gottweis et al., 2025), and Schol-arQA (AllenAI, 2025) demonstrating remarkable improvements in literature review generation, citation accuracy, and retrieval strategies.Moreover, to accompany our scientific work here, and our older work on this theme (Agarwal et al., 2024), we have build a full working demo based on our proposed retrieval and generation pipeline (see Figure 16 in the Appendix for a screenshot), which we will release to the community.We hope that authors can use this demonstration system to better understand how these techniques-and future alternatives-can be most helpful for assistance in literature review generation.</p>
<p>Limitations and Future Work.Because of the low coverage for retrieval, we evaluate different components independently.During generation, this strategy assumes that we already have filtered relevant papers corresponding to the main paper.In the future, we would like to improve the search for relevant work using embedding-based models to get better coverage and, thus, the ability to evaluate the system end-to-end.Our retrieval component currently suffers from surface-level information about the papers, while in practice, authors frame search keywords based on information (such as underlying datasets) that might not be present in the abstract.Another issue stems from the retrieval evaluation setup based on coverage related to the ground truth papers, where the authors might have different biases.We also acknowledge that including more of the paper, like introduction and methodology, might help improve the set of initial candidate papers; however, using additional sections could inadvertently allow the model to detect explicit citations or references, essentially "cheating" by using these as hints to retrieve specific papers.By focusing on the abstract alone, we maintain a more controlled setup that reflects a realistic, early-stage research scenario.It is important to highlight that while we operate in this abstract-only setup, our pipeline is designed to be flexible and interactive.As the paper matures and more content becomes available, researchers can provide additional context to the pipeline to improve retrieval accuracy.This adaptability ensures that our approach remains relevant and effective throughout different stages of the research process, allowing for incremental refinement of related work as drafts evolve.</p>
<p>(∼215G compressed json) where research documents are linked with arXiv and Microsoft Academic Graph (MAG) (Sinha et al., 2015) IDs, when available.This corpus provides full text of the research papers (parsed using a complex pipeline consisting of multiple LaTeX and PDF parsers such as GROBID (Lopez, 2023) and in-house parsers. 16).The full text is also aligned with annotation spans (character level on the full text), which identify sections, paragraphs, and other useful information.It also includes spans for citation mentions and the matching semantic corpus-based ID for bibliographical entries, making it easier to align with references compared to other academic datasets such as LoRaLay (Nguyen et al., 2023), UnarXive (Saier &amp; Färber, 2020;Saier et al., 2023), etc. or relying on citation graphs like OpenAlex (Priem et al., 2022), next-generation PDF parsers (Blecher et al., 2023) or other HTML webpages.17For the Multi-XScience, we obtain the full text of papers for 85% of records from the S2ORC data using the span annotations from the corpus aligned with citation information.</p>
<p>RollingEval datasets</p>
<p>Llama 2 was publicly released on 18th July 2023 and GPT-4 on 14 March 2023.</p>
<p>Both provide limited information about their training corpus, and academic texts in the Multi-XScience may or may not have been part of their training data.To avoid overlap with the training data of these LLMs, we process a new dataset using papers posted after their release date.To do so, we first filter the papers published in August 2023 from S2ORC that contain an arXiv ID, resulting in ∼15k papers.S2ORC does not provide the publication date of the papers directly, so we use regex '2308' on the arXiv ID to extract papers posted in 08'23.We then use section annotations to get the section names and match using synonyms ('Related Work, Literature Review, Background') to extract section spans.We take the rest of the text as conditioning context except the related work section which results in ∼4.7k documents.Using the citation annotations, we extract the full text of cited papers from the S2ORC corpus again using corpus ID.Similar to Multi-XScience, we use paragraph annotations to create a dataset for the latest papers (∼6.2k rows).We create a subset of 1,000 examples (RollingEval-Aug) where we have the content of all the cited papers.The average length of a related work summary is 95 words, while the average length of abstracts is 195.On average, we have 2 citations per example, which makes the dataset comparable to the original Multi-XScience dataset.</p>
<p>C Other Generation Experiments</p>
<p>Llama 2 fine-tuning In parallel, we also fine-tune Llama 2 models on the train set with the original shorter context, but they are very sensitive to hyperparameter configuration.When we instruct-finetune Llama 2 7B, it initially produces code.We find a slight improvement when fine-tuning the Llama 2 7B model for 30k steps with an LR of 5e-6 over 0-shot model (see Table 8), but it quickly overfits as we increase the LR or the number of steps.We leave hyperparameter optimization, fine-tuning larger models with RoPE scaling and plan-based generation for future work.(Kadous, 2023) than GPT-3.5 or GPT-4 (2048 and 4096 tokens respectively), implying that the effective number of words in Llama 2 is considerably lower than GPT-4 and only a bit higher than GPT-3.5.We experiment with the popular RoPE scaling (Su et al., 2021) in 0-shot Llama models to increase the context length (4k-6k).This permits using the full text of the papers instead of just their abstracts.Results in Table 9 show that directly using RoPE scaling on 0-shot models produces gibberish results.Instead, one needs This example illustrates how Normalized Recall@k differs from standard recall.Instead of being limited by the total number of ground truth citations, it evaluates how well the method ranks the retrievable relevant papers.In this case, despite a low precision, the normalized recall is relatively high, indicating that the method effectively ranks the relevant papers it does retrieve.
Model ROUGE1 ↑ ROUGE2 ↑ ROUGEL ↑ Llama 2-Chat 7B -0-</p>
<p>D.2 Generation Implementation</p>
<p>We use HuggingFace Transformers and PyTorch (Paszke et al., 2017) for our experiments. 19We calculate ROUGE scores (Lin, 2004) using the Huggingface (Wolf et al., 2019) evaluate library20 .To split sentences, we use 'en_core_web_sm' model from SpaCy21 .Additionally, we use Anyscale endpoints22 to generate 0-shot Llama 2 results and OpenAI API23 to generate results for GPT-3.5-turbo and GPT-4.</p>
<p>D.3 Demo implementation</p>
<p>We build our system using the ReactJS framework, which provides a nice interface to build system demos quickly and efficiently.More details about the demo implementation can be found in our system paper Agarwal et al. (2024) and the project page.</p>
<p>We query the Semantic Scholar API available to search for the relevant papers.Specifically, we use the Academic Graph24 and Recommendations25 API endpoint.We use OpenAI API to generate results for LLM using GPT-3.5-turboand GPT-4 models.At the same time, our modular pipeline allows using any LLM (proprietary or open-sourced) for different components.We also allow the end-user to sort the retrieved papers by relevance (default S2 results), citation count, or year.More details about the demo system can be found in our system paper.</p>
<p>D.4 SPECTER Implementation</p>
<p>We build an index of 150M SPECTER2 embeddings that we can use as an alternative to both a search engine and a prompting-based ranking module.Figure 6 shows our pipeline for creating the index.Specifically, the SPECTER2 database comes with 908 json.gzfiles containing compressed embeddings.For each json.gzfile, we construct a FAISS index that we can query for the nearest neighbors of a given query embedding.</p>
<p>We perform index construction in a multi-threaded manner to speed up the process.Upon constructing a FAISS index for all the json.gzfiles, we iterate over each query paper, search for the top 100 relevant papers using the SPECTER embeddings in each FAISS index, and then finally merge the results to get the top 1000 papers for each query paper.</p>
<p>D.5 Comparative analysis of the computational costs</p>
<p>We compare the costs of different LLMs for both stages in Table 11.</p>
<p>Ranking:</p>
<p>We explore two types of LLM-based reranking mechanisms: permutation and debate ranking.</p>
<p>For n query papers (=500 for our RollingEval datasets) and top-k candidates retrieved from S2 per query paper (k=100 in our experiments), permutation ranking would require n API calls, whereas debate ranking would require n * k API calls.Debate ranking needs more API calls as it involves one additional API call per candidate paper to generate the citation probability score and reasoning.Therefore, there are k additional API calls per query paper compared to permutation ranking, where we prompt the LLM to directly rank</p>
<p>GPT-4 0-shot</p>
<p>The field of multi-document summarization has been a challenging one due to the lack of large-scale datasets (@cite_1).One significant attempt to address this gap is the Multi-XScience dataset, a large-scale collection of scientific articles designed to simulate the task of writing the related-work section of a research paper (@cite_1).This approach was inspired by extreme summarization, a methodology that promotes abstractive modeling approaches.However, the current study offers a novel strategy for literature review generation using large language models (LLMs), which goes beyond the scope of the Multi-XScience dataset.The proposed method involves the creation of an intermediate plan, which is then used to generate the actual text.Empirical evidence suggests that this approach improves the quality of the generated literature reviews compared to the conventional zero-shot generation.Additionally, this work expands the Multi-XScience corpus to include the full text of research papers, further enhancing the depth and breadth of information available for literature review generation.This study also ensures the validity of zero-shot experiments by creating a new test corpus from recent arXiv papers, thus avoiding test set contamination.This reflects a more comprehensive and rigorous approach to the generation of literature reviews using LLMs.</p>
<p>GPT-3.5 Plan</p>
<p>The task of generating literature reviews using large language models (LLMs) has gained attention recently.One approach is to condition the LLM on the abstracts of the papers to be cited and generate the literature review directly.However, the authors propose a novel strategy where they first generate a plan for the review and then use it to generate the actual text.This intermediate planning step is found to improve the quality of the generated literature reviews compared to vanilla zero-shot generation.The authors extend the Multi-XScience corpus, previously containing only abstracts, to include the full text of research papers.Additionally, they create a new test corpus consisting of recent arXiv papers, ensuring that their zero-shot experiments are not affected by contamination from the test set.The Multi-XScience dataset, used in their work, is particularly suitable for abstractive models due to its challenging multi-document summarization task (@cite_1).The empirical results from several state-of-the-art models trained on this dataset demonstrate its effectiveness for abstractive modeling approaches.GPT-3.5 0-shot Prior work in the field of generating literature reviews has primarily focused on the use of large language models (LLMs) and multi-document summarization.Multi-document summarization is a challenging task due to the lack of large-scale datasets (@cite_1).The Multi-XScience dataset was proposed as a solution to this problem, specifically designed for generating the related-work section of a paper based on its abstract and the articles it references.This dataset introduced a challenging multidocument summarization task that favors abstractive modeling approaches (@cite_1).</p>
<p>Our work builds upon these prior efforts by exploring the zero-shot abilities of recent LLMs for the task of generating literature reviews.We propose a novel strategy that involves generating an intermediate plan for the review, which is then used to generate the actual text.While modern LLMs can generate literature reviews without such intermediate plans, our empirical study shows that the use of these plans improves the quality of the generated reviews compared to vanilla zero-shot generation.</p>
<p>In addition to extending the Multi-XScience corpus to include the full text of research papers, we also create a new test corpus consisting of recent arXiv papers.This allows us to ensure that our zero-shot experiments are not affected by test set contamination.</p>
<p>The prior work on multi-document summarization and the Multi-XScience dataset provides a valuable foundation for our study.However, they do not specifically address the use of intermediate plans for literature review generation, which is a key contribution of our work.By incorporating these intermediate plans, we aim to enhance the quality and coherence of the generated literature reviews.</p>
<p>Llama 2 Plan</p>
<p>The task of generating literature reviews for scientific research papers has gained significant attention in recent years, with a focus on using large language models (LLMs) for zero-shot generation.Inspired by extreme summarization, Multi-XScience was introduced as a large-scale dataset for multi-document summarization, including the challenging task of writing related-work sections based on abstracts and referenced articles.Our work builds upon this foundation by exploring the effectiveness of intermediate plans for literature review generation.Our empirical study shows that these plans improve the quality of generated literature reviews, and we contribute to the extension of the Multi-XScience corpus to include full-text papers.Additionally, we create a new test corpus to ensure zero-shot experiments are free from test set contamination. (@cite_1) Llama 2 0-shot In recent years, there has been growing interest in using large language models (LLMs) for zero-shot literature review generation.However, most studies have relied on training or prompting LLMs to condition on all abstracts of papers to be cited, which can be limiting in terms of quality and efficiency.In contrast, our proposed approach involves generating a plan for the literature review and then using it to generate the actual text.We evaluate the effectiveness of this approach using a novel extended version of the Multi-XScience corpus, which now includes the full text of research papers.Our results show that intermediate plans improve the quality of generated literature reviews over vanilla zero-shot generation.One of the key challenges in multi-document summarization is the lack of large-scale datasets.To address this, we have created a new test corpus consisting of recent arXiv papers with full content, ensuring that our zero-shot experiments are not contaminated by the release of the LLMs used in our study.Our work builds on the concept of extreme summarization, which favors abstractive modeling approaches.We have employed several state-of-the-art models trained on the Multi-XScience dataset to evaluate the effectiveness of our approach.The results show that our dataset is well-suited for abstractive models, providing a valuable resource for researchers working in this area.In summary, our contribution includes a novel approach to literature review generation using LLMs, an extended version of the Multi-XScience corpus, and a new test corpus.Our results demonstrate the effectiveness of our approach and highlight the potential of using LLMs for zero-shot literature review generation.References: * @cite_1: Multi-XScience: A Large-Scale Multi-Document Summarization Dataset.* @cite_2: Extreme Summarization: A Dataset Construction Protocol for Abstractive Modeling.</p>
<p>Table 13: We show outputs of different models corresponding to the same plan and prompt as paragraph of our related work (by GPT-4 Plan).Llama 2 70B 0-shot has the tendency to hallucinate citation (in red) which is remedied in plan based generation.Abstract of reference paper shown in Table 12 while the main abstract is a slight modified version of our current abstract.We also find that the 0-shot results are more verbose, copying the exact text verbatim, while the generated literature review using plans are more representative of the way humans write literature review.</p>
<p>GPT-4 Plan</p>
<p>The concept of generating the related-work section from abstracts and referenced articles is not entirely new, as evidenced by the Multi-XScience dataset proposed by @cite_1.The approach of extending abstractive summarization to lengthy documents via an extractive step for conditioning, as described by @cite_2, aligns with the proposed methodology here.However, the strategy introduced in this study differentiates itself by creating an iterative plan prior to the generation of the review, a step not present in the work of @cite_1.The LLM used in this study also builds upon the capabilities of models like Galactica @cite_3, which have demonstrated exceptional performance on scientific tasks.The results of @cite_2 and @cite_3 support the potential of large language models in scientific literature review generation, while the proposed approach aims to further enhance the quality of these outputs through structured planning.</p>
<p>GPT-4 0-shot</p>
<p>The literature review generation task, a form of multi-document summarization, has been tackled through different strategies.@cite_1 proposed a dataset, MultiXScience, to foster abstractive approaches, while @cite_2 used an extractive step to enhance neural summarization.Our work complements these by introducing intermediate plans to improve generation quality.Meanwhile, @cite_3's Galactica demonstrated potential for language models in scientific tasks, a direction we explore with zero-shot experiments.</p>
<p>GPT-3.5 Plan</p>
<p>The task of generating a literature review using large language models (LLMs) has gained attention recently.One approach is to condition the LLM on the abstract and content of related papers to generate the review (@cite_1).However, it has been observed that generating an intermediate plan for the review improves its quality compared to vanilla zero-shot generation.This motivated the proposal and examination of a novel strategy where a plan is first generated and then used to generate the actual text (@cite_1).Another method to improve abstractive summarization is by performing an extractive step before generating the summary.This step conditions the transformer language model on relevant information, leading to better summarization results compared to prior work that employs a copy mechanism (@cite_2).Additionally, the use of large language models like Galactica has shown promising results in organizing and reasoning about scientific knowledge (@cite_3).Galactica outperforms existing models on technical knowledge probes and downstream tasks, demonstrating the potential for language models as a new interface for science (@cite_3).GPT-3.5 0-shot Prior work has explored the use of large language models (LLMs) for various natural language processing tasks.Multi-document summarization is one such task, and the Multi-XScience dataset (@cite_1) has been created specifically for this purpose.This dataset focuses on writing the related-work section of a scientific paper based on its abstract and the articles it references.</p>
<p>The dataset construction protocol favors abstractive modeling approaches, making it suitable for LLMs.Another approach to abstractive summarization is presented in a paper (@cite_2), where the authors propose a method to generate summaries of long documents.They employ an extractive step to gather relevant information before conditioning the transformer language model for summary generation.This approach yields more abstractive summaries compared to prior work, while still achieving higher rouge scores.Furthermore, the concept of using LLMs for scientific knowledge organization and reasoning is introduced in the Galactica model (@cite_3).This model is trained on a large scientific corpus and outperforms existing models on various scientific tasks.It demonstrates superior performance on technical knowledge probes and reasoning tasks, surpassing GPT-3 and other models.Galactica also achieves state-of-the-art results on downstream tasks such as PubMedQA and MedMCQA, showcasing the potential of LLMs as a new interface for science.Based on these prior works, we propose a novel strategy for literature review generation using LLMs.We first generate a plan for the review and then use it to generate the actual text.While modern LLMs can generate literature reviews without intermediate plans, our empirical study indicates that these plans improve the quality of generated reviews.Additionally, we create a new test corpus consisting of recent arXiv papers, ensuring that our zero-shot experiments are not affected by test set contamination.</p>
<p>Llama 2 Plan</p>
<p>In recent years, there has been a growing interest in using large language models (LLMs) for scientific literature review generation (@cite_1).One approach is to use a novel strategy that involves generating a plan for the review and then using it to generate the actual text (@cite_1).This approach has been shown to improve the quality of generated literature reviews over vanilla zero-shot generation (@cite_1).Another approach is to use a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information (@cite_2).This approach has been shown to produce more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores (@cite_2).Additionally, there have been efforts to train LLMs on large scientific corpora to improve their ability to store, combine, and reason about scientific knowledge (@cite_3).For example, Galactica, a large language model, has been trained on a range of scientific tasks and has outperformed existing models on technical knowledge probes, reasoning, and downstream tasks (@cite_3).These advancements in LLMs have the potential to revolutionize the way we interact with scientific literature and accelerate scientific progress.Llama 2 0-shot In this section, we discuss the related work in the field of scientific literature review generation using large language models (LLMs).Recent works have explored the use of LLMs for multi-document summarization tasks, such as generating the related work section of a paper based on its abstract and the articles it references (@cite_1).These works have shown that LLMs can produce high-quality summaries, but may suffer from the lack of relevant information and context (@cite_2).To address this issue, we propose a novel strategy for literature review generation that involves generating a plan for the review before generating the actual text.This approach has been shown to improve the quality of generated literature reviews over vanilla zero-shot generation (@cite_3).Additionally, we create a new test corpus consisting of recent arXiv papers to ensure that our zero-shot experiments do not suffer from test set contamination.Our work builds upon the recent advances in LLMs and their ability to store, combine, and reason about scientific knowledge (@cite_3).We believe that our approach has the potential to revolutionize the way scientific literature is reviewed and organized, and we open-source our model for the benefit of the scientific community.In summary, our related work demonstrates the potential of LLMs for scientific literature review generation and highlights the importance of context and information in producing high-quality summaries.Our proposed approach aims to address these limitations and provide a more effective way of generating literature reviews using LLMs.</p>
<p>Table 14: We show outputs of different models corresponding to the reference cited abstracts and plan mentioned in Table 12 while the main abstract is a slightly modified version of our current abstract.In this example, though, we have all the citations covered by all the models, we can see GPT-4 (Plan) output to be concise and closely following the plan.Moreover, GPT-3.5 and Llama 0-shot outputs are excessively elaborate, making them unsuitable for inclusion in the literature review.Our findings indicate that while LLMs can help in certain aspects, the final output still heavily relies on inputs from a human researcher.""" You are a helpful research assistant who is helping with literature review of a research idea .Your task is to rank some papers based on their relevance to the query abstract .</p>
<h2>Instruction : Given the query abstract : &lt; query_abstract &gt;{ query_ abstract } &lt;/ query_abstract &gt; Given the candidate reference paper abstract : &lt; candidate_paper_abstracts &gt;{ r e f e r e n c e _p a p e r s } &lt;/ candidate_paper_abstracts &gt; * Given the abstract of the candidate reference papers , provide me with a number between 0 and 100 ( upto two decimal places ) that is proportional to the probability of a paper with the given query abstract including the candidate reference paper in its literature review .<em> In addition to the probability , give me arguments for and against including this paper in the literature review .</em> You must enclose your arguments for including the paper within &lt; arguments_for &gt; and &lt;/ arguments_for &gt; tags .First, the abstract is summarized into keywords, which are used to query a search engine.Retrieved results are re-ranked (in blue) using an LLM, which is then used as context to generate the related work.Users could also provide a sentence plan (in green) according to their preference to generate a concise, readily usable literature review.</h2>
<p>Figure 4 :
4
Figure 4: Pipeline of generation task where the model needs to generate the related work of the query paper conditioned on reference papers.Our method employs an optional plan -shown by the dotted purple box, either generated by the model or appended to the prompt.</p>
<p>We use different chat versions (7B, 13B, 70B) of Llama 2-Chat 8 (Touvron et al., 2023) as zero-shot open-source LLM baselines.</p>
<p>Figure 6 :Figure 7 :
67
Figure 6: Pipeline for creating FAISS indexes for 150M SPECTER2 embeddings.</p>
<p>Figure 8 :
8
Figure 8: Interface of our human evaluation setup.</p>
<p>Figure 9 :
9
Figure 9: Prompt used for Vanilla 0-shot generation.</p>
<p>Figure 10 :
10
Figure 10: Prompt used for plan-based generation.Underlined text shows the variation compared to the vanilla 0-shot prompting, where the user provides a structure of the expected paragraph.</p>
<p>Figure 11 :
11
Figure 11: Prompt used when the plan is learned during generation.The model first generates a plan of sentences and citations which it would then condition upon to generate the final related work text, which can be considered as an extension of CoT style thinking step by step.</p>
<p>Figure 16 :
16
Figure 16: LitLLM interfaceAgarwal et al. (2024).Our system works on the Retrieval Augmented Generation (RAG) principle to generate the literature review grounded in retrieved relevant papers.The user needs to provide the abstract in the textbox (in purple) and press send to get the generated related work (in red).First, the abstract is summarized into keywords, which are used to query a search engine.Retrieved results are re-ranked (in blue) using an LLM, which is then used as context to generate the related work.Users could also provide a sentence plan (in green) according to their preference to generate a concise, readily usable literature review.</p>
<p>Ranker Predictions RollingEval-Aug (%) RollingEval-Dec (%)
Complete Ranked list55.159.7Incomplete list41.540.2Repeated Value3.30.1</p>
<p>Table 2 :
2
Error modes of GPT-4 based ranking.When using GPT-4 to provide ranks, it suffers from multiple issues.While recent works likeSun et al. (</p>
<p>Table 3 :
3
Zero-shot results for different models on the Multi-XScience dataset.
Model ClassModelROUGE1 ↑ ROUGE2 ↑ ROUGEL ↑One line baseline26.8694.46914.386ExtractiveLexRank30.9165.96615.916TextRank31.4395.81716.398Abstractive FinetunedHiersum Pointer-Generator PRIMER29.861 33.947 26.9265.029 6.754 5.02416.429 18.203 14.131Long T519.5153.36112.201AbstractiveFlan T521.9593.99212.7780-shotGalactica-1.3B18.4614.5629.894Falcon-180B22.8762.81812.087Open-source 0-shotLlama 2-Chat 7B (No plan) Llama 2-Chat 13B (No plan) Llama 2-Chat 70B (No plan)24.636 26.719 28.8665.189 5.958 6.91913.133 13.635 14.407LLama-3.1-70B (No Plan)33.2898.05015.898Closed-source 2-stageGPT-3.5-turbo (Per cite) 1st stage GPT-3.5-turbo (Per cite) 2nd stage GPT-3.5-turbo (Sentence by sentence)26.483 24.359 31.6546.311 5.594 6.44213.718 12.859 15.577Closed-sourceGPT-3.5-turbo (No plan)29.6967.32514.5620-shotGPT-4 (No plan)33.2137.60915.798Llama 2-Chat 70B (Prompted plan)30.3897.22114.911PlanGPT-3.5-turbo (Prompted plan) GPT-4 (Prompted plan)32.187 34.8197.788 7.89215.398 16.634Llama 2-Chat 70B (Plan)34.6548.37117.089GPT-3.5-turbo (Plan)35.0428.42317.136GPT-4 (Plan)37.1988.85918.772Llama-3.1-70B (Plan)35.5759.40618.772ModelROUGE1 ↑ ROUGE2 ↑ ROUGEL↑ BERTScore↑ Llama-3-Eval↑CodeLlama 34B-Instruct22.6085.03212.55382.41866.898CodeLlama 34B-Instruct (Plan)27.3695.82914.70583.38667.362Llama 2-Chat 7B23.2765.10412.58382.84168.689Llama 2-Chat 13B23.9985.47212.92382.85569.237Llama 2-Chat 70B23.7695.61912.74582.94370.980GPT-3.5-turbo (0-shot)25.1126.11813.17183.35272.434GPT-4 (0-shot)29.2896.47915.04884.20872.951Llama 2-Chat 70B (Plan)30.9197.07915.99184.39271.354GPT-3.5-turbo (Plan)30.1927.02815.55184.20372.729GPT-4 (Plan)33.0447.35217.62485.15175.240</p>
<p>Table 4 :
4
Zero-shot results on the proposed RollingEval-Aug dataset.</p>
<dl>
<dt>Table 8 :</dt>
<dt>8</dt>
<dt>Results after fine-tuning Llama 2-Chat 7B on Multi-XScience dataset Longer context While Llama 2 can ingest 4096 tokens, recent studies have found that it uses 19% more tokens</dt>
<dt>shot26.7195.95813.635Llama 2-Chat 7B -10k steps (LR 5e-6)24.7895.98612.708Llama 2-Chat 7B -30k steps (LR 5e-6)27.7956.60114.409Llama 2-Chat 7B -60k steps (LR 1e-5)22.5555.51111.749</dt>
<dt>E.g. over 4,000 ML papers were submitted to arXiv in October</dt>
<dd>https://arxiv.org/list/cs.LG/2024-10
Relevant papers Relevant papersRelevant papersRe-ranked papersRe-ranked papersUser Defined Papers of Interest
Relevant papersRelevant papers Relevant papersRelevant papers 1)2)3) 4)
We use the plan: Please generate 5 sentences in 60 words. Cite @cite_1 at line 1,
and 5. We postprocess to replace delexicalized tokens with latex commands. Outputs from other models are compared later in Appendix (Tables13 and 14).
This sentence was inserted by the authors.
https://pypi.org/project/arxiv/
https://serpapi.com/
In our experiments, we generate three queries for each abstract.
search list using LLM-based keyword search and different academic search engines. We note that using multiple queries gives us an edge over using a single query, and we obtain a similar coverage for Semantic Scholar and the SERP API-based Google Search.
Code can be accessed at https://github.com/LitLLM/litllms-for-literature-review-tmlr
We refer to Llama 2-Chat models as Llama 2 and GPT-3.5-turbo as GPT-3.5 for brevity.
We validated these strategies only on GPT-3.5 due to the high incurred cost with GPT-4 models.
This was done to reduce cognitive load on annotators to read abstracts of 4+ papers and 2 model outputs. Annotators have
 We used McNemar test (Lachenbruch, 2014)  to measure statistical significance.
https://www.explainpaper.com/, https://x.writefull.com/
https://scite.ai/
ICLR'24 Large Language Models guidelines https://iclr.cc/Conferences/2024/CallForPapers
Dataset available at http://api.semanticscholar.org/datasets/v1/
https://github.com/allenai/papermage
https://ar5iv.labs.arxiv.org/ and https://www.arxiv-vanity.com/
https://github.com/huggingface/text-generation-inference
Code will be released at github.com
https://huggingface.co/spaces/evaluate-metric/rouge Since it is a known issue in the NLG community of different implementations producing different results, we stick to evaluate==0.4.0 for reporting all the results, reproducing the ROUGE scores for baselines from Multi-XScience model outputs.
https://spacy.io/usage/linguistic-features
https://app.endpoints.anyscale.com/
https://platform.openai.com/docs/guides/gpt
https://api.semanticscholar.org/api-docs/graph
https://api.semanticscholar.org/api-docs/recommendations
Published in Transactions on Machine LearningResearch (12/2024) <br />
Coverage and human evaluation:We evaluate coverage as the percentage of model outputs with the same number of citations as ground truth (identified using regex on the delexicalized citation in the generated related work).Table6shows the efficacy of plan-based approaches.All plan models provide more coverage than their counterparts, with GPT-4 achieving 98% in covering all the citations.The largest uplift is for (vanilla) 0-shot Llama 2 70B.Using a plan raises its coverage from 59% to 82%.Similar to results in Table5, we find that the coverage of GPT-3.5 does not improve much.ModelCoverage ↑ Avg.wordsLlama 2-Chat 70B (0-shot) 59.31% 284.65 Llama 2-Chat 70B (Plan) 82.62% 191.45  63.11% 293.69  68.03% 202.81  91.34% 215.15  98.52% 125.10Table6: Coverage (in %) on the Multi-XScience dataset defines the number of citations covered in the generated response.Appendix A Ethics StatementThe rapid advancements in LLMs and NLP technologies for scientific writing have led to the emergence of increasingly powerful systems such as DeepResearch, AI Co-Scientist, and ScholarQA.These tools extend beyond earlier systems like Explainpaper and Writefull 12 , which assist in paper comprehension and abstract generation, and Scite 13 , which helps with citation discovery.As AI-powered tools become more deeply integrated into the scientific workflow, ethical considerations around their use continue to evolve.Many conferences, such as ICLR, have begun collecting statistics on authors' usage of LLMs for literature review generation and paraphrasing, and have issued guidelines on responsible usage.14 While writing assistant technology could have great promise as an aide to scientists, we think their use should be disclosed to the reader.As such assistants become more powerful, they might be abused in certain contexts, for example, where students are supposed to create a literature review as a part of their learning process.The use of such tools might also be problematic as authors of scientific work should read the articles that they cite, and heavy reliance on such tools could lead to short-term gains at the cost of a deeper understanding of a subject over the longer term.Any commercially deployed systems authors use should also contain appropriate mechanisms to detect if words have been copied exactly from the source material and provide that content in a quoted style.Additionally, as newer tools like DeepResearch, AI Co-Scientist, and ScholarQA continue to improve, it is crucial to assess their long-term impact on scientific research.The use of these tools should complement, rather than replace, human expertise in literature analysis.Finally, the rolling evaluations we present here do not involve training LLMs on arXiv papers.This mitigates concerns regarding the copyright status of arXiv papers and their use for LLM training.B New DatasetsWhile there are datasets available for different tasks in academic literature (see Table7), we use the Multi-XScience dataset(Lu et al., 2020)for our experiments.Recent work(Chen et al., 2021b;Funkquist et al., 2022) also focuses on related work generation and provides a similar dataset.As part of this work, we release two corpora: 1.We extend the Multi-XScience corpus to include the full text of research papers, and 2. We create a new test corpus, RollingEval-Aug, consisting of recent (August 2023) arXiv papers (with full content).Dataset TaskBigSurvey-MDS(Liu et al., 2023a) Survey Introduction HiCaD(Zhu et al., 2023)Survey Catalogue SciXGen(Chen et al., 2021a)Context-aware text generation CORWA(Li et al., 2022)Citation Span Generation TLDR(Cachola et al., 2020)TLDR generation Multi-XScienceLu et al. (2020)Related Work GenerationCode LLMsWe evaluate the performance of code-generating LLMs to write related-work sections requiring more formal and structured language.Since Code LLMs are pre-trained on text they might offer the best of both worlds.However, we observe that for our task, the models produce bibtex and Python code with relevant comments as part of the generated outputs.As shown in Table10, CodeLlama (34B Instruct) is good at following instructions and at generating natural language (ROUGE2 of 5.8 and 5.02 on Multi-XScience and RollingEval-Aug dataset).With a plan, CodeLlama even surpasses vanilla 0-shot Llama 2 70B (Table4).relevance for all the candidate papers.We refer the reader to Figure15forThe exact prompt used for debate ranking.ModelGeneration: There was only one request per query abstract in the RollingEval dataset, so 500 requests in total for each experiment (as n = 500 in RollingEval).The table below summarizes the API analysis for the two stages of the pipeline for the RollingEval experiments.Abstract of Multi-XScience paper (Lu et al., 2020)Reference @cite_1: Multi-document summarization is a challenging task for which there exists little large-scale datasets.We propose Multi-XScience, a large-scale multi-document summarization dataset created from scientific articles.MultiXScience introduces a challenging multi-document summarization task: writing the related-work section of a paper based on its abstract and the articles it references.Our work is inspired by extreme summarization, a dataset construction protocol that favours abstractive modeling approaches.Descriptive statistics and empirical results-using several state-of-the-art models trained on the MultiXScience dataset-reveal that Multi-XScience is well suited for abstractive models.Abstract of Extractive and Abstractive Summarization paper (Pilault et al., 2020)Reference @cite_2: We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization.We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary.We show that this extractive step significantly improves summarization results.We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher rouge scores.Note: The abstract above was not written by the authors, it was generated by one of the models presented in this paper.Abstract of Galactica paper (Taylor et al., 2022)Reference @cite_3: Information overload is a major obstacle to scientific progress.The explosive growth in scientific literature and data has made it ever harder to discover useful insights in a large mass of information.Today scientific knowledge is accessed through search engines, but they are unable to organize scientific knowledge alone.In this paper we introduce Galactica: a large language model that can store, combine and reason about scientific knowledge.We train on a large scientific corpus of papers, reference material, knowledge bases and many other sources.We outperform existing models on a range of scientific tasks.On technical knowledge probes such as LaTeX equations, Galactica outperforms the latest GPT-3 by 68.2% versus 49.0%.Galactica also performs well on reasoning, outperforming Chinchilla on mathematical MMLU by 41.3% to 35.7%, and PaLM 540B on MATH with a score of 20.4% versus 8.8%.It also sets a new state-of-the-art on downstream tasks such as PubMedQA and MedMCQA dev of 77.6% and 52.9%.And despite not being trained on a general corpus, Galactica outperforms BLOOM and OPT-175B on BIG-bench.We believe these results demonstrate the potential for language models as a new interface for science.We open source the model for the benefit of the scientific community.Plan for Table 13Please generate 5 sentences in 120 words.Cite @cite_1 at line 1, 3 and 5.Plan for Table 13Please generate 5 sentences in 120 words.Cite @cite_1 at line 1 and 3. Cite @cite_2 at line 2 and 5. Cite @cite_3 at line 4 and 5.Table12: Abstracts of papers which are reference citations in Tables13 and 14.Keyword summarization promptYou are a helpful research assistant who is helping with literature review of a research idea.You will be provided with an abstract of a scientific document.Your task is to summarize the abstract in max 5 keywords to search for related papers using API of academic search engine.<code>`Abstract: {abstract}</code>F
Sanity checks for saliency maps. Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim, Advances in Neural Information Processing Systems. 2018</dd>
</dl>
<p>Litllm: A toolkit for scientific literature review. Shubham Agarwal, H Issam, Laurent Laradji, Christopher Charlin, Pal, arXiv:2402.017882024arXiv preprint</p>
<p>Introducing ai2 scholarqa. Allenai, 2025</p>
<p>Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of language models: Towards open frontier models. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Maitha Alhammadi, Mazzotta Daniele, Daniel Heslow, Julien Launay, 2023Quentin MalarticTo appear</p>
<p>Exploring the boundaries of reality: Investigating the phenomenon of artificial intelligence hallucination in scientific writing through chatgpt references. Sandeep Sai Anirudh Athaluri, V Varma Manthena, Manoj S R Krishna, Vineel Kesapragada, Tirth Yarlagadda, Rama Dave, Siri Tulasi, Duddumpudi, Cureus. 152023</p>
<p>Mint-1t: Scaling open-source multimodal data by 10x: A multimodal dataset with one trillion tokens. Anas Awadalla, Le Xue, Oscar Lo, Manli Shu, Hannah Lee, Etash Kumar Guha, Matt Jordan, Sheng Shen, Mohamed Awadalla, Silvio Savarese, Caiming Xiong, Ran Xu, Yejin Choi, Ludwig Schmidt, 2024</p>
<p>SciBERT: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, 10.18653/v1/D19-1371Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>Nougat: Neural optical understanding for academic documents. Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic, 2023</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mccandlish, Alec Radford, Ilya Sutskever, Dario Amodei, 2020</p>
<p>TLDR: Extreme summarization of scientific documents. Isabel Cachola, Kyle Lo, Arman Cohan, Daniel Weld, doi: 10.18653Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsNovember 2020</p>
<p>URL. </p>
<p>SciXGen: A scientific paper dataset for context-aware text generation. Hong Chen, Hiroya Takamura, Hideki Nakayama, 10.18653/v1/2021.findings-emnlp.128Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational LinguisticsNovember 2021a</p>
<p>Capturing relations between scientific papers: An abstractive model for related work section generation. Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, Rui Yan, 10.18653/v1/2021.acl-long.473Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingAssociation for Computational LinguisticsAugust 2021b1</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>A discourse-aware attention model for abstractive summarization of long documents. Arman Cohan, Franck Dernoncourt, Soon Doo, Trung Kim, Seokhwan Bui, Walter Kim, Nazli Chang, Goharian, 10.18653/v1/N18-2097Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Short Papers. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational LinguisticsJune 20182</p>
<p>Specter: Document-level representation learning using citation-informed transformers. Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, Daniel S Weld, arXiv:2004.071802020arXiv preprint</p>
<p>Contextcite: Attributing model generation to context. Benjamin Cohen-Wang, Harshay Shah, Kristian Georgiev, Aleksander Madry, 2024</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint</p>
<p>Lexrank: Graph-based lexical centrality as salience in text summarization. Günes Erkan, Dragomir R Radev, Journal of artificial intelligence research. 222004</p>
<p>Interpretable explanations of black boxes by meaningful perturbation. Ruth C Fong, Andrea Vedaldi, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2017</p>
<p>Citebench: A benchmark for scientific citation text generation. Martin Funkquist, Ilia Kuznetsov, Yufang Hou, Iryna Gurevych, arXiv:2212.095772022arXiv preprint</p>
<p>Enabling large language models to generate text with citations. Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen, arXiv:2305.146272023arXiv preprint</p>
<p>End-to-end content and plan selection for data-to-text generation. Sebastian Gehrmann, Falcon Dai, Henry Elder, Alexander Rush, 10.18653/v1/W18-6505Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The NetherlandsAssociation for Computational LinguisticsNovember 2018</p>
<p>Interpretation of neural networks is fragile. Amirata Ghorbani, Abubakar Abid, James Zou, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201933</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>LongT5: Efficient text-to-text transformer for long sequences. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, Yinfei Yang, 10.18653/v1/2022.findings-naacl.55Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>
<p>Llama 2 is about as factually accurate as gpt-4 for summaries and is 30x cheaper. Waleed Kadous, Aug 2023</p>
<p>On the attribution of confidence to large language models. Geoff Keeling, Winnie Street, 2024</p>
<p>The (un) reliability of saliency methods. Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, T Kristof, Sven Schütt, Dumitru Dähne, Been Erhan, Kim, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. 2019</p>
<p>The semantic scholar open data platform. Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, arXiv:2301.101402023arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>A Peter, Lachenbruch, Mcnemar test. Wiley StatsRef: Statistics Reference Online. 2014</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>A survey of large language models attribution. Dongfang Li, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Ziyang Chen, Baotian Hu, Aiguo Wu, Min Zhang, 2023</p>
<p>Visualizing and understanding neural models in nlp. Jiwei Li, Xinlei Chen, Eduard Hovy, Dan Jurafsky, Proceedings of the 2016 Conference of the North American Chapter. the 2016 Conference of the North American ChapterAssociation for Computational Linguistics2016</p>
<p>CORWA: A citation-oriented related work annotation dataset. Xiangci Li, Biswadip Mandal, Jessica Ouyang, 10.18653/v1/2022.naacl-main.397Proceedings of the 2022 Conference of the North American Chapter. the 2022 Conference of the North American ChapterSeattle, United StatesAssociation for Computational LinguisticsJuly 2022</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsJuly 2004</p>
<p>Generating a structured summary of numerous academic papers: Dataset and method. Shuaiqi Liu, Jiannong Cao, Ruosong Yang, Zhiyuan Wen, arXiv:2302.045802023aarXiv preprint</p>
<p>Hierarchical transformers for multi-document summarization. Yang Liu, Mirella Lapata, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023b</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, Daniel Weld, 10.18653/v1/2020.acl-main.447Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020</p>
<p>. Patrice Lopez, Grobid, date: 2012-09-13T15:48:54ZFebruary 2023</p>
<p>Multi-XScience: A large-scale dataset for extreme multi-document summarization of scientific articles. Yao Lu, Yue Dong, Laurent Charlin, 10.18653/v1/2020.emnlp-main.648Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>Zero-shot listwise document reranking with a large language model. Xueguang Ma, Xinyu Zhang, Ronak Pradeep, Jimmy Lin, arXiv:2305.021562023arXiv preprint</p>
<p>Textrank: Bringing order into text. Rada Mihalcea, Paul Tarau, Proceedings of the 2004 conference on empirical methods in natural language processing. the 2004 conference on empirical methods in natural language processing2004</p>
<p>LoRaLay: A multilingual and multimodal dataset for long range and layout-aware summarization. Laura Nguyen, Thomas Scialom, Benjamin Piwowarski, Jacopo Staiano, 10.18653/v1/2023.eacl-main.46Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European ChapterDubrovnik, CroatiaAssociation for Computational LinguisticsMay 2023</p>
<p>GPT-4 technical report. arXiv, 2023. OpenAI. Introducing deep research. 2025OpenAI</p>
<p>Automatic differentiation in PyTorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, NeurIPS-W. 2017</p>
<p>On extractive and abstractive neural document summarization with transformer language models. Jonathan Pilault, Raymond Li, Sandeep Subramanian, Chris Pal, 10.18653/v1/2020.emnlp-main.748Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>Rankvicuna: Zero-shot listwise document reranking with open-source large language models. Ronak Pradeep, Sahel Sharifymoghaddam, Jimmy Lin, arXiv:2309.150882023aarXiv preprint</p>
<p>Ronak Pradeep, Sahel Sharifymoghaddam, Jimmy Lin, arXiv:2312.02724Rankzephyr: Effective and robust zero-shot listwise reranking is a breeze!. 2023barXiv preprint</p>
<p>Openalex: A fully-open index of scholarly works, authors, venues, institutions, and concepts. Jason Priem, Heather Piwowar, Richard Orr, arXiv:2205.018332022arXiv preprint</p>
<p>Data-to-text generation with macro planning. Ratish Puduppully, Mirella Lapata, 10.1162/tacl_a_00381/101876/Data-to-text-Generation-with-Macro-PlanningTransactions of the Association for Computational Linguistics. 92021</p>
<p>Data-to-text generation with content selection and planning. Ratish Puduppully, Li Dong, Mirella Lapata, 10.1609/aaai.v33i01.33016908The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence. Honolulu, Hawaii, USAAAAI PressJanuary 27 -February 1, 2019. 20192019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERT-networks. Nasim Rahaman, Martin Weiss, Manuel Wüthrich, Yoshua Bengio, Li Erran Li, Chris Pal, Bernhard Schölkopf, 10.18653/v1/D19-1410arXiv:2403.14443Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Kentaro Inui, Jing Jiang, Vincent Ng, Xiaojun Wan, the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2024. November 2019arXiv preprintLanguage models can reduce asymmetry in information markets</p>
<p>Building applied natural language generation systems. Ehud Reiter, Robert Dale, Natural Language Engineering. 311997</p>
<p>why should i trust you?": Explaining the predictions of any classifier. Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACM2016</p>
<p>Bigdocs: An open and permissively-licensed dataset for training multimodal models on document and code tasks. Juan Rodriguez, Xiangru Jian, Siba Smarak Panigrahi, Tianyu Zhang, Aarash Feizi, Abhay Puri, Akshay Kalkunte, François Savard, Ahmed Masry, Shravan Nayak, Rabiul Awal, Mahsa Massoud, Amirhossein Abaskohi, Zichao Li, Suyuchen Wang, Pierre-André Noël, Leon Mats, Saverio Richter, Shubbam Vadacchino, Sanket Agarwal, Sara Biswas, Ying Shanian, Noah Zhang, Kurt Bolger, Simon Macdonald, Sathwik Fauvel, Srinivas Tejaswi, Joao Sunkara, Monteiro, D J Krishnamurthy, Torsten Dvijotham, Nicolas Scholak, Sepideh Chapados, Sean Kharagani, M Hughes, Siva Özsu, Marco Reddy, Pedersoli, Bengio, Christopher Pal, Issam Laradji, Spandanna Gella, Perouz Taslakian, David Vazquez, and Sai Rajeswar2024a</p>
<p>Juan A Rodriguez, Abhay Puri, Shubham Agarwal, Issam H Laradji, Pau Rodriguez, Sai Rajeswar, David Vazquez, Christopher Pal, Marco Pedersoli, Starvector, Generating scalable vector graphics code from images and text. 2024b</p>
<p>unarXive: A Large Scholarly Data Set with Publications' Full-Text, Annotated In-Text Citations, and Links to Metadata. Tarek Saier, Michael Färber, 10.1007/s11192-020-03382-zScientometrics. 1588-28611253December 2020</p>
<p>All arXiv Publications Pre-Processed for NLP, Including Structured Full-Text and Citation Network. Tarek Saier, Johan Krause, Michael Färber, Proceedings of the 23rd ACM/IEEE Joint Conference on Digital Libraries. the 23rd ACM/IEEE Joint Conference on Digital Libraries2022. 202323</p>
<p>Get to the point: Summarization with pointer-generator networks. Abigail See, Peter J Liu, Christopher D Manning, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational Linguistics2017</p>
<p>Learning important features through propagating activation differences. Avanti Shrikumar, Peyton Greenside, Anshul Kundaje, International Conference on Machine Learning. PMLR2017</p>
<p>Deep inside convolutional networks: Visualising image classification models and saliency maps. Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, 2014</p>
<p>SciRepEval: A multi-format benchmark for scientific document representations. Amanpreet Singh, Mike D' Arcy, Arman Cohan, Doug Downey, Sergey Feldman, arXiv:2211.133082022arXiv preprint</p>
<p>An overview of microsoft academic service (mas) and applications. Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, Kuansan Wang, Proceedings of the 24th international conference on world wide web. the 24th international conference on world wide web2015</p>
<p>Smoothgrad: Removing noise by adding noise. Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, Martin Wattenberg, 2017</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Trainable sentence planning for complex information presentations in spoken dialog systems. Amanda Stent, Rashmi Prasad, Marilyn Walker, 10.3115/1218955.1218966Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04). the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)Barcelona, SpainJuly 2004</p>
<p>Roformer: Enhanced transformer with rotary position embedding. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, arXiv:2104.098642021arXiv preprint</p>
<p>Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, Zhaochun Ren, arXiv:2304.09542Is ChatGPT good at search? investigating large language models as re-ranking agent. 2023arXiv preprint</p>
<p>Axiomatic attribution for deep networks. Mukund Sundararajan, Ankur Taly, Qiqi Yan, International Conference on Machine Learning. PMLR2017</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Individual and domain adaptation in sentence planning for dialogue. Marilyn A Walker, Amanda Stent, François Mairesse, Rashmi Prasad, Journal of Artificial Intelligence Research. 302007</p>
<p>Plan-andsolve prompting: Improving zero-shot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, arXiv:2305.040912023arXiv preprint</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, arXiv:1910.037712019arXiv preprint</p>
<p>Primera: Pyramid-based masked sentence pre-training for multi-document summarization. Wen Xiao, Iz Beltagy, Giuseppe Carenini, Arman Cohan, arXiv:2110.084992021arXiv preprint</p>
<p>Re3: Generating longer stories with recursive reprompting and revision. Kevin Yang, Yuandong Tian, Nanyun Peng, Dan Klein, arXiv:2210.067742022arXiv preprint</p>
<p>Automatic evaluation of attribution by large language models. Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, Huan Sun, 2023</p>
<p>Xinyu Zhang, Sebastian Hofstätter, Patrick Lewis, Raphael Tang, Jimmy Lin, arXiv:2312.02969Rank-without-gpt: Building gpt-independent listwise rerankers on open-source large language models. 2023arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022arXiv preprint</p>
<p>Hierarchical catalogue generation for literature review: A benchmark. Kun Zhu, Xiaocheng Feng, Xiachong Feng, Yingsheng Wu, Bing Qin, arXiv:2304.035122023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
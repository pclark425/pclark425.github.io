<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6439 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6439</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6439</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-12407be490a4e4633da7f25a93af000be573a288</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/12407be490a4e4633da7f25a93af000be573a288" target="_blank">Zep: A Temporal Knowledge Graph Architecture for Agent Memory</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Zep is introduced, a novel memory layer service for AI agents that outperforms the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR) benchmark and is validated through the more challenging LongMemEval benchmark, which better reflects enterprise use cases through complex temporal reasoning tasks.</p>
                <p><strong>Paper Abstract:</strong> We introduce Zep, a novel memory layer service for AI agents that outperforms the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR) benchmark. Additionally, Zep excels in more comprehensive and challenging evaluations than DMR that better reflect real-world enterprise use cases. While existing retrieval-augmented generation (RAG) frameworks for large language model (LLM)-based agents are limited to static document retrieval, enterprise applications demand dynamic knowledge integration from diverse sources including ongoing conversations and business data. Zep addresses this fundamental limitation through its core component Graphiti -- a temporally-aware knowledge graph engine that dynamically synthesizes both unstructured conversational data and structured business data while maintaining historical relationships. In the DMR benchmark, which the MemGPT team established as their primary evaluation metric, Zep demonstrates superior performance (94.8% vs 93.4%). Beyond DMR, Zep's capabilities are further validated through the more challenging LongMemEval benchmark, which better reflects enterprise use cases through complex temporal reasoning tasks. In this evaluation, Zep achieves substantial results with accuracy improvements of up to 18.5% while simultaneously reducing response latency by 90% compared to baseline implementations. These results are particularly pronounced in enterprise-critical tasks such as cross-session information synthesis and long-term context maintenance, demonstrating Zep's effectiveness for deployment in real-world applications.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6439.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6439.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zep</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zep (with Graphiti temporal knowledge graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A production memory layer service for LLM agents that uses Graphiti, a temporally-aware dynamic knowledge graph, to store episodic messages, semantic entities/facts, and community summaries; supports vector/BM25/graph search, reranking, and a constructor to produce compact LLM context strings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Zep (Graphiti)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-agent memory layer: ingest conversation messages as episodic nodes, extract entities and facts into a semantic subgraph, and maintain community summaries. Retrieval composes hybrid search (cosine embedding, BM25 full-text, breadth-first graph expansion), reranking (RRF/MMR/node-distance/cross-encoder), and a constructor that emits formatted context for an LLM. Updates include entity resolution, embedding-based deduplication, temporal extraction, and LLM-assisted edge invalidation; communities updated via dynamic label-propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (experiments used gpt-4-turbo, gpt-4o-mini, gpt-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external temporally-aware knowledge graph (episodic + semantic + community subgraphs) — retrieval-augmented generation (graph-backed RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw episodic messages (stored non-lossily), semantic entity nodes with summaries and embeddings, semantic edges (facts) with temporal fields (valid/invalid timestamps), and community nodes with summaries and embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Hybrid retrieval: cosine similarity over embeddings, Okapi BM25 full-text search (Lucene), breadth-first graph expansion from recent episodes; reranking via RRF/MMR, mention-frequency and node-distance heuristics, or cross-encoder LLM re-ranker; constructor formats selected nodes/edges + temporal ranges into LLM prompt context. Writes: ingestion pipeline (entity/fact extraction, embedding, entity/edge deduplication/resolution via LLM prompts), temporal extraction, LLM-based contradiction detection for edge invalidation, dynamic community assignment via label propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Deep Memory Retrieval (DMR) and LongMemEval (LongMemEval_s / LongMemEval_a)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>dialogue / long-term memory / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>DMR: 94.8% accuracy (gpt-4-turbo); 98.2% accuracy (gpt-4o-mini). LongMemEval: 63.8% (gpt-4o-mini, accuracy) with avg context tokens ~1.6k and latency 3.20 s; 71.2% (gpt-4o, accuracy) with latency 2.58 s.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>DMR baselines: full-conversation: 94.4% (gpt-4-turbo) and 98.0% (gpt-4o-mini); conversation summaries: 78.6% (gpt-4-turbo) and 88.0% (gpt-4o-mini). LongMemEval full-context: 55.4% (gpt-4o-mini) and 60.2% (gpt-4o) with avg context tokens 115k and latencies 31.3 s and 28.9 s respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Substantial latency and token-cost reductions compared to full-context (approx. 90% latency reduction), much smaller prompt sizes (~1.6k tokens vs ~115k tokens). Computational cost tradeoffs include LLM calls for extraction/resolution and optional cross-encoder reranking (high cost). Dynamic community updates trade off accuracy for latency/compute (dynamic label-propagation deviates from full recomputation and requires periodic refresh).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>On DMR (short conversations) improvements were marginal because full-context fits in model windows; Zep performed worse on the 'single-session-assistant' question type (drops of 9.06% with gpt-4o-mini and 17.7% with gpt-4o) vs full-context. Less-capable models may require additional development to fully utilize Zep's temporal data. Community dynamic update heuristic diverges from full label-propagation and needs periodic refreshes. Some retrieval/reranking options (cross-encoder) are computationally expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, Daniel Chalef. (2025). Zep: A Temporal Knowledge Graph Architecture for AGENT MEMORY. (technical report / preprint)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zep: A Temporal Knowledge Graph Architecture for Agent Memory', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6439.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6439.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT (from Packer et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system that augments LLM agents with memory; presented a Deep Memory Retrieval (DMR) benchmark and reported high accuracy with gpt-4-turbo on that task.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memgpt: Towards llms as operating systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Previous LLM-agent memory framework described in [3]; referenced as prior work and as the leader on the DMR benchmark in the original MemGPT paper. This paper cites MemGPT's reported DMR accuracy but does not reimplement its architecture or detail its memory internals.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified in this paper (original MemGPT reported results using gpt-4-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>described generally as LLM memory augmentation in original work (details not provided in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>not specified in this paper (see original MemGPT paper)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>not specified in this paper (see original MemGPT paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Deep Memory Retrieval (DMR)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>dialogue / long-term memory / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>DMR: 93.4% accuracy (reported in [3], gpt-4-turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline reported in [3] — recursive summarization baseline: 35.3% (gpt-4-turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Not detailed in this paper; referenced results only. The current paper notes MemGPT could not be directly ingested for LongMemEval because MemGPT's framework lacks direct ingestion of existing message histories.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>This paper reports inability to evaluate MemGPT on LongMemEval due to MemGPT not supporting direct ingestion of existing message histories; further methodological details were insufficient to reproduce MemGPT results on gpt-4o-mini.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, Joseph E. Gonzalez. (2024). Memgpt: Towards llms as operating systems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zep: A Temporal Knowledge Graph Architecture for Agent Memory', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6439.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6439.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full-conversation baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Full-conversation (full-context in-context baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline memory approach that supplies the entire conversation history directly in the model context window (no external memory layer).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Full-conversation in-context</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>No external memory store; entire multi-session conversation is concatenated and provided as prompt context to the LLM (relies on model context window to store memory).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (experiments used gpt-4-turbo, gpt-4o-mini, gpt-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>in-context full conversation (no external memory service)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw conversation tokens in the model prompt (up to model context window capacity)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Implicit via LLM attention over the prompt (no external retrieval or indexing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Deep Memory Retrieval (DMR) and LongMemEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>dialogue / retrieval / long-context</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Not applicable (this approach is the baseline); reported performance: DMR: 94.4% (gpt-4-turbo), 98.0% (gpt-4o-mini). LongMemEval: 55.4% (gpt-4o-mini) and 60.2% (gpt-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>N/A (this is the 'no external memory' in-context approach).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Very large prompt sizes (avg ~115k tokens in LongMemEval experiments), high latency (reported ~28–31 s), high token and compute cost, and poor scalability to longer histories. Performs well when entire history fits within context window.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Scales poorly to very long histories; token costs and latency grow linearly with history length; in LongMemEval, accuracy is substantially worse than Zep while incurring much higher latency and context size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Zep paper (this work) — baseline experimental condition</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zep: A Temporal Knowledge Graph Architecture for Agent Memory', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6439.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6439.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conversation summaries baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Session / conversation summaries baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory baseline where session-level summaries are provided to the LLM instead of entire conversation tokens; reduces prompt size at the cost of possible information loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Conversation summaries</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Summarization-based memory: compress previous sessions into summaries and provide those summaries as context to the LLM rather than full raw messages.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (experiments used gpt-4-turbo, gpt-4o-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>summarization-based external memory (dense textual summaries stored and retrieved)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Session-level summary text (shorter textual representation of past messages)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Summaries generated (presumably via an LLM) and included in prompt context; retrieval is simply selecting summaries corresponding to relevant sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Deep Memory Retrieval (DMR)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>dialogue / retrieval / summarization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>DMR: 78.6% accuracy (gpt-4-turbo); 88.0% (gpt-4o-mini).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Recursive summarization baseline: 35.3% (gpt-4-turbo) reported as alternative summarization strategy in [3].</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Lower prompt size and cost than full-conversation; lower recall/accuracy on fact-retrieval tasks compared to full-conversation and graph memory (Zep).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Summaries are lossy and can omit details needed for precise fact retrieval; performs worse than full-conversation and Zep on DMR in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zep: A Temporal Knowledge Graph Architecture for Agent Memory', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6439.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6439.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recursive summarization baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursive summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach that repeatedly summarizes conversation history (a hierarchical summarization pipeline); used as a low-performing baseline on DMR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Recursive summarization</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Hierarchical/lazy summarization approach that compresses conversation recursively to form a compact memory representation for the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not specified (baseline reported using gpt-4-turbo in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>summarization-based memory (recursive/hierarchical summaries)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Compressed textual summaries (recursive/hierarchical)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Summaries are precomputed and then provided to the LLM as context; retrieval is selecting relevant summary segments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Deep Memory Retrieval (DMR)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>dialogue / summarization / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>DMR: 35.3% (gpt-4-turbo) — reported as baseline in [3].</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>N/A (this is a baseline memory approach).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Very compact context (low token cost) at the expense of large accuracy loss on the DMR fact-retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails at fine-grained fact retrieval compared to full-conversation and graph-based memory; shown to be a weak baseline on DMR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Zep: A Temporal Knowledge Graph Architecture for Agent Memory', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Memgpt: Towards llms as operating systems <em>(Rating: 2)</em></li>
                <li>LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory <em>(Rating: 2)</em></li>
                <li>From local to global: A graph rag approach to query-focused summarization <em>(Rating: 2)</em></li>
                <li>Arigraph: Learning knowledge graph world models with episodic memory for llm agents <em>(Rating: 2)</em></li>
                <li>LightRAG: Simple and fast retrieval-augmented generation <em>(Rating: 1)</em></li>
                <li>Distill-synthkg: Distilling knowledge graph synthesis workflow for improved coverage and efficiency <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6439",
    "paper_id": "paper-12407be490a4e4633da7f25a93af000be573a288",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "Zep",
            "name_full": "Zep (with Graphiti temporal knowledge graph)",
            "brief_description": "A production memory layer service for LLM agents that uses Graphiti, a temporally-aware dynamic knowledge graph, to store episodic messages, semantic entities/facts, and community summaries; supports vector/BM25/graph search, reranking, and a constructor to produce compact LLM context strings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Zep (Graphiti)",
            "agent_description": "An LLM-agent memory layer: ingest conversation messages as episodic nodes, extract entities and facts into a semantic subgraph, and maintain community summaries. Retrieval composes hybrid search (cosine embedding, BM25 full-text, breadth-first graph expansion), reranking (RRF/MMR/node-distance/cross-encoder), and a constructor that emits formatted context for an LLM. Updates include entity resolution, embedding-based deduplication, temporal extraction, and LLM-assisted edge invalidation; communities updated via dynamic label-propagation.",
            "model_size": "not specified (experiments used gpt-4-turbo, gpt-4o-mini, gpt-4o)",
            "memory_used": true,
            "memory_type": "external temporally-aware knowledge graph (episodic + semantic + community subgraphs) — retrieval-augmented generation (graph-backed RAG)",
            "memory_representation": "Raw episodic messages (stored non-lossily), semantic entity nodes with summaries and embeddings, semantic edges (facts) with temporal fields (valid/invalid timestamps), and community nodes with summaries and embeddings.",
            "memory_access_mechanism": "Hybrid retrieval: cosine similarity over embeddings, Okapi BM25 full-text search (Lucene), breadth-first graph expansion from recent episodes; reranking via RRF/MMR, mention-frequency and node-distance heuristics, or cross-encoder LLM re-ranker; constructor formats selected nodes/edges + temporal ranges into LLM prompt context. Writes: ingestion pipeline (entity/fact extraction, embedding, entity/edge deduplication/resolution via LLM prompts), temporal extraction, LLM-based contradiction detection for edge invalidation, dynamic community assignment via label propagation.",
            "task_name": "Deep Memory Retrieval (DMR) and LongMemEval (LongMemEval_s / LongMemEval_a)",
            "task_category": "dialogue / long-term memory / retrieval",
            "performance_with_memory": "DMR: 94.8% accuracy (gpt-4-turbo); 98.2% accuracy (gpt-4o-mini). LongMemEval: 63.8% (gpt-4o-mini, accuracy) with avg context tokens ~1.6k and latency 3.20 s; 71.2% (gpt-4o, accuracy) with latency 2.58 s.",
            "performance_without_memory": "DMR baselines: full-conversation: 94.4% (gpt-4-turbo) and 98.0% (gpt-4o-mini); conversation summaries: 78.6% (gpt-4-turbo) and 88.0% (gpt-4o-mini). LongMemEval full-context: 55.4% (gpt-4o-mini) and 60.2% (gpt-4o) with avg context tokens 115k and latencies 31.3 s and 28.9 s respectively.",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (percentage)",
            "tradeoffs_reported": "Substantial latency and token-cost reductions compared to full-context (approx. 90% latency reduction), much smaller prompt sizes (~1.6k tokens vs ~115k tokens). Computational cost tradeoffs include LLM calls for extraction/resolution and optional cross-encoder reranking (high cost). Dynamic community updates trade off accuracy for latency/compute (dynamic label-propagation deviates from full recomputation and requires periodic refresh).",
            "limitations_or_failure_cases": "On DMR (short conversations) improvements were marginal because full-context fits in model windows; Zep performed worse on the 'single-session-assistant' question type (drops of 9.06% with gpt-4o-mini and 17.7% with gpt-4o) vs full-context. Less-capable models may require additional development to fully utilize Zep's temporal data. Community dynamic update heuristic diverges from full label-propagation and needs periodic refreshes. Some retrieval/reranking options (cross-encoder) are computationally expensive.",
            "citation": "Preston Rasmussen, Pavlo Paliychuk, Travis Beauvais, Jack Ryan, Daniel Chalef. (2025). Zep: A Temporal Knowledge Graph Architecture for AGENT MEMORY. (technical report / preprint)",
            "uuid": "e6439.0",
            "source_info": {
                "paper_title": "Zep: A Temporal Knowledge Graph Architecture for Agent Memory",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "MemGPT",
            "name_full": "MemGPT (from Packer et al.)",
            "brief_description": "A prior system that augments LLM agents with memory; presented a Deep Memory Retrieval (DMR) benchmark and reported high accuracy with gpt-4-turbo on that task.",
            "citation_title": "Memgpt: Towards llms as operating systems",
            "mention_or_use": "mention",
            "agent_name": "MemGPT",
            "agent_description": "Previous LLM-agent memory framework described in [3]; referenced as prior work and as the leader on the DMR benchmark in the original MemGPT paper. This paper cites MemGPT's reported DMR accuracy but does not reimplement its architecture or detail its memory internals.",
            "model_size": "not specified in this paper (original MemGPT reported results using gpt-4-turbo)",
            "memory_used": true,
            "memory_type": "described generally as LLM memory augmentation in original work (details not provided in this paper)",
            "memory_representation": "not specified in this paper (see original MemGPT paper)",
            "memory_access_mechanism": "not specified in this paper (see original MemGPT paper)",
            "task_name": "Deep Memory Retrieval (DMR)",
            "task_category": "dialogue / long-term memory / retrieval",
            "performance_with_memory": "DMR: 93.4% accuracy (reported in [3], gpt-4-turbo).",
            "performance_without_memory": "Baseline reported in [3] — recursive summarization baseline: 35.3% (gpt-4-turbo).",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (percentage)",
            "tradeoffs_reported": "Not detailed in this paper; referenced results only. The current paper notes MemGPT could not be directly ingested for LongMemEval because MemGPT's framework lacks direct ingestion of existing message histories.",
            "limitations_or_failure_cases": "This paper reports inability to evaluate MemGPT on LongMemEval due to MemGPT not supporting direct ingestion of existing message histories; further methodological details were insufficient to reproduce MemGPT results on gpt-4o-mini.",
            "citation": "Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, Joseph E. Gonzalez. (2024). Memgpt: Towards llms as operating systems.",
            "uuid": "e6439.1",
            "source_info": {
                "paper_title": "Zep: A Temporal Knowledge Graph Architecture for Agent Memory",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Full-conversation baseline",
            "name_full": "Full-conversation (full-context in-context baseline)",
            "brief_description": "A baseline memory approach that supplies the entire conversation history directly in the model context window (no external memory layer).",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Full-conversation in-context",
            "agent_description": "No external memory store; entire multi-session conversation is concatenated and provided as prompt context to the LLM (relies on model context window to store memory).",
            "model_size": "not specified (experiments used gpt-4-turbo, gpt-4o-mini, gpt-4o)",
            "memory_used": false,
            "memory_type": "in-context full conversation (no external memory service)",
            "memory_representation": "Raw conversation tokens in the model prompt (up to model context window capacity)",
            "memory_access_mechanism": "Implicit via LLM attention over the prompt (no external retrieval or indexing)",
            "task_name": "Deep Memory Retrieval (DMR) and LongMemEval",
            "task_category": "dialogue / retrieval / long-context",
            "performance_with_memory": "Not applicable (this approach is the baseline); reported performance: DMR: 94.4% (gpt-4-turbo), 98.0% (gpt-4o-mini). LongMemEval: 55.4% (gpt-4o-mini) and 60.2% (gpt-4o).",
            "performance_without_memory": "N/A (this is the 'no external memory' in-context approach).",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (percentage)",
            "tradeoffs_reported": "Very large prompt sizes (avg ~115k tokens in LongMemEval experiments), high latency (reported ~28–31 s), high token and compute cost, and poor scalability to longer histories. Performs well when entire history fits within context window.",
            "limitations_or_failure_cases": "Scales poorly to very long histories; token costs and latency grow linearly with history length; in LongMemEval, accuracy is substantially worse than Zep while incurring much higher latency and context size.",
            "citation": "Zep paper (this work) — baseline experimental condition",
            "uuid": "e6439.2",
            "source_info": {
                "paper_title": "Zep: A Temporal Knowledge Graph Architecture for Agent Memory",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Conversation summaries baseline",
            "name_full": "Session / conversation summaries baseline",
            "brief_description": "A memory baseline where session-level summaries are provided to the LLM instead of entire conversation tokens; reduces prompt size at the cost of possible information loss.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Conversation summaries",
            "agent_description": "Summarization-based memory: compress previous sessions into summaries and provide those summaries as context to the LLM rather than full raw messages.",
            "model_size": "not specified (experiments used gpt-4-turbo, gpt-4o-mini)",
            "memory_used": true,
            "memory_type": "summarization-based external memory (dense textual summaries stored and retrieved)",
            "memory_representation": "Session-level summary text (shorter textual representation of past messages)",
            "memory_access_mechanism": "Summaries generated (presumably via an LLM) and included in prompt context; retrieval is simply selecting summaries corresponding to relevant sessions.",
            "task_name": "Deep Memory Retrieval (DMR)",
            "task_category": "dialogue / retrieval / summarization",
            "performance_with_memory": "DMR: 78.6% accuracy (gpt-4-turbo); 88.0% (gpt-4o-mini).",
            "performance_without_memory": "Recursive summarization baseline: 35.3% (gpt-4-turbo) reported as alternative summarization strategy in [3].",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (percentage)",
            "tradeoffs_reported": "Lower prompt size and cost than full-conversation; lower recall/accuracy on fact-retrieval tasks compared to full-conversation and graph memory (Zep).",
            "limitations_or_failure_cases": "Summaries are lossy and can omit details needed for precise fact retrieval; performs worse than full-conversation and Zep on DMR in these experiments.",
            "uuid": "e6439.3",
            "source_info": {
                "paper_title": "Zep: A Temporal Knowledge Graph Architecture for Agent Memory",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Recursive summarization baseline",
            "name_full": "Recursive summarization",
            "brief_description": "A baseline approach that repeatedly summarizes conversation history (a hierarchical summarization pipeline); used as a low-performing baseline on DMR.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Recursive summarization",
            "agent_description": "Hierarchical/lazy summarization approach that compresses conversation recursively to form a compact memory representation for the LLM.",
            "model_size": "not specified (baseline reported using gpt-4-turbo in referenced work)",
            "memory_used": true,
            "memory_type": "summarization-based memory (recursive/hierarchical summaries)",
            "memory_representation": "Compressed textual summaries (recursive/hierarchical)",
            "memory_access_mechanism": "Summaries are precomputed and then provided to the LLM as context; retrieval is selecting relevant summary segments.",
            "task_name": "Deep Memory Retrieval (DMR)",
            "task_category": "dialogue / summarization / retrieval",
            "performance_with_memory": "DMR: 35.3% (gpt-4-turbo) — reported as baseline in [3].",
            "performance_without_memory": "N/A (this is a baseline memory approach).",
            "has_comparative_results": true,
            "performance_metric": "Accuracy (percentage)",
            "tradeoffs_reported": "Very compact context (low token cost) at the expense of large accuracy loss on the DMR fact-retrieval tasks.",
            "limitations_or_failure_cases": "Fails at fine-grained fact retrieval compared to full-conversation and graph-based memory; shown to be a weak baseline on DMR.",
            "uuid": "e6439.4",
            "source_info": {
                "paper_title": "Zep: A Temporal Knowledge Graph Architecture for Agent Memory",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Memgpt: Towards llms as operating systems",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory",
            "rating": 2,
            "sanitized_title": "longmemeval_benchmarking_chat_assistants_on_longterm_interactive_memory"
        },
        {
            "paper_title": "From local to global: A graph rag approach to query-focused summarization",
            "rating": 2,
            "sanitized_title": "from_local_to_global_a_graph_rag_approach_to_queryfocused_summarization"
        },
        {
            "paper_title": "Arigraph: Learning knowledge graph world models with episodic memory for llm agents",
            "rating": 2,
            "sanitized_title": "arigraph_learning_knowledge_graph_world_models_with_episodic_memory_for_llm_agents"
        },
        {
            "paper_title": "LightRAG: Simple and fast retrieval-augmented generation",
            "rating": 1,
            "sanitized_title": "lightrag_simple_and_fast_retrievalaugmented_generation"
        },
        {
            "paper_title": "Distill-synthkg: Distilling knowledge graph synthesis workflow for improved coverage and efficiency",
            "rating": 1,
            "sanitized_title": "distillsynthkg_distilling_knowledge_graph_synthesis_workflow_for_improved_coverage_and_efficiency"
        }
    ],
    "cost": 0.01419225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Zep: A Temporal Knowledge Graph Architecture for AGENT MEMORY</h1>
<p>Preston Rasmussen<br>Zep AI<br>preston@getzep.com<br>Pavlo Paliychuk<br>Zep AI<br>paul@getzep.com<br>Travis Beauvais<br>Zep AI<br>travis@getzep.com<br>Jack Ryan<br>Zep AI<br>jack@getzep.com<br>Daniel Chalef<br>Zep AI<br>daniel@getzep.com</p>
<h4>Abstract</h4>
<p>We introduce Zep, a novel memory layer service for AI agents that outperforms the current state-of-the-art system, MemGPT, in the Deep Memory Retrieval (DMR) benchmark. Additionally, Zep excels in more comprehensive and challenging evaluations than DMR that better reflect real-world enterprise use cases. While existing retrieval-augmented generation (RAG) frameworks for large language model (LLM)-based agents are limited to static document retrieval, enterprise applications demand dynamic knowledge integration from diverse sources including ongoing conversations and business data. Zep addresses this fundamental limitation through its core component Graphiti-a temporally-aware knowledge graph engine that dynamically synthesizes both unstructured conversational data and structured business data while maintaining historical relationships. In the DMR benchmark, which the MemGPT team established as their primary evaluation metric, Zep demonstrates superior performance ( $94.8 \%$ vs $93.4 \%$ ). Beyond DMR, Zep's capabilities are further validated through the more challenging LongMemEval benchmark, which better reflects enterprise use cases through complex temporal reasoning tasks. In this evaluation, Zep achieves substantial results with accuracy improvements of up to $18.5 \%$ while simultaneously reducing response latency by $90 \%$ compared to baseline implementations. These results are particularly pronounced in enterprisecritical tasks such as cross-session information synthesis and long-term context maintenance, demonstrating Zep's effectiveness for deployment in real-world applications.</p>
<h2>1 Introduction</h2>
<p>The impact of transformer-based large language models (LLMs) on industry and research communities has garnered significant attention in recent years [1]. A major application of LLMs has been the development of chat-based agents. However, these agents' capabilities are limited by the LLMs' context windows, effective context utilization, and knowledge gained during pre-training. Consequently, additional context is required to provide out-of-domain (OOD) knowledge and reduce hallucinations.</p>
<p>Retrieval-Augmented Generation (RAG) has emerged as a key area of interest in LLM-based applications. RAG leverages Information Retrieval (IR) techniques pioneered over the last fifty years[2] to supply necessary domain knowledge to LLMs.</p>
<p>Current approaches using RAG have focused on broad domain knowledge and largely static corpora-that is, document contents added to a corpus seldom change. For agents to become pervasive in our daily lives, autonomously solving problems from trivial to highly complex, they will need access to a large corpus of continuously evolving data from users' interactions with the agent, along with related business and world data. We view empowering agents with this broad and dynamic "memory" as a crucial building block to actualize this vision, and we argue that current RAG approaches are unsuitable for this future. Since entire conversation histories, business datasets, and other domainspecific content cannot fit effectively inside LLM context windows, new approaches need to be developed for agent</p>
<p>memory. Adding memory to LLM-powered agents isn’t a new idea-this concept has been explored previously in MemGPT [3].
Recently, Knowledge Graphs (KGs) have been employed to enhance RAG architectures to address many of the shortcomings of traditional IR techniques[4]. In this paper, we introduce Zep[5], a memory layer service powered by Graphiti[6], a dynamic, temporally-aware knowledge graph engine. Zep ingests and synthesizes both unstructured message data and structured business data. The Graphiti KG engine dynamically updates the knowledge graph with new information in a non-lossy manner, maintaining a timeline of facts and relationships, including their periods of validity. This approach enables the knowledge graph to represent a complex, evolving world.
As Zep is a production system, we’ve focused heavily on the accuracy, latency, and scalability of its memory retrieval mechanisms. We evaluate these mechanisms’ efficacy using two existing benchmarks: a Deep Memory Retrieval task (DMR) from MemGPT[3], as well as the LongMemEval benchmark[7].</p>
<h1>2 Knowledge Graph Construction</h1>
<p>In Zep, memory is powered by a temporally-aware dynamic knowledge graph $\mathcal{G}=(\mathcal{N}, \mathcal{E}, \phi)$, where $\mathcal{N}$ represents nodes, $\mathcal{E}$ represents edges, and $\phi: \mathcal{E} \rightarrow \mathcal{N} \times \mathcal{N}$ represents a formal incidence function. This graph comprises three hierarchical tiers of subgraphs: an episode subgraph, a semantic entity subgraph, and a community subgraph.</p>
<ul>
<li>Episode Subgraph $\mathcal{G}<em i="i">{e}$ : Episodic nodes (episodes), $n</em>} \in \mathcal{N<em i="i">{e}$, contain raw input data in the form of messages, text, or JSON. Episodes serve as a non-lossy data store from which semantic entities and relations are extracted. Episodic edges, $e</em>} \in \mathcal{E<em e="e">{e} \subseteq \phi^{*}\left(\mathcal{N}</em>\right)$, connect episodes to their referenced semantic entities.} \times \mathcal{N}_{s</li>
<li>Semantic Entity Subgraph $\mathcal{G}<em i="i">{s}$ : The semantic entity subgraph builds upon the episode subgraph. Entity nodes (entities), $n</em>} \in \mathcal{N<em i="i">{s}$, represent entities extracted from episodes and resolved with existing graph entities. Entity edges (semantic edges), $e</em>} \in \mathcal{E<em s="s">{s} \subseteq \phi^{*}\left(\mathcal{N}</em>\right)$, represent relationships between entities extracted from episodes.} \times \mathcal{N}_{s</li>
<li>Community Subgraph $\mathcal{G}<em i="i">{c}$ : The community subgraph forms the highest level of Zep's knowledge graph. Community nodes (communities), $n</em>} \in \mathcal{N<em s="s">{c}$, represent clusters of strongly connected entities. Communities contain high-level summarizations of these clusters and represent a more comprehensive, interconnected view of $\mathcal{G}</em>}$ 's structure. Community edges, $e_{i} \in \mathcal{E<em c="c">{c} \subseteq \phi^{*}\left(\mathcal{N}</em>\right)$, connect communities to their entity members.} \times \mathcal{N}_{s</li>
</ul>
<p>The dual storage of both raw episodic data and derived semantic entity information mirrors psychological models of human memory. These models distinguish between episodic memory, which represents distinct events, and semantic memory, which captures associations between concepts and their meanings [8]. This approach enables LLM agents using Zep to develop more sophisticated and nuanced memory structures that better align with our understanding of human memory systems. Knowledge graphs provide an effective medium for representing these memory structures, and our implementation of distinct episodic and semantic subgraphs draws from similar approaches in AriGraph [9].
Our use of community nodes to represent high-level structures and domain concepts builds upon work from GraphRAG [4], enabling a more comprehensive global understanding of the domain. The resulting hierarchical organiza-tion-from episodes to facts to entities to communities-extends existing hierarchical RAG strategies [10][11].</p>
<h3>2.1 Episodes</h3>
<p>Zep's graph construction begins with the ingestion of raw data units called Episodes. Episodes can be one of three core types: message, text, or JSON. While each type requires specific handling during graph construction, this paper focuses on the message type, as our experiments center on conversation memory. In our context, a message consists of relatively short text (several messages can fit within an LLM context window) along with the associated actor who produced the utterance.
Each message includes a reference timestamp $t_{\text {ref }}$ indicating when the message was sent. This temporal information enables Zep to accurately identify and extract relative or partial dates mentioned in the message content (e.g., "next Thursday," "in two weeks," or "last summer"). Zep implements a bi-temporal model, where timeline $T$ represents the chronological ordering of events, and timeline $T^{\prime}$ represents the transactional order of Zep's data ingestion. While the $T^{\prime}$ timeline serves the traditional purpose of database auditing, the $T$ timeline provides an additional dimension for modeling the dynamic nature of conversational data and memory. This bi-temporal approach represents a novel advancement in LLM-based knowledge graph construction and underlies much of Zep's unique capabilities compared to previous graph-based RAG proposals.</p>
<p>The episodic edges, $\mathcal{E}_{e}$, connect episodes to their extracted entity nodes. Episodes and their derived semantic edges maintain bidirectional indices that track the relationships between edges and their source episodes. This design reinforces the non-lossy nature of Graphiti's episodic subgraph by enabling both forward and backward traversal: semantic artifacts can be traced to their sources for citation or quotation, while episodes can quickly retrieve their relevant entities and facts. While these connections are not directly examined in this paper's experiments, they will be explored in future work.</p>
<h1>2.2 Semantic Entities and Facts</h1>
<h3>2.2.1 Entities</h3>
<p>ntity extraction represents the initial phase of episode processing. During ingestion, the system processes both the current message content and the last $n$ messages to provide context for named entity recognition. For this paper and in Zep's general implementation, $n=4$, providing two complete conversation turns for context evaluation. Given our focus on message processing, the speaker is automatically extracted as an entity. Following initial entity extraction, we employ a reflection technique inspired by reflexion[12] to minimize hallucinations and enhance extraction coverage. The system also extracts an entity summary from the episode to facilitate subsequent entity resolution and retrieval operations.
After extraction, the system embeds each entity name into a 1024-dimensional vector space. This embedding enables the retrieval of similar nodes through cosine similarity search across existing graph entity nodes. The system also performs a separate full-text search on existing entity names and summaries to identify additional candidate nodes. These candidate nodes, together with the episode context, are then processed through an LLM using our entity resolution prompt. When the system identifies a duplicate entity, it generates an updated name and summary.
Following entity extraction and resolution, the system incorporates the data into the knowledge graph using predefined Cypher queries. We chose this approach over LLM-generated database queries to ensure consistent schema formats and reduce the potential for hallucinations.
Selected prompts for graph construction are provided in the appendix.</p>
<h3>2.2.2 Facts</h3>
<p>or each fact containing its key predicate. Importantly, the same fact can be extracted multiple times between different entities, enabling Graphiti to model complex multi-entity facts through an implementation of hyper-edges.
Following extraction, the system generates embeddings for facts in preparation for graph integration. The system performs edge deduplication through a process similar to entity resolution. The hybrid search for relevant edges is constrained to edges existing between the same entity pairs as the proposed new edge. This constraint not only prevents erroneous combinations of similar edges between different entities but also significantly reduces the computational complexity of the deduplication process by limiting the search space to a subset of edges relevant to the specific entity pair.</p>
<h3>2.2.3 Temporal Extraction and Edge Invalidation</h3>
<p>A key differentiating feature of Graphiti compared to other knowledge graph engines is its capacity to manage dynamic information updates through temporal extraction and edge invalidation processes.
The system extracts temporal information about facts from the episode context using $t_{\text {ref }}$. This enables accurate extraction and datetime representation of both absolute timestamps (e.g., "Alan Turing was born on June 23, 1912") and relative timestamps (e.g., "I started my new job two weeks ago"). Consistent with our bi-temporal modeling approach, the system tracks four timestamps: $t^{\prime}$ created and $t^{\prime}$ expired $\in T^{\prime}$ monitor when facts are created or invalidated in the system, while $t_{\text {valid }}$ and $t_{\text {invalid }} \in T$ track the temporal range during which facts held true. These temporal data points are stored on edges alongside other fact information.
The introduction of new edges can invalidate existing edges in the database. The system employs an LLM to compare new edges against semantically related existing edges to identify potential contradictions. When the system identifies temporally overlapping contradictions, it invalidates the affected edges by setting their $t_{\text {invalid }}$ to the $t_{\text {valid }}$ of the invalidating edge. Following the transactional timeline $T^{\prime}$, Graphiti consistently prioritizes new information when determining edge invalidation.
This comprehensive approach enables the dynamic addition of data to Graphiti as conversations evolve, while maintaining both current relationship states and historical records of relationship evolution over time.</p>
<p>Using Knowledge Graphs to power LLM-Agent Memory</p>
<h3>2.3 Communities</h3>
<p>After establishing the episodic and semantic subgraphs, the system constructs the community subgraph through community detection. While our community detection approach builds upon the technique described in GraphRAG [4], we employ a label propagation algorithm [13] rather than the Leiden algorithm [14]. This choice was influenced by label propagation's straightforward dynamic extension, which enables the system to maintain accurate community representations for longer periods as new data enters the graph, delaying the need for complete community refreshes.</p>
<p>The dynamic extension implements the logic of a single recursive step in label propagation. When the system adds a new entity node $n_{i} \in \mathcal{N}_{s}$ to the graph, it surveys the communities of neighboring nodes. The system assigns the new node to the community held by the plurality of its neighbors, then updates the community summary and graph accordingly. While this dynamic updating enables efficient community extension as data flows into the system, the resulting communities gradually diverge from those that would be generated by a complete label propagation run. Therefore, periodic community refreshes remain necessary. However, this dynamic updating strategy provides a practical heuristic that significantly reduces latency and LLM inference costs.</p>
<p>Following [4], our community nodes contain summaries derived through an iterative map-reduce-style summarization of member nodes. However, our retrieval methods differ substantially from GraphRAG's map-reduce approach [4]. To support our retrieval methodology, we generate community names containing key terms and relevant subjects from the community summaries. These names are embedded and stored to enable cosine similarity searches.</p>
<h2>3 Memory Retrieval</h2>
<p>The memory retrieval system in Zep provides powerful, complex, and highly configurable functionality. At a high level, the Zep graph search API implements a function $f: S \rightarrow S$ that accepts a text-string query $\alpha \in S$ as input and returns a text-string context $\beta \in S$ as output. The output $\beta$ contains formatted data from nodes and edges required for an LLM agent to generate an accurate response to query $\alpha$. The process $f(\alpha) \rightarrow \beta$ comprises three distinct steps:</p>
<ul>
<li>Search $(\varphi)$ : The process begins by identifying candidate nodes and edges potentially containing relevant information. While Zep employs multiple distinct search methods, the overall search function can be represented as $\varphi: S \rightarrow \mathcal{E}<em s="s">{s}^{n} \times \mathcal{N}</em>$. Thus, $\varphi$ transforms a query into a 3-tuple containing lists of semantic edges, entity nodes, and community nodes-the three graph types containing relevant textual information.}^{n} \times \mathcal{N}_{c}^{n</li>
<li>$\operatorname{Reranker}(\rho)$ : The second step reorders search results. A reranker function or model accepts a list of search results and produces a reordered version of those results: $\rho: \varphi(\alpha), \ldots \rightarrow \mathcal{E}<em s="s">{s}^{n} \times \mathcal{N}</em>$.}^{n} \times \mathcal{N}_{c}^{n</li>
<li>Constructor $(\chi)$ : The final step, the constructor, transforms the relevant nodes and edges into text context: $\chi: \mathcal{E}<em s="s">{s}^{n} \times \mathcal{N}</em>}^{n} \times \mathcal{N} c^{n} \rightarrow S$. For each $e_{i} \in \mathcal{E} s, \chi$ returns the fact and $t$ valid, $t$ invalid fields; for each $n_{i} \in \mathcal{N<em i="i">{s}$, the name and summary fields; and for each $n</em>$, the summary field.} \in \mathcal{N}_{c</li>
</ul>
<p>With these definitions established, we can express $f$ as a composition of these three components: $f(\alpha)=$ $\chi(\rho(\varphi(\alpha)))=\beta$.</p>
<p>Sample context string template:
FACTS and ENTITIES represent relevant context to the current conversation.
These are the most relevant facts and their valid date ranges. If the fact is about an event, the event takes place during this time.
format: FACT (Date range: from - to)
$&lt;$ FACTS $&gt;$
{facts }
$&lt;/$ FACTS $&gt;$
These are the most relevant entities
ENTITY_NAME: entity summary
$&lt;$ ENTITIES $&gt;$
{entities}
$&lt;/$ ENTITIES $&gt;$</p>
<h1>3.1 Search</h1>
<p>Zep implements three search functions: cosine semantic similarity search $\left(\varphi_{\text {sos }}\right)$, Okapi BM25 full-text search $\left(\varphi_{\text {bm25 }}\right)$, and breadth-first search $\left(\varphi_{\text {bfs }}\right)$. The first two functions utilize Neo4j's implementation of Lucene [15][16]. Each</p>
<p>search function offers distinct capabilities in identifying relevant documents, and together they provide comprehensive coverage of candidate results before reranking. The search field varies across the three object types: for $\mathcal{E}<em s="s">{s}$, we search the fact field; for $\mathcal{N}</em>}$, the entity name; and for $\mathcal{N<em _mathrm_bfs="\mathrm{bfs">{c}$, the community name, which comprises relevant keywords and phrases covered in the community. While developed independently, our community search approach parallels the high-level key search methodology in LightRAG [17]. The hybridization of LightRAG's approach with graph-based systems like Graphiti presents a promising direction for future research.
While cosine similarity and full-text search methodologies are well-established in RAG [18], breadth-first search over knowledge graphs has received limited attention in the RAG domain, with notable exceptions in graph-based RAG systems such as AriGraph [9] and Distill-SynthKG [19]. In Graphiti, the breadth-first search enhances initial search results by identifying additional nodes and edges within $n$-hops. Moreover, $\varphi</em>$ can accept nodes as parameters for the search, enabling greater control over the search function. This functionality proves particularly valuable when using recent episodes as seeds for the breadth-first search, allowing the system to incorporate recently mentioned entities and relationships into the retrieved context.
The three search methods each target different aspects of similarity: full-text search identifies word similarities, cosine similarity captures semantic similarities, and breadth-first search reveals contextual similarities-where nodes and edges closer in the graph appear in more similar conversational contexts. This multi-faceted approach to candidate result identification maximizes the likelihood of discovering optimal context.}</p>
<h1>3.2 Reranker</h1>
<p>While the initial search methods aim to achieve high recall, rerankers serve to increase precision by prioritizing the most relevant results. Zep supports existing reranking approaches such as Reciprocal Rank Fusion (RRF) [20] and Maximal Marginal Relevance (MMR) [21]. Additionally, Zep implements a graph-based episode-mentions reranker that prioritizes results based on the frequency of entity or fact mentions within a conversation, enabling a system where frequently referenced information becomes more readily accessible. The system also includes a node distance reranker that reorders results based on their graph distance from a designated centroid node, providing context localized to specific areas of the knowledge graph. The system's most sophisticated reranking capability employs cross-encoders-LLM s that generate relevance scores by evaluating nodes and edges against queries using cross-attention, though this approach incurs the highest computational cost.</p>
<h2>4 Experiments</h2>
<p>This section analyzes two experiments conducted using LLM-memory based benchmarks. The first evaluation employs the Deep Memory Retrieval (DMR) task developed in [3], which uses a 500-conversation subset of the Multi-Session Chat dataset introduced in "Beyond Goldfish Memory: Long-Term Open-Domain Conversation" [22]. The second evaluation utilizes the LongMemEval benchmark from "LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory" [7]. Specifically, we use the LongMemEval $_{s}$ dataset, which provides an extensive conversation context of on average 115,000 tokens.
For both experiments, we integrate the conversation history into a Zep knowledge graph through Zep's APIs. We then retrieve the 20 most relevant edges (facts) and entity nodes (entity summaries) using the techniques described in Section 3. The system reformats this data into a context string, matching the functionality provided by Zep's memory APIs.
While these experiments demonstrate key retrieval capabilities of Graphiti, they represent a subset of the system's full search functionality. This focused scope enables clear comparison with existing benchmarks while reserving the exploration of additional knowledge graph capabilities for future work.</p>
<h3>4.1 Choice of models</h3>
<p>Our experimental implementation employs the BGE-m3 models from BAAI for both reranking and embedding tasks [23] [24]. For graph construction and response generation, we utilize gpt-4o-mini-2024-07-18 for graph construction, and both gpt-4o-mini-2024-07-18 and gpt-4o-2024-11-20 for the chat agent generating responses to provided context.
To ensure direct comparability with MemGPT's DMR results, we also conducted the DMR evaluation using gpt-4-turbo-2024-04-09.
The experimental notebooks will be made publicly available through our GitHub repository, and relevant experimental prompts are included in the Appendix.</p>
<p>Table 1: Deep Memory Retrieval</p>
<table>
<thead>
<tr>
<th>Memory</th>
<th>Model</th>
<th>Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>Recursive Summarization ${ }^{\dagger}$</td>
<td>gpt-4-turbo</td>
<td>$35.3 \%$</td>
</tr>
<tr>
<td>Conversation Summaries</td>
<td>gpt-4-turbo</td>
<td>$78.6 \%$</td>
</tr>
<tr>
<td>MemGPT ${ }^{\dagger}$</td>
<td>gpt-4-turbo</td>
<td>$93.4 \%$</td>
</tr>
<tr>
<td>Full-conversation</td>
<td>gpt-4-turbo</td>
<td>$94.4 \%$</td>
</tr>
<tr>
<td>Zep</td>
<td>gpt-4-turbo</td>
<td>$\mathbf{9 4 . 8 \%}$</td>
</tr>
<tr>
<td>Conversation Summaries</td>
<td>gpt-4o-mini</td>
<td>$88.0 \%$</td>
</tr>
<tr>
<td>Full-conversation</td>
<td>gpt-4o-mini</td>
<td>$98.0 \%$</td>
</tr>
<tr>
<td>Zep</td>
<td>gpt-4o-mini</td>
<td>$\mathbf{9 8 . 2 \%}$</td>
</tr>
</tbody>
</table>
<p>${ }^{\dagger}$ Results reported in [3].</p>
<h1>4.2 Deep Memory Retrieval (DMR)</h1>
<p>The Deep Memory Retrieval evaluation, introduced by [3], comprises 500 multi-session conversations, each containing 5 chat sessions with up to 12 messages per session. Each conversation includes a question/answer pair for memory evaluation. The MemGPT framework [3] currently leads performance metrics with $93.4 \%$ accuracy using gpt-4-turbo, a significant improvement over the $35.3 \%$ baseline achieved through recursive summarization.
To establish comparative baselines, we implemented two common LLM memory approaches: full-conversation context and session summaries. Using gpt-4-turbo, the full-conversation baseline achieved $94.4 \%$ accuracy, slightly surpassing MemGPT's reported results, while the session summary baseline achieved $78.6 \%$. When using gpt-4o-mini, both approaches showed improved performance: $98.0 \%$ for full-conversation and $88.0 \%$ for session summaries. We were unable to reproduce MemGPT's results using gpt-4o-mini due to insufficient methodological details in their published work.
We then evaluated Zep's performance by ingesting the conversations and using its search functions to retrieve the top 10 most relevant nodes and edges. An LLM judge compared the agent's responses to the provided golden answers. Zep achieved $94.8 \%$ accuracy with gpt-4-turbo and $98.2 \%$ with gpt-4o-mini, showing marginal improvements over both MemGPT and the respective full-conversation baselines. However, these results must be contextualized: each conversation contains only 60 messages, easily fitting within current LLM context windows.
The limitations of the DMR evaluation extend beyond its small scale. Our analysis revealed significant weaknesses in the benchmark's design. The evaluation relies exclusively on single-turn, fact-retrieval questions that fail to assess complex memory understanding. Many questions contain ambiguous phrasing, referencing concepts like "favorite drink to relax with" or "weird hobby" that were not explicitly characterized as such in the conversations. Most critically, the dataset poorly represents real-world enterprise use cases for LLM agents. The high performance achieved by simple full-context approaches using modern LLMs further highlights the benchmark's inadequacy for evaluating memory systems.
This inadequacy is further emphasized by findings in [7], which demonstrate rapidly declining LLM performance on the LongMemEval benchmark as conversation length increases. The LongMemEval dataset [7] addresses many of these shortcomings by presenting longer, more coherent conversations that better reflect enterprise scenarios, along with more diverse evaluation questions.</p>
<h3>4.3 LongMemEval (LME)</h3>
<p>We evaluated Zep using the LongMemEvals dataset, which provides conversations and questions representative of realworld business applications of LLM agents. The LongMemEvals dataset presents significant challenges to existing LLMs and commercial memory solutions [7], with conversations averaging approximately 115,000 tokens in length. This length, while substantial, remains within the context windows of current frontier models, enabling us to establish meaningful baselines for evaluating Zep's performance.
The dataset incorporates six distinct question types: single-session-user, single-session-assistant, single-sessionpreference, multi-session, knowledge-update, and temporal-reasoning. These categories are not uniformly distributed throughout the dataset; for detailed distribution information, we refer readers to [7].
We conducted all experiments between December 2024 and January 2025. We performed testing using a consumer laptop from a residential location in Boston, MA, connecting to Zep's service hosted in AWS us-west-2. This dis-</p>
<p>tributed architecture introduced additional network latency when evaluating Zep's performance, though this latency was not present in our baseline evaluations.</p>
<p>For answer evaluation, we employed GPT-4o with the question-specific prompts provided in [7], which have demonstrated high correlation with human evaluators.</p>
<h1>4.3.1 LongMemEval and MemGPT</h1>
<p>To establish a comparative benchmark between Zep and the current state-of-the-art MemGPT system [3], we attempted to evaluate MemGPT using the LongMemEval dataset. Given that the current MemGPT framework does not support direct ingestion of existing message histories, we implemented a workaround by adding conversation messages to the archival history. However, we were unable to achieve successful question responses using this approach. We look forward to seeing evaluations of this benchmark by other research teams, as comparative performance data would benefit the broader development of LLM memory systems.</p>
<h3>4.3.2 LongMemEval results</h3>
<p>Zep demonstrates substantial improvements in both accuracy and latency compared to the baseline across both model variants. Using gpt-4o-mini, Zep achieved a $15.2 \%$ accuracy improvement over the baseline, while gpt-4o showed an $18.5 \%$ improvement. The reduced prompt size also led to significant latency cost reductions compared to the baseline implementations.</p>
<p>Table 2: LongMemEval $_{\mathrm{a}}$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Memory</th>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Score</th>
<th style="text-align: left;">Latency</th>
<th style="text-align: left;">Latency IQR</th>
<th style="text-align: left;">Avg Context Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Full-context</td>
<td style="text-align: left;">gpt-4o-mini</td>
<td style="text-align: left;">$55.4 \%$</td>
<td style="text-align: left;">31.3 s</td>
<td style="text-align: left;">8.76 s</td>
<td style="text-align: left;">115 k</td>
</tr>
<tr>
<td style="text-align: left;">Zep</td>
<td style="text-align: left;">gpt-4o-mini</td>
<td style="text-align: left;">$\mathbf{6 3 . 8 \%}$</td>
<td style="text-align: left;">$\mathbf{3 . 2 0} \mathbf{~ s}$</td>
<td style="text-align: left;">1.31 s</td>
<td style="text-align: left;">$\mathbf{1 . 6 k}$</td>
</tr>
<tr>
<td style="text-align: left;">Full-context</td>
<td style="text-align: left;">gpt-4o</td>
<td style="text-align: left;">$60.2 \%$</td>
<td style="text-align: left;">28.9 s</td>
<td style="text-align: left;">6.01 s</td>
<td style="text-align: left;">115 k</td>
</tr>
<tr>
<td style="text-align: left;">Zep</td>
<td style="text-align: left;">gpt-4o</td>
<td style="text-align: left;">$\mathbf{7 1 . 2 \%}$</td>
<td style="text-align: left;">$\mathbf{2 . 5 8} \mathbf{~ s}$</td>
<td style="text-align: left;">0.684 s</td>
<td style="text-align: left;">$\mathbf{1 . 6 k}$</td>
</tr>
</tbody>
</table>
<p>Analysis by question type reveals that gpt-4o-mini with Zep showed improvements in four of the six categories, with the most substantial gains in complex question types: single-session-preference, multi-session, and temporalreasoning. When using gpt-4o, Zep further demonstrated improved performance in the knowledge-update category, highlighting its effectiveness with more capable models. However, additional development may be needed to improve less capable models' understanding of Zep's temporal data.</p>
<p>Table 3: LongMemEval ${ }_{\mathrm{a}}$ Question Type Breakdown</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question Type</th>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Full-context</th>
<th style="text-align: left;">Zep</th>
<th style="text-align: left;">Delta</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">single-session-preference</td>
<td style="text-align: left;">gpt-4o-mini</td>
<td style="text-align: left;">$30.0 \%$</td>
<td style="text-align: left;">$\mathbf{5 3 . 3 \%}$</td>
<td style="text-align: left;">$\mathbf{7 7 . 7 \%} \uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">single-session-assistant</td>
<td style="text-align: left;">gpt-4o-mini</td>
<td style="text-align: left;">$\mathbf{8 1 . 8 \%}$</td>
<td style="text-align: left;">$75.0 \%$</td>
<td style="text-align: left;">$9.06 \% \downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">temporal-reasoning</td>
<td style="text-align: left;">gpt-4o-mini</td>
<td style="text-align: left;">$36.5 \%$</td>
<td style="text-align: left;">$\mathbf{5 4 . 1 \%}$</td>
<td style="text-align: left;">$\mathbf{4 8 . 2 \%} \uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">multi-session</td>
<td style="text-align: left;">gpt-4o-mini</td>
<td style="text-align: left;">$40.6 \%$</td>
<td style="text-align: left;">$\mathbf{4 7 . 4 \%}$</td>
<td style="text-align: left;">$\mathbf{1 6 . 7 \%} \uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">knowledge-update</td>
<td style="text-align: left;">gpt-4o-mini</td>
<td style="text-align: left;">$\mathbf{7 6 . 9 \%}$</td>
<td style="text-align: left;">$74.4 \%$</td>
<td style="text-align: left;">$3.36 \% \downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">single-session-user</td>
<td style="text-align: left;">gpt-4o-mini</td>
<td style="text-align: left;">$81.4 \%$</td>
<td style="text-align: left;">$\mathbf{9 2 . 9 \%}$</td>
<td style="text-align: left;">$\mathbf{1 4 . 1 \%} \uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">single-session-preference</td>
<td style="text-align: left;">gpt-4o</td>
<td style="text-align: left;">$20.0 \%$</td>
<td style="text-align: left;">$\mathbf{5 6 . 7 \%}$</td>
<td style="text-align: left;">$\mathbf{1 8 4 \%} \uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">single-session-assistant</td>
<td style="text-align: left;">gpt-4o</td>
<td style="text-align: left;">$\mathbf{9 4 . 6 \%}$</td>
<td style="text-align: left;">$80.4 \%$</td>
<td style="text-align: left;">$17.7 \% \downarrow$</td>
</tr>
<tr>
<td style="text-align: left;">temporal-reasoning</td>
<td style="text-align: left;">gpt-4o</td>
<td style="text-align: left;">$45.1 \%$</td>
<td style="text-align: left;">$\mathbf{6 2 . 4 \%}$</td>
<td style="text-align: left;">$\mathbf{3 8 . 4 \%} \uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">multi-session</td>
<td style="text-align: left;">gpt-4o</td>
<td style="text-align: left;">$44.3 \%$</td>
<td style="text-align: left;">$\mathbf{5 7 . 9 \%}$</td>
<td style="text-align: left;">$\mathbf{3 0 . 7 \%} \uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">knowledge-update</td>
<td style="text-align: left;">gpt-4o</td>
<td style="text-align: left;">$78.2 \%$</td>
<td style="text-align: left;">$\mathbf{8 3 . 3 \%}$</td>
<td style="text-align: left;">$\mathbf{6 . 5 2 \%} \uparrow$</td>
</tr>
<tr>
<td style="text-align: left;">single-session-user</td>
<td style="text-align: left;">gpt-4o</td>
<td style="text-align: left;">$81.4 \%$</td>
<td style="text-align: left;">$\mathbf{9 2 . 9 \%}$</td>
<td style="text-align: left;">$\mathbf{1 4 . 1 \%} \uparrow$</td>
</tr>
</tbody>
</table>
<p>These results demonstrate Zep's ability to enhance performance across model scales, with the most pronounced improvements observed in complex and nuanced question types when paired with more capable models. The latency improvements are particularly noteworthy, with Zep reducing response times by approximately $90 \%$ while maintaining higher accuracy.</p>
<p>The decrease in performance for single-session-assistant questions-17.7\% for gpt-40 and $9.06 \%$ for gpt-40-mini-represents a notable exception to Zep's otherwise consistent improvements, and suggest further research and engineering work is needed.</p>
<h1>5 Conclusion</h1>
<p>We have introduced Zep, a graph-based approach to LLM memory that incorporates semantic and episodic memory alongside entity and community summaries. Our evaluations demonstrate that Zep achieves state-of-the-art performance on existing memory benchmarks while reducing token costs and operating at significantly lower latencies.
The results achieved with Graphiti and Zep, while impressive, likely represent only initial advances in graph-based memory systems. Multiple research avenues could build upon these frameworks, including integration of other GraphRAG approaches into the Zep paradigm and novel extensions of our work.
Research has already demonstrated the value of fine-tuned models for LLM-based entity and edge extraction within the GraphRAG paradigm, improving accuracy while reducing costs and latency [19][25]. Similar models finetuned for Graphiti prompts may enhance knowledge extraction, particularly for complex conversations. Additionally, while current research on LLM-generated knowledge graphs has primarily operated without formal ontologies [9][4][17][19][26], domain-specific ontologies present significant potential. Graph ontologies, foundational in preLLM knowledge graph work, warrant further exploration within the Graphiti framework.
Our search for suitable memory benchmarks revealed limited options, with existing benchmarks often lacking robustness and complexity, frequently defaulting to simple needle-in-a-haystack fact-retrieval questions [3]. The field requires additional memory benchmarks, particularly those reflecting business applications like customer experience tasks, to effectively evaluate and differentiate memory approaches. Notably, no existing benchmarks adequately assess Zep's capability to process and synthesize conversation history with structured business data. While Zep focuses on LLM memory, its traditional RAG capabilities should be evaluated against established benchmarks such as those in [17], [27], and [28].
Current literature on LLM memory and RAG systems insufficiently addresses production system scalability in terms of cost and latency. We have included latency benchmarks for our retrieval mechanisms to begin addressing this gap, following the example set by LightRAG's authors in prioritizing these metrics.</p>
<h2>6 Appendix</h2>
<h3>6.1 Graph Construction Prompts</h3>
<h3>6.1.1 Entity Extraction</h3>
<div class="codehilite"><pre><span></span><code><span class="nt">&lt;PREVIOUS</span><span class="w"> </span><span class="err">MESSAGES</span><span class="nt">&gt;</span>
{previous_messages}
<span class="err">&lt;</span>/PREVIOUS<span class="w"> </span>MESSAGES&gt;
<span class="nt">&lt;CURRENT</span><span class="w"> </span><span class="err">MESSAGE</span><span class="nt">&gt;</span>
{current_message}
<span class="err">&lt;</span>/CURRENT<span class="w"> </span>MESSAGE&gt;
</code></pre></div>

<p>Given the above conversation, extract entity nodes from the CURRENT MESSAGE that are explicitly or implicitly mentioned:
Guidelines:</p>
<ol>
<li>ALWAYS extract the speaker/actor as the first node. The speaker is the part before the colon in each line of dialogue.</li>
<li>Extract other significant entities, concepts, or actors mentioned in the CURRENT MESSAGE.</li>
<li>DO NOT create nodes for relationships or actions.</li>
<li>DO NOT create nodes for temporal information like dates, times or years (these will be added to edges later).</li>
<li>Be as explicit as possible in your node names, using full names.</li>
<li>DO NOT extract entities mentioned only</li>
</ol>
<h1>6.1.2 Entity Resolution</h1>
<div class="codehilite"><pre><span></span><code><span class="nt">&lt;PREVIOUS</span><span class="w"> </span><span class="err">MESSAGES</span><span class="nt">&gt;</span>
{previous_messages}
<span class="err">&lt;</span>/PREVIOUS<span class="w"> </span>MESSAGES&gt;
<span class="nt">&lt;CURRENT</span><span class="w"> </span><span class="err">MESSAGE</span><span class="nt">&gt;</span>
{current_message}
<span class="err">&lt;</span>/CURRENT<span class="w"> </span>MESSAGE&gt;
<span class="nt">&lt;EXISTING</span><span class="w"> </span><span class="err">NODES</span><span class="nt">&gt;</span>
{existing_nodes}
<span class="err">&lt;</span>/EXISTING<span class="w"> </span>NODES&gt;
Given<span class="w"> </span>the<span class="w"> </span>above<span class="w"> </span>EXISTING<span class="w"> </span>NODES,<span class="w"> </span>MESSAGE,<span class="w"> </span>and<span class="w"> </span>PREVIOUS<span class="w"> </span>MESSAGES.<span class="w"> </span>Determine<span class="w"> </span>if<span class="w"> </span>the<span class="w"> </span>NEW<span class="w"> </span>NODE
extracted<span class="w"> </span>from<span class="w"> </span>the<span class="w"> </span>conversation<span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>duplicate<span class="w"> </span>entity<span class="w"> </span>of<span class="w"> </span>one<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>EXISTING<span class="w"> </span>NODES.
<span class="nt">&lt;NEW</span><span class="w"> </span><span class="err">NODE</span><span class="nt">&gt;</span>
{new_node}
<span class="err">&lt;</span>/NEW<span class="w"> </span>NODE&gt;
Task:
1.<span class="w"> </span>If<span class="w"> </span>the<span class="w"> </span>New<span class="w"> </span>Node<span class="w"> </span>represents<span class="w"> </span>the<span class="w"> </span>same<span class="w"> </span>entity<span class="w"> </span>as<span class="w"> </span>any<span class="w"> </span>node<span class="w"> </span>in<span class="w"> </span>Existing<span class="w"> </span>Nodes,<span class="w"> </span>return<span class="w"> </span>&#39;is_duplicate:<span class="w"> </span>true&#39;<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>response.
Otherwise,<span class="w"> </span>return<span class="w"> </span>&#39;is_duplicate:<span class="w"> </span>false&#39;
2.<span class="w"> </span>If<span class="w"> </span>is_duplicate<span class="w"> </span>is<span class="w"> </span>true,<span class="w"> </span>also<span class="w"> </span>return<span class="w"> </span>the<span class="w"> </span>uuid<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>existing<span class="w"> </span>node<span class="w"> </span>in<span class="w"> </span>the<span class="w"> </span>response
3.<span class="w"> </span>If<span class="w"> </span>is_duplicate<span class="w"> </span>is<span class="w"> </span>true,<span class="w"> </span>return<span class="w"> </span>a<span class="w"> </span>name<span class="w"> </span>for<span class="w"> </span>the<span class="w"> </span>node<span class="w"> </span>that<span class="w"> </span>is<span class="w"> </span>the<span class="w"> </span>most<span class="w"> </span>complete<span class="w"> </span>full<span class="w"> </span>name.
Guidelines:
1.<span class="w"> </span>Use<span class="w"> </span>both<span class="w"> </span>the<span class="w"> </span>name<span class="w"> </span>and<span class="w"> </span>summary<span class="w"> </span>of<span class="w"> </span>nodes<span class="w"> </span>to<span class="w"> </span>determine<span class="w"> </span>if<span class="w"> </span>the<span class="w"> </span>entities<span class="w"> </span>are<span class="w"> </span>duplicates,<span class="w"> </span>duplicate<span class="w"> </span>nodes<span class="w"> </span>may<span class="w"> </span>have
different<span class="w"> </span>names
</code></pre></div>

<h3>6.1.3 Fact Extraction</h3>
<div class="codehilite"><pre><span></span><code><span class="nt">&lt;PREVIOUS</span><span class="w"> </span><span class="err">MESSAGES</span><span class="nt">&gt;</span>
{previous_messages}
<span class="err">&lt;</span>/PREVIOUS<span class="w"> </span>MESSAGES&gt;
<span class="nt">&lt;CURRENT</span><span class="w"> </span><span class="err">MESSAGE</span><span class="nt">&gt;</span>
{current_message}
<span class="err">&lt;</span>/CURRENT<span class="w"> </span>MESSAGE&gt;
<span class="nt">&lt;ENTITIES&gt;</span>
{entities}
<span class="nt">&lt;/ENTITIES&gt;</span>
Given<span class="w"> </span>the<span class="w"> </span>above<span class="w"> </span>MESSAGES<span class="w"> </span>and<span class="w"> </span>ENTITIES,<span class="w"> </span>extract<span class="w"> </span>all<span class="w"> </span>facts<span class="w"> </span>pertaining<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>listed<span class="w"> </span>ENTITIES<span class="w"> </span>from<span class="w"> </span>the<span class="w"> </span>CURRENT
MESSAGE.
Guidelines:
1.<span class="w"> </span>Extract<span class="w"> </span>facts<span class="w"> </span>only<span class="w"> </span>between<span class="w"> </span>the<span class="w"> </span>provided<span class="w"> </span>entities.
2.<span class="w"> </span>Each<span class="w"> </span>fact<span class="w"> </span>should<span class="w"> </span>represent<span class="w"> </span>a<span class="w"> </span>clear<span class="w"> </span>relationship<span class="w"> </span>between<span class="w"> </span>two<span class="w"> </span>DISTINCT<span class="w"> </span>nodes.
3.<span class="w"> </span>The<span class="w"> </span>relation_type<span class="w"> </span>should<span class="w"> </span>be<span class="w"> </span>a<span class="w"> </span>concise,<span class="w"> </span>all-caps<span class="w"> </span>description<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>fact<span class="w"> </span>(e.g.,<span class="w"> </span>LOVES,<span class="w"> </span>IS_FRIENDS_WITH,
WORKS_FOR).
4.<span class="w"> </span>Provide<span class="w"> </span>a<span class="w"> </span>more<span class="w"> </span>detailed<span class="w"> </span>fact<span class="w"> </span>containing<span class="w"> </span>all<span class="w"> </span>relevant<span class="w"> </span>information.
5.<span class="w"> </span>Consider<span class="w"> </span>temporal<span class="w"> </span>aspects<span class="w"> </span>of<span class="w"> </span>relationships<span class="w"> </span>when<span class="w"> </span>relevant.

#<span class="w"> </span>6.1.4<span class="w"> </span>Fact<span class="w"> </span>Resolution<span class="w"> </span>
</code></pre></div>

<p>Given the following context, determine whether the New Edge represents any of the edges in the list of Existing Edges.
<EXISTING EDGES>
{existing_edges}
</EXISTING EDGES>
<NEW EDGE>
{new_edge}
</NEW EDGE>
Task:
1. If the New Edges represents the same factual information as any edge in Existing Edges, return 'is_duplicate: true'
in the response. Otherwise, return 'is_duplicate: false'
2. If is_duplicate is true, also return the uuid of the existing edge in the response
Guidelines:
1. The facts do not need to be completely identical to be duplicates, they just need to express the same information.</p>
<div class="codehilite"><pre><span></span><code>### 6.1.5 Temporal Extraction
</code></pre></div>

<p><PREVIOUS MESSAGES>
[previous_messages]
</PREVIOUS MESSAGES>
<CURRENT MESSAGE>
{current_message}
</CURRENT MESSAGE>
<REFERENCE TIMESTAMP>
{reference_timestamp}
</REFERENCE TIMESTAMP>
<FACT>
{fact}
</FACT>
IMPORTANT: Only extract time information if it is part of the provided fact. Otherwise ignore the time mentioned.
Make sure to do your best to determine the dates if only the relative time is mentioned. (eg 10 years ago, 2 mins ago)
based on the provided reference timestamp
If the relationship is not of spanning nature, but you are still able to determine the dates, set the valid_at only.
Definitions:
- valid_at: The date and time when the relationship described by the edge fact became true or was established.
- invalid_at: The date and time when the relationship described by the edge fact stopped being true or ended.
Task:
Analyze the conversation and determine if there are dates that are part of the edge fact. Only set dates if they explicitly
relate to the formation or alteration of the relationship itself.
Guidelines:
1. Use ISO 8601 format (YYYY-MM-DDTHH:MM:SS.SSSSSSZ) for datetimes.
2. Use the reference timestamp as the current time when determining the valid_at and invalid_at dates.
3. If the fact is written in the present tense, use the Reference Timestamp for the valid_at date
4. If no temporal information is found that establishes or changes the relationship, leave the fields as null.
5. Do not infer dates from related events. Only use dates that are directly stated to establish or change the relationship.
6. For relative time mentions directly related to the relationship, calculate the actual datetime based on the reference
timestamp.
7. If only a date is mentioned without a specific time, use 00:00:00 (midnight) for that date.
8. If only year is mentioned, use January 1st of that year at 00:00:00.
9. Always include the time zone offset (use Z for UTC if no specific time zone is mentioned).</p>
<h2>References</h2>
<p>[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.
[2] K. Sparck Jones. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):11-21, 1972.</p>
<p>[3] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Memgpt: Towards llms as operating systems, 2024.
[4] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. From local to global: A graph rag approach to query-focused summarization, 2024.
[5] Zep. Zep: Long-term memory for ai agents. https://www.getzep.com, 2024. Commercial memory layer for AI applications.
[6] Zep. Graphiti: Temporal knowledge graphs for agentic applications. https://github.com/getzep/graphiti, 2024. Graphiti builds dynamic, temporally aware Knowledge Graphs that represent complex, evolving relationships between entities over time.
[7] Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. Longmemeval: Benchmarking chat assistants on long-term interactive memory, 2024.
[8] Wong Gonzalez and Daniela. The relationship between semantic and episodic memory: Exploring the effect of semantic neighbourhood density on episodic memory. PhD thesis, University of Winsor, 2018.
[9] Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, and Evgeny Burnaev. Arigraph: Learning knowledge graph world models with episodic memory for llm agents, 2024.
[10] Xinyue Chen, Pengyu Gao, Jiangjiang Song, and Xiaoyang Tan. Hiqa: A hierarchical contextual augmentation rag for multi-documents qa, 2024.
[11] Krish Goel and Mahek Chandak. Hiro: Hierarchical information retrieval optimization, 2024.
[12] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023.
[13] Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label propagation. 2002.
[14] V. A. Traag, L. Waltman, and N. J. van Eck. From louvain to leiden: guaranteeing well-connected communities. Sci Rep 9, 5233, 2019.
[15] Neo4j. Neo4j - the world's leading graph database, 2012.
[16] Apache Software Foundation. Apache lucene - scoring, 2011. letzter Zugriff: 20. Oktober 2011.
[17] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrieval-augmented generation, 2024.
[18] Jimmy Lin, Ronak Pradeep, Tommaso Teofili, and Jasper Xian. Vector search with openai embeddings: Lucene is all you need, 2023.
[19] Prafulla Kumar Choubey, Xin Su, Man Luo, Xiangyu Peng, Caiming Xiong, Tiep Le, Shachar Rosenman, Vasudev Lal, Phil Mui, Ricky Ho, Phillip Howard, and Chien-Sheng Wu. Distill-synthkg: Distilling knowledge graph synthesis workflow for improved coverage and efficiency, 2024.
[20] Gordon V. Cormack, Charles L. A. Clarke, and Stefan Buettcher. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '09, pages 758-759. ACM, 2009.
[21] Jaime Carbonell and Jade Goldstein. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '98, page 335-336, New York, NY, USA, 1998. Association for Computing Machinery.
[22] Jing Xu, Arthur Szlam, and Jason Weston. Beyond goldfish memory: Long-term open-domain conversation, 2021.
[23] Chaofan Li, Zheng Liu, Shitao Xiao, and Yingxia Shao. Making large language models a better foundation for dense retrieval, 2023.
[24] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-embedding: Multilingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2024.
[25] Shreyas Pimpalgaonkar, Nolan Tremelling, and Owen Colegrove. Triplex: a sota llm for knowledge graph construction, 2024.
[26] Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, Wenbo Su, and Bo Zheng. Graphreader: Building graph-based agent to enhance long-context abilities of large language models, 2024.</p>
<p>[27] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: A new benchmark for financial question answering, 2023.
[28] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models, 2021.</p>            </div>
        </div>

    </div>
</body>
</html>
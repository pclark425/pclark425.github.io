<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1533 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1533</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1533</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-e89a4fe6e8286eccedd702216153f0f248adb151</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e89a4fe6e8286eccedd702216153f0f248adb151" target="_blank">Gibson Env: Real-World Perception for Embodied Agents</a></p>
                <p><strong>Paper Venue:</strong> 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> This paper investigates developing real-world perception for active agents, proposes Gibson Environment for this purpose, and showcases a set of perceptual tasks learned therein.</p>
                <p><strong>Paper Abstract:</strong> Developing visual perception models for active agents and sensorimotor control in the physical world are cumbersome as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we investigate developing real-world perception for active agents, propose Gibson Environment for this purpose, and showcase a set of perceptual tasks learned therein. Gibson is based upon virtualizing real spaces, rather than artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism "Goggles" enabling deploying the trained models in real-world without needing domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1533.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1533.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gibson</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gibson Virtual Environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A custom virtual environment built from scanned real indoor spaces with a neural view-synthesis renderer and an adaptation mechanism ('Goggles') to train embodied perceptual agents for navigation, obstacle avoidance, and locomotion tasks with the goal of transferring to real-world imagery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Gibson Environment</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A simulation environment that virtualizes real indoor spaces (572 buildings, 1447 floors, 211k m^2) using RGB-D panoramas and reconstructed 3D meshes, provides on-board visual streams, additional modalities (depth, normals, semantics), integrates a physics engine, and a neural view-synthesis pipeline (geometric point-cloud rendering + neural 'filler' network) plus a backward mapping 'Goggles' to close domain gap to real images.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / embodied vision / robotics (visual perception and control)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>High visual fidelity relative to synthetic renderers (based on scanned real spaces and neural correction) but not fully photorealistic; medium-fidelity physics (integrated with PyBullet, approximate material/friction modeling).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Visual fidelity: uses real scanned panoramas, point-cloud geometric rendering, neural network 'filler' to correct artifacts and fill dis-occlusions; includes a 'Goggles' backward network to map real images into the rendering space. Physics fidelity: uses PyBullet supporting rigid/soft bodies, discrete/continuous collision detection, Coulomb friction by default; lacks full material properties and some dynamics (no dynamic content like moving objects), known imperfections in 3D reconstruction (missing thin/shiny objects, window views). Rendering operates faster than real-time; multiple filler network sizes trade quality vs FPS.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>husky (UGV), ant (legged), humanoid, drone, minitaur, Jackrabbot</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement-learning agents (PPO) for navigation and locomotion; agents are embodied robots imported via URDF or XML (Roboschool/MuJoCo models); perceptual agents take visual streams (RGB/depth) and non-visual proprioceptive inputs; also static CNNs for depth estimation and scene classification.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Active perceptual tasks: local planning and obstacle avoidance, distant visual navigation, stair-climb locomotion; static perceptual tasks used for transfer evaluation: single-image depth estimation and scene classification.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Local planning: perceptual husky achieved higher reward and learned obstacle avoidance (qualitative higher score). Distant navigation: global navigation behavior emerged after ~1700 episodes (~680k frames); only visual agent succeeded. Stair climb: perceptual ant learned and at test outperformed non-perceptual agent by ~70% in generalizability. (No precise numerical rewards provided beyond these descriptions.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world images / real robots (real camera images; in one cited follow-up, real robot evaluation was performed by another paper using similar backward mapping), and comparison to other simulated datasets rendered with other engines.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Depth estimation transfer: training on Gibson (f(2s)) and testing on u(2t) (Goggles-applied real images) achieved depth error 0.915 m, close to gold standard train/test on real (2t) which is 0.86 m. Table 1 real-world transfer error for Gibson (rendered with Gibson) = 0.92 m, compared to SUNCG (2.89 m) and Matterport3D (2.11 m) rendered via MINOS. Policy discrepancy (action logits L2): training on f(Is) testing on u(It) = 0.204 (best); f(Is)->It = 0.300; Is->It = 0.242.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Comparisons show that using scans of real spaces plus neural rendering plus Goggles (f + u) yields substantially lower real-world transfer error than synthetic datasets rendered with other simulators. Best transfer observed when training on f(rendered) and testing on u(real-through-Goggles) (depth error 0.915 m vs real-real 0.86 m). Gibson's real-scanned basis produced much lower transfer error (0.92 m) than SUNCG rendered with MINOS (2.89 m) and Matterport3D with MINOS (2.11 m).</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper argues semantic complexity of real spaces and reducing perceptual rendering gap (via neural synthesis and Goggles) are important for transfer; physics simulator limitations and full material properties are less solved and may cause physics-related domain gaps. They emphasize real-scanned environments and appearance adaptation as sufficient to improve transfer for vision tasks, but note physics fidelity limitations for dynamics/manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Pre-neural geometric renderings contain stitching, deformations, dis-occlusions; thin/shiny fixtures and window exteriors are poorly reconstructed and suppressed by networks. Physics limitations: no material properties, no dynamic objects, and no manipulation support; these can cause domain gaps in physics-related tasks. Transfer without Goggles leads to higher depth error and greater policy discrepancy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1533.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1533.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PyBullet (physics engine)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics simulation engine integrated into Gibson to provide rigid and soft body simulation, collision handling, and to enforce embodiment and physical constraints for agents during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pybullet, a python module for physics simulation for games, robotics and machine learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Open-source physics engine providing rigid and soft body simulation with discrete and continuous collision detection, built-in fast collision handling, and support for recording interaction metrics; used to simulate agent embodiment (gravity, collisions, friction) within Gibson.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics physics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Medium-fidelity general-purpose physics simulation (suitable for rigid/soft bodies and collision dynamics but not specialized material physics).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Supports discrete/continuous collision detection, Coulomb friction model used by default, rigid and soft body simulation, fast collision handling; does not incorporate per-object material properties from scans (such properties absent in scanned models), so some physics aspects are approximate.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>integrated agents (Mujoco humanoid model, ant, husky, drone, minitaur, Jackrabbot)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Physical embodiment and dynamics simulated in PyBullet while agent policies are learned (e.g., PPO) or controllers (PID, nonholonomic, ideal positional controller) used for abstracted control.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Enforce physical constraints during training of perception and control policies for navigation, obstacle avoidance, and locomotion (stair-climb).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world robot dynamics / physical plausibility of trajectories</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper notes current physics simulators including PyBullet are not optimal and the lack of material properties in scanned models may cause physics-related domain gaps; thus, physics fidelity is a limitation for transfer in physics-heavy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Physics-related domain gaps can arise due to missing material annotations and imperfect reconstruction; no dynamic scene content and lack of manipulation support can limit transferability for tasks requiring richer physics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1533.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1533.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MINOS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multimodal indoor simulator used in this paper as the rendering engine to produce training images for other datasets (SUNCG and Matterport3D) used in cross-dataset transfer comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Minos: Multimodal indoor simulator for navigation in complex environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MINOS</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>An indoor navigation simulator that can render synthetic or scanned environments and produce multimodal observations; in this paper it was used to render SUNCG and Matterport3D images for depth-estimation transfer baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied vision / navigation simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Variable â€” depends on underlying dataset and rendering; in the paper MINOS-rendered SUNCG is synthetic (lower visual fidelity) while Matterport3D inputs are scanned (higher), but MINOS renderings resulted in higher transfer error compared to Gibson's renderer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Rendering fidelity depends on dataset; used as baseline for rendering SUNCG (synthetic) and Matterport3D; transfer experiments show larger domain gap for MINOS-rendered datasets compared to Gibson's rendering with Goggles.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>not applicable (used for rendering training data for CNN depth estimators)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used to generate RGB-depth training pairs for static recognition models (depth estimation and scene classification).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Static perceptual tasks (single-image depth estimation and scene classification) used to measure transfer gap to real images.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>When networks were trained on MINOS-rendered SUNCG images and tested on real 2D-3D-S images, real-world transfer error reported was 2.89 m (depth estimation). For MINOS-rendered Matterport3D, error was 2.11 m.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world images (2D-3D-S dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Depth estimation error to real images: SUNCG+MINOS = 2.89 m; Matterport3D+MINOS = 2.11 m (both worse than Gibson's 0.92 m).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>MINOS-rendered datasets (especially synthetic SUNCG) transferred poorly to real images compared to Gibson-rendered + Goggles; Gibson achieved much lower depth-transfer error.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Results suggest that rendering from synthetic datasets (even via MINOS) lacks necessary visual realism/semantic complexity to transfer well; scanning real spaces and specific render-domain adaptation (Goggles) improves transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>High transfer error for SUNCG indicates low visual realism (synthetic) fails to support transfer for depth estimation; Matterport3D performed better but still worse than Gibson.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1533.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1533.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Roboschool</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Roboschool (agent models/environment components)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of open-source robot models and simulated agents (humanoid, ant, husky, etc.) used/imported as entry-point agents in Gibson for learning locomotion and navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenAI Roboschool.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Roboschool agent models / environments</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Collection of robot models and environments (Mujoco-compatible) providing standard agent morphologies (ant, humanoid, husky) used within Gibson; agents are imported as URDF or Mujoco XML formats.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / locomotion simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Model-level fidelity (standard robotic morphologies) while dynamics are simulated by the integrated physics engine (PyBullet); fidelity depends on agent model fidelity and physics integration.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides standard kinematic/dynamic robot descriptions; used with PyBullet for embodiment and dynamics; also supports idealized controllers for abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ant, humanoid, husky, others (Roboschool models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Embodied robotic agents used to train RL policies (PPO) for tasks such as stair descent (ant) and navigation (husky).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learning control and perception-coupled locomotion tasks: stair-climb (ant), local planning and obstacle avoidance, distant navigation (husky).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Stair-climb: perceptual ant learned slower but generalized better and outperformed non-perceptual by ~70% at test. Navigation: visual husky learned to avoid obstacles and achieved higher reward than non-perceptual variant. (No absolute reward numbers reported.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Not explicitly transferred to real robots in this paper (policy-level transfer evaluated in terms of action-logit discrepancy when changing input domains), but follow-up work referenced performed real robot evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper uses these standard models to illustrate embodiment and perceptual learning; does not claim full physical fidelity, and notes physics simulator limitations may limit transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No direct real-robot transfer reported here; limits noted due to physics/modeling gaps and lack of dynamic scene content.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1533.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1533.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo (models)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo humanoid / MuJoCo XML agent models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MuJoCo-format agent models (e.g., humanoid) are used/imported into Gibson to represent agent morphology and demonstrate physically plausible interactions (illustrated falling humanoid).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo model assets (used inside Gibson with PyBullet)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Agent model descriptions in MuJoCo XML format (humanoid, etc.) used as embodied models within Gibson; dynamics are executed by PyBullet in Gibson experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / embodied simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Model fidelity high at kinematic description level; actual dynamics fidelity depends on physics engine (PyBullet) when used inside Gibson.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>MuJoCo models provide morphological detail; Gibson used a 'Mujoco humanoid model' dropped on stairs to illustrate plausible fall; dynamics simulated via Gibson's physics integration (PyBullet).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Mujoco humanoid model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Embodied humanoid robot model demonstrating interactions (visualized with physics and rendered views).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Demonstration of physically plausible embodied perception (visual observation stream during physical interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Paper uses MuJoCo-format models for embodiment but emphasizes that underlying physics fidelity is provided by PyBullet and notes physics limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit failure cases for MuJoCo models reported; physics mismatches remain possible due to simulator/asset limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1533.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1533.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AirSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AirSim: High-fidelity visual and physical simulation for autonomous vehicles</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-fidelity simulator for visual and physical simulation of autonomous vehicles, mentioned in related work as an example of simulators used for training agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Airsim: Highfidelity visual and physical simulation for autonomous vehicles</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>AirSim</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulator providing photorealistic visuals and physical modeling targeted at autonomous vehicle research (mentioned as related work; not used in Gibson experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>autonomous vehicles / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Reportedly high-fidelity for both visuals and physics (as cited in related work), but not evaluated or used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>High-fidelity rendering and vehicle dynamics (per original AirSim work); no specifics given in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1533.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1533.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CARLA: An open urban driving simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source simulator for urban driving research, cited in related work as another simulation platform adapted to train agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Carla: An open urban driving simulator</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Urban driving simulator providing photorealistic rendering and realistic driving scenarios (mentioned in related work only).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>autonomous driving / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Designed for relatively high visual and scenario fidelity; not used or evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1533.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1533.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VizDoom</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ViZDoom</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Doom-based AI research platform for visual reinforcement learning, mentioned as an example of virtual environments used in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vizdoom: A doom-based ai research platform for visual reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>ViZDoom</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Game-engine-based environment adapted for RL research; mentioned in related work as an example of synthetic/gaming simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>visual reinforcement learning / game-based simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Low-to-moderate visual realism (game graphics), adequate for certain control tasks but limited for photorealistic perception transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Synthetic graphics; no details provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1533.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1533.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Malmo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Malmo platform for artificial intelligence experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Microsoft Malmo (based on Minecraft) mentioned as an example of a virtual environment used to train agents, noted to be limited in visual realism for perception-focused tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The malmo platform for artificial intelligence experimentation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Malmo (Minecraft-based platform)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Environment built on Minecraft for AI experimentation; cited as prior virtual environment adapted for learning agents but synthetic in nature.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied agents / reinforcement learning (synthetic environments)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Low visual realism (blocky/game-like), useful for some control/problem-solving tasks but limited for real-world perception transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Synthetic visuals and simple geometry; not designed for photorealistic perception.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1533.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e1533.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TORCS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TORCS: The Open Racing Car Simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A racing car simulator listed in related work as an example of game-like simulators used historically for training agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Torcs, the open racing car simulator</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>TORCS</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Open racing car simulator used in RL and control research; included in related work as an example of simulators adapted to train agents.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>autonomous driving / vehicle control (simulated)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>Game/simulation-level fidelity suitable for control benchmarks but not for photorealistic perception transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1533.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e1533.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SUNCG / Matterport3D (rendered baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SUNCG dataset and Matterport3D dataset (rendered with MINOS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Datasets / environment collections used as baseline render sources (SUNCG synthetic, Matterport3D scanned) rendered via MINOS to compare transfer to real images against Gibson's renderings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>SUNCG + MINOS; Matterport3D + MINOS (rendering pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>SUNCG is a hand-designed synthetic dataset; Matterport3D is a scanned dataset; both were rendered with MINOS to generate RGB-depth training pairs used to evaluate transfer to real images and compared against Gibson renderings.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>embodied vision / scene understanding</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>SUNCG+MINOS: low visual fidelity for real-world transfer (synthetic); Matterport3D+MINOS: higher (scanned) but still worse transfer than Gibson.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>SUNCG is artificially designed and lacks real semantic complexity; Matterport3D scanned data was used but rendering via MINOS still produced larger domain gaps compared to Gibson's scanner+neural renderer+Goggles approach.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>not applicable (used for training static depth estimation models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Training data used for single-image depth estimation CNNs and scene classification networks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Single-image depth estimation and scene classification used as static recognition transfer tests.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>Real-world depth estimation transfer error: SUNCG+MINOS = 2.89 m; Matterport3D+MINOS = 2.11 m.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real images from 2D-3D-S dataset</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>As above; both perform worse than Gibson-rendered training (0.92 m).</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Synthetic SUNCG (via MINOS) had worst transfer; Matterport3D better but still inferior to Gibson. Indicates importance of data source and rendering/adaptation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Evidence suggests synthetic semantic simplicity and renderer differences harm transfer; scanning real spaces and applying adaptation (Goggles) helps reduce the perceptual gap.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>High transfer errors for SUNCG highlight failure of low-semantic-fidelity synthetic data for real-world depth estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Gibson Env: Real-World Perception for Embodied Agents', 'publication_date_yy_mm': '2018-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Minos: Multimodal indoor simulator for navigation in complex environments <em>(Rating: 2)</em></li>
                <li>Airsim: Highfidelity visual and physical simulation for autonomous vehicles <em>(Rating: 2)</em></li>
                <li>Carla: An open urban driving simulator <em>(Rating: 2)</em></li>
                <li>Vr goggles for robots: Real-to-sim domain adaptation for visual control <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1533",
    "paper_id": "paper-e89a4fe6e8286eccedd702216153f0f248adb151",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "Gibson",
            "name_full": "Gibson Virtual Environment",
            "brief_description": "A custom virtual environment built from scanned real indoor spaces with a neural view-synthesis renderer and an adaptation mechanism ('Goggles') to train embodied perceptual agents for navigation, obstacle avoidance, and locomotion tasks with the goal of transferring to real-world imagery.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "Gibson Environment",
            "simulator_description": "A simulation environment that virtualizes real indoor spaces (572 buildings, 1447 floors, 211k m^2) using RGB-D panoramas and reconstructed 3D meshes, provides on-board visual streams, additional modalities (depth, normals, semantics), integrates a physics engine, and a neural view-synthesis pipeline (geometric point-cloud rendering + neural 'filler' network) plus a backward mapping 'Goggles' to close domain gap to real images.",
            "scientific_domain": "mechanics / embodied vision / robotics (visual perception and control)",
            "fidelity_level": "High visual fidelity relative to synthetic renderers (based on scanned real spaces and neural correction) but not fully photorealistic; medium-fidelity physics (integrated with PyBullet, approximate material/friction modeling).",
            "fidelity_characteristics": "Visual fidelity: uses real scanned panoramas, point-cloud geometric rendering, neural network 'filler' to correct artifacts and fill dis-occlusions; includes a 'Goggles' backward network to map real images into the rendering space. Physics fidelity: uses PyBullet supporting rigid/soft bodies, discrete/continuous collision detection, Coulomb friction by default; lacks full material properties and some dynamics (no dynamic content like moving objects), known imperfections in 3D reconstruction (missing thin/shiny objects, window views). Rendering operates faster than real-time; multiple filler network sizes trade quality vs FPS.",
            "model_or_agent_name": "husky (UGV), ant (legged), humanoid, drone, minitaur, Jackrabbot",
            "model_description": "Reinforcement-learning agents (PPO) for navigation and locomotion; agents are embodied robots imported via URDF or XML (Roboschool/MuJoCo models); perceptual agents take visual streams (RGB/depth) and non-visual proprioceptive inputs; also static CNNs for depth estimation and scene classification.",
            "reasoning_task": "Active perceptual tasks: local planning and obstacle avoidance, distant visual navigation, stair-climb locomotion; static perceptual tasks used for transfer evaluation: single-image depth estimation and scene classification.",
            "training_performance": "Local planning: perceptual husky achieved higher reward and learned obstacle avoidance (qualitative higher score). Distant navigation: global navigation behavior emerged after ~1700 episodes (~680k frames); only visual agent succeeded. Stair climb: perceptual ant learned and at test outperformed non-perceptual agent by ~70% in generalizability. (No precise numerical rewards provided beyond these descriptions.)",
            "transfer_target": "Real-world images / real robots (real camera images; in one cited follow-up, real robot evaluation was performed by another paper using similar backward mapping), and comparison to other simulated datasets rendered with other engines.",
            "transfer_performance": "Depth estimation transfer: training on Gibson (f(2s)) and testing on u(2t) (Goggles-applied real images) achieved depth error 0.915 m, close to gold standard train/test on real (2t) which is 0.86 m. Table 1 real-world transfer error for Gibson (rendered with Gibson) = 0.92 m, compared to SUNCG (2.89 m) and Matterport3D (2.11 m) rendered via MINOS. Policy discrepancy (action logits L2): training on f(Is) testing on u(It) = 0.204 (best); f(Is)-&gt;It = 0.300; Is-&gt;It = 0.242.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Comparisons show that using scans of real spaces plus neural rendering plus Goggles (f + u) yields substantially lower real-world transfer error than synthetic datasets rendered with other simulators. Best transfer observed when training on f(rendered) and testing on u(real-through-Goggles) (depth error 0.915 m vs real-real 0.86 m). Gibson's real-scanned basis produced much lower transfer error (0.92 m) than SUNCG rendered with MINOS (2.89 m) and Matterport3D with MINOS (2.11 m).",
            "minimal_fidelity_discussion": "Paper argues semantic complexity of real spaces and reducing perceptual rendering gap (via neural synthesis and Goggles) are important for transfer; physics simulator limitations and full material properties are less solved and may cause physics-related domain gaps. They emphasize real-scanned environments and appearance adaptation as sufficient to improve transfer for vision tasks, but note physics fidelity limitations for dynamics/manipulation.",
            "failure_cases": "Pre-neural geometric renderings contain stitching, deformations, dis-occlusions; thin/shiny fixtures and window exteriors are poorly reconstructed and suppressed by networks. Physics limitations: no material properties, no dynamic objects, and no manipulation support; these can cause domain gaps in physics-related tasks. Transfer without Goggles leads to higher depth error and greater policy discrepancy.",
            "uuid": "e1533.0",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "PyBullet",
            "name_full": "PyBullet (physics engine)",
            "brief_description": "A physics simulation engine integrated into Gibson to provide rigid and soft body simulation, collision handling, and to enforce embodiment and physical constraints for agents during training.",
            "citation_title": "Pybullet, a python module for physics simulation for games, robotics and machine learning.",
            "mention_or_use": "use",
            "simulator_name": "PyBullet",
            "simulator_description": "Open-source physics engine providing rigid and soft body simulation with discrete and continuous collision detection, built-in fast collision handling, and support for recording interaction metrics; used to simulate agent embodiment (gravity, collisions, friction) within Gibson.",
            "scientific_domain": "mechanics / robotics physics",
            "fidelity_level": "Medium-fidelity general-purpose physics simulation (suitable for rigid/soft bodies and collision dynamics but not specialized material physics).",
            "fidelity_characteristics": "Supports discrete/continuous collision detection, Coulomb friction model used by default, rigid and soft body simulation, fast collision handling; does not incorporate per-object material properties from scans (such properties absent in scanned models), so some physics aspects are approximate.",
            "model_or_agent_name": "integrated agents (Mujoco humanoid model, ant, husky, drone, minitaur, Jackrabbot)",
            "model_description": "Physical embodiment and dynamics simulated in PyBullet while agent policies are learned (e.g., PPO) or controllers (PID, nonholonomic, ideal positional controller) used for abstracted control.",
            "reasoning_task": "Enforce physical constraints during training of perception and control policies for navigation, obstacle avoidance, and locomotion (stair-climb).",
            "training_performance": null,
            "transfer_target": "Real-world robot dynamics / physical plausibility of trajectories",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper notes current physics simulators including PyBullet are not optimal and the lack of material properties in scanned models may cause physics-related domain gaps; thus, physics fidelity is a limitation for transfer in physics-heavy tasks.",
            "failure_cases": "Physics-related domain gaps can arise due to missing material annotations and imperfect reconstruction; no dynamic scene content and lack of manipulation support can limit transferability for tasks requiring richer physics.",
            "uuid": "e1533.1",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "MINOS",
            "name_full": "MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments",
            "brief_description": "A multimodal indoor simulator used in this paper as the rendering engine to produce training images for other datasets (SUNCG and Matterport3D) used in cross-dataset transfer comparisons.",
            "citation_title": "Minos: Multimodal indoor simulator for navigation in complex environments",
            "mention_or_use": "use",
            "simulator_name": "MINOS",
            "simulator_description": "An indoor navigation simulator that can render synthetic or scanned environments and produce multimodal observations; in this paper it was used to render SUNCG and Matterport3D images for depth-estimation transfer baselines.",
            "scientific_domain": "embodied vision / navigation simulation",
            "fidelity_level": "Variable â€” depends on underlying dataset and rendering; in the paper MINOS-rendered SUNCG is synthetic (lower visual fidelity) while Matterport3D inputs are scanned (higher), but MINOS renderings resulted in higher transfer error compared to Gibson's renderer.",
            "fidelity_characteristics": "Rendering fidelity depends on dataset; used as baseline for rendering SUNCG (synthetic) and Matterport3D; transfer experiments show larger domain gap for MINOS-rendered datasets compared to Gibson's rendering with Goggles.",
            "model_or_agent_name": "not applicable (used for rendering training data for CNN depth estimators)",
            "model_description": "Used to generate RGB-depth training pairs for static recognition models (depth estimation and scene classification).",
            "reasoning_task": "Static perceptual tasks (single-image depth estimation and scene classification) used to measure transfer gap to real images.",
            "training_performance": "When networks were trained on MINOS-rendered SUNCG images and tested on real 2D-3D-S images, real-world transfer error reported was 2.89 m (depth estimation). For MINOS-rendered Matterport3D, error was 2.11 m.",
            "transfer_target": "Real-world images (2D-3D-S dataset)",
            "transfer_performance": "Depth estimation error to real images: SUNCG+MINOS = 2.89 m; Matterport3D+MINOS = 2.11 m (both worse than Gibson's 0.92 m).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "MINOS-rendered datasets (especially synthetic SUNCG) transferred poorly to real images compared to Gibson-rendered + Goggles; Gibson achieved much lower depth-transfer error.",
            "minimal_fidelity_discussion": "Results suggest that rendering from synthetic datasets (even via MINOS) lacks necessary visual realism/semantic complexity to transfer well; scanning real spaces and specific render-domain adaptation (Goggles) improves transfer.",
            "failure_cases": "High transfer error for SUNCG indicates low visual realism (synthetic) fails to support transfer for depth estimation; Matterport3D performed better but still worse than Gibson.",
            "uuid": "e1533.2",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Roboschool",
            "name_full": "OpenAI Roboschool (agent models/environment components)",
            "brief_description": "A set of open-source robot models and simulated agents (humanoid, ant, husky, etc.) used/imported as entry-point agents in Gibson for learning locomotion and navigation tasks.",
            "citation_title": "OpenAI Roboschool.",
            "mention_or_use": "use",
            "simulator_name": "Roboschool agent models / environments",
            "simulator_description": "Collection of robot models and environments (Mujoco-compatible) providing standard agent morphologies (ant, humanoid, husky) used within Gibson; agents are imported as URDF or Mujoco XML formats.",
            "scientific_domain": "robotics / locomotion simulation",
            "fidelity_level": "Model-level fidelity (standard robotic morphologies) while dynamics are simulated by the integrated physics engine (PyBullet); fidelity depends on agent model fidelity and physics integration.",
            "fidelity_characteristics": "Provides standard kinematic/dynamic robot descriptions; used with PyBullet for embodiment and dynamics; also supports idealized controllers for abstraction.",
            "model_or_agent_name": "ant, humanoid, husky, others (Roboschool models)",
            "model_description": "Embodied robotic agents used to train RL policies (PPO) for tasks such as stair descent (ant) and navigation (husky).",
            "reasoning_task": "Learning control and perception-coupled locomotion tasks: stair-climb (ant), local planning and obstacle avoidance, distant navigation (husky).",
            "training_performance": "Stair-climb: perceptual ant learned slower but generalized better and outperformed non-perceptual by ~70% at test. Navigation: visual husky learned to avoid obstacles and achieved higher reward than non-perceptual variant. (No absolute reward numbers reported.)",
            "transfer_target": "Not explicitly transferred to real robots in this paper (policy-level transfer evaluated in terms of action-logit discrepancy when changing input domains), but follow-up work referenced performed real robot evaluations.",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper uses these standard models to illustrate embodiment and perceptual learning; does not claim full physical fidelity, and notes physics simulator limitations may limit transfer.",
            "failure_cases": "No direct real-robot transfer reported here; limits noted due to physics/modeling gaps and lack of dynamic scene content.",
            "uuid": "e1533.3",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "MuJoCo (models)",
            "name_full": "MuJoCo humanoid / MuJoCo XML agent models",
            "brief_description": "MuJoCo-format agent models (e.g., humanoid) are used/imported into Gibson to represent agent morphology and demonstrate physically plausible interactions (illustrated falling humanoid).",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "MuJoCo model assets (used inside Gibson with PyBullet)",
            "simulator_description": "Agent model descriptions in MuJoCo XML format (humanoid, etc.) used as embodied models within Gibson; dynamics are executed by PyBullet in Gibson experiments.",
            "scientific_domain": "robotics / embodied simulation",
            "fidelity_level": "Model fidelity high at kinematic description level; actual dynamics fidelity depends on physics engine (PyBullet) when used inside Gibson.",
            "fidelity_characteristics": "MuJoCo models provide morphological detail; Gibson used a 'Mujoco humanoid model' dropped on stairs to illustrate plausible fall; dynamics simulated via Gibson's physics integration (PyBullet).",
            "model_or_agent_name": "Mujoco humanoid model",
            "model_description": "Embodied humanoid robot model demonstrating interactions (visualized with physics and rendered views).",
            "reasoning_task": "Demonstration of physically plausible embodied perception (visual observation stream during physical interactions).",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Paper uses MuJoCo-format models for embodiment but emphasizes that underlying physics fidelity is provided by PyBullet and notes physics limitations.",
            "failure_cases": "No explicit failure cases for MuJoCo models reported; physics mismatches remain possible due to simulator/asset limitations.",
            "uuid": "e1533.4",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "AirSim",
            "name_full": "AirSim: High-fidelity visual and physical simulation for autonomous vehicles",
            "brief_description": "A high-fidelity simulator for visual and physical simulation of autonomous vehicles, mentioned in related work as an example of simulators used for training agents.",
            "citation_title": "Airsim: Highfidelity visual and physical simulation for autonomous vehicles",
            "mention_or_use": "mention",
            "simulator_name": "AirSim",
            "simulator_description": "Simulator providing photorealistic visuals and physical modeling targeted at autonomous vehicle research (mentioned as related work; not used in Gibson experiments).",
            "scientific_domain": "autonomous vehicles / robotics",
            "fidelity_level": "Reportedly high-fidelity for both visuals and physics (as cited in related work), but not evaluated or used in this paper.",
            "fidelity_characteristics": "High-fidelity rendering and vehicle dynamics (per original AirSim work); no specifics given in this paper.",
            "model_or_agent_name": "",
            "model_description": "",
            "reasoning_task": "",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1533.5",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "CARLA",
            "name_full": "CARLA: An open urban driving simulator",
            "brief_description": "An open-source simulator for urban driving research, cited in related work as another simulation platform adapted to train agents.",
            "citation_title": "Carla: An open urban driving simulator",
            "mention_or_use": "mention",
            "simulator_name": "CARLA",
            "simulator_description": "Urban driving simulator providing photorealistic rendering and realistic driving scenarios (mentioned in related work only).",
            "scientific_domain": "autonomous driving / robotics",
            "fidelity_level": "Designed for relatively high visual and scenario fidelity; not used or evaluated in this paper.",
            "fidelity_characteristics": "Not specified in this paper.",
            "model_or_agent_name": "",
            "model_description": "",
            "reasoning_task": "",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1533.6",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "VizDoom",
            "name_full": "ViZDoom",
            "brief_description": "A Doom-based AI research platform for visual reinforcement learning, mentioned as an example of virtual environments used in prior work.",
            "citation_title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning",
            "mention_or_use": "mention",
            "simulator_name": "ViZDoom",
            "simulator_description": "Game-engine-based environment adapted for RL research; mentioned in related work as an example of synthetic/gaming simulators.",
            "scientific_domain": "visual reinforcement learning / game-based simulation",
            "fidelity_level": "Low-to-moderate visual realism (game graphics), adequate for certain control tasks but limited for photorealistic perception transfer.",
            "fidelity_characteristics": "Synthetic graphics; no details provided in this paper.",
            "model_or_agent_name": "",
            "model_description": "",
            "reasoning_task": "",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1533.7",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "Malmo",
            "name_full": "Malmo platform for artificial intelligence experimentation",
            "brief_description": "Microsoft Malmo (based on Minecraft) mentioned as an example of a virtual environment used to train agents, noted to be limited in visual realism for perception-focused tasks.",
            "citation_title": "The malmo platform for artificial intelligence experimentation",
            "mention_or_use": "mention",
            "simulator_name": "Malmo (Minecraft-based platform)",
            "simulator_description": "Environment built on Minecraft for AI experimentation; cited as prior virtual environment adapted for learning agents but synthetic in nature.",
            "scientific_domain": "embodied agents / reinforcement learning (synthetic environments)",
            "fidelity_level": "Low visual realism (blocky/game-like), useful for some control/problem-solving tasks but limited for real-world perception transfer.",
            "fidelity_characteristics": "Synthetic visuals and simple geometry; not designed for photorealistic perception.",
            "model_or_agent_name": "",
            "model_description": "",
            "reasoning_task": "",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1533.8",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "TORCS",
            "name_full": "TORCS: The Open Racing Car Simulator",
            "brief_description": "A racing car simulator listed in related work as an example of game-like simulators used historically for training agents.",
            "citation_title": "Torcs, the open racing car simulator",
            "mention_or_use": "mention",
            "simulator_name": "TORCS",
            "simulator_description": "Open racing car simulator used in RL and control research; included in related work as an example of simulators adapted to train agents.",
            "scientific_domain": "autonomous driving / vehicle control (simulated)",
            "fidelity_level": "Game/simulation-level fidelity suitable for control benchmarks but not for photorealistic perception transfer.",
            "fidelity_characteristics": "Not specified in this paper.",
            "model_or_agent_name": "",
            "model_description": "",
            "reasoning_task": "",
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "",
            "failure_cases": "",
            "uuid": "e1533.9",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        },
        {
            "name_short": "SUNCG / Matterport3D (rendered baselines)",
            "name_full": "SUNCG dataset and Matterport3D dataset (rendered with MINOS)",
            "brief_description": "Datasets / environment collections used as baseline render sources (SUNCG synthetic, Matterport3D scanned) rendered via MINOS to compare transfer to real images against Gibson's renderings.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "SUNCG + MINOS; Matterport3D + MINOS (rendering pipeline)",
            "simulator_description": "SUNCG is a hand-designed synthetic dataset; Matterport3D is a scanned dataset; both were rendered with MINOS to generate RGB-depth training pairs used to evaluate transfer to real images and compared against Gibson renderings.",
            "scientific_domain": "embodied vision / scene understanding",
            "fidelity_level": "SUNCG+MINOS: low visual fidelity for real-world transfer (synthetic); Matterport3D+MINOS: higher (scanned) but still worse transfer than Gibson.",
            "fidelity_characteristics": "SUNCG is artificially designed and lacks real semantic complexity; Matterport3D scanned data was used but rendering via MINOS still produced larger domain gaps compared to Gibson's scanner+neural renderer+Goggles approach.",
            "model_or_agent_name": "not applicable (used for training static depth estimation models)",
            "model_description": "Training data used for single-image depth estimation CNNs and scene classification networks.",
            "reasoning_task": "Single-image depth estimation and scene classification used as static recognition transfer tests.",
            "training_performance": "Real-world depth estimation transfer error: SUNCG+MINOS = 2.89 m; Matterport3D+MINOS = 2.11 m.",
            "transfer_target": "Real images from 2D-3D-S dataset",
            "transfer_performance": "As above; both perform worse than Gibson-rendered training (0.92 m).",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Synthetic SUNCG (via MINOS) had worst transfer; Matterport3D better but still inferior to Gibson. Indicates importance of data source and rendering/adaptation pipeline.",
            "minimal_fidelity_discussion": "Evidence suggests synthetic semantic simplicity and renderer differences harm transfer; scanning real spaces and applying adaptation (Goggles) helps reduce the perceptual gap.",
            "failure_cases": "High transfer errors for SUNCG highlight failure of low-semantic-fidelity synthetic data for real-world depth estimation.",
            "uuid": "e1533.10",
            "source_info": {
                "paper_title": "Gibson Env: Real-World Perception for Embodied Agents",
                "publication_date_yy_mm": "2018-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Minos: Multimodal indoor simulator for navigation in complex environments",
            "rating": 2
        },
        {
            "paper_title": "Airsim: Highfidelity visual and physical simulation for autonomous vehicles",
            "rating": 2
        },
        {
            "paper_title": "Carla: An open urban driving simulator",
            "rating": 2
        },
        {
            "paper_title": "Vr goggles for robots: Real-to-sim domain adaptation for visual control",
            "rating": 2
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 1
        }
    ],
    "cost": 0.020113,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Gibson Env: Real-World Perception for Embodied Agents</h1>
<p>Fei Xiaâ€  Amir R. Zamirâ€ ,2 Zhi-Yang Heâ€  Alexander Sax1 Jitendra Malik2 Silvio Savarese1
1 Stanford University 2 University of California, Berkeley
http://gibson.vision/</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p><strong>Figure 1:</strong> Two agents in <em>Gibson Environment</em> for real-world perception. The agent is active, embodied, and subject to constraints of physics and space (a,b). It receives a constant stream of visual observations as if it had an on-board camera (c). It can also receive additional modalities, e.g., depth, semantic labels, or normals (d,e,f). The visual observations are from real-world rather than an artificially designed space.</p>
<h2>Abstract</h2>
<p><em>Developing visual perception models for active agents and sensorimotor control are cumbersome to be done in the physical world, as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning-in-simulation which consequently casts a question on whether the results transfer to real-world. In this paper, we are concerned with the problem of developing real-world perception for active agents, propose Gibson Virtual Environment</em> 1 <em>for this purpose, and showcase sample perceptual tasks learned therein. Gibson is based on virtualizing real spaces, rather than using artificially designed ones, and currently includes over 1400 floor spaces from 572 full buildings. The main characteristics of Gibson are: I. being from the real-world and reflecting its semantic complexity, II. having an internal synthesis mechanism, "Goggles", enabling deploying the trained models in real-world without needing domain adaptation, III. embodiment of agents and making them subject to constraints of physics and space.</em></p>
<h2>1. Introduction</h2>
<p>We would like our robotic agents to have compound perceptual and physical capabilities: a drone that autonomously surveys buildings, a robot that rapidly finds victims in a disaster area, or one that safely delivers our packages, just to name a few. Apart from the application perspective, the findings supportive of a close relationship between visual perception and being physically active are prevalent on various fronts: evolutionary and computational biologists have hypothesized a key role for intermixing perception and locomotion in development of complex behaviors and species [65, 95, 24]; neuroscientists have extensively argued for a hand in hand relationship between developing perception and being active [87, 45]; pioneer roboticists have similarly advocated entanglement of the two [15, 16]. This all calls for developing principled perception models specifically with active agents in mind.</p>
<p>By perceptual active agent, we are generally referring to an agent that receives a visual observation from the environment and accordingly effectuates a set of actions which can lead a physical change in the environment (~manipulation) and/or the agent's own particulars (~locomotion). Developing such perceptual agents entails the questions of <em>how</em> and <em>where</em> to do so.</p>
<p><sup>1</sup>Named after <strong>JJ Gibson</strong>, the author of <em>Ecological Approach to Visual Perception</em>, 1979. "We must perceive in order to move, but we must also move in order to perceive" â€“ JJ Gibson [38]</p>
<p><sup>*</sup>Authors contributed equally.</p>
<p>On the how front, the problem has been the focus of a broad set of topics for decades, from classical control $[68,13,53]$ to more recently sensorimotor control [35, $58,59,5]$, reinforcement learning $[6,77,78]$, acting by prediction [30], imitation learning [25], and other concepts $[63,106,97,96]$. These methods generally assume a sensory observation from environment is given and subsequently devise one or a series of actions to perform a task.</p>
<p>A key question is where this sensory observation should come from. Conventional computer vision datasets [34, 28, 61] are passive and static, and consequently, lacking for this purpose. Learning in the physical world, though not impossible [40, 7, 59, 67], is not the ideal scenario. It would bound the learning speed to real-time, incur substantial logistical cost if massively parallelized, and discount rare yet important occurrences. Robots are also often costly and fragile. This has led to popularity of learning-in-simulation with a fruitful history going back to decades ago $[68,56,17]$ and remaining an active topic today. The primary questions around this option are naturally around generalization from simulation to real-world: how to ensure I. the semantic complexity of the simulated environment is a good enough replica of the intricate real-world, and II. the rendered visual observation in simulation is close enough to what a camera in real-world would capture (photorealism).</p>
<p>We attempt to address some of these concerns and propose Gibson, a virtual environment for training and testing real-world perceptual agents. An arbitrary agent, e.g. a humanoid or a car (see Fig. 1) can be imported, it will be then embodied (i.e. contained by its physical body) and placed in a large and diverse set of real spaces. The agent is subject to constraints of space and physics (e.g. collision, gravity) through integration with a physics engine, but can freely perform any mobility task as long as the constraints are satisfied. Gibson provides a stream of visual observation from arbitrary viewpoints as if the agent had an on-board camera. Our novel rendering engine operates notably faster than real-time and works given sparsely scanned spaces, e.g. 1 panorama per 5-10 $m^{2}$.</p>
<p>The main goal of Gibson is to facilitate transferring the models trained therein to real-world, i.e. holding up the results when the stream of images switches to come from a real camera rather than Gibson's rendering engine. This is done by: first, resorting to the world itself to represent its own semantic complexity [85, 15] and forming the environment based off of scanned real spaces, rather than artificial ones [88, 51, 49]. Second, embedding a mechanism to dissolve differences between Gibson's renderings and what a real camera would produce. As a result, an image coming from a real camera vs the corresponding one from Gibson's rendering engine look statistically indistinguishable to the agent, and hence, closing the (perceptual) gap. This is done by employing a neural network based rendering approach which jointly trains a network for making render-
ings look more like real images (forward function) as well as a network which makes real images look like renderings (backward function). The two functions are trained to produce equal outputs, thus bridging the two domains. The backward function resembles deployment-time corrective glasses for the agent, so we call it Goggles.</p>
<p>Finally, we showcase a set of active perceptual tasks (local planning for obstacle avoidance, distant navigation, visual stair climbing) learned in Gibson. Our focus in this paper is on the vision aspect only. The statements should not be viewed to be necessarily generalizable to other aspects of learning in virtual environments, e.g. physics simulation.</p>
<p>Gibson Environment and our software stack are available to public for research purposes at http://gibson.vision/. Visualizations of Gibson space database can be seen here.</p>
<h2>2. Related Work</h2>
<p>Active Agents and Control: As discussed in Sec.1, operating and controlling active agents have been the focus of a massive body of work. A large portion of them are nonlearning based [53, 29, 52], while recent methods have attempted learning visuomotor policies end-to-end [106, 58] taking advantage of imitation learning [73], reinforcement learning $[78,44,77,44,5,6]$, acting by prediction [30] or self-supervision [40, 67, 30, 66, 46]. These methods are all potential users of (ours and other) virtual environments.</p>
<p>Virtual Environments for Learning: Conventionally vision is learned in static datasets [34, 28, 61] which are of limited use when it comes to active agent. Similarly, video datasets [57, 70, 101] are pre-recorded and thus passive. Virtual environments have been a remedy for this, classically [68] and today [106, 36, 31, 83, 47, 41, 11, 9, 72, 8, 98]. Computer games, e.g. Minecraft [49], Doom [51] and GTA5 [69] have been adapted for training and benchmarking learning algorithms. While these simulators are deemed reasonably effective for certain planning or control tasks, the majority of them are of limited use for perception and suffer from oversimplification of the visual world due to using synthetic underlying databases and/or rendering pipeline deficiencies. Gibson addresses some of such concerns by striving to target perception in real-world via using real spaces as its base, a custom neural view synthesizer, and a baked-in adaption mechanism, Goggles.</p>
<h2>Domain Adaptation and Transferring to Real-World:</h2>
<p>With popularity of simulators, different approaches for domain adaption for transferring the results to real world has been investigated [12, 27, 89, 75, 93, 99], e.g. via domain randomization [74, 93] or forming joint spaces [81]. Our approach is relatively simple and makes use of the fact that, in our case, large amounts of paired data for target-source domains are available enabling us to train forward and backward models to form a joint space. This makes us a bakedin mechanism in our environment for adaption, minimizing the need for additional and custom adaptation.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our view synthesis pipeline. The input is a sparse set of RGB-D Panoramas with their global camera pose. (a,b) Each RGB-D panorama is projected to the target camera pose and rendered. (b) View Selection determines from which panorama each target pixel should be picked, favoring panoramas that provide denser pixels for each region. (c) The pixels are selected and local gaps are interpolated with bilinear sampling. (d) A neural network (f) takes in the interpolated image and fills in the dis-occluded regions and fixes artifacts.</p>
<p>View Synthesis and Image-Based Rendering: Rendering novel views of objects and scenes is one of the classic problems in vision and graphics [80, 84, 91, 23, 60]. A number of relevantly recent methods have employed neural networks in a rendering pipeline, e.g. via an encoder-decoder like architecture that directly renders pixels [32, 55, 92] or predicts a flow map for pixels [105]. When some from of 3D information, e.g. depth, is available in the input [42, 62, 20, 82], the pipeline can make use of geometric approaches to be more robust to large viewpoint changes and implausible deformations. Further, when multiple images in the input are available, a smart selection mechanism (often referred to as Image Based Rendering) can help with lighting inconsistencies and handling more difficult and non-lambertian surfaces [43, 64, 94], compared to rendering from a textured mesh or as such entirely geometric methods. Our approach is a combination of above in which we geometrically render a base image for the target view, but resort to a neural network to correct artifacts and fill in the dis-occluded areas, along with jointly training a backward function for mapping real images onto the synthesized one.</p>
<h2>3 Real-World Perceptual Environment</h2>
<p>Gibson includes a neural network based view synthesis (described in Sec. 3.2) and a physics engine (described in Sec. 3.3). The underlying scene database and integrated agents are explained in sections 3.1 and 3.3, respectively.</p>
<h3>3.1 Gibson Database of Spaces</h3>
<p>Gibson's underlying database of spaces includes 572 full buildings composed of 1447 floors covering a total area of 211k mÂ². Each space has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. The base format of the data is similar to 2D-3D-Semantics dataset [10], but is more diverse and includes 2 orders of magnitude more spaces. Various 2D, 3D, and video visualizations of each space in Gibson database can be accessed here. This dataset is released in asset files of Gibson<sup>2</sup>.</p>
<p>We have also integrated 2D-3D-Semantics dataset [10] and Matterport3D [18] in Gibson for optional use.</p>
<h3>3.2 View Synthesis</h3>
<p>Our view synthesis module takes a sparse set of RGB-D panoramas in the input and renders a panorama from an arbitrary novel viewpoint. A 'view' is a 6D camera pose of x, y, z. Cartesian coordinates and roll, pitch, yaw angles, denoted as Î¸, Ï†, Î³. An overview of our view synthesis pipeline can be seen in Fig. 2. It is composed of a geometric point cloud rendering followed by a neural network to fix artifacts and fill in the dis-occluded areas, jointly trained with a backward function. Each step is described below:</p>
<p>Geometric Point Cloud Rendering. Scans of real spaces include sparsely captured images, leading to a sparse set of sampled lightings from the scene. The quality of sensory depth and 3D meshes are also limited by 3D reconstruction algorithms or scanning devices. Reflective surfaces or small objects are often poorly reconstructed or entirely missing. All these prevent simply rendering from textured meshes to be a sufficient approach to view synthesis.</p>
<p>We instead adopt a two-stage approach, with the first stage being geometrically rendering point clouds: the given RGB-D panoramas are transformed into point clouds and each pixel is projected from equirectangular coordinates to Cartesian coordinates. For the desired target view v<sub>j</sub> = (x<sub>j</sub>, y<sub>j</sub>, z<sub>j</sub>, Î¸<sub>j</sub>, Ï†<sub>j</sub>, Î³<sub>j</sub>), we choose the nearest k views in the scene database, denoted as v<sub>j,1</sub>, v<sub>j,2</sub>, . . . , v<sub>j,k</sub>. For each view v<sub>j,i</sub>, we transform the point cloud from v<sub>j,i</sub> coordinate to v<sub>j</sub> coordinate with a rigid body transformation and project the point cloud onto an equirectangular image. The pixels may open up and show a gap in-between, when rendered from the target view. Hence, the pixels that are supposed to be occluded may become visible through the gaps. To filter them out, we render an equirectangular depth as seen from the target view v<sub>j</sub> since we have the full reconstruction of the space. We then do a depth test and filter out the pixels with a difference &gt; 0.1m in their depth from the corresponding point in the target equirectangular depth. We now have sparse RGB points projected in equirectangulars for each reference panorama (see Fig. 2 (a)).</p>
<p>The points from all reference panoramas are aggregated to make one panorama using a locally weighted mixture (see Density Map in Fig. 2 (b)). We calculate the point density for each spatial position (average number of points per pixel) of each panorama, denoted as</p>
<p><sup>2</sup>Stanford AI lab has the copyright to all models.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Loss configuration for neural network based view synthesis. The loss contains two terms. The first is to transform the renderings to ground truth target images. The second is to alter ground truth target images to match the transformed rendering. A sample case is shown. (Best viewed on screen and zoomed-in.)</p>
<p><em>d</em><sub>1</sub>, . . . , <em>d</em><sub><em>k</em></sub>. For each position, the weight for view <em>i</em> is exp(Î»<sub><em>d</em></sub>d<sub><em>i</em></sub>)/âˆ‘<sub>m</sub> exp(Î»<sub><em>d</em></sub>d<sub>m</sub>), where Î»<sub><em>d</em></sub> is a hyperparameter. Hence, the points in the aggregated panorama are adaptively selected from all views, rather than superimposed blindly which would expose lighting inconsistency and misalignment artifacts.</p>
<p>Finally, we do a bilinear interpolation on the aggregated points in one equirectangular to reduce the empty space between rendered pixels (see Fig. 2 (c)).</p>
<p>See the first row of Fig. 6 which shows the so-far output still includes major artifacts, including stitching marks, deformed objects, or large dis-occluded regions.</p>
<p>Neural Network Based Rendering. We use a neural network, <em>f</em> or "filler", to fix artifacts and generate a more real looking image given the output of geometric point cloud rendering. We use a set of novelties to produce good results efficiently, including a stochastic identity initialization and adding color moment matching in perceptual loss.</p>
<p>Architecture: The architecture and hyperparameters of our convolutional neural network <em>f</em> are detailed in the supplementary material. We utilize dilated convolutions [102] to aggregate contextual information. We use a 18-layer network, with 3 Ã— 3 kernels for dilated convolution layers. The maximal dilation is 32. This allows us to achieve a large receptive field but not shrink the size of the feature map by too much. The minimal feature map size is Â¼ Â¼ of the original image size. We also use two architectures with the number of kernels being 48 or 256, depending on whether speed or quality is prioritized.</p>
<p>Identity Initialization: Though the output of the point cloud rendering suffers from notable artifacts, it is yet quite close to the ground truth target image numerically. Thus, an identity function (i.e. input image=output image) is a good place for initializing the neural network <em>f</em> at. We develop a stochastic approach to initializing the network at identity, to keep the weights nearly randomly distributed. We initialize <em>half</em> of the weights randomly with Gaussian and <em>freeze</em> them, then optimize the rest with back propagation to make the network's output the same as input. After convergence, the weights are our stochastic identity initialization. Other forms of identity initialization involve manually specifying the kernel weights, e.g. [22], which severely skews the distribution of weights (mostly 0s and some 1s). We found that to lead to slower converge and poorer results.</p>
<p>Loss: We use a perceptual loss [48] defined as:</p>
<p>$$D(I_1, I_2) = \sum_{l} \lambda_l ||\Psi_l(I_1) - \Psi_l(I_2)||<em i_j="i,j">1 + \gamma \sum</em>} ||\bar{I<em i_j="i,j">{1</em>}} - \bar{I<em i_j="i,j">{2</em> ||_1.$$}</p>
<p>For Î¨, we use a pretrained VGG16 [86]. Î¨<sub><em>l</em></sub>(<em>I</em>) denotes the feature map for input image <em>I</em> at <em>l</em>-th convolutional layer. We used all layers except for output layers. Î»<sub><em>l</em></sub> is a scaling coefficient normalized with the number of elements in the feature map. We found perceptual loss to be inherently lossy w.r.t. color information (different colors were projected on one point). Therefore, we add a term to enforce matching statistical moments of color distribution. <em>I</em><sub><em>i,j</em></sub> is the average color vector of a 32 Ã— 32 tile of the image which is enforced to be matching between <em>I</em><sub>1</sub> and <em>I</em><sub>2</sub> using L1 distance and Î³ is a mixture hyperparameter. We found our final setup to produce superior rendering results to GAN based losses (consistent with some recent works [21]).</p>
<h3>3.2.1 Closing the Gap with Real-World: Goggles</h3>
<p>With all of the imperfections in 3D inputs and geometric renderings, it is implausible to gain fully photo-realistic rendering with neural network fixes. Thus a domain gap with real images would remain. Therefore, we instead formulate the rendering problem as forming a joint space [81] (elaborated below) ensuring a correspondence between rendered and real images, and consequently, dissolving the gap.</p>
<p>If one wishes to create a mapping <em>S</em> â†¦ <em>T</em> between domain <em>S</em> and domain <em>T</em> by training a function <em>f</em>, usually a loss with the following form is optimized:</p>
<p>$$\mathcal{L} = \mathbb{E}\left[D\left(f(\mathcal{I}_s), \mathcal{I}_t\right)\right],\tag{1}$$</p>
<p>where <em>I</em><sub><em>s</em></sub> âˆˆ <em>S</em>, <em>I</em><sub><em>t</em></sub> âˆˆ <em>T</em>, and <em>D</em> is a distance function. However, in our case the mapping between <em>S</em> (renderings) and <em>T</em> (real images) is not bijective, or at least the two directions <em>S</em> â†¦ <em>T</em> and <em>T</em> â†¦ <em>S</em> do not appear to be equally difficult. As an example, there is no unique solution to dis-occlusion filling, so the domain gap cannot reach zero exercising only <em>S</em> â†¦ <em>T</em> direction. Hence, we add another function <em>u</em> to jointly utilize <em>T</em> â†¦ <em>S</em> and define the objective to be minimizing the distance between <em>f</em>(I<sub><em>s</em></sub>) and <em>u</em>(I<sub><em>t</em></sub>). Network <em>u</em> is trained to alter an image taken in real-world, I<sub><em>t</em></sub>, to look like the corresponding rendered image in Gibson, I<sub><em>s</em></sub>, after passing through network <em>f</em> (see Fig. 3). Function <em>u</em> can be seen as corrective glasses of the agent, thus the name <em>Goggles</em>.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Physics Integration and Embodiment. A Mujoco humanoid model is dropped onto a stairway demonstrating a physically plausible fall along with the corresponding visual observations by the humanoid's eye. The first and second rows show the physics engine view of 4 sampled time steps and their corresponding rendered RGB views, respectively.</p>
<p>To avoid the trivial solution of all images collapsing to a single point, we add the first term in the following final loss to enforce preserving a one-to-one mapping. The loss for training networks <em>u</em> and <em>f</em> is:</p>
<p>$$
\mathcal{L} = \mathbb{E}\left[D(f(\mathcal{I}_s), \mathcal{I}_t)\right] + \mathbb{E}\left[D(f(\mathcal{I}_s), u(\mathcal{I}_t))\right]. \tag{2}
$$</p>
<p>See Fig. 3 for a visual example. <em>D</em> is the distance defined in Sec 3.2. We use the same network architecture for <em>f</em> and <em>u</em>.</p>
<h3>3.3. Embodiment and Physics Integration</h3>
<p>Perception and physical constraints are closely related. For instance, the perception model of a human-sized agent should seamlessly develop the notion that it does not fit in the gap under the door and hence should not attend such areas when solving a navigation task; a mouse-sized agent though could fit and its perception should attend such areas. It is thus important for the agent to be constantly subject to constraints of space and physics, e.g., collision, gravity, friction, throughout learning.</p>
<p>We integrated Gibson with a physics engine PyBullet [26] which supports rigid body and soft body simulation with discrete and continuous collision detection. We also use PyBullet's built-in fast collision handling system to record agent's certain interactions, such as how many times it collides with physical obstacles. We use Coulomb friction model by default, as scanned models do not come with material property annotations and certain physics aspects, such as friction, cannot be directly simulated.</p>
<p><strong>Agents:</strong> Gibson supports importing arbitrary agents with URDFs. Also, a number of agents are integrated as entry points, including humanoid and ant of Roboschool [4, 79], husky car [1], drone, minitaur [3], Jackrabbot [2]. Agent models are in ROS or Mujoco XML format.</p>
<p><strong>Integrated Controllers:</strong> To enable (optionally) abstracting away low-level control and robot dynamics for the tasks that are wished to be approached in a more high-level manner, we also provide a set of practical and ideal controllers to deduce the complexity of learning to control from scratch. We integrated a PID controller and a Nonholonomic controller as well as an ideal positional controller which completely abstracts away agent's motion dynamics.</p>
<h3>3.4. Additional Modalities</h3>
<p>Besides rendering RGB images, Gibson provides additional channels, such as depth, surface normals, and semantics. Unlike RGB images, these channels are more robust to noise in input data and lighting changes, and we render them directly from mesh files. Geometric modalities, e.g., depth, are provided for all models and semantics are available for 52,561 <em>m</em><sup>2</sup> of area with semantic annotations from 2D-3D-S [10] and Matterport3D [18] datasets.</p>
<p>Similar to other robotic simulation platforms, we also provide configurable proprioceptive sensory data. A typical proprioceptive sensor suite includes information of joint positions, angle velocity, robot orientation with respect to navigation target, position and velocity. We refer to this typical setup as "non-visual sensory" to distinguish from "visual" modalities in the rest of the paper.</p>
<h2>4. Tasks</h2>
<p><strong>Input-Output Abstraction:</strong> Gibson allows defining arbitrary tasks for an agent. To provide a common abstraction for this, we follow the interface of OpenAI Gym [14]: at each timestep, the agent performs an action at the environment; then the environment runs a forward step (integrated with the physics engine) and returns the accordingly rendered visual observation, reward, and termination signal. We also provide utility functions to keyboard operate an agent or visualize a recorded run.</p>
<h3>4.1. Experimental Validation Tasks</h3>
<p>In our experiments, we use a set of sample active perceptual tasks and static-recognition tasks to validate Gibson. The active tasks include:</p>
<p><strong>Local Planning and Obstacle Avoidance:</strong> An agent is randomly placed in an environment and needs to travel to a random nearby target location provided as relative coordinates (similar to flag run [4]). The agent receives no information about the environment except a continuous stream of depth and/or RGB frames and needs to plan perceptually (e.g., go around a couch to reach the target behind).</p>
<p><strong>Distant Visual Navigation:</strong> Similar to the previous task, but the target location is significantly further away and fixed. Agent's initial location is still randomized. This is similar to the task of auto-docking for robots from a distant location. Agent receives no external odometry or GPS information, and needs to form a contextual map to succeed.</p>
<p><strong>Stair Climb:</strong> An (ant [4]) agent is placed on one top of a stairway and the target location is at the bottom. It needs to learn a controller for its complex dynamics to plausibly go down the stairway without flipping, using visual inputs.</p>
<p>To benchmark how close to real images the renderings of Gibson are, we used two static-recognition tasks: depth estimation and scene classification. We train a neural network using (<em>rendering</em>, <em>ground truth</em>) pairs as training</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Sample spaces in Gibson database. The spaces are diverse in terms of size, visuals, and function, e.g. businesses, construction sites, houses. Upper: Sample 3D models. Lower: Sample images from Gibson database (left) and some of other environments [31, 49, 71, 83, 51, 100, 37, 106] (right).</p>
<p>data, but test them on (real image, ground truth). If Gibson renderings are close enough to real images and Goggles mechanism is effective, test results on real images are expected to be satisfactory. This also enables quantifying the impact of Goggles, i.e. using $u(\mathcal{I}_t)$ vs. $\mathcal{I}_s$, $f(\mathcal{I}_s)$, and $\mathcal{I}_t$.</p>
<p><strong>Depth Estimation:</strong> Predicting depth given a single RGB image, similar to [33]. We train 4 networks to predict the depth given one of the following 4 as input images: $\mathcal{I}_s$ (preneural network rendering), $f(\mathcal{I}_s)$ (post-neural network rendering), $u(\mathcal{I}_t)$ (real image seen with Goggles), and $\mathcal{I}_t$ (real image). We compare the performance of these in Sec. 5.3.</p>
<p><strong>Scene Classification:</strong> The same as previous task, but the output is scene classes rather than depth. As our images do not have scene class annotations, we generate them using a well performing network trained on Places dataset [104].</p>
<h2>5. Experimental Results</h2>
<h3>5.1. Benchmarking Space Databases</h3>
<p>The spaces in Gibson database are collected using various scanning devices, including NavVis, Matterport, or DotProduct, covering a diverse set of spaces, e.g. offices, garages, stadiums, grocery stores, gyms, hospitals, houses. All spaces are fully reconstructed in 3D and post processed to fill the holes and enhance the mesh. We benchmark some of the existing synthetic and real databases of spaces (SUNCG [88] and Matterport3D [18]) vs Gibson's using the following metrics in Table 1:</p>
<p><strong>Specific Surface Area (SSA):</strong> the ratio of inner mesh surface and volume of convex hull of the mesh. This is a measure of clutter in the models.</p>
<p><strong>Navigation Complexity:</strong> Longest $A^<em>$ navigation distance between randomly placed two points divided by the straight line distance. We compute the highest navigation complexity $\max_{s_i, s_j} \frac{d_{A^</em>, (s_i, s_j})}{d_{l^*, (s_i, s_j})}$ for every model.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Gibson</th>
<th>SUNCG</th>
<th>Matterport3D</th>
</tr>
</thead>
<tbody>
<tr>
<td>Number of Spaces</td>
<td>572</td>
<td>45622</td>
<td>90</td>
</tr>
<tr>
<td>Total Coverage $m^2$</td>
<td>211k</td>
<td>5.8M</td>
<td>46.6K</td>
</tr>
<tr>
<td>SSA</td>
<td>1.38</td>
<td>0.74</td>
<td>0.92</td>
</tr>
<tr>
<td>Nav. Complexity</td>
<td>5.98</td>
<td>2.29</td>
<td>7.80</td>
</tr>
<tr>
<td>Real-World Transfer Err</td>
<td>0.92Â§</td>
<td>2.89â€ </td>
<td>2.11â€ </td>
</tr>
</tbody>
</table>
<p>Table 1: Benchmarking Space Databases: Comparison of Gibson database with SUNCG [88] (hand designed synthetic), and Matterport3D [18]. Â§ Rendered with Gibson, â€  rendered with MINOS [76].</p>
<p><strong>Real-World Transfer Error:</strong> We train a neural network for depth estimation using the images of each database and test them on real images of 2D-3D-S dataset [10]. Training images of SUNCG and Matterport3D are rendered using MINOS [76] and our dataset is rendered using Gibson's engine. The training set of each database is 20k random RGB-depth image pairs with 90Â° field of view. The reported value is average depth estimation error in meters.</p>
<p><strong>Scene Diversity:</strong> We perform scene classification on 10k randomly picked images for each database using a network pretrained on [104]. We report the entropy of the distribution of top-1 classes for each environment. Gibson, SUNCG [88], and THOR [106] gain the scores of 3.72, 2.89, and 3.32, respectively (highest possible entropy = 5.90).</p>
<h3>5.2. Evaluation of View Synthesis</h3>
<p>To train the networks $f$ and $u$ of our neural network based synthesis framework, we sampled 4.3k 1024 Ã— 2048 $\mathcal{I}_s$â€”$\mathcal{I}_t$ panorama pairs and randomly cropped them to 256 Ã— 256. We use Adam [54] optimizer with learning rate 2 Ã— 10$^{-4}$. We first train $f$ for 50 epochs until convergence, then we train $f$ and $u$ jointly for another 50 epochs with learning rate 2 Ã— 10$^{-5}$. The learning finishes in 3 days on 2 Nvidia Titan X GPUs.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Qualitative results of view synthesis and Goggles. Top to bottom rows show images before neural network correction, after neural network correction, target image seen through Goggles, and target image (i.e. ground truth real image). The first column shows a pano and the rest are sample zoomed-in patches. Note the high similarity between 2nd and 3rd row, signifying the effectiveness of Goggles. (Best viewed on screen and zoomed-in.)</p>
<table>
<thead>
<tr>
<th>Resolution</th>
<th>128x128</th>
<th>256x256</th>
<th>512x512</th>
</tr>
</thead>
<tbody>
<tr>
<td>RGBD, pre network f</td>
<td>109.1</td>
<td>58.5</td>
<td>26.5</td>
</tr>
<tr>
<td>RGBD, post network f</td>
<td>77.7</td>
<td>30.6</td>
<td>14.5</td>
</tr>
<tr>
<td>RGBD, post small network f</td>
<td>87.4</td>
<td>40.5</td>
<td>21.2</td>
</tr>
<tr>
<td>Depth only</td>
<td>253.0</td>
<td>197.9</td>
<td>124.7</td>
</tr>
<tr>
<td>Surface Normal only</td>
<td>207.7</td>
<td>129.7</td>
<td>57.2</td>
</tr>
<tr>
<td>Semantic only</td>
<td>190.0</td>
<td>144.2</td>
<td>55.6</td>
</tr>
<tr>
<td>Non-Visual Sensory</td>
<td>396.1</td>
<td>396.1</td>
<td>396.1</td>
</tr>
</tbody>
</table>
<p>Table 2: Rendering speed (FPS) of Gibson on a single GPU for different resolutions and output configurations. Tested on E5-2697 v4 with Tesla V100 in headless rendering mode. As a faster setup ("small network"), we also trained a smaller filler network with donwsized input geometric renderings. This setup achieves a higher FPS at the expense of inferior visual quality compared to full-size filler network.</p>
<p>Sample renderings and their corresponding real image (ground truth) are shown in Fig. 6. Note that pre-neural network renderings suffer from geometric artifacts which are partially resolved in post-neural network results. Also, though the contrast of the post-neural network images is lower than real ones and color distributions are still different, Goggles could effectively alter the real images to match the renderings (compare 2nd and 3rd rows). In additional, the network f and Goggles u jointly addressed some of the pathological domain gaps. For instance, as lighting fixtures are often thin and shiny, they are not well reconstructed in our meshes and usually fail to render properly. Network f and Goggles learned to just suppress them altogether from images to not let a domain gap remain. The scene out the windows also often have large re-projection errors, so they are usually turned white by f and u.</p>
<p>Appearance columns in Table 3 quantify view synthesis results in terms image similarity metrics L1 and SSIM. They echo that the smallest gap is between f(2s) and u(2t).</p>
<p>Rendering Speed of Gibson is provided in Table 2.</p>
<h3>5.3 Transferring to Real-World</h3>
<p>We quantify the effectiveness of Goggles mechanism in reducing the domain gap between Gibson renderings and real imagery in two ways: via the static-recognition tasks</p>
<table>
<thead>
<tr>
<th>Train</th>
<th>Test</th>
<th>Static Tasks</th>
<th></th>
<th>Appearance</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Scene Class Acc.</td>
<td>Depth Est. Error</td>
<td>SSIM</td>
<td>L1</td>
</tr>
<tr>
<td>2s</td>
<td>2t</td>
<td>0.280</td>
<td>1.026</td>
<td>0.627</td>
<td>0.096</td>
</tr>
<tr>
<td>f(2s)</td>
<td>2t</td>
<td>0.266</td>
<td>1.560</td>
<td>0.480</td>
<td>0.10</td>
</tr>
<tr>
<td>f(2s)</td>
<td>u(2t)</td>
<td>0.291</td>
<td>0.915</td>
<td>0.816</td>
<td>0.051</td>
</tr>
</tbody>
</table>
<p>Table 3: Evaluation of view synthesis and transferring to real-world. Static Tasks column shows on both scene classification task and depth estimation tasks, it is easiest to transfer from f(2s) to u(2t) compared with other cross-domain transfers. Appearance columns compare L1 and SSIM distance metrics for different pairs showing the combination of network f and Goggles u achieves best results.</p>
<p>described in Sec. 4.1 and by comparing image distributions.</p>
<p>Evaluation of transferring to real images via scene classification and depth estimation are summarized in Table. 3. Also, Fig. 7 (a) provides depth estimation results for all feasible train-test combinations for reference. The diagonal values of the 4 Ã— 4 matrix represent training and testing on the same domain. The gold standard is train and test on 2t (real images) which yields the error of 0.86. The closest combination to that in the entire table is train on f(1s) (f output) and test on u(1t) (real image through Goggles) giving 0.91, which signifies the effectiveness of Goggles.</p>
<p>In terms of distributional quantification, we used two metrics of Maximum Mean Discrepancy (MMD) [39] and CORAL [90] to test how well f(2s) and u(2t) domains are aligned. The metrics essentially determine how likely it is for two samples to be drawn from different distributions. We calculate MMD and CORAL values using the features of the last convolutional layer of VGG16 [86] and kernel k(x, y) = xT y. Results are summarized in Fig. 7 (b) and (c). For each metric, f(2s) - u(2t) is smaller than other pairs, showing that the two domains are well matching.</p>
<p>In order to quantitatively show the networks f and u do not give degenerate solutions (i.e. collapsing all images to few points to close the gap by cheating), we use f(2s) and u(2t) as queries to retrieve their nearest neighbor using VGG16 features from 2s and 2t, respectively. Top-1, 2 and 5 accuracies for f(2s) â†¦ 2s are 91.6%, 93.5%, 95.6%.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Evaluation of transferring to real-world from Gibson. (a) Error of depth estimation for all train-test combinations. (b,c) MMD and CORAL distributional distances. All tests are in support of Goggles.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Visual Local planning and obstacle avoidance. Reward curves for perceptual vs non-perceptual husky agents and a sample trajectory.</p>
<p>Top-1, 2 and 5 accuracies for $u(\mathcal{I}<em t="t">{t}) \mapsto \mathcal{I}</em>$ are $85.9 \%$, $87.2 \%, 89.6 \%$. This indicates a good correspondence between pre and post neural network images is preserved, and thus, no collapse is observed.</p>
<h3>5.4 Validation Tasks Learned in Gibson</h3>
<p>The results of the active perceptual tasks discussed in Sec. 4.1 are provided here. In each experiment, the nonvisual sensor outputs include agent position, orientation, and relative position to target. The agents are rewarded by the decrease in their distance towards their targets. In Local Planning and Visual Obstacle Avoidance, they receive an additional penalty for every collision.</p>
<h4>Local Planning and Visual Obstacle Avoidance Results</h4>
<p>We trained a perceptual and non-perceptual husky agent according to the setting in Sec. 4.1 with PPO [78] for 150 episodes ( 300 iterations, 150k frames). Both agents have a four-dimensional discrete action space: forward/backward/left/right. The average reward over 10 iterations are plotted in Fig 8. The agent with perception achieves a higher score and developed obstacle avoidance behavior to reach the goal faster.</p>
<h4>Distant Visual Navigation Results</h4>
<p>Fig. 9 shows the target and sample random initial locations as well as the reward curves. Global navigation behavior emerges after 1700 episodes ( 680 k frames), and only the agent with visual state was able to accomplish the task. The action space is the same as previous experiment.</p>
<p>Also, we use the trained policy of distant navigation to evaluate the impact of Goggles on an active task: we go to camera locations where $\mathcal{I}<em t="t">{t}$ is available. Then we measure the policy discrepancy in terms of L2 distance of output action logits when different renderings and $\mathcal{I}</em>}$ are provided as input. Training on $f(\mathcal{I<em t="t">{s})$ and testing on $u(\mathcal{I}</em>})$ yields discrepancy of 0.204 (best), while training on $f(\mathcal{I<em t="t">{s})$ and testing on $\mathcal{I}</em>}$ gives 0.300 and training on $\mathcal{I<em t="t">{s}$ and testing on $\mathcal{I}</em>$</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Distant Visual Navigation. The initial locations and target are shown. The agent succeeds only when provided with visual inputs.</p>
<p>gives 0.242. After the initial release of our work, a paper recently reported an evaluation done on a real robot for adaptation using backward mapping from real images to renderings [103], with positive results. They did not use paired data, unlike Gibson, which would be expected to further enhance the results.</p>
<h4>Stair Climb</h4>
<p>As explained in Sec. 4.1, an ant [4] is trained to perform the complex locomotive task of plausibly climbing down a stairway without flipping. The action space is eight dimensional continuous torque values. We train one perceptual and one non-perceptual agent starting at a fixed initial location, but at test time slightly and randomly move their initial and target location around. They start to acquire stair-climbing skills after 1700 episodes ( 700 k time steps). While the perceptual agent learned slower, it showed better generalizability at test time coping with the location shifts and outperformed the nonperceptual agent by $70 \%$. Full details of this experiment is provided in the supplementary material.</p>
<h3>6 Limitations and Conclusion</h3>
<p>We presented Gibson Environment for developing real-world perception for active agents and validated it using a set of tasks. While we think this is a step forward, there are some limitations that should be noted. First, though Gibson provides a good basis for learning complex navigation and locomotion, currently it does not include dynamic content (e.g. other moving objects) and does not support manipulation. This can be potentially solved by integrating our approach with synthetic objects [19, 50]. Second, we do not have full material properties and no existing physics simulator is optimal; this may lead to physics related domain gaps. Finally, we provided quantitative evaluations of Goggles mechanism for transferring to real world mostly using static recognition tasks. The ultimate test is evaluating Goggles on real robots.</p>
<h4>Acknowledgement</h4>
<p>We gratefully acknowledge the support of Facebook, Toyota (1186781-31-UDARO), ONR MURI (N00014-14-1-0671), ONR (1165419-10-TDAUZ); Nvidia, CloudMinds, Panasonic (1192707-1-GWMSX).</p>
<h2>References</h2>
<p>[1] Husky UGV - Clearpath Robotics. http://wiki.ros. org/Robots/Husky. Accessed: 2017-09-30. 5
[2] Jackrabbot - Stanford Vision and Learning Group. http://cvgl.stanford.edu/projects/ jackrabbot/. Accessed: 2018-01-30. 5
[3] Legged UGVs - Ghost Robotics. https://www. ghostrobotics.io/copy-of-robots. Accessed: 2017-09-30. 5
[4] OpenAI Roboschool. http://blog.openai.com/ roboschool/. Accessed: 2018-02-02. 5, 8
[5] P. Abbeel, A. Coates, and A. Y. Ng. Autonomous helicopter aerobatics through apprenticeship learning. The International Journal of Robotics Research, 29(13):1608-1639, 2010. 2
[6] P. Abbeel, A. Coates, M. Quigley, and A. Y. Ng. An application of reinforcement learning to aerobatic helicopter flight. In Advances in neural information processing systems, pages 1-8, 2007. 2
[7] P. Agrawal, A. V. Nair, P. Abbeel, J. Malik, and S. Levine. Learning to poke by poking: Experiential learning of intuitive physics. In Advances in Neural Information Processing Systems, pages 5074-5082, 2016. 2
[8] P. Ammirato, P. Poirson, E. Park, J. KoÅ¡eckÃ¡, and A. C. Berg. A dataset for developing and benchmarking active vision. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 1378-1385. IEEE, 2017. 2
[9] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. SÃ¼nderhauf, I. Reid, S. Gould, and A. van den Hengel. Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2
[10] I. Armeni, A. Sax, A. R. Zamir, and S. Savarese. Joint 2D-3D-Semantic Data for Indoor Scene Understanding. ArXiv e-prints, Feb. 2017. 3, 5, 6
[11] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res.(JAIR), 47:253-279, 2013. 2
[12] J. Blitzer, R. McDonald, and F. Pereira. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120-128. Association for Computational Linguistics, 2006. 2
[13] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan. Linear matrix inequalities in system and control theory. SIAM, 1994. 2
[14] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016. 5
[15] R. A. Brooks. Elephants don't play chess. Robotics and autonomous systems, 6(1-2):3-15, 1990. 1, 2
[16] R. A. Brooks. Intelligence without representation. Artificial intelligence, 47(1-3):139-159, 1991. 1
[17] J. Carbonell and G. Hood. The world modelers project: Objectives and simulator architecture. In Machine Learning, pages 29-34. Springer, 1986. 2
[18] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. NieÃŸner, M. Savva, S. Song, A. Zeng, and Y. Zhang. Matterport3d: Learning from rgb-d data in indoor environments. arXiv preprint arXiv:1709.06158, 2017. 3, 5, 6
[19] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. 8
[20] C.-F. Chang, G. Bishop, and A. Lastra. Ldi tree: A hierarchical representation for image-based rendering. In Proceedings of the 26th annual conference on Computer graphics and interactive techniques, pages 291-298. ACM Press/Addison-Wesley Publishing Co., 1999. 3
[21] Q. Chen and V. Koltun. Photographic image synthesis with cascaded refinement networks. arXiv preprint arXiv:1707.09405, 2017. 4
[22] Q. Chen, J. Xu, and V. Koltun. Fast image processing with fully-convolutional networks. In IEEE International Conference on Computer Vision, 2017. 4
[23] S. E. Chen and L. Williams. View interpolation for image synthesis. In Proceedings of the 20th annual conference on Computer graphics and interactive techniques, pages 279288. ACM, 1993. 3
[24] P. S. Churchland, V. S. Ramachandran, and T. J. Sejnowski. A critique of pure vision. Large-scale neuronal theories of the brain, pages 23-60, 1994. 1
[25] F. Codevilla, M. MÃ¼ller, A. Dosovitskiy, A. LÃ³pez, and V. Koltun. End-to-end driving via conditional imitation learning. arXiv preprint arXiv:1710.02410, 2017. 2
[26] E. Coumans and Y. Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. http://pybullet.org, 2016-2018. 5
[27] H. DaumÃ© III. Frustratingly easy domain adaptation. arXiv preprint arXiv:0907.1815, 2009. 2
[28] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248-255. IEEE, 2009. 2
[29] J. P. Desai, J. P. Ostrowski, and V. Kumar. Modeling and control of formations of nonholonomic mobile robots. IEEE transactions on Robotics and Automation, 17(6):905908, 2001. 2
[30] A. Dosovitskiy and V. Koltun. Learning to act by predicting the future. arXiv preprint arXiv:1611.01779, 2016. 2
[31] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun. Carla: An open urban driving simulator. In Conference on Robot Learning, pages 1-16, 2017. 2, 6
[32] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. In Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 1538-1546. IEEE, 2015. 3
[33] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in neural information processing systems, pages 2366-2374, 2014. 6</p>
<p>[34] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303338, 2010.2
[35] C. Finn, X. Y. Tan, Y. Duan, T. Darrell, S. Levine, and P. Abbeel. Deep spatial autoencoders for visuomotor learning. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 512-519. IEEE, 2016. 2
[36] A. Gaidon, Q. Wang, Y. Cabon, and E. Vig. Virtual worlds as proxy for multi-object tracking analysis. In CVPR, 2016. 2
[37] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):1231-1237, 2013. 6
[38] J. J. Gibson. The ecological approach to visual perception. Psychology Press, 2013. 1
[39] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. SchÃ¶lkopf, and A. Smola. A kernel two-sample test. Journal of Machine Learning Research, 13(Mar):723-773, 2012. 7
[40] A. Gupta. Supersizing self-supervision: Learning perception and action without human supervision. 2016. 2
[41] S. Gupta, J. Davidson, S. Levine, R. Sukthankar, and J. Malik. Cognitive mapping and planning for visual navigation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2616-2625, 2017. 2
[42] R. Hartley and A. Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003. 3
[43] P. Hedman, T. Ritschel, G. Drettakis, and G. Brostow. Scalable inside-out image-based rendering. ACM Transactions on Graphics (TOG), 35(6):231, 2016. 3
[44] N. Heess, S. Sriram, J. Lemmon, J. Merel, G. Wayne, Y. Tassa, T. Erez, Z. Wang, A. Eslami, M. Riedmiller, et al. Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286, 2017. 2
[45] R. Held and A. Hein. Movement-produced stimulation in the development of visually guided behavior. Journal of comparative and physiological psychology, 56(5):872, 1963. 1
[46] N. Hirose, A. Sadeghian, M. VÃ¡zquez, P. Goebel, and S. Savarese. Gonet: A semi-supervised deep learning approach for traversability estimation. arXiv preprint arXiv:1803.03254, 2018. 2
[47] C. Jiang, Y. Zhu, S. Qi, S. Huang, J. Lin, X. Guo, L.-F. Yu, D. Terzopoulos, and S.-C. Zhu. Configurable, photorealistic image rendering and ground truth synthesis by sampling stochastic grammars representing indoor scenes. arXiv preprint arXiv:1704.00112, 2017. 2
[48] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, pages 694-711. Springer, 2016. 4
[49] M. Johnson, K. Hofmann, T. Hutton, and D. Bignell. The malmo platform for artificial intelligence experimentation. In IJCAI, pages 4246-4247, 2016. 2, 6
[50] K. Karsch, V. Hedau, D. Forsyth, and D. Hoiem. Rendering synthetic objects into legacy photographs. In ACM Transactions on Graphics (TOG), volume 30, page 157. ACM, 2011. 8
[51] M. Kempka, M. Wydmuch, G. Runc, J. Toczek, and W. JaÅ›kowski. Vizdoom: A doom-based ai research platform for visual reinforcement learning. In Computational Intelligence and Games (CIG), 2016 IEEE Conference on, pages 1-8. IEEE, 2016. 2, 6
[52] O. Khatib. Real-time obstacle avoidance for manipulators and mobile robots. In Autonomous robot vehicles, pages 396-404. Springer, 1986. 2
[53] O. Khatib. A unified approach for motion and force control of robot manipulators: The operational space formulation. IEEE Journal on Robotics and Automation, 3(1):4353, 1987. 2
[54] D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6
[55] T. D. Kulkarni, W. F. Whitney, P. Kohli, and J. Tenenbaum. Deep convolutional inverse graphics network. In Advances in Neural Information Processing Systems, pages 2539-2547, 2015. 3
[56] P. Langley, D. Nicholas, D. Klahr, and G. Hood. A simulated world for modeling learning and development. In Proceedings of the Third Conference of the Cognitive Science Society, pages 274-276, 1981. 2
[57] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1-8. IEEE, 2008. 2
[58] S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):1-40, 2016. 2
[59] S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. The International Journal of Robotics Research, page 0278364917710318, 2016. 2
[60] M. Levoy and P. Hanrahan. Light field rendering. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 31-42. ACM, 1996. 3
[61] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. DollÃ¡r, and C. L. Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014. 2
[62] W. R. Mark, L. McMillan, and G. Bishop. Post-rendering 3d warping. In Proceedings of the 1997 symposium on Interactive 3D graphics, pages 7-ff. ACM, 1997. 3
[63] R. Mottaghi, H. Bagherinezhad, M. Rastegari, and A. Farhadi. Newtonian scene understanding: Unfolding the dynamics of objects in static images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3521-3529, 2016. 2
[64] R. Ortiz-Cayon, A. Djelouah, and G. Drettakis. A bayesian approach for selective image-based rendering using superpixels. In International Conference on 3D Vision-3DV, 2015. 3
[65] A. R. Parker. On the origin of optics. Optics \&amp; Laser Technology, 43(2):323-329, 2011. 1
[66] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-supervised prediction. arXiv preprint arXiv:1705.05363, 2017. 2</p>
<p>[67] L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 3406-3413. IEEE, 2016. 2
[68] D. A. Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural information processing systems, pages 305-313, 1989. 2
[69] S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing for data: Ground truth from computer games. In European Conference on Computer Vision, pages 102-118. Springer, 2016. 2
[70] M. D. Rodriguez, J. Ahmed, and M. Shah. Action mach a spatio-temporal maximum average correlation height filter for action recognition. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1-8. IEEE, 2008. 2
[71] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. Lopez. The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes. 2016. 6
[72] G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez. The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3234-3243, 2016. 2
[73] S. Ross, G. J. Gordon, and D. Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In International Conference on Artificial Intelligence and Statistics, pages 627-635, 2011. 2
[74] F. Sadeghi and S. Levine. rl: Real singleimage flight without a single real image. arxiv preprint. arXiv preprint arXiv:1611.04201, 12, 2016. 2
[75] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In European conference on computer vision, pages 213-226. Springer, 2010. 2
[76] M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun. Minos: Multimodal indoor simulator for navigation in complex environments. arXiv preprint arXiv:1712.03931, 2017. 6
[77] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1889-1897, 2015. 2
[78] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2, 8
[79] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal Policy Optimization Algorithms. ArXiv e-prints, July 2017. 5
[80] S. M. Seitz and C. R. Dyer. View morphing. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques, pages 21-30. ACM, 1996. 3
[81] O. Sener, H. O. Song, A. Saxena, and S. Savarese. Learning transferrable representations for unsupervised domain adaptation. In Advances in Neural Information Processing Systems, pages 2110-2118, 2016. 2, 4
[82] J. Shade, S. Gortler, L.-w. He, and R. Szeliski. Layered depth images. In Proceedings of the 25th annual conference
on Computer graphics and interactive techniques, pages 231-242. ACM, 1998. 3
[83] S. Shah, D. Dey, C. Lovett, and A. Kapoor. Airsim: Highfidelity visual and physical simulation for autonomous vehicles. In Field and Service Robotics, 2017. 2, 6
[84] E. Shechtman, A. Rav-Acha, M. Irani, and S. Seitz. Regenerative morphing. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, pages 615-622. IEEE, 2010. 3
[85] H. A. Simon. The sciences of the artificial. MIT press, 1996. 2
[86] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 4, 7
[87] L. Smith and M. Gasser. The development of embodied cognition: Six lessons from babies. Artificial life, 11(1-2):13-29, 2005. 1
[88] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. Semantic scene completion from a single depth image. IEEE Conference on Computer Vision and Pattern Recognition, 2017. 2, 6
[89] B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In AAAI, volume 6, page 8, 2016. 2
[90] B. Sun and K. Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Computer Vision-ECCV 2016 Workshops, pages 443-450. Springer, 2016. 7
[91] S. Suwajanakorn, I. Kemelmacher-Shlizerman, and S. M. Seitz. Total moving face reconstruction. In European Conference on Computer Vision, pages 796-812. Springer, 2014. 3
[92] M. Tatarchenko, A. Dosovitskiy, and T. Brox. Multi-view 3d models from single images with a convolutional network. In European Conference on Computer Vision, pages 322-337. Springer, 2016. 3
[93] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. arXiv preprint arXiv:1703.06907, 2017. 2
[94] M. Waechter, N. Moehrle, and M. Goesele. Let there be color! large-scale texturing of 3d reconstructions. In European Conference on Computer Vision, pages 836-850. Springer, 2014. 3
[95] D. M. Wolpert and Z. Ghahramani. Computational principles of movement neuroscience. Nature neuroscience, 3:1212-1217, 2000. 1
[96] J. Wu, J. J. Lim, H. Zhang, J. B. Tenenbaum, and W. T. Freeman. Physics 101: Learning physical object properties from unlabeled videos. In $B M V C$, volume 2, page 7, 2016. 2
[97] J. Wu, I. Yildirim, J. J. Lim, B. Freeman, and J. Tenenbaum. Galileo: Perceiving physical object properties by integrating a physics engine with deep learning. In Advances in neural information processing systems, pages 127-135, 2015. 2
[98] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian. Building generalizable agents with a realistic and rich 3d environment. arXiv preprint arXiv:1801.02209, 2018. 2</p>
<p>[99] M. Wulfmeier, A. Bewley, and I. Posner. Addressing appearance change in outdoor robotics with adversarial domain adaptation. arXiv preprint arXiv:1703.01461, 2017. 2
[100] B. Wymann, E. EspiÃ©, C. Guionneau, C. Dimitrakakis, R. Coulom, and A. Sumner. Torcs, the open racing car simulator. Software available at http://torcs. sourceforge. net, 4, 2000. 6
[101] H. Xu, Y. Gao, F. Yu, and T. Darrell. End-to-end learning of driving models from large-scale video datasets. arXiv preprint arXiv:1612.01079, 2016. 2
[102] F. Yu and V. Koltun. Multi-scale context aggregation by dilated convolutions. arXiv preprint arXiv:1511.07122, 2015. 4
[103] J. Zhang, L. Tai, Y. Xiong, M. Liu, J. Boedecker, and W. Burgard. Vr goggles for robots: Real-to-sim domain adaptation for visual control. arXiv preprint arXiv:1802.00265, 2018. 8
[104] B. Zhou, A. Lapedriza, A. Khosla, A. Oliva, and A. Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. 6
[105] T. Zhou, S. Tulsiani, W. Sun, J. Malik, and A. A. Efros. View synthesis by appearance flow. In European Conference on Computer Vision, pages 286-301. Springer, 2016. 3
[106] Y. Zhu, R. Mottaghi, E. Kolve, J. J. Lim, A. Gupta, L. FeiFei, and A. Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 3357-3364. IEEE, 2017. 2, 6</p>            </div>
        </div>

    </div>
</body>
</html>
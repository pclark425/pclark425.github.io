<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3284 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3284</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3284</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f" target="_blank">Hypothesis Search: Inductive Reasoning with Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> An automated pipeline using LLM summaries achieves 30% accuracy, outperforming the direct prompting baseline, and performance is boosted to 33%.</p>
                <p><strong>Paper Abstract:</strong> Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which robustly generalize to novel scenarios. Recent work evaluates large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding"in context learning."This works well for straightforward inductive tasks but performs poorly on complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be verified by running on observed examples and generalized to novel inputs. To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, string transformation dataset SyGuS, and list transformation dataset List Functions. On a random 100-problem subset of ARC, our automated pipeline using LLM summaries achieves 30% accuracy, outperforming the direct prompting baseline (accuracy of 17%). With the minimal human input of selecting from LLM-generated candidates, performance is boosted to 33%. Our ablations show that both abstract hypothesis generation and concrete program representations benefit LLMs on inductive reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3284.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3284.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Direct Prompting (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Direct In-Context Learning Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline method that prompts an LLM with training input-output examples and asks it to directly predict outputs for novel inputs (standard in-context learning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hypothesis Search: Inductive Reasoning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613 / gpt-4-0314 reported)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 family (chat-completion API variants gpt-4-0613 and earlier gpt-4-0314) used for generation and code synthesis; large pre-trained transformer with multi-modal prompt handling in experiments via textual grid encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['in-context learning (direct prediction)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Models are given training input-output pairs in the prompt and asked to directly generate the target outputs for test examples (no intermediate hypothesis or program representation).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single/similar style (direct single-step ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ARC, 1D-ARC, SyGuS, List Functions (as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Inductive reasoning benchmarks: ARC (2D grid visual tasks), 1D-ARC (one-dimensional adaptation), SyGuS (string transformation/program synthesis), List Functions (list-to-list transformations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>ARC: 17% (GPT-4, top-1 on 100 sampled tasks); 1D-ARC: 39.6% (from Xu et al. baseline reported in paper); List Functions: 31%; SyGuS: not applicable (SyGuS evaluated as program induction).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct prompting is the baseline and is outperformed by methods that introduce programmatic representations or explicit hypothesis generation; e.g., ARC direct 17% vs Program Only 23% and Full pipeline 30%+.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Direct in-context prediction performs poorly on complex inductive tasks (especially ARC) relative to approaches that use programmatic hypothesis representations; on simpler/sequential tasks (1D-ARC) baseline is stronger but still behind programmatic approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On SyGuS and some easier domains direct program generation or program-aided methods outperform direct prompting; no cases where direct prompting beats programmatic/full-pipeline in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Search: Inductive Reasoning with Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3284.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3284.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Program-Only</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Programmatic Hypothesis - Program-Only Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate executable Python programs directly from training examples (no intermediate natural-language hypotheses), then execute programs on training examples and pick best for test inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hypothesis Search: Inductive Reasoning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (primary); GPT-3.5 (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLM family used to synthesize Python programs implementing candidate transformations; multiple program samples per task and execution feedback for repair.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['program synthesis / program-as-representation', 'execution-and-repair (self-debug feedback)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LLM is prompted to output Python code implementing the mapping; many programs are sampled, executed on training pairs, and programs are refined via execution feedback (error messages or failing examples used to prompt repairs).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar style (programmatic reasoning only, no abstract hypothesis layer)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ARC, 1D-ARC, SyGuS, List Functions</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same inductive reasoning benchmarks; program-only treats transformation as executable Python function synthesized by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>ARC: 23% (GPT-4, 100-task subset); 1D-ARC: 61.1%; SyGuS: 94.3% (8 programs with 2 rounds of feedback); List Functions: 59%. GPT-3.5 variants had lower performance (see Appendix A results).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Program-Only improves substantially over Direct Prompting across datasets (e.g., ARC +6 percentage points; 1D-ARC +~21.5 points) and matches or nearly matches full pipeline on some program-synthesis tasks (SyGuS).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Representing candidate rules as executable programs boosts performance significantly; for string/program-synthesis tasks (SyGuS) program-only achieves near-state-of-the-art performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On ARC visual tasks, Program-Only is still weaker than the Full pipeline that includes explicit abstract hypotheses and human-written hypotheses (Program-Only 23% vs Human-Written Hypo 45%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Search: Inductive Reasoning with Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3284.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3284.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Full Pipeline (Hypothesis -> Program)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-level Hypothesis + Program Implementation Pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generate multiple natural-language hypotheses (abstract), filter/summarize/select a subset, implement hypotheses as Python programs, validate on training examples, and use passing programs for test inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hypothesis Search: Inductive Reasoning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (hypothesis generation and program generation); GPT-3.5 evaluated in ablations</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 used to generate natural-language hypotheses (sampling many candidates), optionally use GPT-4 to summarize candidates or humans to select, then GPT-4 to synthesize Python implementations with execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['abstract natural-language hypothesis generation', 'programmatic implementation and execution', 'hypothesis filtering (LLM summarization or human selection)', 'execution feedback (program repair)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Pipeline separates abstraction (NL hypotheses) from precise implementations (Python). LLM samples many hypotheses; LLM or humans summarize/select promising ones; for each hypothesis LLM synthesizes programs and uses execution feedback to repair; successful programs are used to infer test outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse / hierarchical (different styles: abstract NL hypotheses and concrete programmatic reasoning combined)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ARC, 1D-ARC, SyGuS, List Functions</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Inductive reasoning tasks spanning visual grid transformations, sequential transformations, string-program synthesis, and list-function transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>ARC (100 tasks): Summarized Hypotheses: 30%; Human-Selected Hypotheses: 33%; Human-Written Hypotheses (oracle): 45%; Program-Only: 23%; Direct: 17%. 1D-ARC: Full pipeline 73.1% vs Direct 39.6% and Program-Only 61.1%. SyGuS: Full 94.3% (same as Program-Only). List Functions: Full 69% vs Program-Only 59% vs Direct 31%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Full pipeline (diverse two-level reasoning) outperforms single-style approaches in most domains (notably ARC and 1D-ARC and List Functions). The human-written-hypothesis oracle upper bound is higher, indicating hypothesis generation is a bottleneck. Summarization loses some correct hypotheses vs testing all candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining abstract (natural-language) and concrete (program) representations produces substantial gains; hierarchical/diverse reasoning methods outperform similar single-style methods (direct ICL or program-only) on complex inductive tasks. Human selection of hypotheses further increases success rate, indicating value of filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>On SyGuS, adding NL hypotheses does not improve over Program-Only (both 94.3%), showing diverse methods don't always help when program synthesis is already easy; summarization can drop correct hypotheses and harm performance relative to exhaustive hypothesis testing; cost of sampling many hypotheses is a practical limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Search: Inductive Reasoning with Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3284.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3284.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought (CoT) Prompting Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation testing the effect of intermediate natural-language chain-of-thought style reasoning (without program execution) on ARC performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hypothesis Search: Inductive Reasoning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4 prompted to produce intermediate natural-language reasoning steps (Chain-of-Thought) but not programmatic implementations in the ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought prompting (step-by-step natural-language reasoning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LLM is prompted to produce an intermediate natural-language chain-of-thought or explanation before final answer, isolating the effect of intermediate language-only reasoning without program representations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar style (natural-language intermediate reasoning only)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ARC</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>2D-grid inductive reasoning tasks requiring precise transformations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Chain-of-Thought ablation on ARC: performance dropped to 19% (compared with Direct 17% and Program-Only 23%), and remained around 19% regardless of whether intermediate hypothesis or human-written hypotheses were used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT (NL-step) did not provide benefits compared to programmatic approaches; pipeline that includes programs provides larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Intermediate natural-language chain-of-thought without programmatic grounding offers little or no advantage on ARC; precise programmatic representations are more effective than CoT for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT produced negligible improvement and even degraded performance compared to programmatic methods, showing a case where a similar style (language-only reasoning) is inferior to programmatic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Search: Inductive Reasoning with Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3284.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3284.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Execution Feedback (Self-debug)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Execution Feedback / Program Repair Loop</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Iterative loop where the LLM receives execution errors or failed example outputs and is prompted to revise candidate programs; used to increase the chance of finding programs consistent with training examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hypothesis Search: Inductive Reasoning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (program revision); GPT-3.5 ablations</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM-driven program repair where failing programs are executed, exceptions or mismatches are inserted into the prompt, and the LLM outputs a corrected program; repeated for N feedback iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['execution feedback / self-debug / iterative repair']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Programs are executed on training inputs; failures (exceptions or wrong outputs) are appended to prompts and the LLM is asked to revise; multiple rounds often increase correct program recovery.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>compositional usage (programmatic reasoning combined with execution-based iterative correction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ARC, 1D-ARC, SyGuS, List Functions</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above; feedback improves program quality by leveraging execution outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>ARC summarized-hypotheses with 0->3 feedback iterations: 24% -> 30% (summarized); Human-Selected 26% -> 33%; Human-Written 38% -> 45% (Table 2 shows gains plateauing after a few iterations). Execution feedback consistently improves accuracy, with diminishing returns.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Execution feedback improves all program-based variants compared to zero-feedback; gains plateau after ~2-3 rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Execution feedback is an important practical mechanism to convert LLM-generated programs into correct implementations; it contributes substantially to final performance but has diminishing returns.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Performance gains plateau and additional rounds bring minimal improvements; feedback is costly since executing and reparsing many programs increases compute.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Search: Inductive Reasoning with Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3284.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3284.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parsel Compositional Method</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parsel: Compositional Program Generation with Decompositions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A compositional program generation method that decomposes a solution into functions (Parsel pseudo-code) and implements functions separately, then recombines implementations to produce programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hypothesis Search: Inductive Reasoning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used to implement Parsel and Python functions)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Parsel (Zelikman et al., 2023) is applied in the pipeline: hypotheses are turned into Parsel pseudo-code (specifying subfunctions), LLM implements subfunctions in single API calls, then recombination forms many programs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['compositional decomposition (Parsel) + program synthesis']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Introduce an intermediate parsable decomposition (Parsel) describing functions and their behaviors; sample multiple implementations per function and search combinatorially over compositions to produce many candidate programs with fewer LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse/hierarchical (adds another abstraction layer between NL hypothesis and Python implementation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ARC (subset experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Used to boost program-generation efficiency and search coverage on ARC tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>On 40 randomly selected ARC questions with human-written hypotheses, Parsel + implementations achieved 47.5% (4 Parsel programs Ã— 8 python programs) vs 37.5% from direct generation; but for LLM-generated hypotheses Parsel hurt performance (13-task subset: direct 92% vs Parsel pipeline 69%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Parsel helps when hypotheses are high-quality (human-written) by increasing search coverage efficiently, but adding the extra abstraction layer can hurt when upstream hypotheses are noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Compositional program generation can increase coverage and improve performance with high-quality hypotheses, but it can also introduce error propagation and worsen results when upstream hypothesis quality is low.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Parsel reversed gains for LLM-generated hypotheses (worse performance), illustrating that adding similar/higher-level abstractions can be detrimental if they increase transformation noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Search: Inductive Reasoning with Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3284.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3284.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0301 / gpt-3.5-0613) Ablation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Lower-cost OpenAI chat models evaluated as ablations to assess cost/performance trade-offs of the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hypothesis Search: Inductive Reasoning with Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo-0301 and gpt-3.5-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller, cheaper ChatGPT-family models with shorter context windows (earlier variants 4096 tokens, some with 16384), used for hypothesis and program generation ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['same pipeline approaches as GPT-4 but with weaker hypothesis/program generation']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Applied Program-Only and Full pipeline variants with GPT-3.5; in ARC GPT-3.5 often generated poor hypotheses, but given human-written hypotheses it could produce programs with lower success.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar styles as GPT-4 but lower capability (both NL-hypothesis and programmatic reasoning attempted)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>ARC, 1D-ARC, SyGuS, List Functions (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same inductive reasoning tasks evaluated to measure the effect of cheaper models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>ARC: GPT-3.5 with human hypotheses achieved 27% (128 programs) and 30% with larger context variant; 1D-ARC / SyGuS / List Functions: Program-Only and Full results reported in Appendix (e.g., SyGuS ~80.9% program-only, Full 86.5% in Table A.1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>GPT-3.5 is substantially cheaper but weaker: often fails to generate meaningful hypotheses for ARC; given human-written hypotheses it is capable but still underperforms GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>There is a trade-off between cost and performance: GPT-3.5 can be effective on simpler domains or when provided high-quality hypotheses but is less reliable on complex visual tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>GPT-3.5 generated mostly meaningless hypotheses for ARC and could not fit large prompts due to shorter context; performance remained below GPT-4 in the same configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hypothesis Search: Inductive Reasoning with Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Pal: Program-aided language models <em>(Rating: 2)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 2)</em></li>
                <li>Crossbeam: Learning to search in bottom-up program synthesis <em>(Rating: 2)</em></li>
                <li>Parsel: Algorithmic reasoning with language models by composing decompositions <em>(Rating: 2)</em></li>
                <li>Bustle: Bottom-up program synthesis through learning-guided exploration <em>(Rating: 1)</em></li>
                <li>Abstraction and reasoning challenge 1st place solution <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3284",
    "paper_id": "paper-4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "Direct Prompting (ICL)",
            "name_full": "Direct In-Context Learning Prompting",
            "brief_description": "Baseline method that prompts an LLM with training input-output examples and asks it to directly predict outputs for novel inputs (standard in-context learning).",
            "citation_title": "Hypothesis Search: Inductive Reasoning with Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0613 / gpt-4-0314 reported)",
            "model_description": "OpenAI GPT-4 family (chat-completion API variants gpt-4-0613 and earlier gpt-4-0314) used for generation and code synthesis; large pre-trained transformer with multi-modal prompt handling in experiments via textual grid encodings.",
            "model_size": null,
            "reasoning_methods": [
                "in-context learning (direct prediction)"
            ],
            "reasoning_methods_description": "Models are given training input-output pairs in the prompt and asked to directly generate the target outputs for test examples (no intermediate hypothesis or program representation).",
            "diversity_of_methods": "single/similar style (direct single-step ICL)",
            "reasoning_task_name": "ARC, 1D-ARC, SyGuS, List Functions (as baselines)",
            "reasoning_task_description": "Inductive reasoning benchmarks: ARC (2D grid visual tasks), 1D-ARC (one-dimensional adaptation), SyGuS (string transformation/program synthesis), List Functions (list-to-list transformations).",
            "performance_by_method": "ARC: 17% (GPT-4, top-1 on 100 sampled tasks); 1D-ARC: 39.6% (from Xu et al. baseline reported in paper); List Functions: 31%; SyGuS: not applicable (SyGuS evaluated as program induction).",
            "comparison_of_methods": "Direct prompting is the baseline and is outperformed by methods that introduce programmatic representations or explicit hypothesis generation; e.g., ARC direct 17% vs Program Only 23% and Full pipeline 30%+.",
            "key_findings": "Direct in-context prediction performs poorly on complex inductive tasks (especially ARC) relative to approaches that use programmatic hypothesis representations; on simpler/sequential tasks (1D-ARC) baseline is stronger but still behind programmatic approaches.",
            "counter_examples_or_negative_results": "On SyGuS and some easier domains direct program generation or program-aided methods outperform direct prompting; no cases where direct prompting beats programmatic/full-pipeline in reported experiments.",
            "uuid": "e3284.0",
            "source_info": {
                "paper_title": "Hypothesis Search: Inductive Reasoning with Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Program-Only",
            "name_full": "Programmatic Hypothesis - Program-Only Generation",
            "brief_description": "Generate executable Python programs directly from training examples (no intermediate natural-language hypotheses), then execute programs on training examples and pick best for test inference.",
            "citation_title": "Hypothesis Search: Inductive Reasoning with Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4 (primary); GPT-3.5 (ablation)",
            "model_description": "Same LLM family used to synthesize Python programs implementing candidate transformations; multiple program samples per task and execution feedback for repair.",
            "model_size": null,
            "reasoning_methods": [
                "program synthesis / program-as-representation",
                "execution-and-repair (self-debug feedback)"
            ],
            "reasoning_methods_description": "LLM is prompted to output Python code implementing the mapping; many programs are sampled, executed on training pairs, and programs are refined via execution feedback (error messages or failing examples used to prompt repairs).",
            "diversity_of_methods": "similar style (programmatic reasoning only, no abstract hypothesis layer)",
            "reasoning_task_name": "ARC, 1D-ARC, SyGuS, List Functions",
            "reasoning_task_description": "Same inductive reasoning benchmarks; program-only treats transformation as executable Python function synthesized by the LLM.",
            "performance_by_method": "ARC: 23% (GPT-4, 100-task subset); 1D-ARC: 61.1%; SyGuS: 94.3% (8 programs with 2 rounds of feedback); List Functions: 59%. GPT-3.5 variants had lower performance (see Appendix A results).",
            "comparison_of_methods": "Program-Only improves substantially over Direct Prompting across datasets (e.g., ARC +6 percentage points; 1D-ARC +~21.5 points) and matches or nearly matches full pipeline on some program-synthesis tasks (SyGuS).",
            "key_findings": "Representing candidate rules as executable programs boosts performance significantly; for string/program-synthesis tasks (SyGuS) program-only achieves near-state-of-the-art performance.",
            "counter_examples_or_negative_results": "On ARC visual tasks, Program-Only is still weaker than the Full pipeline that includes explicit abstract hypotheses and human-written hypotheses (Program-Only 23% vs Human-Written Hypo 45%).",
            "uuid": "e3284.1",
            "source_info": {
                "paper_title": "Hypothesis Search: Inductive Reasoning with Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Full Pipeline (Hypothesis -&gt; Program)",
            "name_full": "Two-level Hypothesis + Program Implementation Pipeline",
            "brief_description": "Generate multiple natural-language hypotheses (abstract), filter/summarize/select a subset, implement hypotheses as Python programs, validate on training examples, and use passing programs for test inference.",
            "citation_title": "Hypothesis Search: Inductive Reasoning with Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4 (hypothesis generation and program generation); GPT-3.5 evaluated in ablations",
            "model_description": "GPT-4 used to generate natural-language hypotheses (sampling many candidates), optionally use GPT-4 to summarize candidates or humans to select, then GPT-4 to synthesize Python implementations with execution feedback.",
            "model_size": null,
            "reasoning_methods": [
                "abstract natural-language hypothesis generation",
                "programmatic implementation and execution",
                "hypothesis filtering (LLM summarization or human selection)",
                "execution feedback (program repair)"
            ],
            "reasoning_methods_description": "Pipeline separates abstraction (NL hypotheses) from precise implementations (Python). LLM samples many hypotheses; LLM or humans summarize/select promising ones; for each hypothesis LLM synthesizes programs and uses execution feedback to repair; successful programs are used to infer test outputs.",
            "diversity_of_methods": "diverse / hierarchical (different styles: abstract NL hypotheses and concrete programmatic reasoning combined)",
            "reasoning_task_name": "ARC, 1D-ARC, SyGuS, List Functions",
            "reasoning_task_description": "Inductive reasoning tasks spanning visual grid transformations, sequential transformations, string-program synthesis, and list-function transformations.",
            "performance_by_method": "ARC (100 tasks): Summarized Hypotheses: 30%; Human-Selected Hypotheses: 33%; Human-Written Hypotheses (oracle): 45%; Program-Only: 23%; Direct: 17%. 1D-ARC: Full pipeline 73.1% vs Direct 39.6% and Program-Only 61.1%. SyGuS: Full 94.3% (same as Program-Only). List Functions: Full 69% vs Program-Only 59% vs Direct 31%.",
            "comparison_of_methods": "Full pipeline (diverse two-level reasoning) outperforms single-style approaches in most domains (notably ARC and 1D-ARC and List Functions). The human-written-hypothesis oracle upper bound is higher, indicating hypothesis generation is a bottleneck. Summarization loses some correct hypotheses vs testing all candidates.",
            "key_findings": "Combining abstract (natural-language) and concrete (program) representations produces substantial gains; hierarchical/diverse reasoning methods outperform similar single-style methods (direct ICL or program-only) on complex inductive tasks. Human selection of hypotheses further increases success rate, indicating value of filtering.",
            "counter_examples_or_negative_results": "On SyGuS, adding NL hypotheses does not improve over Program-Only (both 94.3%), showing diverse methods don't always help when program synthesis is already easy; summarization can drop correct hypotheses and harm performance relative to exhaustive hypothesis testing; cost of sampling many hypotheses is a practical limitation.",
            "uuid": "e3284.2",
            "source_info": {
                "paper_title": "Hypothesis Search: Inductive Reasoning with Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Chain-of-Thought Ablation",
            "name_full": "Chain-of-Thought (CoT) Prompting Ablation",
            "brief_description": "An ablation testing the effect of intermediate natural-language chain-of-thought style reasoning (without program execution) on ARC performance.",
            "citation_title": "Hypothesis Search: Inductive Reasoning with Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "GPT-4 prompted to produce intermediate natural-language reasoning steps (Chain-of-Thought) but not programmatic implementations in the ablation.",
            "model_size": null,
            "reasoning_methods": [
                "chain-of-thought prompting (step-by-step natural-language reasoning)"
            ],
            "reasoning_methods_description": "LLM is prompted to produce an intermediate natural-language chain-of-thought or explanation before final answer, isolating the effect of intermediate language-only reasoning without program representations.",
            "diversity_of_methods": "similar style (natural-language intermediate reasoning only)",
            "reasoning_task_name": "ARC",
            "reasoning_task_description": "2D-grid inductive reasoning tasks requiring precise transformations.",
            "performance_by_method": "Chain-of-Thought ablation on ARC: performance dropped to 19% (compared with Direct 17% and Program-Only 23%), and remained around 19% regardless of whether intermediate hypothesis or human-written hypotheses were used.",
            "comparison_of_methods": "CoT (NL-step) did not provide benefits compared to programmatic approaches; pipeline that includes programs provides larger gains.",
            "key_findings": "Intermediate natural-language chain-of-thought without programmatic grounding offers little or no advantage on ARC; precise programmatic representations are more effective than CoT for these tasks.",
            "counter_examples_or_negative_results": "CoT produced negligible improvement and even degraded performance compared to programmatic methods, showing a case where a similar style (language-only reasoning) is inferior to programmatic reasoning.",
            "uuid": "e3284.3",
            "source_info": {
                "paper_title": "Hypothesis Search: Inductive Reasoning with Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Execution Feedback (Self-debug)",
            "name_full": "Execution Feedback / Program Repair Loop",
            "brief_description": "Iterative loop where the LLM receives execution errors or failed example outputs and is prompted to revise candidate programs; used to increase the chance of finding programs consistent with training examples.",
            "citation_title": "Hypothesis Search: Inductive Reasoning with Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4 (program revision); GPT-3.5 ablations",
            "model_description": "LLM-driven program repair where failing programs are executed, exceptions or mismatches are inserted into the prompt, and the LLM outputs a corrected program; repeated for N feedback iterations.",
            "model_size": null,
            "reasoning_methods": [
                "execution feedback / self-debug / iterative repair"
            ],
            "reasoning_methods_description": "Programs are executed on training inputs; failures (exceptions or wrong outputs) are appended to prompts and the LLM is asked to revise; multiple rounds often increase correct program recovery.",
            "diversity_of_methods": "compositional usage (programmatic reasoning combined with execution-based iterative correction)",
            "reasoning_task_name": "ARC, 1D-ARC, SyGuS, List Functions",
            "reasoning_task_description": "As above; feedback improves program quality by leveraging execution outcomes.",
            "performance_by_method": "ARC summarized-hypotheses with 0-&gt;3 feedback iterations: 24% -&gt; 30% (summarized); Human-Selected 26% -&gt; 33%; Human-Written 38% -&gt; 45% (Table 2 shows gains plateauing after a few iterations). Execution feedback consistently improves accuracy, with diminishing returns.",
            "comparison_of_methods": "Execution feedback improves all program-based variants compared to zero-feedback; gains plateau after ~2-3 rounds.",
            "key_findings": "Execution feedback is an important practical mechanism to convert LLM-generated programs into correct implementations; it contributes substantially to final performance but has diminishing returns.",
            "counter_examples_or_negative_results": "Performance gains plateau and additional rounds bring minimal improvements; feedback is costly since executing and reparsing many programs increases compute.",
            "uuid": "e3284.4",
            "source_info": {
                "paper_title": "Hypothesis Search: Inductive Reasoning with Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Parsel Compositional Method",
            "name_full": "Parsel: Compositional Program Generation with Decompositions",
            "brief_description": "A compositional program generation method that decomposes a solution into functions (Parsel pseudo-code) and implements functions separately, then recombines implementations to produce programs.",
            "citation_title": "Hypothesis Search: Inductive Reasoning with Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4 (used to implement Parsel and Python functions)",
            "model_description": "Parsel (Zelikman et al., 2023) is applied in the pipeline: hypotheses are turned into Parsel pseudo-code (specifying subfunctions), LLM implements subfunctions in single API calls, then recombination forms many programs.",
            "model_size": null,
            "reasoning_methods": [
                "compositional decomposition (Parsel) + program synthesis"
            ],
            "reasoning_methods_description": "Introduce an intermediate parsable decomposition (Parsel) describing functions and their behaviors; sample multiple implementations per function and search combinatorially over compositions to produce many candidate programs with fewer LLM calls.",
            "diversity_of_methods": "diverse/hierarchical (adds another abstraction layer between NL hypothesis and Python implementation)",
            "reasoning_task_name": "ARC (subset experiments)",
            "reasoning_task_description": "Used to boost program-generation efficiency and search coverage on ARC tasks.",
            "performance_by_method": "On 40 randomly selected ARC questions with human-written hypotheses, Parsel + implementations achieved 47.5% (4 Parsel programs Ã— 8 python programs) vs 37.5% from direct generation; but for LLM-generated hypotheses Parsel hurt performance (13-task subset: direct 92% vs Parsel pipeline 69%).",
            "comparison_of_methods": "Parsel helps when hypotheses are high-quality (human-written) by increasing search coverage efficiently, but adding the extra abstraction layer can hurt when upstream hypotheses are noisy.",
            "key_findings": "Compositional program generation can increase coverage and improve performance with high-quality hypotheses, but it can also introduce error propagation and worsen results when upstream hypothesis quality is low.",
            "counter_examples_or_negative_results": "Parsel reversed gains for LLM-generated hypotheses (worse performance), illustrating that adding similar/higher-level abstractions can be detrimental if they increase transformation noise.",
            "uuid": "e3284.5",
            "source_info": {
                "paper_title": "Hypothesis Search: Inductive Reasoning with Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-3.5 Ablation",
            "name_full": "GPT-3.5 (gpt-3.5-turbo-0301 / gpt-3.5-0613) Ablation",
            "brief_description": "Lower-cost OpenAI chat models evaluated as ablations to assess cost/performance trade-offs of the pipeline.",
            "citation_title": "Hypothesis Search: Inductive Reasoning with Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo-0301 and gpt-3.5-0613)",
            "model_description": "Smaller, cheaper ChatGPT-family models with shorter context windows (earlier variants 4096 tokens, some with 16384), used for hypothesis and program generation ablations.",
            "model_size": null,
            "reasoning_methods": [
                "same pipeline approaches as GPT-4 but with weaker hypothesis/program generation"
            ],
            "reasoning_methods_description": "Applied Program-Only and Full pipeline variants with GPT-3.5; in ARC GPT-3.5 often generated poor hypotheses, but given human-written hypotheses it could produce programs with lower success.",
            "diversity_of_methods": "similar styles as GPT-4 but lower capability (both NL-hypothesis and programmatic reasoning attempted)",
            "reasoning_task_name": "ARC, 1D-ARC, SyGuS, List Functions (ablation)",
            "reasoning_task_description": "Same inductive reasoning tasks evaluated to measure the effect of cheaper models.",
            "performance_by_method": "ARC: GPT-3.5 with human hypotheses achieved 27% (128 programs) and 30% with larger context variant; 1D-ARC / SyGuS / List Functions: Program-Only and Full results reported in Appendix (e.g., SyGuS ~80.9% program-only, Full 86.5% in Table A.1).",
            "comparison_of_methods": "GPT-3.5 is substantially cheaper but weaker: often fails to generate meaningful hypotheses for ARC; given human-written hypotheses it is capable but still underperforms GPT-4.",
            "key_findings": "There is a trade-off between cost and performance: GPT-3.5 can be effective on simpler domains or when provided high-quality hypotheses but is less reliable on complex visual tasks.",
            "counter_examples_or_negative_results": "GPT-3.5 generated mostly meaningless hypotheses for ARC and could not fit large prompts due to shorter context; performance remained below GPT-4 in the same configurations.",
            "uuid": "e3284.6",
            "source_info": {
                "paper_title": "Hypothesis Search: Inductive Reasoning with Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Pal: Program-aided language models",
            "rating": 2
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 2
        },
        {
            "paper_title": "Crossbeam: Learning to search in bottom-up program synthesis",
            "rating": 2
        },
        {
            "paper_title": "Parsel: Algorithmic reasoning with language models by composing decompositions",
            "rating": 2
        },
        {
            "paper_title": "Bustle: Bottom-up program synthesis through learning-guided exploration",
            "rating": 1
        },
        {
            "paper_title": "Abstraction and reasoning challenge 1st place solution",
            "rating": 1
        }
    ],
    "cost": 0.015867,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Hypothesis Search: Inductive Reasoning with Language Models</h1>
<p>Ruocheng Wang ${ }^{1 <em>}$, Eric Zelikman ${ }^{1 </em>}$, Gabriel Poesia ${ }^{1}$, Yewen $\mathbf{P u}^{2}$, Nick Haber ${ }^{1}$, Noah D. Goodman ${ }^{1}$<br>${ }^{1}$ Stanford University, ${ }^{2}$ Autodesk Research</p>
<h4>Abstract</h4>
<p>Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which robustly generalize to novel scenarios. Recent work evaluates large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding "in context learning." This works well for straightforward inductive tasks but performs poorly on complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be verified by running on observed examples and generalized to novel inputs. To reduce the hypothesis search space, we explore steps to filter the set of hypotheses to implement: we either ask the LLM to summarize them into a smaller set of hypotheses or ask human annotators to select a subset. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, string transformation dataset SyGuS, and list transformation dataset List Functions. On a random 100-problem subset of ARC, our automated pipeline using LLM summaries achieves $30 \%$ accuracy, outperforming the direct prompting baseline (accuracy of $17 \%$ ). With the minimal human input of selecting from LLM-generated candidates, performance is boosted to $33 \%$. Our ablations show that both abstract hypothesis generation and concrete program representations benefit LLMs on inductive reasoning tasks.</p>
<h2>1 INTRODUCTION</h2>
<p>Inductive reasoning - the ability to infer general principles from specific examples and apply them to novel situations - is a core aspect of human intelligence (Peirce, 1868). Recently, large-scale pre-trained language models have received significant interest for their performance across a diverse range of reasoning tasks such as commonsense, arithmetic and symbolic reasoning (Rajani et al., 2019; Shwartz et al., 2020; Nye et al., 2021; Wei et al., 2022; MarasoviÄ‡ et al., 2021; Lampinen et al., 2022; Zelikman et al., 2022; Zhou et al., 2022). There has been extensive discussion of language models' impressive "in-context learning" capabilities, a form of inductive reasoning. However, other work suggests that in-context learning of these models has a highly limited capacity to perform inductive reasoning tasks where precise behavior is required (Chollet, 2019; Johnson et al., 2021).
The Abstraction and Reasoning Corpus (ARC) is a particularly challenging inductive reasoning benchmark (Chollet, 2019). For each task in ARC, models are given a set of training input-output pairs with a shared transformation rule, and the goal is to predict the corresponding output(s) given the novel test input(s), as illustrated in Fig 2 (a). ARC is interesting because the answers are fairly natural for humans yet require a complex and precise transformation. Evaluations of LLMs on ARC (Xu et al., 2023b; Mirchandani et al., 2023; Gendron et al., 2023) have directly prompted LLMs to predict outputs by in-context learning, finding poor performance relative to humans (Chollet, 2019; Johnson et al., 2021). We instead take inspiration from Bayesian models of human inductive reasoning (Tenenbaum et al., 2006; Goodman et al., 2008). That research frames inductive reasoning as posterior prediction: an ideal Bayesian learner assumes a large hypothesis space of possible rules, uses Bayes' rule to form a posterior distribution over hypotheses from examples, then responds accordingly with a posterior-predictive distribution. Studies of human inductive learning have found that people likely</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of our pipeline. From left to right, starting from a task in the dataset, a language model 1) generates a set of candidate hypotheses, 2) selects a subset, 3) implements each hypothesis in code as a function, and 4) validates the implementations against the training examples.</p>
<p>approximate the full posterior with just a few hypotheses (Vul et al., 2014). Furthermore, people often represent hypotheses of the world at multiple levels of abstraction (Tenenbaum et al., 2011), with more abstract hypotheses guiding the search for more specific ones (Goodman et al., 2011).</p>
<p>We thus propose an approach that improves the inductive reasoning ability of LMs by decomposing the task via hypothesis formation at two levels of abstraction: first by generating hypotheses in natural language and then by realizing these as specific programs that are used for making predictions. Natural language provides abstract representations that uncover key features but are difficult to verify and potentially ambiguous. Programmatic hypotheses are directly verifiable on examples via execution and can naively generalize to new inputs but involve many implementation details that can be distracting to a language model. In other words, we use particular programmatic implementations to act as a precise, generalizable representation of a given inductive hypothesis formulated in natural language. Our pipeline thus disentangles inductive reasoning tasks primarily into two capabilities: the ability to propose accurate natural language hypotheses and the ability to formalize them as programs.</p>
<p>However, in practice LLMs are not yet able to find a good hypothesis with one try. Sampling multiple hypotheses and multiple programs per hypothesis turns out to be sufficient, but can be extremely costly. Thus, we also investigate approaches to reduce the number of hypotheses that must be considered. First, we use an LLM to summarize multiple hypotheses into a smaller number of hypotheses. Second, we experiment with querying a human oracle to go through all hypotheses and indicate which can be ignored. The latter can be viewed as a lower bound on performance that would be achieved by our approach without filtering, because we also find that programs which are correct on all examples almost always generalize correctly, an interesting feature of complex inductive reasoning domains.</p>
<p>We conduct experiments on four inductive reasoning datasets: the Abstraction and Reasoning Corpus (ARC), the one-dimensional variant of ARC (1D-ARC), the Syntax-Guided Synthesis (SyGuS) dataset, and the List Functions dataset. Our results indicate that explicit hypothesis formation substantially improves performance over the direct prompting (ICL) approach. Ablation studies suggest both levels of abstraction â€“ natural-language hypothesis generation and programmatic hypothesis representations â€“ are beneficial to performing inductive reasoning tasks.</p>
<p>Contributions. We summarize the contributions of our paper as follows:</p>
<ul>
<li>We propose a pipeline that uses language models to solve inductive reasoning tasks by generating and testing hypotheses in natural languages and code.</li>
<li>We conduct experiments to demonstrate our pipeline achieves significant improvement over baselines on four inductive reasoning tasks across different domains.</li>
<li>We explore and analyze techniques for reducing the hypothesis search space.</li>
</ul>
<h2>2 Method</h2>
<h3>2.1 Problem Statement</h3>
<p>We consider inductive reasoning tasks that require discovering an underlying transformation rule given input-output examples that follow this unknown rule. More formally, we are given a set of training examples $(x_{1},y_{1}),(x_{2},y_{2}),\ldots,(x_{n},y_{n})$ where each $y_{i}=f(x_{i})$ for some unknown function $f$. Our goal is for the model to infer the outputs $y_{1}^{\prime},y_{2}^{\prime},\ldots, y_{n}^{\prime}$ for a list of novel inputs $x_{1}^{\prime},x_{2}^{\prime},\ldots, x_{n}^{\prime}$</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="p">:</span><span class="w"> </span><span class="nx">Implementing</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">Python</span><span class="w"> </span><span class="nx">Program</span><span class="w"> </span><span class="nx">from</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">Natural</span><span class="w"> </span><span class="nx">Language</span><span class="w"> </span><span class="nx">Hypothesis</span>
<span class="nx">Input</span><span class="p">:</span><span class="w"> </span><span class="nx">Training</span><span class="w"> </span><span class="nx">examples</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">n</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="err">ï¼Œ</span><span class="w"> </span><span class="nx">natural</span><span class="w"> </span><span class="nx">language</span><span class="w"> </span><span class="nx">hypothesis</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">L</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">maximum</span>
<span class="w">        </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">feedback</span><span class="w"> </span><span class="nx">iterations</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">feedback</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">initial</span><span class="w"> </span><span class="nx">LLM</span><span class="w"> </span><span class="nx">prompt</span><span class="w"> </span><span class="nx">template</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">m</span><span class="err">\</span><span class="p">),</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="nx">of</span>
<span class="w">        </span><span class="nx">programs</span><span class="w"> </span><span class="nx">per</span><span class="w"> </span><span class="nx">hypothesis</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">K</span><span class="err">\</span><span class="p">)</span>
<span class="nx">Output</span><span class="p">:</span><span class="w"> </span><span class="nx">A</span><span class="w"> </span><span class="nx">Python</span><span class="w"> </span><span class="nx">program</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">expected</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">consistent</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">training</span><span class="w"> </span><span class="nx">examples</span><span class="w"> </span><span class="k">and</span>
<span class="w">            </span><span class="nx">hypothesis</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">L</span><span class="err">\</span><span class="p">)</span>
<span class="err">\</span><span class="p">(</span><span class="nx">P</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">LLM</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">m</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">format</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">L</span><span class="p">,</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">n</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="nx">n</span><span class="p">=</span><span class="nx">K</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Generate</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">K</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">programs</span>
<span class="nx">foreach</span><span class="w"> </span><span class="nx">program</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">P</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="k">forall</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">n</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}:</span><span class="w"> </span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="nx">y_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">program</span><span class="w"> </span><span class="nx">succeeds</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">examples</span><span class="p">,</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="nx">it</span><span class="p">.</span>
<span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">i</span><span class="p">=</span><span class="mi">1</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">N_</span><span class="p">{</span><span class="err">\</span><span class="nx">text</span><span class="w"> </span><span class="p">{</span><span class="nx">feedback</span><span class="w"> </span><span class="p">}}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">    </span><span class="nx">foreach</span><span class="w"> </span><span class="nx">program</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">P</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">n</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">do</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="nx">e</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">CatchException</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">e</span><span class="w"> </span><span class="err">\</span><span class="nx">neq</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">null</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="nx">m</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">append</span><span class="p">}(</span><span class="nx">p</span><span class="p">,</span><span class="w"> </span><span class="nx">e</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Add</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">caught</span><span class="w"> </span><span class="nx">exception</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">prompt</span>
<span class="w">                </span><span class="k">break</span>
<span class="w">            </span><span class="k">else</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">neq</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="err">\</span><span class="p">(</span><span class="nx">m</span><span class="w"> </span><span class="p">.</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">append</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">p</span><span class="p">,</span><span class="w"> </span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Add</span><span class="w"> </span><span class="nx">failed</span><span class="w"> </span><span class="nx">example</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">prompt</span>
<span class="w">                </span><span class="k">break</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="nx">p</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="err">\</span><span class="nx">operatorname</span><span class="p">{</span><span class="nx">LLM</span><span class="p">}(</span><span class="nx">m</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="nx">quad</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="o">/</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">Generate</span><span class="w"> </span><span class="nx">one</span><span class="w"> </span><span class="nx">revised</span><span class="w"> </span><span class="nx">program</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="k">forall</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">),</span><span class="w"> </span><span class="err">\</span><span class="nx">ldots</span><span class="p">,</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">n</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="nx">n</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="err">\</span><span class="p">}:</span><span class="w"> </span><span class="nx">p</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="nx">y_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">then</span>
<span class="w">                </span><span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="nx">p</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="w">            </span><span class="err">\</span><span class="p">(</span><span class="nx">p</span><span class="w"> </span><span class="err">\</span><span class="nx">leftarrow</span><span class="w"> </span><span class="nx">p</span><span class="o">^</span><span class="p">{</span><span class="err">\</span><span class="nx">prime</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
<span class="k">return</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">arg</span><span class="w"> </span><span class="err">\</span><span class="nx">max</span><span class="w"> </span><span class="nx">_</span><span class="p">{</span><span class="nx">p</span><span class="w"> </span><span class="err">\</span><span class="k">in</span><span class="w"> </span><span class="nx">P</span><span class="p">}</span><span class="err">\</span><span class="nx">left</span><span class="o">|</span><span class="err">\</span><span class="nx">left</span><span class="err">\</span><span class="p">{</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">},</span><span class="w"> </span><span class="nx">y_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">):</span><span class="w"> </span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="nx">y_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">.</span><span class="err">\</span><span class="nx">right</span><span class="p">.</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">CatchException</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">p</span><span class="err">\</span><span class="nx">left</span><span class="p">(</span><span class="nx">x_</span><span class="p">{</span><span class="nx">i</span><span class="p">}</span><span class="err">\</span><span class="nx">right</span><span class="p">)</span><span class="err">\</span><span class="nx">right</span><span class="p">)=</span><span class="err">\</span><span class="p">)</span><span class="w"> </span><span class="nx">null</span><span class="w"> </span><span class="err">\</span><span class="p">(</span><span class="err">\</span><span class="p">}</span><span class="err">\</span><span class="p">)</span>
</code></pre></div>

<p>that captures the transformation $f$. This formulation applies to all four datasets we consider in the experiment settings, as shown in Figure 2. This task is widely studied in program synthesis literature (Acquaviva et al., 2022; Odena et al., 2020; Ellis et al., 2023; Xu et al., 2023a), where a program written in a manually-designed Domain-Specific Language (DSL) is used to represent the transformation, which is applied to the test inputs to obtain the predicted outputs. Recently, there are also multiple works (Webb et al., 2022; Xu et al., 2023b; Mirchandani et al., 2023; Gendron et al., 2023) that do not predict the rule explicitly. Instead, large language models are used to predict the output for novel input examples directly given the training input-output pairs.</p>
<h1>2.2 OVERVIEW</h1>
<p>As illustrated in Figure 1, in our pipeline, we first prompt an LLM to generate hypotheses about the transformation rule shared across the input-output pairs in natural language. We then filter out a smaller set of hypotheses, using either an LLM or human annotator - the goal of this step is simply to reduce the computational cost of later steps. The filtered hypotheses are used to prompt an LLM to generate programs that take in an input example and output the transformed result. These programs are then tested against the initial training examples. Note that, in these domains, we observed that programs that successfully generated outputs for training pairs almost always generalized to test items.</p>
<h3>2.3 Generating Hypotheses</h3>
<p>The first step in our pipeline is to prompt a language model to generate natural language hypotheses for inductive reasoning problems. For each problem, we provide GPT-4 with a description of the task setup and the problem-specific input-output examples and prompt it to generate hypotheses about possible underlying rules or patterns that could explain the transformation in the given examples. We also provide two problems with human-annotated hypotheses as few-shot demonstrations in the prompt. More precisely, when doing an ARC task, we provide GPT-4 with the input-output examples in the form of a grid of numbers and specify the corresponding colors for each number as part of the prompt, with the exact prompt included in the Appendix A. We sample multiple responses from GPT-4, with a temperature of 1.0 , as the hypothesis candidates.</p>
<h3>2.4 Reducing Number of Candidate Hypotheses</h3>
<p>Ideally, we would like to directly test all of the generated hypotheses by implementing them as Python programs. However, given a potentially large number of hypotheses, testing all of them can be expensive. Thus, we investigate several methods to identify the most promising hypotheses from a set of proposals. For an end-to-end approach, we investigate using LLMs to summarize the full set</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Example problems in each of our four evaluation datasets.
of hypotheses into a smaller number of hypotheses. Specifically, we directly present GPT-4 with all candidate hypotheses and ask it to produce a smaller number of hypotheses summarizing the given candidate hypotheses. In addition, to help estimate a lower bound on performance if we were to test all hypotheses, we ask a human annotator to go through candidate hypotheses and select correct ones, if any.</p>
<h1>2.5 Implementing Python Programs From Hypotheses</h1>
<p>The pseudocode for this stage is presented in Algorithm 1. After obtaining a set of candidate hypotheses for each problem, we individually use each hypothesis as the input for GPT-4 and prompt it to generate multiple Python programs that implement the described transformation. Then, we run these programs against the problem's original input-output examples (while still holding out the test examples), determining whether they yield correct outputs for each case. If a code implementation correctly generates the outputs for each of the training examples, it is selected for generating the prediction on the test input example. If no implementation passes all of the training examples, we repeatedly ask GPT-4 to revise the implementations according to the execution results on the training set, including error messages and desired outputs, similar to Chen et al. (2023). This leverages research on code repair spanning multiple decades (Schulte et al., 2010; Pang, 2018; Vasic et al., 2019; Rahman et al., 2021, inter alia). If we cannot achieve a program that passes all the training examples after a preset number of feedback rounds, we select the program that passes the most examples for generating the prediction.</p>
<h2>3 EXPERIMENTS AND RESULTS</h2>
<h3>3.1 DATASETS</h3>
<p>We evaluate our approach on four distinct datasets: the Abstraction and Reasoning Corpus (ARC), the one-dimensional variant of ARC (1D-ARC), BUSTLE's Syntax-Guided Synthesis (SyGuS) dataset and List Functions dataset. These datasets offer diverse and challenging reasoning tasks in the domains of 2D grids, number sequences and strings, enabling us to thoroughly assess the inductive reasoning capabilities of our method. We provide examples of tasks in these datasets in Figure 2.
ARC. The Abstraction and Reasoning Corpus (ARC), proposed by Chollet (2019), is a dataset designed to assess models' generalizable reasoning capabilities. It is a dataset of 400 training and 400 evaluation problems. Each problem consists of a set of input-output 2D grids that capture a specific underlying rule or pattern such as geometric transformation and object counting. Each example is a grid with $1 \times 1$ to $30 \times 30$ pixels of any of ten colors - note that the input and output grid need not have the same shape. To effectively analyze this task despite the high cost of GPT-4, in our paper, we randomly select a subset of 100 problems from the 400 training problems as the evaluation dataset.
1D-ARC. 1D-ARC is a one-dimensional adaptation of the original ARC dataset proposed in (Xu et al., 2023b). Although simpler than the two-dimensional ARC problems, 1D-ARC offers a more controlled setting to investigate the inductive reasoning abilities of language models as they are trained to handle sequential data. We once again select a random subset for evaluation, this time randomly choosing 6 tasks from each of 1D-ARC's 18 categories for a total of 108 problems.
SyGuS. The SyGuS dataset in the BUSTLE paper contains 89 tasks that require representing a mapping between pairs of strings as a program (Odena et al., 2020). This task represents the kinds of problems solved by FlashFill (Gulwani, 2011), a feature in Excel that has been widely cited as an influential real-world example of program synthesis (Le et al., 2017).</p>
<p>List Functions. The List Functions dataset proposed in <em>Rule et al. (2020)</em> is a cognitive-science-inspired inductive reasoning benchmark that involves mapping a list of numbers to another list of numbers. The transformation covers basic list operations like duplication, and removal, as well as more complex combinations of recursive, conditional, and numerical reasoning (e.g., sorting, computing difference). The dataset has 250 tasks, each with 8 train and 8 test examples.</p>
<h1>3.2 ARC</h1>
<p>Settings. We measure the performance of different methods by computing the accuracy of models' prediction on the test input cases ${ }^{1}$. Although the input-output examples are typically visually presented in 2D pixel grids, we convert them to a text format in the style of NumPy arrays. We include the prompt templates in Appendix A.</p>
<h3>3.2.1 MAIN ReSULTS</h3>
<p>We compare the direct prompting baseline to different variants and ablations of our pipeline.
Direct Prompting. As done in previous work (Xu et al., 2023b; Mirchandani et al., 2023; Gendron et al., 2023), we provide training examples in a prompt and ask GPT- 4 to directly infer novel test inputs' output grids.
Program Only. In this ablation, we directly prompt GPT-4 to output Python programs for training examples. We generate 64 programs per task, selecting one passing the most training examples to generate test outputs.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Accuracy (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">17</td>
</tr>
<tr>
<td style="text-align: center;">Program Only</td>
<td style="text-align: center;">23</td>
</tr>
<tr>
<td style="text-align: center;">Summarized Hypo.</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">Human-Selected Hypo.</td>
<td style="text-align: center;">33</td>
</tr>
<tr>
<td style="text-align: center;">Human-Written Hypo.</td>
<td style="text-align: center;">45</td>
</tr>
</tbody>
</table>
<p>Table 1: Results of the baseline and variants of our method on the randomly selected 100 ARC tasks. Our method outperforms baselines with or without human supervision.</p>
<p>Summarized Hypotheses. For each problem, we first use GPT-4 to generate 64 candidate hypotheses and then ask GPT-4 to summarize 8 hypotheses from the 64 candidates. We then generate 8 programs for each hypothesis, resulting in 64 candidate programs per problem. This is followed by 3 rounds of execution feedback. Note that during our experiments, we found that GPT-4 only generates correct hypotheses for 49 tasks, according to human annotators. Of the 30 tasks solved with summarized hypotheses, 28 had a correct hypothesis before summarization.
Human-Selected Hypotheses. We first prompt GPT-4 to generate 64 hypotheses and then ask a human to manually select a correct one of them for each task, if any exist. Then we generate 8 programs for each hypothesis, followed by up to 3 rounds of execution feedback. As we observe a low falsepositive rate, this is approximately a lower-bound to evaluating all hypotheses. Here, we only generate programs for the 49 tasks with selected hypotheses. Of these, 33 ( $67 \%$ ) led to correct programs.
Human-Written Hypotheses. For this version, we leverage the human language annotations from the LARC dataset <em>(Acquaviva et al., 2022)</em> as golden hypotheses. We then generate 8 programs for each hypothesis, followed by 3 rounds of execution feedback. We treat these human-written hypotheses as oracle solutions in order for us to better understand the extent to which this pipeline is separately bottlenecked by hypothesis generation as opposed to program generation. The main results are shown in Table 1. Using a programmatic representation already boosts the performance over the direct prompting baseline by a large margin, from $17 \%$ to $23 \%$. Leveraging the summarized hypotheses is also helpful, improving the performance from $23 \%$ to $30 \%$. We obtain the best accuracy $33 \%$ when generating programs using human-selected hypotheses. This is on par with the version where we directly leverage the golden human-generated hypotheses. For reference, we also evaluated the method from <em>Icecuber (2020)</em>, which is the current DSL-based state-of-the-art approach <em>(Mirchandani et al., 2023)</em>. Although it does not use a language model, it provides useful context. We found that it answered $43 \%$ of our 100 sampled items correctly, slightly underperforming the human-written results.</p>
<h3>3.2.2 QUALITATIVE ReSULTS</h3>
<p>We show an example of generated hypotheses and the corresponding programs generated from the considered methods in Fig. 3. We observe that many of the correct hypotheses generated by GPT-4 are similar to the human-written hypotheses in terms of their specificity, although often less concise. Summarized hypotheses can often become vague and ambiguous, which is potentially the reason for degraded performance. Sometimes the correct hypothesis is omitted from the summarized hypotheses. Note, because we prompt GPT-4 to treat the grids as NumPy <em>(Harris et al., 2020)</em> arrays, we observe that GPT-4 tends to leverage various NumPy functions to perform the desired transformation.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<h1>Summarized Hypothesis and its Corresponding Generated Program</h1>
<p>To make the output, extract the colored shape(s) from the input grid, expand or duplicate them according to the specified pattern, and place the resulting shape(s) into the output grid.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">def</span><span class="w"> </span><span class="nx">transform_grid</span><span class="p">(</span><span class="nx">input_grid</span><span class="p">:</span><span class="w"> </span><span class="nx">np</span><span class="p">.</span><span class="nx">ndarray</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="nx">np</span><span class="p">.</span><span class="nx">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="nx">unique_numbers</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">np</span><span class="p">.</span><span class="nx">unique</span><span class="p">(</span><span class="nx">input_grid</span><span class="p">)</span>
<span class="w">    </span><span class="nx">output_grid</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">np</span><span class="p">.</span><span class="nx">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="mi">6</span><span class="p">),</span><span class="w"> </span><span class="nx">dtype</span><span class="p">=</span><span class="nx">int</span><span class="p">)</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">unique_numbers</span><span class="p">:</span>
<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="nx">number</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">:</span>
<span class="w">            </span><span class="k">continue</span>
<span class="w">            </span><span class="nx">num_rows</span><span class="p">,</span><span class="w"> </span><span class="nx">num_cols</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">np</span><span class="p">.</span><span class="k">where</span><span class="p">(</span><span class="nx">input_grid</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nx">number</span><span class="p">)</span>
<span class="w">            </span><span class="nx">min_row</span><span class="p">,</span><span class="w"> </span><span class="nx">max_row</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">min</span><span class="p">(</span><span class="nx">num_rows</span><span class="p">),</span><span class="w"> </span><span class="nx">max</span><span class="p">(</span><span class="nx">num_rows</span><span class="p">)</span>
<span class="w">            </span><span class="nx">min_col</span><span class="p">,</span><span class="w"> </span><span class="nx">max_col</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">min</span><span class="p">(</span><span class="nx">num_cols</span><span class="p">),</span><span class="w"> </span><span class="nx">max</span><span class="p">(</span><span class="nx">num_cols</span><span class="p">)</span>
<span class="w">            </span><span class="nx">shape</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">input_grid</span><span class="p">(</span><span class="nx">min_row</span><span class="p">:</span><span class="nx">max_row</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="nx">min_col</span><span class="p">:</span><span class="nx">max_col</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span>
<span class="w">            </span><span class="nx">output_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nx">shape</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="mi">0</span><span class="p">:</span><span class="nx">shape</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">shape</span>
<span class="w">            </span><span class="nx">output_grid</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="nx">shape</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="mi">3</span><span class="p">:</span><span class="mi">3</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">shape</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">shape</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span><span class="nx">shape</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="p">&lt;</span><span class="w"> </span><span class="mi">3</span><span class="p">:</span>
<span class="w">                </span><span class="nx">output_grid</span><span class="p">[</span><span class="nx">shape</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">shape</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="p">:</span><span class="nx">shape</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">shape</span>
<span class="w">            </span><span class="nx">output_grid</span><span class="p">[</span><span class="nx">shape</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">shape</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="mi">3</span><span class="p">:</span><span class="mi">3</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">shape</span><span class="p">.</span><span class="nx">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="nx">shape</span>
<span class="w">            </span><span class="k">return</span><span class="w"> </span><span class="nx">output_grid</span>
</code></pre></div>

<h2>Human-Selected Hypothesis and its Corresponding Generated Program</h2>
<p>In the input, you should see a black grid with an 8x8 size. The grid contains one colored shape (either teal, red, or blue), which is a $2 \times 2$ square with a corner missing. The output grid size should be $3 \times 6$. To make the output, you have to extract the colored shape, and then expand it horizontally by duplicating the entire shape. The result will be a colored shape that is twice as wide as the original one.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">transform_grid</span><span class="p">(</span><span class="n">input_grid</span><span class="o">:</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="o">:</span>
<span class="w">    </span><span class="n">input</span><span class="w"> </span><span class="n">numpy</span><span class="w"> </span><span class="kr">as</span><span class="w"> </span><span class="n">np</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Find</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">non</span><span class="o">-</span><span class="n">zero</span><span class="w"> </span><span class="n">elements</span><span class="w"> </span><span class="kr">and</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="n">indices</span>
<span class="w">    </span><span class="n">non_zero_indices</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">argwhere</span><span class="p">(</span><span class="n">input_grid</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span>
<span class="w">    </span><span class="n">min_row</span><span class="p">,</span><span class="w"> </span><span class="n">min_col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">num_zero_indices</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="w">    </span><span class="n">max_row</span><span class="p">,</span><span class="w"> </span><span class="n">max_col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">non_zero_indices</span><span class="p">,</span><span class="w"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Extract</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">colored</span><span class="w"> </span><span class="n">shape</span>
<span class="w">    </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_grid</span><span class="p">(</span><span class="n">min_row</span><span class="o">:</span><span class="n">max_row</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">min_col</span><span class="o">:</span><span class="n">max_col</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">Expand</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="n">horizontally</span>
<span class="w">    </span><span class="n">expanded_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">shape</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="p">))</span>
<span class="w">    </span><span class="kr">return</span><span class="w"> </span><span class="n">expanded_shape</span>
</code></pre></div>

<p>Figure 3: An ARC example of generated hypotheses using different methods and their corresponding generated programs. The summarized and human-selected hypotheses from LLM-generated candidates both yield correct programs. Note, these are gpt-4-0314-generated and we omit empty lines.</p>
<h3>3.2.3 MORE AbLATION STUDIES</h3>
<p>Chain of Thought (Wei et al., 2022). For this ablation, we wanted to understand the effect of the intermediate language without programs. We found GPT-4's performance dropped to $19 \%$, regardless of whether it generated the intermediate hypothesis or whether human-written hypotheses were used.</p>
<p>Execution Feedback. The results of models using different numbers of execution feedback iterations are summarized in Table 2. Execution feedback plays an important role regardless of how hypotheses are generated. However, the performance gain plateaus as the number of feedback iterations increases.</p>
<h3>3.3 1D-ARC</h3>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"># feedback iterations</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Method</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">Summarized Hypo.</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">28</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">Human-Selected Hypo.</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">33</td>
</tr>
<tr>
<td style="text-align: center;">Human-Written Hypo.</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">44</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">45</td>
</tr>
</tbody>
</table>
<p>Table 2: Accuracy (\%) of GPT-4 on ARC using different numbers of feedback iterations.</p>
<p>In contrast to the ARC experiments, GPT-4's performance on 1D-ARC was notably higher. We observed reasonably correct hypotheses by simply generating 16 hypothesis candidates. Thus, we do not need to obtain a subset of hypotheses to reduce the cost of implementing programs, but instead evaluate on all the hypotheses generated. We compare the direct prompting baseline with our method, with and without natural language hypotheses.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Accuracy (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Direct (Xu et al.)</td>
<td>39.6</td>
</tr>
<tr>
<td>Program Only (Ours)</td>
<td>61.1</td>
</tr>
<tr>
<td>Full (Ours)</td>
<td>73.1</td>
</tr>
</tbody>
</table>
<p>Table 3: Experimental results on 1D-ARC. Program and hypothesis generation both contribute to performance improvements.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Accuracy (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>CrossBeam (Shi et al.)</td>
<td>74.8</td>
</tr>
<tr>
<td>Program Only (Ours)</td>
<td>94.3</td>
</tr>
<tr>
<td>Full (Ours)</td>
<td>94.3</td>
</tr>
</tbody>
</table>
<p>Table 4: Experiment results on SyGuS. Our directly generated programmatic hypotheses and natural-language-conditioned programmatic hypotheses perform similarly.</p>
<p>Direct Prompting For this experiment, we report the accuracy of direct prompting results from <em>Xu et al. (2023b)</em> on the selected 108 tasks.</p>
<p>Program Only. We directly prompt GPT-4 to output Python programs given the training examples. We generate 80 programs for each task and select the program that passed most training examples.</p>
<p>Full. We first generate 16 different language hypotheses, then generate 4 programs for each, resulting in 64 programs per problem.</p>
<p>We summarize our results in Table 3. Generating hypotheses and implementing programs significantly improves the performance on 1D-ARC compared with the direct prompting method.</p>
<h3>3.4 SyGuS</h3>
<p>Settings. We evaluate all 89 tasks from the SyGuS dataset. Unlike ARC and 1D-ARC datasets, we follow the convention in the program synthesis literature and treat all examples as training. Accuracy is computed by whether a program passes all training examples.</p>
<p>Results. We find that GPT-4 can generate correct programs for 94.3% of the SyGuS tasks using 8 programs with two rounds of feedback without hypothesis generation, demonstrating strong performance in a direct program generation approach. Of the five remaining tasks, we find that three of the tasks have mistakes in their examples. As a result, when using natural language hypotheses to guide the code generation process, GPT-4â€™s performance does not meaningfully change, achieving the same performance 94.3% by generating 4 hypotheses and implementing 2 program for each hypothesis. As a comparison, the state-of-the-art program induction approach CrossBeam <em>(Shi et al., 2022)</em> can solve 74.8% of the dataset using a domain-specific language with 50K program candidates. For reference, the state-of-the-art DSL-based baseline CrossBeam only achieves an accuracy of 74.8 on this dataset.</p>
<h3>3.5 List Functions</h3>
<p>For this dataset, we sample 100 tasks from it for evaluation. Empirically we found that on this dataset, a correct language hypothesis almost always leads to a correct Python implementation, therefore our Full pipeline will generate 64 hypotheses and implement one Python program for each hypothesis. And the program only ablation will generate 64 programs for each task. The results are summarized in Table 5. Our method consistently outperforms baselines.</p>
<h2>4 Discussion</h2>
<h3>4.1 Failure Cases</h3>
<p>Currently, there are two types of failures in our pipeline. First, the model may be unable to generate a correct and sufficiently precise natural language hypothesis. Second, the model can still generate incorrect programs given a correct hypothesis.</p>
<p>Hypothesis Generation. Hypothesis generation is especially challenging for the ARC dataset as it involves recognizing visual patterns in 2D grids. While we observe that GPT-4 has a primitive ability to recognize points, lines, and rectangles, and to identify repetition and symmetry relationships, it has trouble understanding more complex shapes and visual relationships like translation, scaling, and containment. This is unsurprising as GPT-4 is trained primarily on text corpora, and the visual grid is input as text in our experiments. Furthermore, we observe that GPT-4 has difficulty proposing reasonable hypotheses for very large grids, possibly due to the limited context length. In contrast, GPT-4 was</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Accuracy (%)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Direct</td>
<td>31</td>
</tr>
<tr>
<td>Program Only (Ours)</td>
<td>59</td>
</tr>
<tr>
<td>Full (Ours)</td>
<td>69</td>
</tr>
</tbody>
</table>
<p>Table 5: Experiment results on List Functions. Code and language generation both contribute to performance improvements.</p>
<p>quite good at hypothesis generation on 1D-ARC. While the concepts may be easier for this dataset it is certainly the case that the visual encoding is easier. We thus tentatively suggest that current LMs are non-trivially capable of hypothesis generation for inductive learning and anticipate that vision-language models (Driess et al., 2023) may close the remaining gap for visual tasks like ARC.
Program Generation. Even with correct hypotheses, difficulties may arise when the task is hard to implement in Python. For example, task 444801 d 8 shown in Figure 4 was one where the language model failed when given a correct hypothesis. The task is difficult to solve programmatically, even for humans, as it requires identifying an irregular shape and then filling it according to an irregular pattern. This suggests a limitation of using generic Python programs for solving visual inductive reasoning tasks. Natural language hypotheses may also contain ambiguous concepts that mismatch the biases of the program generator. The human-written hypothesis for task 363442 ee in Figure 4 is: "In the input, you should see a color pattern on the left side and blue squares on the right. The output grid size same size as the input. To make the output, you have to use the blue square as the middle square and recreate the same pattern replacing the blue square with the same color in the middle as the pattern." GPT-4 is unable to understand what "color pattern" refers to and generates an incorrect program by treating the first three columns as the pattern. On the other hand, GPT-4's generated hypothesis mentions that "In the input, you should see a $\mathbf{3 x 3}$ colored square on the left side...", which yields the correct Python implementation. Thus a good match is needed between hypothesis generator and program synthesis, suggesting dircetions for future work.</p>
<h1>4.2 Considering Every Candidate Hypothesis.</h1>
<p>Currently, our pipeline does not consider many candidate hypotheses; we note that this is not a theoretical limitation of our method. In our experiments, we found that when the generated program passes all training cases, it almost always passed the test case (we only observe a single task that is an exception). Therefore, the performance of human-selected hypotheses can reasonably be treated as a lower bound for the performance if we consider every candidate hypothesis. However, we need to sample a large number of (64) hypotheses to have a reasonable hit rate of correct ones, and testing a single candidate hypothesis can take up to $1.5 \$$ ( 8 programs with two rounds of feedback) - leading us to evaluate summarizing and human filtering. This suggests that the effectiveness of our method will improve automatically as the inference cost of language models decreases.</p>
<h3>4.3 Combinatorial Search with Parsel</h3>
<p>We also explore the application of Parsel, an efficient compositional program generation method (Zelikman et al., 2023), in combination with GPT-4 to enhance the model's ability to generate and evaluate code implementations. This approach aims to capitalize on the benefits of compositional reasoning in problem-solving, by first decomposing a solution, generating multiple implementations of each part of the solution, and then searching over combinations of the implementations. This allows for many more programs to be tested with fewer LLM calls. For human-written hypotheses, this improved performance to $47.5 \%$, but for language-model-generated hypotheses it had the reverse effect. Details can be found in Appendix B.</p>
<h2>5 Related Works</h2>
<p>Inductive Reasoning. Techniques to allow automatic inductive reasoning have been widely studied by the artificial intelligence community as well as the program synthesis community. Given a set of observations, these efforts aim to computationally infer the underlying rules for a set of observations that can be generalized to novel scenarios. Traditional methods usually rely on programs written in manually designed domain-specific languages to represent the rule space and perform searching on the space to obtain the desired program. A number of heuristics have been proposed to speed up the search process. BUSTLE (Odena et al., 2020) proposes a neural search algorithm that takes the intermediate results of partial programs into account during the search process. DreamCoder (Ellis et al., 2023) introduces a wake-sleep algorithm that will dynamically build library functions on top of the primitive operations for solving more complex tasks with less running time. These methods typically require training on a corpora of related tasks, and cannot generalize across different domains due to the limited DSL. Earlier work in this area showed that introducing linguistic knowledge and selecting relevant language descriptions allows for better classifiers (Andreas et al., 2018). Recently, multiple works (Mirchandani et al., 2023; Gendron et al., 2023) tried to evaluate large language models on inductive reasoning tasks. These works directly prompt models to predict the output given the novel input as well as training examples, which leads to poor performance. Our work draws inspiration</p>
<p>from previous program synthesis literature to use programs as representations of the underlying rules. But we instead leverage a general programming language Python, which makes our method applicable to a wide range of different domains such as grid transformation and string transformation.
Reasoning with Programs. There has been a consistent effort to introduce program representations into different types of reasoning tasks such as visual reasoning (Andreas et al., 2016; Mao et al., 2019) and question answering (Dong \&amp; Lapata, 2016; Zhong et al., 2017). Naturally, these programmatic reasoning works build on prior work on generating programs with language models (Jacob \&amp; Tairas, 2010; Schulam et al., 2013; Kushman, 2015; Desai et al., 2016; Chen et al., 2021; Austin et al., 2021; Jain et al., 2022). Programs provide various advantages over end-to-end methods, such as interpretability, generalizability, and efficiency. Mainstream approaches have focused on learning to parse natural language questions into programs of domain-specific languages that can be executed to obtain the answer; a program executor is often jointly learned to execute primitive functions (Andreas et al., 2016; Mao et al., 2019).
Recently, LLMs have been shown to be capable of generating programs written in general-purpose programming languages. This inspired multiple works to leverage LLMs to reason with programmatic representations. Gao et al. (2022) introduced Program-Aided Language models (PAL), and Chen et al. (2022) proposed the "Program of Thoughts" (PoT) prompting, both of which prompt large language models to solve step-by-step math and symbolic reasoning tasks by proposing programs and offload the computation to a Python interpreter. Visprog (Gupta \&amp; Kembhavi, 2023) and ViperGPT (SurÃ­s et al., 2023) generated programs that can be executed using pretrained perception modules to tackle visual reasoning tasks. These approaches are superior in performance and require minimal data for in-context learning without the need for any training. Notably, the code generated by the models in these papers has primarily served as a computational aid, not a general task representation. In our case, programs serve as testable hypotheses for solving inductive reasoning tasks.
Lastly, Clement et al. (1986) investigated the correlation between analogical reasoning ability and the programming skills of high school students, indicating a significant relationship between the ability to perform analogical reasoning and write compositional programs. Given previously observed parallels between language model behavior and cognitive psychology experiments (e.g., Dasgupta et al. 2022; Aher et al. 2022), language models may exhibit a similar trend.</p>
<h1>6 Limitations and Future Work</h1>
<p>Currently, our method requires multiple LLM queries, which may be costly depending on the complexity of tasks. But, with the rapid improvement and decreasing costs, this allows for increasingly complex tasks to be solved at a given cost. There are also several promising directions for future work building on these results. First, some tasks and objectives are inherently stochastic or challenging to express explicitly as a program - for these, supporting objectives besides exact match is essential (e.g., ROUGE (Lin, 2004)) and may benefit from leveraging from code that leverages other machine learning models (e.g., VisProg (Gupta \&amp; Kembhavi, 2023) for visual tasks). However, Python is already notably more expressive than the DSLs that were standard in prior work on program synthesis (Devlin et al., 2017; Ellis et al., 2019; Sharma et al., 2017; Guu et al., 2017).
Lastly, although we observed a low false-positive rate on our datasets (i.e., if a working program was found, it almost always generalized), other inductive reasoning tasks may require further work to avoid false-positives. Under the assumption of a low false-positive rate, which we empirically observed, our method is approximately the same as importance sampling in a hierarchical Bayesian model, where the importance distribution is that of the language model conditioned on the examples. This could potentially guide future work toward more efficient algorithms.</p>
<h2>7 CONCLUSIONS</h2>
<p>In this work, we propose a pipeline that facilitates better inductive reasoning in large language models. The core idea is to first prompt LLMs to generate hypotheses of the underlying rule in natural language, to then implement the hypotheses as Python programs, and to search for programs which can be verified on the training examples and executed on novel inputs for inference. We evaluate the effectiveness of our pipeline on four challenging datasets Abstraction and Reasoning Corpus (ARC), its variant 1D-ARC, a string transformation dataset SyGuS, and List Functions dataset. Our pipeline outperforms the baseline methods by a large margin on all four datasets.</p>
<h1>8 ACKNOWLEDGEMENTS</h1>
<p>This work was supported in part by the Microsoft Accelerate Foundation Models Research program, National Science Foundation Grant No. 2302701, and NSF Expeditions Grant No. 1918771. Gabriel Poesia is supported by a Stanford Interdisciplinary Graduate Fellowship.</p>
<h2>REFERENCES</h2>
<p>Sam Acquaviva, Yewen Pu, Marta Kryven, Theodoros Sechopoulos, Catherine Wong, Gabrielle Ecanow, Maxwell Nye, Michael Tessler, and Josh Tenenbaum. Communicating natural programs to humans and machines. Advances in Neural Information Processing Systems, 35:3731-3743, 2022.</p>
<p>Gati Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans. arXiv preprint arXiv:2208.10264, 2022.</p>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Learning to compose neural networks for question answering. In NAACL, 2016.</p>
<p>Jacob Andreas, Dan Klein, and Sergey Levine. Learning with latent language. In ACL, 2018.
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.</p>
<p>Xinyun Chen, Maxwell Lin, Nathanael SchÃ¤rli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023.</p>
<p>FranÃ§ois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.
Catherine A Clement, D Midian Kurland, Ronald Mawby, and Roy D Pea. Analogical reasoning and computer programming. Journal of Educational Computing Research, 2(4):473-486, 1986.</p>
<p>Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. Language models show human-like content effects on reasoning. arXiv preprint arXiv:2207.07051, 2022.</p>
<p>Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, and Subhajit Roy. Program synthesis using natural language. In Proceedings of the 38th International Conference on Software Engineering, pp. 345-356, 2016.</p>
<p>Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy i/o. In International conference on machine learning, pp. 990-998. PMLR, 2017.</p>
<p>Li Dong and Mirella Lapata. Language to logical form with neural attention. In ACL, 2016.
Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. In ICML, 2023.</p>
<p>Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-Lezama. Write, execute, assess: Program synthesis with a repl. Advances in Neural Information Processing Systems, 32, 2019.</p>
<p>Kevin Ellis, Lionel Wong, Maxwell Nye, Mathias Sable-Meyer, Luc Cary, Lore Anaya Pozo, Luke Hewitt, Armando Solar-Lezama, and Joshua B Tenenbaum. Dreamcoder: growing generalizable, interpretable knowledge with wake-sleep bayesian program learning. Philosophical Transactions of the Royal Society A, 381(2251):20220050, 2023.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.</p>
<p>GaÃ«l Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie. Large language models are not abstract reasoners. arXiv preprint arXiv:2305.19555, 2023.</p>
<p>Noah D Goodman, Joshua B Tenenbaum, Jacob Feldman, and Thomas L Griffiths. A rational analysis of rule-based concept learning. Cognitive science, 32(1):108-154, 2008.</p>
<p>Noah D Goodman, Tomer D Ullman, and Joshua B Tenenbaum. Learning a theory of causality. Psychological review, 118(1):110, 2011.</p>
<p>Sumit Gulwani. Automating string processing in spreadsheets using input-output examples. ACM Sigplan Notices, 46(1):317-330, 2011.</p>
<p>Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning without training. In CVPR, 2023.</p>
<p>Kelvin Guu, Panupong Pasupat, Evan Liu, and Percy Liang. From language to programs: Bridging reinforcement learning and maximum marginal likelihood. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1051-1062, 2017.</p>
<p>Charles R. Harris, K. Jarrod Millman, StÃ©fan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer, Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime FernÃ¡ndez del RÃ­o, Mark Wiebe, Pearu Peterson, Pierre GÃ©rard-Marchant, Kevin Sheppard, Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, and Travis E. Oliphant. Array programming with NumPy. Nature, 585(7825):357-362, September 2020. doi: 10.1038/s41586-020-2649-2. URL https://doi.org/10.1038/s41586-020-2649-2.</p>
<p>Icecuber. Abstraction and rasoning challenge 1st place solution. 2020. URL https:// www.kaggle.com/competitions/abstraction-and-reasoning-challenge/ discussion/154597.</p>
<p>Ferosh Jacob and Robert Tairas. Code template inference using language models. In Proceedings of the 48th Annual Southeast Regional Conference, pp. 1-6, 2010.</p>
<p>Naman Jain, Skanda Vaidyanath, Arun Iyer, Nagarajan Natarajan, Suresh Parthasarathy, Sriram Rajamani, and Rahul Sharma. Jigsaw: Large language models meet program synthesis. In Proceedings of the 44th International Conference on Software Engineering, pp. 1219-1231, 2022.</p>
<p>Aysja Johnson, Wai Keen Vong, Brenden Lake, and Todd M Gureckis. Fast and flexible: Human program induction in abstract reasoning tasks. In Proceedings of the Annual Meeting of the Cognitive Science Society, 2021.</p>
<p>Nate Kushman. Generating computer programs from natural language descriptions. PhD thesis, Massachusetts Institute of Technology, 2015.</p>
<p>Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022.</p>
<p>Vu Le, Daniel Perelman, Oleksandr Polozov, Mohammad Raza, Abhishek Udupa, and Sumit Gulwani. Interactive program synthesis. arXiv preprint arXiv:1703.03539, 2017.</p>
<p>Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pp. 74-81, 2004.</p>
<p>Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In $I C L R, 2019$.</p>
<p>Ana MarasoviÄ‡, Iz Beltagy, Doug Downey, and Matthew E Peters. Few-shot self-rationalization with natural language prompts. arXiv preprint arXiv:2111.08284, 2021.</p>
<p>Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. arXiv preprint arXiv:2307.04721, 2023.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>
<p>Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh, Charles Sutton, and Hanjun Dai. Bustle: Bottom-up program synthesis through learning-guided exploration. arXiv preprint arXiv:2007.14381, 2020.</p>
<p>Nancy Pang. Deep learning for code repair. University of British Columbia, 2018.
Charles S Peirce. Questions concerning certain faculties claimed for man. The Journal of Speculative Philosophy, 2(2):103-114, 1868.</p>
<p>Md Mostafizer Rahman, Yutaka Watanabe, and Keita Nakamura. A bidirectional lstm language model for code evaluation and repair. Symmetry, 13(2):247, 2021.</p>
<p>Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. ACL, 2019.</p>
<p>Joshua S Rule, Joshua B Tenenbaum, and Steven T Piantadosi. The child as hacker. Trends in cognitive sciences, 24(11):900-915, 2020.</p>
<p>Peter Schulam, Roni Rosenfeld, and Premkumar Devanbu. Building statistical language models of code. In 2013 1st International Workshop on Data Analysis Patterns in Software Engineering (DAPSE), pp. 1-3. IEEE, 2013.</p>
<p>Eric Schulte, Stephanie Forrest, and Westley Weimer. Automated program repair through the evolution of assembly code. In Proceedings of the 25th IEEE/ACM International Conference on Automated Software Engineering, pp. 313-316, 2010.</p>
<p>Gopal Sharma, Rishabh Goyal, Difan Liu, Evangelos Kalogerakis, and Subhransu Maji. Csgnet: Neural shape parser for constructive solid geometry. 2018 ieee. In CVF Conference on Computer Vision and Pattern Recognition, pp. 5515-5523, 2017.</p>
<p>Kensen Shi, Hanjun Dai, Kevin Ellis, and Charles Sutton. Crossbeam: Learning to search in bottom-up program synthesis. In $I C L R, 2022$.</p>
<p>Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised commonsense question answering with self-talk. In EMNLP, 2020.</p>
<p>DÃ­dac SurÃ­s, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. arXiv preprint arXiv:2303.08128, 2023.</p>
<p>Joshua B Tenenbaum, Thomas L Griffiths, and Charles Kemp. Theory-based bayesian models of inductive learning and reasoning. Trends in cognitive sciences, 10(7):309-318, 2006.</p>
<p>Joshua B Tenenbaum, Charles Kemp, Thomas L Griffiths, and Noah D Goodman. How to grow a mind: Statistics, structure, and abstraction. science, 331(6022):1279-1285, 2011.</p>
<p>Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh. Neural program repair by jointly learning to localize and repair. arXiv preprint arXiv:1904.01720, 2019.</p>
<p>Edward Vul, Noah Goodman, Thomas L Griffiths, and Joshua B Tenenbaum. One and done? optimal decisions from very few samples. Cognitive science, 38(4):599-637, 2014.</p>
<p>Taylor Webb, Keith J Holyoak, and Hongjing Lu. Emergent analogical reasoning in large language models. arXiv preprint arXiv:2212.09196, 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Yudong Xu, Elias B Khalil, and Scott Sanner. Graphs, constraints, and search for the abstraction and reasoning corpus. In AAAI, 2023a.</p>
<p>Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, and Elias B Khalil. Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. arXiv preprint arXiv:2305.18354, 2023b.</p>
<p>Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. In NeurIPS, 2022.</p>
<p>Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman, and Nick Haber. Parsel: Algorithmic reasoning with language models by composing decompositions, 2023.</p>
<p>Tianyi Zhang, Tao Yu, Tatsunori Hashimoto, Mike Lewis, Wen-tau Yih, Daniel Fried, and Sida Wang. Coder reviewer reranking for code generation. In ICML, 2023.</p>
<p>Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103, 2017.</p>
<p>Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville, Behnam Neyshabur, and Hanie Sedghi. Teaching algorithmic reasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022.</p>
<h1>A EXPERIMENT DETAILS</h1>
<p>Prompts and Hyperparameters. For hypothesis generation, The prompts are shown in Figure A.8, Figure A. 10 and Figure A.11. We set the temperature to be 1.0 and the maximum number of tokens in response to be 200. For program generation and execution feedback, we use a temperature of 0.7 and set the maximum number of tokens to be 1000 . Throughout the experiments, we use gpt-4-0613 and gpt-3.5-turbo-0301. Earlier results used gpt-4-0314, which we include in Appendix C.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure A.5: Accuracy of our methods with varying numbers of hypotheses and programs.
Execution Feedback. After obtaining programs from language models, we directly execute them on training examples. If there are no programs passing all training examples, we prompt the language model again with the first example that the program fails to pass, and ask it to correct the program. To save cost for experiments on ARC where we generate more than 64 programs, we do not run execution feedback on every program. Instead, we cluster programs by their output on the training examples. Only one program is selected from each cluster for feedback execution.</p>
<h2>B ADDITIONAL ReSULTS AND DiSCUSSIONS</h2>
<h2>B. 1 MORE Ablations</h2>
<h2>Ablation on the Number of Hypotheses \&amp; Programs.</h2>
<p>First, we show the percentage of tasks that have a correct hypothesis when we increase the number of samples in Figure A.6. Increasing the sample size will lead to a steady improvement, although the slope is flattening. We also report how the accuracy changes when we vary the number of hypotheses for each task on ARC-1D and List Func, fixing the number of programs for each hypothesis. For both datasets, our pipeline and the program-only ablation will steadily gain performance improvement as the number of samples increases, as shown in Figure A.5.
Zero-shot Hypothesis Generation. Here, we explore the effect of examples for hypothesis generation. On 1D-ARC, we remove all the example tasks in the prompt while still generating 16 hypotheses and 4 programs for each hypothesis. This gives an accuracy of $71.3 \%$, which is a little worse than the performance with two-shot prompting $73.1 \%$.</p>
<h2>B. 2 GPT-3.5</h2>
<p>In this ablation, we leverage GPT-3.5 instead of GPT-4 in our pipeline.
ARC. Compared with GPT-4, we find GPT-3.5 mostly generates meaningless hypotheses given the inputs from ARC. We then test GPT-3.5's ability to generate program implementations when given the human-written hypotheses. Because GPT-3.5's context length is only 4096 tokens (GPT-4, which</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">1D-ARC</th>
<th style="text-align: center;">SyGuS</th>
<th style="text-align: center;">List Func</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">Program Only (Ours)</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">46</td>
</tr>
<tr>
<td style="text-align: center;">Full (Ours)</td>
<td style="text-align: center;">26.9</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">57</td>
</tr>
</tbody>
</table>
<p>Table A.1: Accuracy of GPT-3.5 on 1D-ARC, SyGuS and List Functions datasets.
we use for most of our experiments, has a context length of 8192), only 33 tasks can fit into the prompt. Therefore, we treat the problems that do not fit in the context window as incorrect and do not leverage execution feedback. GPT-3.5 achieves an accuracy of $27 \%$ with 128 programs when given human-written hypotheses. Using a 16384 context length version of GPT 3.5 boosts the performance to $30 \%$. This is still worse than GPT-4, but GPT-3.5 is approximately 20 times cheaper than GPT-4. This shows a trade-off between the performance and cost when choosing base models.
Other datasets. We continue to evaluate GPT-3.5 on other three datasets, where we found that GPT-3.5 have reasonable performance for both hypothesis generation and program generation. Therefore we keep the same setting and rerun all experiments with gpt-3.5-0613. The results are summarized in Table A.1. For 1D-ARC, we generate 16 hypotheses and implement 8 programs for each hypothesis; for SyGuS, we generate 4 hypotheses and implement 2 programs for each hypothesis with two rounds of feedback; for List Functions, we generate 64 hypotheses and 1 program for each hypothesis. All program-only ablation generates the same number of programs in total. We can observe that our method is also effective on GPT-3.5 on a wide range of domains.</p>
<h1>B. 3 Parsel for Program Generation</h1>
<p>To enhance the performance of program generation, we also adapt a recently proposed method Parsel (Zelikman et al., 2023) to our settings. Instead of directly generating programs, we first generate an intermediate pseudocode program written in Parsel language from a given hypothesis, as shown in Figure A.7. The Parsel language specifies the functions needed to be implemented by specifying the function name, arguments and its desired behavior in natural language. Then the Parsel program is passed to a language model for implementing individual functions.
To allow functions to be implemented with knowledge of their context, unlike the original Parsel paper, we implement all functions needed in a single API call. We then sample multiple trials and extract multiple implementations of each function specified in the Parsel program. Then we will recombine every implementation of each function to generate multiple programs. Using humanwritten hypotheses, we achieve an accuracy of $47.5 \%$ on the 40 randomly selected questions from ARC by generating 4 Parsel Programs for each hypothesis and 8 programs for each Parsel program without any feedback, surpassing the $37.5 \%$ accuracy obtained by directly generating programs from hypotheses. However, we found that this yields worse performance with LLM-generated hypotheses: on the 13 selected tasks that GPT-4 can generate correct hypotheses, directly generating 8 programs with 1 round of execution feedback yields an accuracy of $92 \%$ while 4 Parsel programs $\times 8$ python programs with 1 round of feedback only yields an accuracy $69 \%$. We suspect that this is due to Parsel introducing a new level of abstraction into our pipeline: given that error might accumulate during the transformation between different levels of abstraction, Parsel increases the probability of generating incorrect final programs. We believe leveraging better code generation techniques is a promising direction to improve our pipeline.</p>
<h2>B. 4 Pilot Experiments and Non-Systematic Findings</h2>
<p>Specifying the Python Types of Matrices for Grids in ARC. In the prompt we use for ARC, we indicate the grids are represented as NumPy arrays using Python type hint (numpy. ndarray [int]). The typing hint plays an important role in generating programs from language hypotheses, since it encourages LLMs to leverage NumPy functions that are suited for grid transformation, such as flipping, 2D indexing. If we change the typing hint to List [List [int] ], LLMs will no longer leverage this library function, which makes the program longer and more error-prone. Using humanwritten hypotheses, 8 programs and one round of execution feedback. GPT-4 can only achieve $32.5 \%$, compared with the $37.5 \%$ performance the using NumPy array signature.</p>
<p>Using LLMs to Rank Hypotheses. We also explore to use LLMs to rank language hypotheses to throw away bad hypotheses. This is inspired by Zhang et al. (2023), which reranks code generated from a description based on its probability of generating the description. Because GPT-4 does not expose the log-probabilities of its generated items, and there is no clear way to extract the log-probabilities of the hypotheses, we instead use GPT-3 to rerank the hypotheses generated by GPT-4 by looking at their probabilities of generating the input-output examples, given the hypothesis. We evaluate this ranking method on the 21 tasks where GPT-4 is able to generate a correct language hypothesis from 64 candidates. After the ranking, there are only 10 tasks where the correct hypotheses are placed in the top 16 candidates. This prevents us from reducing the hypotheses needed to implement without sacrificing the overall performance.</p>
<p>High-Level Representations of ARC Grids. We observed that many for many tasks in ARC, it is easy for LLMs to come up with reasonable hypotheses if the grids are parsed into a useful geometric representation, such as irregular shapes, diagnoal lines. As a result, we explored the possibility of using alternative geometric representations, similar to concurrent work (Xu et al., 2023b). In particular, we attempted to treat each grid as the result of a sequence of shape placements, for example:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Blue</span><span class="w"> </span><span class="n">Rectangle</span><span class="o">:</span><span class="w"> </span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="n">size</span><span class="o">:</span><span class="w"> </span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="mi">5</span><span class="p">)</span>
<span class="n">Black</span><span class="w"> </span><span class="n">Line</span><span class="o">:</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)</span>
<span class="n">Red</span><span class="w"> </span><span class="n">Points</span><span class="o">:</span><span class="w"> </span><span class="p">[(</span><span class="mi">7</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">),</span><span class="w"> </span><span class="p">(</span><span class="mi">9</span><span class="p">,</span><span class="w"> </span><span class="mi">4</span><span class="p">)]</span>
</code></pre></div>

<p>We implemented an algorithm to identify the shortest possible sequence of shape placements that would result in the observed grid. While we observed that this allowed the model to propose more reasonable hypotheses for a subset of the problems, it harmed performance on more of them. This is due in part to the inherent difficulty of proposing a useful general representation for ARC tasks.</p>
<p>Potential Data Memorization. While large language models have shown remarkable performance on numerous benchmarks, there are recurring concerns about whether these models have simply memorized the answers due to observing the problems during training, instead of actually solving the desired tasks. This is particularly true for closed-source models where details of the training set are not publicly available, such as GPT-4. Since the ARC (as well as the LARC dataset with human-written hypotheses) and SyGuS datasets are publicly available on the internet, there is a possibility that GPT-4's training data contains these datasets, which might affect how we interpret these results. While differentiating between memorization and generalization for these close-sourced models remains an open problem, there are few pieces of evidence that show the effectiveness of our method. First, as far as we know, there are no public attempts to solve ARC or SyGuS datasets with Python programs. Second, we tried prompting GPT-4 with some examples in a task and asked it to output other examples in the same task, and GPT-4 failed to do so. Third, the substantial boost of our full pipeline over the direct prediction baseline cannot be simply explained by data memorization.</p>
<h1>C ReSults With GPT-4-0314</h1>
<p>We initially ran our experiments with GPT-4-0314. Later, due partially to its deprecation, and partially due to a change in our available compute resources, we reran our experiments with GPT -4-0613. We include the original result tables here for reference. All the conclusions remain consistent when we scale up the experiments.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Accuracy (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Direct</td>
<td style="text-align: center;">12.5</td>
</tr>
<tr>
<td style="text-align: center;">Program Only</td>
<td style="text-align: center;">17.5</td>
</tr>
<tr>
<td style="text-align: center;">Summarized Hypo.</td>
<td style="text-align: center;">27.5</td>
</tr>
<tr>
<td style="text-align: center;">Human-Selected Hypo.</td>
<td style="text-align: center;">37.5</td>
</tr>
<tr>
<td style="text-align: center;">Human-Written Hypo.*</td>
<td style="text-align: center;">37.5</td>
</tr>
</tbody>
</table>
<p>Table A.2: Results of the baseline and variants of our method on the randomly selected 40 tasks from ARC. Our method outperforms baselines with or without human supervision.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Accuracy (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Direct (Xu et al.)</td>
<td style="text-align: center;">38.8</td>
</tr>
<tr>
<td style="text-align: center;">Program Only (Ours)</td>
<td style="text-align: center;">58.3</td>
</tr>
<tr>
<td style="text-align: center;">Full (Ours)</td>
<td style="text-align: center;">77.8</td>
</tr>
</tbody>
</table>
<p>Table A.3: Experiment results on 1D-ARC. Program and hypothesis generation both contribute to performance improvements.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Accuracy (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CrossBeam (Shi et al.)</td>
<td style="text-align: center;">74.8</td>
</tr>
<tr>
<td style="text-align: center;">Program Only (Ours)</td>
<td style="text-align: center;">94.3</td>
</tr>
<tr>
<td style="text-align: center;">Full (Ours)</td>
<td style="text-align: center;">93.2</td>
</tr>
</tbody>
</table>
<p>Table A.4: Experiment results on SyGuS. Our directly generated programmatic hypotheses and natural-language-conditioned programmatic hypotheses perform similarly.</p>
<p>Summarized Hypothesis and its Corresponding Generated Parsel Program and Python Program
To make the output, extract the colored shape(s) from the input grid, expand or duplicate them according to the specified pattern, and place the resulting shape(s) into the output grid.</p>
<div class="codehilite"><pre><span></span><code>transform_grid(input_grid): Extract figure from input grid and create output grid
    --extract_figure(input_grid): Identify the non-zero elements in the input grid and their position
    --create_output_grid(figure): Create a 2e6 grid and copy the figure two times into it
</code></pre></div>

<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure A.7: An ARC example of generated hypotheses using Parsel (Zelikman et al., 2023) using the same task as Figure 3.</p>
<h1>Prompt for Hypothesis Generation</h1>
<p>[Role: system]
You will be given a list of input-output pairs. Each input and output is a grid of numbers representing representing a visual grid. There is a SINGLE pattern that transforms each input grid to the corresponding output grid.
The pattern may involve counting or sorting objects (e.g. sorting by size), comparing numbers (e.g. which shape or symbol appears the most? Which is the largest object? Which objects are the same size?), or repeating a pattern for a fixed number of time.
There are other concepts that may be relevant.</p>
<ul>
<li>Lines, rectangular shapes</li>
<li>Symmetries rotations, translations.</li>
<li>Shape upscaling or downscaling, elastic distortions.</li>
<li>Containing / being contained / being inside or outside of a perimeter.</li>
<li>Drawing lines, connecting points, orthogonal projections.</li>
<li>Copying, repeating objects.</li>
</ul>
<p>You should treat black cells as empty cells (backgrounds).
The number in the input grid can be mapped to the following colors:0:black; 1:blue; 2:red; 3:green; 4:yellow; 5:grey; 6:fuschia; 7:orange; 8:teal; 9:brown
Output the language description of the transformation.
[Role: user]
Case 0:
Input:
$[[338]$
$[370]$
$[500]]$
Output:
[[0 0 5]
$[073]$
$[833]]$
Case 1:
Input:
[[5 5 2]
$[100]$
$[000]]$
Output:
[[0 0 0]
$[001]$
$[255]]$
[Role: assistant]
Describing the input grid: In the input, you should see a black grid with a colored shape
Describing the size of the output grid: The output grid size is the same as the input grid
Describing how to transform the grid: To make the output, you have to rotate the whole grid two times. Imagine that the entire grid has been flipped vertically and horizontally.
[Role: user]
Case 0:
Input:
[[0 10 1]
$[0001]$
$[1010]$
$[0001]$
$[4444]$
$[0202]$
$[0002]$
$[2002]$
$[2220]]$
Output:
[[0 0 0 0]
$[0000]$
$[0033]$
$[3333]]$</p>
<p>Figure A.8: The prompt used to generate the hypotheses. Continued in Figure A.9. We use the Chat completions API, where every prompt consists of a list of messages representing a conversation. Each message contains a text and a role indicating the agent producing the text.</p>
<div class="codehilite"><pre><span></span><code>    Case 1:
    Input:
    [[1 1 0 0]
    [1 0 1 0]
    [1 1 0 1]
    [0 1 1 0]
    [4 4 4 4]
    [0 2 2 2]
    [2 0 2 0]
    [2 2 2 2]
    [2 2 2 2]]
    Output:
    [[3 0 3 3]
    [0 0 0 0]
    [0 0 3 0]
    [3 0 0 3]]
    Case 2:
    Input:
    [[0 1 0 0]
    [1 0 1 1]
    [1 1 1 0]
    [1 1 1 0]
    [4 4 4 4]
    [0 0 0 0]
    [0 2 0 2]
    [2 2 0 2]
    [0 2 0 0]]
    Output:
    [[0 3 0 0]
    [3 3 3 0]
    [0 0 3 3]
    [3 0 3 0]]
    Case 3:
    Input:
    [[1 0 1 1]
    [0 0 0 1]
    [1 1 0 0]
    [0 0 1 1]
    [4 4 4 4]
    [0 2 2 2]
    [0 2 2 2]
    [2 0 2 2]
    [2 2 2 2]]
    Output:
    [[3 3 0 0]
    [0 3 3 0]
    [0 3 3 3]
    [3 3 0 0]]
</code></pre></div>

<p>[Role: assistant]
Describing the input grid: In the input, you should see a $4 \times 4$ square of blue and black boxes on top of a yellow line. Below the yellow line is a $4 \times 4$ square of red and black boxes.
Describing the size of the output grid: The output grid size .should be $4 \times 4$.
Describing how to transform the grid: To make the output, you have to compare the squares in the top and bottom $4 \times 4$ square. If they match (both black or both color) make the output square black. If they do not match, make the output square green.
[Role: user]
Case 0:
...
$&lt;$ Training examples of the task to be solved $&gt;$
...
Figure A.9: The prompt used to generate the hypotheses (First half in Figure A.8).</p>
<h1>Prompt for Program Implementation</h1>
<p>[Role: User]
Example 0:
Input:
$[[000000000]$
$[08800000]$
$[00800000]$
$[08880000]$
$[00000000]$
$[00000000]$
$[00000000]$
Output:
[[880880]
$[080080]$
$[888888]]$
Example 1:
Input:
[[00000000]
$[00000000]$
$[00000000]$
$[00000000]$
$[00000000]$
$[00020000]$
$[00222000]$
$[00220000]]$
Output:
[[020020]
$[222222]$
$[220220]]$
Example 2:
Input:
[[00000000]
$[00000110]$
$[00001000]$
$[00000100]$
$[00000000]$
$[00000000]$
$[00000000]$
$[00000000]$
Output:
[[011011]
$[100100]$
$[010010]]$
Now, please write a python program transform_grid(input_grid: np.ndarray[int]) -&gt; np.ndarray[int] that transforms the input grid to the corresponding output grid.
Hint: You may want to use the following guidance to implement the function:
Hint: You may want to use the following guidance to implement the function:
To make the output, extract the colored shape(s) from the input grid, expand or duplicate them according to the specified pattern, and place the resulting shape(s) into the output grid.
The number in the input grid can be mapped to the following colors:0:black; 1:blue; 2:red; 3:green; 4:yellow; 5:grey; 6:fuschia; 7:orange; 8:teal; 9:brown
Just reply with the implementation of transform_grid(input_grid: np.ndarray[int]) in Python and nothing else, each cell in the output should only be numbers from 0 to 9 .</p>
<p>Figure A.10: The prompt used to generate the program given the hypothesis (bold in the text) for the same task as Figure 3.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The ARC challenge officially uses top-3 accuracy, checking if any of three model outputs are correct, but we consider top-1 accuracy, like most related works <em>(Xu et al., 2023b; Gendron et al., 2023; Mirchandani et al., 2023)</em>.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
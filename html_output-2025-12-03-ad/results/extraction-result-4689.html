<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4689 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4689</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4689</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d" target="_blank">Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is found that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large, and suggests that the easy two-step strategy of linear probing then full fine- Tuning (LP-FT) combines the benefits of both fine- tuning and linear probing.</p>
                <p><strong>Paper Abstract:</strong> When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer—the “head”). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR → STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head—this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4689",
    "paper_id": "paper-29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005385249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution</h1>
<p>Ananya Kumar Aditi Raghunathan Robbie Jones Tengyu Ma Percy Liang<br>Stanford University<br>Department of Computer Science<br>{ananya, aditir, rmjones, tengyuma, pliang}@cs.stanford.edu</p>
<h4>Abstract</h4>
<p>When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer-the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR $\rightarrow$ STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2\% higher accuracy ID but $7 \%$ lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head-this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets ( $1 \%$ better ID, $10 \%$ better OOD than full fine-tuning).</p>
<h2>1 Introduction</h2>
<p>Pretraining a model on a large dataset before transferring to a downstream task's training data substantially improves accuracy over training from scratch-for example, pretraining a ResNet-50 on unlabeled ImageNet boosts accuracy on CIFAR-10 from $94 \%$ to $98 \%$ (Chen et al., 2020a,b). Achieving high in-distribution accuracy is not enough: high-stakes applications such as poverty mapping in under-resourced countries (Jean et al., 2016), self-driving cars (Yu et al., 2020), and medical diagnosis (AlBadawy et al., 2018), require models that also generalize to circumstances not seen in the training distribution. In addition to testing on data drawn from the downstream task's training distribution (in-distribution; ID), it is increasingly important to test on data distributions unseen during training (out-of-distribution; OOD). OOD accuracy can be much lower than ID accuracy; for example, an ImageNet pretrained ResNet-50 fine-tuned on CIFAR-10 gets 98\% accuracy on CIFAR-10 (ID) but $82 \%$ on STL (OOD).</p>
<p>After initializing with a pretrained model, two popular transfer methods are fine-tuning (running gradient descent on all the model parameters), and linear probing (tuning the head but freezing lower layers). In the ID setting, it is well known that fine-tuning leads to better accuracy than linear probing (Kornblith et al., 2019;</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Given a good feature extractor (top-left), a randomly initialized head is added to map features to outputs and we can (a) fine-tune all the model parameters or (b) linear-probe, which freezes the feature extractor and trains only the head. We run experiments on ten distribution shifts. Fine-tuning does well when the test example is sampled from the fine-tuning distribution (ID), but can underperform on test examples sampled from OOD distributions (when the distribution shift is large). (c) Our theory indicates that fine-tuning can distort the pretrained feature extractor and lead to poor OOD accuracy, but initializing with a linear probed head can fix this—empirically LP-FT gets better accuracies both ID and OOD.</p>
<p>Zhai et al., 2020; He et al., 2020), ${ }^{1}$ and even when testing OOD, prior work usually fine-tunes all parameters of their model (Hendrycks et al., 2019a; Miller et al., 2021; Andreassen et al., 2021). Intuitively, fine-tuning all layers of a network can improve pretrained features by adapting them to the specific task, while linear probing simply inherits the frozen pretrained features.</p>
<p>In this work, we investigate the OOD accuracy of fine-tuning and linear probing and find that surprisingly, fine-tuning can do worse than linear probing in the presence of large distribution shift. We experiment on ten distribution shift benchmarks (Breeds Living17, Breeds Entity30, DomainNet, CIFAR $\rightarrow$ STL, CIFAR10.1, FMoW geo-shift, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), initializing with good pretrained features from MoCo-v2 (Chen et al., 2020b) and CLIP (Radford et al., 2021). While both methods offer gains over training from scratch, fine-tuning improves the average ID accuracy relative to linear probing from $83 \%$ to $85 \%$ but brings down the OOD accuracy from $66 \%$ to $59 \%$ (Figure 1).</p>
<p>Under what conditions does fine-tuning underperform linear probing? We theoretically consider fine-tuning a two-layer linear network in an overparameterized regression setting where the feature extractor layer has been pretrained to map high-dimensional inputs to useful, lower-dimensional, features. We prove that fine-tuning is worse than linear probing on directions outside the span of the training data when using "good" pretrained features. Even with an infinitesimally small learning rate, fine-tuning distorts pretrained features-the features of ID training data are updated while those of OOD data change less. Since the head and feature extractor are simultaneously optimized during fine-tuning to a configuration that works well on ID training data, the head only accomodates the distorted features of ID points and performs poorly (relative to linear probing) on the less changed features of OOD points. Interestingly, we show that this feature distortion issue cannot be simply fixed by early stopping-throughout the entire process of fine-tuning, we never pass through parameters that do well OOD (relative to linear probing). On the other hand, given "good" features, linear-probing</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>extrapolates better OOD because it preserves pretrained features, but does not do as well as fine-tuning ID because linear probing cannot adapt the features to the downstream task.</p>
<p>Technical challenges. Existing theoretical work on transfer learning focuses on linear probing (Wu et al., 2020; Tripuraneni et al., 2020; Du et al., 2020). In contrast, analyses of fine-tuning is scarce and challenging because it requires understanding the training dynamics, instead of only the loss function and its global minimizers. In fact, fine-tuning and training from scratch optimize the same training loss and only differ in their initializations (pretrained vs random). A mathematical analysis that distinguishes them needs to capture properties of the different minima that these algorithms converge to, a phenomenon that is sometimes theoretically referred to as the implicit regularization effect of initialization (Neyshabur et al., 2014). Accordingly, our analysis reasons about the parameters that gradient methods pass through starting from the pretrained initialization, which is challenging because this is a non-convex optimization problem and there is no known closed form for this trajectory. Two-layer linear networks are widely studied in the literature on implicit regularization (Saxe et al., 2014; Gunasekar et al., 2017; Gidel et al., 2019; Arora et al., 2018). However, they analyze random and often small initializations, which don't capture pretraining.</p>
<p>Algorithmic implications. Our theory shows that fine-tuning underpeforms because when trying to fit ID training data with a randomly initialized head, the feature extractor changes significantly for ID examples, making features for ID and OOD examples largely inconsistent. This can be fixed by initializing with a good head that does not need to be updated much during fine-tuning, reducing how much the feature extractor changes. This suggests a simple two-step strategy of first linear-probing to find a good head and then full fine-tuning (LP-FT). Empirically, LP-FT outperforms fine-tuning and linear-probing, both ID and OOD. Even on CIFAR-10.1 (small distribution shift), where fine-tuning is better for both ID and OOD, we find LP-FT outperforms fine-tuning on both metrics. LP-FT and vanilla fine-tuning use similar amounts of compute because the first step of linear probing is relatively very cheap. Prior work has used LP-FT (Levine et al., 2016; Kanavati \&amp; Tsuneki, 2021) (or variants such as layerwise fine-tuning (Howard \&amp; Ruder, 2018) or larger learning rates for the head layer (Prabhu et al., 2021))—however it has not been used for robustness / OOD accuracy, and we show that it addresses the ID-OOD tradeoff theoretically and empirically. Note that LP-FT is not meant to be a SOTA method but a simple, principled way to get good ID and OOD accuracy-we hope our analysis inspires even better methods for robust fine-tuning.</p>
<p>Empirical validation. Finally, we check whether fine-tuning underperforms and LP-FT works, for the reasons predicted by our feature distortion theory. As predicted by the theory, we find that: (1) fine-tuning indeed never matches the OOD accuracy of linear probing throughout the course of training (if the pretrained features are good, and OOD shift is large); (2) fine-tuning changes the features for ID examples more than for OOD examples, leading to distortions; (3) LP-FT indeed changes both ID and OOD features $10 \times-100 \times$ less than fine-tuning does; (4) fine-tuning can do better than linear probing OOD if the pretrained features are not very high quality (MoCo-v1 instead of MoCo-v2) or the ID and OOD datasets are very close (e.g., CIFAR-10 and CIFAR-10.1); and (5) LP-FT gets the best of both worlds, better accuracies than fine-tuning and linear probing, both ID and OOD (Figure 1).</p>
<h1>2 Setup</h1>
<p>Task and evaluation. Given training examples sampled from some distribution $P_{\mathrm{id}}$, our goal is to learn a predictor $f: \mathbb{R}^{d} \rightarrow \mathcal{Y}$ to map inputs $x \in \mathbb{R}^{d}$ to outputs $y \in \mathcal{Y}$. We evaluate predictors on their standard "in-distribution" (ID) performance $L_{\mathrm{id}}$ on new test samples drawn from $P_{\mathrm{id}}$ that the training data is also</p>
<p>sampled from. We also evaluate classifiers on their "out-of-distribution" (OOD) performance $L_{\text {ood }}$ on test samples drawn from a new distribution $P_{\text {ood }}$ that is different from $P_{\text {id }}$. Formally, for some loss function $\ell$, we evaluate classifiers on:</p>
<p>$$
L_{\mathrm{id}}(f)=\underset{(x, y) \sim P_{\mathrm{id}}}{\mathbb{E}}[\ell(f(x), y)] \text { and } L_{\text {ood }}(f)=\underset{(x, y) \sim P_{\text {ood }}}{\mathbb{E}}[\ell(f(x), y)]
$$</p>
<p>Models. In this work, we focus on predictors that leverage pretrained representations. We parameterize the final predictor $f$ as follows: given features $g_{B}(x) \in \mathbb{R}^{k}$ for some feature extractor parameters $B \in \mathcal{B}$, and a linear "head" $v \in \mathcal{V}$, we have $f_{v, B}(x)=v^{\top} g_{B}(x)$. In our experiments (Section 4), $g_{B}$ is a deep network and in our theory (Section 3), $g_{B}$ is a linear projection.</p>
<p>We assume access to some initial pretrained feature extractor $B_{0}$ that is obtained by training on potentially large amounts of data from a distribution that contains unlabeled or weakly supervised $x$ inputs from $P_{\text {id }}$ and $P_{\text {ood }}$. We focus on two popular methods to learn a predictor $f_{v, B}$ given training data from $P_{\text {id }}$ : (i) linear probing where $B=B_{0}$ and the linear head is obtained by minimizing some loss (e.g., logistic loss for classification, squared loss for regression) on the training data, and (ii) fine-tuning where both $v$ and $B$ are updated by performing gradient descent on some loss on the training data with $B$ initialized at $B_{0}$.</p>
<h1>3 Theory: fine-tuning distorts pretrained features</h1>
<p>Our goal is to understand under what conditions fine-tuning does worse than linear probing out-of-distribution (OOD). ${ }^{2}$ We consider a linear setting (feature extractor $g_{B}$ is linear) where the pretrained features are "good" and the OOD shift is large (Section 3.1). We prove our main result: that fine-tuning, in which all model parameters are updated, distorts features and gets suboptimal OOD error (Section 3.2, Theorem 3.3). We use this result to show that linear probing gets better OOD error but worse ID error than fine-tuning (Section 3.3). Finally, we explain why linear probing then fine-tuning can mitigate this ID-OOD tradeoff (Section 3.4).</p>
<p>Our analysis handles two key challenges which distinguishes it from prior work on transfer learning in linear models (Wu et al., 2020; Tripuraneni et al., 2020; Du et al., 2020; Xie et al., 2021a). Prior work focuses on linear probing, while we study fine-tuning where the resulting optimization problem is non-convex. We also study overparameterized models where the training loss alone does not determine test performance-this captures the fact that both training neural networks from scratch and fine-tuning them have the same training loss but very different test performance. However, it also makes the analysis challenging because we need to reason about the trajectory of gradient methods starting from a pretrained initialization, which has no known closed form.</p>
<h3>3.1 Linear overparameterized setting</h3>
<p>For our analysis, we focus on regression, where $\mathcal{Y}=\mathbb{R}$ and $\ell(\widehat{y}, y)=(\widehat{y}-y)^{2}$ is the squared loss.
Models. Recall from Section 2 that we parameterize predictors in terms of feature extractor and head parameters. In this section, we study models where the feature extractor is linear, i.e. $f_{v, B}(x)=v^{\top} B x$ where $B \in \mathcal{B}=\mathbb{R}^{k \times d}$, and $v \in \mathcal{V}=\mathbb{R}^{k}$.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Good pretrained features. For simplicity, we assume the models are well-specified i.e. $y=v_{\star}^{\top} B_{\star} x$ where $v_{\star} \in \mathbb{R}^{k}$ and $B_{\star} \in \mathbb{R}^{k \times d} .{ }^{3}$ Note that $B_{\star}$ and $v_{\star}$ are only unique up to rotations, i.e., for any rotation matrix $U,\left(U v_{\star}\right)^{T}\left(U B_{\star}\right) x=v_{\star}^{T} B_{\star} x$. As in prior work (Tripuraneni et al., 2020) suppose $B_{\star}, B_{0}$ have been orthogonalized to have orthonormal rows. Suppose we have a pretrained feature extractor $B_{0}$ close to $B_{\star}$, so $d\left(B_{0}, B_{\star}\right) \leq \epsilon$ where the distance $d$ is defined below:</p>
<p>Definition 3.1 (Feature Extractor Distance). The distance between feature extractors $B, B^{\prime} \in \mathbb{R}^{k \times d}$ (with orthonormal rows) is given by (where the min is over rotation matrices $U \in \mathbb{R}^{k \times k}$ ):</p>
<p>$$
d\left(B, B^{\prime}\right)=\min <em 2="2">{U}\left|B-U B^{\prime}\right|</em>
$$</p>
<p>Pretraining coverage intuition: Intuitively, the existence of $B_{\star}$ corresponds to assuming that there exists a shared set of useful features for ID $\left(P_{\mathrm{id}}\right)$ and $\mathrm{OOD}\left(P_{\text {ood }}\right)$. We also assume that $B_{0}$ is close to $B_{\star}$-one way this can happen is if pretraining is done on large scale data and has seen unlabeled or weakly supervised $x$ inputs that cover the support of $P_{\mathrm{id}}$ and $P_{\text {ood }}$. Formally, the task diversity assumption in Tripuraneni et al. (2020) is sufficient (but not necessary) for obtaining a good $B_{0}$. In our paper we show that even if we have these good features, fine-tuning can distort them and lead to low OOD accuracy.</p>
<p>Training data. Let $X \in \mathbb{R}^{n \times d}, X \neq 0$ be a matrix encoding $n$ training examples from $P_{\mathrm{id}}$ where each of the $n$ rows is a training input. Let $Y \in \mathbb{R}^{n}$ be the corresponding outputs. Let $S=\operatorname{rowspace}(X)$ be the $m$-dimensional subspace spanning the training examples. We consider an overparameterized setting where $1 \leq m&lt;d-k$. Intuitively, the input dimension $d$ is high (e.g., 10K), feature dimension $k$ is lower (e.g., 100) and $m$ is in the middle (e.g., 5 K$)^{4}$</p>
<p>Large OOD shift. We assume that the OOD data contains examples outside the span of the training data. Formally, let $P_{\text {ood }}$ have second moment $\Sigma=\mathbb{E}\left[x x^{\top}\right]$ where $x \sim P_{\text {ood }}$, and we assume $\Sigma$ is invertible. ${ }^{56}$</p>
<p>Training methods. Given training data and a pretrained feature extractor $B_{0}$, we study the two popular methods of linear probing (LP) and fine-tuning (FT) to learn the final predictor. Both methods involve optimizing the training loss via gradient descent (or variants). In order to effectively analyze these gradient based algorithms, we study vanishing step sizes leading to gradient flows. Gradient flows can be thought of as a continuous time analogue of gradient based methods and have been extensively studied in recent years as a way to understand gradient based methods (Gunasekar et al., 2017; Arora et al., 2018; Du et al., 2018).</p>
<p>Formally, for training loss $\widetilde{L}(v, B)=\left|X B^{\top} v-Y\right|_{2}^{2}$, the gradient flow differential equations for LP and FT</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: A toy version of our theory illustrating why fine-tuning distorts features, with inputs in 2D. Given input $x$, the ground truth output is $y=w_{\star}^{\top} x$. The ID data is along the $x$-axis and the pretrained feature extractor is $B_{0}$. (a) Linear probing learns $w_{\mathrm{lp}}$, a scaling of the pretrained feature extractor that gets the ID data correct ( $w_{\mathrm{lp}}$ and $w_{\star}$ have the same $x$ coordinate as indicated by the vertical dotted line). (b) Fine-tuning updates the pretrained feature extractor along the ID data (so horizontally) to get $B_{\mathrm{ft}}$, and then learns a scaling of these features that gets the ID data correct. While both methods get ID data correct, fine-tuning makes large errors perpendicular to the ID data, because fine-tuning updates $B_{0}$ along the ID direction but not the perpendicular direction (we call this feature "distortion").
are as follows:</p>
<p>$$
\begin{gathered}
\partial_{t} v_{\mathrm{ft}}(t)=-\nabla_{v} \widetilde{L}\left(v_{\mathrm{ft}}(t), B_{\mathrm{ft}}(t)\right), \partial_{t} B_{\mathrm{ft}}(t)=-\nabla_{B} \widetilde{L}\left(v_{\mathrm{ft}}(t), B_{\mathrm{ft}}(t)\right) \
\partial_{t} v_{\mathrm{lp}}(t)=-\nabla_{v} \widetilde{L}\left(v_{\mathrm{lp}}(t), B_{0}\right), \partial_{t} B_{\mathrm{lp}}(t)=0
\end{gathered}
$$</p>
<p>initialized with $B_{\mathrm{ft}}(0)=B_{\mathrm{lp}}(0)=B_{0}$ and $v_{\mathrm{ft}}(0)=v_{\mathrm{lp}}(0)=v_{0}$. In practice, the head parameter $v_{0}$ is initialized randomly—our results hold for any standard random initialization (Glorot \&amp; Bengio, 2010), for example $v_{0} \sim \mathcal{N}\left(0, \sigma^{2} I\right)$ for any $\sigma^{2}$, or zero initialization where $v_{0}=0$. Recall that the initial value of the feature extractor $B_{0}$ is obtained via pretraining.</p>
<p>The final LP and FT solutions are the limit points of the corresponding gradient flows:</p>
<p>$$
\begin{aligned}
&amp; v_{\mathrm{ft}}^{\infty}=\lim <em _mathrm_ft="\mathrm{ft">{t \rightarrow \infty} v</em>=\lim }}(t) \text { and } B_{\mathrm{ft}}^{\infty<em _mathrm_ft="\mathrm{ft">{t \rightarrow \infty} B</em>(t) \
&amp; v_{\mathrm{lp}}^{\infty}=\lim }<em _mathrm_lp="\mathrm{lp">{t \rightarrow \infty} v</em>=\lim }}(t) \text { and } B_{\mathrm{lp}}^{\infty<em _mathrm_lp="\mathrm{lp">{t \rightarrow \infty} B</em>
\end{aligned}
$$}}(t)=B_{0</p>
<h1>3.2 Fine-tuning distorts pretrained features</h1>
<p>The more common method of using a pretrained feature extractor is fine-tuning (FT) which typically improves ID performance relative to linear probing (LP). In this section, we show theoretically that FT can distort features leading to poor OOD performance. We first present the key intuitions demonstrating potential issues of FT and then present our formal theorem lower bounding the OOD error of FT (Section 3.2.2).</p>
<h3>3.2.1 Key intuitions</h3>
<p>There are two main observations that we use to characterize when and why FT has higher OOD error than linear probing.</p>
<ol>
<li>Features get distorted: representations change only in the ID subspace (i.e., subspace spanned by the training data) and are unchanged in the orthogonal subspace. To see this, we take the derivative of the training loss $\widehat{L}(v, B)=\left|X B^{\top} v-Y\right|_{2}^{2}$ with respect to the feature extractor parameter $B$ :</li>
</ol>
<p>$$
\nabla_{B} \widehat{L}(v, B)=2 v(Y-X B v)^{\top} X
$$</p>
<p>By definition, if $u$ is a direction orthogonal to the training subspace $S=\operatorname{rowspace}(X)$, then $\nabla_{B} \widehat{L}(v, B) u=$ 0 , that is the gradient updates to $B$ do not modify $B u$ for $u \in S^{\perp}$. However, the gradient is non-zero for directions $u$ in the ID subspace and the corresponding features $B u$ change across the fine-tuning process. We call this feature distortion: the features in some directions are changed but not others. Next, we explain why this can lead to high OOD error.
2. Distorted features can lead to higher OOD error. Consider a toy example (Figure 2) where $d=2$ and the dimensionality of the representations $k=1$. The linear head $v$ is a scalar quantity that denotes how much the feature extractor $B$ has to be scaled by. Suppose the ID-subspace is the $x$-axis. There are different ways of fitting the ID subspace depending on the feature extractors $B$ as shown in the Figure-both fine-tuned and linear probed estimators match the true parameter in the ID subspace (since $w_{\mathrm{lp}}, w_{\mathrm{ft}}, w_{\star}$ have the same projection on the $x$-axis). If the feature extractor were optimal or scaled versions of the optimal, good performance on the ID subspace would translate to good performance everywhere, even in directions orthogonal to the ID subspace. However, in FT, the features change only for inputs in the ID subspace (see (1)) and thus the updated features are not simply scaled but distorted. In Figure 2, this corresponds to the feature extractor $B_{0}$ changing along the $x$-axis. In this case even if the ID error is low, error in directions orthogonal to the ID subspace can be high, leading to high OOD error.</p>
<p>The only way the pretrained features are not distorted and only scaled during FT is if the initial feature extractor $B_{0}$ is exactly aligned with the ID subspace. In Figure 2, if $B_{0}$ is along the $x$-axis (the ID subspace), then updating the features exclusively along the $x$-axis would simply scale the initial features. In this case linear probing and fine-tuning will have identical behavior. If the angle between $B_{0}$ and the $x$-axis is non-zero-which occurs with probability 1 if the training data $X$ or pretrained feature extractor $B_{0}$ involves even a tiny amount of randomness e.g., from SGD in pretraining-the updates would lead to distortions. In high dimensions, we measure the alignment between $B_{0}$ and the ID subspace with the largest principal angle:</p>
<p>Definition 3.2 (largest principal angle). Let $A$ and $B$ be arbitrary subspaces, and $E$ and $F$ be matrices with orthonormal columns than span $A$ and $B$ respectively, with $r=\min (\operatorname{dim}(A), \operatorname{dim}(B))$. Then $\cos \theta_{\max }(A, B)=\sigma_{r}\left(E^{\top} F\right)$, which is the $r$-th largest singular value of $E^{\top} F$.</p>
<p>Note that $E, F$ are not unique in Definition 3.2, but $\sigma_{r}\left(E^{\top} F\right)$ is the same for every valid choice of $E$ and $F$. See Appendix A. 1 for more information on principal angles.</p>
<h1>3.2.2 General result on the OOD error of fine-tuning</h1>
<p>Our main theorem lower bounds the OOD error of fine-tuning outside the span of the training data. In Section 3.3 we compare this lower bound with an upper bound on the OOD error of linear probing.</p>
<p>Theorem 3.3. In the overparameterized linear setting, let $S^{\perp}=\operatorname{rowspace}(X)^{\perp}, R_{0}=\operatorname{rowspace}\left(B_{0}\right)$, and $v_{\star}, B_{\star}$ be the optimal parameters with $w_{\star}=B_{\star} v_{\star}$. If $\cos \theta_{\max }\left(R_{0}, S^{\perp}\right)&gt;0$, then for all time steps $t$, the</p>
<p>OOD error of the fine-tuning iterates $\left(B_{\mathrm{ft}}(t), v_{\mathrm{ft}}(t)\right)$ is lower bounded:</p>
<p>$$
\sqrt{L_{\mathrm{ood}}\left(v_{\mathrm{ft}}(t), B_{\mathrm{ft}}(t)\right)} \geq \sqrt{\sigma_{\min }(\Sigma)}\left(\frac{\cos \theta_{\max }\left(R_{0}, S^{\perp}\right)}{\sqrt{k}} \frac{\min \left(\varphi, \varphi^{2} /\left|w_{\star}\right|<em _star="\star">{2}\right)}{\left(1+\left|w</em>-\epsilon\right)
$$}\right|_{2}\right)^{2}</p>
<p>where $\varphi^{2}=\left|\left(v_{0}^{\top} v_{\star}\right)^{2}-\left(v_{\star}^{\top} v_{\star}\right)^{2}\right|$ is defined to be inital head alignment error and $\epsilon \geq d\left(B_{0}, B_{\star}\right)$ is the error in the pretrained feature extractor.</p>
<p>Proof sketch. Since the features do not change for examples in $S^{\perp}$ (perpendicular to the training data), we show that in order to achieve low error on $S^{\perp}$ the linear head $v_{\mathrm{ft}}(t)$ would have to become very similar to the optimal $v_{\star}$ at some time $t$. The head initialization $v_{0}$ is random (or zero) and likely to be far from $v_{\star}$ (measured by the alignment error $\varphi$ ), so the head would have to change a lot to get close to $v_{\star}$. As we see from the fine-tuning gradient flow (3.2), $v_{\mathrm{ft}}(t)$ and $B_{\mathrm{ft}}(t)$ change in a "coupled" manner, and a "'balancedness" invariant in Du et al. (2018) holds across the fine-tuning trajectory. Correspondingly, if $v_{\mathrm{ft}}(t)$ changes a lot and gets close to $v_{\star}$, the features $B_{\mathrm{ft}}(t)$ also change a lot for examples in $S$-we show that this would lead to high error on examples in $S$. Either way, fine-tuning would get some subspace ( $S$ or $S^{\perp}$ ) of examples wrong, leading to high OOD error. The full proof appears in Appendix A.</p>
<p>Interpretations of various quantities. Quality of pretrained features ( $\epsilon$ ). To unpack the bound consider a special case where the pretrained features are perfect $(\epsilon=0)$. With perfect features, Proposition A. 20 shows that linear probing gets zero OOD error. Theorem 3.3 shows that $L_{\text {ood }}\left(v_{\mathrm{ft}}(t), B_{\mathrm{ft}}(t)\right)&gt;0$ at all times $t$-so fine-tuning underperforms when the features are perfect. The $\epsilon&gt;0$ case just captures the fact that even if the features are not perfect, fine-tuning can still get positive error. Ideally we would like the lower bound to increase if we have worse features (so " $+\epsilon$ " instead of " $-\epsilon$ " in the bound)-the reason we do not is that the errors of the pretrained feature extractor $d\left(B_{0}, B_{\star}\right)$ and the fine-tuning step can potentially cancel out. ${ }^{7}$</p>
<p>Alignment error of random head initialization $\left(\varphi^{2}\right)$. The lower bound (Equation A.14) increases as $\varphi^{2}$ increases i.e. alignment error increases because the gradient updates to the head and feature extractor are coupled. If the head were somehow initialized perfectly at $v_{\star}$, then fine-tuning updates may not increase the OOD error. However, when the head is randomly initialized (or initialized to zero) as is standard in fine-tuning, the alignment error is high, leading to high OOD error. We use this insight in Section 3.4 to show that better head initialization (namely via linear probing) improves OOD performance of fine-tuning.</p>
<p>Span of Training data ( $S$ ). Theorem 3.3 lower bounds the error outside the span of the training data. If the training dataset is very small, then even the support of the ID distribution $P_{\mathrm{id}}$ may not be spanned by the training data, and the ID error can be large. Indeed, even in the ID setting Kornblith et al. (2019) show that linear probing can do better than fine-tuning if we have very few training examples, but fine-tuning does better on all 11 of their datasets once we have more than just 30 examples per class.</p>
<p>Conjectures for improved bounds. We believe it may be possible to improve $\cos \theta_{\max }\left(R_{0}, S^{\perp}\right)$ to the cosine of the minimum principal angle. ${ }^{8}$ This may look like a technicality but would be a substantial improvement, because it would imply that fine-tuning has error in every direction outside the training span, whereas we show that it would have errors in some directions. Our proof strategy requires the maximum principal angle (a crucial step is a variational characterization of the maximal principal angle in Lemma A.2-we use this in Step 1 of the proof in Appendix A to show that to get low OOD error $v_{\mathrm{ft}}(t)$ must become similar to $v_{\star}$ ).</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>3.3 Linear probing vs. fine-tuning</h1>
<p>In this section, we use our main theorem on fine-tuning (Theorem 3.3) and adapt prior work on linear probing to show that linear probing is better than fine-tuning OOD, but worse ID, when the ID distribution has density on a lower $m&lt;d$ dimensional subspace $S$, and $B_{0}$ is close to $B_{*}$ (so we have "good" pretrained features).</p>
<p>Assumption 3.4 (ID subspace assumption). We assume that the ID data lies on an m-dimensional subspace $S$ where $k&lt;m&lt;d-k$, and we have $n \geq m$ training examples. Formally, let $P_{z}$ be a distribution on $\mathbb{R}^{m}$ which has density, and let the columns of $F \in \mathbb{R}^{d \times m}$ form an orthonormal basis for $S$. Then $P_{\mathrm{id}}$ has the distribution of $F z$ where $z \sim P_{z}$.</p>
<p>Recall that the ID error is the expected mean-squared error over the ID distribution $P_{\mathrm{id}}$ :</p>
<p>$$
L_{\mathrm{id}}(v, B)=\underset{x \sim P_{\mathrm{id}}}{\mathbb{E}}\left[\left(v_{<em>}^{\top} B_{</em>} x-v^{\top} B x\right)^{2}\right]
$$</p>
<p>OOD comparison: Under mild non-degeneracy conditions, we show that as the feature extractor error $\epsilon$ goes to 0 , linear probing does much better than fine-tuning OOD: the ratio of the losses goes to 0 . The nondegeneracy conditions are similar to Section 3.2-we require that the training data cannot be exactly in the same direction or orthogonal to the pretrained features, formally that $\cos \theta_{\max }\left(R_{<em>}, S\right)$ and $\cos \theta_{\max }\left(R_{</em>}, S^{\perp}\right)$ are not 0 where $R_{<em>}=$ rowspace $\left(B_{</em>}\right)$. In the toy example in Figure 2, this means that $x_{\mathrm{id}}$ cannot be exactly in the same direction or orthogonal to $B_{0}^{\top}$-in these cases fine-tuning and linear probing get the same loss but in all other cases in the toy example in Figure 2 linear probing does better OOD.</p>
<p>Theorem 3.5 (Informal version of Theorem A.8). In the linear overparameterized setting, under the ID subspace assumption (Assumption 3.4), if $\cos \theta_{\max }\left(R_{<em>}, S\right) \neq 0$ and $\cos \theta_{\max }\left(R_{</em>}, S^{\perp}\right) \neq 0$ where $R_{<em>}=$ rowspace $\left(B_{</em>}\right)$, then,</p>
<p>$$
\frac{L_{\mathrm{ood}}\left(v_{\mathrm{ip}}^{\infty}, B_{0}\right)}{L_{\mathrm{ood}}\left(v_{\mathrm{ft}}(t), B_{\mathrm{ft}}(t)\right)} \xrightarrow{p} 0, \text { as } B_{0} \rightarrow B_{*}
$$</p>
<p>This holds for all times $t$ for $F T$ (and therefore also for the limit $v_{\mathrm{ft}}^{\infty}, B_{\mathrm{ft}}^{\infty}$ ) and the LP iterates converge to $v_{\mathrm{ip}}^{\infty}, B_{0}$ as a result of the gradient flow on a convex problem.</p>
<p>Intuitively, if the pretrained features are good, LP learns a near optimal linear head which has small OOD error (Lemma A.14) but fine-tuning has high OOD error (Theorem 3.3). We give a more formal version of Theorem 3.5 and a proof in Appendix A.3. If $P_{z}$ is isotropic Gaussian, we can get a better result: Theorem A. 15 derives a threshold $T$ (in terms of $d, n, k$ ) where LP does better than FT if $\epsilon&lt;T$, instead of just the asymptotic result $\left(B_{0} \rightarrow B_{<em>}\right)$. Theorem 3.5 requires that $\cos \theta_{\max }\left(R_{</em>}, S\right) \neq 0$ and $\cos \theta_{\max }\left(R_{*}, S^{\perp}\right) \neq 0$-intuitively, for any subspace a small perturbation would make these angles nonzero and the assumption would hold. To illustrate that these assumptions typically hold, Lemma A. 16 in Appendix A proves that if $S$ is a random $m$-dimensional subspace then these angles are non-zero almost surely.</p>
<p>ID comparison: When the pretrained features have some error, we show that fine-tuning does better than linear probing ID because fine-tuning can update the features to fit the ID data.</p>
<p>If the pretrained features are perfect so that the optimal predictor can be written as a linear combination of the pretrained features $\left(w_{<em>}=B_{</em>}^{\top} v_{*} \in\right.$ rowspace $\left.\left(B_{0}\right)\right)$, then both linear probing and fine-tuning get zero ID error. However, if the pretrained representation has some error, and the training data satisfies a mild non-degeneracy</p>
<p>condition, then LP has high ID error because there is no linear head on $B_{0}$ that fits the training data perfectly. FT, on the other hand, can update the features to find a new $B_{\mathrm{ft}}^{\infty}$ that can fit the training data perfectly with a linear head $v_{\mathrm{ft}}^{\infty}$.</p>
<p>The non-degeneracy condition is similar to our previous results, and holds with probability 1 if the ID subspace is chosen randomly, from Lemma A.16. Formally, let $R_{\text {aug }}$ be a $k+1$ dimensional subspace spanning $R_{0} \cup\left{w_{\star}\right}$, where we recall that $R_{0}=\operatorname{rowspace}\left(B_{0}\right)$. Then we just require that the ID subspace $S$ and $R_{\text {aug }}$ are not orthogonal: $\cos \theta_{\max }\left(S, R_{\text {aug }}\right) \neq 0$. We state the formal proposition below and give a proof in Appendix A.</p>
<p>Proposition 3.6. In the linear overparameterized setting, under the ID subspace assumption (Assumption 3.4), let $R_{0}=\operatorname{rowspace}\left(B_{0}\right)$, and $R_{\text {aug }}=\operatorname{Span}\left(\left{w_{\star}\right} \cup R_{0}\right)$. Suppose $w_{\star} \notin R_{0}, \cos \theta_{\max }\left(S, R_{\text {aug }}\right) \neq 0$, and that fine-tuning converges to a local minimum of its loss, then fine-tuning does better ID almost surely: $L_{\mathrm{id}}\left(v_{\mathrm{ft}}^{\infty}, B_{\mathrm{ft}}^{\infty}\right)&lt;L_{\mathrm{id}}\left(v_{\mathrm{ip}}^{\infty}, B_{0}\right)$ with probability 1 (over the randomness of the training examples).</p>
<p>To summarize, we proved that there are tradeoffs between ID and OOD error: FT has lower ID error but higher OOD error than LP. In the next section, we extend our theoretical insights to illustrate why a simple variant of FT may mitigate such tradeoffs.</p>
<h1>3.4 Linear probing then fine-tuning: a simple variant to mitigate tradeoffs</h1>
<p>The advantage of fine-tuning is it can adapt both the feature extractor and head to fit the downstream task. Can we keep this benefit while ensuring that our OOD error is low when we have good pretrained features?</p>
<p>Going back to Theorem 3.3, we see that the alignment error in the head initialization $\varphi^{2}=\left(v_{0}^{\top} v_{\star}\right)^{2}-\left(v_{\star}^{\top} v_{\star}\right)^{2}$ plays an important role. The issue with FT was that under random or zero initialization, $\varphi^{2}$ is usually large and since the gradient updates to the feature extractor parameter are coupled with that of the head parameter, the features get distorted in a manner that increases the OOD error. This suggests that we should use a better head initialization-one obtained from linear probing. If the pretrained features are decent, a linear probed head would be much better aligned with $v_{\star}$ allowing the features to be updated in a manner that does not increase the OOD error much.</p>
<p>We formally prove this intuition in a simple setting where we have perfect pretrained features. Of course, if we have perfect pretrained features, linear probing alone gets zero OOD error-so Proposition 3.7 is just a first cut result to illustrate that if initialized well, full fine-tuning does not distort features.</p>
<p>Proposition 3.7. Suppose we have perfect pretrained features $B_{0}=U B_{\star}$ for some rotation $U$. Let $R_{0}=$ $\operatorname{rowspace}\left(B_{0}\right)$. Under the non-degeneracy conditions $\cos \theta_{\max }\left(R_{0}, S\right) \neq 0, \cos \theta_{\max }\left(R_{0}, S^{\perp}\right) \neq 0$ :</p>
<p>$$
\begin{aligned}
&amp; \forall t, L_{\text {ood }}\left(B_{\mathrm{ft}}(t)^{\top} v_{\mathrm{ft}}(t)\right)&gt;0, \text { if } v_{0} \sim \mathcal{N}\left(0, \sigma^{2} I\right) \text { is randomly initialized }(F T) \
&amp; \forall t, L_{\text {ood }}\left(B_{\mathrm{ft}}(t)^{\top} v_{\mathrm{ft}}(t)\right)=0, \text { if } v_{0} \text { is initialized to } v_{\mathrm{ip}}^{\infty}(L P-F T)
\end{aligned}
$$</p>
<p>The case where we do not have perfect features $\left(d\left(B_{0}, B_{\star}\right)&gt;0\right)$ is challenging to analyze because except in very special cases, there is no closed form for the fine-tuning iterates $\left(v_{\mathrm{ft}}(t), B_{\mathrm{ft}}(t)\right)$. Our proof of Theorem 3.3 leveraged invariants to show a lower bound on the error of fine-tuning when $v_{0}$ and $v_{\star}$ are different, but we were not able to show an upper bound.</p>
<h1>4 Experiments</h1>
<p>We run experiments on ten distribution shifts to see if our theoretical predictions on the relative performance of linear probing (LP), fine-tuning (FT), and LP-FT, generalize to deep neural networks on real datasets. As expected, given good pretrained features, fine-tuning (FT) does better ID but worse on large OOD shifts than linear probing (LP). In particular, ID and OOD accuracy are not correlated, unlike Recht et al. (2018) but like Xie et al. (2021a). As predicted by the theory, we find that LP-FT does better than both methods ID and OOD and gets around this tradeoff. Our theory also predicts that the reason for these trends is that fine-tuning distorts features, and we see that this distortion indeed happens in practice. For more details on datasets, pretraining models, and experiment protocols, see Appendix B. The datasets we use are:</p>
<ul>
<li>DomainNet (Peng et al., 2019) is a standard domain adaptation dataset. Here, our ID dataset contains "sketch" images (e.g., drawings of apples, elephants, etc), and the OOD dataset contains "real", "clipart", and "painting" images of the same categories. We use the version of the dataset from Tan et al. (2020).</li>
<li>Living-17 and Entity-30 are sub-population shift datasets from the BREEDS benchmark (Santurkar et al., 2020). In Living-17 the goal is to classify an image as one of 17 animal categories such as "bear"-for example, the ID dataset contains images of black bears and sloth bears and the OOD dataset has images of brown bears and polar bears. In Entity-30 the goal is to classify an image as one of 30 entities such as "fruit" or "insect".</li>
<li>FMoW Geo-shift is adapted from the satellite remote sensing dataset Functional Map of the World (Christie et al., 2018; Koh et al., 2021). The goal is to classify a satellite image into one of 62 categories such as "impoverished settlement" or "hospital". Our ID dataset contains images from North America, and the OOD dataset contains images from Africa and Europe.</li>
<li>CIFAR-10 $\rightarrow$ STL is a standard domain adaptation dataset (French et al., 2018), where the ID is CIFAR10 (Krizhevsky, 2009), and the OOD is STL (Coates et al., 2011). The task is to classify an image into one of 10 categories such as "dog", "cat", or "airplane"-as usual, we remove the "monkey" class in STL since CIFAR-10 has no "monkey" images.</li>
<li>CIFAR-10 $\rightarrow$ CIFAR-10.1 (Recht et al., 2018) is a dataset collected using a very similar protocol to CIFAR-10, and the authors describe it as "a minute distributional shift". The hope is that a classifier trained on CIFAR-10 gets high accuracy on CIFAR-10.1.</li>
<li>ImageNet-1K (Russakovsky et al., 2015) is a large scale dataset containing over a million images, where the goal is to classify an image into one of 1000 categories such as "Yorkshire terrier", "Labrador retriever", "acoustic guitar", "library", "school bus", etc. We fine-tune on ImageNet as the ID dataset, and evaluate on four standard OOD datasets: ImageNetV2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2020), ImageNet-A (Hendrycks et al., 2019b), and ImageNet-Sketch (Wang et al., 2019).</li>
</ul>
<p>Pretraining and models. We use a CLIP pretrained ViT-B/16 for ImageNet. For the other datasets we use a ResNet-50 architecture and consider a diverse range of pretraining methods and datasets: MoCo-v2 (Chen et al., 2020b), CLIP (Radford et al., 2021), and MoCo-TP (Ayush et al., 2020). In Appendix B, we also show results for a CLIP-ViT-B/16 and more fine-tuning baselines on Living-17.</p>
<p>Table 1: ID accuracies with $90 \%$ confidence intervals over 3 runs-fine-tuning does better than linear probing on all datasets except DomainNet (which could be because the version of the DomainNet training dataset from Tan et al. (2020) is fairly small, with around 20K examples). LP-FT does the best on all except FMoW where it is in between linear probing and fine-tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">CIFAR-10</th>
<th style="text-align: center;">Ent-30</th>
<th style="text-align: center;">Liv-17</th>
<th style="text-align: center;">DomainNet</th>
<th style="text-align: center;">FMoW</th>
<th style="text-align: center;">ImageNet</th>
<th style="text-align: center;">Average</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">FT</td>
<td style="text-align: center;">$\mathbf{9 7 . 3 ( 0 . 2 )}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 6 ( 0 . 2 )}$</td>
<td style="text-align: center;">$97.1(0.2)$</td>
<td style="text-align: center;">$84.5(0.6)$</td>
<td style="text-align: center;">$\mathbf{5 6 . 5 ( 0 . 3 )}$</td>
<td style="text-align: center;">$\mathbf{8 1 . 7}$ (-)</td>
<td style="text-align: center;">85.1</td>
</tr>
<tr>
<td style="text-align: center;">LP</td>
<td style="text-align: center;">$91.8(0.0)$</td>
<td style="text-align: center;">$90.6(0.2)$</td>
<td style="text-align: center;">$96.5(0.2)$</td>
<td style="text-align: center;">$89.4(0.1)$</td>
<td style="text-align: center;">$49.1(0.0)$</td>
<td style="text-align: center;">$79.7(-)$</td>
<td style="text-align: center;">82.9</td>
</tr>
<tr>
<td style="text-align: center;">LP-FT</td>
<td style="text-align: center;">$\mathbf{9 7 . 5 ( 0 . 1 )}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 7 ( 0 . 1 )}$</td>
<td style="text-align: center;">$\mathbf{9 7 . 8 ( 0 . 2 )}$</td>
<td style="text-align: center;">$\mathbf{9 1 . 6 ( 0 . 0 )}$</td>
<td style="text-align: center;">$51.8(0.2)$</td>
<td style="text-align: center;">$\mathbf{8 1 . 7}$ (-)</td>
<td style="text-align: center;">$\mathbf{8 5 . 7}$</td>
</tr>
</tbody>
</table>
<h1>4.1 Linear probing vs. fine-tuning</h1>
<p>Experiment protocols. We initialize with the pretrained model, and fine-tune or linear probe on ID training examples. For fine-tuning on each dataset we swept over 6 learning rates, using a cosine learning rate schedule and batch size of 64 . We early stop and choose the best learning rate using ID validation accuracy. For linear probing we train an $\ell_{2}$-regularized logistic regression classifier on frozen features from the penultimate layer of the pretrained model, selecting the best $\ell_{2}$-regularization hyperparameter based on ID validation accuracy. For all methods, we run each hyperparameter configuration 3 times (with different random seeds), and take the average accuracy. We used a slightly different protocol for ImageNet because the dataset is much larger and running these experiments involves more computational resources: we used a batch size of 128, swept over 3 learning rates for both fine-tuning and linear probing (we did not sweep over $\ell_{2}$-regularization), and ran each hyperparameter configuration once. In all cases, OOD data was only used for evaluation.</p>
<p>Results. Fine-tuning does better than linear probing on 5 out of 6 ID datasets (average accuracy of $85.1 \%$ for fine-tuning vs. $82.9 \%$ for linear probing, see Table 1). This is consistent with prior work and intuitions. However, linear-probing does better on 8 out of 10 OOD datasets (average accuracy of $66.2 \%$ for linear probing vs. $59.3 \%$ for fine-tuning, see Table 2)—linear probing does better on all datasets except CIFAR-10.1 and ImageNetV2, where the OOD is designed to closely replicate the ID dataset. This matches our theoretical predictions, which says that linear probing does better than fine-tuning when the ID and OOD are very different (and the pretrained features are "good"). Our training datasets vary in size from 20K examples to over a million examples, so linear probing does not appear to perform better than fine-tuning simply because of a small training set.</p>
<h3>4.2 Linear probing then fine-tuning (LP-FT)</h3>
<p>Experiment protocols. For LP-FT, we initialize the neural network head using the linear probed solution, and then fine-tune the model. LP-FT and fine-tuning use similar compute because the linear probing step is much faster than fine-tuning. As with fine-tuning, we swept over 6 learning rates, early stopping using ID validation accuracy. For the ImageNet experiments we swept over 3 learning rates, and explicitly ensured that LP-FT and fine-tuning use exactly the same compute (we ran each stage of LP-FT for half as many epochs as we ran vanilla fine-tuning).</p>
<p>Results. We find that LP-FT gets the best accuracy ID (average: $85.7 \%$ ) and OOD (average: $68.9 \%$ ). This is true for 5/6 ID and 10/10 OOD datasets-every dataset except FMoW ID, where LP-FT is better than linear probing but worse than fine-tuning. Since the ID accuracy on FMoW is low (56.5\%), this could be because</p>
<p>Table 2: OOD accuracies with $90 \%$ confidence intervals over 3 runs. Linear probing does better than fine-tuning on all datasets except CIFAR-10.1 and ImageNetV2, where the ID and OOD are very similar (this is consistent with our theory). LP-FT matches or exceeds fine-tuning and linear probing on all 10 datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">STL</th>
<th style="text-align: center;">CIFAR-10.1</th>
<th style="text-align: center;">Ent-30</th>
<th style="text-align: center;">Liv-17</th>
<th style="text-align: center;">DomainNet</th>
<th style="text-align: center;">FMoW</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">FT</td>
<td style="text-align: center;">$82.4(0.4)$</td>
<td style="text-align: center;">$92.3(0.4)$</td>
<td style="text-align: center;">$60.7(0.2)$</td>
<td style="text-align: center;">$77.8(0.7)$</td>
<td style="text-align: center;">$55.5(2.2)$</td>
<td style="text-align: center;">$32.0(3.5)$</td>
</tr>
<tr>
<td style="text-align: center;">LP</td>
<td style="text-align: center;">$85.1(0.2)$</td>
<td style="text-align: center;">$82.7(0.2)$</td>
<td style="text-align: center;">$\mathbf{6 3 . 2 ( 1 . 3 )}$</td>
<td style="text-align: center;">$82.2(0.2)$</td>
<td style="text-align: center;">$79.7(0.6)$</td>
<td style="text-align: center;">$\mathbf{3 6 . 6 ( 0 . 0 )}$</td>
</tr>
<tr>
<td style="text-align: center;">LP-FT</td>
<td style="text-align: center;">$\mathbf{9 0 . 7 ( 0 . 3 )}$</td>
<td style="text-align: center;">$\mathbf{9 3 . 5 ( 0 . 1 )}$</td>
<td style="text-align: center;">$\mathbf{6 2 . 3 ( 0 . 9 )}$</td>
<td style="text-align: center;">$\mathbf{8 2 . 6 ( 0 . 3 )}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 7 ( 0 . 9 )}$</td>
<td style="text-align: center;">$\mathbf{3 6 . 8 ( 1 . 3 )}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ImNetV2</td>
<td style="text-align: center;">ImNet-R</td>
<td style="text-align: center;">ImNet-Sk</td>
<td style="text-align: center;">ImNet-A</td>
<td style="text-align: center;">Average</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">FT</td>
<td style="text-align: center;">$\mathbf{7 1 . 5 ( - )}$</td>
<td style="text-align: center;">$52.4(-)$</td>
<td style="text-align: center;">$40.5(-)$</td>
<td style="text-align: center;">$27.8(-)$</td>
<td style="text-align: center;">59.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LP</td>
<td style="text-align: center;">$69.7(-)$</td>
<td style="text-align: center;">$70.6(-)$</td>
<td style="text-align: center;">$46.4(-)$</td>
<td style="text-align: center;">$45.7(-)$</td>
<td style="text-align: center;">66.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LP-FT</td>
<td style="text-align: center;">$\mathbf{7 1 . 6 ( - )}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 9 ( - )}$</td>
<td style="text-align: center;">$\mathbf{4 8 . 4 ( - )}$</td>
<td style="text-align: center;">$\mathbf{4 9 . 1 ( - )}$</td>
<td style="text-align: center;">$\mathbf{6 8 . 9}$</td>
</tr>
</tbody>
</table>
<p>the pretrained features are not good.</p>
<h1>4.3 Examining the feature distortion theory</h1>
<p>Early stopping does not mitigate feature distortion. One might think that fine-tuning is simply overfitting ID, and so early stopping on OOD data (if it were available) might match linear probing OOD. However, our theory predicts that fine-tuning can do worse OOD (than linear probing) throughout the process of fine-tuning, and not just at the end. To test this, we early stop each fine-tuning method and choose the best learning rate based on OOD test accuracy (OOD data was not used except for this ablation). As expected, fine-tuning does improve a little, but linear probing (average accuracy: $67.1 \%$ ) is still better than fine-tuning (average accuracy: $61.3 \%$ ). See Appendix B for per-dataset results.</p>
<p>ID-OOD features get distorted from fine-tuning. The feature distortion theory predicts that fine-tuning changes features for ID examples more than for OOD examples, which is why fitting a head on ID examples performs poorly OOD. To test this, for each example $x$ in Living-17 (results for other datasets are in Appendix B), we took the Euclidean distance of the ResNet-50 features before and after fine-tuning: $|g_{B}(x)-$ $\left.g_{B_{0}}(x)\right|_{2}$. As expected, the average distance for ID examples $(0.0188 \pm 0.0001)$ is more than for OOD examples $(0.0167 \pm 0.0001)$. The theory also predicts that LP-FT changes features less than fine-tuning does. As expected, the average distance changed by LP-FT both ID $(0.0011 \pm 0.0001)$ and OOD $(0.0009 \pm 0.0001)$ is $20 \times$ smaller than for fine-tuning.</p>
<p>Pretrained features must be good, ID-OOD far apart. Our theory gives conditions under which linear probing can do better than fine-tuning OOD. Specifically, we require that the ID distribution $P_{\mathrm{id}}$ and OOD distribution $P_{\text {ood }}$ are quite different, and the pretrained features are good ( $B_{0}$ is close to $B_{\star}$ )-otherwise fine-tuning can do better OOD by adjusting the feature extractor ID. Here we test that these conditions are essential—when they are violated fine-tuning can do better than linear probing OOD.</p>
<p>Feature quality: We use a checkpoint of MoCo-v1 that got $10 \%$ worse accuracy (on ImageNet) and compare linear probing and fine-tuning on Living-17. With worse features, both methods do worse, but fine-tuning ( $96 \%$ ID, $71 \%$ OOD) does better than linear probing ( $92 \%$ ID, $66 \%$ OOD).</p>
<p>$I D \approx O O D$ : We fine-tune / linear probe on CIFAR-10, and test on CIFAR-10.1, a dataset collected using a similar protocol to CIFAR-10. As expected, fine-tuning ( $92.3 \%$ ) outperforms linear probing OOD ( $82.7 \%$ ). Even in this case, where we have no tradeoffs, LP-FT does the best ( $93.5 \%$ ).</p>
<h1>5 Related work and discussion</h1>
<p>Fine-tuning vs. linear probing. Fine-tuning (FT) and linear probing (LP) are popular transfer learning algorithms. There is substantial evidence of FT outperforming LP in-distribution (ID) including recent large-scale investigations (Kornblith et al., 2019; Chen et al., 2021a; Zhai et al., 2020; Chen et al., 2020b) (the only notable exception is in Peters et al. (2019) where LP performs better than FT when using ELMo representations, but worse using BERT). ${ }^{9}$ FT is therefore the method of choice for improving accuracy, while LP is used to analyze properties of representations (Peters et al., 2018; Belinkov et al., 2017; Hewitt \&amp; Manning, 2019). In our work, we find that FT can underperform LP especially when using high quality pretrained features in the presence of a large distribution shift. There are a variety of other fine-tuning heuristics (Ge \&amp; Yu, 2017; Guo et al., 2019; Zhang et al., 2020; Zhu et al., 2020; Jiang et al., 2021; Aghajanyan et al., 2021)—combining our insights with these ideas might lead to better methods.</p>
<p>The benefit of preserving pretrained features. Our work adds to growing evidence that lightweight fine-tuning, where only a small part of a pretrained model are updated, performs better under distribution shifts-and we give a theoretical grounding to why this might be the case. Zero-shot language prompting in vision (Radford et al., 2021) and other lightweight fine-tuning approaches in NLP (Houlsby et al., 2019; Li \&amp; Liang, 2021; Xie et al., 2021b; Lester et al., 2021; Utama et al., 2021; Zhou et al., 2021) have been shown to improve OOD performance. In independent and concurrent work, Andreassen et al. (2021) observe that through the course of fine-tuning, ID accuracy continues to increase but OOD accuracy plateaus. Our work shows something stronger: at no point in the fine-tuning process does FT outperform LP.</p>
<p>Mitigating ID-OOD tradeoffs. While LP-FT has sometimes been used as a fine-tuning heuristic (Levine et al., 2016; Kanavati \&amp; Tsuneki, 2021; fastai), it has not been used for robustness / OOD accuracy, and we show that it addresses the ID-OOD tradeoff theoretically and empirically. Tradeoffs between ID and OOD accuracy are widely studied and prior work self-trains on large amounts of unlabeled data to mitigate such tradeoffs (Raghunathan et al., 2020; Xie et al., 2021a; Khani \&amp; Liang, 2021). In contrast, LP-FT uses no extra unlabeled data and is a simple variant of fine-tuning. In concurrent and independent work, Wortsman et al. (2021) show that ensembling the weights of a zero-shot and fine-tuned model mitigates the ID-OOD tradeoff between these approaches, and this method could be promising for our datasets as well.</p>
<p>Theoretical analysis of transfer learning. Prior works on transfer learning mainly analyze linear probing (Wu et al., 2020; Tripuraneni et al., 2020; Du et al., 2020). In recent work, (Chua et al., 2021) study regularized fine-tuning in an underparameterized regime where there is a unique global optimum. In contrast, our analysis studies the overparameterized regime (mirroring modern settings of zero train loss) where we need to analyze the trajectory of fine-tuning from the pretrained initialization because there is no unique optimizer of the objective function. Prior works also focus on ID error, while we analyze OOD error. See Section C for additional related work on theory of overparameterized models.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>6 Conclusion.</h1>
<p>There is a strong trend towards leveraging pretrained models to improve downstream performance, and whenever feasible, it is common to fine-tune all model parameters. In this work, we show theoretically and empirically that preserving features might be important for robustness, and simpler approaches like linear-probing can improve out-of-distribution (OOD) performance. This OOD gap between fine-tuning and linear probing grows as the quality of pretrained features improve, so we believe our results are likely to gain significance over time with growing innovations and scale of pretraining.</p>
<p>Theoretical understanding of modern deep learning remains limited, especially the effect of pretraining and transfer learning. In addition to our specific results on fine-tuning, our work introduces some tools and ideas for dealing with the main challenge of characterizing properties of the trajectory from a specific initialization in the presence of multiple global optima (implicit regularization effect of initialization). There are several open questions and extensions such as dealing with non-linear activations, different layerwise learning rates, and the effect of explicit regularization. ${ }^{10}$</p>
<p>Finally, we showed LP-FT can mitigate tradeoffs between ID and OOD accuracy in our context. LP-FT could be useful in other situations, for example in CLIP we could initialize the final layer with the zero-shot classifier and then fine-tune the entire model, as done in concurrent work (Wortsman et al., 2021). LP-FT is just a first step in leveraging the intuition from our theoretical analysis and we hope that this work inspires new methods of leveraging powerful pretrained models.</p>
<p>Proofs and Reproducibility: We include proofs for our theoretical results in Appendix A and additional experiment details in Appendix B.</p>
<p>Acknowledgements: We would like to thank Kumar Ayush and Burak Uzkent for MoCo checkpoints pretrained on unlabeled FMoW images, Nilesh Tripuraneni for clarifications on his work and references on principal angles, Daniel Levy for useful suggestions on experiments to run, Niladri Chatterji, Jeff Z. HaoChen, and Colin Wei for useful papers and comments on figures, Niladri Chatterji and Kaidi Cao for reviewing the paper at ML paper swap, Kevin Yang for his help with analyzing differential equations, Tri Dao and Pang Wei Koh for help with writing, Suriya Gunasekar, Adam Kalai, Simon Kornblith, Ting Chen, Sang Michael Xie, Albert Gu, and Kendrick Shen for useful discussions, and Pang Wei Koh, Niladri Chatterji, and Tri Dao for suggestions on framing our results better.</p>
<p>Ananya Kumar was supported by the Rambus Corporation Stanford Graduate Fellowship. Percy Liang was supported by the Open Philantropy Project and NSF Award Grant No. 1805310. Aditi Raghunathan was supported by a Google PhD Fellowship and Open Philanthropy Project AI Fellowship. Tengyu Ma acknowledges support of a Google Faculty Award, NSF IIS 2045685, the Sloan Fellowship, JD.com, SAIL, and SDSI.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>References</h1>
<p>Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and Sonal Gupta. Better fine-tuning by reducing representational collapse. In International Conference on Learning Representations (ICLR), 2021.</p>
<p>EA AlBadawy, A Saha, and MA Mazurowski. Deep learning for segmentation of brain tumors: Impact of cross-institutional training and testing. Med Phys., 45, 2018.</p>
<p>Anders Andreassen, Yasaman Bahri, Behnam Neyshabur, and Rebecca Roelofs. The evolution of out-ofdistribution robustness throughout fine-tuning. arXiv, 2021.</p>
<p>Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration by overparameterization. In International Conference on Machine Learning (ICML), pp. 244-253, 2018.</p>
<p>Kumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, M. Burke, D. Lobell, and Stefano Ermon. Geography-aware self-supervised learning. arXiv, 2020.</p>
<p>Peter L. Bartlett, Philip M. Long, G'abor Lugosi, and Alexander Tsigler. Benign overfitting in linear regression. arXiv, 2019.</p>
<p>Yonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What do neural machine translation models learn about morphology? In Association for Computational Linguistics (ACL), pp. 861-872, 2017.</p>
<p>Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv, 2019.
Koby Bibas, Yaniv Fogel, and Meir Feder. A new look at an old problem: A universal learning approach to linear regression. In 2019 IEEE International Symposium on Information Theory (ISIT), pp. 2304-2308, 2019.</p>
<p>Tianle Cai, Ruiqi Gao, J. Lee, and Qi Lei. A theory of label propagation for subpopulation shift. In International Conference on Machine Learning (ICML), 2021.</p>
<p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning (ICML), pp. 15971607, 2020a.</p>
<p>Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv, 2020b.</p>
<p>Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021a.</p>
<p>Yining Chen, Elan Rosenfeld, Mark Sellke, Tengyu Ma, and Andrej Risteski. Iterative feature matching: Toward provable domain generalization with logarithmic environments. arXiv, 2021b.</p>
<p>Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In Computer Vision and Pattern Recognition (CVPR), 2018.</p>
<p>Kurtland Chua, Qi Lei, and Jason D Lee. How fine-tuning allows for effective meta-learning. arXiv preprint arXiv:2105.02221, 2021.</p>
<p>Adam Coates, Andrew Ng, and Honlak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15, pp. 215-223, 2011.</p>
<p>Simon S. Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning the representation, provably. arXiv, 2020.</p>
<p>Simon Shaolei Du, Wei Hu, and Jason Lee. Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced. In Advances in Neural Information Processing Systems (NeurIPS), 2018.
fastai. fastai tutorial on transfer learning. https://github.com/fastai/course-v3/blob/ master/nbs/dl1/lesson1-pets.ipynb.</p>
<p>Geoff French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation. In International Conference on Learning Representations, 2018.</p>
<p>Weifeng Ge and Yizhou Yu. Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning. In Computer Vision and Pattern Recognition (CVPR), 2017.</p>
<p>Gauthier Gidel, Francis R. Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient dynamics in deep linear neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2019.</p>
<p>Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics, 2010.</p>
<p>Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University Press, 2013.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit regularization in matrix factorization. In Advances in Neural Information Processing Systems (NeurIPS), pp. 6151-6159, 2017.</p>
<p>Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and Rogerio Feris. Spottune: Transfer learning through adaptive fine-tuning. In Computer Vision and Pattern Recognition (CVPR), 2019.</p>
<p>Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.</p>
<p>Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Computer Vision and Pattern Recognition (CVPR), 2020.</p>
<p>Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness and uncertainty. In International Conference on Machine Learning (ICML), 2019a.</p>
<p>Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. arXiv preprint arXiv:1907.07174, 2019b.</p>
<p>Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020 .</p>
<p>John Hewitt and Christopher D. Manning. A structural probe for finding syntax in word representations. In Association for Computational Linguistics (ACL), 2019.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. arXiv, 2019.</p>
<p>Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification. In Association for Computational Linguistics (ACL), 2018.</p>
<p>Neal Jean, Marshall Burke, Michael Xie, W. Matthew Davis, David B. Lobell, and Stefano Ermon. Combining satellite imagery and machine learning to predict poverty. Science, 353, 2016.</p>
<p>Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. Smart: Robust and efficient fine-tuning for pre-trained natural language models through principled regularized optimization. In International Conference on Learning Representations (ICLR), 2021.</p>
<p>Pritish Kamath, Akilesh Tangella, Danica J. Sutherland, and Nathan Srebro. Does invariant risk minimization capture invariance? In Artificial Intelligence and Statistics (AISTATS), 2021.</p>
<p>Fahdi Kanavati and Masayuki Tsuneki. Partial transfusion: on the expressive influence of trainable batch norm parameters for transfer learning. In Medical Imaging with Deep Learning, 2021.</p>
<p>Fereshte Khani and Percy Liang. Removing spurious features can hurt accuracy and affect groups disproportionately. In ACM Conference on Fairness, Accountability, and Transparency (FAccT), 2021.</p>
<p>Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma Pierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution shifts. In International Conference on Machine Learning (ICML), 2021.</p>
<p>Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do better imagenet models transfer better? In Computer Vision and Pattern Recognition (CVPR), 2019.</p>
<p>Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.</p>
<p>Thomas Laurent and James H. von Brecht. Deep linear neural networks with arbitrary loss: All local minima are global. In International Conference on Machine Learning (ICML), 2018.</p>
<p>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.
S. Levine, Chelsea Finn, Trevor Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research (JMLR), 17, 2016.</p>
<p>Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Association for Computational Linguistics (ACL), 2021.</p>
<p>Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with convolutional networks. In International Conference on Machine Learning (ICML), 2018.</p>
<p>Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.</p>
<p>John Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In International Conference on Machine Learning (ICML), 2021.</p>
<p>Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpolation of noisy data in regression. IEEE Journal on Selected Areas in Information Theory, 1(1):67-83, 2020.</p>
<p>Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of implicit regularization in deep learning. arXiv, 2014.</p>
<p>Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In International Conference on Computer Vision (ICCV), 2019.</p>
<p>Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In North American Association for Computational Linguistics (NAACL), 2018.</p>
<p>Matthew E Peters, Sebastian Ruder, and Noah A Smith. To tune or not to tune? adapting pretrained representations to diverse tasks. In Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pp. 7-14, 2019.</p>
<p>Viraj Prabhu, Shivam Khare, Deeksha Karthik, and Judy Hoffman. Selective entropy optimization via committee consistency for unsupervised domain adaptation. In International Conference on Computer Vision (ICCV), 2021.</p>
<p>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), volume 139, pp. 8748-8763, 2021.</p>
<p>Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C. Duchi, and Percy Liang. Understanding and mitigating the tradeoff between robustness and accuracy. In International Conference on Machine Learning (ICML), 2020.</p>
<p>Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do CIFAR-10 classifiers generalize to CIFAR-10? arXiv, 2018.</p>
<p>Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning (ICML), 2019.</p>
<p>Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. In International Conference on Learning Representations (ICLR), 2021.</p>
<p>Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. Communications on Pure and Applied Mathematics, 62:1707-1739, 2009.</p>
<p>Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej</p>
<p>Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.</p>
<p>Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation shift. arXiv, 2020.</p>
<p>Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv, 2014.</p>
<p>Shuhan Tan, Xingchao Peng, and Kate Saenko. Class-imbalanced domain adaptation: An empirical odyssey. arXiv preprint arXiv:1910.10320, 2020.</p>
<p>Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. arXiv preprint arXiv:2007.00644, 2020.</p>
<p>Nilesh Tripuraneni, Michael I. Jordan, and Chi Jin. On the theory of transfer learning: The importance of task diversity. arXiv, 2020.</p>
<p>Joel A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in Machine Learning, 8:1-230, 2015.</p>
<p>Prasetya Ajie Utama, Nafise Sadat Moosavi, Victor Sanh, and Iryna Gurevych. Avoiding inference heuristics in few-shot prompt-based finetuning. arXiv preprint arXiv:2109.04144, 2021.</p>
<p>Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In Advances in Neural Information Processing Systems (NeurIPS), 2019.</p>
<p>Mitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt. Robust fine-tuning of zero-shot models. arXiv preprint arXiv:2109.01903, 2021.</p>
<p>Sen Wu, Hongyang R. Zhang, and Christopher Ré. Understanding and improving information transfer in multi-task learning. In International Conference on Learning Representations (ICLR), 2020.</p>
<p>Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-N-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness. In International Conference on Learning Representations (ICLR), 2021a.</p>
<p>Sang Michael Xie, Tengyu Ma, and Percy Liang. Composed fine-tuning: Freezing pre-trained denoising autoencoders for improved generalization. In International Conference on Machine Learning (ICML), 2021b.</p>
<p>Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In Computer Vision and Pattern Recognition (CVPR), 2020.</p>
<p>Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv, 2020.</p>
<p>Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning: A baseline for network adaptation via additive side networks. In European Conference on Computer Vision (ECCV), 2020.</p>
<p>Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. arXiv preprint arXiv:2109.01134, 2021.</p>
<p>Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. FreeLB: Enhanced adversarial training for natural language understanding. In International Conference on Learning Representations (ICLR), 2020.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ We found that LP-FT outperforms explicit regularization and using a higher learning rate for the linear layer on Living-17 (Appendix B.4), but a more extensive theoretical and empirical study on this is important.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
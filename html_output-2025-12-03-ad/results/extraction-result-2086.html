<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2086 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2086</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2086</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-277621634</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.03810v1.pdf" target="_blank">Hierarchically Encapsulated Representation for Protocol Design in Self-Driving Labs</a></p>
                <p><strong>Paper Abstract:</strong> Self-driving laboratories have begun to replace human experimenters in performing single experimental skills or predetermined experimental protocols. However, as the pace of idea iteration in scientific research has been intensified by Artificial Intelligence, the demand for rapid design of new protocols for new discoveries become evident. Efforts to automate protocol design have been initiated, but the capabilities of knowledge-based machine designers, such as Large Language Models, have not been fully elicited, probably for the absence of a systematic representation of experimental knowledge, as opposed to isolated, flatten pieces of information. To tackle this issue, we propose a multi-faceted, multi-scale representation, where instance actions, generalized operations, and product flow models are hierarchically encapsulated using Domain-Specific Languages. We further develop a data-driven algorithm based on non-parametric modeling that autonomously customizes these representations for specific domains. The proposed representation is equipped with various machine designers to manage protocol design tasks, including planning, modification, and adjustment. The results demonstrate that the proposed method could effectively complement Large Language Models in the protocol design process, serving as an auxiliary module in the realm of machine-assisted scientific exploration.</p>
                <p><strong>Cost:</strong> 0.03</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2086.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2086.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models (LLMs) used as protocol designers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Knowledge-based language models (e.g., GPT-family) employed to generate experimental protocols, translate natural-language protocols into DSL/pseudocode, and implement in-context verification and refinement loops; used both as pure generators and as components within structured pipelines (RAG, internal prompting, external verifier).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Large Language Models (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>experimental protocol design (biology/medical/bioengineering/ecology)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>natural-language protocol steps, Python pseudocode, DSL program instantiations (protocols)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>primarily in-distribution / incremental (paper reports LLMs tend to generate protocols similar to existing ones and struggle with distinct dependency distributions / out-of-distribution novelty)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompted generation with Retrieval-Augmented Generation (RAG) and in-context learning; generation via direct NL output or instantiation into DSL programs; chain-of-thought style reasoning sometimes used but constrained by DSL prompts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>DSL-based syntactic/semantic checks (precondition/postcondition matching) and external DSL verifiers when available; ultimately human expert certification required for final acceptance</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Varies widely by representation and prompting: pure LLM baseline (Flatten-Baseline, FB) yields low protocol-level agreement with ground truth (example: modification task FB IoU(Op)=0.181, IoU(Prod)=0.050, IoU(Dev)=0.038, Sim(Exec)=0.304, Sim(Goal)=0.796, Sim(Param)=0.809; standard errors reported in paper tables). Generation speed/costs: design uses GPT-4o-mini for preprocessing and design with modest monetary cost (~$10 for the design runs reported).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>When combined with DSL-based verification (external verifier), outputs improve substantially (see Encapsulated-External+ EE+ and Encapsulated-Internal+ EI+ results); e.g., modification task EE+ achieved IoU(Op)=0.640, IoU(Prod)=0.661, IoU(Dev)=0.410, Sim(Exec)=0.757, Sim(Goal)=0.893, Sim(Param)=0.953. Raw LLM-only validation (no DSL) is weak or absent.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>No numeric FP/FN rates reported for final protocol acceptance. Paper notes qualitative false positives in model abstraction (properties wrongly attributed to components across phases) and false negatives (synonym/reference-name mismatches like 'Acetylsalicylic Acid' vs 'Aspirin'); explicit numeric FP rate for generated protocols is not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported numerically. Qualitatively, LLMs can omit critical configuration details leading to false negatives (valid protocols deemed incomplete), but no per-condition rates are given.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Performance decreases as novelty/out-of-distributionness increases (planning tasks with more novel goals have lower baseline LLM performance). DSL-constrained methods (EI+/EE+) reduce this degradation but do not eliminate need for human review.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper explicitly compares generation-only (Baseline, FB) to generation+validation (External verifier EE/EE+). There is an asymmetry: LLMs can generate candidate protocols but they often lack critical details and/or produce inapplicable dependency distributions; DSL verification catches structural mismatches and improves agreement metrics substantially (statistically significant improvements reported).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No calibrated per-output uncertainty scores provided by LLMs in experiments. Authors measured model perplexity to test memorization (higher perplexity on novel protocols), but no explicit confidence/calibration outputs from LLMs were used for decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported. The paper notes LLMs can produce coherent but potentially unprofessional or unsafe outputs and thus require guardrails; no calibration metrics (ECE, calibration plots) are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Quantitatively poorer for more novel (out-of-distribution) protocols: baseline methods perform worse on planning tasks (more novel). Exact out-of-distribution numeric ROC/accuracy metrics not provided beyond the IoU/Sim scores per task.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — the study uses proxy metrics (IoU(Op), IoU(Prod), IoU(Dev), Sim(Exec) sequence alignment, Sim(Goal) cosine similarity of serialized representations, Sim(Param) parameter-wise cosine similarity) as proxies for functional correctness rather than direct experimental execution.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>All designed protocols intended to undergo human expert certification for final acceptance; the paper stresses human review is required for certification and particularly for long-tail / risky cases; frequency effectively 'always' for certification in this work and increases in importance with novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / semi-formal (experimental laboratory protocols in biology/medical/etc. — real-world procedures with some standardization but not fully formalized like mathematics); DSLs introduce more formal structure but domain remains empirical.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Introduce hierarchical DSL representations (instance actions, operation-centric interfaces, product-flow-centric models), automated representation generation (non-parametric DPMM + Gaussian Process), and external DSL verifiers that cross-validate operation and product flow; these strategies substantially improve agreement metrics (EE+/EI+ outperform baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Empirical results: pure LLM baselines (FB/IB) achieve low IoU/Sim scores while DSL-constrained + verifier approaches (EI+/EE+) achieve much higher agreement; qualitative observations that LLMs generate protocols similar to existing ones and omit execution-critical details; authors argue verification is necessary as LLMs act like 'novice' scientists.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Improved representations and verification significantly reduce the gap (EE+/EI+ results), suggesting that the gap can be mitigated by structured representations and verification—however, full machine-only certification remains unattained.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>No explicit numeric ratio of validation cost to generation cost reported. Paper reports preprocessing/design costs were modest (≈$60 for preprocessing across domains; ≈$10 for machine designer runs) and representation generation required iterative modeling (1,000 iterations; average ~55s/iteration for operation-centric view). Overall statement: representation generation is heavier (offline) while per-protocol generation+verification is relatively low-cost.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2086.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2086.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>gpt-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o mini (used for preprocessing and design in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A specific LLM variant (gpt-4o mini) used by the authors for preprocessing (entity classification, NER, synonym merging) and for producing DSL-based designs during experiments; used as the primary LLM in RAG/in-context pipelines reported.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>gpt-4o mini</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protocol design preprocessing and DSL program generation</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>entity classifications, DSL program text, pseudocode and natural-language protocol steps</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>in-distribution generation for preprocessing tasks; used to produce novel protocol drafts but constrained by DSL prompts (paper reports limited OOD capability for novel dependency distributions)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>prompted in-context learning, classification and NER tasks during preprocessing, and RAG-assisted generation for protocols</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Outputs are validated via the DSL verification loop (external verifier) and human expert review for final certification; gpt-4o-mini also used to categorize opcodes/entities during preprocessing and for synonym detection (human-reviewed merge afterwards).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Used extensively in preprocessing; monetary cost ~ $60 for preprocessing across four domains, and ~ $10 for machine designer runs. Per-protocol generation performance measured via downstream IoU/Sim metrics when used inside designer pipelines (see corresponding machine-designer entries).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Not separately reported; validation performed by DSL verifier/human experts rather than by the model itself.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically. Preprocessing steps include synonym merges verified by GPT + human checks to reduce FP/FN.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Perplexity experiments indicate the LLM has higher perplexity on novel protocols (suggesting less confidence/greater uncertainty), but no calibrated uncertainty used.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Used as generator component; when paired with DSL-based validation, overall pipeline performance improves — see EE+/EI+ metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Perplexity was computed in a memorization test (higher perplexity on novel test set), but no output-level uncertainty scores were used in validation.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Perplexity higher on novel protocols; generation quality degrades for more novel test items (qualitative/aggregate evidence only).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Same proxies (IoU, Sim) used to measure downstream quality of gpt-4o-mini outputs when employed in designers.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human expert review for final certification of designed protocols (applies to outputs produced using gpt-4o-mini).</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / semi-formal</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Constrain gpt-4o-mini outputs with DSL prompts and external verifier; use automated representation generation to improve DSL quality.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Perplexity gap and downstream lower IoU/Sim for baseline LLM-based approaches support generation-validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>DSL-constrained outputs from gpt-4o-mini show much higher agreement, indicating gap can be reduced with structured constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Reported costs: preprocessing ~$60; design ~$10. No explicit validation-to-generation cost ratio provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2086.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2086.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-DSL (DPMM+GP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic representation generator using hierarchical non-parametric modeling (Dirichlet Process Mixture Model plus Gaussian Process)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Data-driven algorithm that automatically generates the three-level hierarchical DSLs (instance actions, operation-centric interfaces, product-flow models) from domain-specific protocol corpora using hierarchical DPMM for clustering interfaces and GP integration for parameter-value modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automatic representation generator (AutoDSL)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hierarchical non-parametric model (Dirichlet Process Mixture Model with Gaussian Process for parameter values)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>domain-specific DSL induction for experimental protocol representation (biology domains)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>domain-specific DSL specifications: operation-centric view (interfaces/patterns) and product-flow-centric view (flow units/states)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>method produces new, domain-specific representations (novel in the paper's contribution); not a generator of experimental hypotheses per se but of the representational language which permits novel protocol generation.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>hierarchical DPMM clusters instance-level contexts into interface instances; Gaussian Process models parameter value distributions; unification rules merge redundant interfaces (Martelli-Montanari style unification).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Convergence monitored via likelihood curves; qualitative inspection (examples) and downstream utility assessed by improvement in protocol-design metrics when DSLs used by designers; human experts validated test set and final protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Quantified DSL statistics across domains (e.g., Genetics: 304 operations with avg 7.9 interface instances; Model abstraction states: Genetics 17,190 states; Medical 12,472; Bioengineering 11,418; Ecology 2,205). Convergence curves presented (Fig.2B/C).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Indirect: DSLs improve downstream protocol-design metrics substantially (encapsulated representations yield higher IoU/Sim scores in design tasks). No standalone FP/FN rates for representation elements are given.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically. Paper discusses qualitative FP/FN issues in model abstraction (property attribution across phases leading to false positives; synonyms causing false negatives) and addresses them via modeling choices (discard interface design for model abstraction).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Larger domain corpora appear to yield 'better' DSLs (paper observes Genetics corpus produced best performance; Ecology corpus smaller and DSL less effective), implying representation quality (and thus validation capability) depends on available data and affects ability to handle novel protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>AutoDSL is a generation tool for representation; comparison is indirect — DSLs produced by AutoDSL enable improved generation+validation pipelines (EE+/EI+) compared to no-DSL baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>DPMM implicitly models uncertainty via non-parametric clustering; GP models parameter-value uncertainty; no calibrated end-to-end uncertainty scores for protocol validity provided.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported as a single metric.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>DSLs learned from larger in-domain corpora generalize better; explicit OOD numeric metrics not provided for the representation generator itself.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses likelihood convergence and downstream protocol-design performance (IoU/Sim) as proxies for DSL quality.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human experts used for testing set selection and ground-truth checking; DSLs themselves are validated primarily by downstream improvement and expert review.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal (DSLs introduce formal structure to an empirical domain)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Auto-generated hierarchical DSLs provide structured constraints and richer background knowledge to reduce LLM generation errors and aid verification; unification step reduces redundant interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Paper notes that without structured representation (flattened NL), LLMs only retrieve similar protocols and fail to generate distinct dependency distributions; AutoDSL-generated DSLs alleviate this.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None directly; AutoDSL improves downstream metrics but does not fully remove need for human certification.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Representation generation: 1,000 iterations; avg ~55s/iteration (operation-centric) and ~2s/iteration (product-centric). Monetary LLM preprocessing cost ~ $60. Per-protocol generation/verification cost is small in comparison. No explicit numeric ratio of validation-to-generation costs reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2086.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2086.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DSL External Verifier (reciprocative verification)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Domain-Specific Language external verifier implementing reciprocative verification over operation- and product-flow-centric DSLs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated verifier that cross-checks operation pre/postconditions against product-flow states (two-thread reciprocative verification), emits structured error messages when mismatches occur, and drives an LLM feedback-refine loop until verification passes or iteration limit reached.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DSL External Verifier (Reciprocative Verification)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>symbolic program verifier / neurosymbolic verifier (DSL-based deterministic checks)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protocol design verification (biology/medical/etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>verification verdicts, structured error messages indicating precondition/postcondition or product-flow mismatches</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>novel in this paper as an integrated verifier for dual DSL views</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>N/A (verification system) — checks consistency between operation DSL programs and product-flow DSL programs via set inclusion and state updates</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Deterministic checks: OFVERIFICATION ensures preconditions are satisfied by available products (M(Ω) updates), PFVERIFICATION tracks product generation and property alignment; errors reported back to LLM for refinement; used as an external validation layer.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Not applicable (verifier does not generate protocols). Its effectiveness is measured by downstream improvements in IoU/Sim when used as external verifier: Encapsulated-External+ (EE+) achieves higher metrics than internal-only variants (statistically significant improvements reported).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Substantially improves downstream protocol agreement (e.g., modification task EE+ IoU(Prod)=0.661 vs EB/IB lower baselines). Statistical tests show EE+ and EI+ significantly outperform counterparts (paired t-tests, p < .0001).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not quantified numerically. The verifier can generate false positives (flagging issues that may be naming mismatches rather than real logical errors); prompts instruct LLMs to ignore certain name mismatches when appropriate. Paper explicitly notes DSL verification is not sufficient for certification and may miss long-tail risky cases.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not quantified. Paper acknowledges data-driven certifiers may miss long-tail failure modes and thus human experts remain required.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Verifier reduces performance degradation on novel tasks (planning/modification) compared to LLM-only approaches — demonstrated by higher IoU/Sim across planning/modification/adjustment tasks when verifier is used.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper demonstrates that adding the external verifier to LLM generation pipeline bridges part of the gap: external-verified outputs (EE/EE+) outperform internal-only prompting. However, verifier cannot fully replace humans for certification.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Verifier is deterministic and does not output probabilistic uncertainty; no calibrated uncertainties provided.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Improves robustness on novel (OOD) protocol generation relative to no-verifier baselines (quantified by higher IoU/Sim scores), but absolute performance still limited and human review needed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Verifier checks structural proxies: pre/postcondition set inclusion and simulated product-flow state transitions rather than real-world experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Verifier reduces but does not eliminate need for human certification; human experts still required for final sign-off, especially for long-tail/risky cases.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal (DSL enables formal-like program checks in an empirical domain)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Use of dual-view DSL and reciprocative verification as an external constraint layer and feedback to LLMs; instruct LLM to refine until verifier passes.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Despite verifier improvements, authors explicitly state DSL verification is insufficient for certification and human experts remain necessary — supporting existence of a generation-validation gap for high-stakes outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Significant metric gains when verifier is used show the gap can be materially narrowed with appropriate representation and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported explicitly; verifier is computationally cheap relative to offline DSL generation and LLM costs, and is used within iterative feedback loops (cost per iteration modest).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2086.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2086.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flatten-Baseline (FB)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flatten representation with Baseline LLM + RAG (FB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pure LLM baseline using Retrieval-Augmented Generation on original natural-language protocol corpora to generate new protocol steps/pseudocode without structured DSL constraints or external verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Flatten-Baseline (FB)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model with RAG</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protocol design (biology domains)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>natural-language protocols and pseudocode</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>mostly in-distribution / retrieves and adapts similar existing protocols; poor at producing distinct dependency distributions</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>RAG: retrieve top-3 similar protocols from corpora and prompt LLM to generate a new protocol based on them</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>None automated in pipeline (no DSL-based verification); human experts used post-hoc for certification in study design but not as part of automatic pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported metrics (examples): Modification task: IoU(Op)=0.181, IoU(Prod)=0.050, IoU(Dev)=0.038, Sim(Exec)=0.304, Sim(Goal)=0.796, Sim(Param)=0.809 (standard errors in tables). Adjustment and planning tasks also show low operation/product IoUs relative to DSL-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>No automated validation; downstream agreement metrics are low relative to DSL-constrained approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically; qualitatively high risk of missing critical configuration details and producing implausible/incomplete steps.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Performs poorly on more novel test items (planning), as baseline LLM tends to retrieve similar protocols rather than invent new valid dependency structures.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Baseline generation without verification underperforms methods that include DSL constraints and verification (II, EI, EE+).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not used.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor relative to DSL-enhanced systems; specific IoU/Sim numbers in tables show gap.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Downstream IoU/Sim metrics used to evaluate generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human experts used to construct testing ground truth and for certification; baseline outputs would also require human correction.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / semi-formal (flat NL representation)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Not present in this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Low IoU/Sim results demonstrate generation-only approach is insufficient; LLMs produce outputs requiring manual correction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Reported LLM costs for design modest (~$10), but no explicit generation-vs-validation cost ratio since no automated validation used.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2086.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2086.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instance-Baseline (IB)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instance actions representation with Baseline LLM retrieval (IB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM with RAG retrieving instance-action-structured protocol fragments (instance-level key-value attributes) and generating protocol pseudocode; representation improves syntactic detail but limits reuse/generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Instance-Baseline (IB)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model with RAG over structured instance-action corpora</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protocol design</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>pseudocode based on instance-action building blocks</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>limited novelty; relies on exact or near-exact instance matches (low generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Retrieval of instance-action fragments then LLM synthesis into protocol pseudocode</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>No external DSL verification in IB pipeline; evaluation done by comparison to ground truth and human checks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Example metrics (modification task): IoU(Op)=0.150, IoU(Prod)=0.038, IoU(Dev)=0.039, Sim(Exec)=0.281, Sim(Goal)=0.772, Sim(Param)=0.788.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Low; better than Flatten only in some respects but substantially worse than encapsulated representations.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically; qualitative limitation: over-specific instance actions lead to low matching for novel contexts (effective false negatives for reuse).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Struggles on novel tasks where exact instance matches are unlikely (planning tasks particularly affected).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Outperformed by higher-level representations (II, EI, EI+, EE+), indicating that increasing abstraction improves generation and downstream validation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor due to lack of generalization; metrics show lower IoU/Sim.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Same proxy metrics used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human experts involved in evaluating outputs; required for certification.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical with structured instance representation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Not in IB; improved by moving to operation-centric and product-flow-centric DSLs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Low downstream metrics indicate generation alone insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2086.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2086.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instance-Internal (II)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instance representation used as internal LLM prompt constraints (II)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM prompted with instruction set assembly (ISA) of instance actions (structured instance-level DSL included in prompt) so generation is constrained to use provided instance-action primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Instance-Internal (II)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model with internal prompting using instance-level representation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protocol design</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>pseudocode/DSL-like program using provided instance primitives</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>improved over IB but still limited; more structured prompts help, but abstraction limited</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting LLM with instance-action ISA and examples; LLM must synthesize a protocol using given pseudofunction definitions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>No external verifier; evaluation uses corpus ground-truth comparisons and human checks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Example metrics (modification task): IoU(Op)=0.331, IoU(Prod)=0.101, IoU(Dev)=0.061, Sim(Exec)=0.416, Sim(Goal)=0.802, Sim(Param)=0.851.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Better than flat baselines but inferior to operation-centric and dual-view methods.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not numerically reported. Qualitative: internal constraint reduces some generation errors but cannot enforce inter-step product coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Outperforms simple baselines on less-novel tasks; still degrades on planning (more novel) tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Internal prompting helps, but external verification and higher-level abstraction deliver larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Intermediate; improved over FB/IB but below EI/EE+.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>IoU/Sim metrics used.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human review used for final certification.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal (instance-level structured prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Internal prompting constrains generation; less effective than structural DSL + external verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>II improves over baseline but still requires verification (evidence of generation-validation gap persists).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>None.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2086.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2086.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Encapsulated-Internal (EI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Operation-centric DSL used as internal prompt to LLM (EI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM prompted with operation-centric DSL (function abstractions/interfaces) as part of input, producing DSL-instantiated protocols constrained by operation interfaces; internal approach (no external verifier).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Encapsulated-Internal (EI)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM with internal DSL prompts (operation-centric)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protocol design</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>operation-centric DSL program instantiations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>more capable of generating novel compositions due to operation-level abstraction (moderately novel), but still benefits from product-flow verification for inter-step coherence</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting LLM with operation-centric DSL patterns and examples; LLM instantiates interfaces to assemble protocols</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Internal constraints only; no external DSL verifier in EI variant; outputs evaluated against ground truth and via human review</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Strong improvement over lower-level methods. Example (modification task): IoU(Op)=0.593, IoU(Prod)=0.318, IoU(Dev)=0.336, Sim(Exec)=0.602, Sim(Goal)=0.866, Sim(Param)=0.937.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Better than II/IB/FB but slightly below external verification variants (EE/EE+).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not numerically reported; internal constraints reduce some false positives but do not check product-flow continuity across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Handles more novel compositions better than instance-level methods, but still benefits from product-flow checks on highly novel tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>EI outperforms II; however, adding external product-flow verification (EI+) further improves results significantly.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Improved relative to baselines; numeric improvements shown in tables but absolute OOD capability limited.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>IoU/Sim metrics used.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human certification required for final acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal (operation-centric DSL)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Operation-level abstraction reduces generation errors; adding product-flow view + external verifier (EI+) yields larger gains.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Internal-only approach still leaves inter-step product coherence issues unverified.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Improved generation quality shows structured representation narrows the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2086.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2086.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Encapsulated-Internal+ (EI+)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual representation (operation + product-flow) used as internal prompt to LLM (EI+)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM prompted with both operation-centric and product-flow-centric DSLs (dual view) included in the prompt, producing DSL programs for operations and product flow alternately; internal approach without external verifier but with dual consistency constraints provided in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Encapsulated-Internal+ (EI+)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM with internal dual-view DSL prompting</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protocol design</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>dual-view DSL programs (operation and product-flow representations)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel; dual view improves ability to design protocols for novel objectives and increases reuse of operations across contexts</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompting LLM with both DSL views and examples to instantiate coherent operation/product-flow alternations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Internal prompt instructions encourage cross-checks, but no external deterministic verifier; final outputs still compared to ground truth and reviewed by humans</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Strong performance. Example (modification task): EI+ IoU(Op)=0.648, IoU(Prod)=0.626, IoU(Dev)=0.413, Sim(Exec)=0.765, Sim(Goal)=0.883, Sim(Param)=0.952 (std errors reported in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>High relative to other internal methods; addition of external verifier (EE+) gives further improvement but EI+ already strong.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported numerically. Dual view reduces many false positives related to inter-step incoherence but cannot eliminate long-tail issues.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Performs well on planning/modification/adjustment tasks; dual view narrows performance drop for novel tasks compared to single-view or flat baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>EI+ significantly outperforms II and FB baselines (statistically significant), showing representation + prompting can narrow the gap though external verifier helps further.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Better OOD robustness than simpler methods; quantified by higher IoU/Sim on novel test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>IoU/Sim metrics used.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human certification required for final acceptance; frequency and criticality increase with novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal (dual DSL prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Dual representation in prompt reduces incoherence and improves inter-step consistency; effective even without external verifier but best combined with verifier.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Even with EI+, authors state human certification still needed and DSL-level verification insufficient for full certification.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>High EI+ performance demonstrates generation can approach validation when constrained by good representations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2086.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2086.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Encapsulated-External (EE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Operation-centric DSL with external verifier (EE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM generates protocols under operation-centric DSL constraints and then an external DSL verifier checks pre/postconditions; verifier feedback is used iteratively to refine the LLM output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Encapsulated-External (EE)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM + external DSL verifier (operation-centric)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protocol design</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>operation-centric DSL programs validated via external verifier</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel for composition tasks; verification enforces structural soundness</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM prompted with operation-centric DSL; RAG/in-context; generated programs are sent to external verifier which checks pre/postcondition coherence</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>External DSL verifier checking that preconditions are satisfied by previous outputs and that outputs are used subsequently; mismatch errors returned to LLM for iterative refinement</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Strong. Example (modification task): EE IoU(Op)=0.588, IoU(Prod)=0.403, IoU(Dev)=0.332, Sim(Exec)=0.601, Sim(Goal)=0.873, Sim(Param)=0.940.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Improves correctness relative to internal-only; however dual-view external (EE+) yields even higher performance.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not numerically reported; verifier may still miss long-tail experimental risks and cannot guarantee zero false positives for real-world execution.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>External verification improves robustness to novelty relative to internal-only variants, but some novel cases still require human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>EE outperforms internal-only (EI) and instance-based approaches; EE+ (dual-view external) outperforms EE.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Verifier deterministic; no probabilistic uncertainty estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Improved compared to no-verifier baselines (quantified by IoU/Sim), but absolute OOD robustness limited.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Deterministic DSL checks and downstream IoU/Sim.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Human certification required for final acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>External verification reduces generation errors by enforcing pre/postcondition consistency and driving iterative LLM refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Although EE improves results, authors emphasize verifier alone is insufficient for certification, implying generation still outpaces safe validation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>EE demonstrates substantial reduction of generation errors relative to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not explicitly quantified; verifier adds iterative cost but per-iteration overhead is small compared to offline DSL generation and LLM inference.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2086.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2086.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Encapsulated-External+ (EE+)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual-view (operation + product-flow) DSL with external verifier (EE+)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combined pipeline using both operation-centric and product-flow-centric DSLs with an external reciprocative verifier that cross-checks operations and product-flow units; iterative refine loop with LLM produces the highest-performing automated protocol designs in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Encapsulated-External+ (EE+)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neurosymbolic pipeline: LLM + dual-view DSL + external verifier</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>protocol design across Genetics, Medical, Bioengineering, Ecology</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>dual-view DSL protocol programs (operations and product flows), refined iteratively</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately to substantially novel within the constrained DSL space; best empirical handling of novel protocol generation among tested systems</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM generates DSL program instantiations for both views; external reciprocative verifier checks inter-view consistency and returns structured feedback for iterative refinement (self-refine loop)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Reciprocative verification: OFVERIFICATION checks operation pre/postcondition satisfaction; PFVERIFICATION tracks product flow states and properties; verifier emits errors prompting LLM refinement until program passes or iteration limit reached; final human certification required.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Best overall in experiments. Example (modification task): EE+ IoU(Op)=0.640, IoU(Prod)=0.661, IoU(Dev)=0.410, Sim(Exec)=0.757, Sim(Goal)=0.893, Sim(Param)=0.953 (standard errors reported). Similar high performance across planning and adjustment tasks in appendix tables.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>High relative to all other tested systems; paired t-tests show EE+ significantly outperforms EE and II (p < .0001) and other baselines, demonstrating that combined representation + verification materially improves validation proxies.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not given numerically; paper cautions that DSL verification still may yield false positives/negatives for real experimental risk and cannot fully replace human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not numerically reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>EE+ narrows the performance drop on novel (planning) tasks compared to baselines and internal-only methods; still requires human review for certification but shows best OOD robustness among tested pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Directly demonstrates that adding structured representation and external verification bridges much of the generation-validation divide: EE+ has much higher IoU/Sim than generation-only baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>No probabilistic uncertainty quantification provided; verifier is deterministic while LLM uncertainty not used.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Best of all tested methods on novel test subsets (planning/modification tasks), as shown by higher IoU/Sim numbers in tables; no additional OOD metrics (e.g., AUC) are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Uses DSL consistency checks as validation plus proxy agreement metrics (IoU/Sim) against held-out ground truth protocols.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Final human certification required for all generated protocols; human review remains especially necessary for long-tail and potentially dangerous cases.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal (dual DSL provides strong structural formalization but domain experimental outcomes remain empirical)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Dual-view DSL + reciprocative external verification + iterative LLM refinement (self-refine loop) — empirically effective at reducing generation errors and improving agreement with ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Authors explicitly note that DSL verification is not sufficient for certification and human experts remain essential — supporting existence of remaining gap despite improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>High EE+ performance demonstrates substantial mitigation of the gap; metrics show statistically significant gains.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>Not provided as a single number. Paper reports representation generation (offline) is the most computationally/time-consuming step (1,000 iterations, 55s/iteration for op-centric DSL), while per-protocol generation+verification costs are modest (LLM inference and verifier checks).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Bioplanner: Automatic evaluation of llms on protocol planning in biology <em>(Rating: 2)</em></li>
                <li>Augmenting large language models with chemistry tools <em>(Rating: 1)</em></li>
                <li>An integrated self-optimizing programmable chemical synthesis and reaction engine <em>(Rating: 1)</em></li>
                <li>Reconfigurable system for automated optimization of diverse chemical reactions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2086",
    "paper_id": "paper-277621634",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "LLM (general)",
            "name_full": "Large Language Models (LLMs) used as protocol designers",
            "brief_description": "Knowledge-based language models (e.g., GPT-family) employed to generate experimental protocols, translate natural-language protocols into DSL/pseudocode, and implement in-context verification and refinement loops; used both as pure generators and as components within structured pipelines (RAG, internal prompting, external verifier).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Large Language Models (LLMs)",
            "system_type": "large language model",
            "scientific_domain": "experimental protocol design (biology/medical/bioengineering/ecology)",
            "output_type": "natural-language protocol steps, Python pseudocode, DSL program instantiations (protocols)",
            "novelty_level": "primarily in-distribution / incremental (paper reports LLMs tend to generate protocols similar to existing ones and struggle with distinct dependency distributions / out-of-distribution novelty)",
            "generation_method": "prompted generation with Retrieval-Augmented Generation (RAG) and in-context learning; generation via direct NL output or instantiation into DSL programs; chain-of-thought style reasoning sometimes used but constrained by DSL prompts",
            "validation_method": "DSL-based syntactic/semantic checks (precondition/postcondition matching) and external DSL verifiers when available; ultimately human expert certification required for final acceptance",
            "generation_performance": "Varies widely by representation and prompting: pure LLM baseline (Flatten-Baseline, FB) yields low protocol-level agreement with ground truth (example: modification task FB IoU(Op)=0.181, IoU(Prod)=0.050, IoU(Dev)=0.038, Sim(Exec)=0.304, Sim(Goal)=0.796, Sim(Param)=0.809; standard errors reported in paper tables). Generation speed/costs: design uses GPT-4o-mini for preprocessing and design with modest monetary cost (~$10 for the design runs reported).",
            "validation_performance": "When combined with DSL-based verification (external verifier), outputs improve substantially (see Encapsulated-External+ EE+ and Encapsulated-Internal+ EI+ results); e.g., modification task EE+ achieved IoU(Op)=0.640, IoU(Prod)=0.661, IoU(Dev)=0.410, Sim(Exec)=0.757, Sim(Goal)=0.893, Sim(Param)=0.953. Raw LLM-only validation (no DSL) is weak or absent.",
            "false_positive_rate": "No numeric FP/FN rates reported for final protocol acceptance. Paper notes qualitative false positives in model abstraction (properties wrongly attributed to components across phases) and false negatives (synonym/reference-name mismatches like 'Acetylsalicylic Acid' vs 'Aspirin'); explicit numeric FP rate for generated protocols is not reported.",
            "false_negative_rate": "Not reported numerically. Qualitatively, LLMs can omit critical configuration details leading to false negatives (valid protocols deemed incomplete), but no per-condition rates are given.",
            "performance_vs_novelty": "Performance decreases as novelty/out-of-distributionness increases (planning tasks with more novel goals have lower baseline LLM performance). DSL-constrained methods (EI+/EE+) reduce this degradation but do not eliminate need for human review.",
            "generation_validation_comparison": "Paper explicitly compares generation-only (Baseline, FB) to generation+validation (External verifier EE/EE+). There is an asymmetry: LLMs can generate candidate protocols but they often lack critical details and/or produce inapplicable dependency distributions; DSL verification catches structural mismatches and improves agreement metrics substantially (statistically significant improvements reported).",
            "uncertainty_quantification": "No calibrated per-output uncertainty scores provided by LLMs in experiments. Authors measured model perplexity to test memorization (higher perplexity on novel protocols), but no explicit confidence/calibration outputs from LLMs were used for decision-making.",
            "calibration_quality": "Not reported. The paper notes LLMs can produce coherent but potentially unprofessional or unsafe outputs and thus require guardrails; no calibration metrics (ECE, calibration plots) are provided.",
            "out_of_distribution_performance": "Quantitatively poorer for more novel (out-of-distribution) protocols: baseline methods perform worse on planning tasks (more novel). Exact out-of-distribution numeric ROC/accuracy metrics not provided beyond the IoU/Sim scores per task.",
            "validation_proxy_metrics": "Yes — the study uses proxy metrics (IoU(Op), IoU(Prod), IoU(Dev), Sim(Exec) sequence alignment, Sim(Goal) cosine similarity of serialized representations, Sim(Param) parameter-wise cosine similarity) as proxies for functional correctness rather than direct experimental execution.",
            "human_validation_required": true,
            "human_validation_frequency": "All designed protocols intended to undergo human expert certification for final acceptance; the paper stresses human review is required for certification and particularly for long-tail / risky cases; frequency effectively 'always' for certification in this work and increases in importance with novelty.",
            "formal_verification_used": true,
            "domain_formalization_level": "empirical / semi-formal (experimental laboratory protocols in biology/medical/etc. — real-world procedures with some standardization but not fully formalized like mathematics); DSLs introduce more formal structure but domain remains empirical.",
            "gap_mitigation_strategies": "Introduce hierarchical DSL representations (instance actions, operation-centric interfaces, product-flow-centric models), automated representation generation (non-parametric DPMM + Gaussian Process), and external DSL verifiers that cross-validate operation and product flow; these strategies substantially improve agreement metrics (EE+/EI+ outperform baselines).",
            "evidence_supporting_gap": "Empirical results: pure LLM baselines (FB/IB) achieve low IoU/Sim scores while DSL-constrained + verifier approaches (EI+/EE+) achieve much higher agreement; qualitative observations that LLMs generate protocols similar to existing ones and omit execution-critical details; authors argue verification is necessary as LLMs act like 'novice' scientists.",
            "evidence_contradicting_gap": "Improved representations and verification significantly reduce the gap (EE+/EI+ results), suggesting that the gap can be mitigated by structured representations and verification—however, full machine-only certification remains unattained.",
            "computational_cost_ratio": "No explicit numeric ratio of validation cost to generation cost reported. Paper reports preprocessing/design costs were modest (≈$60 for preprocessing across domains; ≈$10 for machine designer runs) and representation generation required iterative modeling (1,000 iterations; average ~55s/iteration for operation-centric view). Overall statement: representation generation is heavier (offline) while per-protocol generation+verification is relatively low-cost.",
            "uuid": "e2086.0"
        },
        {
            "name_short": "gpt-4o-mini",
            "name_full": "gpt-4o mini (used for preprocessing and design in experiments)",
            "brief_description": "A specific LLM variant (gpt-4o mini) used by the authors for preprocessing (entity classification, NER, synonym merging) and for producing DSL-based designs during experiments; used as the primary LLM in RAG/in-context pipelines reported.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "gpt-4o mini",
            "system_type": "large language model",
            "scientific_domain": "protocol design preprocessing and DSL program generation",
            "output_type": "entity classifications, DSL program text, pseudocode and natural-language protocol steps",
            "novelty_level": "in-distribution generation for preprocessing tasks; used to produce novel protocol drafts but constrained by DSL prompts (paper reports limited OOD capability for novel dependency distributions)",
            "generation_method": "prompted in-context learning, classification and NER tasks during preprocessing, and RAG-assisted generation for protocols",
            "validation_method": "Outputs are validated via the DSL verification loop (external verifier) and human expert review for final certification; gpt-4o-mini also used to categorize opcodes/entities during preprocessing and for synonym detection (human-reviewed merge afterwards).",
            "generation_performance": "Used extensively in preprocessing; monetary cost ~ $60 for preprocessing across four domains, and ~ $10 for machine designer runs. Per-protocol generation performance measured via downstream IoU/Sim metrics when used inside designer pipelines (see corresponding machine-designer entries).",
            "validation_performance": "Not separately reported; validation performed by DSL verifier/human experts rather than by the model itself.",
            "false_positive_rate": "Not reported numerically. Preprocessing steps include synonym merges verified by GPT + human checks to reduce FP/FN.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Perplexity experiments indicate the LLM has higher perplexity on novel protocols (suggesting less confidence/greater uncertainty), but no calibrated uncertainty used.",
            "generation_validation_comparison": "Used as generator component; when paired with DSL-based validation, overall pipeline performance improves — see EE+/EI+ metrics.",
            "uncertainty_quantification": "Perplexity was computed in a memorization test (higher perplexity on novel test set), but no output-level uncertainty scores were used in validation.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Perplexity higher on novel protocols; generation quality degrades for more novel test items (qualitative/aggregate evidence only).",
            "validation_proxy_metrics": "Same proxies (IoU, Sim) used to measure downstream quality of gpt-4o-mini outputs when employed in designers.",
            "human_validation_required": true,
            "human_validation_frequency": "Human expert review for final certification of designed protocols (applies to outputs produced using gpt-4o-mini).",
            "formal_verification_used": true,
            "domain_formalization_level": "empirical / semi-formal",
            "gap_mitigation_strategies": "Constrain gpt-4o-mini outputs with DSL prompts and external verifier; use automated representation generation to improve DSL quality.",
            "evidence_supporting_gap": "Perplexity gap and downstream lower IoU/Sim for baseline LLM-based approaches support generation-validation gap.",
            "evidence_contradicting_gap": "DSL-constrained outputs from gpt-4o-mini show much higher agreement, indicating gap can be reduced with structured constraints.",
            "computational_cost_ratio": "Reported costs: preprocessing ~$60; design ~$10. No explicit validation-to-generation cost ratio provided.",
            "uuid": "e2086.1"
        },
        {
            "name_short": "Auto-DSL (DPMM+GP)",
            "name_full": "Automatic representation generator using hierarchical non-parametric modeling (Dirichlet Process Mixture Model plus Gaussian Process)",
            "brief_description": "Data-driven algorithm that automatically generates the three-level hierarchical DSLs (instance actions, operation-centric interfaces, product-flow models) from domain-specific protocol corpora using hierarchical DPMM for clustering interfaces and GP integration for parameter-value modeling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Automatic representation generator (AutoDSL)",
            "system_type": "hierarchical non-parametric model (Dirichlet Process Mixture Model with Gaussian Process for parameter values)",
            "scientific_domain": "domain-specific DSL induction for experimental protocol representation (biology domains)",
            "output_type": "domain-specific DSL specifications: operation-centric view (interfaces/patterns) and product-flow-centric view (flow units/states)",
            "novelty_level": "method produces new, domain-specific representations (novel in the paper's contribution); not a generator of experimental hypotheses per se but of the representational language which permits novel protocol generation.",
            "generation_method": "hierarchical DPMM clusters instance-level contexts into interface instances; Gaussian Process models parameter value distributions; unification rules merge redundant interfaces (Martelli-Montanari style unification).",
            "validation_method": "Convergence monitored via likelihood curves; qualitative inspection (examples) and downstream utility assessed by improvement in protocol-design metrics when DSLs used by designers; human experts validated test set and final protocols.",
            "generation_performance": "Quantified DSL statistics across domains (e.g., Genetics: 304 operations with avg 7.9 interface instances; Model abstraction states: Genetics 17,190 states; Medical 12,472; Bioengineering 11,418; Ecology 2,205). Convergence curves presented (Fig.2B/C).",
            "validation_performance": "Indirect: DSLs improve downstream protocol-design metrics substantially (encapsulated representations yield higher IoU/Sim scores in design tasks). No standalone FP/FN rates for representation elements are given.",
            "false_positive_rate": "Not reported numerically. Paper discusses qualitative FP/FN issues in model abstraction (property attribution across phases leading to false positives; synonyms causing false negatives) and addresses them via modeling choices (discard interface design for model abstraction).",
            "false_negative_rate": "Not reported numerically.",
            "performance_vs_novelty": "Larger domain corpora appear to yield 'better' DSLs (paper observes Genetics corpus produced best performance; Ecology corpus smaller and DSL less effective), implying representation quality (and thus validation capability) depends on available data and affects ability to handle novel protocols.",
            "generation_validation_comparison": "AutoDSL is a generation tool for representation; comparison is indirect — DSLs produced by AutoDSL enable improved generation+validation pipelines (EE+/EI+) compared to no-DSL baselines.",
            "uncertainty_quantification": "DPMM implicitly models uncertainty via non-parametric clustering; GP models parameter-value uncertainty; no calibrated end-to-end uncertainty scores for protocol validity provided.",
            "calibration_quality": "Not reported as a single metric.",
            "out_of_distribution_performance": "DSLs learned from larger in-domain corpora generalize better; explicit OOD numeric metrics not provided for the representation generator itself.",
            "validation_proxy_metrics": "Uses likelihood convergence and downstream protocol-design performance (IoU/Sim) as proxies for DSL quality.",
            "human_validation_required": true,
            "human_validation_frequency": "Human experts used for testing set selection and ground-truth checking; DSLs themselves are validated primarily by downstream improvement and expert review.",
            "formal_verification_used": true,
            "domain_formalization_level": "semi-formal (DSLs introduce formal structure to an empirical domain)",
            "gap_mitigation_strategies": "Auto-generated hierarchical DSLs provide structured constraints and richer background knowledge to reduce LLM generation errors and aid verification; unification step reduces redundant interfaces.",
            "evidence_supporting_gap": "Paper notes that without structured representation (flattened NL), LLMs only retrieve similar protocols and fail to generate distinct dependency distributions; AutoDSL-generated DSLs alleviate this.",
            "evidence_contradicting_gap": "None directly; AutoDSL improves downstream metrics but does not fully remove need for human certification.",
            "computational_cost_ratio": "Representation generation: 1,000 iterations; avg ~55s/iteration (operation-centric) and ~2s/iteration (product-centric). Monetary LLM preprocessing cost ~ $60. Per-protocol generation/verification cost is small in comparison. No explicit numeric ratio of validation-to-generation costs reported.",
            "uuid": "e2086.2"
        },
        {
            "name_short": "DSL External Verifier (reciprocative verification)",
            "name_full": "Domain-Specific Language external verifier implementing reciprocative verification over operation- and product-flow-centric DSLs",
            "brief_description": "An automated verifier that cross-checks operation pre/postconditions against product-flow states (two-thread reciprocative verification), emits structured error messages when mismatches occur, and drives an LLM feedback-refine loop until verification passes or iteration limit reached.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "DSL External Verifier (Reciprocative Verification)",
            "system_type": "symbolic program verifier / neurosymbolic verifier (DSL-based deterministic checks)",
            "scientific_domain": "protocol design verification (biology/medical/etc.)",
            "output_type": "verification verdicts, structured error messages indicating precondition/postcondition or product-flow mismatches",
            "novelty_level": "novel in this paper as an integrated verifier for dual DSL views",
            "generation_method": "N/A (verification system) — checks consistency between operation DSL programs and product-flow DSL programs via set inclusion and state updates",
            "validation_method": "Deterministic checks: OFVERIFICATION ensures preconditions are satisfied by available products (M(Ω) updates), PFVERIFICATION tracks product generation and property alignment; errors reported back to LLM for refinement; used as an external validation layer.",
            "generation_performance": "Not applicable (verifier does not generate protocols). Its effectiveness is measured by downstream improvements in IoU/Sim when used as external verifier: Encapsulated-External+ (EE+) achieves higher metrics than internal-only variants (statistically significant improvements reported).",
            "validation_performance": "Substantially improves downstream protocol agreement (e.g., modification task EE+ IoU(Prod)=0.661 vs EB/IB lower baselines). Statistical tests show EE+ and EI+ significantly outperform counterparts (paired t-tests, p &lt; .0001).",
            "false_positive_rate": "Not quantified numerically. The verifier can generate false positives (flagging issues that may be naming mismatches rather than real logical errors); prompts instruct LLMs to ignore certain name mismatches when appropriate. Paper explicitly notes DSL verification is not sufficient for certification and may miss long-tail risky cases.",
            "false_negative_rate": "Not quantified. Paper acknowledges data-driven certifiers may miss long-tail failure modes and thus human experts remain required.",
            "performance_vs_novelty": "Verifier reduces performance degradation on novel tasks (planning/modification) compared to LLM-only approaches — demonstrated by higher IoU/Sim across planning/modification/adjustment tasks when verifier is used.",
            "generation_validation_comparison": "Paper demonstrates that adding the external verifier to LLM generation pipeline bridges part of the gap: external-verified outputs (EE/EE+) outperform internal-only prompting. However, verifier cannot fully replace humans for certification.",
            "uncertainty_quantification": "Verifier is deterministic and does not output probabilistic uncertainty; no calibrated uncertainties provided.",
            "calibration_quality": "Not applicable.",
            "out_of_distribution_performance": "Improves robustness on novel (OOD) protocol generation relative to no-verifier baselines (quantified by higher IoU/Sim scores), but absolute performance still limited and human review needed.",
            "validation_proxy_metrics": "Verifier checks structural proxies: pre/postcondition set inclusion and simulated product-flow state transitions rather than real-world experimental outcomes.",
            "human_validation_required": true,
            "human_validation_frequency": "Verifier reduces but does not eliminate need for human certification; human experts still required for final sign-off, especially for long-tail/risky cases.",
            "formal_verification_used": true,
            "domain_formalization_level": "semi-formal (DSL enables formal-like program checks in an empirical domain)",
            "gap_mitigation_strategies": "Use of dual-view DSL and reciprocative verification as an external constraint layer and feedback to LLMs; instruct LLM to refine until verifier passes.",
            "evidence_supporting_gap": "Despite verifier improvements, authors explicitly state DSL verification is insufficient for certification and human experts remain necessary — supporting existence of a generation-validation gap for high-stakes outputs.",
            "evidence_contradicting_gap": "Significant metric gains when verifier is used show the gap can be materially narrowed with appropriate representation and verification.",
            "computational_cost_ratio": "Not reported explicitly; verifier is computationally cheap relative to offline DSL generation and LLM costs, and is used within iterative feedback loops (cost per iteration modest).",
            "uuid": "e2086.3"
        },
        {
            "name_short": "Flatten-Baseline (FB)",
            "name_full": "Flatten representation with Baseline LLM + RAG (FB)",
            "brief_description": "A pure LLM baseline using Retrieval-Augmented Generation on original natural-language protocol corpora to generate new protocol steps/pseudocode without structured DSL constraints or external verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Flatten-Baseline (FB)",
            "system_type": "large language model with RAG",
            "scientific_domain": "protocol design (biology domains)",
            "output_type": "natural-language protocols and pseudocode",
            "novelty_level": "mostly in-distribution / retrieves and adapts similar existing protocols; poor at producing distinct dependency distributions",
            "generation_method": "RAG: retrieve top-3 similar protocols from corpora and prompt LLM to generate a new protocol based on them",
            "validation_method": "None automated in pipeline (no DSL-based verification); human experts used post-hoc for certification in study design but not as part of automatic pipeline",
            "generation_performance": "Reported metrics (examples): Modification task: IoU(Op)=0.181, IoU(Prod)=0.050, IoU(Dev)=0.038, Sim(Exec)=0.304, Sim(Goal)=0.796, Sim(Param)=0.809 (standard errors in tables). Adjustment and planning tasks also show low operation/product IoUs relative to DSL-based methods.",
            "validation_performance": "No automated validation; downstream agreement metrics are low relative to DSL-constrained approaches.",
            "false_positive_rate": "Not reported numerically; qualitatively high risk of missing critical configuration details and producing implausible/incomplete steps.",
            "false_negative_rate": "Not reported numerically.",
            "performance_vs_novelty": "Performs poorly on more novel test items (planning), as baseline LLM tends to retrieve similar protocols rather than invent new valid dependency structures.",
            "generation_validation_comparison": "Baseline generation without verification underperforms methods that include DSL constraints and verification (II, EI, EE+).",
            "uncertainty_quantification": "Not used.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Poor relative to DSL-enhanced systems; specific IoU/Sim numbers in tables show gap.",
            "validation_proxy_metrics": "Downstream IoU/Sim metrics used to evaluate generation quality.",
            "human_validation_required": true,
            "human_validation_frequency": "Human experts used to construct testing ground truth and for certification; baseline outputs would also require human correction.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / semi-formal (flat NL representation)",
            "gap_mitigation_strategies": "Not present in this baseline.",
            "evidence_supporting_gap": "Low IoU/Sim results demonstrate generation-only approach is insufficient; LLMs produce outputs requiring manual correction.",
            "evidence_contradicting_gap": "None.",
            "computational_cost_ratio": "Reported LLM costs for design modest (~$10), but no explicit generation-vs-validation cost ratio since no automated validation used.",
            "uuid": "e2086.4"
        },
        {
            "name_short": "Instance-Baseline (IB)",
            "name_full": "Instance actions representation with Baseline LLM retrieval (IB)",
            "brief_description": "LLM with RAG retrieving instance-action-structured protocol fragments (instance-level key-value attributes) and generating protocol pseudocode; representation improves syntactic detail but limits reuse/generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Instance-Baseline (IB)",
            "system_type": "large language model with RAG over structured instance-action corpora",
            "scientific_domain": "protocol design",
            "output_type": "pseudocode based on instance-action building blocks",
            "novelty_level": "limited novelty; relies on exact or near-exact instance matches (low generalization)",
            "generation_method": "Retrieval of instance-action fragments then LLM synthesis into protocol pseudocode",
            "validation_method": "No external DSL verification in IB pipeline; evaluation done by comparison to ground truth and human checks",
            "generation_performance": "Example metrics (modification task): IoU(Op)=0.150, IoU(Prod)=0.038, IoU(Dev)=0.039, Sim(Exec)=0.281, Sim(Goal)=0.772, Sim(Param)=0.788.",
            "validation_performance": "Low; better than Flatten only in some respects but substantially worse than encapsulated representations.",
            "false_positive_rate": "Not reported numerically; qualitative limitation: over-specific instance actions lead to low matching for novel contexts (effective false negatives for reuse).",
            "false_negative_rate": "Not reported numerically.",
            "performance_vs_novelty": "Struggles on novel tasks where exact instance matches are unlikely (planning tasks particularly affected).",
            "generation_validation_comparison": "Outperformed by higher-level representations (II, EI, EI+, EE+), indicating that increasing abstraction improves generation and downstream validation metrics.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Poor due to lack of generalization; metrics show lower IoU/Sim.",
            "validation_proxy_metrics": "Same proxy metrics used for evaluation.",
            "human_validation_required": true,
            "human_validation_frequency": "Human experts involved in evaluating outputs; required for certification.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical with structured instance representation",
            "gap_mitigation_strategies": "Not in IB; improved by moving to operation-centric and product-flow-centric DSLs.",
            "evidence_supporting_gap": "Low downstream metrics indicate generation alone insufficient.",
            "evidence_contradicting_gap": "None.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2086.5"
        },
        {
            "name_short": "Instance-Internal (II)",
            "name_full": "Instance representation used as internal LLM prompt constraints (II)",
            "brief_description": "LLM prompted with instruction set assembly (ISA) of instance actions (structured instance-level DSL included in prompt) so generation is constrained to use provided instance-action primitives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Instance-Internal (II)",
            "system_type": "large language model with internal prompting using instance-level representation",
            "scientific_domain": "protocol design",
            "output_type": "pseudocode/DSL-like program using provided instance primitives",
            "novelty_level": "improved over IB but still limited; more structured prompts help, but abstraction limited",
            "generation_method": "Prompting LLM with instance-action ISA and examples; LLM must synthesize a protocol using given pseudofunction definitions",
            "validation_method": "No external verifier; evaluation uses corpus ground-truth comparisons and human checks",
            "generation_performance": "Example metrics (modification task): IoU(Op)=0.331, IoU(Prod)=0.101, IoU(Dev)=0.061, Sim(Exec)=0.416, Sim(Goal)=0.802, Sim(Param)=0.851.",
            "validation_performance": "Better than flat baselines but inferior to operation-centric and dual-view methods.",
            "false_positive_rate": "Not numerically reported. Qualitative: internal constraint reduces some generation errors but cannot enforce inter-step product coherence.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Outperforms simple baselines on less-novel tasks; still degrades on planning (more novel) tasks.",
            "generation_validation_comparison": "Internal prompting helps, but external verification and higher-level abstraction deliver larger gains.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Intermediate; improved over FB/IB but below EI/EE+.",
            "validation_proxy_metrics": "IoU/Sim metrics used.",
            "human_validation_required": true,
            "human_validation_frequency": "Human review used for final certification.",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal (instance-level structured prompts)",
            "gap_mitigation_strategies": "Internal prompting constrains generation; less effective than structural DSL + external verifier.",
            "evidence_supporting_gap": "II improves over baseline but still requires verification (evidence of generation-validation gap persists).",
            "evidence_contradicting_gap": "None.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2086.6"
        },
        {
            "name_short": "Encapsulated-Internal (EI)",
            "name_full": "Operation-centric DSL used as internal prompt to LLM (EI)",
            "brief_description": "LLM prompted with operation-centric DSL (function abstractions/interfaces) as part of input, producing DSL-instantiated protocols constrained by operation interfaces; internal approach (no external verifier).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Encapsulated-Internal (EI)",
            "system_type": "LLM with internal DSL prompts (operation-centric)",
            "scientific_domain": "protocol design",
            "output_type": "operation-centric DSL program instantiations",
            "novelty_level": "more capable of generating novel compositions due to operation-level abstraction (moderately novel), but still benefits from product-flow verification for inter-step coherence",
            "generation_method": "Prompting LLM with operation-centric DSL patterns and examples; LLM instantiates interfaces to assemble protocols",
            "validation_method": "Internal constraints only; no external DSL verifier in EI variant; outputs evaluated against ground truth and via human review",
            "generation_performance": "Strong improvement over lower-level methods. Example (modification task): IoU(Op)=0.593, IoU(Prod)=0.318, IoU(Dev)=0.336, Sim(Exec)=0.602, Sim(Goal)=0.866, Sim(Param)=0.937.",
            "validation_performance": "Better than II/IB/FB but slightly below external verification variants (EE/EE+).",
            "false_positive_rate": "Not numerically reported; internal constraints reduce some false positives but do not check product-flow continuity across steps.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Handles more novel compositions better than instance-level methods, but still benefits from product-flow checks on highly novel tasks.",
            "generation_validation_comparison": "EI outperforms II; however, adding external product-flow verification (EI+) further improves results significantly.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Improved relative to baselines; numeric improvements shown in tables but absolute OOD capability limited.",
            "validation_proxy_metrics": "IoU/Sim metrics used.",
            "human_validation_required": true,
            "human_validation_frequency": "Human certification required for final acceptance.",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal (operation-centric DSL)",
            "gap_mitigation_strategies": "Operation-level abstraction reduces generation errors; adding product-flow view + external verifier (EI+) yields larger gains.",
            "evidence_supporting_gap": "Internal-only approach still leaves inter-step product coherence issues unverified.",
            "evidence_contradicting_gap": "Improved generation quality shows structured representation narrows the gap.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2086.7"
        },
        {
            "name_short": "Encapsulated-Internal+ (EI+)",
            "name_full": "Dual representation (operation + product-flow) used as internal prompt to LLM (EI+)",
            "brief_description": "LLM prompted with both operation-centric and product-flow-centric DSLs (dual view) included in the prompt, producing DSL programs for operations and product flow alternately; internal approach without external verifier but with dual consistency constraints provided in prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Encapsulated-Internal+ (EI+)",
            "system_type": "LLM with internal dual-view DSL prompting",
            "scientific_domain": "protocol design",
            "output_type": "dual-view DSL programs (operation and product-flow representations)",
            "novelty_level": "moderately novel; dual view improves ability to design protocols for novel objectives and increases reuse of operations across contexts",
            "generation_method": "Prompting LLM with both DSL views and examples to instantiate coherent operation/product-flow alternations",
            "validation_method": "Internal prompt instructions encourage cross-checks, but no external deterministic verifier; final outputs still compared to ground truth and reviewed by humans",
            "generation_performance": "Strong performance. Example (modification task): EI+ IoU(Op)=0.648, IoU(Prod)=0.626, IoU(Dev)=0.413, Sim(Exec)=0.765, Sim(Goal)=0.883, Sim(Param)=0.952 (std errors reported in tables).",
            "validation_performance": "High relative to other internal methods; addition of external verifier (EE+) gives further improvement but EI+ already strong.",
            "false_positive_rate": "Not reported numerically. Dual view reduces many false positives related to inter-step incoherence but cannot eliminate long-tail issues.",
            "false_negative_rate": "Not reported.",
            "performance_vs_novelty": "Performs well on planning/modification/adjustment tasks; dual view narrows performance drop for novel tasks compared to single-view or flat baselines.",
            "generation_validation_comparison": "EI+ significantly outperforms II and FB baselines (statistically significant), showing representation + prompting can narrow the gap though external verifier helps further.",
            "uncertainty_quantification": "Not reported.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Better OOD robustness than simpler methods; quantified by higher IoU/Sim on novel test sets.",
            "validation_proxy_metrics": "IoU/Sim metrics used.",
            "human_validation_required": true,
            "human_validation_frequency": "Human certification required for final acceptance; frequency and criticality increase with novelty.",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal (dual DSL prompting)",
            "gap_mitigation_strategies": "Dual representation in prompt reduces incoherence and improves inter-step consistency; effective even without external verifier but best combined with verifier.",
            "evidence_supporting_gap": "Even with EI+, authors state human certification still needed and DSL-level verification insufficient for full certification.",
            "evidence_contradicting_gap": "High EI+ performance demonstrates generation can approach validation when constrained by good representations.",
            "computational_cost_ratio": "Not reported.",
            "uuid": "e2086.8"
        },
        {
            "name_short": "Encapsulated-External (EE)",
            "name_full": "Operation-centric DSL with external verifier (EE)",
            "brief_description": "LLM generates protocols under operation-centric DSL constraints and then an external DSL verifier checks pre/postconditions; verifier feedback is used iteratively to refine the LLM output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Encapsulated-External (EE)",
            "system_type": "LLM + external DSL verifier (operation-centric)",
            "scientific_domain": "protocol design",
            "output_type": "operation-centric DSL programs validated via external verifier",
            "novelty_level": "moderately novel for composition tasks; verification enforces structural soundness",
            "generation_method": "LLM prompted with operation-centric DSL; RAG/in-context; generated programs are sent to external verifier which checks pre/postcondition coherence",
            "validation_method": "External DSL verifier checking that preconditions are satisfied by previous outputs and that outputs are used subsequently; mismatch errors returned to LLM for iterative refinement",
            "generation_performance": "Strong. Example (modification task): EE IoU(Op)=0.588, IoU(Prod)=0.403, IoU(Dev)=0.332, Sim(Exec)=0.601, Sim(Goal)=0.873, Sim(Param)=0.940.",
            "validation_performance": "Improves correctness relative to internal-only; however dual-view external (EE+) yields even higher performance.",
            "false_positive_rate": "Not numerically reported; verifier may still miss long-tail experimental risks and cannot guarantee zero false positives for real-world execution.",
            "false_negative_rate": "Not reported numerically.",
            "performance_vs_novelty": "External verification improves robustness to novelty relative to internal-only variants, but some novel cases still require human intervention.",
            "generation_validation_comparison": "EE outperforms internal-only (EI) and instance-based approaches; EE+ (dual-view external) outperforms EE.",
            "uncertainty_quantification": "Verifier deterministic; no probabilistic uncertainty estimates.",
            "calibration_quality": "Not applicable.",
            "out_of_distribution_performance": "Improved compared to no-verifier baselines (quantified by IoU/Sim), but absolute OOD robustness limited.",
            "validation_proxy_metrics": "Deterministic DSL checks and downstream IoU/Sim.",
            "human_validation_required": true,
            "human_validation_frequency": "Human certification required for final acceptance.",
            "formal_verification_used": true,
            "domain_formalization_level": "semi-formal",
            "gap_mitigation_strategies": "External verification reduces generation errors by enforcing pre/postcondition consistency and driving iterative LLM refinement.",
            "evidence_supporting_gap": "Although EE improves results, authors emphasize verifier alone is insufficient for certification, implying generation still outpaces safe validation.",
            "evidence_contradicting_gap": "EE demonstrates substantial reduction of generation errors relative to baselines.",
            "computational_cost_ratio": "Not explicitly quantified; verifier adds iterative cost but per-iteration overhead is small compared to offline DSL generation and LLM inference.",
            "uuid": "e2086.9"
        },
        {
            "name_short": "Encapsulated-External+ (EE+)",
            "name_full": "Dual-view (operation + product-flow) DSL with external verifier (EE+)",
            "brief_description": "Combined pipeline using both operation-centric and product-flow-centric DSLs with an external reciprocative verifier that cross-checks operations and product-flow units; iterative refine loop with LLM produces the highest-performing automated protocol designs in the study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Encapsulated-External+ (EE+)",
            "system_type": "neurosymbolic pipeline: LLM + dual-view DSL + external verifier",
            "scientific_domain": "protocol design across Genetics, Medical, Bioengineering, Ecology",
            "output_type": "dual-view DSL protocol programs (operations and product flows), refined iteratively",
            "novelty_level": "moderately to substantially novel within the constrained DSL space; best empirical handling of novel protocol generation among tested systems",
            "generation_method": "LLM generates DSL program instantiations for both views; external reciprocative verifier checks inter-view consistency and returns structured feedback for iterative refinement (self-refine loop)",
            "validation_method": "Reciprocative verification: OFVERIFICATION checks operation pre/postcondition satisfaction; PFVERIFICATION tracks product flow states and properties; verifier emits errors prompting LLM refinement until program passes or iteration limit reached; final human certification required.",
            "generation_performance": "Best overall in experiments. Example (modification task): EE+ IoU(Op)=0.640, IoU(Prod)=0.661, IoU(Dev)=0.410, Sim(Exec)=0.757, Sim(Goal)=0.893, Sim(Param)=0.953 (standard errors reported). Similar high performance across planning and adjustment tasks in appendix tables.",
            "validation_performance": "High relative to all other tested systems; paired t-tests show EE+ significantly outperforms EE and II (p &lt; .0001) and other baselines, demonstrating that combined representation + verification materially improves validation proxies.",
            "false_positive_rate": "Not given numerically; paper cautions that DSL verification still may yield false positives/negatives for real experimental risk and cannot fully replace human experts.",
            "false_negative_rate": "Not numerically reported.",
            "performance_vs_novelty": "EE+ narrows the performance drop on novel (planning) tasks compared to baselines and internal-only methods; still requires human review for certification but shows best OOD robustness among tested pipelines.",
            "generation_validation_comparison": "Directly demonstrates that adding structured representation and external verification bridges much of the generation-validation divide: EE+ has much higher IoU/Sim than generation-only baselines.",
            "uncertainty_quantification": "No probabilistic uncertainty quantification provided; verifier is deterministic while LLM uncertainty not used.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Best of all tested methods on novel test subsets (planning/modification tasks), as shown by higher IoU/Sim numbers in tables; no additional OOD metrics (e.g., AUC) are provided.",
            "validation_proxy_metrics": "Uses DSL consistency checks as validation plus proxy agreement metrics (IoU/Sim) against held-out ground truth protocols.",
            "human_validation_required": true,
            "human_validation_frequency": "Final human certification required for all generated protocols; human review remains especially necessary for long-tail and potentially dangerous cases.",
            "formal_verification_used": true,
            "domain_formalization_level": "semi-formal (dual DSL provides strong structural formalization but domain experimental outcomes remain empirical)",
            "gap_mitigation_strategies": "Dual-view DSL + reciprocative external verification + iterative LLM refinement (self-refine loop) — empirically effective at reducing generation errors and improving agreement with ground truth.",
            "evidence_supporting_gap": "Authors explicitly note that DSL verification is not sufficient for certification and human experts remain essential — supporting existence of remaining gap despite improvements.",
            "evidence_contradicting_gap": "High EE+ performance demonstrates substantial mitigation of the gap; metrics show statistically significant gains.",
            "computational_cost_ratio": "Not provided as a single number. Paper reports representation generation (offline) is the most computationally/time-consuming step (1,000 iterations, 55s/iteration for op-centric DSL), while per-protocol generation+verification costs are modest (LLM inference and verifier checks).",
            "uuid": "e2086.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        },
        {
            "paper_title": "Bioplanner: Automatic evaluation of llms on protocol planning in biology",
            "rating": 2
        },
        {
            "paper_title": "Augmenting large language models with chemistry tools",
            "rating": 1
        },
        {
            "paper_title": "An integrated self-optimizing programmable chemical synthesis and reaction engine",
            "rating": 1
        },
        {
            "paper_title": "Reconfigurable system for automated optimization of diverse chemical reactions",
            "rating": 1
        }
    ],
    "cost": 0.03012575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HIERARCHICALLY ENCAPSULATED REPRESENTATION FOR PROTOCOL DESIGN IN SELF-DRIVING LABS
4 Apr 2025</p>
<p>Yu-Zhe Shi 
Department of Advanced Manufacturing and Robotics
Peking University</p>
<p>Mingchen Liu 
School of Computer Science and Technology
Huazhong University of Science and Technology ⋆ Equal contribution</p>
<p>Fanxu Meng 
Department of Advanced Manufacturing and Robotics
Peking University</p>
<p>Qiao Xu 
Zhangqian Bi 
Department of Advanced Manufacturing and Robotics
Peking University</p>
<p>School of Computer Science and Technology
Huazhong University of Science and Technology ⋆ Equal contribution</p>
<p>Kun He 
School of Computer Science and Technology
Huazhong University of Science and Technology ⋆ Equal contribution</p>
<p>Lecheng Ruan ruanlecheng@ucla.edu 
Department of Advanced Manufacturing and Robotics
Peking University</p>
<p>Qining Wang qiningwang@pku.edu.cn 
Department of Advanced Manufacturing and Robotics
Peking University</p>
<p>HIERARCHICALLY ENCAPSULATED REPRESENTATION FOR PROTOCOL DESIGN IN SELF-DRIVING LABS
4 Apr 2025EDD38CD17EB8C7C912EC6439E3D4B78EarXiv:2504.03810v1[cs.AI]
Self-driving laboratories have begun to replace human experimenters in performing single experimental skills or predetermined experimental protocols.However, as the pace of idea iteration in scientific research has been intensified by Artificial Intelligence, the demand for rapid design of new protocols for new discoveries become evident.Efforts to automate protocol design have been initiated, but the capabilities of knowledge-based machine designers, such as Large Language Models, have not been fully elicited, probably for the absence of a systematic representation of experimental knowledge, as opposed to isolated, flatten pieces of information.To tackle this issue, we propose a multi-faceted, multiscale representation, where instance actions, generalized operations, and product flow models are hierarchically encapsulated using Domain-Specific Languages.We further develop a data-driven algorithm based on non-parametric modeling that autonomously customizes these representations for specific domains.The proposed representation is equipped with various machine designers to manage protocol design tasks, including planning, modification, and adjustment.The results demonstrate that the proposed method could effectively complement Large Language Models in the protocol design process, serving as an auxiliary module in the realm of machine-assisted scientific exploration.</p>
<p>INTRODUCTION</p>
<p>The rapid advancement of Artificial Intelligence (AI) models for the assistance of scientific discovery (Wang et al., 2023b) has precipitated an increased demand for rapid iteration of ideas, from the generation to the verification of hypotheses.Although AI models have expedited the process of hypothesis generation, the validation phase still requires intensive empirical experimentation from human.The concept of self-driving laboratory has been introduced to substantially accelerate the validation process, in organic chemical synthesis (Mehr et al., 2020;Burger et al., 2020), cell biology for medical research (Kanda et al., 2022), and novel material discovery (Szymanski et al., 2023).With the expertise and effort of experimental scientists and automation engineers, mobile robots and Internet of Things (IoT) pipelines are configured to perform a sequence of actions in accordance with a detailed description of the specific experimental procedure, referred to as the protocol.</p>
<p>While existing protocols suffice for some experimental tasks, discovery processes often demand a higher degree of specificity, including: (i) confirmation of unverified experimental objectives to seek specific findings; (ii) testing parallel hypotheses or solutions; and (iii) replication of established experiments within the constraints of available laboratory resources.These necessitate the design of new protocols, going beyond the reuse of existing ones available in the protocol databases.Particularly, this includes the planning of novel protocols, and the modification and adjustment of current protocols as appropriate, respectively.Unfortunately, self-driving laboratories currently only execute isolated and duplicated experimental skills (Bédard et al., 2018;Steiner et al., 2019), or pre-specified protocols with sequential actions (Rohrbach et al., 2022;Manzano et al., 2022).Any innovation in protocols imposes intensive manual design burden (McNutt, 2014;Baker, 2016), potentially becoming a bottleneck in accelerating scientific discovery.Consequently, there is a quest for the automatic design of protocols tailored to specific goals for self-driving laboratories.</p>
<p>Designing new protocols is a non-trivial task even for human scientists.Novice scientists tend to adhere strictly to established protocols and may be at a loss when faced with the need for variations, from minor adjustments like different available devices to more significant shifts in the overall experimental goal.In contrast, veteran scientists typically have the capability to create or modify protocols as needed, from variations in available resources ("what I have") to desired outcomes ("what I want"), even in situations where a similar protocol was not encountered before.</p>
<p>The distinction arises because veteran scientists possess a systematic understanding of every ingredient and procedure, contextualizing them globally within the domain of experiment.They know "what kind of ingredient is used for what purposes" and "what kind of operation is used under what conditions', while novice scientists mechanically memorize the sequential execution orders and corresponding parameters in a local context.This systematic understanding, or conceptual knowledge (Ryle &amp; Tanney, 1949), includes the background knowledge of ingredients and atomic operations, as well as the relationships between them.Experienced experimental scientists develop such conceptual knowledge as a representation for protocol design (McCarthy, 1959), which serves as the vehicle for reasoning processes.Reasoning over conceptual knowledge leverages the rich context of generalized, abstracted concepts of ingredients and operations rather than specified, instantialized ones, which spans a semantic space where originally isolated dots are connected with each other, thereby enhancing the simplicity and flexibility of protocol design (Boden, 1980;Newell, 1982).In summary, veteran scientists' capability to design new protocols stems from an appropriate representation of background knowledge that supports reasoning processes (see Fig. 1A).</p>
<p>To implement automatic protocol design on machines, a reasonable choice may be leveraging a Large Language Model (LLM).Trained on extensive corpora, including scientific documents, LLMs possess the potential to facilitate protocol design with the corresponding background knowledge (AI4Science &amp; Quantum, 2023).Recently, researchers have made beneficial attempts to design new protocols using LLMs based on descriptions of new experimental goals (Boiko et al., 2023;M. Bran et al., 2024).Regrettably, benchmarking results indicate that the expected capability of LLMs in protocol design is not fully elicited (O'Donoghue et al., 2023).One significant limitation is that LLMs excel at generating new protocols similar to existing ones, i.e., protocols with similar sequential execution orders, but fail to generate those with distinct dependency distributions.This limitation hampers LLMs in scenarios where experimental goals change in high intensity.Another limitation is that the generated protocols sometimes lose critical configuration details for operation execution, necessitating manual correction.These empirical evidences suggest that LLMs exhibit limitations akin to those of novice human experts, implying that LLMs may necessitate a more suitable representation of background knowledge to fully unleash their potential in protocol design.</p>
<p>Protocol design is a multi-faceted, multi-scale effort requiring the integration of information from different perspectives, from low-level to high-level.This information includes detailed configurations of each atomic operation, temporal relationships between atomic operations, the scope of application for atomic operations with the same reference name, and the reactive relationships between reagents and operations.While LLMs undoubtedly capture such knowledge from their training corpora, the pieces of knowledge remain isolated, unorganized, and not articulated.These flatten background knowledge, rather than conceptual knowledge, hinders LLMs from flying over a global view of the novel objectives and diving into the details of operations.Therefore, we propose developing a multi-faceted and multi-scale representation for protocol design that provides the designer, such as LLMs, with a vehicle to reason over conceptual knowledge of ingredients and procedures.</p>
<p>We draw inspiration from both cognitive science literature on rationality (Monsell, 2003;Griffiths, 2020), which suggests that we cannot consider information from different views and scales in a single thread (Shi et al., 2023a).We also learn from computer science literature on hierarchical abstraction (Liskov, 1987), which indicates that higher-level abstraction semantics possess more powerful expressivity compared to their lower-level counterparts (Abelson &amp; Sussman, 1996;Hopcroft et al., 1996).Combining these insights, we suggest that our desired representation should encapsulate information of different granularities in corresponding hierarchies of abstraction, gaining global design insights with higher-level semantics while completing execution configurations with lowerlevel semantics.Specifically, we investigate three levels of encapsulation (see Fig. 3B).Starting from the set of original protocols, namely the basic level, we have (i) protocol element instantialization, which decomposes full protocols into instance operations with attributes, within the local context of the specific protocol, resulting a structural representation of the elementary information; (ii) function abstraction, which offers an operation-centric view that generalizes the precondition, postcondition, and execution configurations of each operation in the global context of the experiment domain, resulting a sequential representation of the operations; (iii) model abstraction, which offers an reagent and intermediate product centric view that unifies the status transitions in the global context of the experiment domain, resulting a continuous representation of the experimental environ- ment.This hierarchical structure provides the designer with a representation to consider all possible associations among operations, among products, and between operations and products, with a high degree of freedom, by disentangling originally intertwined information.We implement the representation using Domain-Specific Languages (DSLs).The hierarchical syntax of DSLs maintains both the abstract semantics at the high-level and the precise information at the low-level.Furthermore, the compositionality of DSL syntax facilitates the flexible protocol designs, addressing the "flying over global views" requirement; while DSL program verification over the generated protocols upholds their soundness and completeness; addressing the "diving into details" requirements.</p>
<p>However, the proposed representation does not come without drawbacks-it can be highly dependent on domain-specific knowledge (Mernik et al., 2005;Fowler, 2010).The distributions of reagents, operations, and execution dependencies vary significantly across different domains in experimental sciences, such as Genetics, Medical, Bioengineering, and Ecology.Manually crafting DSLs specialized for these domains requires deep integration between domain experts and programming language experts, which is labour-intensive, case-by-case, and costly (Shi et al., 2024a;b).This obstacle hinders the application of our representation to a broader set of domains (Shi et al., 2024d).</p>
<p>To make the representation specification more affordable, we develop an algorithm that conducts multi-hierarchy encapsulation automatically driven by the domain-specific corpus of existing protocols.Ultimately, we may be able to take a critical step toward closing the loop of autonomous scientific discovery by establishing these two building blocks: (i) the automatic generation of representation for protocol design; and (ii) the automatic designer working on the representation.</p>
<p>Our contributions in this work are three-fold: (i) we identify the problem of representation for protocol design and develop a hierarchically encapsulated representation for protocol design (Sec.2);</p>
<p>(ii) we propose a data-driven algorithm that automatically generates the representation for protocol design specialized for the domain of application (Sec.3); and (iii) we demonstrate the utility of the resulting representation by conducting protocol planning, modification and adjustment tasks using a variety of machine designers across different domains (Sec. 4).This further indicates that our proposed automatic representation generation approach possesses the potential to function as an auxiliary module for LLMs, enhancing their capability on protocol design.</p>
<p>REPRESENTATION FOR PROTOCOL DESIGN</p>
<p>In this section, we describe our representation for protocol design (see Fig. 1B).We first formulate the basic protocol design problem in Sec.2.1.Afterwards, starting from the original full protocol, we introduce the three hierarchies of representations: (i) structural representation, i.e., instance actions with attributes (Sec.2.2); (ii) sequential operation-centric representation, i.e., function abstraction (Sec.2.3); and (iii) continuous product-flow-centric representation, i.e., model abstraction (Sec.2.4).Furthermore, we describe how the dual representation of operation-centric and product-flow-centric views reciprocatively facilitates the verification of the designed protocols in Sec.2.5.</p>
<p>THE PROTOCOL DESIGN PROBLEM</p>
<p>Protocol design problem PD = (Φ | ω * , P, Ω) is generating a desired protocol Φ given the new coming experimental objective ρ, domain of experiment P, and available reagents Ω.A protocol Φ = ⟨φ 1 , φ 2 , . . .⟩ is a sequence of experimental steps φ t .An experimental objective ω * is the expected final product of the experiment.Experimental objectives can range from preparing a desired product, to testing the significance of a specific hypothesis and detecting a predicted behavior, with the latter two potentially followed by additional standalone steps for property test, observation, and interpretation.We denote domains of experiment as P, which influences the distributions of protocols by means of the distributions of operations, reagents, and execution orders, etc.The set of available reagents Ω includes originally accessible reagents and excludes those requiring production.</p>
<p>INSTANCE ACTIONS WITH ATTRIBUTES</p>
<p>Protocols are originally represented in Natural Language (NL), which is the representation suitable for humans' comprehension, but not for machines (Bartley et al., 2023).Without a syntax decomposing a NL-based protocol into information elements precisely, machines are likely to capture only overall, coarse-grained information of protocols and may only retrieve within existing protocols for the one that is most similar to the new experimental objective.Consequently, according to the standards and conventions of experimental sciences (Baker, 2021), the prerequisite of representation for a machine protocol designer should be a structural representation which decompose NL-based protocols into instance actions with attributes {φ t | (φ prec t , φ post t , φ exec t )}.The instance actions are decomposed by execution order and their attributes are the exact context for their execution, namely the precondition φ prec t , i.e., the availability of resources required for this action, postcondition φ post t , i.e., resulting product of the operation, and execution configurations φ exec t .Execution configurations includes the configuration parameters and their corresponding values, e.g., the device for conducting the operation and required experimental conditions such as duration, acidity, and lightening.An instance action can be reusable in another protocol once the execution context is matched.</p>
<p>With such reusability, we are on the first time to have building blocks for constructing a new protocol rather than retrieving existing ones.These building blocks capture fine-grained execution configuration parameters through maintaining the nested data structures of key-value pairs.This structural representation serves as a syntactic constraint on the preciseness of designed protocols.Practical attempts have been made echoing this idea (O'Donoghue et al., 2023;Leonov et al., 2024).</p>
<p>OPERATION-CENTRIC VIEW WITH FUNCTION ABSTRACTION</p>
<p>The reusability of instance actions with attributes is highly limited, as their semantics are highly specified in the low-level.The total amount of the instance actions can be extremely high, i.e., about 150K per domain, thus the probability of the exact matching between execution contexts can be extremely low.Consider the three different instance actions with attributes "Homogenization of mouse liver tissue using a bead mill", "Homogenization of bacterial cell suspension using an ultrasonic homogenizer", and "Homogenization of bacterial air samples using a nebulizer".Although they come with totally different preconditions, postconditions, and execution configurations, particularly the required device varying according to the phase of the experimental subject, they share the semantic identifier "Homogenization" for reference.Sharing semantic identifier indicates that these instance actions share the same purpose on the semantics level.In experimental sciences, "Homogenization" always refers to the breakdown of a sample into a uniform mixture.Whether it's tissue, cell suspension, or gas doesn't change the purpose of the operation.This is critical for protocol design, since it essentially requires satisfying the ultimate goal through a series of subgoals.Therefore, the desired representation should generalize the semantics of operations to any possible contexts in the corresponding domain of experiment, rather than only specific contexts.</p>
<p>We implement such generalization by encapsulating varied instances of preconditions, postconditions, and execution configurations into an interface for the operation.Namely, we refer to an operation with semantic identifier φ through an interface ϕ to a set of execution contexts, in the form of ⟨φ → ϕ → {(φ prec , φ post , φ exec )}⟩.The operation φ can be grounded to a corresponding instance action in any matched execution contexts, echoing modular design (Hirtz et al., 2002).The reusability of encapsulated operations comes with greater significance than that of instance actions, as there are only about 1K operations per domain in total, which is only 1/150 of that of instance actions.As flexible building blocks, operations can be easily fitted into any breakpoints with suitable preconditions and postconditions in the constructing experiment sequence.This sequential representation of the operations serves as a semantic constraint on the compact permissible set of primitives for protocol design (Shi et al., 2023b), maintaining both degree of freedom and correctness.</p>
<p>PRODUCT-FLOW-CENTRIC VIEW WITH MODEL ABSTRACTION</p>
<p>Sequence of operations make up of protocols.However, operations are the methods to realize rather than the objectives to achieve.For experimental objectives of testing, preparing, or detecting (Schwab &amp; Held, 2020), the common focus is always the specific status of final product, not the operations.Starting from initial reagents, the status of product flow is manipulated step-by-step by the operations, till the final product.Unfortunately, the information of product status transition is latent in protocols and is twisted with descriptions of experimental steps.For the operation-centric view, the transitions of product flow statuses remains a black box environment.For example, the operation description "Centrifuge the tubes at 15,000 x g for 20 minutes" does not directly reveal the transition from product in mixture status to products in distinct phases.The lack of coherent tracking of the product flow is problematic of protocol design, as the product flow holds spatialtemporal invariance, just the same as the general physical environment.Status transitions of the product flow are primarily caused by the effects of operations, thereby it serves as the invariant in executing the protocol from the perspective of programming.Therefore, the desired representation should also serve as the model interacting with the sequence of operations.</p>
<p>To disentangle product status from their latent representation in the operation-centric view, we propose an explicit product flow centric view that tracks the status of the product flows with detail, such as component, volume, container, and other physical and chemical properties of the product, and also the predecessor operation that yields the product and the successor operation that takes the product as input.Each product flow unit, i.e., one individual component in the product flow between two adjacent steps, is an instance with attributes {ω t | (ω pred t , ω succ t , ω prop t )}.Analogous to the generalization of operations' semantics, product flow units share commonalities between components with the same semantic identifier for reference-they may share a specific range of predecessor operations ω pred and successor operations ω succ , and a selected set of key properties to consider ω prop .For example, the "supernatant" is usually generated by a "centrifugation" operation, passing into "filtration" or "spectrophotometric analysis", and focusing on the properties acidity and viscosity rather than other possible properties.Thus, we encapsulate the information of contexts and properties into the semantics of product flow units, in the form of ⟨ω → (ω pred , ω succ , ω prop )⟩.As solid pipelines bridging the building blocks, product flow units can verify the coherency of the entire designed protocol.This continuous representation of the environments serves as a program verifier, checking the prerequisite and simulating the effect of each operation, alleviating unpredictable behaviors among the interaction between operations and product flows.</p>
<p>RECIPROCATIVE VERIFICATION OVER THE DUAL REPRESENTATION</p>
<p>Algorithm 1 Reciprocative Verification procedure OFVERIFICATION(M , φ) ▷ Check that the pre/ post conditions are met The dual representation of operation-centric and product-flow-centric views intrinsically equips with a verification mechanism through a reciprocative process akin to two interacting threads.The first thread focuses on verifying the operation flow, taking as input an operation φ t along with its precondition φ prec t and postcondition φ post t .The second thread handles the verification of the product flow, taking as input a product ω t along with its predecessor operation ω pred t and successor operation ω succ t .Specifically, for the operation verification (corresponding to OFVERIFICATION in Alg.1), we ensure that each operation can be correctly executed given its input reagents and that it yields the expected output products.This involves checking that the preconditions are satisfied by the available products from preceding operations and that the postconditions are well-defined for subsequent use.Concurrently, the product flow verification (corresponding to PFVERIFICATION in Alg. 1) involves tracking each
CHECKOPCONDITIONS(φ, φ prec , φ post ) if φ prec ⊆ M (Ω) then M (Ω) ← (M (Ω) \ φ prec ) ∪ φ post ▷ ProceedÀ ÉÈÇAEÄÅÃÂÈÁAE 4$3%2$ 0 ('$&amp;#)#"!"$0 0 4$1 1)$0
YEVGYRW5EWDGYSGTICYHFY@I@GEEIU9BBA 8G7W7@GUT Y6IEVYCY7GDQ5Q9IRC5BBA EVGYRG55YTGHDI7YIUYEVGY5F7CEGBBA PDGRI@IECEG EVGYRG55YTGHDI7YIUY BBA 8GSQXGY YHFY@I@GEEIU9BBA EVGY@G55GEY6IEVYCY7GDQ5Q9IRC5BBB unit of product flow through the protocol.We verify that the product is generated by the specified operation and that it possesses the necessary properties ω prop t for consumption by the next operation.</p>
<p>The interaction between these two threads forms a feedback loop where the verification of operations and products mutually inform and constrain each other.This reciprocative method allows us to iteratively refine the protocol, ensuring that each step is both operationally feasible and chemically coherent.LLMs are employed to implement the functions CHECKOPCONDITIONS and CHECK-PROPERTIES, extracting and verifying operation conditions and product properties from natural language protocol descriptions through instruction-following in-context learning (Wei et al., 2021;Brown et al., 2020).For the prompts employed, readers are referred to Appx.D.6.</p>
<p>AUTOMATIC REPRESENTATION GENERATION</p>
<p>In this section, we describe the proposed data-driven algorithm to automatically generate the hierarchically encapsulated representation for protocol design (see Fig. 2A).We first define the problem of generating the desired representation by means of DSL design (Sec.3.1).We then introduce methods for generating operation-centric (Sec.3.2) and product-flow-centric (Sec.3.3) DSL views.</p>
<p>THE REPRESENTATION GENERATION PROBLEM</p>
<p>We denote the problem of generating the representation for protocol design within a given domain as RG = ({⟨φ⟩, ⟨ω⟩} | P, C).The representation is a DSL with language features accommodating both the operation-centric program view ⟨φ⟩ and the product-flow-centric program view ⟨ω⟩.</p>
<p>The domain-specific corpus C = {Φ 1 , Φ 2 , . . ., Φ |C| } consists of existing protocols published in top-quality journals within the corresponding experimental domain.The source and profiles of C of each domain is detailed in Appx.E.1.We can obtain instance action with attributes based on C in a straightforward way through NL information extraction (see Appx.D.3 for implementation details).</p>
<p>The prior knowledge of operations and products, p(φ) and p(ω), including the basic syntax of the key-value structures and the elementary taxonomies, is derived according to the general commonsense of experimental sciences, as aforementioned in Sec. 2. Specifically, the problem essentially aims to fit the joint distribution models p(φ, ϕ, φ prec , φ post , φ exec ) and p(ω, ω pred , ω succ , ω prop ) with domain-specific corpus C given prior knowledge p(φ) and p(ω).</p>
<p>AUTOMATIC FUNCTION ABSTRACTION</p>
<p>The key challenge of encapsulating the operation-centric view is to aggregate all possible execution contexts for an operation, and then generalize the contexts to the interface.If we keep each of the use case as one single instance of the interface, which can be in thousands regarding one operation, the generalization is meaningless.Since there is no prior knowledge about the interface in advance, we develop the algorithm following the idea of non-parametric modeling, i.e., Dirichlet Process Mixture Model (DPMM), resulting in flexible identification of interface instances.</p>
<p>Hierarchical non-parametric modeling As we must handle information coming in different granularities, from interface structures to values of parameters, we choose to model the operations in a hierarchical fashion.Compared with the flatten spectral clustering approach developed by Shi et al. (2024a), which compresses all information of an operation into a embedding vector, our modeling is competent for considering information at different levels comprehensively.We carefully adopt the prerequisite that the interface is generated subject to the operation, preconditions, postconditions, and execution configurations are generated subject to the interface, and the value of configuration parameters are generated subject to their corresponding keys.Thus, we have the model:
p(φ, ϕ, φ prec , φ post , φ exec , φ exec-v ) = p(φ exec-v | φ, ϕ, φ exec )p(φ exec | φ, ϕ)p(φ prec | φ, ϕ)p(φ post | φ, ϕ)p(ϕ | φ)p(φ),(1)
where φ exec-v denotes the values of configuration parameters.Within each iteration of the DPMM process, we sample the variables level-by-level.Since the structures of preconditions, postconditions, and the selection of devices and configuration parameters are discrete, we sample them directly from the Dirichlet Process.As permissible values of parameters can be discrete, e.g., an array of specific values, common in acidity preparation; continuous, e.g., an interval with minimum and maximum values, common in temperature setting; or mixed, e.g., an array of specific values with random perturbations around the mean, common in timing control, we conduct the sampling by integrating Gaussian Process with Dirichlet Process
φ exec-v | φ, ϕ, φ exec ∼ DP (α, H(φ exec ), ϕ, φ) × GP (m, K)
, where α, H, m, and K are corresponding hyperparameters.</p>
<p>Unification of the interface While clustering similar interface instances encapsulates operations, there may remain redundant interfaces due to minor discrepancies.These discrepancies often arise from differences in parameter values or naming conventions that do not fundamentally alter the operation's functionality.To alleviate such redundancies, we implement a unification process for the interfaces.Specifically, interface instances associated with the same operation are considered equivalent if they have the same number of slots and emits and share the same keys in their execution configuration parameters.By abstracting away differences in parameter values and names, we unify these interfaces into a single, generalized interface, akin to the algorithm proposed by Martelli &amp; Montanari (1982).Unification enhances the generality of the operation-centric view by consolidating functionally-identical interfaces, maintaining a concise and representative set of operations.</p>
<p>Results Function abstraction converges on the domains respectively, as shown by the likelihood curve yielded by non-parametric model in Fig. 2B.In the DSL of Genetics, there are 304 operations in total, with an average of 7.9 interface instances per operation; for Medical, these two quantities are 269 and 6.9; for Bioengineering, they are 196 and 7.8; and for Ecology, they are 100 and 3.5.We find that a majority of operations with high occurrence frequency are unique to one domain, such as Pipette to Medical and Lyse to Genetics (see Fig. 2D).There are also common operations across domains, such as Concentrate and Culture.Take Concentrate for an example, its interface captures the instances with different devices according to input phases, e.g., use Bench-top_centrifuge for Liquid while Isotope_separation_centrifuge for Gas, and also instances with different emits, e.g., selecting Supernatant or Suspension as the product to keep.</p>
<p>AUTOMATIC MODEL ABSTRACTION</p>
<p>The key challenge of encapsulating the product-flow-centric view is to select proper descriptive properties of a flow unit component.There exists false positive cases, where properties are attributed to components with the same semantic identifier but in different phases, e.g., we consider ethanol with the property volume when it comes in liquid and with the property pressure when it comes in gas.There also exists false negative cases, where exact same components are regarded as different ones due to different reference names, e.g., Acetylsalicylic Acid, ASA, and Aspirin refer to the same thing.To alleviate false positive and false negative results, we discard the design choice of the interface in the operation-centric view, which tends to cover the possibly richest context, and thereby have the non-parametric model:
p(ω, ω pred , ω succ , ω prop , ω prop-v ) = p(ω prop-v | ω prop , ω)p(ω prop | ω)p(ω pred | ω)p(ω succ | ω)p(ω),(2)
where ω prop-v denotes the values of property parameters.Results Model abstraction converges on the domains respectively, as shown by the likelihood curve in Fig. 2C.In the DSL of Genetics, there are 17, 190 model states, i.e., product flow unit as product status, in total; the quantity is 12, 472 for Medical; 11, 418 for Bioengineering; and 2, 205 for Ecology.We find that most components of product flow units with high occurrence frequency are unique to one domain, such as RNA to Genetics and HCC to Medical (see Fig. 2E/F).Take Ethanol for example, the model captures its possible concentrations in liquid rather than in gas.</p>
<p>EXPERIMENTS AND DISCUSSION</p>
<p>In this section, we report and discuss the results of our experiments.We start from describing our realistic novel protocol design tasks (Sec.4.1), along with the metrics to measure the consistency between the designed protocol and the groundtruth protocol (Sec.4.2).Afterwards, we introduce the alternative representations and machine designers used for comparison (Sec.4.3).Finally, we report and analyze the experimental results both quantitatively and qualitatively (Sec.4.4).Generating unverified experimental objectives and their corresponding protocols specially for our protocol design tasks is impractical because those experiments which have not been peer-reviewed and published can be problematic regarding the contents themselves.To maintain both reality and scale of the testing set, for each domain we filter out a small subset of protocols which significantly differ from the remaining major part of the protocol set and exclude this subset from the corpora for automatic representation generation (Appx.E.1).This selected subset form the groundtruth of the testing set.</p>
<p>PROTOCOL DESIGN TASKS</p>
<p>We exploit quantitative indicators to assist testing set selection, which follows the convention of measuring a protocol's novelty in experimental sciences (Schwab &amp; Held, 2020).We comprehensively consider three indicators: (i) similarity between the text embedding of the NL-based description of purpose of protocols, employing the evaluation model in O'Donoghue et al. ( 2023); (ii) Intersection over Union (IoU) between the instance actions of protocols; (iii) similarity between the execution sequence of protocols, implemented through the Sequence Alignment (SA) algorithm (Smith et al., 1981).To note, indicators (ii) and (iii) are calculated upon the protocols pre-processed by the workflow described in Appx.D.3.Indicator (i) captures the high-level idea of protocol design, indicator (ii) is correlated to the implementation of the protocol design, while indicator (iii) captures the low-level information of protocol execution.</p>
<p>In response to the three purposes of protocol design introduced in Sec. 1, we specify the planning, modification, and adjustment tasks of protocol design.Candidate planning tasks, which are the confirmation of unverified experimental goals, come with relatively low scores (within the 20% lowest) on indicators (i) and (ii).Candidate modification tasks come with fair scores (around the 40% lowest) on indicators (i) and (ii) and relative low score on indicator (iii).Candidate adjustment tasks come with relatively high scores (within the 40% highest) on all of the three indicators.</p>
<p>We obtain the final testing set through a human-machine collaborative workflow.We first detect the outliers of the original protocol corpus of each domain under the metrics above, thereby forming a candidate set.Afterwards, experts of the corresponding domain (holding at least a Master's degree majoring in that domain) manually check the applicability of protocols in the candidate set with cross-validation, discarding the misclassified ones, requesting for more candidate protocols, and refining the groundtruth file when necessary.The testing set includes 140 new protocols and 1757 steps in total, across the domains of Genetics, Medical, Bioengineering, and Ecology, with 23% for planning, 52% for modification, and 25% for adjustment (see Tab. 1 and Fig. 3A for details).</p>
<p>INTER-PROTOCOL CONSISTENCY METRICS</p>
<p>Evaluating the consistency between a designed protocol and the groundtruth is not like comparing between two plain strings (O'Donoghue et al., 2023).Based on the corresponding commonground in experimental sciences (Bartley et al., 2023) S(Φ ′ )).These six dimensions capture protocol information from low to high granularities, and also measure the consistency of both ingredient knowledge and procedural knowledge, offering a relatively objective evaluation standard.</p>
<p>MACHINE DESIGNERS</p>
<p>We implement an array of designers by combining different representations with different LLMbased automatic designers under tractable computing load (see Appx.D.7).We investigate four types of representations, including the original NL-based protocol representation (Flatten) and the three levels of encapsulation described in Sec. 2, i.e., instance actions with attributes (Instance), operation-centric view only (Encapsulated), and the dual representation with operation-and product-flow-centric views (Encapsulated+).We consider three types of LLM-based protocol designers: (i) Baseline, a pure LLM-based approach with Retrieval-Augmented Generation (RAG) on the corresponding corpora (Appx.D.4); (ii) Internal, which takes the specific representation as part of the prompt of an LLM, requesting it to output the protocol under the constraint of the given representation (Appx.D.5); (iii) External, where the representation serves as an external constraint layer for the output of an LLM, verifying and refining the designed protocols (Appx.D.6).Notably, the external verifier is part of the resulting DSL as our proposed representation for protocol design.</p>
<p>The combination of representation and designer does not span a Cartesian space due to the intrinsic limitations of Flatten and Baseline.Therefore, we implement seven machine designers, including: (i) Flatten-Baseline(FB), LLM with RAG on original protocol corpora; (ii) Instance-Baseline(IB), LLM retrieval on the protocol corpora translated into instance actions; (iii) Instance-Internal(II), prompting LLM with the Instruction Set Assembly (ISA) of instance actions, following the implementation of the currently state-of-the-art method O'Donoghue et al.</p>
<p>PROTOCOL DESIGN RESULTS</p>
<p>The complete quantitative results across the four domains, the three tasks, and the six dimensions of evaluation metrics are presented at Appx.B. Through paired samples t-test, we find that EE+ and EI+ significantly outperform other alternative approaches (EE+ outperforms EE: t(278) = 8.007, µ d &lt; 0, p &lt; .0001;EI+ outperforms EI: t(278) = 8.397, µ d &lt; 0, p &lt; .0001;EE+ outperforms II: t(278) = 24.493,µ d &lt; 0, p &lt; .0001;EI+ outperforms II: t(278) = 23.855,µ d &lt; 0, p &lt; .0001;see Fig. 3C-E).These comparisons demonstrate the suitability of our desired representation for protocol design.Similarly, we find that approaches equipping with a relatively higher-level representation significantly outperforms their counterparts with a relatively lower-level representation (EE outperforms II: t(278) = 16.315,µ d &lt; 0, p &lt; .0001;EI outperforms II: t(278) = 15.259,µ d &lt; 0, p &lt; .0001;II outperforms FB: t(278) = 8.340, µ d &lt; 0, p &lt; .0001;see Fig. 3B).</p>
<p>DISCUSSION</p>
<p>This work proposes a hierarchically encapsulated representation for the conceptual knowledge in experimental sciences, including instance actions with attributes, sequential representation of operations with function abstraction, and continuous representation of product-flows with model abstraction, to fully elicit LLMs' capability on protocol design as an auxiliary module.The following discussions on results reveal the design rationality, scalability, and generality of the representation.</p>
<p>Contributions of the building blocks</p>
<p>The encapsulated representation approaches with dual views outperform their counterparts without dual views by enhancing both intra-step and inter-step details.At the intra-step level, EI and EE offer richer semantic information than IB and II, leveraging protocol-centric view to capture detailed configuration each operation.This feature accounts for their satisfactory performance on IoU(Op).At the inter-step level, EI+ and EE+ treat each step as a FlowUnit, incorporating both preceding and succeeding step contexts, leading to notable improvements in Sim(Exec) and IoU(Prod).This creates a double assurance mechanism (Shi et al., 2024c): the first assurance comes from internal input/output checks within each instruction, and the second from the input/output characteristics inferred from neighboring instructions.Namely, we estimate the output of the preceding operation and check its alignment with the current step's input.This design enhances step linkage, verification, and overall coherence, ensuring higher consistency and robustness in complex protocol workflows.Please refer to Appx.I.1 for the case study.</p>
<p>Handling different task complexities</p>
<p>The overall performance aligns with the trend in complexity across the three tasks (Fig. 3A); however, the dual-view encapsulated representations, EI+ and EE+, demonstrate superior performance compared to their counterparts.In planning, these methods consider all necessary components, enabling creative yet structured protocol generation.For modification tasks, they provide feedback on parameter changes, detecting inconsistencies that their counterparts might fail to capture.In adjustment tasks, EE+'s external verifier maintains protocol integrity by identifying component relationships.Please refer to Appx.I.2 for the case study.</p>
<p>Generality across domains Our DSL-based approaches offer a unified, modular representation with generalizability across scientific domains (see domain-indexed results at Appx.B.2).The dualview approach abstracts experimental processes into operations and flow units, capturing essential details while remaining applicable across fields.By representing dependencies between steps and tracking product flow, the replication of experiments could be enhanced.The framework captures cross-domain commonalities while allowing domain-specific content like specialized operations and reagents.This unified representation standardizes protocols and enables researchers to adopt experimental protocols from multiple fields, fostering interdisciplinary collaboration and innovation.Please refer to Appx.I.3 for the case study.Limitations on generality are discussed at Appx.G.</p>
<p>A ADDITIONAL REMARKS</p>
<p>A.1 RATIONALE OF THE OVERALL DESIGN CHOICE</p>
<p>It seems that we can formulate the protocol design problem in the fashion of Markov Decision Process (MDP) and solve it by heuristic-based planning methods or Hierarchical Reinforcement Learning (HRL) approaches.However, although the formulation itself is feasible, solving the problem may not be practical.Consider solving the problem through an HRL approach designed for heterogeneous action space with parameters (as the protocol is required to decide both the key properties of an operation and the corresponding values).This hierarchical agent may be trained to converge on a fine-grained environment with a clearly designed reward function, or on a large dataset with trajectories for offline learning.Unfortunately, we have access to neither an interactive environment simulating the experiments nor sufficient data to support offline training (Pateria et al., 2021).</p>
<p>Treating the experimental procedures as a white box and creating digital twins for experiments can be an elegant solution and thereby facilitate various applications other than protocol design.This effort requires elaborated design of simulation granularity, exhaustive collection of primitive principles of the system, efficient implementation of rule production, and define precise metrics for evaluating the distance between current and objective states (serving as a reward function), which can be labor-intensive and is far out of the scope of this work.On the other hand, viewing those published protocols as trajectories for offline training, the scale of the offline dataset and the density of the reward function are much too insufficient to support training to convergence.Augmenting the data, synthesizing realistic trajectories, or enhancing the accessibility of protocols, are out of the scope of this work.Given the current obstacles, we choose not to formulate the problem in an MDP fashion.Though an MDP-style formulation can be more precise and elegant, it may misguide the readers to some extent.Instead, we decide to leverage the rich domain-specific knowledge provided by knowledge-based agents such as LLMs, where knowledge may complement the lack of data and dense reward function.This design choice is also in line with the initial attempts on automatic experiment design (Boiko et al., 2023;M. Bran et al., 2024).</p>
<p>In summary, our design choice of formulation is a compromise based on currently limited resources and restricted scope.Nonetheless, the exploration of more precise and elegant formulations represents a promising avenue for future research.</p>
<p>A.2 INTUITION BEHIND THE INTERFACE</p>
<p>Interface is a concept of functional abstraction (Abelson &amp; Sussman, 1996).Interface disentangles the abstract functionality on the semantics level and its corresponding implementation details on the execution level.This approach encapsulates the implementation of an operation into a black-box, so the users of the operation would only need to consider its input and output.Therefore, with such encapsulated representation for protocol design, we only need to care about the consistency between the output of the predecessor operation and the input of the successor operation, without caring about their implementation details.This is the idea behind operationalization.Operationalization makes the interface an abstract function over all relative instance actions.The interface is abstracted from the execution contexts of all instance actions with the same reference name, i.e., the same purpose, and can be instantiated to an instance action given a specific execution context.A specific context can be the predecessor operation, the successor operation, the precondition, or the postcondition of the considered operation.An instance action configures a specific implementation for a specific execution context.For the operation "Homogenization", the implementation of one instance action can be "using an ultrasonic homogenizer" if the precondition, namely, the execution context, has intermediate product "cell suspension" available; the implementation of another instance action can be "using a bead mill" if the precondition contains tissue.This example demonstrates the relationship between interface and instance actions of an operation: the interface is abstracted from the set of instance actions and can be instantiated to instance actions.</p>
<p>Here we also give a more intuitive example to enhance the reader's comprehension.Consider the culinary scenario with the actions "frying the egg", "frying the fish", and "frying the steak".These are different instance actions coming with the same purpose "to fry something".Therefore, we can abstract the interface from these instance actions to operationalize the operation "fry".The input of "fry" should be something raw and its output should be something fried.Given different preconditions with available eggs or pieces of steak, the abstract semantic operation "fry" can be grounded to instance actions "frying the egg" or "frying the steak" respectively, through the instantiation of the interface.In summary, an interface serves as the bridge between the semantics level and the execution level.</p>
<p>A.3 VALUES OF MANUAL PROTOCOL CERTIFICATION</p>
<p>Certification is always one of the central focuses in the engineering practices of automation.In our practice, we only automate the process of protocol design, which is the primary objective of this work, and keep the manual certification part.On one hand, relieving experimental scientists from the labour-intensive protocol design tasks, thereby allowing them more time for high-level thinking, is a sufficiently significant improvement so far.On the other hand, engineering practices such as lab automation and manufacturing are in high demand for preciseness.This leads to the requirement of manual certification.Domain experts handle subtle cases through their tacit domainspecific knowledge and are responsible for their decisions (Wang et al., 2023b).According to these considerations and the standard operating processes of experimental sciences, we choose to certify the designed protocols by domain experts.</p>
<p>Our current choice is a compromise on the limitation of techniques and the demand for preciseness.In future work, we can conduct investigations on how to build digital twins of self-driving laboratories.Such digital twins support prediction, explanation, and counterfactual analysis of unseen behaviors of the experiments, which may facilitate machine-based protocol certification.Grounding these blue-sky thoughts necessitates addressing the challenging problems regarding the decision of simulation granularity, the implementation of data-efficient simulation model construction, and the injection of tacit domain-specific knowledge.In summary, the exploration of generated-protocolcertification by machines represents a promising avenue for future research.</p>
<p>A.4 LIMITATIONS OF AUTOMATIC PROTOCOL CERTIFICATION</p>
<p>LLMs can be much too uncontrollable for engineering practices such as lab automation, which may lead to unpredictable dangerous situations (Wang et al., 2023b).There comes a dilemma-we try to exploit the capability of reasoning over knowledge of LLMs, while we try to alleviate the drawbacks brought up by the uncontrollable nature of LLMs.Our proposed representation is dedicated to resolving the dilemma.The representations not only elicit LLMs' potential on protocol design through structural knowledge representation, but also serve as a guardrail for LLMs.Since the generated protocols are represented as corresponding DSL programs, the permissible output space is much more confined compared with that of pure LLMs, serving as constraints upon the LLMgenerated protocols.Thanks to the verification mechanisms provided by DSLs, the correctness of the generated protocols can be checked to some extent.Therefore, by equipping LLMs with an auxiliary constraint layer, we may approach a balance between knowledge utilization and preciseness.</p>
<p>However, the current verification on the level of DSL programs is far from sufficient for serving as a certification.Certification is a serious process, where any possibilities of reporting false positive cases are required to be eliminated.Some cases can be highly long-tailed distributed, which may not be detected by data-driven and knowledge-driven machine certifiers.In this context, human domain experts are responsible for coming up with these potential risks through their experiences and tacit knowledge.Therefore, we are not likely to move human experts out of the loop, except that we can efficiently build up appropriate digital twins for self-driving laboratories.In current practices, the automation of protocol design puts human experts into a larger loop without focusing on the lowlevel details of experiments.As a result, they are allowed more time for high-level thoughts on things like values, which are not likely to be alternated by machines.In summary, it is neither practical nor necessary to totally move human experts out of the loop of automatic scientific discovery.The investigation of human-machine coordination in protocol certification represents a promising avenue for future research.</p>
<p>A.5 RATIONALE FOR THE REAGENT CONSUMPTION MODEL</p>
<p>We treat the instantiation and the consumption of reagents a one-time deal without considering the exact volume of consumption and the corresponding remainder.The rationale for such design choice comes from both the current Standard Operating Process (SOP) of experimental sciences and the properties of self-driving laboratories (Bartley et al., 2023).</p>
<p>In the current SOP for manually conducted experiments, experimenters are required to use prefabricated sets of reagents.Similarly, experimenters use specific containers with predefined capacities to transfer intermediate products.Therefore, one pack of reagents or one container of intermediate products is only used once for an operation, without considering the remainder.This results in a more succinct representation where reagents are regarded as discrete elements rather than continuous volumes.</p>
<p>For self-driving laboratories, this is deliberately designed for efficient variable management following the corresponding principles in computer system design (Abelson &amp; Sussman, 1996).In computer systems, not removing used variables would cause out-of-memory errors, let alone in physical automation systems, where the physical memory slots are much harder than the virtual memory slots in computer systems to manage.Hence, we exploit this variable management mechanism to enhance the execution efficiency of self-driving laboratories.</p>
<p>A.6 RELATION TO LLM REASONING</p>
<p>We would like to clarify that our objective is not to alternate Chain-of-Thought (CoT) reasoning.According to recent studies on the properties of CoT, LLMs with CoT may generate coherent but unprofessional text in expertise-intensive application scenarios (Xiao et al., 2023).Therefore, our proposed representation serves as an auxiliary guardrail module for LLMs with reasoning techniques such as CoT, enhancing LLMs' reasoning capability from two aspects: (i) the representation constrain the scope of reasoning into a close set of entities, such as available operations, reagents, and devices commonly used in the domain; and (ii) the representation provides fine-grained injection of domain-specific knowledge for LLMs, resulting in not only coherent but also expertise-compatible generated content.</p>
<p>A.7 APPLICABILITY TO DOMAINS BEYOND SCIENTIFIC EXPERIMENT</p>
<p>In theory, our framework can be applied to any field that requires adherence to specific protocols and has a need for automated execution.Let us consider an automated kitchen controlled by a computer as an example.</p>
<p>Assuming the automated kitchen's computer is already programmed to prepare "braised pork ribs" and "steamed sea bass": Next, we can derive the corresponding DSL.For instance:</p>
<p>{ " cooking_methods ": { " braise ": { " steps ": [ {" type ": " fry ", " temperature ": " high ", " time ": "5 min "} , {" type ": " simmer ", " temperature ": " medium ", " time ": "30 min "} ], " seasoning ": [" soy sauce ", " sugar "] }, " steam ": { " steps ": [ {" type ": " steam ", " temperature ": " high ", " time ": "15 min "} ], " seasoning ": [" ginger ", " scallion "] } } , " ingredients ": { " ribs ": { " category ": " meat ", " default_braise_time ": "30 min " }, " sea_bass ": { " category ": " fish ", " default_braise_time ": "20 min ", " default_steam_time ": "15 min " } } } Now, let us create a new recipe for Braised Sea Bass by combining the braising technique with sea bass as the main ingredient.</p>
<p>START SELECT ingredient : sea bass ACTION : fry , temperature : high , time : 5 min ADD seasoning : soy sauce , sugar ACTION : simmer , temperature : medium , time : 20 min END</p>
<p>B COMPLETE RESULTS</p>
<p>B.1 TASK-INDEXED COMPLETE RESULTS</p>
<p>Table A1: Complete quantitative results on protocol design, specifically the planning task.Each cell represents the machine designer's average score (at the top of the cell) on all testing samples across the four domains and the corresponding standard error of mean (at the bottom of the cell).For each dimension, we highlight the results of both the best and the second best ones.</p>
<p>IoU(Op) IoU(Prod) IoU(Dev) Sim(Exec) Sim( Goal   Table A4: Complete quantitative results on protocol design, specifically the Genetics domain.</p>
<p>Each cell represents the machine designer's average score (at the top of the cell) on all testing samples across the three tasks and the corresponding standard error of mean (at the bottom of the cell).For each dimension, we highlight the results of both the best and the second best ones.The testing set selection and groundtruth checking tasks conducted by human experts in this work has been approved by the Institutional Review Board (IRB) of Peking University.We have been committed to upholding the highest ethical standards in conducting this study and ensuring the protection of the rights and welfare of all participants.We paid the domain experts a wage of $22.5/h for their work in this study.</p>
<p>We have obtained informed consent from all human experts, including clear and comprehensive information about the purpose of the study, the procedures involved, the risks and benefits, and the right to withdraw at any time without penalty.Participants were also assured of the confidentiality of their information.Any personal data collected (including name, age, and gender) was handled in accordance with applicable laws and regulations.</p>
<p>C.2 CORPORA COLLECTION</p>
<p>We carefully ensure that all protocols included in our corpora strictly comply with open access policies under the Creative Commons license.This strategy guarantees adherence to copyright and intellectual property laws, thereby preventing any potential infringement or unauthorized use of protected materials.By exclusively employing resources that are freely accessible and legally distributable, we maintain the highest standards of ethical research conduct, promoting transparency and respect for the intellectual property rights of others.This commitment ensures that our work advances the frontiers of knowledge in a manner that is both legally sound and ethically responsible.</p>
<p>D.3 PRE-PROCESSING OF THE PROTOCOLS</p>
<p>The protocol pre-processing steps begin by reading all JSON files of the protocols.Each protocol is then splitted sentence-by-sentence using Spacy1 , with the constraint that every sentence is longer than ten characters.Due to the large volume of data, sentence splitting is handled in parallel.Afterwards, deeper sentence splitting is performed based on specific conditions for further refinement, such as the presence of "and/then/and then" followed by a verb2 .We then parse sentences into root verbs and purpose clauses, which are identified using token.dep_== "ROOT" for root verbs and prepositional/adverbial/modals for purpose clauses.Lastly, we merge phrases based on punctuation, and their classification into valid sentences or decorative phrases depends on whether they contain a root verb or lack a purpose clause.</p>
<p>The first verb in each sentence is extracted as an opcode, again utilizing parallel processing for efficiency.Opcode frequency is filtered to exclude stopwords, which are recorded in a separate text file.Then we categorize these opcodes into high-level operation classes using a GPT model (gpt-4o mini), where each opcode is classified into categories like Transfer Operations, Transformation Operations, or Data Operations.</p>
<p>Once operation classification is complete, entity recognition is performed (also using gpt-4o mini) to identify entities like devices, input_flow_units, output_flow_units, and total_time.Each flow unit is further categorized (also using gpt-4o mini) with a high-level classification composed of a phase, i.e., Gas, Liquid, Solid, etc.; and a type, i.e., Chemical Compound, Biological Material, etc.When both phase and type are successfully labeled, phase is preferred as the feature of the flow unit.If phase labeling fails, we use type the feature of the flow unit.If neither phase nor type is successfully labeled, the corresponding feature is set to None.Part of the rationale is that there are non-reagent components in the general sense, i.e., data, files, obscure or undefined substances, etc.Therefore, we apply this strategy to maximize the possibility that there is a meaningful upper class labeling of the components without any redundancy.</p>
<p>Finally, we conduct a synonym merge process on the devices, which starts by using transformers AutoTokenizer3 to get an embedding for each device name.Afterwards, we use sklearn4 to identify potentially similar entity pairs by calculating the cosine similarity of the candidate entities, and then passing these entity pairs to the GPT model for synonym detection, thereby merging devices belonging to the same type.The reference names of these combined devices will be one of the features.</p>
<p>D.4 PURE LLM-BASED DESIGNER</p>
<p>The pure LLM-based designer employs RAG to retrieve similar protocols from the corresponding corpora for representation, following the design choice of the baseline in O' Donoghue et al. (2023).Specifically, in the FB approach, three similar protocols are first retrieved from the original protocol corpora using RAG, and then, along with the title and description of the target protocol, they are provided to the LLM to generate a NL plan.The LLM subsequently translates the NL plan into Python pseudocode.In the IB approach, three similar protocols' instance actions (like Python pseudofunctions definitions) are first retrieved from the corpora, and after randomizing their order, they are provided to the LLM along with the title and description of the target protocol to generate a plan in the form of Python pseudocode.</p>
<p>[ Prompt for retrieving similar protocols from corpora ] You are an expert in biology and you are very familiar with the experiment protocols .</p>
<p>I would like to make a protocol for { title }.I will give you some related protocols in the database .</p>
<p>Could you find me the most three similar and relevant protocols for reference in the given range ?</p>
<p>Here is an example of how to convert a protocol for { example protocol title } into python pseudocode { example protocol } { example python pseudocode } YOUR TASK : Here is a biology protocol entitled '{ title } ' The protocol steps are as follows : { protocol } Please convert this protocol into python pseudocode .python pseudocode :</p>
<p>[ Prompt for generating plan in pseudocode ] Your goal is to generate python pseudocode for biology protocols .</p>
<p>Here is an example of how to generate pseudocode for a biology protocol .EXAMPLE : { example protocol title } Here are some extra details about the protocol : { example protocol description } example pseudocode : { example pseudocode } YOUR TASK : Generate pseudocode for a protocol for { title }.Here are some extra details about the protocol : { details } You may only make use of the following python pseudocode functions : { psuedofunctions } your pseudocode :</p>
<p>D.5 INTERNAL DESIGNER</p>
<p>The internal designer incorporates the specific representation as part of the prompt for an LLM, asking it to output the protocol while adhering to the given representation constraints, echoing the idea of Wang et al. (2023a).Specifically, in II, the instance actions retrieved from the corpora via RAG and the pseudofunctions definitions of the target protocol are shuffled and then provided together to the LLM, constraining it to generate a plan in the form of Python pseudocode using the given pseudofunctions definitions.In EI and EI+, relevant DSL instructions are selected from a domain-specific operation-centric view DSL and product-flow-centric view DSL, respectively.These instructions and the target protocol's title and description are provided to the LLM, prompting it to output the corresponding plan as instantiated DSL instructions.</p>
<p>[ Protocol for generating plan in DSL program using operation -centric view DSL ]</p>
<p>Your goal is to generate plan in domain specific language ( DSL ) for biology protocols .</p>
<p>The DSL specifications related to the operations involved in the experiment are provided .The DSL specification of each operation consists of multiple patterns , each pattern is an operation execution paradigm .Here is an example of how to generate plan in DSL for a biology protocol .EXAMPLE : { example protocol title } Here are some extra details about the protocol : { example protocol description } example plan in DSL : { example plan } [ Requirements ] 1. Design the experiment with finer granularity , incorporating more steps to complete the experiment in a more rigorous , complex , and comprehensive manner .</p>
<ol>
<li>There are some missing parameters in the DSL specification .You should generate each step of the DSL program as detailed as possible based on your understanding of the protocol plan .[ Protocol for generating plan in DSL program using dual representation ] Your goal is to generate plan in domain specific language ( DSL ) for biology protocols .</li>
</ol>
<p>Two perspectives of the DSL specification are provided : the specification for experimental operations and the specification for experimental products .</p>
<p>The DSL specification of each operation or product consists of multiple patterns , each pattern is an operation execution paradigm or a product flow paradigm .</p>
<p>Output every operation of the plan in the form of an operation DSL program and every product of the plan in the form of a product DSL program .Here is an example of how to generate plan in DSL for a biology protocol .If you believe the error in a particular step is due to a mismatch in product names between the two perspectives rather than an actual error , you can ignore this error .Output your refined plan in DSL , returning a JSON block without any additional information or comments .YOUR TASK : Refine the plan in DSL for a protocol for { title }.</p>
<p>Here are some extra details about the protocol : { details } Refine the following plan : { plan } Here is the feedback of the plan : { feedback } Your refined plan in DSL :</p>
<p>D.7 COMPUTING LOAD OF THE MACHINE DESIGNERS</p>
<p>For automated representation generation, we primarily used GPT-4o mini with OpenAI's Batch API5 for preprocessing, incurring a cost of approximately $60 across four domains.The design of the DSLs was executed on a MacBook with an M2 chip, running 1,000 iterations to ensure convergence.This process required an average of 55 seconds per iteration for the operation-centric view DSL and an average of 2 seconds per iteration for the product-centric view DSL.For the machine designer, we primarily utilized GPT-4o mini combined with RAG for design, with a total cost of approximately $10 (7 methods, 140 protocols).In summary, the overall computational load is relatively low, highlighting the accessibility of our machine designers when utilizing the proposed representations and the corresponding automatic representation generation modules.</p>
<p>E DATA COLLECTION E.1 CORPORA SOURCES</p>
<p>The corpora C for the automatic generation of representations (Sec.3.1) and the corpora for selecting the testing set (Sec.4.1) are both retrieved from open-sourced websites run by top-tier publishers, including Nature's Protocolexchange 6 , Cell's Star-protocols 7 , Bio-protocol 8 , Wiley's Current Pro-tocols9 , and Jove10 .These sources compile a dataset of 15,837 experimental protocols across four domains: Genetics (8794 protocols), Medical (7351), Ecology (812), and Bioengineering (3597), with minimal overlap between them.We aggregated the corpora and analyzed the themes of the protocols according to the first-and second-level labels attached to them.We adopt measures to ensure that C is mutually exclusive with the testing set.</p>
<p>Other domains, such as Physics and Chemistry, are also representative domains of experimental sciences, besides Biology, Medical, and Ecology.The preliminary factor that restricts our current scope is data accessibility.Due to the higher cost of accessing the corpora of protocols for conducting physics and chemistry experiments, for example, mining the protocol from the "method" section of relevant published papers, we leave the application to Physics and Chemistry for future work.We employ the broadly accepted standard operating process to empirically verify that LLMs have not memorized the data we use.We adopt the methodology outlined in Section 5.2 of Skywork (Wei et al., 2023) and draw upon recent studies on detecting memorization in LLMs (Carlini et al., 2021;2022).Specifically, we use gpt-4o mini to synthesize data resembling the style of steps from novel protocols, and then calculate the perplexity on the test set and reference set.Since the reference set is newly generated, we consider it clean, not belonging to any training set of any model.</p>
<p>We randomly sample 100 sequences each from the test set and the reference set of the novel protocols.Each sequence corresponds to a single procedural step described in NL.We truncate the final 50 tokens of each sequence, retaining the prefixes.These prefixes are then used as prompts for the LLM to predict the next 50 tokens, for which we calculate the perplexity.If the perplexity of the test set is significantly lower than that of the reference set, the test set might have appeared in the model's training phase.</p>
<p>The results indicate that the LLM's average perplexity on the test set is significantly higher than that on the reference set (t(198) = 3.040, µ d &lt; 0, p &lt; .05;see Fig. A1), suggesting that the LLM encounters greater uncertainty with the novel protocols in the test set.This finding implies that for a published, widely accepted, and standardized operating process, there is no evidence to suggest that the LLM has memorized the data.</p>
<p>E.3 ON THE DIVERSITY OF NOVEL PROTOCOLS</p>
<p>Assessing diversity among novel protocols is both informative and meaningful.To further support our analysis, we incorporate a t-SNE visualization of the experimental objectives (described in natural language) for the novel protocols we select, as shown at Fig. A2.The results demonstrate a well-dispersed distribution, indicating a sufficient level of diversity among the protocols.3. Cell lysates are homogenized by passing through 22 -gauge needles , and tubes are put on ice for 15 min to complete the lysis .Crude extracts are then centrifuged at 2500 RPM for 5 min .Supernatants are transferred to fresh centrifuge tubes , and cold 5 M NaCl is added to each sample to make a salt concentration of between 0.7 -1.0 M to disrupt protein -protein interactions .</p>
<p>E.4 SHOWCASES</p>
<p>Spin the crude extracts by ultracentrifugation at 55000 RPM to</p>
<p>properly pellet residual insoluble proteins from the extract .Transfer supernatants into fresh centrifuge tubes .</p>
<p>Immunoprecipitation 5. Rinse Protein A beads in Hypotonic Buffer and place on ice until ready for use .</p>
<ol>
<li>
<p>Take a volume of cell lysates ( prepared as described above ) , and dilute with Hypotonic Buffer to 250 -500 mM salt to enable proteinprotein interactions .</p>
</li>
<li>
<p>Add 2 µg of preclearing antibody to the diluted lysate (e.g., anti -Myc or anti -p65 ) , vortex , add 50 µL of Protein A beads , and rock for 45 min .</p>
</li>
<li>
<p>Touchspin samples , and transfer supernatant to a fresh tube .9. Add 2 µg of polyclonal anti -MEKK1 to the lysates , and rock for 1 h.</p>
</li>
</ol>
<p>After this period , add 50 µL of Protein A beads and rock tubes at 4 • C for 1 h.</p>
<ol>
<li>
<p>Touchspin beads , wash beads with hypotonic buffer ( supplemented with NaCl to a concentration of 300 mM ) , vortex , and rock for 10 min .In total , 3 -5 washes of the beads are performed .</p>
</li>
<li>
<p>Finally , wash once with Hypotonic Buffer , and resuspend in Kinase Assay Buffer .Purified MEKK1 may be stored by snap -freezing in liquid nitrogen and long -term storage at -80 • C. Kinase assay Following preparation of MEKK1 immunoprecipitates ( as above ) , incubate with 7 µ g of JNKK1 ( K131M ) along with 5 µCi of ATP in Kinase Assay Buffer for 30 min at 30 • C ."</p>
</li>
<li>
<p>Collect the cells by spinning down without freezing on ice .Discard supernatant .</p>
</li>
<li>
<p>Re -suspend cells with 1 mL water and transfer to a 1.5 eppendorf tube , quickly spin down at 3 ,000 x g for 15 sec .12. Spin down at 20 ,000 x g for 5 min in a standard laboratory microfuge .13. Transfer supernatant ( around 350 µL) to a fresh 1.5 mL eppendorf tube .Add CHCl3 : isoamyl alcohol (24:1) .Vortex vigorously for 1 min at RT .</p>
</li>
<li>
<p>Transfer aqueous supernatant to a fresh 1.5 mL microfuge tube .If white cloudy precipitate is observed between the aqueous phase and organic phase , repeat steps 17 -18.</p>
</li>
<li>
<p>Add 1/10 volume of 3 M NaOAc ( pH 5) and vortex vigorously .Add 2.5 volumes of ethanol .Vortex again .</p>
</li>
<li>
<p>Place at -20 4. Dry the plate and add 100 µL of blocking solution per well to the plate .</p>
</li>
<li>
<p>Incubate the plate at room temperature ( RT ) for 1.5 h.</p>
</li>
<li>
<p>Discard the blocking solution and wash the plate with 1x PBS -Tween 5 times .</p>
</li>
<li>
<p>Dry the plate and keep it at 4 • C for later use .8. Harvest the spleen and create a single -cell suspension by gently smashing spleen pieces with the frosted surface of a pair of microscope slides in 5 mL of DMEM .9. Transfer the cells into 50 mL conical tubes and spin down the cells at 300 RCF for 5 min at 4 • C.</p>
</li>
<li>
<p>Discard the supernatant with aspiration without disturbing the pellet .</p>
</li>
<li>
<p>Re -suspend the cells with 5 mL of 0.17 M ammonium chloride and keep the cells on ice for 5 min .</p>
</li>
<li>
<p>Add 15 mL DMEM to the cells and spin at 300 RCF for 5 min at 4 • C. 13.Discard the supernatant and re -suspend the cells with 20 mL of DMEM and count the cells .</p>
</li>
<li>
<p>Re -suspend 2 x 10^7 cells in 2 mL of 10% DMEM and make a three -fold serial dilution (a total of 8 dilutions ) with 10% DMEM .</p>
</li>
<li>
<p>Add 50 µL/ well of the serial dilutions on the DNA -coated plate and centrifuge at 300 RCF for 5 min at 4 • C.</p>
</li>
<li>
<p>Incubate the cells at 30 • C for 2 h in a cell -culture incubator with 6% CO2 .17. Add 50 µL/ well of biotin -conjugated anti -IgM or anti -IgG (1:350 in 10% DMEM ) to the cells .</p>
</li>
<li>
<p>Centrifuge the cells at 300 RCF for 5 min at 4 • C and incubate the cells overnight in a cell -culture incubator with 6% CO2 .</p>
</li>
<li>
<p>Discard the cells and wash the plates 10 times with 10 x PBS -Tween 20.20.Dry the plates and add 50 µL of streptavidin alkaline phosphatase (1:1 ,000 in 1% BSA / PBS ) to the plate .</p>
</li>
<li>
<p>Incubate the plate at RT for 1 h and wash the plate 10 times with 10 x PBS -Tween 20.</p>
</li>
<li>
<p>Dry the plate and add 50 µL/ well of 1 mg / mL BCIP in AMP buffer to develop the plate .</p>
</li>
</ol>
<p>23.When the spots are clearly visible under a dissecting microscope , stop the development by discarding the BCIP solution and rinsing the plate with tap water thoroughly .</p>
<ol>
<li>Spots can be counted using a dissecting microscope or using an ELISpot reader ."</li>
</ol>
<p>F REPRODUCIBILITY</p>
<p>The project page with supplementary files for reproducing the results of this paper will be available at https://autodsl.org/procedure/papers/iclr25shi.html.</p>
<p>G LIMITATIONS</p>
<p>As a representation designed for a relatively new problem, the design and evaluation of the proposed framework come with limitations, leading to further investigations:</p>
<p>• Overall, our method achieves promising results across the four domains.Specifically, it performs best in experimental design for Genetics, shows comparable effectiveness in Medical and Bioengineering, but is less effective in Ecology.Notably, the Genetics corpus is the largest among the four domains, while the Ecology corpus is significantly smaller than the others.These observations suggest a potential positive correlation between the size of the domain-specific corpus and the "quality" of the resulting DSL.In other words, a larger corpus may lead to a "better" representation, thereby influencing the outcomes of protocol design.This hypothesis necessitates further investigation through rigorously designed experiments and carefully defined metrics for evaluating what constitutes a "better" representation.• We majorly consider the imperative programming DSLs as the implementation of representation in this work.This raises the question of whether incorporating objective-oriented programming paradigms could enhance the representation of complex entities within protocols, particularly the properties of reagents and intermediate products.If we are able to make the DSLs model the finegrained reactions between different components and automate the design of those DSLs based on a broader source of data, such as the Wikipedia pages, we can ultimately manage to build up a symbolic digital twin for a domain-specific system, such as the cell cultivation environment.Such simulation systems may greatly benefit protocol design with their power of prediction, explanation, and counterfactual analysis.• Can we explicitly extend our proposed representation to a hierarchical graph, thereby establishing the foundation for employing the advanced algorithms on graph routing and graph optimization?Results on the hierarchical graph can also serve as a external heuristic and constraint for LLMbased protocol designers.This hybrid approach may combine both the advantages of LLMs, i.e., exploitation of background knowledge, and those of classical algorithms, i.e., white-boxed properties with high explainability.• Can we apply the representation and the automatic representation generator to other critical domains with a high demand for automating procedure design, such as designing product route sheets for advanced manufacturing?</p>
<p>With many questions unanswered, we hope to explore more on automated protocol design for selfdriving laboratories and beyond.</p>
<p>H THE AUTOMATICALLY GENERATED REPRESENTATIONS</p>
<p>H.1 OPERATION-CENTRIC VIEW DSL { " Operation ": " Precipitate ", " pattern_0 ": { " Precond ": { " SlotArgNum ": 2, " SlotArg ": [" Liquid ", " Solid "] }, " Execution ": { " DeviceType ": " falcon tube ", " Capacity ": "15 mL ", " Config ": {} }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [" Liquid "] }, " Example ": [ " Precipitate RNA by adding 600 µL of 100 % EtOH , 20 µL of 3 M NaOAc ( pH 5.5 ) , and 3 µL of glycogen .", " Precipitate the DNA in each tube by adding 20 µl of 3 M sodium acetate ( pH 5.2 ) and 550 µl of 100 % ethanol .", " Ethanol precipitate the RNA by adding 5 µl 3 M sodium acetate ( pH 5.2 ) ,2 µl of glycogen ( 20 mg / ml ) , and 171 µl of 100 % ethanol .", " Nuclei washing and tagmentation : Spin down nuclei at 600 g for 10 mins at 4 • C , resuspended with 50 µL Complete Buffer .", " Spin the sample at 4 ,000 × g at 4 • C until the volume reduces to about 1 mL .Quantify protein concentration as described in step 60." , " Spin down the 15 mL tubes at 2 ,500 ×g and 4 • C for 20 min .", " Spin for 2 min at 1 ,000 x g.Save a few µL of concentrated sample to run on an agarose gel later .", " Spin the tube for 30 sec at 12 ,000 x g to consolidate the gel at the bottom of the tube .", " Spin plate at 300×g for 1 min to collect liquid at the bottom of the wells .", " Once NXT PCR program is complete , quick spin the sample tube then place it on the magnet for 1 min .Transfer the supernatant containing the amplified mRNA -seq library into a new PCR tube .", " Spin down 2 mill nuclei at 600×g for 5 min ( whole liver nuclei ) or use a magnet ( bead -bound nuclei ) ." ] }, " pattern_1 ": { " Precond ": { " SlotArgNum ": 1, " SlotArg ": [" Mixture "] }, " Execution ": { " DeviceType ": " microcentrifuge ", " Config ": {} }, " Postcond ": {} , " Example ": [ " Small volumes , 1 -3 mL should be spun in a small tube where these fewer EVPs can more readily be collected .", " Briefly spin down the bead -lysate mixture .", " Spin down the mix tube to eliminate bubbles / air in a bench microcentrifuge .Add 19 µL of the mix to each well ."] }, " pattern_2 ": { " Precond ": { " SlotArgNum ": 1, " SlotArg ": [" Liquid "] }, " Execution ": { " DeviceType ": " centrifuge ", " Config ": { " speed ": ["800 g "] , " time ": ["7 min "] } }, " SlotArg ": [" Liquid ", " Solid "] }, " Execution ": [ { " DeviceType ": " bransonic ", " Config ": { " temperature ": ["60 • C "] , " time ": ["90 min "] } }, { " DeviceType ": " sonicator ", " Config ": {} } ], " Postcond ": {} , " Example ": [ " Sonicate 10 µg BAC DNA or 50 µg genomic DNA in total ( you will recover 10 % DNA after sonication and size selection ) ." , " Sonicated chromatin is immunoprecipitated with the chosen antibodies and non -enriched chromatin washed with a series of washing buffers .", " If the herring sperm DNA has not been sufficiently sonicated or too much has been used , the DNA pellet might not adhere to the microfuge tube and can be lost with the ethanol .", " Sonicate the lipid tube to dissolve lipids with the mineral oil for 90 min at 60 • C by using Bransonic ."] } } H.2 PRODUCT-FLOW-CENTRIC VIEW DSL { " Pred ": " Modification Operations ", " FlowUnit ": { " Component ": " FBS ", " ComponentType ": " Liquid ", " UnitArgType ": " MAT " " Vol ": ["0.1 mL ", "0.5 mL ", "1 mL ", "1.5 mL ", "2 mL ", "3 mL ", "5 mL ", "10 mL ", "25 mL ", "50 mL ", "400 µL", "500 µL", "500 mL "] ,</p>
<p>" Container ": " Tube ", " Cond ": { " Concentration ": ["0.5%" , "1%" , "2%" , "2.5%" , "5%" , "10%" , "15%" , "20%" , "30%" , "50%" , "90%" , "100%"] ,</p>
<p>" Temperature ": [" -150 • C", "4 • C", "18 • C -26 • C", "37 • C", "56 • C "] ,</p>
<p>" State ": " heat -inactivated " } " Succ ": " Transfer Operations " } , { " Pred ": " Detection and Measurement Operations ", " FlowUnit ": { " Component ": " ethidium bromide ", " ComponentType ": " Solid ", " UnitArgType ": " MAT ", " Vol ": ["0.25 µL/ mL ", "0.5 µL/ mL ", "2 -3 µL", "10 µL", "15 µL", "10 µg/ mL ", "0.5 µg/µL "] ,</p>
<p>" Container ": " Flask ", " Cond ": { " Concentration ": ["0.0024%" , "0.3 -10 µg/ mL ", "1.5% (w/v)", "5 µM", "1:1000"] ,</p>
<p>" Temperature ": ["25 • C", " room temperature "] , " State ": [" toxic ", " carcinogenic "] , " Charge ": [" positively charged "] } }, " Succ ": " Modification Operations " } , { " Pred ": " Transfer Operations ", " FlowUnit ": { " Component ": " gel ", " ComponentType ": " Semi -Solid ", " UnitArgType ": " MAT ", " Vol ": ["0.5 mL "] , " Container ": [" Gel Cassette ", " Tank ", " Tube "] , " Cond ": { " Impedance ": [" under 20 kOhm "] , " Size ": ["50 -250 nt "] } }, " Succ ": " Transfer Operations " } I CASE STUDIES I.1 CASE STUDY: CONTRIBUTIONS OF THE BUILDING BLOCKS Part of protocol designed by EE+: { " Pred ": "" , " FlowUnit ": { " Component ": " Lysis solution ", " ComponentType ": " Liquid ", " RefName ": " Lysis_solution -1" , " UnitArgType ": " MAT ", " Vol ": "50 µL", " Container ": "" ,</p>
<p>Figure 1 :
1
Figure 1: The representations for protocol design.(A) The example of protocol design by novice and veteran experimental scientists.(B) The hierarchies of our proposed representation, from original full protocol representation, to dual representation of operation-and product-flow-centric views.</p>
<p>Figure 2 :
2
Figure 2: Diagram of automatic representation generation.(A) Illustration of the workflow.(B) Convergence curve of automatic function abstraction.(C) Convergence curve of automatic model abstraction.(D-F) Confusion matrices on operation distribution (D), product distribution (E), and device distribution (F), between DSLs across domains.Correlation scores are low except the ones along the diagonals, indicating the significant inter-domain distinctions between the resulting DSLs.</p>
<p>Figure 3 :
3
Figure 3: Results of protocol design.(A) Profile of text-level similarity between testing sets of the three tasks.(B) Pairwise comparison between the capabilities of different machine designers across the six dimensions.(C-E) Performances of the seven machine designers on the planning (C), modification (D), and adjustment (E) tasks across the six dimensions (index by column).</p>
<p>, we design six-dimensional metrics to comprehensively cover all of the major factors without biased weighting and composition.The six dimensions include: (i) IoU on operations, IoU(Op) = IoU({φ 1...|Φ| }, {φ ′ 1...|Φ ′ | }), IoU between instance actions of the designed protocol Φ and the groundtruth Φ ′ ; (ii) IoU on reagents and intermediate products, IoU(Prod) = IoU({ω 0...|Φ| }, {ω ′ 0...|Φ ′ | }); (iii) IoU on devices, IoU(Dev) = IoU({φ(Dev) 1...|Φ| }, {φ(Dev) ′ 1...|Φ ′ | }), where φ(Dev) t denotes the exact device for conducting the instance action φ t ; (iv) Similarity between the execution sequences, Sim(Exec) = SeqAlign(⟨φ 0...|Φ| ⟩, ⟨φ ′ 0...|Φ ′ | ⟩), where SeqAlign(•, •) denotes the ordered sequence similarity score calculation by the SA algorithm; (v) Similarity between experimental objectives, Sim(Goal) = Cos(S(ρ), S(ρ ′ )), where S(•) represents the serialization operation on structural representations of protocols; (vi) Similarity between complete protocols at parameter-wise level, Sim(Param) = Cos(S(Φ),</p>
<p>(2023); (iv) Encapsulated-Internal(EI), prompting LLM with the DSL with operation-centric view; (v) Encapsulated-External(EE), LLM equipping with the external verifier provided by the DSL with operation-centric view; (vi) Encapsulated-Internal+(EI+), prompting LLM with the DSL with the dual representation; and (vii) Encapsulated-External+(EE+), LLM equipping with the external verifier provided by the DSL with the dual representation.</p>
<p>D IMPLEMENTATION DETAILS D.1 PRIOR MODEL OF PRODUCT FLOW-CENTRIC VIEW &lt; Pred &gt; ::= &lt; Operation .UniqueName &gt; &lt; Succ &gt; ::= &lt; Operation .UniqueName &gt; &lt; FlowUnit &gt; ::= &lt; Component &gt; &lt; ComponentType &gt; &lt; RefName &gt; <Vol > &lt; Container &gt; <em>&lt; Cond &gt; &lt; Component &gt; ::= <STR > &lt; ComponentType &gt; ::= Gas | Liquid | Solid | Semi -Solid | Mixture | ChemicalCompound | BiologicalMaterial | Reagent | PhysicalObject | File / Data | ... [ Known component types ] &lt; RefName &gt; ::= &lt; Component &gt; &lt; Index &gt; &lt; UnitArgType &gt; ::= MAT | PROD <Vol > ::= <REAL > <MEAS > &lt; Container &gt; ::= Tube | Flask | Pipette | ... [ Known container types ] &lt; Cond &gt; ::= &lt; ArgKey &gt; &lt; ArgValue &gt; &lt; ArgKey &gt; ::= Temperature | Pressure | Acidity | Lighting | ... [ Known conditional keys ] &lt; ArgValue &gt; ::= <REAL > <MEAS > D.2 PRIOR MODEL OF OPERATION-CENTRIC VIEW &lt; Operation &gt; ::= &lt; UniqueName &gt; </em>&lt; Pattern &gt; &lt; UniqueName &gt; ::= <STR > &lt; Pattern &gt; ::= &lt; Precond &gt; &lt; Execution &gt; &lt; Postcond &gt; <em>&lt; Example &gt; &lt; Precond &gt; ::= &lt; SlotArgNum &gt; </em>&lt; SlotArg &gt; &lt; SlotArgNum &gt; ::= <INT > &lt; SlotArg &gt; ::= &lt; ProductFlow .FlowUnit .ComponentType &gt; &lt; Postcond &gt; ::= &lt; EmitArgNum &gt; <em>&lt; EmitArg &gt; &lt; EmitArgNum &gt; ::= <INT > &lt; EmitArg &gt; ::= &lt; ProductFlow .FlowUnit .ComponentType &gt; &lt; Example &gt; ::= <STR > &lt; Execution &gt; ::= &lt; DeviceType &gt; &lt; Capacity &gt; </em>&lt; Config &gt; &lt; DeviceType &gt; ::= Incubator | Autoclave | Centrifuge | ... [ Known device types ] &lt; Capacity &gt; ::= <REAL > <MEAS > &lt; Config &gt; ::= &lt; ArgKey &gt; &lt; ArgValue &gt; &lt; ArgKey &gt; ::= Duration | Pace | Power | Quantity | ... [ Known device configuration items ] &lt; ArgValue &gt; ::= <REAL > <MEAS ></p>
<ol>
<li>In Precond and Postcond , use formal name of the component to represent the SlotArg and EmitArg of each step .The component name should clearly describe the content of the component .YOUR TASK : Generate plan in DSL for a protocol for { title }.Here are some extra details about the protocol : { details } You can choose to instantiate the following DSL specification to construct the DSL program : { DSL } Your plan in DSL program :</li>
</ol>
<p>EXAMPLE : { example protocol title } Here are some extra details about the protocol : { example protocol description } example plan in DSL : { example plan } YOUR TASK : Generate plan in DSL for a protocol for { title }.Here are some extra details about the protocol : { details } You can choose to instantiate the following DSL specifications to construct the DSL program : Operation -view DSL specification : { Operation -DSL } Product -view DSL specification : { Product -DSL } Your plan in DSL program : If you believe the error in a particular step is due to the step preparing reagents rather than using a previous intermediate product , you can ignore this error .Output your refined plan in DSL , returning a JSON block without any additional information or comments .YOUR TASK : Refine the plan in DSL for a protocol for { title }.Here are some extra details about the protocol : { details } Refine the following plan : { plan } Here is the feedback of the plan : { feedback } Your refined plan in DSL : [ Prompt for refining the plan according to the feedback vertified by DSL with dual representation ]Your task is to improve a Biology experimental protocol plan represented in domain -specific language ( DSL ) based on provided feedback .The input plan in DSL consists of multiple DSL programs from two perspectives : operation -view and product -view .The DSL programs from these two perspectives alternate and constrain each other .This is the format of a product -view DSL program : // Each product view DSL program represents the state of the product at that moment .{ Pred : &lt; Operation &gt;, // Pred represents the operation that precedes the creation of this product , need to align to the operation name in the operation view DSL program .If the product is in its initial state , return "".FlowUnit : { // FlowUnit defines the properties of the product being processed .Component : , // Component represents the actual product or material being processed , need to be the formal name of the component .ComponentType : Gas | Liquid | Solid | Semi -Solid | Mixture | ChemicalCompound | BiologicalMaterial | Reagent | PhysicalObject | File / Data , // ComponentType describes the type of the component , which can be one of the following : Gas , Liquid , Solid , Semi -Solid , Mixture , The provided feedback indicates errors that occurred when compiling the DSL programs .You need to correct the program to ensure that the state changes of each product ' s RefName in the Product -view are caused by the corresponding operations in the Operation -view .</p>
<p>EFigure A1 :
A1
Figure A1: Comparison between the perplexity of the test set and the reference set</p>
<p>approximately 1 x 10^7 cells by centrifugation at 2000 RPM for 5 min .Aspirate media and resuspend cell pellet with 1 mL of icecold PBS and transfer to a 1 mL centrifuge tube .Microcentrifuge at 2000 RPM for 5 min at 4 • C.</p>
<p>Figure A2 :
A2
Figure A2: Visualization of diversity between novel protocols</p>
<p>Table 1 :
1
Statistics of the testing set.Each cell presents the total number of protocols m and experimental steps n in the form m (n).
Genetics Medical Bioengineering EcologyPlanning10 (130)7 (96)12 (157)2 (25)Modification 37 (442) 15 (225)16 (210)6 (59)Adjustment23 (219)5 (87)2 (26)5 (81)</p>
<p>Table A2 :
A2
Complete quantitative results on protocol design, specifically the modification task.Each cell represents the machine designer's average score (at the top of the cell) on all testing samples across the four domains and the corresponding standard error of mean (at the bottom of the cell).For each dimension, we highlight the results of both the best and the second best ones.
IoU(Op) IoU(Prod) IoU(Dev) Sim(Exec) Sim(Goal) Sim(Param)FB0.1810.0500.0380.3040.7960.809(0.102)(0.071)(0.071)(0.102)(0.090)(0.060)IB0.1500.0380.0390.2810.7720.788(0.100)(0.065)(0.076)(0.100)(0.089)(0.060)II0.3310.1010.0610.4160.8020.851(0.143)(0.131)(0.135)(0.127)(0.087)(0.059)EI0.5930.3180.3360.6020.8660.937(0.186)(0.158)(0.235)(0.164)(0.066)(0.030)EI+0.6480.6260.4130.7650.8830.952(0.210)(0.188)(0.256)(0.170)(0.055)(0.031)EE0.5880.4030.3320.6010.8730.940(0.185)(0.192)(0.228)(0.164)(0.053)(0.028)EE+0.6400.6610.4100.7570.8930.953(0.213)(0.179)(0.253)(0.170)(0.043)(0.032)</p>
<p>Table A3 :
A3
Complete quantitative results on protocol design, specifically the adjustment task.Each cell represents the machine designer's average score (at the top of the cell) on all testing samples across the four domains and the corresponding standard error of mean (at the bottom of the cell).For each dimension, we highlight the results of both the best and the second best ones.
IoU(Op) IoU(Prod) IoU(Dev) Sim(Exec) Sim(Goal) Sim(Param)FB0.1920.0770.0510.3190.8110.823(0.100)(0.104)(0.094)(0.103)(0.078)(0.051)IB0.1970.0390.0060.3370.8020.810(0.131)(0.063)(0.021)(0.141)(0.082)(0.049)II0.4530.1150.0910.5080.8050.873(0.208)(0.161)(0.211)(0.184)(0.081)(0.056)EI0.5870.3280.4000.6230.8630.944(0.190)(0.186)(0.265)(0.165)(0.055)(0.027)EI+0.6680.5450.4490.7750.8830.950(0.208)(0.259)(0.247)(0.152)(0.056)(0.040)EE0.5810.4040.3950.6160.8750.946(0.184)(0.205)(0.261)(0.162)(0.039)(0.026)EE+0.6500.5890.4410.7580.8930.950(0.220)(0.229)(0.248)(0.160)(0.033)(0.042)B.2 DOMAIN-INDEXED COMPLETE RESULTS</p>
<p>Table A5 :
A5
Complete quantitative results on protocol design, specifically the Medical domain.Each cell represents the machine designer's average score (at the top of the cell) on all testing samples across the three tasks and the corresponding standard error of mean (at the bottom of the cell).For each dimension, we highlight the results of both the best and the second best ones.
IoU(Op) IoU(Prod) IoU(Dev) Sim(Exec) Sim(Goal) Sim(Param)FB0.1740.0480.0300.3120.7960.839(0.085)(0.070)(0.067)(0.075)(0.087)(0.043)IB0.1390.0290.0230.2640.7210.795(0.054)(0.038)(0.063)(0.045)(0.123)(0.070)II0.3730.0810.0910.4240.7760.871(0.093)(0.087)(0.205)(0.072)(0.097)(0.041)EI0.6040.3220.3090.5940.8610.932(0.167)(0.146)(0.253)(0.148)(0.079)(0.031)EI+0.6150.5740.4000.7580.8710.952(0.196)(0.242)(0.198)(0.149)(0.060)(0.021)EE0.5910.3730.2980.5830.8730.936(0.158)(0.166)(0.234)(0.149)(0.054)(0.030)EE+0.6150.6130.3900.7560.8910.955(0.197)(0.210)(0.202)(0.151)(0.040)(0.019)</p>
<p>Table A6 :
A6
Complete quantitative results on protocol design, specifically the Ecology domain.Each cell represents the machine designer's average score (at the top of the cell) on all testing samples across the three tasks and the corresponding standard error of mean (at the bottom of the cell).For each dimension, we highlight the results of both the best and the second best ones.
IoU(Op) IoU(Prod) IoU(Dev) Sim(Exec) Sim(Goal) Sim(Param)FB0.1550.0300.0210.2970.7810.807(0.085)(0.035)(0.048)(0.088)(0.096)(0.056)IB0.1620.0060.0300.2750.7630.788(0.118)(0.015)(0.058)(0.105)(0.090)(0.063)II0.3860.0430.0270.4480.7880.856(0.176)(0.062)(0.062)(0.131)(0.065)(0.044)EI0.4580.2590.3510.5140.8790.933(0.171)(0.134)(0.195)(0.142)(0.048)(0.028)EI+0.4110.5690.3590.5860.8880.945(0.134)(0.133)(0.175)(0.127)(0.052)(0.023)EE0.4580.3470.3510.5070.8740.934(0.171)(0.151)(0.195)(0.138)(0.048)(0.029)EE+0.4140.5810.3460.5860.9100.944(0.142)(0.141)(0.177)(0.131)(0.035)(0.024)</p>
<p>Table A7 :
A7
Complete quantitative results on protocol design, specifically the Bioengineering domain.Each cell represents the machine designer's average score (at the top of the cell) on all testing samples across the three tasks and the corresponding standard error of mean (at the bottom of the cell).For each dimension, we highlight the results of both the best and the second best ones.
IoU(Op) IoU(Prod) IoU(Dev) Sim(Exec) Sim(Goal) Sim(Param)FB0.1760.0480.0500.3000.7900.826(0.085)(0.089)(0.081)(0.084)(0.090)(0.042)IB0.1490.0500.0380.2860.7670.797(0.077)(0.087)(0.091)(0.078)(0.083)(0.046)II0.3520.0620.0660.4430.8100.860(0.151)(0.090)(0.187)(0.125)(0.073)(0.045)EI0.5650.3070.3100.6030.8510.930(0.164)(0.186)(0.249)(0.169)(0.072)(0.033)EI+0.6570.5770.3940.7430.8880.944(0.209)(0.177)(0.241)(0.179)(0.056)(0.041)EE0.5580.3920.3030.5980.8550.933(0.162)(0.214)(0.246)(0.165)(0.076)(0.030)EE+0.6530.6140.4010.7420.9000.945(0.206)(0.172)(0.246)(0.176)(0.046)(0.041)C ETHICS STATEMENTC.1 HUMAN EXPERT PARTICIPANTS
&lt; ProductFlow &gt; ::= <Pred > &lt; FlowUnit &gt; <Succ >
https://spacy.io/api/sentencizer
https://spacy.io/api/matcher#<em>title
https://huggingface.co/docs/transformers/v4.45.1/en/model_doc/auto#transformers. AutoTokenizer
https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine</em> similarity.html#cosine-similarity
https://platform.openai.com/docs/guides/batch/batch-api
https://protocolexchange.researchsquare.com/
https://star-protocols.cell.com/
https://bio-protocol.org/en
https://currentprotocols.onlinelibrary.wiley.com/
https://www.jove.com/
ACKNOWLEDGEMENTSThis work was partially supported by the National Natural Science Foundation of China under Grants 52475001.Q. Xu is a visiting student at Peking University from University of Science and Technology of China.The authors would like to thank Haofei Hou for his earlier works regarding domain-specific representations, and also Jiawen Liu for her assistance in figure drawings.Please output id of your selected protocols , separating with a comma .Don ' t output any other information .[ Output format ] id_1 , id_2 , id_3 [ Related protocols ] { context } Answer :[ Prompt for generating NL plan ] Your goal is to generate steps for a biology protocol .These protocol steps must accurately describe a complete scientific protocol to obtain a result .Steps of some similar protocols will be provided as a reference for you to generate the new one .Output should only contain the steps without any other information .Here is an example of how to generate steps for a biology protocol .EXAMPLE : { example protocol title } Here are some extra details about the protocol : { example protocol description } example steps : { example protocol steps } YOUR TASK : Generate steps for a protocol for { title }.Here are some extra details about the protocol : { details } Here are some similar protocols ' steps for reference : { steps } your steps :[ Prompt for translating NL plan to pseudocode ] Your goal is to convert biology protocols into python pseudocode .EXAMPLED.6 EXTERNAL DESIGNERThe external designer combines (i) deductive verification through DSL; and (ii) self-improvement by the LLM(Madaan et al., 2023).In EE, the external verifier is provided by the operation-centric view DSL and performs checks on two main aspects: (i) whether the precondition of each operation is an intermediate product of a previous step rather than appearing from nowhere; and (ii) whether the postcondition of each operation is used in subsequent steps rather than being omitted.Similarly, in EE+, the external verifier is provided by the DSL with a dual representation, focusing on crossverifying the parallel dual tracks (the two perspectives of the DSL program).It checks whether the corresponding operation causes each status transition of the product: (i) whether the product in each product-view program is the output of its preceding operation; and (ii) whether the product in each product-view program is the input for its succeeding operation.If a mismatch occurs, the verifier generates corresponding error messages, such as "Error: The product {product} required by operation {operation} at step {i} is not available from previous steps."These error messages are then fed into the feedback-refine loop as feedback for the LLM to revise the plan.The loop terminates when the program passes the verification or reaches the maximum number of iterations, and the best result is retained based on the verification information.// EmitArg represents the output product or material resulting from the operation , using formal component names from the product perspective DSL program , with serial numbers to distinguish repeated components in different states .} } ] }, " pattern_1 ": { " Precond ": { " SlotArgNum ": 4, " SlotArg ": [" Liquid ", " Liquid ", " Liquid ", " Liquid "] }, " Execution ": { " DeviceType ": " centrifuge ", " capacity ": "1.5 ml ", " Config ": { " time ": "10 -15 min ", " speed ": ["12 ,000 × g", "20 ,000 × g "] ," temperature ": "4 • C", } }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [" Solid "] }, " examples ": [ " Precipitate the cell debris in the lysate by centrifugation at 20 ,000 × g for 10 -15 min at 4 • C ." , " precipitate DNA with 13.5 µL of following mixture (1 µL of 20 mg / ml Glycogen , 12.5 µL of 3 M NaOAc [ pH 5.3]) and 340 µL ethanol .", " precipitate the total RNA by centrifuging at 12 ,000 × g for 15 min at 4" Operation ": " Spin ", " pattern_0 ": { " Precond ": { " SlotArgNum ": 2, " SlotArg ": [" Liquid ", " Liquid "] }, " Execution ": { " DeviceType ": " spin plate ", " Config ": { " time ": ["1 min "] } }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [" Physical Object "] }, " Example ": [ " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [" Liquid "] }, " Example ": [ " Spin lysate at 14 krcf for 10 min at 4 • C; transfer cleared lysate to new tube .", " Spin down the beads for 60 s at 2 ,000 x g.Discard the supernatant by carefully pipetting out the buffer .", " Spin at 12 ,000 × g until the total volume in both filters is reduced to 120 µL ( &lt;=30 min ).Keep aside 5 µL of purified labeled histone for SDS -PAGE analysis ." , " Quickly spin the FACS tube to allow the cell suspension to pass through the filter to remove undigested large tissue debris .", " Spin once for 7 min at 800 g.Use the BD cytofix / cytoperm kit according to the manufacturer ' s instructions and thereafter add antibodies for intracellular detection of IFN and TNF ."] } } , { " Operation ": " Sonicate ", " pattern_0 ": { " Precond ": { " SlotArgNum ": 1, " SlotArg ": [" Liquid "] }, " Execution ": { " DeviceType ": " sonicator ", " Config ": { " time ": ["20 -30 s "] } }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [" Semi -Solid "] }, " Example ": [ " Sonicate the pellet suspension on ice under a 50 % duty cycle for 5 min .", " Agarose gel of sonicated Arabidopsis chromatin .", " Sonicate proteoliposomes for 20 -30 s or 3 times for 10 s , placing on ice in between sonication , if necessary .", " The lipid suspension is sonicated to form small unilamellar vesicles ( SUVs ) ." ] }, " pattern_1 ": { " Precond ": { " SlotArgNum ": 2, " Cond ": { " State ": " Liquid " } }, " Succ ": " Pipette " } , { " Operation ": " Pipette ", " Precond ": { " SlotArgNum ": 1, " SlotArg ": [ " Lysis_solution -1" ] }, " Execution ": { " DeviceType ": " Pipette ", " Config ": { " time ": "10 times ", " volume ": "50 µL" } }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [ " Lysis_solution -2" ] } } Part of protocol designed by EE: { " Operation ": " Add ", " Precond ": { " SlotArgNum ": 1, " SlotArg ": [ " Triton -X" ] }, " Execution ": { " DeviceType ": "8 -channel pipette ", " Config ": { " Volume ": "1% solution " } }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [ " Triton_X_Solution " ] } } { " Operation ": " Run ", " Precond ": { " SlotArgNum ": 1, " SlotArg ": [ " Cell_Lysis_Mixture " ] }, " Execution ": { " DeviceType ": " Thermal Cycler ", " Config ": { " Temperature ": "70 • C", " Time ": "15 min " } }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [ " cDNA_Reverse_Transcription " ] } } Part of protocol designed by EI: { " Operation ": " Run ", " Precond ": { " SlotArgNum ": 1, " SlotArg ": [ " Cell_Lysis_Mixture " ] }, " Execution ": { " DeviceType ": " Thermal Cycler ", " Config ": { " Temperature ": "70 • C", " Time ": "15 min " } }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [ " cDNA_Reverse_Transcription " ] } } Part of protocol designed by II: " reverse_transcribe ": { " muscs ": " cells ", " buffer ": " RT buffer ", " enzyme ": " reverse transcriptase ", " incubation_time ": "60 minutes ", " temperature ": "42" } " prepare_single_cell_suspension ": { " input_cells ": " lysed cells " } Part of protocol designed by IB:" reverse_transcribe ": { " muscs ": " RNA ", " buffer ": " reverse transcription buffer ", " enzyme ": " reverse transcriptase ", " incubation_time ": "60 minutes ", " temperature ": "42" } " prepare_single_cell_suspension ": { " input_cells ": " single -cell samples " } Part of protocol designed by FB:" sort_single_cell ": { " plate ": " PCR plate ", " nozzle_size ": "100 µm", " mode ": " single -cell purity " }I.2 CASE STUDY: HANDLING DIFFERENT TASK COMPLEXITIESPart of protocol designed in Planning:{ " Operation ": " Obtain ", " Precond ": { " SlotArgNum ": 1, " SlotArg ": [ " File / Data " ] }, " Execution ": { " DeviceType ": " QIAGEN Blood &amp; Cell Culture DNA Maxi Kit ", " Config ": {} }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [ " HMW genomic DNA " ] } } Part of protocol designed in Modification:{ " Operation ": " Centrifuge ", " Precond ": { " SlotArgNum ": 1, " SlotArg ": [ " Serum_Plasma_in_PBS -1" ] }, " Execution ": { " DeviceType ": " Ultracentrifuge ", " Config ": { " speed ": [ "12 ,000 × g" ], " time ": [ "20 min " ], " temperature ": [ "4Part of protocol designed in Adjustment:{ " Operation ": " Incubate ", " Precond ": { " SlotArgNum ": 2, " SlotArg ": [ " Washed sections with 1 st antibody -1" , "2 nd antibody mixture -1" ] }, " Execution ": { " DeviceType ": " Moistening box ", " Config ": { " temperature ": "37 C", " time ": "1 h" } }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [ " Sections with 2 nd antibody -1" ] } } I.3 CASE STUDY: GENERALITY ACROSS DOMAINS Part of protocol designed for Bioengineering: { { " Operation ": " Incubate ", " Precond ": { " SlotArgNum ": 2, " SlotArg ": [ " Lysis Mixture -2" , " Stop Buffer -1" ] }, " Execution ": { " DeviceType ": " Thermocycler ", " Config ": { " temperature ": "65 • C", " time ": "30 min " } }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [ " Neutralized Mixture -1" ] } }, { " Pred ": " Incubate ", " FlowUnit ": { " Component ": " Neutralized Mixture ", " ComponentType ": " Mixture ", " RefName ": " Neutralized Mixture -1" , " UnitArgType ": " PROD ", " Vol ": "60 µL", " Container ": "0.2 -ml PCR tube ", " Cond ": { " State ": " Neutralized " } }, " Succ ": " Mix " }, { " Operation ": " Elute ", " Precond ": { " SlotArgNum ": 1, " SlotArg ": [ " Washed Beads -1" ] }, " Execution ": { " DeviceType ": " Centrifuge ", " Config ": { " time ": "1 min " } }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [ " Eluted Product -1" ] } }, { " Pred ": " Elute ", " FlowUnit ": { " Component ": " Eluted Product ", " ComponentType ": " BiologicalMaterial ", " RefName ": " Eluted Product -1" , " UnitArgType ": " PROD ", " Vol ": "50 µL", " Container ": "0.2 -ml PCR tube ", " Cond ": { " State ": " Eluted " } }, " Succ ": " Incubate Clear " }, { " Operation ": " Quantify ", " Precond ": { " SlotArgNum ": 1, " SlotArg ": [ " Clear Eluted Solution -1" ] }, " Execution ": { " DeviceType ": " Nanodrop ", " Config ": {} }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [ " Quantified Sample -1" ] } }, { " Pred ": " Quantify ", " FlowUnit ": { " Component ": " Quantified Sample ", " ComponentType ": " Liquid ", " RefName ": " Quantified Sample -1" , " UnitArgType ": " PROD ", " Vol ": "50 µL", " Container ": "0.2 -ml PCR tube ", " Cond ": { " State ": " Quantified ", " Concentration ": "150 ng /µL", " A260 / A280 ": 1.85 , " A260 / A230 ": 2.1 } }, " Succ ": " Dilute " } } Part of protocol designed for Ecology: { { " Operation ": " Grow ", " Precond ": { " SlotArgNum ": 1, " SlotArg ": [ " Watered_Rice_Plants -1" ] }, " Execution ": { " DeviceType ": " Environmental growth chamber ", " Config ": { " Temperature ": "24 • C", " LightCycle ": "12 h light /12 h dark " } }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [ " Mature rice plants " ] } }, { " Pred ": " Grow ", " FlowUnit ": { " Component ": " Mature rice plants ", " ComponentType ": " BiologicalMaterial ", " RefName ": " Mature_Rice_Plants -1" , " UnitArgType ": " PROD ", " Vol ": "N/A", " Container ": " Plastic pot ", " Cond ": { " State ": " Mature ", " Height ": "50 -60 cm " } }, " Succ ": " Anesthetize " }, { " Operation ": " Collect ", " Precond ": { " SlotArgNum ": 2, " SlotArg ": [ " Monitored_Aphid -1" , " Mature_Rice_Plants -1" ] }, " Execution ": { " DeviceType ": " Microcapillary tube ", " Config ": {} }, " Postcond ": { " EmitArgNum ": 1, " EmitArg ": [ " Phloem sap " ] } }, { " Pred ": " Collect ", " FlowUnit ": { " Component ": " Phloem sap ", " ComponentType ": " Liquid ", " RefName ": " Phloem_Sap -1" , " UnitArgType ": " PROD ", " Vol ": "1 -2 µL", " Container ": " Microcapillary tube ", " Cond ": {
Structure and interpretation of computer programs. Harold Abelson, Gerald Jay Sussman, 1996The MIT Press</p>
<p>Monya Baker. 1,500 scientists lift the lid on reproducibility. arXiv:2311.07361Microsoft Research AI4Science and Microsoft Azure Quantum. The impact of large language models on scientific discovery: a preliminary study using gpt-4. 2023. 2016. 2021533arXiv preprintNature</p>
<p>Building an open representation for biological protocols. Bryan Bartley, Jacob Beal, Miles Rogers, Daniel Bryce, Benjamin Robert P Goldman, Peter Keller, Vanessa Lee, Joshua Biggers, Mark Nowak, Weston, ACM Journal on Emerging Technologies in Computing Systems. 1932023</p>
<p>Reconfigurable system for automated optimization of diverse chemical reactions. Anne-Catherine Bédard, Andrea Adamo, Grace Kosi C Aroh, Aaron A Russell, Jeremy Bedermann, Brian Torosian, Klavs F Yue, Timothy F Jensen, Jamison, Science. 36164082018</p>
<p>Artificial intelligence and natural man. Margaret Boden, Synthese. 4331980</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 2020</p>
<p>Rob Clowes, et al. A mobile robotic chemist. Benjamin Burger, Vladimir V Phillip M Maffettone, Catherine M Gusev, Yang Aitchison, Xiaoyan Bai, Xiaobo Wang, Li, Buyi Ben M Alston, Li, Nature. 58378152020</p>
<p>Extracting training data from large language models. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, 30th USENIX Security Symposium (USENIX Security 21). 2021</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, arXiv:2202.076462022arXiv preprint</p>
<p>Domain-specific languages. Martin Fowler, 2010Pearson Education</p>
<p>Understanding human intelligence through human limitations. L Thomas, Griffiths, Trends in Cognitive Sciences. 202024</p>
<p>A functional basis for engineering design: reconciling and evolving previous efforts. Julie Hirtz, Robert B Stone, Daniel A Mcadams, Simon Szykman, Kristin L Wood, 200213Research in Engineering Design</p>
<p>Introduction to Automata Theory, Languages, and Computation. John E Hopcroft, Rajeev Motwani, Jeffrey D Ullman, 1996Addison-Wesley Longman Publishing Co., Inc</p>
<p>Robotic search for optimal cell culture in regenerative medicine. Taku Genki N Kanda, Motoki Tsuzuki, Noriko Terada, Naohiro Sakai, Tomohiro Motozawa, Mitsuhiro Masuda, Nishida, Tatsuki Chihaya T Watanabe, Higashi, A Shuhei, Horiguchi, 2022Elife11e77007</p>
<p>An integrated self-optimizing programmable chemical synthesis and reaction engine. Alexander Js Artem I Leonov, Slawomir Hammer, S Lach, Dario Hessam M Mehr, Davide Caramelli, Aamir Angelone, Khan, O' Steven, Matthew Sullivan, Liam Craven, Wilbraham, Nature Communications. 15112402024</p>
<p>Keynote address-data abstraction and hierarchy. Barbara Liskov, Addendum to the proceedings on Object-oriented programming systems, languages and applications (Addendum). 1987</p>
<p>Augmenting large language models with chemistry tools. Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, Nature Machine Intelligence. 2024</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 2023</p>
<p>An autonomous portable platform for universal chemical synthesis. Wenduan J Sebastián Manzano, Hou, Przemyslaw Sergey S Zalesskiy, Hsin Frei, Philip J Wang, Leroy Kitson, Cronin, Nature Chemistry. 14112022</p>
<p>An efficient unification algorithm. Alberto Martelli, Ugo Montanari, ACM Transactions on Programming Languages and Systems (TOPLAS). 421982</p>
<p>Programs with common sense. John Mccarthy, 1959London</p>
<p>. Marcia Mcnutt, Reproducibility, Science. 34361682014</p>
<p>A universal system for digitization and automatic execution of the chemical synthesis literature. M Hessam, Matthew Mehr, Artem I Craven, Graham Leonov, Leroy Keenan, Cronin, Science. 37065122020</p>
<p>When and how to develop domain-specific languages. Marjan Mernik, Jan Heering, Anthony M Sloane, ACM Computing Surveys (CSUR). 3742005</p>
<p>Allen Newell. The knowledge level. Stephen Monsell, Trends in Cognitive Sciences. 732003. 1982Artificial Intelligence</p>
<p>Bioplanner: Automatic evaluation of llms on protocol planning in biology. Aleksandar Odhran O'donoghue, John Shtedritski, Ralph Ginger, Ali Abboud, Samuel Ghareeb, Rodriques, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Hierarchical reinforcement learning: A comprehensive survey. Shubham Pateria, Budhitama Subagdja, Ah-Hwee Tan, Chai Quek, ACM Computing Surveys (CSUR). 5452021</p>
<p>Digitization and validation of a chemical synthesis literature database in the chempu. Simon Rohrbach, Mindaugas Šiaučiulis, Greig Chisholm, Petrisor-Alin Pirvan, Michael Saleeb, S Hessam M Mehr, Ekaterina Trushina, I Artem, Graham Leonov, Aamir Keenan, Khan, Science. 37766022022</p>
<p>The concept of mind. Gilbert Ryle, Julia Tanney, 1949Routledge</p>
<p>Different worlds confirmatory versus exploratory research. Simon Schwab, Leonhard Held, Significance. 1722020</p>
<p>PersLEARN: Research Training through the Lens of Perspective Cultivation. Yu-Zhe Shi, Shiqian Li, Xinyi Niu, Qiao Xu, Jiawen Liu, Yifan Xu, Shiyu Gu, Bingru He, Xinyang Li, Xinyu Zhao, Annual Meeting of the Association for Computational Linguistics. 2023a</p>
<p>On the complexity of Bayesian generalization. Yu-Zhe Shi, Manjie Xu, John E Hopcroft, Kun He, Joshua B Tenenbaum, Song-Chun Zhu, Ying Nian Wu, Wenjuan Han, Yixin Zhu, International Conference on Machine Learning. 2023b</p>
<p>AutoDSL: Automated domain-specific language design for structural representation of procedures with constraints. Yu-Zhe Shi, Haofei Hou, Zhangqian Bi, Fanxu Meng, Xiang Wei, Lecheng Ruan, Qining Wang, Yu-Zhe Shi, Haotian Li, Annual Meeting of the Association for Computational Linguistics. 2024a. 2024bIEEE Visualization and Visual Analytics Gen4DS Workshop</p>
<p>Expert-level protocol translation for self-driving labs. Yu-Zhe Shi, Fanxu Meng, Haofei Hou, Zhangqian Bi, Qiao Xu, Lecheng Ruan, Qining Wang, Advances in Neural Information Processing Systems. 2024c</p>
<p>Abstract Hardware Grounding towards the Automated Design of Automation Systems. Yu-Zhe Shi, Qiao Xu, Fanxu Meng, Lecheng Ruan, Qining Wang, International Conference on Intelligent Robotics and Applications. 2024d</p>
<p>Identification of common molecular subsequences. F Temple, Smith, Michael S Waterman, Journal of Molecular Biology. 14711981</p>
<p>Organic synthesis in a modular robotic system driven by a chemical programming language. Sebastian Steiner, Jakob Wolf, Stefan Glatzel, Anna Andreou, M Jarosław, Graham Granda, Trevor Keenan, Gerardo Hinkley, Philip J Aragon-Camarasa, Davide Kitson, Angelone, Science. 363642322112019</p>
<p>Ekin Dogus Cubuk, Amil Merchant, et al. An autonomous laboratory for the accelerated synthesis of novel materials. Nathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J Mcdermott, Max Gallant, Nature. 62479902023</p>
<p>Grammar prompting for domain-specific language generation with large language models. Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A Saurous, Yoon Kim, Advances in Neural Information Processing Systems. 2023a</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023b</p>
<p>Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Quoc V Dai, Le, arXiv:2109.01652Finetuned language models are zero-shot learners. 2021arXiv preprint</p>
<p>Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, arXiv:2310.19341A more open bilingual foundation model. 2023arXiv preprint</p>
<p>Chain-of-experts: When llms meet complex operations research problems. Ziyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Jessica Yuan, Xiongwei Wang, Xiaojin Han, Tao Fu, Jia Zhong, Mingli Zeng, Song, International Conference on Learning Representations. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
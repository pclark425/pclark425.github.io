<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9719 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9719</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9719</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-272146585</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.16081v2.pdf" target="_blank">Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are increasingly explored as general-purpose reasoners, particularly in agentic contexts. However, their outputs remain prone to mathematical and logical errors. This is especially challenging in open-ended tasks, where unstructured outputs lack explicit ground truth and may contain subtle inconsistencies. To address this issue, we propose Logic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs with formal logic to enable validation and refinement of natural language reasoning. LELMA comprises three components: an LLM-Reasoner, an LLM-Translator, and a Solver, and employs autoformalization to translate reasoning into logic representations, which are then used to assess logical validity. Using game-theoretic scenarios such as the Prisoner's Dilemma as testbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro) and advanced (GPT-4o) models in generating logically sound reasoning. LELMA achieves high accuracy in error detection and improves reasoning correctness via self-refinement, particularly in GPT-4o. The study also highlights challenges in autoformalization accuracy and in evaluation of inherently ambiguous open-ended reasoning tasks.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9719.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9719.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-as-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (used as an automatic judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o was used to automatically evaluate correctness of natural-language reasoning produced by LLM agents; its ratings were compared against aggregated human judgments and another LLM judge (Claude 3.7 Sonnet). The paper reports low agreement between GPT-4o-as-judge and human evaluators, especially on more challenging outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluation of game-theoretic natural-language reasoning (Prisoner's Dilemma, Stag Hunt, Hawk-Dove)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o (gpt-4o-2024-08-06 version noted for judging experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Each LLM judge received a simplified version of the human evaluation protocol and the generated reasoning samples; judges produced binary correctness ratings (correct/incorrect) based on the protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Three independent human evaluators manually reviewed every reasoning sample using a pre-defined evaluation protocol and scoring sheet; a sample was labeled correct only if all three agreed (conservative unanimous aggregation).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's Kappa between each LLM judge and aggregated human judgment. Reported values: for reasoning generated by Gemini 1.0 Pro, GPT-4o-as-judge vs aggregated humans κ = 0.044; for reasoning generated by GPT-4o, GPT-4o-as-judge vs aggregated humans κ = 0.159. Also Fleiss' Kappa (human inter-rater): κ = 0.31 for GPT-4o-generated reasoning (fair agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Substantial loss of alignment with human judgment: GPT-4o as judge shows very low agreement with humans and therefore frequently diverges from human evaluations, missing or disagreeing on errors; this reduces trustworthiness and reliability of automated evaluation and can fail to detect subtle/logically complex errors that humans identify.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Concrete divergences reported: Cohen's Kappa κ = 0.044 (GPT-4o judge vs aggregated humans) on Gemini-generated reasoning, and κ = 0.159 on GPT-4o-generated reasoning — both indicate very low agreement. The paper states GPT-4o, when acting as a judge, showed low agreement with both human raters and Claude 3.7 Sonnet.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>None reported where GPT-4o-as-judge matched humans reliably; the paper highlights that more sophisticated (harder-to-evaluate) outputs correlate with lower agreement, which partly explains GPT-4o judge's poor alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Evaluation and 'Inter-rater agreement' paragraphs in Results; Figures 2 and 3; Discussion (sections describing LLM-as-a-judge comparisons and Cohen's Kappa values).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9719.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9719.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-as-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.7 Sonnet (used as an automatic judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude 3.7 Sonnet was used as an alternative LLM judge and compared with human evaluators; it achieved substantially higher agreement with human raters in some conditions, but still missed errors in the most challenging cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluation of game-theoretic natural-language reasoning (Prisoner's Dilemma, Stag Hunt, Hawk-Dove)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Claude 3.7 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Claude received the same simplified evaluation protocol as other LLM judges and rated correctness of the generated reasoning samples.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Three independent human evaluators using the pre-defined evaluation protocol and scoring sheet; unanimous agreement required to mark a sample correct.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's Kappa between Claude and aggregated human judgments. Reported values: for reasoning generated by Gemini 1.0 Pro, Claude vs aggregated humans κ = 0.555 (substantially higher and comparable to inter-human agreement); for reasoning generated by GPT-4o, Claude vs aggregated humans κ = 0.301. Human Fleiss' Kappa reported as κ = 0.59 for Gemini outputs (moderate agreement) and κ = 0.31 for GPT-4o outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>While Claude often aligned much better with humans than GPT-4o did, it still exhibited omission of errors in the hardest cases (notably reasoning produced by GPT-4o), meaning some subtle logical mistakes were missed; thus LLM-as-judge can give a false sense of reliability and may systematically omit errors on complex outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Paper reports Claude κ = 0.555 vs humans on Gemini-generated reasoning (good alignment), but for GPT-4o-generated reasoning Claude's agreement dropped to κ = 0.301 and the paper states Claude 'frequent[ly] omission of errors' for GPT-4o outputs — i.e., Claude often failed to detect errors in the most challenging reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Claude matched human agreement levels in the easier/less sophisticated case (Gemini-generated reasoning) where κ = 0.555 is comparable to inter-human agreement, indicating LLM-as-judge can be adequate for less challenging outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Evaluation and 'Inter-rater agreement' paragraphs in Results; Figures 2 and 3; Discussion (statements on Claude's agreement and omission of errors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9719.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9719.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge (summary)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge comparisons with human evaluation (summary from paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper evaluates LLMs as automatic judges versus human evaluators and finds that model choice matters: some LLM judges (Claude 3.7 Sonnet) can approach human agreement on easier outputs, while others (GPT-4o-as-judge) show very low agreement; overall limitations include missed errors, false positives, and reduced trustworthiness for open-ended, sophisticated outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluative judgment of natural-language chain-of-thought reasoning in game-theoretic tasks</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4o and Claude 3.7 Sonnet (both tested as judges)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Each LLM judge was given a simplified version of the human evaluation protocol and the reasoning samples to rate correctness; comparisons were made against unanimous aggregated human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Three independent human evaluators; an explicit evaluation protocol and scoring sheet were used; conservative aggregation labeled a sample correct only if all three agreed. Fleiss' Kappa computed to measure inter-human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's Kappa for LLM vs aggregated humans (reported κ values: GPT-4o judge κ = 0.044 on Gemini outputs and κ = 0.159 on GPT-4o outputs; Claude κ = 0.555 on Gemini outputs and κ = 0.301 on GPT-4o outputs). Fleiss' Kappa for human raters: κ = 0.59 (Gemini) and κ = 0.31 (GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Losses include: reduced agreement with human judgments (especially for more advanced/sophisticated outputs), omission of subtle logical errors, susceptibility to false positives/negatives (which undermine trust), variability across judge models, and sensitivity to translation/representation errors that can produce false negatives/positives; overall LLM judges are not yet a reliable substitute for humans in this open-ended logical-evaluation setting.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Numerical examples: GPT-4o-as-judge κ down to 0.044 (almost no agreement) on Gemini outputs; GPT-4o-as-judge κ = 0.159 on GPT-4o outputs; Claude omitting errors for GPT-4o outputs (agreement κ = 0.301). The paper also reports that false positives (flagging correct reasoning as erroneous) were more common than false negatives in detection, undermining trust.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Claude 3.7 Sonnet achieved agreement comparable to human judges for less challenging outputs (κ = 0.555 on Gemini outputs), suggesting LLM judges can be useful in some contexts; the paper also notes that improving translation accuracy, fine-tuning judges, or using human-LLM hybrid evaluation may mitigate many losses.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Results (Evaluation; Confusion matrices and accuracy; Inter-rater agreement sections), Figures 2–3, and Discussion (statements summarizing LLM-as-a-judge performance and limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>LLM-based nlg evaluation: Current status and challenges <em>(Rating: 2)</em></li>
                <li>GTBench: Uncovering the strategic reasoning limitations of LLMs via game-theoretic evaluations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9719",
    "paper_id": "paper-272146585",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "GPT-4o-as-judge",
            "name_full": "GPT-4o (used as an automatic judge)",
            "brief_description": "GPT-4o was used to automatically evaluate correctness of natural-language reasoning produced by LLM agents; its ratings were compared against aggregated human judgments and another LLM judge (Claude 3.7 Sonnet). The paper reports low agreement between GPT-4o-as-judge and human evaluators, especially on more challenging outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Evaluation of game-theoretic natural-language reasoning (Prisoner's Dilemma, Stag Hunt, Hawk-Dove)",
            "llm_judge_model": "GPT-4o (gpt-4o-2024-08-06 version noted for judging experiments)",
            "llm_judge_setup": "Each LLM judge received a simplified version of the human evaluation protocol and the generated reasoning samples; judges produced binary correctness ratings (correct/incorrect) based on the protocol.",
            "human_evaluation_setup": "Three independent human evaluators manually reviewed every reasoning sample using a pre-defined evaluation protocol and scoring sheet; a sample was labeled correct only if all three agreed (conservative unanimous aggregation).",
            "agreement_metric": "Cohen's Kappa between each LLM judge and aggregated human judgment. Reported values: for reasoning generated by Gemini 1.0 Pro, GPT-4o-as-judge vs aggregated humans κ = 0.044; for reasoning generated by GPT-4o, GPT-4o-as-judge vs aggregated humans κ = 0.159. Also Fleiss' Kappa (human inter-rater): κ = 0.31 for GPT-4o-generated reasoning (fair agreement).",
            "losses_identified": "Substantial loss of alignment with human judgment: GPT-4o as judge shows very low agreement with humans and therefore frequently diverges from human evaluations, missing or disagreeing on errors; this reduces trustworthiness and reliability of automated evaluation and can fail to detect subtle/logically complex errors that humans identify.",
            "examples_of_loss": "Concrete divergences reported: Cohen's Kappa κ = 0.044 (GPT-4o judge vs aggregated humans) on Gemini-generated reasoning, and κ = 0.159 on GPT-4o-generated reasoning — both indicate very low agreement. The paper states GPT-4o, when acting as a judge, showed low agreement with both human raters and Claude 3.7 Sonnet.",
            "counterexamples_or_caveats": "None reported where GPT-4o-as-judge matched humans reliably; the paper highlights that more sophisticated (harder-to-evaluate) outputs correlate with lower agreement, which partly explains GPT-4o judge's poor alignment.",
            "paper_reference": "Evaluation and 'Inter-rater agreement' paragraphs in Results; Figures 2 and 3; Discussion (sections describing LLM-as-a-judge comparisons and Cohen's Kappa values).",
            "uuid": "e9719.0",
            "source_info": {
                "paper_title": "Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Claude-as-judge",
            "name_full": "Claude 3.7 Sonnet (used as an automatic judge)",
            "brief_description": "Claude 3.7 Sonnet was used as an alternative LLM judge and compared with human evaluators; it achieved substantially higher agreement with human raters in some conditions, but still missed errors in the most challenging cases.",
            "citation_title": "",
            "mention_or_use": "use",
            "task_domain": "Evaluation of game-theoretic natural-language reasoning (Prisoner's Dilemma, Stag Hunt, Hawk-Dove)",
            "llm_judge_model": "Claude 3.7 Sonnet",
            "llm_judge_setup": "Claude received the same simplified evaluation protocol as other LLM judges and rated correctness of the generated reasoning samples.",
            "human_evaluation_setup": "Three independent human evaluators using the pre-defined evaluation protocol and scoring sheet; unanimous agreement required to mark a sample correct.",
            "agreement_metric": "Cohen's Kappa between Claude and aggregated human judgments. Reported values: for reasoning generated by Gemini 1.0 Pro, Claude vs aggregated humans κ = 0.555 (substantially higher and comparable to inter-human agreement); for reasoning generated by GPT-4o, Claude vs aggregated humans κ = 0.301. Human Fleiss' Kappa reported as κ = 0.59 for Gemini outputs (moderate agreement) and κ = 0.31 for GPT-4o outputs.",
            "losses_identified": "While Claude often aligned much better with humans than GPT-4o did, it still exhibited omission of errors in the hardest cases (notably reasoning produced by GPT-4o), meaning some subtle logical mistakes were missed; thus LLM-as-judge can give a false sense of reliability and may systematically omit errors on complex outputs.",
            "examples_of_loss": "Paper reports Claude κ = 0.555 vs humans on Gemini-generated reasoning (good alignment), but for GPT-4o-generated reasoning Claude's agreement dropped to κ = 0.301 and the paper states Claude 'frequent[ly] omission of errors' for GPT-4o outputs — i.e., Claude often failed to detect errors in the most challenging reasoning.",
            "counterexamples_or_caveats": "Claude matched human agreement levels in the easier/less sophisticated case (Gemini-generated reasoning) where κ = 0.555 is comparable to inter-human agreement, indicating LLM-as-judge can be adequate for less challenging outputs.",
            "paper_reference": "Evaluation and 'Inter-rater agreement' paragraphs in Results; Figures 2 and 3; Discussion (statements on Claude's agreement and omission of errors).",
            "uuid": "e9719.1",
            "source_info": {
                "paper_title": "Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "LLM-as-a-judge (summary)",
            "name_full": "LLM-as-a-judge comparisons with human evaluation (summary from paper)",
            "brief_description": "The paper evaluates LLMs as automatic judges versus human evaluators and finds that model choice matters: some LLM judges (Claude 3.7 Sonnet) can approach human agreement on easier outputs, while others (GPT-4o-as-judge) show very low agreement; overall limitations include missed errors, false positives, and reduced trustworthiness for open-ended, sophisticated outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Evaluative judgment of natural-language chain-of-thought reasoning in game-theoretic tasks",
            "llm_judge_model": "GPT-4o and Claude 3.7 Sonnet (both tested as judges)",
            "llm_judge_setup": "Each LLM judge was given a simplified version of the human evaluation protocol and the reasoning samples to rate correctness; comparisons were made against unanimous aggregated human labels.",
            "human_evaluation_setup": "Three independent human evaluators; an explicit evaluation protocol and scoring sheet were used; conservative aggregation labeled a sample correct only if all three agreed. Fleiss' Kappa computed to measure inter-human agreement.",
            "agreement_metric": "Cohen's Kappa for LLM vs aggregated humans (reported κ values: GPT-4o judge κ = 0.044 on Gemini outputs and κ = 0.159 on GPT-4o outputs; Claude κ = 0.555 on Gemini outputs and κ = 0.301 on GPT-4o outputs). Fleiss' Kappa for human raters: κ = 0.59 (Gemini) and κ = 0.31 (GPT-4o).",
            "losses_identified": "Losses include: reduced agreement with human judgments (especially for more advanced/sophisticated outputs), omission of subtle logical errors, susceptibility to false positives/negatives (which undermine trust), variability across judge models, and sensitivity to translation/representation errors that can produce false negatives/positives; overall LLM judges are not yet a reliable substitute for humans in this open-ended logical-evaluation setting.",
            "examples_of_loss": "Numerical examples: GPT-4o-as-judge κ down to 0.044 (almost no agreement) on Gemini outputs; GPT-4o-as-judge κ = 0.159 on GPT-4o outputs; Claude omitting errors for GPT-4o outputs (agreement κ = 0.301). The paper also reports that false positives (flagging correct reasoning as erroneous) were more common than false negatives in detection, undermining trust.",
            "counterexamples_or_caveats": "Claude 3.7 Sonnet achieved agreement comparable to human judges for less challenging outputs (κ = 0.555 on Gemini outputs), suggesting LLM judges can be useful in some contexts; the paper also notes that improving translation accuracy, fine-tuning judges, or using human-LLM hybrid evaluation may mitigate many losses.",
            "paper_reference": "Results (Evaluation; Confusion matrices and accuracy; Inter-rater agreement sections), Figures 2–3, and Discussion (statements summarizing LLM-as-a-judge performance and limitations).",
            "uuid": "e9719.2",
            "source_info": {
                "paper_title": "Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "LLM-based nlg evaluation: Current status and challenges",
            "rating": 2,
            "sanitized_title": "llmbased_nlg_evaluation_current_status_and_challenges"
        },
        {
            "paper_title": "GTBench: Uncovering the strategic reasoning limitations of LLMs via game-theoretic evaluations",
            "rating": 1,
            "sanitized_title": "gtbench_uncovering_the_strategic_reasoning_limitations_of_llms_via_gametheoretic_evaluations"
        }
    ],
    "cost": 0.010997749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents</p>
<p>Agnieszka Mensfelt 
Royal Holloway
University of London
EghamSurreyUK</p>
<p>Kostas Stathis 
Royal Holloway
University of London
EghamSurreyUK</p>
<p>Vince Trencsenyi 
Royal Holloway
University of London
EghamSurreyUK</p>
<p>Towards Logically Sound Natural Language Reasoning with Logic-Enhanced Language Model Agents
8FACFA8326017C48528639A0F74DB73A
Large language models (LLMs) are increasingly explored as general-purpose reasoners, particularly in agentic contexts.However, their outputs remain prone to mathematical and logical errors.This is especially challenging in open-ended tasks, where unstructured outputs lack explicit ground truth and may contain subtle inconsistencies.To address this issue, we propose Logic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs with formal logic to enable validation and refinement of natural language reasoning.LELMA comprises three components: an LLM-Reasoner, an LLM-Translator, and a Solver, and employs autoformalization to translate reasoning into logic representations, which are then used to assess logical validity.Using game-theoretic scenarios such as the Prisoner's Dilemma as testbeds, we highlight the limitations of both less capable (Gemini 1.0 Pro) and advanced (GPT-4o) models in generating logically sound reasoning.LELMA achieves high accuracy in error detection and improves reasoning correctness via self-refinement, particularly in GPT-4o.The study also highlights challenges in autoformalization accuracy and in evaluation of inherently ambiguous open-ended reasoning tasks.</p>
<p>Introduction</p>
<p>Large language models (LLMs) are capable of generating sophisticated natural language explanations and decisions across a wide range of tasks.Their ability to process and produce human-like text makes them attractive candidates for use in agentic AI systems, where autonomous agents interact with both humans and other agents through natural language communication.This vision includes scenarios ranging from collaborative multi-agent systems to negotiation, coordination, and human-agent teaming [30,10].</p>
<p>However, despite their extensive capabilities, LLMs remain fundamentally unreliable reasoners.Their outputs often suffer from mathematical, logical, and factual errors, as well as hallucinations [19,1].Furthermore, although significant research effort has been devoted to improving single-agent safety and alignment, ensuring that a model produces safe and accurate outputs in isolation does not automatically guarantee safe and reliable behavior in multi-agent settings [4].</p>
<p>In this context, game theory [28] offers a particularly suitable testbed.As a formal model of strategic interaction, it allows controlled evaluation of reasoning in multi-agent scenarios where choices must be justified based on payoffs, incentives, and anticipated actions of others.Recent works have explored LLMs as agents in game-theoretic environments, primarily focusing on ac-tion optimality -whether models select payoff-maximizing strategies [2,11,17,24].However, optimality alone may result from wellrepresented game-theoretic examples in the training data, and thus does not guarantee robustness when the model encounters novel or out-of-distribution scenarios.</p>
<p>In this work, we shift the focus from action outcomes to reasoning processes.We introduce Logic-Enhanced Language Model Agents (LELMA), a framework that integrates LLMs with formal logic to validate and refine natural language reasoning.LELMA employs a modular architecture consisting of three components: an LLM-Reasoner, an LLM-Translator, and a formal Solver.Through a process of autoformalization [39,18,20,40,29,8,6,12], the system translates natural language reasoning into formal logic queries, enabling logical validity checks.Detected errors trigger feedback, allowing the LLM to iteratively improve its reasoning through selfrefinement.</p>
<p>Using classic game-theoretic scenarios such as the Prisoner's Dilemma as evaluation tasks, we investigate reasoning quality in both advanced (GPT-4o) and less capable (Gemini 1.0 Pro) models.Our results show that LELMA effectively improves reasoning correctness -particularly for GPT-4o.Still, challenges remain in translation accuracy and evaluation of open-ended, ambiguous reasoning.This highlights the need for more robust methods to ensure that LLMs, when deployed as agents in interactive settings, reason logically soundly.</p>
<p>This paper makes the following contributions:</p>
<p>(i) We introduce, a neurosymbolic framework that integrates LLMgenerated reasoning with formal logic to validate and refine natural language outputs.(ii) We conduct extensive experiments with GPT-4o and Gemini 1.0 Pro, showing improvements in reasoning correctness, especially for advanced models.(iii) We identify key challenges, including limitations in autoformalization and evaluation of open-ended reasoning.</p>
<p>The remainder of this paper is organized as follows.Section 2 presents the preliminaries, covering large language models, game theory, and general game playing.Section 3 introduces the LELMA framework, followed by the detailed methodology in Section 4. Section 5 presents the experimental evaluation and key findings.We discuss these results in depth in Section 6, and finally, Section 7 summarizes the work and outlines directions for future research.</p>
<p>arXiv:2408.16081v2 [cs.AI] 29 May 2025 2 Preliminaries</p>
<p>Large Language Models</p>
<p>The term Large Language Model typically refers to a model based on the transformer architecture with billions of parameters.In this work, GPT-4o and Gemini 1.0 Pro were used to compare a state-of-the-art (SOTA) model with a less capable counterpart.</p>
<p>GPT-4o</p>
<p>OpenAI's GPT-4o [27]-based on and successor of GPT-4 [1]-a current SOTA LLM.Its multi-modal architecture supports text, image and audio inputs and outputs.It was pre-trained with publicly accessible data and third-party resources, and fine-tuned with human feedback.GPT-4o demonstrates enhanced performance in reasoning tasks compared to its predecessor and is reported to have increased factuality.However, GPT-4o still faces challenges with reliability due to hallucinations.</p>
<p>Gemini 1.0 Pro Gemini 1.0 Pro is Google's previous largest model, with good general performance [32].The Gemini model family supports various forms of textual, visual and audio inputs and can produce interleaved textual and visual outputs.Gemini models are pre-trained on publicly available resources for each modality.They support external tool use and web search, increasing their performance in reasoning tasks.</p>
<p>Game Theory</p>
<p>Game theory provides the mathematical tools to analyse the strategic interaction between decision-makers [28].Its applications range from analysing day-to-day social situations to complex political or economic problems.</p>
<p>Games in our experiments</p>
<p>Table 1.General payoff matrix and specific payoff matrices for considered games.We experiment with symmetric non-cooperative games presented in normal form, usually a matrix showing the players, strategies, and payoffs [31].Table 2a demonstrates a general game matrix, where the row player's actions are denoted by U and D, and the column player's actions are marked by L and R. The four possible outcomes W,X,Y,Z denote the players' payoff πi for the given action pairs as the tuple (πrow, π col ).
row/col L R U (Wrow, W col ) (Xrow, X col ) D (Yrow, Y col ) (Zrow, Z col )(
All games in our experiment can be mapped over the general game matrix.We classify actions U and L as "Defect" denoted by D and actions D and R as "Cooperate" denoted by C.This allows us to use the well-known terminology from Axelrod's tournaments to define these games in terms of the four outcomes: The Prisoner's Dilemma (PD) is a classical concept in game theory involving two players.Each player can decide to cooperate for mutual benefit or betray their partner (defect) for individual reward, characterised by the matrix in Table 2b.The Stag Hunt (SH) is a coordination game, where the players have two options: be selfish and hunt Hare (D), or be cooperative and hunt Stag (C).The SH has a payoff structure that favours mutual cooperation the most, instead of the temptation to defect as in the PD-Table 2c.Our third game, Hawk Dove (HD), involves the defective action Hawk, and cooperative action Dove.As demonstrated by the game matrix in Table 2d, HD is described by having the temptation to defect as the most rewarding outcome, but the mutual defection yields the least amount of payoff.</p>
<p>General game playing</p>
<p>General game playing ( [15]) aims at creating intelligent systems that understand the rules of arbitrary new games and learns to play them without human intervention.The Game Description Language (GDL) has been proposed as a formal, machine-processable language for describing the rules of arbitrary games ( [25]).GDL focused on information games only, so it was extended in GDL-II ( [35]) to cover n-player games with incomplete information and games in extensive normal form ([36]). GDL-II is based on the standard syntax and semantics of logic programming and characterised by the special keywords shown in Table 2.A difficulty with GDL systems is that learning without human guidance poses a reasoning challenge.Players must infer the possible actions of others, essentially evaluating hypothetical game situations before taking action.Action formalisms like the classical Situation Calculus [26] have been developed for precisely this purpose.Formal schemes and inference methods are readily available for Situation Calculus [16,23], while their deployment in general game playing presupposes a translation from GDL into existing, suitably expressive action languages.One such scheme [34] shows how to fully embed GDL-II into a version of the Situation Calculus based on knowledge fluents [33].Our game solver, presented later in section 3.2, is inspired by these previous works, to support light GDL specifications that can be readily combined with LLM APIs to validate their reasoning.</p>
<p>3 Logic-Enhanced Language Model Agents The Logic-Enhanced Language Model Agents (LELMA) framework is implemented in Python and Prolog [38].Owing to the framework's modularity, the Prolog solver can be easily replaced with another type of formal solver.The system (Fig. 1) comprises the following components:</p>
<p>Reasoner: An LLM agent responsible for generating natural language reasoning.Translator: An LLM agent that maps statements from the Reasoner's output to logical queries sent to the Solver.Solver: A standard logic program implemented in Prolog that evaluates the queries.Feedback loop: A mechanism for providing natural language feedback to the Reasoner when a query evaluation fails.The source code of the framework and the results of the experimental evaluation are publicly available. 1</p>
<p>Solver</p>
<p>We specify the solver module as a logic program implemented in Prolog ( [22], [38]), a declarative programming language well-suited for high-level problem definitions.Logic programs enhance the reliability of stochastic LLM outputs by providing a robust framework for logical rule-based reasoning.Our solver consists of three main components: a game-independent part representing the rules of any game in normal form, a game-dependent part defining the rules of a specific game using the predicates of the game-independent part, and a set of auxiliary predicates built on top of these representations to evaluate LLM reasoning for that specific game.We adhere to the usual conventions for representing a game as an implementable logic program: variables are denoted by uppercase letters and predicates and function symbols by lowercase letters.An underscore (_) denotes a variable whose value is unused within a definition.The state of a game is represented by a situation, initially defined as a constant (e.g., s0).The special binary function do(M, S) represents the situation resulting from the execution of move M in situation S, as in the Situation Calculus.In the logical rules that follow, the symbols if, and, and not correspond to their logical equivalents.</p>
<p>Game-independent part</p>
<p>We recursively specify as a logic program all legal transitions of an extended form game from an initial situation S to a final situation F as follows: A fluent F holds in the initial situation, a new fluent F is initiated by the effects of a move M executed in a situation S, and a fluent F persists after a move is made, provided it is not abnormal; abnormal fluents are terminated (do not persist).We also use rules of the form:
finally(F, S) if Conditions.
to return derived fluents F describing the result of the game, when the Conditions hold in the final situation S.</p>
<p>Game-dependent part</p>
<p>To represent a specific game we need to define game-dependent predicates for the initial state initial/1, the legal moves legal/2, what holds in the initial game situation via initially/2, the effects of a move on a situation via effect/3, what stops persisting in a situation after the execution of a move via abnormal/3, the final situation final/1, and the result of the game via finally/2 definitions.To exemplify these definitions, we show how to describe a PD game to our solver.The initial situation s0 is defined as:
initial(s0).
What holds in this initial situation we specify it as: initially(player(p1), s0).initially(player(p2), s0).initially(role(p1,row), s0).initially(role(p2,col), s0).initially(control(p1), s0).initially(control(p2), s0).</p>
<p>These assertions define first the player names represented by unique identifiers (p1 and p2), their roles (p1 is the row player, while p2 is the column player), and then the fact that initially either of them can the game next (via the control/1 fluent,as in GDL).What holds in the initial situation changes as a result of move being made in the game.We represent moves as terms of the form choice(P, M), where P is a player, and M is a move.As it is possible for any player in a Prisoner's Dilemma game to choose defect ('D') or cooperate ('C'), we write this as: possible(choice(P, 'D'), S) if holds(player(P), S). possible(choice(P, 'C'), S) if holds(player(P), S).</p>
<p>It is then legal for a player to choose a possible move if they have the control to execute it in the current situation:</p>
<p>legal(choice(P, M), S) if possible(choice(P, M), S) and holds(control(P), S).</p>
<p>When a legal move M is made by a player P, the effect that this move is actually made is recorded in the next situation:</p>
<p>effect(did(P, M), choice(P, M), S).</p>
<p>Once a legal move is executed, the player loses control, which we specify in our framework as:</p>
<p>abnormal(control(P), choice(P, M), S).</p>
<p>In other words, after a choice is made by a player, the player loses control and therefore cannot play a move again from that situation onwards.Moves made in this way bring about the final situation, which we specify as a situation term with two choices made from the initial situation, one for each player.Assuming the payoff matrix of Table 2b defined as:</p>
<p>payoff('D', 'D', 1, 1).payoff('C', 'D', 0, 5).payoff('D', 'C', 5, 0).payoff('C', 'C', 3, 3).</p>
<p>the outcome of the game holds information about the actual moves made by the players, and their payoffs.</p>
<p>finally(outcome(P1,M1,U1,P2,M2,U2), S) if final(S) and holds(role(P1, row), S) and holds(did(P1, M1), S) and holds(role(P2, col), S) and holds(did(P2, M2), S) and payoff(M1, M2, U1, U2).</p>
<p>We can then extract specific outcome information, e.g. the player's utility, through a goal/2 fluent (as in GDL) e.g.:</p>
<p>finally(goal(P1, U1), S) if finally(outcome(P1, <em>, U1, </em>, <em>, </em>), S). finally(goal(P2, U2), S) if finally(outcome(<em>, </em>, <em>, P2, </em>, U2), S).</p>
<p>This completes the definition of a PD game, and allows us to use the above game description to reason about the game.If, in the specifications above, we substitute the symbol if with ':-', the symbol and with ',', and the symbol not with '+' (representing negation by failure), we obtain an executable Prolog program that a player can query to perform reasoning within the game.For example, if player p1 wanted to reason about the game and determine the actions needed to achieve a utility of 5, they could express this as the query shown below (queries are initiated with the symbol '?-'): ?-game(s0,F), finally(goal(p1,5),F).based on the game's payoff matrix, our solver provides the following answers for F:</p>
<p>1 do(choice(p2,'C'),do(choice(p1,'D'),s0)); 2 do(choice(p1,'D'),do(choice(p2,'C'),s0)); 3 false.</p>
<p>In the first answer, p1 acted first and p2 second, while in the second answer, the order is reversed.This is not unexpected, as in our game definition both players have control initially, so the answers show both combinations.</p>
<p>Translator</p>
<p>Table 3.A set of queries corresponding to natural language statements and their explanation, as provided to the Translator.For each predicate that determines the higher/highest payoff of a given type, except for * , there exists a corresponding predicate determining the lower/lowest payoff.</p>
<p>finally(outcome(you,B,1,them,R,$ _),S) 'you' corresponds to the reasoner, 'them' to their opponent, 10 to payoff, and 'B', 'R' to choices assumed to give this payoff to the reasoner higher(1, 3) 1 and 3 correspond to numerical payoffs, 1 is assumed to be higher than 3 highest_possible_individual_payoff(1) 1 corresponds to the assumed highest possible payoff for the reasoner highest_individual_payoff_for_choice(1,B) 1 corresponds to the assumed highest individual payoff for a given choice, e.g.'B' highest_guaranteed_payoff_choice(B) * 'B' corresponds to the choice assumed to give the highest guaranteed (worst-case) individual payoff higher_guaranteed_payoff(B,R) 'B' and 'R' correspond to choices, and 'B' is assumed to give the higher guaranteed payoff highest_mutual_payoff(R,R) 'R' and 'R' correspond to choices, assumed to give the highest mutual payoff</p>
<p>The Solver is extended with predicates derived from a preliminary analysis of common LLM reasoning errors in the evaluation task.Translation employs one-shot learning, using a prompt that lists predicates with sample arguments and their natural language descriptions (Table 3).For failed predicates, the Solver identifies correct variable values; the queries are then translated back into natural language and incorporated into feedback for the Reasoner.Queries that are syntactically invalid or do not match expected forms are discarded.Constants outside the allowable set are replaced with variables.</p>
<p>Feedback loop</p>
<p>The feedback loop logic is shown in Listing 1.The max attempts parameter limits the number of reasoning iterations.If all attempts fail (i.e., each contains some failed queries), the final attempt is returned as the most refined.The feedback prompt is dynamically populated with natural language corrections for each failed query.For example, if the original reasoning includes "I choose B because it gives me payoff 3 which is higher than 5," the corresponding query higher (3,5).fails and is translated as "payoff 3 is lower than payoff 5." The prompt instructs the agent to reconsider its reasoning in light of these corrections, but also allows reaffirming the original reasoning.This accounts for possible false positives-that is, queries that were incorrectly translated.While the Solver guarantees the correctness of the generated feedback, some feedback--such as that suggesting a particular choice yields a higher guaranteed payofmay potentially bias the agent's reasoning toward that option.</p>
<p>Methods</p>
<p>Task</p>
<p>To evaluate the effectiveness of the LELMA framework in detecting and correcting natural language reasoning errors, we conducted experiments using the games introduced in Section 2.2.The rules and payoff structures of each game were presented in a natural language instruction prompt, following the style commonly used in humansubject game-theoretic studies.The expected output was an action choice accompanied by natural language reasoning, consistent with the Chain-of-Thought [37] approach.Correctness was defined as the absence of logical errors in the reasoning about payoffs.</p>
<p>To evaluate both the robustness of the approach and the baseline performance of LLMs, we deliberately obfuscated the task-that is, we modified its presentation to differ from formats likely present in the training data.Prior work has shown that LLM performance can degrade when familiar tasks are presented in unfamiliar ways [21].In our setup, the standard game-theoretic action labels were replaced with abstract symbols 'B' and 'R'.Furthermore, preliminary experiments indicated improved performance from GPT-4o when the payoff matrix followed a specific layout-particularly when the Cooperate/Cooperate outcome appeared in the top-left corner.Consequently, we inverted the matrix order in the experimental instructions.</p>
<p>Settings</p>
<p>In the experiments, we used two LLMs as LLM-Reasoner agents: GPT-4o and Gemini 1.0 Pro.In both setups, GPT-4o was used as the LLM-Translator.The experimental parameters are summarized in Table 4.</p>
<p>Correctness criteria and evaluation</p>
<p>In evaluating correctness, we assume that both actions in the considered games are valid and can be justified by factors such as risk aversion or cooperativeness (e.g., a cooperatively minded agent may choose "Cooperate" even if it does not yield the highest utility).A reasoning sample is marked as incorrect only if it contains logical or mathematical errors related to payoff assignment or inference-errors that human participants typically do not make (e.g., "payoff 3 is higher than payoff 5").Misuse of game-theoretic terminology (e.g., incorrect application of Nash equilibrium) does not count against correctness.Accordingly, the evaluation focuses strictly on the logical accuracy of reasoning about payoffs, independent of theoretical knowledge.</p>
<p>To assess the framework's accuracy in detecting such errors, we conducted a manual evaluation of all reasoning samples generated in the experiments.To ensure objectivity and reproducibility, we developed an evaluation protocol and accompanying scoring sheet (available in the repository).Three independent evaluators reviewed each sample to reduce the impact of individual oversight.Due to the task's cognitive demands, some errors may have been missed, and false negatives (missed errors) are more likely than false positives.Thus, we adopted a conservative aggregation method, labeling a sample as correct only if all evaluators agreed on its correctness.</p>
<p>Additionally, to assess the utility of the LLM-as-a-judge [41] approach, we evaluated the same reasoning samples using two LLMs: GPT-4o (gpt-4o-2024-08-06 version) and Claude 3.7 Sonnet [3], a state-of-the-art model not involved in generation.Each model was given a simplified version of the evaluation protocol used by human judges.Their ratings were then compared against human evaluations.Table 5 presents the distribution of reasoning attempts for GPT-4o and Gemini 1.0 Pro.For GPT-4o, 78% of samples required only one or two attempts, with the maximum number of attempts (five) reached in just four cases.In all four instances, the final attempt was correct.In contrast, Gemini 1.0 Pro performed worse: 27% of its samples reached the five-attempt limit, and in 83% of those cases, the final attempt remained incorrect.</p>
<p>Results</p>
<p>Experiments</p>
<p>Attempts distribution</p>
<p>Choices distribution</p>
<p>Table 6 shows the frequency of choosing action 'B' in both the initial and final reasoning attempts for GPT-4o and Gemini 1.0 Pro.When Overall, risk-averse choices became more frequent in final attempts.The Hawk-Dove game showed the highest consistency, with 'Dove' being the dominant action for both models: GPT-4o selected it in 100% of initial and final attempts, while Gemini 1.0 Pro increased from 66.67% initially to 70% in the final attempt.This reflects a preference for avoiding the worst possible payoff (0), even at the expense of forgoing the highest payoff.</p>
<p>In contrast, the Prisoner's Dilemma exhibited the most pronounced shift.For GPT-4o, the frequency of choosing 'Cooperate' dropped from 96.67% to 23.33% after feedback, indicating that initial cooperative choices were often based on incorrect reasoning.In the Stag Hunt, feedback similarly led to reduced selection of the less riskaverse 'Stag': for GPT-4o, from 100% to 33%, and for Gemini 1.0 Pro, from 80% to 60%.</p>
<p>Correctness</p>
<p>Table 7 presents the percentage of correct reasoning samples in the initial and final attempts for both models.Initially, both LLMs exhibited low correctness: GPT-4o achieved an average of 22.22%, while Gemini 1.0 Pro achieved 18.79%.After receiving corrective feedback, GPT-4o demonstrated significant improvement, reaching an average correctness of 77.78%, whereas Gemini 1.0 Pro showed lower gain, reaching 36.65%.</p>
<p>A detailed analysis of the reasoning samples revealed that GPT-4o was often able to incorporate feedback effectively, correcting prior errors-sometimes resulting in a change of action.This effect was particularly notable in the Prisoner's Dilemma and Stag Hunt scenarios (see Table 6).In contrast, Gemini 1.0 Pro's reasoning frequently reiterated previous justifications, either repeating the same errors or introducing new ones.This pattern is consistent with the number of required attempts reported in Table 5.  Confusion matrices were computed by comparing manual evaluations (representing ground-truth correctness) with the presence or absence of failed predicates (representing predicted correctness).Tables 8 and 9 show the confusion matrices for the initial reasoning attempts of both LLMs.Overall, error recognition was high, with accuracy rates of 84% for GPT-4o and 86% for Gemini 1.0 Pro.The lowest detection accuracy was observed in the Hawk-Dove game-73% for GPT-4o and 81% for Gemini 1.0 Pro.Among the detection errors, false positives were more common than false negatives.</p>
<p>Evaluation</p>
<p>Confusion matrices and accuracy</p>
<p>False positives undermine trustworthiness, as they indicate correct reasoning being incorrectly flagged as erroneous.In contrast, false negatives primarily increase computational cost without compromising correctness.Since the feedback generated by the solver is always factually accurate, false negatives are unlikely to cause an agent to revise correct reasoning into incorrect reasoning.Cohen's Kappa To assess inter-rater agreement among the three human evaluators, Fleiss' Kappa [13] was computed.For GPT-4o, the Kappa value was κ = 0.31, indicating fair agreement, while for Gemini 1.0 Pro, it was κ = 0.59, indicating moderate agreement.These results underscore the inherent challenges of the evaluation task.The higher agreement for Gemini 1.0 Pro likely reflects the presence of more apparent and easier-to-detect errors compared to those found in GPT-4o.</p>
<p>Inter-rater agreement</p>
<p>To explore LLM-as-a-judge as a scalable alternative to human evaluation, we measured agreement using Cohen's Kappa [7] between each LLM and the aggregated human judgment, as well as pairwise agreement among all raters (human and LLMs).Figures 2  and 3 display the pairwise Cohen's Kappa values for reasoning generated by Gemini 1.0 Pro and GPT-4o, respectively.</p>
<p>GPT-4o, when acting as a judge, showed low agreement with both human raters and Claude 3.7 Sonnet.In contrast, Claude 3.7 Sonnet exhibited substantially higher agreement with human raters-comparable to inter-human agreement.</p>
<p>For reasoning generated by Gemini 1.0 Pro, the agreement between GPT-4o as a judge and the aggregated human judgment was low, with Cohen's Kappa κ = 0.044.In contrast, Claude 3.7 Sonnet showed substantially higher agreement with the aggregated human ratings, achieving κ = 0.555.For reasoning generated by GPT-4o, the agreement between GPT-4o as a judge and the aggregated human judgment was similarly low, with κ = 0.159.Claude 3.7 Sonnet again showed better alignment, with κ = 0.301.</p>
<p>Computational cost</p>
<p>In experiments with GPT-4o, an average of 1269.77tokens were used per attempt in the instruction prompt, and the model generated an average of 1224.38 tokens in response.The average duration of a single reasoning attempt was 16.53 seconds for GPT-4o and 34.66 seconds for Gemini 1.0 Pro.The total runtime for all experiments was approximately 53 minutes for GPT-4o and 173 minutes for Gemini.</p>
<p>Discussion</p>
<p>LELMA Accuracy The framework achieved 84% accuracy in detecting reasoning correctness for GPT-4o and 86% for Gemini 1.0 Pro.Although error detection accuracy was high for both models, the improvement resulting from corrective feedback was considerably greater for GPT-4o (ranging from 27 to 73 percentage points) compared to Gemini 1.0 Pro (10 to 27 percentage points).Notably, after receiving validation-based feedback, GPT-4o's cooperation rate in the Prisoner's Dilemma (PD) aligned more closely with rates observed in human participant experiments [5].GPT-4o more effectively incorporated feedback to refine its reasoning.By contrast, while Gemini 1.0 Pro was often less successful at correcting errors, it occasionally produced errors related to incorrect calculations of expected payoffs-issues not covered by the existing set of predicates.This suggests that predicate design should be preceded by systematic error sampling to ensure adequate coverage of model-specific error types.</p>
<p>LLM Performance in Social Dilemmas LLMs have attracted increasing interest as potential agents in game-theoretic contexts [2,11,17,24].This study evaluated the correctness of strategic reasoning produced by GPT-4o and Gemini 1.0 Pro across three oneshot games.Initial correctness, prior to any feedback, was generally low, ranging from 3.33% to 46.67%.These results highlight the limitations of existing automated benchmarks (e.g., [9]), which tend to emphasize outcome optimality rather than the validity of reasoning.Furthermore, the findings underscore the challenges of employing LLMs as agents in social simulations.</p>
<p>Despite the high overall accuracy in detecting incorrect reasoning, the process was not entirely error-free.Detection performance depended on the accuracy of translating natural language into formal predicates, a task handled by a separate LLM instance.Although translating atomic statements into formal representations is generally simpler than generating coherent reasoning, it remains a nontrivial challenge with room for improvement.One potential avenue for enhancing translation -and thus improving error detection -is to fine-tune the LLM responsible for the translation task.</p>
<p>LELMA Evaluation</p>
<p>The moderate and fair inter-rater agreement values indicate that this evaluation task is challenging even for human judges.The lower agreement observed for the more advanced model suggests that in open-ended tasks such as this, more sophisticated outputs demand greater cognitive effort to assess, making errors harder to detect.Although manual evaluation remains essential at this stage, it is not a scalable solution.Potential alternatives include automated and semi-automated methods, such as LLM-derived metrics, prompting LLMs, human-LLM collaborative evaluation, and fine-tuning approaches [14].</p>
<p>We assessed the LLM-as-a-judge approach using an adapted version of our evaluation protocol.Among the two tested models, Claude 3.7 Sonnet achieved inter-rater agreement levels comparable to those between human judges.However, in the more challenging case of reasoning generated by GPT-4o, the agreement between Claude and aggregated human evaluations was low, indicating frequent omission of errors.Thus, it cannot yet be considered a reliable alternative to human judgment in this context.Among the remaining options, fine-tuning appears particularly promising, provided that a sufficiently large and representative training dataset can be gathered.</p>
<p>Limitations This study has several limitations.First, the need for manual assessment of reasoning correctness constrained the size of the evaluation sample.Second, the accuracy of translating natural language into formal representations directly impacts the effectiveness of reasoning error detection.Imperfect translation may lead to false negatives, which undermine trustworthiness, and false positives, which increase computational cost and may inadvertently bias the model toward specific choices.Therefore, further improvement in translation accuracy is needed.</p>
<p>Conclusions and Future Work</p>
<p>This work introduces LELMA, a framework for validating the logical soundness of natural language reasoning generated by large language models.The initial evaluation of GPT-4o and Gemini 1.0 Pro in game-theoretic scenarios revealed frequent reasoning errors, particularly in payoff attribution and comparison.Experimental results demonstrated that reasoning correctness improved for both models-especially for GPT-4o-when integrated within the LELMA framework.The evaluation also highlighted the need to improve translation accuracy and underscored the challenges of assessing ambiguous, open-ended tasks.</p>
<p>Future work will focus on enhancing the accuracy of natural language to query translation, potentially through fine-tuning.We also plan to validate the framework across a wider range of language models.Given LELMA's modular design, which enables the flexible substitution of LLM agents, query sets, and solvers, we aim to extend its application to additional domains beyond game theory.</p>
<p>3 )
3
a) A general game in matrix form.(d) Hawk-Dove: T &gt; R &gt; S &gt; P .</p>
<p>T</p>
<p>(Temptation to defect): (D, C) R (Reward for mutual cooperation): (C, C) P (Punishment for mutual defection): (D, D) S (Sucker's payoff): (C, D)</p>
<p>"Figure 1 .
1
Figure 1.Overview of the LELMA framework.The relevant statements generated by the Reasoner are translated into a set of queries Q, which are evaluated by the Solver.If any of the queries fail, the Reasoner is prompted to refine its reasoning in light of the facts derived from the failed queries.</p>
<p>final(S) ifground(S) and S = do(choice(<em>, </em>), do(choice(<em>, </em>), I)) and initial(I).</p>
<p>Table 8 .
8
Confusion matrices for initial reasoning for Gemini 1.0 Pro.Rows: actual correctness, columns: predicted correctness, 1: correct, 0: incorrect, A: accuracy.</p>
<p>Figure 2 .
2
Figure 2. Pairwise inter-rater agreement for GPT-4o.</p>
<p>Figure 3 .
3
Figure 3. Pairwise inter-rater agreement for Gemini 1.0 Pro.</p>
<p>Table 2 .
2
Subset of GDL-II (uppercase letters denote variables, lowercase letters denote predicates/function symbols)
role(R)R is a playerinit(F)F holds in the initial positiontrue(F)F holds in the current positionlegal(R, M) R can do move M in the current positiondoes(R, M)player R does move Mnext(F)F holds in the next positionterminalthe current position is terminalgoal(R, N)R gets N points in the current position</p>
<p>Table 4 .
4
Experimental parameters.* gpt-4o-2024-05-13.
Hawk-DoveGamesPrisoner's DilemmaStag HuntLLM-Reasoner AgentsGPT-4o  *  Gemini 1.0 ProTemperature1Maximum output tokens1024Maximum attempts number 5Repetitions30</p>
<p>Table 5 .
5
Distribution of reasoning for GPT-4o and Gemini 1.0 Pro.
Attempts12345GPT-4o24 46 10 64Gemini 1.0 Pro 26 20 11 9 24</p>
<p>Table 6 .
6
Choice 'B' distribution for initial and final reasoning (%).
GPT-4oGemini 1.0 ProGame Initial (%) Final (%) Initial (%) Final (%)HD100.00100.0066.6770.00PD96.6723.3336.6730.00SH100.0033.3380.0060.00</p>
<p>Table 7 .
7
Percentage of correct reasoning samples in the original and final attempt.no feedback was provided, the initial choice remained unchanged.The action 'B' corresponds to 'Dove' in Hawk-Dove, 'Cooperate' in the Prisoner's Dilemma, and 'Stag' in Stag Hunt.
GPT-4oGemini 1.0 ProGame Initial (%) Final (%) Initial (%) Final (%)HD46.6773.3325.8135.48PD3.3370.0017.2434.48SH16.6790.0013.3340.00</p>
<p>Table 9 .
9
Confusion matrices for initial reasoning of GPT-4o.Rows: actual correctness, columns: predicted correctness, 1: correct, 0: incorrect, A: accuracy.Total (A=84%)
HD (A=73%)PD (A=93%)SH (A=87%)101010101 1551 1261 011 32096102100 1 280 2 23
https://github.com/dicelab-rhul/LELMA
AcknowledgementsThis work was supported by a Leverhulme Trust International Professorship Grant (LIP-2022-001).We also thank Heartwin Haveluck for his assistance in the evaluation.
. J Achiam, S Adler, S Agarwal, arXiv:2303.087742023Gpt-4 technical report. arXiv preprint</p>
<p>Playing repeated games with large language models. E Akata, L Schulz, J Coda-Forno, arXiv:2305.168672023arXiv preprint</p>
<p>Large language model. Anthropic, Claude, 2025</p>
<p>Foundational challenges in assuring alignment and safety of large language models. U Anwar, A Saparov, J Rando, D Paleka, M Turpin, P Hase, E S Lubana, E Jenner, S Casper, O Sourbut, arXiv:2404.099322024arXiv preprint</p>
<p>Cooperation under the shadow of the future: experimental evidence from infinitely repeated games. P D Bó, American economic review. 9552005</p>
<p>Nl2tl: Transforming natural languages to temporal logics using large language models. Y Chen, R Gandhi, Y Zhang, C Fan, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>A coefficient of agreement for nominal scales. J Cohen, Educational and psychological measurement. 2011960</p>
<p>nl2spec: interactively translating unstructured natural language to temporal logics with large language models. M Cosler, C Hahn, D Mendoza, F Schmitt, C Trippel, International Conference on Computer Aided Verification. Springer2023</p>
<p>Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations. J Duan, R Zhang, J Diffenderfer, B Kailkhura, L Sun, E Stengel-Eskin, M Bansal, T Chen, K Xu, arXiv:2402.123482024arXiv preprint</p>
<p>Agent ai: Surveying the horizons of multimodal interaction. Z Durante, Q Huang, N Wake, R Gong, J S Park, B Sarkar, R Taori, Y Noda, D Terzopoulos, Y Choi, arXiv:2401.035682024arXiv preprint</p>
<p>Can large language models serve as rational players in game theory? a systematic analysis. C Fan, J Chen, Y Jin, H He, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>J Feng, R Xu, J Hao, H Sharma, Y Shen, D Zhao, W Chen, arXiv:2311.06158Language models can be logical solvers. 2023arXiv preprint</p>
<p>Measuring nominal scale agreement among many raters. J L Fleiss, Psychological bulletin. 7653781971</p>
<p>M Gao, X Hu, J Ruan, X Pu, X Wan, arXiv:2402.01383LLM-based nlg evaluation: Current status and challenges. 2024arXiv preprint</p>
<p>General game playing: Overview of the AAAI competition. M R Genesereth, N Love, B Pell, AI Mag. 2622005</p>
<p>Situation calculus based programs for representing and reasoning about game structures. G D Giacomo, Y Lespérance, A R Pearce, Principles of Knowledge Representation and Reasoning: Proceedings of the Twelfth International Conference, KR 2010. F Lin, U Sattler, M Truszczynski, Toronto, Ontario, CanadaAAAI PressMay 9-13, 2010. 2010</p>
<p>F Guo, arXiv:2305.05516GPT agents in game theory experiments. 2023arXiv preprint</p>
<p>Solving math word problems by combining language models with symbolic solvers. J He-Yueya, G Poesia, R E Wang, N D Goodman, arXiv:2304.091022023arXiv preprint</p>
<p>S Imani, L Du, H Shrivastava, arXiv:2303.05398Mathprompter: Mathematical reasoning using large language models. 2023arXiv preprint</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. A Q Jiang, S Welleck, J P Zhou, W Li, J Liu, M Jamnik, T Lacroix, Y Wu, G Lample, arXiv:2210.122832022arXiv preprint</p>
<p>Llms can't plan, but can help planning in llm-modulo frameworks. S Kambhampati, K Valmeekam, L Guan, M Verma, K Stechly, S Bhambri, L Saldyt, A Murthy, arXiv:2402.018172024arXiv preprint</p>
<p>P Körner, M Leuschel, J Barbosa, V S Costa, V Dahl, M V Hermenegildo, J F Morales, J Wielemaker, D Diaz, S Abreu, G Ciatto, 10.1017/S1471068422000102Fifty Years of Prolog and Beyond. Theory and Practice of Logic Programming. 202222</p>
<p>Abstraction of situation calculus concurrent game structures. Y Lesperance, G De Giacomo, M Rostamigiv, S M Khan, 10.1609/aaai.v38i9.28933Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMar. 202438</p>
<p>Strategic behavior of large language models. N Lorè, B Heydari, arXiv:2309.05898Game structure vs. contextual framing. 2023arXiv preprint</p>
<p>General Game Playing: Game Description Language Specification. N Love, T Hinrichs, D Haley, E Schkufza, M Genesereth, LG-2006-012006Stanford UniversityTechnical Report</p>
<p>Some philosophical problems from the standpoint of artificial intelligence. J Mccarthy, P Hayes, Readings in Artificial Intelligence. B L Webber, N J Nilsson, Morgan Kaufmann1981</p>
<p>. Openai, Gpt-4o, 25/07/2024</p>
<p>Introduction to Game Theory. M Osborne, 2004Oxford University PressUSA</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. L Pan, A Albalak, X Wang, W Wang, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>Games and information an introduction to game theory. E Rasmusen, 2006Blackwell</p>
<p>J.-B A Rohan Anil, Sebastian Borgeaud, arXiv:2312.11805A family of highly capable multimodal models. 2024arXiv preprint</p>
<p>Knowledge, action, and the frame problem. R B Scherl, H J Levesque, 10.1016/S0004-3702(02)00365-XArtif. Intell. 1441-23652003</p>
<p>Reasoning about general games described in gdl-ii. S Schiffel, M Thielscher, 10.1609/aaai.v25i1.7944Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceAug. 201125</p>
<p>A general game description language for incomplete information games. M Thielscher, Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI'10. the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI'10AAAI Press2010</p>
<p>The general game playing description language is universal. M Thielscher, Proceedings of the 22nd International Joint Conference on Artificial Intelligence. T Walsh, the 22nd International Joint Conference on Artificial IntelligenceBarcelona, Catalonia, Spain2011. July 16-22, 2011. 2011IJCAI/AAAI</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>Swi-prolog. Theory and Practice of Logic Programming. J Wielemaker, T Schrijvers, M Triska, T Lager, 201212</p>
<p>Autoformalization with large language models. Y Wu, A Q Jiang, W Li, M Rabe, C Staats, M Jamnik, C Szegedy, Advances in Neural Information Processing Systems. 202235</p>
<p>Coupling large language models with logic programming for robust and general reasoning from text. Z Yang, A Ishay, J Lee, arXiv:2307.076962023arXiv preprint</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E Xing, Advances in Neural Information Processing Systems. 202336</p>            </div>
        </div>

    </div>
</body>
</html>
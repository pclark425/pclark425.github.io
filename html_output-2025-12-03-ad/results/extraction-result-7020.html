<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7020 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7020</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7020</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-232240435</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2021.emnlp-main.351.pdf" target="_blank">Structural Adapters in Pretrained Language Models for AMR-to-Text Generation</a></p>
                <p><strong>Paper Abstract:</strong> Pretrained language models (PLM) have recently advanced graph-to-text generation, where the input graph is linearized into a sequence and fed into the PLM to obtain its representation. However, efficiently encoding the graph structure in PLMs is challenging because such models were pretrained on natural language, and modeling structured data may lead to catastrophic forgetting of distributional knowledge. In this paper, we propose StructAdapt, an adapter method to encode graph structure into PLMs. Contrary to prior work, StructAdapt effectively models interactions among the nodes based on the graph connectivity, only training graph structure-aware adapter parameters. In this way, we incorporate task-specific knowledge while maintaining the topological structure of the graph. We empirically show the benefits of explicitly encoding graph structure into PLMs using StructAdapt, outperforming the state of the art on two AMR-to-text datasets, training only 5.1% of the PLM parameters.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7020.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7020.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DFS linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Depth-first traversal AMR linearization (canonical)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential serialization of an AMR graph produced by a depth-first traversal of the canonical human-created AMR; nodes and edge labels are emitted as a token sequence that can be fed to pretrained language models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Depth-first linearization (canonical)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes the AMR as a token sequence by performing a depth-first traversal of the canonical AMR and emitting nodes and edges in traversal order. Re-entrant node variables are replaced by their co-referring concept, producing a linear sequence of node and edge labels.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential; token-based; lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>depth-first traversal of canonical AMR</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2017T10, LDC2020T02</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (graph-to-text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (base, large) encoder-decoder PLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained T5 encoder-decoder Transformer (T5-base and T5-large) used with adapters; encoder optionally frozen while adapter modules are trained.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, chrF++, MF-score (meaning component), BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Baseline FINE-TUNE using DFS linearization: T5-base BLEU 38.3 (LDC2017T10), T5-large BLEU 41.2 (LDC2017T10); other metrics reported (chrF++, M, BERT) in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Allows direct use of PLMs by turning graphs into text sequences; however, models fine-tuned on this representation can suffer from catastrophic forgetting of language-pretraining and must infer graph connectivity from sequence tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Dilutes explicit graph connectivity (structural information is weakened), may cause catastrophic forgetting when fine-tuning PLMs on non-linguistic sequences, and requires the model to recover edge connections from sequence context.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Outperforms naive sequential encoders trained from scratch only when combined with structure-aware modules; STRUCTADAPT leveraging DFS linearization improves performance substantially versus plain fine-tuning on the same linearization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7020.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7020.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PENMAN notation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PENMAN AMR notation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-readable parenthesized representation of AMR (PENMAN) that preserves tree-like structure and uses parentheses and variable names to encode graph structure; used in prior graph-to-text work as a textual input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>PENMAN notation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encodes AMR in a bracketed, PENMAN format where relations and reentrancies are shown via parentheses and variable references; yields longer token sequences compared to node/edge linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>hierarchical/sequential; token-based; more explicit (less lossy than naive linearization)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>PENMAN formatted serialization (parentheses and variable tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Discussed in relation to AMR corpora (prior work) — not used in main experiments</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (mentioned as alternative representation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mentioned in context of GPT-2 / PLMs in prior work</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prior works applied large language models to PENMAN inputs (making input sequences longer); authors note this is orthogonal to STRUCTADAPT and could be incorporated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not directly reported in this paper for PENMAN (prior works reported strong results)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>PENMAN increases input length (more tokens) which can change pretraining-transfer dynamics; prior work achieved strong results but this paper did not directly evaluate PENMAN.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Longer inputs (higher token cost); orthogonal to the adapters approach and not used in core experiments here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Authors note prior works using PENMAN obtain strong results and that PENMAN is orthogonal to STRUCTADAPT; STRUCTADAPT could also be applied on top of PENMAN encodings.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7020.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7020.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bipartite + token graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bipartite unlabeled graph conversion and token-level graph mapping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two-step conversion: (1) convert labeled AMR graph into an unlabeled bipartite graph by turning each labeled edge (u, r, v) into two unlabeled edges (u,r) and (r,v); (2) expand nodes into subword tokens and build a token-level graph connecting tokens of source and target nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Bipartite unlabeled graph -> Token graph (subword nodes connected)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>First replace each labeled edge with an intermediate node for the relation (bipartite conversion) to create an unlabeled graph; then split node labels into subword tokens and create a token graph by connecting every token of a source node to every token of a target node (or according to rep variants).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph-preserving (lossless with respect to the expanded token graph), token-based, structured</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Bipartite conversion of edges to relation-nodes, then tokenization into subword nodes and inter-token edge creation (connect token sets for adjacent graph nodes)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2017T10, LDC2020T02</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (with STRUCTADAPT graph-convolutional adapters)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 encoder-decoder with frozen pretrained parameters and STRUCTADAPT adapters that run graph convolutions over the token graph at each encoder layer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, chrF++, MF-score (meaning), BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Used as the input representation for STRUCTADAPT experiments; best STRUCTADAPT variants (RGCN/GCN) using token-graph representations achieved state-of-the-art BLEU (e.g., T5-large STRUCTADAPT-RGCN BLEU 46.6 on LDC2017T10 and 48.0 on LDC2020T02 in reported tables).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables explicit propagation of graph connectivity into PLM representations via STRUCTADAPT's graph convolution, improving accuracy while training few task-specific parameters (e.g., 5.1% of PLM parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires mapping and additional graph-structured adapter computations; expanding nodes into tokens increases graph size; average token counts not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared favorably to sequential-only linearizations and to fully-connected (complete) token graphs — authors found explicit token graph connectivity (especially rep1 variant) outperforms a complete graph and improves robustness to linearization permutations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7020.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7020.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>rep1/rep2/rep3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Subword token connection strategies (rep1, rep2, rep3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three alternative designs for how tokens of multi-token nodes are connected across edges: rep1 connects every token of source to every token of target; rep2 connects last token of source to first token of target and within-node tokens sequentially; rep3 connects first token to first token and within-node tokens sequentially.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Subword token representations: rep1, rep2, rep3</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>All representations operate on tokenized node labels; rep1: full bipartite token-token connections between source and target node tokens; rep2: connect last token of source to first token of target and chain tokens within a node sequentially; rep3: connect first token of source to first token of target and chain tokens within a node sequentially.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token-based; structured graph; variants trade off connectivity vs. sparsity</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Token-level edge construction according to rep1/rep2/rep3 rules applied after tokenizing each node label</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2017T10 (dev analyses) and LDC2020T02</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 with STRUCTADAPT (GCN/RGCN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained T5 plus STRUCTADAPT graph-convolution adapters that use the token graph formed by rep variants to aggregate neighborhood information at every encoder layer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (primary), chrF++, MF-score, BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports rep1 as the best configuration; combined with STRUCTADAPT this setting yielded the top-performing systems in the paper (e.g., STRUCTADAPT-RGCN on T5-large achieving BLEU up to 46.6 on LDC2017T10 and 48.0 on LDC2020T02 in reported tables). Exact numeric ablation per-rep variant not tabulated in main text but rep1 highlighted as best.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Choosing rep1 increased direct token-to-token influence across node boundaries and produced better downstream generation when used with STRUCTADAPT, improving model robustness and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>rep1 increases connectivity (more edges), which may increase computational cost of graph convolution; rep2/rep3 try to reduce edges but were empirically inferior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>rep1 > rep2, rep3 and > complete graph in performance; rep1 gives more direct interactions and better token influence between neighboring node tokens per the authors' analysis.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7020.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7020.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Complete graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fully-connected token/sequence graph (complete graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representation that connects every token/node to every other token/node (mirrors self-attention fully-connected view) — evaluated as an extreme baseline to test the value of explicit original graph connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Complete (fully-connected) graph representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>All tokens (nodes and edge-nodes) are connected to all other tokens, yielding a fully-connected graph structure analogous to treating the sequence as a fully connected graph for graph convolutions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph (complete), token-based; highly lossy w.r.t. original topology</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Fully connect token-level nodes (all-pairs edges) irrespective of original AMR connectivity</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2017T10 (dev comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation (ablation/comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 with STRUCTADAPT variants</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 with adapter-based graph convolution applied over a complete token graph.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (dev comparisons reported in Table 6)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Complete graph representation produced relatively inferior performance compared to explicit nodes+edges token graph variants (rep1), as stated in the paper (exact BLEU numbers in Table 6 of paper).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Less effective structural signal to the graph-convolution adapters; demonstrates that explicit AMR connectivity matters.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Does not preserve original topology; inferior performance compared to representations that encode original graph connectivity; blurs structural inductive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Complete graph < explicit nodes+edges representations (e.g., rep1); shows that naively treating sequences as fully connected graphs is suboptimal for graph-to-text when graph structure matters.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7020.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7020.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nodes-only (RGCN adapter)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Node-only linearization with relations handled by RGCN adapters</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linearization variant where only nodes have explicit textual representations in the sequence and edge relation types are not emitted as tokens but instead are modeled by the adapter's relational GCN parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Node-only representation (edges as adapter relations)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode only AMR nodes in the input sequence while omitting explicit edge tokens; relation types and directions are captured by the STRUCTADAPT RGCN relation-aware adapter weights rather than by sequence tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (nodes only) combined with relational graph encoding (adapter) — semi-lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Emit node tokens only; use RGCN (adapter) with relation types (default and reverse) to model edge information</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2017T10 (dev comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 + STRUCTADAPT-RGCN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 encoder with layer-wise STRUCTADAPT using relational GCN updates to capture edge directionality and relation-specific transforms; PLM parameters frozen, only adapter params trained.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (dev comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Authors report that explicitly considering nodes and edges (nodes+edges) is beneficial, but nodes-only variant has the advantage of allowing the model to handle new edge relations during inference; exact BLEU delta for nodes-only vs nodes+edges is reported in Table 6 (nodes+edges better overall).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Reduces sequence length (fewer tokens) and shifts edge modeling into adapter parameters; can generalize to unseen edge labels at inference but may lose the direct signal of explicit edge tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>While flexible to new edge relations, empirically slightly worse than explicit nodes+edges tokenization for generation quality in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Nodes+edges (with rep1) outperformed nodes-only in generation quality; nodes-only advantageous in that edge types are represented in model parameters rather than tied to the vocabulary.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7020.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7020.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearization permutations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph linearization permutations: CANON, RECONF, RANDOM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Three alternative linearization orders of the same AMR graph used to test robustness: CANON (canonical human order), RECONF (reconfigured canonical order ignoring much of canonical sequence), RANDOM (valid traversal starting at a random node).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Canonical / Reconfigured / Random linearizations (CANON, RECONF, RANDOM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Different orderings of the depth-first linearization: CANON uses canonical order from AMR corpora, RECONF shuffles canonical order except top node, RANDOM starts traversal from a random node and disregards canonical order while remaining a valid traversal.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (ordering variants); lossy in same ways as DFS linearization</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Depth-first traversal with different node visit order schemes (canonical, reconfigured, random start)</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LDC2017T10 (robustness experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR-to-text generation; robustness to linearization</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5 (FINE-TUNE, ADAPT, STRUCTADAPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5 encoder-decoder used in multiple training regimes (full fine-tuning, adapter-based fine-tuning, STRUCTADAPT with graph convolutions); comparisons across linearizations evaluate sensitivity to token order.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU (differences with respect to FINE-TUNE reported); statistical analysis of robustness</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>STRUCTADAPT showed best performance across linearizations; for RANDOM linearization, the gap between STRUCTADAPT and FINE-TUNE was largest (+5.9 BLEU); RECONF degraded all models but STRUCTADAPT remained strongest.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>STRUCTADAPT substantially reduced sensitivity to linearization permutations (more robust), whereas plain fine-tuning performance drops drastically on RANDOM linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>RECONF negatively impacts all models; linearization dependence remains a concern for sequential-only approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>STRUCTADAPT robustly outperforms ADAPT and FINE-TUNE across CANON/RECONF/RANDOM, demonstrating that explicit graph-structure encoding reduces reliance on a particular linearization order.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Structural neural encoders for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>GPT-too: A language-model-first approach for AMR-to-text generation <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>Promoting graph awareness in linearized graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Investigating pretrained language models for graph-to-text generation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7020",
    "paper_id": "paper-232240435",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "DFS linearization",
            "name_full": "Depth-first traversal AMR linearization (canonical)",
            "brief_description": "A sequential serialization of an AMR graph produced by a depth-first traversal of the canonical human-created AMR; nodes and edge labels are emitted as a token sequence that can be fed to pretrained language models.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Depth-first linearization (canonical)",
            "representation_description": "Encodes the AMR as a token sequence by performing a depth-first traversal of the canonical AMR and emitting nodes and edges in traversal order. Re-entrant node variables are replaced by their co-referring concept, producing a linear sequence of node and edge labels.",
            "representation_type": "sequential; token-based; lossy",
            "encoding_method": "depth-first traversal of canonical AMR",
            "canonicalization": true,
            "average_token_length": null,
            "dataset_name": "LDC2017T10, LDC2020T02",
            "task_name": "AMR-to-text generation (graph-to-text generation)",
            "model_name": "T5 (base, large) encoder-decoder PLM",
            "model_description": "Pretrained T5 encoder-decoder Transformer (T5-base and T5-large) used with adapters; encoder optionally frozen while adapter modules are trained.",
            "performance_metric": "BLEU, chrF++, MF-score (meaning component), BERTScore",
            "performance_value": "Baseline FINE-TUNE using DFS linearization: T5-base BLEU 38.3 (LDC2017T10), T5-large BLEU 41.2 (LDC2017T10); other metrics reported (chrF++, M, BERT) in paper tables.",
            "impact_on_training": "Allows direct use of PLMs by turning graphs into text sequences; however, models fine-tuned on this representation can suffer from catastrophic forgetting of language-pretraining and must infer graph connectivity from sequence tokens.",
            "limitations": "Dilutes explicit graph connectivity (structural information is weakened), may cause catastrophic forgetting when fine-tuning PLMs on non-linguistic sequences, and requires the model to recover edge connections from sequence context.",
            "comparison_with_other": "Outperforms naive sequential encoders trained from scratch only when combined with structure-aware modules; STRUCTADAPT leveraging DFS linearization improves performance substantially versus plain fine-tuning on the same linearization.",
            "uuid": "e7020.0"
        },
        {
            "name_short": "PENMAN notation",
            "name_full": "PENMAN AMR notation",
            "brief_description": "A human-readable parenthesized representation of AMR (PENMAN) that preserves tree-like structure and uses parentheses and variable names to encode graph structure; used in prior graph-to-text work as a textual input.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "PENMAN notation",
            "representation_description": "Encodes AMR in a bracketed, PENMAN format where relations and reentrancies are shown via parentheses and variable references; yields longer token sequences compared to node/edge linearizations.",
            "representation_type": "hierarchical/sequential; token-based; more explicit (less lossy than naive linearization)",
            "encoding_method": "PENMAN formatted serialization (parentheses and variable tokens)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "Discussed in relation to AMR corpora (prior work) — not used in main experiments",
            "task_name": "AMR-to-text generation (mentioned as alternative representation)",
            "model_name": "Mentioned in context of GPT-2 / PLMs in prior work",
            "model_description": "Prior works applied large language models to PENMAN inputs (making input sequences longer); authors note this is orthogonal to STRUCTADAPT and could be incorporated.",
            "performance_metric": "Not directly reported in this paper for PENMAN (prior works reported strong results)",
            "performance_value": null,
            "impact_on_training": "PENMAN increases input length (more tokens) which can change pretraining-transfer dynamics; prior work achieved strong results but this paper did not directly evaluate PENMAN.",
            "limitations": "Longer inputs (higher token cost); orthogonal to the adapters approach and not used in core experiments here.",
            "comparison_with_other": "Authors note prior works using PENMAN obtain strong results and that PENMAN is orthogonal to STRUCTADAPT; STRUCTADAPT could also be applied on top of PENMAN encodings.",
            "uuid": "e7020.1"
        },
        {
            "name_short": "Bipartite + token graph",
            "name_full": "Bipartite unlabeled graph conversion and token-level graph mapping",
            "brief_description": "Two-step conversion: (1) convert labeled AMR graph into an unlabeled bipartite graph by turning each labeled edge (u, r, v) into two unlabeled edges (u,r) and (r,v); (2) expand nodes into subword tokens and build a token-level graph connecting tokens of source and target nodes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Bipartite unlabeled graph -&gt; Token graph (subword nodes connected)",
            "representation_description": "First replace each labeled edge with an intermediate node for the relation (bipartite conversion) to create an unlabeled graph; then split node labels into subword tokens and create a token graph by connecting every token of a source node to every token of a target node (or according to rep variants).",
            "representation_type": "graph-preserving (lossless with respect to the expanded token graph), token-based, structured",
            "encoding_method": "Bipartite conversion of edges to relation-nodes, then tokenization into subword nodes and inter-token edge creation (connect token sets for adjacent graph nodes)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2017T10, LDC2020T02",
            "task_name": "AMR-to-text generation",
            "model_name": "T5 (with STRUCTADAPT graph-convolutional adapters)",
            "model_description": "T5 encoder-decoder with frozen pretrained parameters and STRUCTADAPT adapters that run graph convolutions over the token graph at each encoder layer.",
            "performance_metric": "BLEU, chrF++, MF-score (meaning), BERTScore",
            "performance_value": "Used as the input representation for STRUCTADAPT experiments; best STRUCTADAPT variants (RGCN/GCN) using token-graph representations achieved state-of-the-art BLEU (e.g., T5-large STRUCTADAPT-RGCN BLEU 46.6 on LDC2017T10 and 48.0 on LDC2020T02 in reported tables).",
            "impact_on_training": "Enables explicit propagation of graph connectivity into PLM representations via STRUCTADAPT's graph convolution, improving accuracy while training few task-specific parameters (e.g., 5.1% of PLM parameters).",
            "limitations": "Requires mapping and additional graph-structured adapter computations; expanding nodes into tokens increases graph size; average token counts not reported.",
            "comparison_with_other": "Compared favorably to sequential-only linearizations and to fully-connected (complete) token graphs — authors found explicit token graph connectivity (especially rep1 variant) outperforms a complete graph and improves robustness to linearization permutations.",
            "uuid": "e7020.2"
        },
        {
            "name_short": "rep1/rep2/rep3",
            "name_full": "Subword token connection strategies (rep1, rep2, rep3)",
            "brief_description": "Three alternative designs for how tokens of multi-token nodes are connected across edges: rep1 connects every token of source to every token of target; rep2 connects last token of source to first token of target and within-node tokens sequentially; rep3 connects first token to first token and within-node tokens sequentially.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Subword token representations: rep1, rep2, rep3",
            "representation_description": "All representations operate on tokenized node labels; rep1: full bipartite token-token connections between source and target node tokens; rep2: connect last token of source to first token of target and chain tokens within a node sequentially; rep3: connect first token of source to first token of target and chain tokens within a node sequentially.",
            "representation_type": "token-based; structured graph; variants trade off connectivity vs. sparsity",
            "encoding_method": "Token-level edge construction according to rep1/rep2/rep3 rules applied after tokenizing each node label",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2017T10 (dev analyses) and LDC2020T02",
            "task_name": "AMR-to-text generation",
            "model_name": "T5 with STRUCTADAPT (GCN/RGCN)",
            "model_description": "Pretrained T5 plus STRUCTADAPT graph-convolution adapters that use the token graph formed by rep variants to aggregate neighborhood information at every encoder layer.",
            "performance_metric": "BLEU (primary), chrF++, MF-score, BERTScore",
            "performance_value": "Paper reports rep1 as the best configuration; combined with STRUCTADAPT this setting yielded the top-performing systems in the paper (e.g., STRUCTADAPT-RGCN on T5-large achieving BLEU up to 46.6 on LDC2017T10 and 48.0 on LDC2020T02 in reported tables). Exact numeric ablation per-rep variant not tabulated in main text but rep1 highlighted as best.",
            "impact_on_training": "Choosing rep1 increased direct token-to-token influence across node boundaries and produced better downstream generation when used with STRUCTADAPT, improving model robustness and accuracy.",
            "limitations": "rep1 increases connectivity (more edges), which may increase computational cost of graph convolution; rep2/rep3 try to reduce edges but were empirically inferior.",
            "comparison_with_other": "rep1 &gt; rep2, rep3 and &gt; complete graph in performance; rep1 gives more direct interactions and better token influence between neighboring node tokens per the authors' analysis.",
            "uuid": "e7020.3"
        },
        {
            "name_short": "Complete graph",
            "name_full": "Fully-connected token/sequence graph (complete graph)",
            "brief_description": "Representation that connects every token/node to every other token/node (mirrors self-attention fully-connected view) — evaluated as an extreme baseline to test the value of explicit original graph connectivity.",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Complete (fully-connected) graph representation",
            "representation_description": "All tokens (nodes and edge-nodes) are connected to all other tokens, yielding a fully-connected graph structure analogous to treating the sequence as a fully connected graph for graph convolutions.",
            "representation_type": "graph (complete), token-based; highly lossy w.r.t. original topology",
            "encoding_method": "Fully connect token-level nodes (all-pairs edges) irrespective of original AMR connectivity",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2017T10 (dev comparisons)",
            "task_name": "AMR-to-text generation (ablation/comparison)",
            "model_name": "T5 with STRUCTADAPT variants",
            "model_description": "T5 with adapter-based graph convolution applied over a complete token graph.",
            "performance_metric": "BLEU (dev comparisons reported in Table 6)",
            "performance_value": "Complete graph representation produced relatively inferior performance compared to explicit nodes+edges token graph variants (rep1), as stated in the paper (exact BLEU numbers in Table 6 of paper).",
            "impact_on_training": "Less effective structural signal to the graph-convolution adapters; demonstrates that explicit AMR connectivity matters.",
            "limitations": "Does not preserve original topology; inferior performance compared to representations that encode original graph connectivity; blurs structural inductive bias.",
            "comparison_with_other": "Complete graph &lt; explicit nodes+edges representations (e.g., rep1); shows that naively treating sequences as fully connected graphs is suboptimal for graph-to-text when graph structure matters.",
            "uuid": "e7020.4"
        },
        {
            "name_short": "Nodes-only (RGCN adapter)",
            "name_full": "Node-only linearization with relations handled by RGCN adapters",
            "brief_description": "A linearization variant where only nodes have explicit textual representations in the sequence and edge relation types are not emitted as tokens but instead are modeled by the adapter's relational GCN parameters.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Node-only representation (edges as adapter relations)",
            "representation_description": "Encode only AMR nodes in the input sequence while omitting explicit edge tokens; relation types and directions are captured by the STRUCTADAPT RGCN relation-aware adapter weights rather than by sequence tokens.",
            "representation_type": "sequential (nodes only) combined with relational graph encoding (adapter) — semi-lossy",
            "encoding_method": "Emit node tokens only; use RGCN (adapter) with relation types (default and reverse) to model edge information",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2017T10 (dev comparisons)",
            "task_name": "AMR-to-text generation",
            "model_name": "T5 + STRUCTADAPT-RGCN",
            "model_description": "T5 encoder with layer-wise STRUCTADAPT using relational GCN updates to capture edge directionality and relation-specific transforms; PLM parameters frozen, only adapter params trained.",
            "performance_metric": "BLEU (dev comparisons)",
            "performance_value": "Authors report that explicitly considering nodes and edges (nodes+edges) is beneficial, but nodes-only variant has the advantage of allowing the model to handle new edge relations during inference; exact BLEU delta for nodes-only vs nodes+edges is reported in Table 6 (nodes+edges better overall).",
            "impact_on_training": "Reduces sequence length (fewer tokens) and shifts edge modeling into adapter parameters; can generalize to unseen edge labels at inference but may lose the direct signal of explicit edge tokens.",
            "limitations": "While flexible to new edge relations, empirically slightly worse than explicit nodes+edges tokenization for generation quality in the experiments.",
            "comparison_with_other": "Nodes+edges (with rep1) outperformed nodes-only in generation quality; nodes-only advantageous in that edge types are represented in model parameters rather than tied to the vocabulary.",
            "uuid": "e7020.5"
        },
        {
            "name_short": "Linearization permutations",
            "name_full": "Graph linearization permutations: CANON, RECONF, RANDOM",
            "brief_description": "Three alternative linearization orders of the same AMR graph used to test robustness: CANON (canonical human order), RECONF (reconfigured canonical order ignoring much of canonical sequence), RANDOM (valid traversal starting at a random node).",
            "citation_title": "",
            "mention_or_use": "use",
            "representation_name": "Canonical / Reconfigured / Random linearizations (CANON, RECONF, RANDOM)",
            "representation_description": "Different orderings of the depth-first linearization: CANON uses canonical order from AMR corpora, RECONF shuffles canonical order except top node, RANDOM starts traversal from a random node and disregards canonical order while remaining a valid traversal.",
            "representation_type": "sequential (ordering variants); lossy in same ways as DFS linearization",
            "encoding_method": "Depth-first traversal with different node visit order schemes (canonical, reconfigured, random start)",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "LDC2017T10 (robustness experiments)",
            "task_name": "AMR-to-text generation; robustness to linearization",
            "model_name": "T5 (FINE-TUNE, ADAPT, STRUCTADAPT)",
            "model_description": "T5 encoder-decoder used in multiple training regimes (full fine-tuning, adapter-based fine-tuning, STRUCTADAPT with graph convolutions); comparisons across linearizations evaluate sensitivity to token order.",
            "performance_metric": "BLEU (differences with respect to FINE-TUNE reported); statistical analysis of robustness",
            "performance_value": "STRUCTADAPT showed best performance across linearizations; for RANDOM linearization, the gap between STRUCTADAPT and FINE-TUNE was largest (+5.9 BLEU); RECONF degraded all models but STRUCTADAPT remained strongest.",
            "impact_on_training": "STRUCTADAPT substantially reduced sensitivity to linearization permutations (more robust), whereas plain fine-tuning performance drops drastically on RANDOM linearization.",
            "limitations": "RECONF negatively impacts all models; linearization dependence remains a concern for sequential-only approaches.",
            "comparison_with_other": "STRUCTADAPT robustly outperforms ADAPT and FINE-TUNE across CANON/RECONF/RANDOM, demonstrating that explicit graph-structure encoding reduces reliance on a particular linearization order.",
            "uuid": "e7020.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Structural neural encoders for AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "structural_neural_encoders_for_amrtotext_generation"
        },
        {
            "paper_title": "GPT-too: A language-model-first approach for AMR-to-text generation",
            "rating": 2,
            "sanitized_title": "gpttoo_a_languagemodelfirst_approach_for_amrtotext_generation"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "Promoting graph awareness in linearized graph-to-text generation",
            "rating": 2,
            "sanitized_title": "promoting_graph_awareness_in_linearized_graphtotext_generation"
        },
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation",
            "rating": 2,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        }
    ],
    "cost": 0.01802575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Structural Adapters in Pretrained Language Models for AMR-to-Text Generation
Association for Computational LinguisticsCopyright Association for Computational LinguisticsNovember 7-11, 2021. 2021</p>
<p>Leonardo F R Ribeiro ribeiro@aiphes.tu-darmstadt.de 
Ubiquitous Knowledge Processing Lab
Technical University of Darmstadt ‡ School of Engineering
Westlake University</p>
<p>Yue Zhang 
Ubiquitous Knowledge Processing Lab
Technical University of Darmstadt ‡ School of Engineering
Westlake University</p>
<p>Iryna Gurevych 
Ubiquitous Knowledge Processing Lab
Technical University of Darmstadt ‡ School of Engineering
Westlake University</p>
<p>Structural Adapters in Pretrained Language Models for AMR-to-Text Generation</p>
<p>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing
the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNovember 7-11, 2021. 2021
Pretrained language models (PLM) have recently advanced graph-to-text generation, where the input graph is linearized into a sequence and fed into the PLM to obtain its representation. However, efficiently encoding the graph structure in PLMs is challenging because such models were pretrained on natural language, and modeling structured data may lead to catastrophic forgetting of distributional knowledge. In this paper, we propose STRUCTADAPT, an adapter method to encode graph structure into PLMs. Contrary to prior work, STRUCTADAPT effectively models interactions among the nodes based on the graph connectivity, only training graph structure-aware adapter parameters. In this way, we incorporate task-specific knowledge while maintaining the topological structure of the graph. We empirically show the benefits of explicitly encoding graph structure into PLMs using STRUCTADAPT, outperforming the state of the art on two AMR-to-text datasets, training only 5.1% of the PLM parameters. 1</p>
<p>Introduction</p>
<p>Data-to-text tasks aim to generate meaningful and coherent natural language text that faithfully conveys structured data. Some examples of structured information include tables (Parikh et al., 2020), Knowledge Graphs (KGs) (Gardent et al., 2017;Vougiouklis et al., 2018) and Abstract Meaning Representation (AMR) (Banarescu et al., 2013). In this work, we focus on AMR-to-text generation where the goal is to generate a fluent and grammatical sentence that is faithful to a given AMR graph (See Figure 1a). AMR is a semantic formalism that has received much research interest (Song et al., 2018;Guo et al., 2019;Ribeiro et al., 2019;Opitz et al., 2020Fu et al., 2021) and has been shown to benefit downstream tasks 1 Our code and checkpoints are available at https://github.com/UKPLab/StructAdapt.  such as text summarization (Liao et al., 2018) and machine translation (Song et al., 2019). Both statistical (Flanigan et al., 2016;Pourdamghani et al., 2016) and neural methods (Bai et al., 2020;Cai and Lam, 2020) have been investigated for AMRto-text generation, and dominant methods make use of Graph Neural Networks (GNNs) (Kipf and Welling, 2017) or Transformers (Vaswani et al., 2017) for representing the input graph.</p>
<p>Pretrained language models (PLMs) (Devlin et al., 2019;Radford et al., 2019; have been shown useful as a general text representation method, giving much improved results on a wide range of tasks . However, they cannot be directly leveraged to benefit AMR-to-text generation, and more generally graph-to-text generation, due to the structural nature of the input. One solution is to transform the structured input into a se-quence, which can be directly fed into PLMs (See Figure 1b). Recent studies (Mager et al., 2020;Harkous et al., 2020;Ribeiro et al., 2020a transform AMRs into sequences by top-down linearization (Konstas et al., 2017). It has been shown that such linearized graph representation can be used to fine-tune a PLM and improve graph-to-text generation performances (Kale, 2020).</p>
<p>The above methods, however, suffer from two salient limitations. First, linearized graph structures are different in nature from natural language. As a result, knowledge from large-scale pretraining intuitively cannot be fully transferred, and finetuning a sentence representation using linearized graphs can lead to catastrophic forgetting of such distributional knowledge (Goodfellow et al., 2014;Kirkpatrick et al., 2017). Second, a linearized representation weakens structural information in the original graphs by diluting the explicit connectivity information (i.e., which nodes are connected to each other), and PLMs must infer how edge connections are specified in the sequence. This fact was also observed by Song et al. (2018), Beck et al. (2018) and Ribeiro et al. (2019), who show that GNN encoders outperform sequential encoders for AMR-to-text generation without pretraining.</p>
<p>To mitigate the issues, we aim to explicitly encode the graph data into a PLM without contaminating its original distributional knowledge. To this end, we propose STRUCTADAPT, a novel structureaware adapter that effectively allows leveraging the input graph structure into PLMs (See Figure 1c). The main idea is to add layer-wise modules, which extract information from the pretrained layers and make use of it in a graph-structure encoding. As shown in Figure 2, STRUCTADAPT employs a graph convolution in order to learn representations built upon the graph connectivity over the PLM encoder. Because STRUCTADAPT is added to each encoder layer, deep integration of linguistic knowledge and graph knowledge can be achieved. During finetuning, only the adapter parameters are trained, whereas the PLM parameters remain unchanged, in contrast to previous methods based on the graph linearizations that fine-tune all model parameters.</p>
<p>Empirically we show that STRUCTADAPT significantly outperforms linearized fine-tuning baselines and naive sequential adapters (Houlsby et al., 2019). Moreover, STRUCTADAPT is more robust to different graph linearizations, better treats reentrancies (nodes with more than one entering edge) and long-range node dependencies. Our proposed models, based on STRUCTADAPT, surpass the current state of the art on LDC2017T10 and LDC2020T02 datasets by up to 3.1 BLEU points, training only 5.1% of the original PLM parameters.</p>
<p>Related Work</p>
<p>Fine-tuning for Graph-to-text Generation. While previous approaches (Song et al., 2018;Ribeiro et al., 2019;Cai and Lam, 2020;Schmitt et al., 2021;Zhang et al., 2020b) have shown that explicitly encoding the graph structure is beneficial, fine-tuning PLMs on linearized structured data has established a new level of performance in data-to-text generation (Nan et al., 2021;Kale, 2020;. Our work can be seen as integrating the advantage of both graph structure encoding and PLMs, using a novel adapter module. Mager et al. (2020) employ cycle consistency to improve the adequacy of generated texts from AMRs using GPT-2 (Radford et al., 2019), whereas Harkous et al. (2020) train a classifier to rank candidate generations based on the semantic fidelity. Ribeiro et al. (2020a) investigate encoder-decoder PLMs for graph-to-text generation, and show that task-specific pretraining can lead to notable improvements and that PLMs benefit much more from the graph structure of AMRs than of KGs. Hoyle et al. (2021) explore the extent to which PLMs are invariant to graph linearization, finding that models trained on canonical linearizations fail to generalize to meaning-preserving alternatives. Compared to this line of work, which tunes all PLM parameters, our method obtains a further 19x reduction in task-specific parameters, tuning only 5.1% of the parameters while achieving state-of-the-art performance, being more robust to permutations of the graph representation and better encoding larger graphs.</p>
<p>Lightweight Fine-tuning. Recently, different approaches have emerged as an alternative training strategy in order to avoid fine-tuning all parameters of a PLM.  train a lightweight "side" network that is fused with the pretrained model via summation. Li and Liang (2021) propose to prepend a trainable continuous prefix as an alternative to adapters, maintaining comparable performance in data-to-text tasks using fewer trained parameters. Liu et al. (2021) develop a method to automatically search prompts in the continuous space and evaluate it in few-shot NLU tasks. Ham-bardzumyan et al. (2021) propose adversarial reprogramming attempts to learn task-specific word embeddings to customize the language model for the downstream task.</p>
<p>Adapter-based approaches (Houlsby et al., 2019;Rebuffi et al., 2017;Lauscher et al., 2020;Pfeiffer et al., 2020a) introduce a small number of task specific parameters, keeping the underlying pretrained model fixed. Pfeiffer et al. (2020b) propose an adapter method to arbitrary tasks and languages by learning modular language and task representations. The above works are related to STRUCTADAPT as it trains much fewer parameters, but also different because they do not explicitly encode the input structure, whereas STRUCTADAPT directly aims to encode it.</p>
<p>Graph-to-Text Model</p>
<p>Let G 0 = (V 0 , E 0 , R 0 ) denote a rooted and directed AMR graph with a node set V 0 and labeled edges (u, r, v) ∈ E 0 , where u, v ∈ V 0 and r ∈ R 0 is a relation type. An example of an AMR graph and its corresponding sentence is shown in Figure 1a.</p>
<p>Encoder-Decoder Architecture</p>
<p>Consider a conditional generation task where the input is a context x and the output y = y 1 , . . . , y |y| is a sequence of tokens. In AMR-to-text generation, the context x is the AMR graph and y is the sentence that describes the AMR graph in natural language.</p>
<p>Let p φ (y | x) denote a PLM parametrized by φ, where x is encoded by a bidirectional encoder, and the decoder predicts y autoregressively, conditioned on the encoded x and its left context. We focus on PLMs based on the Transformer encoderdecoder architecture (Vaswani et al., 2017), as they are suitable for conditional text generation. We define x = LIN(G 0 ), where LIN is a function that linearizes G 0 into a sequence of tokens. 2 Following Damonte and Cohen (2019), as shown in Figure 1b, we linearize the AMR into a sequence of nodes and edges using the depth-first traversal of the canonical human-created AMR. 3 In a nutshell, the hidden representation h l i ∈ R d , for all x i ∈ x, is computed by the encoder layer l, where d is the hidden dimension. The decoder hidden representationĥ l i ∈ R d is computed by the layer l of the 2 The variable of a re-entrant node -node with more than one incoming edge -is replaced with its co-referring concept. 3 Other AMR linearizations are discussed in §6.1. autoregressive decoder at time step i.</p>
<p>Fine-tuning</p>
<p>The model is initialized with pretrained parameters φ (e.g. using T5, Raffel et al., 2019) and fine-tuned to optimize the following log-likelihood objective over each gold instance (x, y):
max φ log p φ (y | x) = |y| i=1 log p φ (y i | y 1:i−1 , x).
(1)</p>
<p>Baseline Adapter</p>
<p>We employ an adapter module after the feedforward sub-layer of each layer on both encoder ( Figure 2a) and decoder (Figure 2b) of the PLM. We modify the adapter architecture from Houlsby et al. (2019), computing the adapter representation at each layer l, given the encoder layer representation h l i (orĥ l i in the decoder), as follows:
z i = W l o (σ(W l p LN(h l i ))) + h l i ,(2)
where σ is the activation function and LN(·) denotes layer normalization. W l o ∈ R d×m and W l p ∈ R m×d are adapter parameters, and m is the hidden dimension of the adapter. Figure 2c illustrates the baseline adapter module, which we call ADAPT. Training. Let the set of adapters' parameters for the encoder and decoder layers be parametrized by θ. The training objective is the same as Equation (1), but the set of trainable parameters changes: the PLM parameters φ are frozen and the adapter parameters θ are the only trainable parameters. In contrast to fine-tuning, adapters substantially reduce the number of trainable parameters that are used to adapt the PLM to the downstream task.</p>
<p>Limitation</p>
<p>Intuitively, the connection between nodes in the input graph can influence the encoding of x by guiding what to extract from x in order to generate y. Note that in both fine-tuning and ADAPT approaches, the self-attention mechanisms of the encoder layers treat the sequence of nodes and edges x essentially as a fully connected graph, greatly diluting the original graph structure. In this way, the model has to retrieve the original connectivity of the graph from x. For example, the AMR linearization in Figure 1b has two mentions of the node she, and the model should capture that both mentions belong to the same node in the original graph.</p>
<p>Structural Adapter</p>
<p>We propose STRUCTADAPT, a lightweight alternative to injecting structural inductive bias 4 into PLMs.</p>
<p>We first describe the intuition in §4.1 and define our method formally in §4.3.</p>
<p>Intuition</p>
<p>Injecting graph structural bias into graph-to-text models trained from scratch improves the performance compared to linearized approaches (Damonte and Cohen, 2019; Ribeiro et al., 2019). However, it is not straightforward how to effectively model the input graph structure when fine-tuning PLMs, which usually are pretrained using natural language and not structured data.</p>
<p>Our key idea is modeling the graph connectivity in the encoder utilizing an adapter module, using information flows between adjacent nodes in a message-passing update, employing a graph convolution (see Figure 2d). In this way, the graph structure substantially impacts the node representations, better encoding the input graph without impacting the knowledge learned during pretraining. This can lead to more efficient and better AMR-to-text generation as we will show in §5 and §6. Moreover, different adapters for distinct graph domains can be used with the same PLM, yielding a high degree of parameter sharing for graph-to-text tasks.</p>
<p>Graph Representation</p>
<p>We convert each G 0 into a bipartite graph G 1 = (V 1 , E 1 ), replacing each labeled edge (u, r, v) ∈ E 0 with two unlabeled edges e 1 = (u, r) and e 2 = (r, v). Similar to Beck et al. (2018), this process converts the graph into its unlabeled version. Figure 3 shows an (a) AMR subgraph and (b) its unlabeled representation.</p>
<p>Note that PLMs typically use a vocabulary with subword units (Sennrich et al., 2016). This presents a challenge in how to represent such a graph using subword tokens. Inspired by Ribeiro et al. (2020b), we transform each G 1 into a new token graph G = (V, E), where each token of a node in V 1 becomes a node v ∈ V. We convert each edge (u 1 , v 1 ) ∈ E 1 into a set of edges and connect every token of u 1 to every token of v 1 . That is, an edge (u, v) will belong to E if and only if there exists an edge (u 1 , v 1 ) ∈ E 1 such that u ∈ u 1 and v ∈ v 1 , where u 1 and v 1 are seen as sets of tokens. Figure 3c shows an example of the token graph.</p>
<p>Method</p>
<p>STRUCTADAPT employs a two-layer architecture in order to re-purpose the PLM for the graph-to-text task using a small number of new parameters. Formally, for each node v ∈ V, given the hidden representation h l v from the encoder layer l, STRUCTADAPT computes:
g l v = GraphConv l (LN(h l v ),{LN(h l u ) : u ∈ N (v)}) z l v = W l e σ(g l v ) + h l v ,(3)
where N (v) is the immediate neighborhood of v in G. GraphConv l (·) is the graph convolution that computes the node representation based on the local neighborhood of v, and W l e ∈ R d×m is a parameter. Figure 2d illustrates STRUCTADAPT. 5 Graph Convolution. The graph convolutional layer allows exploration of distinct strategies for neighborhood aggregation in order to model structural information of the input graph. Different GNN architectures (Velickovic et al., 2018;Xu et al., 2019) can be employed as the graph convolution. Moreover, in this way, we avoid changing the self-attention mechanism of the current pretrained encoder, allowing to also capture global information based on the pretrained knowledge.</p>
<p>Our graph convolution is based on the Graph Convolutional Network (GCN) proposed by Kipf and Welling (2017). At each layer l, we compute the representation of a node v ∈ V as follows:
g l v = u∈N (v) 1 √ d v d u W l g h l u ,(4)
where N (v) is a set of nodes with incoming edges to v and v itself, d v is the degree of v, and W l g ∈ R m×d is a parameter.</p>
<p>We also consider the variant relational GCN (RGCN) (Schlichtkrull et al., 2018) as graph convolution. RGCN allows capturing the reverse edge direction so that we can consider the differences in the incoming and outgoing relations, which has shown to be beneficial (Beck et al., 2018). In particular, the node representation is computed as:
g l v = r∈R u∈Nr(v) 1 |N r (v)| W l r h l u ,(5)
where R denotes the set of relations, i.e., the edge types default and reverse, N r (v) denotes the set of neighbors under relation r ∈ R, and W l r ∈ R m×d encodes the edge type between the nodes u and v.</p>
<p>Note that STRUCTADAPT computes the refined structural node representation z l v based on the local node context, using as input the global representation h l v generated by the current PLM encoder layer. In this way, the model is able to capture both the global context based on the PLM linguistic knowledge and the local context based on the graph knowledge. Finally, we employ ADAPT into the decoder in order to adapt the language model to the graph-to-text task. </p>
<p>Experiments</p>
<p>Our models are initialized with pre-trained T5 (Raffel et al., 2019), but our approach can be combined with other PLMs such as BART . Our implementation is based on Hugging Face Transformer models (Wolf et al., 2019). We use T5 base for all experiments and report results with T5 large for the test sets. 6 We use the Adam optimizer (Kingma and Ba, 2015) and employ a linearly decreasing learning rate schedule without warm-up. BLEU is used for the stopping criterion. Following recent work (Mager et al., 2020;Zhang et al., 2020b), we evaluate our proposed models on LDC2017T10 and LDC2020T02 corpora.</p>
<p>Evaluation. We evaluate the results with BLEU (Papineni et al., 2002) and chrF++ (Popović, 2015) metrics. We also report the meaning (M) component of the MF-score , which measures how well the source AMR graph can be reconstructed from the generated sentence. We use BERTScore (Zhang et al., 2020a) allowing a semantic evaluation that depends less on the surface forms. Finally, we also perform a human evaluation ( §5.2).</p>
<p>Main Results</p>
<p>We compare STRUCTADAPT with four methods: finetuning (FINE-TUNE), fine-tuning only the top or bottom 2 layers (FT-TOP2, FT-BOTTOM2) and ADAPT  models use the same graph linearization generated by the depth-first traversal. We also report recent state-of-the-art results on both datasets. Tables 1 and 2 show the results.</p>
<p>We find that training only 5.1% task-specific parameters, STRUCTADAPT-RGCN achieves a BLEU score of 46.6 in LDC2017T10, substantially improving over FINE-TUNE and other lightweight baselines (ADAPT, FT-TOP2, FT-BOTTOM2), and outperforming Ribeiro et al. (2020a) and Hoyle et al. (2021) which fine-tune T5 updating significantly more parameters. STRUCTADAPT also achieves stateof-the-art performance on LDC2020T02, considerably improving over Bevilacqua et al. (2021), which implicitly models the graph structure information using linearization techniques.</p>
<p>In general, STRUCTADAPT is better than ADAPT when training the same number of parameters, and slightly better even when training only 1.7% of the parameters for both datasets. This highlights that the gains not only come from using an adapter architecture, but from considering the graph connectivity. STRUCTADAPT-RGCN is more effective than STRUCTADAPT-GCN using fewer parameters, demonstrating that considering reverse relations is advantageous. ADAPT is consistently better than FINE-TUNE, agreeing with our intuition of catastrophic forgetting when fine-tuning. Interestingly, in contrast to popular strategies that focus on upper layers in fine-tuning (Howard and Ruder, 2018;Houlsby et al., 2019;Li and Liang, 2021), FT-BOTTOM2's performance is better than FT-TOP2's, suggesting that lower layers have a significant impact in adapting the PLM to structured data. Different from our work, both Mager et al. (2020) and Ribeiro et al. (2020a) use the PENMAN notation which makes the input much longer (containing more tokens), and demonstrate that this representation is able to achieve strong results -this is orthogonal to our STRUCTADAPT representation and  can be incorporated in future work. Overall, the results indicate that explicitly considering the graph structure using an adapter mechanism is effective for AMR-to-text generation, significantly reducing the number of trained parameters while improving generation quality.</p>
<p>Human Evaluation</p>
<p>To further assess the quality of the generated texts by the adapter-based models in LDC2020T02, we conduct a human evaluation via crowdsourcing using Amazon Mechanical Turk. We follow previous work (Ribeiro et al., 2019;Castro Ferreira et al., 2019) and evaluate the meaning similarity, i.e., how close in meaning is the generated text to the reference sentence. 7 We divide the datapoints into 3 different sets by by the graph size, i.e., the number of nodes, after converting edges into nodes (cf.</p>
<p>§4.2). This setting allows us to evaluate the performance of the models based on the complexity of the AMR graph. We randomly select 100 generated texts for each set and each model (total of 600), which annotators then rate on a 1-7 Likert scale. For each text we collect scores from 3 annotators and use MACE (Hovy et al., 2013), a Bayesian model that incorporates the reliability of individual workers, to merge sentence-level labels. 8 Table 3 shows that STRUCTADAPT improves the meaning similarity over ADAPT with statistically significant margins (p&lt;0.05). Note that the gains mainly come from datapoints with &gt;60 nodes, indicating that STRUC-TADAPT is better when encoding larger graphs.</p>
<p>Detailed Discussion</p>
<p>Parameter/Performance Trade-off. We investigate how the number of parameters affects the models. A higher hidden dimensionality means more trainable parameters, and smaller adapters introduce fewer parameters at a possible cost to performance. That is, the adapter size controls the parameter efficiency. Figure 4a shows the effect of the number of trained parameters in the performance measured using BLEU. Each point in the ADAPT and STRUCTADAPT curves represents a hidden dimension in the range [8, 16, . . . , 2048]. STRUCTADAPT-GCN is consistently better than ADAPT over all model capacities, even though both approaches train the same number of parameters. STRUCTADAPT-RGCN achieves similar performance than FINE-TUNE when training only 0.8% of the parameters whereas ADAPT achieves similar performance to 8.5%, demonstrating the effectiveness of injecting the graph structure into the PLM.</p>
<p>Low-data Setting. Previous work (Li and Liang, 2021) has shown that lightweight fine-tuning has an advantage in some generation tasks when the training size is smaller. Therefore, we investigate how STRUCTADAPT behaves in a low-data setting. We subsample the LDC2017T10 training set to analyze different smaller training sets. For each size, we sample 5 different datasets and average over 2 training random seeds. Thus, we average over 10 models to get an estimate for each low-data setting. 9 Figure 4b shows the results. First note that both adapter-based approaches improve over FINE-TUNE. When training with only 1000 datapoints, STRUCTADAPT outperforms FINE-TUNE by 8.2 BLEU points. Also note that the gap between ADAPT and FINE-TUNE decreases when the size of the training set increases. In general, STRUCTADAPT outperforms FINE-TUNE and ADAPT in low-resource scenarios by 7.3 and 4.8 BLEU points on average, respectively, whereas requiring much fewer trained parameters 9 We use the LDC2017T10 dev set to choose hyperparameters and do early stopping.</p>
<p>(b / break-up-08 :ARG1 (i / i) :ARG3 (p / person :ARG0-of (h / have-rel-role-91 :ARG1 (p2 / person :ARG0-of (h2 / have-rel-role-91 :ARG1 i :ARG2 (s3 / son))) :ARG2 (f / father))) :time (s2 / since :op1 (d / date-entity :month 8))) REFERENCE  than FINE-TUNE and fewer number of parameters than ADAPT.</p>
<p>Case Study. We perform a case study to provide a better understanding of the STRUCTADAPT's performance. Table 4 shows an AMR graph in PENMAN notation containing reentrancies (marked in bold) and sentences generated by FINE-TUNE and STRUCTADAPT trained on the LDC2017T10 full training set and in a low-data setting where the models are trained with 2000 data points. FINE-TUNE fails in generating a sentence with the correct concept break-up whereas STRUCTADAPT correctly generates a sentence that describes the input graph. The incorrect verb tense is due to lack of tense information in AMR. FINE-TUNE-2000 mixes the semantic relation between I and son (i.e., mistranslation of the edges in the graph) whereas STRUCTADAPT-2000 generates a correct sentence (except by generating the number 8). Overall, STRUCTADAPT produces a more accurate text output than FINE-TUNE by generating correct pronouns and mentions when control verbs and reentrancies are involved, in both full and lowdata scenarios.</p>
<p>Model Variations. In Table 5, we report an ablation study on the impact of distinct adapter components, using adapters only in the encoder or decoder. We evaluate different architecture configurations keeping the same number of parameters for a fair comparison. We find that only training adapters in  Table 5: Impact of the adapter modules in the encoder or decoder in the LDC2017T10 dev set. All adapterbased models have the same number of parameters.</p>
<p>the decoder is not sufficient for a good performance, even having the same number of parameters. This suggests that adapting the PLM encoder to handle graph structures is key in AMR-to-text tasks. Interestingly, the model that only employs STRUCTADAPT in the encoder (i.e., no ADAPT is used in the decoder) has a better performance (+1.7 BLEU) than using ADAPT in both encoder and decoder, highlighting STRUCTADAPT's strong graph encoding abilities. Finally, the best performance is achieved when we employ STRUCTADAPT in the encoder and ADAPT in the decoder, reaching 41.7 BLEU points.</p>
<p>Graph Representation Evaluation</p>
<p>In this section, we explore how different graph properties impact the models' abilities to encode the input graph structure.</p>
<p>Impact of the Graph Representation</p>
<p>Inspired by Damonte and Cohen (2019), we investigate two different approaches when linearizing the AMR: (i) only nodes have explicit representations, whereas edge relations are represented by the adapter parameters using the RGCN; 10 and (ii) the sequence of nodes and edges using depth-first traversal of the graph. We also propose and evaluate three different graph structures based on subwords (cf. §4.2): rep1: for each edge, we connect every token from the source node to every token of the target node; rep2: we connect the last token of the source node to the first token of the target node and connect the tokens of a node sequentially; rep3: we connect the first token of the source node to the first token of the target node and connect the token of a node sequentially. Figure 3 shows an example of the three representations for an AMR graph structure.  Additionally, we also investigate a fully connected graph structure (complete graph), that is, similarly to the self-attention mechanism in Transformers, all nodes and edges are connected. As shown in Table 6, explicitly considering nodes and edges in the graph linearization is beneficial. This approach has the advantage of allowing the model to handle new edge relations during inference, as they are not encoded as model parameters. Note that the complete graph representation has relatively inferior performance, again demonstrating the advantage of explicitly encoding the input graph connectivity.</p>
<p>Finally, we observe that the best configuration is using nodes and edges with rep1 (see an example in Figure 3c). We believe that this is because rep1 allows direct interactions between all source and target tokens, making all token representations of an AMR node directly influenced by the neighbouring tokens.</p>
<p>Robustness to Graph Linearization</p>
<p>A critical advantage of modeling the graph structure is to be less dependent on linearization strategies because the graph connectivity is invariant to the graph linearization. We thus are interested in measuring the impact of the graph linearization in the models.</p>
<p>Following Hoyle et al. (2021), we investigate three different graph linearizations: (i) CANON: the original order of the canonical human-created linearizations in AMR corpora; (ii) RECONF: the order from the canonical graph linearization is ignored, except for the top node; 11 and (iii) RANDOM: constructs a linearization from a random node in the graph, disregarding all order information from the canonical format, but it remains a valid traversal of the graph. All linearizations are converted to a  sequence of node and edge labels using depth-first traversal and used for both training and evaluation. Examples of such graph linearizations are shown in Appendix C. Table 7 presents the results. Note that while RECONF has a negative impact on all models, STRUC-TADAPT has the best performance. ADAPT has similar performance gains over FINE-TUNE in all graph linearizations. Finally, note that for RANDOM, there is a drastic performance drop in FINE-TUNE and the gap between STRUCTADAPT and FINE-TUNE is widest (+5.9 BLEU), demonstrating that explicitly encoding the graph structure is beneficial and that STRUC-TADAPT is much less impacted by different graph linearizations. Table 8 shows the effects of the graph size, graph diameter and reentrancies in the performance. First, note that the BLEU scores decrease as the graph size increases since larger graphs often are more complex. The performance gap between STRUC-TADAPT and FINE-TUNE becomes larger for relatively larger graphs, showing that STRUCTADAPT is able to better encode complex graphs. As ADAPT is not aware of the graph connectivity, it has much worse scores compared to STRUCTADAPT, especially for larger graphs.</p>
<p>Graph Properties</p>
<p>It is expected that the benefit of the STRUCTADAPT will be more evident for AMR graphs containing larger diameter as the encoder is aware of the input graph structure. As seen in Table 8, similarly to the graph size, the scores decrease as the graph diameter increases. STRUCTADAPT achieves a clear improvement when handling graphs with ≥20 diameter, with a improvement of +4.2 BLEU points over FINE-TUNE. Previous work (Damonte and Cohen, 2019;Szubert et al., 2020) showed that reentrancies (nodes with multiple parents) pose difficulties in encoding AMRs correctly. Because STRUCTADAPT is the only approach to model reentrancies explicitly, we expect it to deal better with these structures. The  gap between STRUCTADAPT and the other models is widest for examples with more reentrancies, confirming our hypothesis. In particular, when graphs contain ≥4 reentrancies, STRUCTADAPT has an improvement of +3.6 BLEU points compared to ADAPT.</p>
<p>Conclusion</p>
<p>We presented STRUCTADAPT, a novel adapter architecture to explicitly model graph structures into pretrained language models, providing an extensive evaluation of our approach and showing that it achieves state-of-the-art results on two AMR-totext benchmarks, training much fewer parameters. We also found that STRUCTADAPT is more effective when encoding complex graphs, when trained on fewer datapoints, and is more robust to different graph linearizations and reentrancies. In future work, we plan to consider other graph-to-text tasks, such as those based on Knowledge Graphs.</p>
<p>Appendices</p>
<p>In this supplementary material, we detail experiments' settings and additional information about the human evaluation and graph representations.</p>
<p>A Details of Models and Hyperparameters</p>
<p>The experiments were executed using the version 3.3.1 of the transformers library released by Hugging Face (Wolf et al., 2019). In Table 9, we report the hyperparameters used to train the models presented in this paper. We train until the development set BLEU has not improved for 5 epochs.  </p>
<p>B Details on the Human Evaluation</p>
<p>The human evaluation was conducted via Amazon Mechanical Turk. We randomly select 100 generated texts for each of the 3 sets and each adapter model (ADAPT, STRUCTADAPT-GCN), with a total of 600 texts to be evaluated. The annotators then rate the meaning similarity on a 1-7 Likert scale. For each text, we collect scores from 3 annotators. We use MACE (Hovy et al., 2013) to further improve upon these raw answers by unsupervised estimation of worker trustworthiness and subsequent recovery of the most likely score. Models are ranked according to the mean of sentence-level scores. We defined a filter for all our evaluations, allowing to participate only workers who have more than 5000 HITs approved and with an acceptance rate of 95% or higher. The task took workers a median time of 1.6 minutes per pair of sentences. We apply a quality control step filtering workers who do not score some faked and known sentences properly or did the experiment in a very short time.</p>
<p>C Example of Graph Linearizations</p>
<p>In Table 10, we present three different linearizations for the same AMR graph and its corresponding reference sentence. Figure 5 shows the two possible graphs that are represented by the linearizations. In particular, Figure 5a shows a graph that is represented by CANON and RECONF linearizations and Figure 5b shows a graph that is represented by RANDOM. Note that whereas the linearizations can greatly differ from each other, the graph structure for all linearizations remains very similar.  </p>
<p>tuning only with graph linearization More power to her to achieve.Lightweight fine-tuning with graph structurePretrained Model with StructAdaptMore power to her for her achievements.</p>
<p>Figure 1 :
1(a) AMR for the sentence More power to her for her achievements. While in (b) the pretrained model gets as input the graph linearization, in (c) it additionally receives the graph connectivity information.</p>
<p>Figure 2 :
2Integration of the adapter modules with the (a) encoder and (b) decoder layers of the Transformer; layer normalization and residual connections are omitted for clarification. (c) ADAPT with two feed-forwards layers. (d) STRUCTADAPT encodes the graph structure using a graph convolutional layer.</p>
<p>Figure 3 :
3An example of (a) an AMR graph structure, (b) its unlabeled version and three different subword representations: (c) rep1, (d) rep2 and (e) rep3.</p>
<dl>
<dt>Figure 4 :</dt>
<dt>4(a) Impact (measure with BLEU) of the number of parameters in the LDC2017T10 dev set. (b) Performance in the LDC2017T10 test set when experimenting with different amounts of training data.</dt>
<dd>
<p>Me and my son's father have been broken up since August. FINE-TUNE-2000: I've broken up with my son and father since August. FINE-TUNE: I've been with my son's father since August. STRUCTADAPT-2000: Since August 8 I have broken up with my son's father. STRUCTADAPT: I've been breaking up with my son's father since August.</p>
</dd>
</dl>
<p>Figure 5 :
5Two AMR graphs with the same meaning. Her utilities are all subsidized.</p>
<p>Table 1: Results on the LDC2017T10 test set. Mean (±s.d.) over 4 seeds.BLEU chrF++ 
M 
BERT </p>
<p>Mager et al. (2020) 
33.0 
63.9 
-
-
Zhang et al. (2020b) 
33.6 
63.2 
-
-
Harkous et al. (2020) 
37.7 
-
-
-
Hoyle et al. (2021) 
44.9 
-
76.54 
-
Ribeiro et al. (2020a) 
45.8 
72.5 
-
-</p>
<p>T5 base </p>
<p>FINE-TUNE </p>
<p>38.3±0.3 68.6±0.1 77.8±0.3 95.5±0.1 </p>
<p>FT-TOP2(14.8%) </p>
<p>29.9±0.1 63.0±0.1 74.1±0.2 94.4±0.2 </p>
<p>FT-BOTTOM2(14.8%) </p>
<p>35.9±0.3 67.0±0.2 76.9±0.1 95.3±0.1 </p>
<p>ADAPT(8.5%) </p>
<p>38.7±0.4 69.2±0.2 78.3±0.1 95.6±0.1 </p>
<p>STRUCTADAPT-GCN(2.1%) </p>
<p>39.0±0.3 69.1±0.2 78.4±0.2 95.7±0.2 </p>
<p>STRUCTADAPT-GCN(8.5%) </p>
<p>41.0±0.5 70.0±0.2 78.4±0.1 95.7±0.1 
STRUCTADAPT-RGCN(6.3%) 44.0±0.3 71.2±0.2 79.4±0.1 95.9±0.2 </p>
<p>T5 large </p>
<p>FINE-TUNE </p>
<p>41.2±0.5 70.2±0.2 78.0±0.1 95.8±0.2 </p>
<p>FT-TOP2(7.9%) </p>
<p>28.8±0.4 61.8±0.5 73.9±0.2 94.1±0.2 </p>
<p>FT-BOTTOM2(7.9%) </p>
<p>37.6±0.3 68.0±0.2 77.2±0.2 95.5±0.1 </p>
<p>ADAPT(6.8%) </p>
<p>42.9±0.3 71.6±0.2 78.9±0.1 96.1±0.1 </p>
<p>STRUCTADAPT-GCN(1.7%) </p>
<p>44.1±0.4 71.8±0.3 79.1±0.1 96.1±0.2 </p>
<p>STRUCTADAPT-GCN(6.8%) </p>
<p>45.8±0.2 72.5±0.1 79.3±0.2 96.2±0.1 
STRUCTADAPT-RGCN(5.1%) 46.6±0.3 72.9±0.2 79.6±0.1 96.3±0.1 </p>
<p>. AllBLEU chrF++ 
M 
BERT </p>
<p>Zhang et al. (2020b) 
34.3 
63.7 
-
-
Bevilacqua et al. (2021) 44.9 
72.9 
-
-</p>
<p>T5 large </p>
<p>FINE-TUNE </p>
<p>41.6±0.6 70.4±0.5 78.5±0.2 96.0±0.1 </p>
<p>FT-TOP2(7.9%) </p>
<p>33.4±0.5 63.5±0.3 73.4±0.4 94.3±0.1 </p>
<p>FT-BOTTOM2(7.9%) </p>
<p>38.2±0.2 68.3±0.1 78.1±0.2 95.6±0.1 </p>
<p>ADAPT(6.8%) </p>
<p>43.0±0.2 71.3±0.2 79.3±0.1 96.2±0.1 </p>
<p>STRUCTADAPT-GCN(1.7%) </p>
<p>46.2±0.2 71.8±0.2 79.4±0.3 96.0±0.2 </p>
<p>STRUCTADAPT-GCN(6.8%) </p>
<p>47.1±0.4 72.5±0.1 79.7±0.2 96.2±0.1 
STRUCTADAPT-RGCN(5.1%) 48.0±0.2 73.2±0.1 80.1±0.3 96.3±0.1 </p>
<p>Table 2 :
2Results on the LDC2020T02 test set.</p>
<p>Table 3 :
3Meaning similarity obtained in the human evaluation. The ranking was determined by Mann-Whitney tests with p&lt;0.05. Difference between systems which have a letter in common is not statistically significant.</p>
<p>Table 4 :
4An example of an AMR graph and generated 
sentences by different models trained on full data and 
on a low-data setting with 2000 datapoints. </p>
<p>Table 6 :
6Performance on the LDC2017T10 dev set when using different graph representation strategies.</p>
<p>Table 7 :
7Differences, with respect to FINE-TUNE, in the BLEU score of the LDC2017T10 test set as a function of different graph linearizations.</p>
<p>Table 8 :
8Differences, with respect to FINE-TUNE, in the BLEU score of the LDC2017T10 test set as a function of the graph size, graph diameter and number of reentrancies.</p>
<p>learning rate batch size beam search sizeFINE-TUNE </p>
<p>3e-05 
4 
5 </p>
<p>FT-TOP2 </p>
<p>1e-04 
4 
5 </p>
<p>FT-BOTTOM2 </p>
<p>1e-04 
4 
5 </p>
<p>ADAPT </p>
<p>1e-04 
4 
5 </p>
<p>STRUCTADAPT </p>
<p>1e-04 
4 
5 </p>
<p>Table 9 :
9Hyperparameter settings for our methods.</p>
<p>Table 10 :
10Different linearizations for an AMR graph.
The model architecture explicitly encodes the graph structure, i.e., which nodes are connected to each other.
Preliminary experiments with other architecture configurations led to worse or similar performance.
Hyperparameter details are in the appendix A.
We also assessed the fluency of the texts and the differences between the models were not statistically significant. 8 Refer to Appendix B for a detailed description of the human evaluation.
We use regularization based on the basis decomposition for relation weights(Schlichtkrull et al., 2018) since AMR can contain around 150 different edge types.
RECONF can significantly modify the linearization, including shifting edge labels (e.g., poss to poss-of).
AcknowledgmentsWe thank our anonymous reviewers for their thoughtful comments. We also would like to thank Jonas Pfeiffer, Jorge Cardona, Juri Opitz, Kevin Stowe, Thy Tran, Tilman Beck and Tim Baumgärtner for their feedback on this work. This work has been supported by the German Research Foundation (DFG) as part of the Research Training Group "Adaptive Preparation of Information form Heterogeneous Sources" (AIPHES, GRK 1994/1) and as part of the DFG funded project UKP-SQuARE with the number GU 798/29-1.
Online back-parsing for AMR-to-text generation. Xuefeng Bai, Linfeng Song, Yue Zhang, 10.18653/v1/2020.emnlp-main.92Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsXuefeng Bai, Linfeng Song, and Yue Zhang. 2020. On- line back-parsing for AMR-to-text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1206-1219, Online. Association for Computa- tional Linguistics.</p>
<p>Abstract Meaning Representation for sembanking. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, Nathan Schneider, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. the 7th Linguistic Annotation Workshop and Interoperability with DiscourseSofia, BulgariaAssociation for Computational LinguisticsLaura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proceedings of the 7th Linguis- tic Annotation Workshop and Interoperability with Discourse, pages 178-186, Sofia, Bulgaria. Associa- tion for Computational Linguistics.</p>
<p>Graph-to-sequence learning using gated graph neural networks. Daniel Beck, Gholamreza Haffari, Trevor Cohn, 10.18653/v1/P18-1026Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-sequence learning using gated graph neural networks. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pages 273-283, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>One spring to rule them both: Symmetric amr semantic parsing and generation without a complex pipeline. Michele Bevilacqua, Rexhina Blloshmi, Roberto Navigli, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Michele Bevilacqua, Rexhina Blloshmi, and Roberto Navigli. 2021. One spring to rule them both: Sym- metric amr semantic parsing and generation with- out a complex pipeline. Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12564- 12573.</p>
<p>Graph transformer for graph-to-sequence learning. Deng Cai, Wai Lam, 10.1609/aaai.v34i05.6243Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Deng Cai and Wai Lam. 2020. Graph transformer for graph-to-sequence learning. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7464-7471.</p>
<p>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Chris Thiago Castro Ferreira, Van Der Lee, Emiel Emiel Van Miltenburg, Krahmer, 10.18653/v1/D19-1052Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsThiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. 2019. Neu- ral data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 552-562, Hong Kong, China. Association for Computational Lin- guistics.</p>
<p>Structural neural encoders for AMR-to-text generation. Marco Damonte, Shay B Cohen, 10.18653/v1/N19-1366Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short PapersAssociation for Computational LinguisticsMarco Damonte and Shay B. Cohen. 2019. Structural neural encoders for AMR-to-text generation. In Pro- ceedings of the 2019 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 1 (Long and Short Papers), pages 3649-3658, Minneapolis, Minnesota. Association for Computa- tional Linguistics.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.</p>
<p>Generation from Abstract Meaning Representation using tree transducers. Jeffrey Flanigan, Chris Dyer, Noah A Smith, Jaime Carbonell, 10.18653/v1/N16-1087Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsJeffrey Flanigan, Chris Dyer, Noah A. Smith, and Jaime Carbonell. 2016. Generation from Abstract Meaning Representation using tree transducers. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 731-739, San Diego, California. Association for Computational Linguistics.</p>
<p>End-to-end AMR corefencence resolution. Qiankun Fu, Linfeng Song, Wenyu Du, Yue Zhang, 10.18653/v1/2021.acl-long.324Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics1Qiankun Fu, Linfeng Song, Wenyu Du, and Yue Zhang. 2021. End-to-end AMR corefencence resolution. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers), pages 4204-4214, Online. Association for Computational Linguistics.</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, SpainAssociation for Computational LinguisticsClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Pro- ceedings of the 10th International Conference on Natural Language Generation, pages 124-133, San- tiago de Compostela, Spain. Association for Compu- tational Linguistics.</p>
<p>An empirical investigation of catastrophic forgeting in gradientbased neural networks. Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, Yoshua Bengio, Proceedings of International Conference on Learning Representations. International Conference on Learning RepresentationsICLRIan J. Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. 2014. An empirical investigation of catastrophic forgeting in gradient- based neural networks. In Proceedings of Inter- national Conference on Learning Representations (ICLR).</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Zhijiang Guo, Yan Zhang, Zhiyang Teng, Wei Lu, 10.1162/tacl_a_00269Transactions of the Association for Computational Linguistics. 7Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional networks for graph-to-sequence learning. Transac- tions of the Association for Computational Linguis- tics, 7:297-312.</p>
<p>WARP: Word-level Adversarial ReProgramming. Karen Hambardzumyan, Hrant Khachatrian, Jonathan , 10.18653/v1/2021.acl-long.381Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingLong Papers1Online. Association for Computational LinguisticsKaren Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021. WARP: Word-level Adversar- ial ReProgramming. In Proceedings of the 59th An- nual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer- ence on Natural Language Processing (Volume 1: Long Papers), pages 4921-4933, Online. Associa- tion for Computational Linguistics.</p>
<p>Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. Hamza Harkous, Isabel Groves, Amir Saffari, 10.18653/v1/2020.coling-main.218Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, SpainInternational Committee on Computational LinguisticsHamza Harkous, Isabel Groves, and Amir Saffari. 2020. Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. In Proceedings of the 28th International Conference on Computational Linguistics, pages 2410-2424, Barcelona, Spain (Online). International Committee on Computational Linguistics.</p>
<p>Parameter-efficient transfer learning for NLP. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, PMLRProceedings of the 36th International Conference on Machine Learning. the 36th International Conference on Machine Learning97Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 2790-2799. PMLR.</p>
<p>Learning whom to trust with MACE. Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, Eduard Hovy, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAtlanta, GeorgiaAssociation for Computational LinguisticsDirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with MACE. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1120-1130, Atlanta, Georgia. Association for Computational Linguistics.</p>
<p>Universal language model fine-tuning for text classification. Jeremy Howard, Sebastian Ruder, 10.18653/v1/P18-1031Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 328-339, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Promoting graph awareness in linearized graph-to-text generation. Alexander Miserlis Hoyle, Ana Marasović, Noah A Smith, 10.18653/v1/2021.findings-acl.82Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Alexander Miserlis Hoyle, Ana Marasović, and Noah A. Smith. 2021. Promoting graph awareness in linearized graph-to-text generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 944-956, Online. Asso- ciation for Computational Linguistics.</p>
<p>Text-to-text pre-training for data-totext tasks. Mihir Kale, arXiv e-printsMihir Kale. 2020. Text-to-text pre-training for data-to- text tasks. arXiv e-prints.</p>
<p>Adam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Semisupervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, 5th International Conference on Learning Representations. Toulon, FranceConference Track ProceedingsThomas N. Kipf and Max Welling. 2017. Semi- supervised classification with graph convolutional networks. In 5th International Conference on Learn- ing Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.</p>
<p>Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, 10.1073/pnas.1611835114Proceedings of the National Academy of Sciences. 11413James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Ag- nieszka Grabska-Barwinska, Demis Hassabis, Clau- dia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526.</p>
<p>Neural AMR: Sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, 10.18653/v1/P17-1014Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational LinguisticsLong Papers)Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: Sequence-to-sequence models for parsing and gener- ation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 146-157, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Common sense or world knowledge? investigating adapter-based knowledge injection into pretrained transformers. Anne Lauscher, Olga Majewska, Leonardo F R Ribeiro, Iryna Gurevych, Nikolai Rozanov, Goran Glavaš, 10.18653/v1/2020.deelio-1.5Proceedings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Integration for Deep Learning ArchitecturesOnline. Association for Computational LinguisticsAnne Lauscher, Olga Majewska, Leonardo F. R. Ribeiro, Iryna Gurevych, Nikolai Rozanov, and Goran Glavaš. 2020. Common sense or world knowledge? investigating adapter-based knowledge injection into pretrained transformers. In Proceed- ings of Deep Learning Inside Out (DeeLIO): The First Workshop on Knowledge Extraction and Inte- gration for Deep Learning Architectures, pages 43- 49, Online. Association for Computational Linguis- tics.</p>
<p>BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsMike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics.</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. Lisa Xiang, Percy Li, Liang, 10.18653/v1/2021.acl-long.353Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics1Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers), pages 4582-4597, Online. Association for Computational Linguistics.</p>
<p>Abstract Meaning Representation for multi-document summarization. Kexin Liao, Logan Lebanoff, Fei Liu, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsSanta Fe, New Mexico, USAAssociation for Computational LinguisticsKexin Liao, Logan Lebanoff, and Fei Liu. 2018. Ab- stract Meaning Representation for multi-document summarization. In Proceedings of the 27th Inter- national Conference on Computational Linguistics, pages 1178-1190, Santa Fe, New Mexico, USA. As- sociation for Computational Linguistics.</p>
<p>. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT understands, too. CoRR, abs/2103.10385Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT understands, too. CoRR, abs/2103.10385.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, Roberta: A robustly optimized bert pretraining approach. arXiv e-printsYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Roberta: A robustly optimized bert pretraining ap- proach. arXiv e-prints.</p>
<p>GPT-too: A language-model-first approach for AMR-to-text generation. Manuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Arafat Md, Young-Suk Sultan, Radu Lee, Salim Florian, Roukos, 10.18653/v1/2020.acl-main.167Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsManuel Mager, Ramón Fernandez Astudillo, Tahira Naseem, Md Arafat Sultan, Young-Suk Lee, Radu Florian, and Salim Roukos. 2020. GPT-too: A language-model-first approach for AMR-to-text gen- eration. In Proceedings of the 58th Annual Meet- ing of the Association for Computational Linguistics, pages 1846-1852, Online. Association for Computa- tional Linguistics.</p>
<p>DART: Open-domain structured data record to text generation. Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, Nazneen Fatema Rajani, 10.18653/v1/2021.naacl-main.37Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational LinguisticsLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xian- gru Tang, Aadit Vyas, Neha Verma, Pranav Kr- ishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mu- tuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. 2021. DART: Open-domain structured data record to text genera- tion. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Com- putational Linguistics: Human Language Technolo- gies, pages 432-447, Online. Association for Com- putational Linguistics.</p>
<p>Weisfeiler-leman in the bamboo: Novel amr graph metrics and a benchmark for amr graph similarity. Juri Opitz, Angel Daza, Anette Frank, Transactions of the Association for Computational Linguistics. Juri Opitz, Angel Daza, and Anette Frank. 2021. Weisfeiler-leman in the bamboo: Novel amr graph metrics and a benchmark for amr graph similarity. Transactions of the Association for Computational Linguistics.</p>
<p>Towards a decomposable metric for explainable evaluation of text generation from AMR. Juri Opitz, Anette Frank, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational LinguisticsJuri Opitz and Anette Frank. 2021. Towards a decom- posable metric for explainable evaluation of text gen- eration from AMR. In Proceedings of the 16th Con- ference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1504-1518, Online. Association for Computational Linguistics.</p>
<p>AMR similarity metrics from principles. Juri Opitz, Letitia Parcalabescu, Anette Frank, 10.1162/tacl_a_00329Transactions of the Association for Computational Linguistics. 8Juri Opitz, Letitia Parcalabescu, and Anette Frank. 2020. AMR similarity metrics from principles. Transactions of the Association for Computational Linguistics, 8:522-538.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>ToTTo: A controlled table-totext generation dataset. Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, Dipanjan Das, 10.18653/v1/2020.emnlp-main.89Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsOnlineAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A controlled table-to- text generation dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 1173-1186, On- line. Association for Computational Linguistics.</p>
<p>AdapterFusion: Non-Destructive Task Composition for Transfer Learning. Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL), Online. the 16th Conference of the European Chapter of the Association for Computational Linguistics (EACL), OnlineAssociation for Computational LinguisticsJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. 2021. AdapterFusion: Non-Destructive Task Composition for Transfer Learning. In Proceedings of the 16th Conference of the European Chapter of the Associa- tion for Computational Linguistics (EACL), Online. Association for Computational Linguistics.</p>
<p>AdapterHub: A framework for adapting transformers. Jonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, Iryna Gurevych, 10.18653/v1/2020.emnlp-demos.7Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aish- warya Kamath, Ivan Vulić, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych. 2020a. AdapterHub: A framework for adapting transform- ers. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing: System Demonstrations, pages 46-54, Online. Asso- ciation for Computational Linguistics.</p>
<p>MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, Sebastian Ruder, 10.18653/v1/2020.emnlp-main.617Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Se- bastian Ruder. 2020b. MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7654-7673, Online. Association for Computa- tional Linguistics.</p>
<p>chrF: character n-gram F-score for automatic MT evaluation. Maja Popović, 10.18653/v1/W15-3049Proceedings of the Tenth Workshop on Statistical Machine Translation. the Tenth Workshop on Statistical Machine TranslationLisbon, PortugalAssociation for Computational LinguisticsMaja Popović. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392-395, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Generating English from Abstract Meaning Representations. Nima Pourdamghani, Kevin Knight, Ulf Hermjakob, 10.18653/v1/W16-6603Proceedings of the 9th International Natural Language Generation conference. the 9th International Natural Language Generation conferenceEdinburgh, UKAssociation for Computational LinguisticsNima Pourdamghani, Kevin Knight, and Ulf Herm- jakob. 2016. Generating English from Abstract Meaning Representations. In Proceedings of the 9th International Natural Language Generation confer- ence, pages 21-25, Edinburgh, UK. Association for Computational Linguistics.</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Technical report. OpenAIAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. In Tech- nical report, OpenAI.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv e-printsColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text trans- former. arXiv e-prints.</p>
<p>Learning multiple visual domains with residual adapters. Hakan Sylvestre-Alvise Rebuffi, Andrea Bilen, Vedaldi, Advances in Neural Information Processing Systems. Curran Associates, Inc30Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. 2017. Learning multiple visual domains with residual adapters. In Advances in Neural Infor- mation Processing Systems, volume 30, pages 506- 516. Curran Associates, Inc.</p>
<p>Enhancing AMR-to-text generation with dual graph representations. F R Leonardo, Claire Ribeiro, Iryna Gardent, Gurevych, 10.18653/v1/D19-1314Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsLeonardo F. R. Ribeiro, Claire Gardent, and Iryna Gurevych. 2019. Enhancing AMR-to-text genera- tion with dual graph representations. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter- national Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 3183-3194, Hong Kong, China. Association for Computational Lin- guistics.</p>
<p>Smelting gold and silver for improved multilingual amr-to-text generation. F R Leonardo, Jonas Ribeiro, Yue Pfeiffer, Iryna Zhang, Gurevych, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingPunta Cana2021Leonardo F. R. Ribeiro, Jonas Pfeiffer, Yue Zhang, and Iryna Gurevych. 2021. Smelting gold and silver for improved multilingual amr-to-text generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Punta Cana, November 7-11, 2021.</p>
<p>Hinrich Schütze, and Iryna Gurevych. 2020a. Investigating pretrained language models for graph-to-text generation. F R Leonardo, Martin Ribeiro, Schmitt, arXiv e-printsLeonardo F. R. Ribeiro, Martin Schmitt, Hinrich Schütze, and Iryna Gurevych. 2020a. Investigating pretrained language models for graph-to-text gener- ation. arXiv e-prints.</p>
<p>Claire Gardent, and Iryna Gurevych. 2020b. Modeling global and local node contexts for text generation from knowledge graphs. F R Leonardo, Yue Ribeiro, Zhang, 10.1162/tacl_a_00332Transactions of the Association for Computational Linguistics. 8Leonardo F. R. Ribeiro, Yue Zhang, Claire Gardent, and Iryna Gurevych. 2020b. Modeling global and local node contexts for text generation from knowl- edge graphs. Transactions of the Association for Computational Linguistics, 8:589-604.</p>
<p>Modeling relational data with graph convolutional networks. Michael Sejr, Thomas N Schlichtkrull, Peter Kipf, Rianne Bloem, Van Den, Ivan Berg, Max Titov, Welling, https:/link.springer.com/chapter/10.1007/978-3-319-93417-4_38ESWC 2018. Heraklion, Crete, GreeceMichael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolutional networks. In ESWC 2018, Heraklion, Crete, Greece, June 3-7, 2018, Proceedings, pages 593-607.</p>
<p>Modeling graph structure via relative position for text generation from knowledge graphs. Martin Schmitt, Leonardo F R Ribeiro, Philipp Dufter, Iryna Gurevych, Hinrich Schütze, Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15). the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15)Mexico City, MexicoAssociation for Computational LinguisticsMartin Schmitt, Leonardo F. R. Ribeiro, Philipp Dufter, Iryna Gurevych, and Hinrich Schütze. 2021. Mod- eling graph structure via relative position for text generation from knowledge graphs. In Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 10-21, Mexico City, Mexico. Association for Computational Linguistics.</p>
<p>Neural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, 10.18653/v1/P16-1162Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyLong Papers1Association for Computational LinguisticsRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715- 1725, Berlin, Germany. Association for Computa- tional Linguistics.</p>
<p>Semantic neural machine translation using amr. Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo Wang, Jinsong Su, 10.1162/tacl_a_00252Transactions of the Association for Computational Linguistics. 7Linfeng Song, Daniel Gildea, Yue Zhang, Zhiguo Wang, and Jinsong Su. 2019. Semantic neural ma- chine translation using amr. Transactions of the As- sociation for Computational Linguistics, 7:19-31.</p>
<p>A graph-to-sequence model for AMRto-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, 10.18653/v1/P18-1150Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaLong Papers1Association for Computational LinguisticsLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel Gildea. 2018. A graph-to-sequence model for AMR- to-text generation. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1616- 1626, Melbourne, Australia. Association for Compu- tational Linguistics.</p>
<p>The role of reentrancies in Abstract Meaning Representation parsing. Ida Szubert, Marco Damonte, Shay B Cohen, Mark Steedman, 10.18653/v1/2020.findings-emnlp.199Findings of the Association for Computational Linguistics: EMNLP 2020. Online. Association for Computational LinguisticsIda Szubert, Marco Damonte, Shay B. Cohen, and Mark Steedman. 2020. The role of reentrancies in Abstract Meaning Representation parsing. In Find- ings of the Association for Computational Linguis- tics: EMNLP 2020, pages 2198-2207, Online. As- sociation for Computational Linguistics.</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, Advances in Neural Information Processing Systems. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett, editors, Advances in Neural Information Pro- cessing Systems 30, pages 5998-6008. Curran Asso- ciates, Inc.</p>
<p>Graph attention networks. Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, Yoshua Bengio, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track ProceedingsPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph attention networks. In 6th Inter- national Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings.</p>
<p>Neural wikipedian: Generating textual summaries from knowledge base triples. Pavlos Vougiouklis, Hady Elsahar, Lucie-Aimée Kaffee, Christophe Gravier, Frédérique Laforest, Jonathon Hare, Elena Simperl, 10.1016/j.websem.2018.07.002Journal of Web Semantics. Pavlos Vougiouklis, Hady Elsahar, Lucie-Aimée Kaffee, Christophe Gravier, Frédérique Laforest, Jonathon Hare, and Elena Simperl. 2018. Neu- ral wikipedian: Generating textual summaries from knowledge base triples. Journal of Web Semantics, 52-53:1 -15.</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in Neural Information Processing Systems. H. Wallach, H. Larochelle, A. Beygelzimer, F. Alché-Buc, E. Fox, and R. Garnett32Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language un- derstanding systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. Alché-Buc, E. Fox, and R. Gar- nett, editors, Advances in Neural Information Pro- cessing Systems 32, pages 3266-3280.</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, 10.18653/v1/W18-5446Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsAlex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In Pro- ceedings of the 2018 EMNLP Workshop Black- boxNLP: Analyzing and Interpreting Neural Net- works for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Huggingface's transformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie BrewThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow- icz, and Jamie Brew. 2019. Huggingface's trans- formers: State-of-the-art natural language process- ing.</p>
<p>How powerful are graph neural networks?. Keyulu Xu, Weihua Hu, Jure Leskovec, Stefanie Jegelka, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How powerful are graph neural net- works? In 7th International Conference on Learn- ing Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019.</p>
<p>Sidetuning: Network adaptation via additive side networks. Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas J Guibas, Jitendra Malik, arXiv e-printsJeffrey O. Zhang, Alexander Sax, Amir Zamir, Leonidas J. Guibas, and Jitendra Malik. 2019. Side- tuning: Network adaptation via additive side net- works. arXiv e-prints.</p>
<p>Bertscore: Evaluating text generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020a. Bertscore: Eval- uating text generation with BERT. In 8th Inter- national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.</p>
<p>Lightweight, dynamic graph convolutional networks for AMR-to-text generation. Yan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B Cohen, Zuozhu Liu, Lidong Bing, 10.18653/v1/2020.emnlp-main.169Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsYan Zhang, Zhijiang Guo, Zhiyang Teng, Wei Lu, Shay B. Cohen, Zuozhu Liu, and Lidong Bing. 2020b. Lightweight, dynamic graph convolutional networks for AMR-to-text generation. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2162-2172, Online. Association for Computational Linguistics.</p>            </div>
        </div>

    </div>
</body>
</html>
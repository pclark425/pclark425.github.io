<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7430 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7430</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7430</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-273163269</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.03278v1.pdf" target="_blank">What do Large Language Models Need for Machine Translation Evaluation?</a></p>
                <p><strong>Paper Abstract:</strong> Leveraging large language models (LLMs) for various natural language processing tasks has led to superlative claims about their performance. For the evaluation of machine translation (MT), existing research shows that LLMs are able to achieve results comparable to fine-tuned multilingual pre-trained language models. In this paper, we explore what translation information, such as the source, reference, translation errors and annotation guidelines, is needed for LLMs to evaluate MT quality. In addition, we investigate prompting techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for eight language pairs covering high-, medium- and low-resource languages, leveraging varying LLM variants. Our findings indicate the importance of reference translations for an LLM-based evaluation. While larger models do not necessarily fare better, they tend to benefit more from CoT prompting, than smaller models. We also observe that LLMs do not always provide a numerical score when generating evaluations, which poses a question on their reliability for the task. Our work presents a comprehensive analysis for resource-constrained and training-less LLM-based evaluation of machine translation. We release the accrued prompt templates, code and data publicly for reproducibility.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7430.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7430.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template 3 (GEMBA variant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Template 3: source + MT output + reference (GEMBA-style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-shot natural-language prompt that supplies the source sentence, the machine translation (MT) output, and the human reference to the LLM and asks for a DA score (0–100). The paper finds this template yields the highest Spearman correlations among tested prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenChat3.5; (also tested: Llama-2-7B, Gemma-7B, Llama-2-13B, Qwen1.5-14B, Mixtral-8x7B-AWQ)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned chat LLMs of varying architectures (dense and MoE), used in chat format for zero-shot evaluation; Mixtral used in AWQ quantized form.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–14B (OpenChat3.5 7B; Llama-2 variants 7B/13B; Qwen1.5 14B; Mixtral ~56B-experts but used 8x7B AWQ form)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation quality evaluation (DA score prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict continuous Direct Assessment (DA) scores (0–100) for MT outputs given source/MT/reference; Spearman correlation against mean human DA scores.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language instruction prompt containing source, MT output and reference; zero-shot scoring request; instruction to output numeric score (often JSON requested).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot base prompt (GEMBA-like). Includes explicit instruction 'Score the following translation... 0 to 100'; sometimes asks for JSON output. Input formatting adjusted per model's chat format. Temperature 0.8, top_p 0.95, input length 1024 for zero-shot runs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman correlation (ρ) with human DA scores; also tracked dropped instances where model did not output a numeric score.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Template 3 produced the highest Spearman correlation among tested templates for many language pairs; e.g., OpenChat3.5 T3 EN-ZH ρ = 0.3995 (reported in Table 4 as T3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Higher Spearman vs templates that omit source or reference (qualitative summary reported; specific absolute differences vary by model/language pair).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot. Models run with instruction-tuned chat format; temperature 0.8, top_p 0.95, input length 1024.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What do Large Language Models Need for Machine Translation Evaluation?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7430.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7430.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Source inclusion effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance impact of including the source sentence in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Including the source in the prompt (comparing 'MT+reference' vs 'source+MT+reference') increases LLM correlation with human DA scores; the authors explicitly note the source is essential for evaluation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed across multiple LLMs (OpenChat3.5, Llama-2-7B, Gemma-7B, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned chat LLMs (dense and MoE variants) evaluated in zero-shot prompting conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–14B model range</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation quality evaluation (DA score prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict DA scores for MT outputs; measure Spearman correlation with human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural-language prompt variants differing in which textual fields are included (Template 2: MT+reference vs Template 3: source+MT+reference).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input content</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Templates explicitly differ by presence/absence of source; same instruction to score 0–100; formatted per model chat format.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman correlation (ρ)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: prompts that include the source yield higher Spearman ρ than those without; the paper states the source is 'an essential component' and notes higher correlations when source included (no single uniform numeric given across all pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Template 2 (MT output + reference) used as the contrasting prompt lacking source (baseline for this effect).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative increase in Spearman correlation when adding source (absolute change not reported uniformly across all pairs; per-pair differences available in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot templates; same hyperparameters as other zero-shot runs.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What do Large Language Models Need for Machine Translation Evaluation?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7430.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7430.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference inclusion effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Performance impact of including the human reference in prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Including reference translations in prompts substantially improves evaluation accuracy (Spearman correlation); removing references (e.g., prompts using only source+MT+error words) lowers correlation scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed across multiple LLMs, highlighted for OpenChat3.5 and others</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned chat LLMs tested in zero-shot and CoT settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–14B range</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation quality evaluation (DA score prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict DA scores using varying prompt inputs; compare reference-present vs reference-absent prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural-language prompts with/without reference translations (Templates 1-6; Template 3 includes reference, Template 4 excludes it).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input content</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Templates compared: (T3) source+MT+reference vs (T4) source+MT+error_words (reference removed). Instruction to output numeric score 0–100.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman correlation (ρ)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: removal of references 'clearly lowered correlation scores' (paper statement); Template 3 often had highest correlations for many language pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Template 3 (reference-including) serves as higher-performing baseline; Template 4 (no reference) lower performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Described as a clear decrease in Spearman correlation when references are removed (absolute changes vary by model and language pair; specific per-pair values in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; same decoding hyperparameters (temperature 0.8, top_p 0.95).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What do Large Language Models Need for Machine Translation Evaluation?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7430.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7430.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Error-word inclusion (Templates 4 & 5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompts augmented with extracted error words from word-level QE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adding 'error words' (tokens labeled BAD from word-level QE) into prompts did not increase Spearman correlation but reduced the number of instances where LLMs failed to output a numeric score (fewer dropped rows), improving output stability/robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenChat3.5 and other LLMs (observed across model set)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned chat LLMs; models evaluated include 7B and 13B variants and a MoE AWQ-quantized model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–14B range (observed behavior across sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation quality evaluation (DA score prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>LLMs asked to score MT outputs; prompts include extracted error words (BAD tokens) in addition to source/MT/reference or instead of reference.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot prompt variants (Templates 4 and 5) that append a list of error tokens to the instruction input; JSON-score requested.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input content</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Templates with 'error words' (either with or without reference). Observed that Templates 4 and 5 produced fewer 'dropped' instances where the model did not output a score.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman correlation (ρ) and dropped-row count (D)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: No improvement in Spearman ρ on average, but 'fewer dropped rows' reported for Templates 4 and 5 (improved consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Template 3 (source+MT+reference) for correlation; Template 1/2 without error words for robustness comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>No positive change in Spearman correlation; reduced dropped rows (improved robustness) compared to templates without error words (absolute dropped-row counts in paper Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot; same decoding hyperparameters; error words extracted from WMT22 word-level QE dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What do Large Language Models Need for Machine Translation Evaluation?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7430.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7430.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) prompting effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Two-step Chain-of-Thought prompting (Template 7)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-step CoT prompt asks the LLM first to analyze differences between MT and reference and then to score based on that analysis. CoT prompting tended to help larger models for some language pairs but decreased performance for smaller 7B models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed across model set; specific effects noted for OpenChat3.5 (7B), Llama-2-7B, Llama-2-13B, Qwen1.5-14B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned chat LLMs; CoT implemented as two chained prompts (analysis then scoring).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 13–14B variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation quality evaluation (DA score prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use a CoT-style two-step prompt (analysis then scoring) and compare Spearman correlation to the zero-shot Template 3.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Chain-of-Thought prompt (two-step): Prompt1 requests stepwise analysis of differences; Prompt2 instructs model to produce a score based on analysis. JSON score requested.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / reasoning mode</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Template 7: two prompts (analysis then scoring), built on Template 3 content (source+MT+reference). Regular-expression extraction of numeric score used for evaluation. Temperature 0.8, top_p 0.95, input length 1024.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman correlation (ρ); dropped rows tracked separately.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Mixed: For 7B models, CoT produced worse Spearman (example OpenChat3.5 EN-DE: T7 ρ=0.2433 vs T3 ρ=0.2849, Δ = -0.0416 absolute). For some larger models/pairs, CoT improved results (e.g., Qwen1.5-14B EN-DE: T7 ρ=0.2388 vs T3 ρ=0.2182, Δ = +0.0206 absolute) — overall effect is model- and language-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Template 3 (zero-shot source+MT+reference) used as baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>For 7B models: decreases in Spearman ρ (example decreases up to ~0.12 absolute for some pairs/models). For some 13–14B models: small absolute improvements for particular pairs (example +0.0206 for Qwen1.5 EN-DE).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>CoT two-step prompt (Template 7); models run with same decoding hyperparameters. Dropped-row counts typically lower (more consistent outputs) with CoT for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What do Large Language Models Need for Machine Translation Evaluation?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7430.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7430.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot (5-shot) prompting effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>5-shot few-shot prompting using Template 8 (five example prefixed demonstrations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Five examples (one sampled from each DA score range 0–20, 21–40, 41–60, 61–80, 81–100) prefixed to the prompt do not consistently improve Spearman correlation; in many cases few-shot performed worse than zero-shot.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenChat3.5 and other models (Llama-2 variants, Gemma-7B, Qwen1.5-14B, Mixtral-AWQ)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned chat LLMs run with 5-shot prefix demonstrations sampled randomly per language pair.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–14B range</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation quality evaluation (DA score prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide five example scored instances as a prefix and ask model to score new MTs; measure Spearman correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompt: Template 8 adds 5 demonstration examples before base instruction; chosen by score bucket sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Five-shot prefix, sampled one example from each score bucket (0–20,21–40,41–60,61–80,81–100). Input length for few-shot set to 3000 tokens. Temperature 0.8, top_p 0.95.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman correlation (ρ)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Often worse than zero-shot; example OpenChat3.5 EN-DE: T8 ρ=0.1756 vs T3 ρ=0.2849 (Δ = -0.1093 absolute). Overall the paper reports 5-shot did not outperform zero-shot in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Template 3 (zero-shot source+MT+reference).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Mostly negative or mixed; notable absolute drops seen for some model/language combinations (example above).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>5-shot; training/validation split used to sample examples; input length 3000; same decoding hyperparameters otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What do Large Language Models Need for Machine Translation Evaluation?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7430.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7430.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Output formatting and robustness issues</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of response format and model-specific formatting on scoring reliability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs frequently produce long textual explanations rather than strict numeric outputs; models differ in propensity to emit a numeric score (many runs required regex extraction), and some models (notably Llama-2 variants) frequently failed to output a valid numeric score ('dropped rows').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B, Llama-2-13B (noted high drop rate); OpenChat3.5 (noted for consistent numeric outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned chat models; models' chat-format training leads to variable adherence to precise output format requests.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 13B variants discussed</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation quality evaluation (DA score prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>LLMs asked to produce a numeric DA score, with instruction to output JSON; reliability measured by count of instances where no numeric score was produced.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Instruction requesting strict JSON numeric output; model-specific prompt formatting applied before inference; extraction performed with regular expressions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / output format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompts often instruct 'Provide the score strictly in JSON format.' Despite this, many models output explanations; authors used regex to extract scores and counted dropped instances (denoted D). OpenChat3.5 produced far fewer dropped rows; Llama-2 models dropped many.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Dropped-row count (instances with no valid numeric score) and downstream Spearman correlation computed after dropping those instances.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: OpenChat3.5 'consistently providing valid quality evaluation scores, with very few dropped rows'; Llama-2 variants 'performed poorly in generating evaluations with valid scores' (many dropped rows). Exact per-model dropped counts are reported in paper Table 6.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Not applicable (diagnostic of reliability rather than a scalar change in correlation); adding error-words or CoT reduced dropped rows for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>All templates used instruction to output JSON; models formatted per their chat format prior to inference; regex extraction and dropped-row handling applied.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What do Large Language Models Need for Machine Translation Evaluation?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7430.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7430.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model size vs prompting sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relationship between model size and responsiveness to prompting styles (CoT/few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Larger models (within the tested range) do not always produce higher absolute correlation, but tend to benefit more from CoT prompting than smaller models; smaller models sometimes outperform larger ones in zero-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Observed across tested models: OpenChat3.5 (7B) outperformed some larger 13–14B models in zero-shot; CoT helped 13B+ models more than 7B ones.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs of different parameter counts and architectures (dense vs MoE AWQ).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (OpenChat3.5, Llama-2-7B, Gemma-7B) and 13–14B (Llama-2-13B, Qwen1.5-14B); Mixtral MoE in AWQ form</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine Translation quality evaluation (DA score prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess correlation with human scores under different prompting techniques (zero-shot, CoT, few-shot) across models of different sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Comparative prompting experiments (zero-shot T3, CoT T7, few-shot T8) applied across models and languages.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / model sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Observed that a 7B model (OpenChat3.5) achieved the highest Spearman for many language pairs in zero-shot; however, CoT yields more benefit for larger models (13B+) for particular language pairs (e.g., EN-DE, EN-MR).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Spearman correlation (ρ)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: 'While larger models are not always better, they tend to benefit more from CoT prompting than smaller models.' Specific numeric examples for CoT improvements/decrements are provided in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot Template 3 per model used as baseline for CoT/few-shot comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Mixed: some larger models show small absolute gains with CoT on some pairs (example Qwen1.5 EN-DE +0.0206 ρ), while 7B models often see decreases (example OpenChat3.5 EN-DE -0.0416 ρ).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>All models evaluated with same decoding settings; input formatting adjusted per model. No models >14B tested due to compute limits.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What do Large Language Models Need for Machine Translation Evaluation?', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 2)</em></li>
                <li>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation <em>(Rating: 2)</em></li>
                <li>Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>COMET: A neural framework for MT evaluation <em>(Rating: 1)</em></li>
                <li>TransQuest: Translation quality estimation with cross-lingual transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7430",
    "paper_id": "paper-273163269",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Template 3 (GEMBA variant)",
            "name_full": "Prompt Template 3: source + MT output + reference (GEMBA-style)",
            "brief_description": "A zero-shot natural-language prompt that supplies the source sentence, the machine translation (MT) output, and the human reference to the LLM and asks for a DA score (0–100). The paper finds this template yields the highest Spearman correlations among tested prompt variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenChat3.5; (also tested: Llama-2-7B, Gemma-7B, Llama-2-13B, Qwen1.5-14B, Mixtral-8x7B-AWQ)",
            "model_description": "Instruction-tuned chat LLMs of varying architectures (dense and MoE), used in chat format for zero-shot evaluation; Mixtral used in AWQ quantized form.",
            "model_size": "7B–14B (OpenChat3.5 7B; Llama-2 variants 7B/13B; Qwen1.5 14B; Mixtral ~56B-experts but used 8x7B AWQ form)",
            "task_name": "Machine Translation quality evaluation (DA score prediction)",
            "task_description": "Predict continuous Direct Assessment (DA) scores (0–100) for MT outputs given source/MT/reference; Spearman correlation against mean human DA scores.",
            "problem_format": "Natural-language instruction prompt containing source, MT output and reference; zero-shot scoring request; instruction to output numeric score (often JSON requested).",
            "format_category": "prompt style",
            "format_details": "Zero-shot base prompt (GEMBA-like). Includes explicit instruction 'Score the following translation... 0 to 100'; sometimes asks for JSON output. Input formatting adjusted per model's chat format. Temperature 0.8, top_p 0.95, input length 1024 for zero-shot runs.",
            "performance_metric": "Spearman correlation (ρ) with human DA scores; also tracked dropped instances where model did not output a numeric score.",
            "performance_value": "Template 3 produced the highest Spearman correlation among tested templates for many language pairs; e.g., OpenChat3.5 T3 EN-ZH ρ = 0.3995 (reported in Table 4 as T3).",
            "baseline_performance": null,
            "performance_change": "Higher Spearman vs templates that omit source or reference (qualitative summary reported; specific absolute differences vary by model/language pair).",
            "experimental_setting": "Zero-shot. Models run with instruction-tuned chat format; temperature 0.8, top_p 0.95, input length 1024.",
            "statistical_significance": null,
            "uuid": "e7430.0",
            "source_info": {
                "paper_title": "What do Large Language Models Need for Machine Translation Evaluation?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Source inclusion effect",
            "name_full": "Performance impact of including the source sentence in prompt",
            "brief_description": "Including the source in the prompt (comparing 'MT+reference' vs 'source+MT+reference') increases LLM correlation with human DA scores; the authors explicitly note the source is essential for evaluation accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed across multiple LLMs (OpenChat3.5, Llama-2-7B, Gemma-7B, etc.)",
            "model_description": "Instruction-tuned chat LLMs (dense and MoE variants) evaluated in zero-shot prompting conditions.",
            "model_size": "7B–14B model range",
            "task_name": "Machine Translation quality evaluation (DA score prediction)",
            "task_description": "Predict DA scores for MT outputs; measure Spearman correlation with human ratings.",
            "problem_format": "Zero-shot natural-language prompt variants differing in which textual fields are included (Template 2: MT+reference vs Template 3: source+MT+reference).",
            "format_category": "prompt style / input content",
            "format_details": "Templates explicitly differ by presence/absence of source; same instruction to score 0–100; formatted per model chat format.",
            "performance_metric": "Spearman correlation (ρ)",
            "performance_value": "Qualitative: prompts that include the source yield higher Spearman ρ than those without; the paper states the source is 'an essential component' and notes higher correlations when source included (no single uniform numeric given across all pairs).",
            "baseline_performance": "Template 2 (MT output + reference) used as the contrasting prompt lacking source (baseline for this effect).",
            "performance_change": "Qualitative increase in Spearman correlation when adding source (absolute change not reported uniformly across all pairs; per-pair differences available in paper tables).",
            "experimental_setting": "Zero-shot templates; same hyperparameters as other zero-shot runs.",
            "statistical_significance": null,
            "uuid": "e7430.1",
            "source_info": {
                "paper_title": "What do Large Language Models Need for Machine Translation Evaluation?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Reference inclusion effect",
            "name_full": "Performance impact of including the human reference in prompt",
            "brief_description": "Including reference translations in prompts substantially improves evaluation accuracy (Spearman correlation); removing references (e.g., prompts using only source+MT+error words) lowers correlation scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed across multiple LLMs, highlighted for OpenChat3.5 and others",
            "model_description": "Instruction-tuned chat LLMs tested in zero-shot and CoT settings.",
            "model_size": "7B–14B range",
            "task_name": "Machine Translation quality evaluation (DA score prediction)",
            "task_description": "Predict DA scores using varying prompt inputs; compare reference-present vs reference-absent prompts.",
            "problem_format": "Zero-shot natural-language prompts with/without reference translations (Templates 1-6; Template 3 includes reference, Template 4 excludes it).",
            "format_category": "prompt style / input content",
            "format_details": "Templates compared: (T3) source+MT+reference vs (T4) source+MT+error_words (reference removed). Instruction to output numeric score 0–100.",
            "performance_metric": "Spearman correlation (ρ)",
            "performance_value": "Qualitative: removal of references 'clearly lowered correlation scores' (paper statement); Template 3 often had highest correlations for many language pairs.",
            "baseline_performance": "Template 3 (reference-including) serves as higher-performing baseline; Template 4 (no reference) lower performance.",
            "performance_change": "Described as a clear decrease in Spearman correlation when references are removed (absolute changes vary by model and language pair; specific per-pair values in paper tables).",
            "experimental_setting": "Zero-shot; same decoding hyperparameters (temperature 0.8, top_p 0.95).",
            "statistical_significance": null,
            "uuid": "e7430.2",
            "source_info": {
                "paper_title": "What do Large Language Models Need for Machine Translation Evaluation?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Error-word inclusion (Templates 4 & 5)",
            "name_full": "Prompts augmented with extracted error words from word-level QE",
            "brief_description": "Adding 'error words' (tokens labeled BAD from word-level QE) into prompts did not increase Spearman correlation but reduced the number of instances where LLMs failed to output a numeric score (fewer dropped rows), improving output stability/robustness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenChat3.5 and other LLMs (observed across model set)",
            "model_description": "Instruction-tuned chat LLMs; models evaluated include 7B and 13B variants and a MoE AWQ-quantized model.",
            "model_size": "7B–14B range (observed behavior across sizes)",
            "task_name": "Machine Translation quality evaluation (DA score prediction)",
            "task_description": "LLMs asked to score MT outputs; prompts include extracted error words (BAD tokens) in addition to source/MT/reference or instead of reference.",
            "problem_format": "Zero-shot prompt variants (Templates 4 and 5) that append a list of error tokens to the instruction input; JSON-score requested.",
            "format_category": "prompt style / input content",
            "format_details": "Templates with 'error words' (either with or without reference). Observed that Templates 4 and 5 produced fewer 'dropped' instances where the model did not output a score.",
            "performance_metric": "Spearman correlation (ρ) and dropped-row count (D)",
            "performance_value": "Qualitative: No improvement in Spearman ρ on average, but 'fewer dropped rows' reported for Templates 4 and 5 (improved consistency).",
            "baseline_performance": "Template 3 (source+MT+reference) for correlation; Template 1/2 without error words for robustness comparison.",
            "performance_change": "No positive change in Spearman correlation; reduced dropped rows (improved robustness) compared to templates without error words (absolute dropped-row counts in paper Table 6).",
            "experimental_setting": "Zero-shot; same decoding hyperparameters; error words extracted from WMT22 word-level QE dataset.",
            "statistical_significance": null,
            "uuid": "e7430.3",
            "source_info": {
                "paper_title": "What do Large Language Models Need for Machine Translation Evaluation?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) prompting effect",
            "name_full": "Two-step Chain-of-Thought prompting (Template 7)",
            "brief_description": "A two-step CoT prompt asks the LLM first to analyze differences between MT and reference and then to score based on that analysis. CoT prompting tended to help larger models for some language pairs but decreased performance for smaller 7B models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed across model set; specific effects noted for OpenChat3.5 (7B), Llama-2-7B, Llama-2-13B, Qwen1.5-14B",
            "model_description": "Instruction-tuned chat LLMs; CoT implemented as two chained prompts (analysis then scoring).",
            "model_size": "7B and 13–14B variants",
            "task_name": "Machine Translation quality evaluation (DA score prediction)",
            "task_description": "Use a CoT-style two-step prompt (analysis then scoring) and compare Spearman correlation to the zero-shot Template 3.",
            "problem_format": "Chain-of-Thought prompt (two-step): Prompt1 requests stepwise analysis of differences; Prompt2 instructs model to produce a score based on analysis. JSON score requested.",
            "format_category": "prompt style / reasoning mode",
            "format_details": "Template 7: two prompts (analysis then scoring), built on Template 3 content (source+MT+reference). Regular-expression extraction of numeric score used for evaluation. Temperature 0.8, top_p 0.95, input length 1024.",
            "performance_metric": "Spearman correlation (ρ); dropped rows tracked separately.",
            "performance_value": "Mixed: For 7B models, CoT produced worse Spearman (example OpenChat3.5 EN-DE: T7 ρ=0.2433 vs T3 ρ=0.2849, Δ = -0.0416 absolute). For some larger models/pairs, CoT improved results (e.g., Qwen1.5-14B EN-DE: T7 ρ=0.2388 vs T3 ρ=0.2182, Δ = +0.0206 absolute) — overall effect is model- and language-dependent.",
            "baseline_performance": "Template 3 (zero-shot source+MT+reference) used as baseline for comparison.",
            "performance_change": "For 7B models: decreases in Spearman ρ (example decreases up to ~0.12 absolute for some pairs/models). For some 13–14B models: small absolute improvements for particular pairs (example +0.0206 for Qwen1.5 EN-DE).",
            "experimental_setting": "CoT two-step prompt (Template 7); models run with same decoding hyperparameters. Dropped-row counts typically lower (more consistent outputs) with CoT for some models.",
            "statistical_significance": null,
            "uuid": "e7430.4",
            "source_info": {
                "paper_title": "What do Large Language Models Need for Machine Translation Evaluation?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Few-shot (5-shot) prompting effect",
            "name_full": "5-shot few-shot prompting using Template 8 (five example prefixed demonstrations)",
            "brief_description": "Five examples (one sampled from each DA score range 0–20, 21–40, 41–60, 61–80, 81–100) prefixed to the prompt do not consistently improve Spearman correlation; in many cases few-shot performed worse than zero-shot.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenChat3.5 and other models (Llama-2 variants, Gemma-7B, Qwen1.5-14B, Mixtral-AWQ)",
            "model_description": "Instruction-tuned chat LLMs run with 5-shot prefix demonstrations sampled randomly per language pair.",
            "model_size": "7B–14B range",
            "task_name": "Machine Translation quality evaluation (DA score prediction)",
            "task_description": "Provide five example scored instances as a prefix and ask model to score new MTs; measure Spearman correlation.",
            "problem_format": "Few-shot prompt: Template 8 adds 5 demonstration examples before base instruction; chosen by score bucket sampling.",
            "format_category": "prompt style / few-shot",
            "format_details": "Five-shot prefix, sampled one example from each score bucket (0–20,21–40,41–60,61–80,81–100). Input length for few-shot set to 3000 tokens. Temperature 0.8, top_p 0.95.",
            "performance_metric": "Spearman correlation (ρ)",
            "performance_value": "Often worse than zero-shot; example OpenChat3.5 EN-DE: T8 ρ=0.1756 vs T3 ρ=0.2849 (Δ = -0.1093 absolute). Overall the paper reports 5-shot did not outperform zero-shot in many cases.",
            "baseline_performance": "Template 3 (zero-shot source+MT+reference).",
            "performance_change": "Mostly negative or mixed; notable absolute drops seen for some model/language combinations (example above).",
            "experimental_setting": "5-shot; training/validation split used to sample examples; input length 3000; same decoding hyperparameters otherwise.",
            "statistical_significance": null,
            "uuid": "e7430.5",
            "source_info": {
                "paper_title": "What do Large Language Models Need for Machine Translation Evaluation?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Output formatting and robustness issues",
            "name_full": "Effect of response format and model-specific formatting on scoring reliability",
            "brief_description": "LLMs frequently produce long textual explanations rather than strict numeric outputs; models differ in propensity to emit a numeric score (many runs required regex extraction), and some models (notably Llama-2 variants) frequently failed to output a valid numeric score ('dropped rows').",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B, Llama-2-13B (noted high drop rate); OpenChat3.5 (noted for consistent numeric outputs)",
            "model_description": "Instruction-tuned chat models; models' chat-format training leads to variable adherence to precise output format requests.",
            "model_size": "7B and 13B variants discussed",
            "task_name": "Machine Translation quality evaluation (DA score prediction)",
            "task_description": "LLMs asked to produce a numeric DA score, with instruction to output JSON; reliability measured by count of instances where no numeric score was produced.",
            "problem_format": "Instruction requesting strict JSON numeric output; model-specific prompt formatting applied before inference; extraction performed with regular expressions.",
            "format_category": "prompt style / output format",
            "format_details": "Prompts often instruct 'Provide the score strictly in JSON format.' Despite this, many models output explanations; authors used regex to extract scores and counted dropped instances (denoted D). OpenChat3.5 produced far fewer dropped rows; Llama-2 models dropped many.",
            "performance_metric": "Dropped-row count (instances with no valid numeric score) and downstream Spearman correlation computed after dropping those instances.",
            "performance_value": "Qualitative: OpenChat3.5 'consistently providing valid quality evaluation scores, with very few dropped rows'; Llama-2 variants 'performed poorly in generating evaluations with valid scores' (many dropped rows). Exact per-model dropped counts are reported in paper Table 6.",
            "baseline_performance": null,
            "performance_change": "Not applicable (diagnostic of reliability rather than a scalar change in correlation); adding error-words or CoT reduced dropped rows for some models.",
            "experimental_setting": "All templates used instruction to output JSON; models formatted per their chat format prior to inference; regex extraction and dropped-row handling applied.",
            "statistical_significance": null,
            "uuid": "e7430.6",
            "source_info": {
                "paper_title": "What do Large Language Models Need for Machine Translation Evaluation?",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Model size vs prompting sensitivity",
            "name_full": "Relationship between model size and responsiveness to prompting styles (CoT/few-shot)",
            "brief_description": "Larger models (within the tested range) do not always produce higher absolute correlation, but tend to benefit more from CoT prompting than smaller models; smaller models sometimes outperform larger ones in zero-shot baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Observed across tested models: OpenChat3.5 (7B) outperformed some larger 13–14B models in zero-shot; CoT helped 13B+ models more than 7B ones.",
            "model_description": "Instruction-tuned LLMs of different parameter counts and architectures (dense vs MoE AWQ).",
            "model_size": "7B (OpenChat3.5, Llama-2-7B, Gemma-7B) and 13–14B (Llama-2-13B, Qwen1.5-14B); Mixtral MoE in AWQ form",
            "task_name": "Machine Translation quality evaluation (DA score prediction)",
            "task_description": "Assess correlation with human scores under different prompting techniques (zero-shot, CoT, few-shot) across models of different sizes.",
            "problem_format": "Comparative prompting experiments (zero-shot T3, CoT T7, few-shot T8) applied across models and languages.",
            "format_category": "prompt style / model sensitivity",
            "format_details": "Observed that a 7B model (OpenChat3.5) achieved the highest Spearman for many language pairs in zero-shot; however, CoT yields more benefit for larger models (13B+) for particular language pairs (e.g., EN-DE, EN-MR).",
            "performance_metric": "Spearman correlation (ρ)",
            "performance_value": "Qualitative: 'While larger models are not always better, they tend to benefit more from CoT prompting than smaller models.' Specific numeric examples for CoT improvements/decrements are provided in Table 4.",
            "baseline_performance": "Zero-shot Template 3 per model used as baseline for CoT/few-shot comparisons.",
            "performance_change": "Mixed: some larger models show small absolute gains with CoT on some pairs (example Qwen1.5 EN-DE +0.0206 ρ), while 7B models often see decreases (example OpenChat3.5 EN-DE -0.0416 ρ).",
            "experimental_setting": "All models evaluated with same decoding settings; input formatting adjusted per model. No models &gt;14B tested due to compute limits.",
            "statistical_significance": null,
            "uuid": "e7430.7",
            "source_info": {
                "paper_title": "What do Large Language Models Need for Machine Translation Evaluation?",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 2,
            "sanitized_title": "large_language_models_are_stateoftheart_evaluators_of_translation_quality"
        },
        {
            "paper_title": "The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation",
            "rating": 2,
            "sanitized_title": "the_devil_is_in_the_errors_leveraging_large_language_models_for_finegrained_machine_translation_evaluation"
        },
        {
            "paper_title": "Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation",
            "rating": 2,
            "sanitized_title": "lost_in_the_source_language_how_large_language_models_evaluate_the_quality_of_machine_translation"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "COMET: A neural framework for MT evaluation",
            "rating": 1,
            "sanitized_title": "comet_a_neural_framework_for_mt_evaluation"
        },
        {
            "paper_title": "TransQuest: Translation quality estimation with cross-lingual transformers",
            "rating": 1,
            "sanitized_title": "transquest_translation_quality_estimation_with_crosslingual_transformers"
        }
    ],
    "cost": 0.01733925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>What do Large Language Models Need for Machine Translation Evaluation?
9 Oct 2024</p>
<p>Shenbin Qian s.qian@surrey.ac.uk 
Archchana Sindhujan a.sindhujan@surrey.ac.uk 
Institute for People-Centred AI
University of Surrey
United Kingdom</p>
<p>Minnie Kabra minniekabra@gmail.com 
Independent Researcher
India</p>
<p>Diptesh Kanojia d.kanojia@surrey.ac.uk 
Institute for People-Centred AI
University of Surrey
United Kingdom</p>
<p>Constantin Orȃsan c.orasan@surrey.ac.uk 
Centre for Translation Studies
University of Surrey
United Kingdom</p>
<p>Tharindu Ranasinghe t.ranasinghe@lancaster.ac.uk 
Lancaster University
United Kingdom</p>
<p>Frédéric Blain 
Tilburg University
The Netherlands</p>
<p>Josh Openai 
Steven Achiam 
Sandhini Adler 
Lama Agarwal 
Ilge Ahmad 
Florencia Akkaya 
Diogo Leoni 
Janko Almeida 
Sam Altenschmidt 
Shyamal Alt- Man 
Red Anadkat 
Igor Avila 
Suchir Babuschkin 
Valerie Balaji 
Paul Balcom 
Haim- Ing Baltescu 
Mohammad Bao 
Jeff Bavarian 
Ir- Wan Belgum 
Jake Bello 
Gabriel Berdine 
Christopher Bernadett-Shapiro 
Lenny Berner 
Oleg Bogdonoff 
Madelaine Boiko 
Anna-Luisa Boyd 
Greg Brakman 
Tim Brock- Man 
Miles Brooks 
Kevin Brundage 
Trevor Button 
Rosie Cai 
Andrew Campbell 
Brittany Cann 
Chelsea Carey 
Rory Carlson 
Brooke Carmichael 
Che Chan 
Fotis Chang 
Derek Chantzis 
Sully Chen 
Ruby Chen 
Jason Chen 
Mark Chen 
Ben Chen 
Chester Chess 
Casey Cho 
HyungWon Chu 
Dave Chung 
Jeremiah Cummings 
Yunxing Currier 
Dai 
Tarun Goel 
Gabriel Gogineni 
Rapha Goh 
Jonathan Gontijo- Lopes 
Centre for Translation Studies
University of Surrey
United Kingdom</p>
<p>Morgan Gordon 
Scott Grafstein 
Ryan Gray 
Joshua Greene 
ShixiangShane Gross 
Yufei Gu 
Chris Guo 
Jesse Hallacy 
Jeff Han 
Yuchen Harris 
Mike He 
Johannes Heaton 
Chris Heidecke 
Alan Hesse 
Wade Hickey 
Peter Hickey 
Brandon Hoeschele 
Kenny Houghton 
Shengli Hsu 
Xin Hu 
Joost Hu 
Shantanu Huizinga 
Shawn Jain 
Joanne Jain 
Angela Jang 
Roger Jiang 
Haozhun Jiang 
Denny Jin 
Shino Jin 
Billie Jomoto 
Hee- Woo Jonn 
Tomer Jun 
Łukasz Kaftan 
Ali Kaiser 
Ingmar Ka- Mali 
Kanitscheider 
Shirish Nitish 
Tabarak Keskar 
Logan Khan 
Jong Wook Kilpatrick 
Christina Kim 
Yongjik Kim 
Jan Hendrik Kim 
Jamie Kirch- Ner 
Matt Kiros 
Daniel Knight 
Łukasz Kokotajlo 
Andrew Kondraciuk 
Aris Kondrich 
Kyle Kon- Stantinidis 
Gretchen Kosic 
Vishal Krueger 
Michael Kuo 
Ikai Lampe 
Teddy Lan 
Jan Lee 
Jade Leike 
Daniel Leung 
ChakMing Levy 
Rachel Li 
Molly Lim 
Stephanie Lin 
Mateusz Lin 
Theresa Litwin 
Ryan Lopez 
Patricia Lowe 
Anna Lue 
Kim Makanju 
Sam Malfacini 
Todor Manning 
Yaniv Markov 
Bianca Markovski 
Katie Martin 
Andrew Mayer 
Bob Mayne 
Scott Mayer Mcgrew 
Christine Mckinney 
Paul Mcleavey 
Jake Mcmillan 
David Mcneil 
Aalok Medina 
Jacob Mehta 
Luke Menick 
Andrey Metz 
Pamela Mishchenko 
Vinnie Mishkin 
Evan Monaco 
Daniel Morikawa 
Tong Mossing 
Mira Mu 
Oleg Murati 
David Murk 
Ashvin Mély 
Reiichiro Nair 
Rajeev Nakano 
Arvind Nayak 
Richard Neelakantan 
Hyeonwoo Ngo 
Long Noh 
Cullen Ouyang 
Jakub O'keefe 
Alex Pachocki 
Joe Paino 
Ashley Palermo 
Pantuliano 
Carl Ross 
Bob Rotsted 
Henri Roussez 
Nick Ry- Der 
Mario Saltarelli 
Ted Sanders 
Shibani Santurkar 
Girish Sastry 
Heather Schmidt 
David Schnurr 
John Schulman 
Daniel Selsam 
Kyla Sheppard 
Toki Sherbakov 
Jessica Shieh 
Sarah Shoker 
Pranav Shyam 
Szymon Sidor 
Eric Sigler 
Maddie Simens 
Jordan Sitkin 
Katarina Slama 
Ian Sohl 
Benjamin Sokolowsky 
Yang Song 
Natalie Staudacher 
Clemens Winter 
Samuel Wolrich 
Hannah Wong 
Lauren Workman 
Sherwin Wu 
Jeff Wu 
Michael Wu 
Kai Xiao 
Tao Xu 
Sarah Yoo 
Kevin Yu 
Qim- Ing Yuan 
Wojciech Zaremba 
Rowan Zellers 
Chong Zhang 
Marvin Zhang 
Shengjia Zhao 
Tianhao Zheng 
Juntang Zhuang 
William Zhuk 
Barret 2024 Zoph 
Gpt- 
Lancaster University
United Kingdom</p>
<p>Cory Decareaux
Thomas Degry
Noah Deutsch
Arka Dhar, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam FedusDamien Deville, David Dohan, Steve Dowling, Niko Felix</p>
<p>Simón Posada Fishman
Juston Forte</p>
<p>Isabella Ful-ford
Elie GeorgesLeo Gao, Christian Gibson, Vik</p>
<p>Giambat-tista Parascandolo
Joel Parish
Emy Parparita, Mikhail Pavlov, Andrew PengAlex Passos</p>
<p>Filipe de Avila Belbute Peres
Adam Perel-man
Michael Petrov</p>
<p>Henrique Ponde de Oliveira Pinto
Poko-rnyMichael</p>
<p>Michelle Pokrass
Vitchyr H. Pong, Tolly Pow-ell, Boris PowerAlethea Power</p>
<p>Elizabeth Proehl
Raul Puri, Alec Radford, Jack Rae, Aditya RameshCameron Raymond</p>
<p>Francis Real
Kendra Rimbach</p>
<p>Fe-lipe Petroski Such
Natalie Summers
Ilya Sutskever
Jie Tang</p>
<p>Nikolas Tezak
Phil Tillet, Jerry TworekMadeleine B. Thompson, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley</p>
<p>Juan Fe-lipe Cerón Uribe
Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter WelinderAlvin Wang, Ben Wang</p>
<p>Ji-ayi Weng
Lilian Weng, Matt Wiethoff, Dave Willner</p>
<p>What do Large Language Models Need for Machine Translation Evaluation?
9 Oct 20248E2FF1507EF2E70A88D28B2C596223F8arXiv:2410.03278v2[cs.CL]
Leveraging large language models (LLMs) for various natural language processing tasks has led to superlative claims about their performance.For the evaluation of machine translation (MT), existing research shows that LLMs are able to achieve results comparable to finetuned multilingual pre-trained language models.In this paper, we explore what translation information, such as the source, reference, translation errors and annotation guidelines, is needed for LLMs to evaluate MT quality.In addition, we investigate prompting techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for eight language pairs covering high-, medium-and lowresource languages, leveraging varying LLM variants.Our findings indicate the importance of reference translations for an LLM-based evaluation.While larger models do not necessarily fare better, they tend to benefit more from CoT prompting, than smaller models.We also observe that LLMs do not always provide a numerical score when generating evaluations, which poses a question on their reliability for the task.Our work presents a comprehensive analysis for resource-constrained and trainingless LLM-based evaluation of machine translation.We release the accrued prompt templates, code and data publicly for reproducibility 1 .</p>
<p>Introduction</p>
<p>Recent surge in the use of large language models (LLMs) for natural language processing (NLP) tasks like question answering (Kocoń et al., 2023;Tan et al., 2023) has taken strides, and significantly improved their applications to other downstream tasks such as machine translation (MT), text summarization, information retrieval and etc., due to advancements in natural language understanding capabilities, contextual awareness, and a versatile knowledge base (Kocmi and Federmann, 2023b;Zhu et al., 2023;Zhang et al., 2024).</p>
<p>For automatic evaluation of MT quality, traditional approaches use metrics such as BLEU (Papineni et al., 2002), BLEURT (Sellam et al., 2020) or BERTScore (Zhang* et al., 2020) to compare MT output with a reference translation.When references are not available, quality estimation (QE) methods such as fine-tuning multilingual pretrained language models (PTLMs) on human evaluation data like Direct Assessment (DA) scores (Graham et al., 2013) are often used to predict estimated scores to approximate human evaluation (Specia et al., 2018).Recent studies leverage prompting techniques and instruct LLMs to output a score for translation quality, claim to achieve promising results (Kocmi and Federmann, 2023b,a).</p>
<p>However, there exists no systematic exploration of what translation information LLMs need for quality evaluation, and whether different prompting techniques, such as Chain-of-Thought (CoT) (Wei et al., 2024) or few-shot prompting, can help boost the performance of LLMs.To that end, we conduct this investigation to systematically explore the ability of LLMs in quality evaluation in a training-less scenario.Our contributions can be summarized as:</p>
<p>• We investigate what translation information, i.e., source, reference, translation errors and annotation guidelines LLMs need to evaluate translation for 8 language pairs covering high-, medium-and low-resource languages.</p>
<p>• We explore different ways of prompting, i.e., zero-shot, CoT and few-shot prompting for LLMs to evaluate MT quality.Our code, data and prompts are released with the paper.</p>
<p>• We compare our prompting methods with finetuning of encoder-based multilingual PTLMs and find LLM performance still lags behind.</p>
<p>• Our analyses of the results on various prompt templates indicate that references are important for accurate translation evaluation with LLMs, and while larger models are not always better, they tend to benefit more from CoT prompting than smaller model variants.</p>
<p>The rest of the paper is structured as follows: Section 2 discusses relevant work in quality evaluation, while Section 3 introduces the dataset we utilize for this work.Section 4 describes the prompting methods and the baselines with the experimental setup.Results and discussion are presented in Section 5. Section 6 concludes our study and outlines future directions.</p>
<p>Related Work</p>
<p>Traditional automatic MT quality evaluation metrics such as BLEU, BLEURT and BERTScore compare the MT output to one or several references, whilst metrics like Translation Error Rate (TER) (Snover et al., 2006) are based on the number of edits required for MT output to become reference, and neither takes semantic variations into account.</p>
<p>Training supervised machine learning systems on human-annotated data based on metrics such as DA or Multi-dimensional Quality Metrics (MQM) (Lommel et al., 2014) can help predict translation quality without any references (Deoghare et al., 2023b).Ranasinghe et al. (2020bRanasinghe et al. ( , 2021) ) proposed the TransQuest framework to utilize the source text and MT output only and finetune XLM-RoBERTa-large (Conneau et al., 2020) to predict a DA score as an estimation of translation quality.COMET (Rei et al., 2020;Stewart et al., 2020;Rei et al., 2022b) was proposed initially to incorporate references along with the source and MT output to train multilingual PTLMs for quality evaluation, but later it also supported reference-less evaluation.Wan et al. (2022) proposed a unified translation evaluation framework that could include source or reference or both as input for quality evaluation.Various approaches achieved promising results in the QE shared task of the Conference on Machine Translation (WMT) (Specia et al., 2020(Specia et al., , 2021;;Zerva et al., 2022;Blain et al., 2023), however, most require supervision and training (Deoghare et al., 2023a;Kanojia et al., 2021).</p>
<p>The advent of LLMs prompted its application to translation quality evaluation.Kocmi and Federmann (2023b) proposed a zero-shot prompting technique, called GEMBA for DA score prediction using GPT-4 (OpenAI et al., 2024), claiming LLMs can achieve performance comparable to state-of-the-art models fine-tuned on multilingual data.Based on the GEMBA prompt, Fernandes et al. (2023) proposed to use LLMs for both DA score prediction and error categorization via fine-tuning to achieve more fine-grained evaluation.Previous research focused on whether LLMs can be better translation evaluators than state-of-theart models.To the best of our knowledge, only Huang et al. (2024) investigated how LLMs leverage the source and reference for quality evaluation.However, they only perform zero-shot prompting for three language pairs.Our work comprehensively examines factors such as translation errors and annotation guidelines across eight language pairs, eight prompt templates, and three different prompting techniques.</p>
<p>Data</p>
<p>We utilized the DA score prediction data released with WMT22 QE shared task (Zerva et al., 2022).This dataset includes the source (mainly from news articles), MT output (from different MT engines) and (post-edited) human references for eight language pairs, i.e., English-German (EN-DE), English-Marathi (EN-MR), English-Chinese (EN-ZH), Estonian-English (ET-EN), Nepali-English (NE-EN), Romanian-English (RO-EN), Russian-English (RU-EN) and Sinhala-English (SI-EN).For each source-MT pair, the dataset contains a DA score ranging from 0 to 100, rated by human annotators for quality assessment.</p>
<p>To include annotated errors in the MT output into our prompts, we obtained word-level QE data from WMT22, where tokens of the MT output have sequence labels with either "OK" or "BAD" indicating translation quality at word level.This dataset also involves the above 8 language pairs, which contains the source, MT output and the tags for translation quality.For each MT output, we extracted the tokens that were tagged as "BAD" as error words.</p>
<p>Since source-MT segments from sentence-level QE data might differ from those of word-level, we compared each source-MT pair in the two datasets and used the overlapping as the main resource of our research.It includes source, MT output, reference translations and error words for the 8 language pairs, covering high-, medium-and low-resource languages.We present different prompt templates in Section 4.2 to selectively include source, reference and error words to test what translation information LLMs need for quality evaluation.</p>
<p>We split the data into training, validation, and test sets in proportions of 80%, 10%, and 10% respectively.We inferenced with LLMs on the test set to obtain evaluation results.Training and validation sets were used to sample examples for few-shot learning (see Section 4.4).The size of the test set for each language pair can be seen in Table 1.</p>
<p>Methodology</p>
<p>This section presents the baselines, and our zeroshot, CoT and few-shot prompting methods.</p>
<p>Baselines</p>
<p>We utilize TransQuest and COMET, two widely used reference-less and reference-based 2 QE frameworks as baselines.For TransQuest, we employed the fine-tuned MonoTransQuest models proposed by Ranasinghe et al. (2020a) on each language pair except EN-MR.For EN-MR, we used the English to any model released with TransQuest.</p>
<p>For COMET, we utilized a fine-tuned multilingual model "Unbabel/wmt22-comet-da" (Rei et al., 2022a) for all language pairs as it does not have models for each language pair.</p>
<p>Zero-shot Prompting</p>
<p>For LLMs to predict translation quality, our prompt includes 1) instructions to perform the task such as "Score the following translation", and 2) translation information such as source or reference.Since Kocmi and Federmann (2023b) have shown that their prompt template can achieve stateof-the-art performance using GPT-4, we mainly followed their template to create our prompt instruction as shown in Figure 1.We used it as our base template and changed slightly different translation information to test what is needed for LLMs to evaluate MT quality.We constructed prompt Template 1 containing "source + MT output" as 2 COMET also supports reference-less evaluation.translation information, Template 2 "MT output + reference", Template 3 "source + MT output + reference" (exact GEMBA prompt), Template 4 "source + MT output + error words", Template 5 "source + MT output + reference + error words".</p>
<p>We augmented the base prompt with summarized guidelines used during human evaluation, as Template 6 to test if this could help LLMs evaluate MT quality.These guidelines instruct evaluators to give a DA score by considering multiple factors including accuracy, contextual understanding, grammar, syntax and overall readability.</p>
<p>CoT Prompting</p>
<p>Apart from the translation information and guidelines added in the prompt, we also tested whether CoT prompting could improve LLMs' performance by utilizing reasoning-based steps for quality evaluation.We devised Template 7 which includes two-step prompts to score MT quality, as shown in Figure 2. In the first prompt, we give translation information (including source, MT output and reference) to the LLM and ask it to analyze step by step where the machine translation is different from the reference.In the second prompt, we instruct the LLM to score the machine translation based on its previous output, i.e., the analysis of machine translation based on reference.Instruction to output a score in JSON format is given to ensure it produces the score first, like other templates.</p>
<p>Few-shot Learning</p>
<p>In addition to zero-shot and CoT prompting, we also added 5 examples based on Template 3, to show how human annotators score machine translations from 0 − 100.We split the training and validation sets into 5 buckets for each language pair according to the score ranges of 0 − 20, 21 − 40, 41 − 60, 61 − 80, 81 − 100.We randomly sampled 1 example from each range.The selected 5 examples for each language pair were given before the instruction for scoring as a prefix (see Figure 3) of the base prompt in Figure 1.We call this prompt Template 8.</p>
<p>Model Selection</p>
<p>We chose 6 models from a variety of open-source LLMs according to their size, popularity and type such as mixture of expert (MoE) (Shazeer et al., 2017) and dense models, and based on our compute capability.For 7-billion-parameter models, we selected Llama-2-7B from Meta (Touvron et al., Score the following translation from {source_lang} to {target_lang} with respect to the {source/human_reference/error_words} on a continuous scale from 0 to 100, where score of zero means "no meaning preserved" and score of one hundred means "perfect meaning and grammar".{translation_information} Score: A large language model did an evaluation of machine translation quality for the {source_language} sentence, which is given as below: {output_from_Prompt1} Based on your analysis, score the machine translation quality on a continuous scale from 0 to 100, where score of zero means "no meaning preserved" and score of one hundred means "perfect meaning and grammar".Provide the score strictly in JSON format.2023), Gemma-7B from Google (Gemma Team et al., 2024) and OpenChat3.5 which was trained with mixed-quality data using Conditional Reinforcement Learning from Human Feedback (Wang et al., 2024).For 13-billion-parameter models, we opted for Llama-2-13B and Qwen1.5-14Bwhich was specifically tested on a diverse set of 12 languages and showed impressive multilingual capabilities (Bai et al., 2023).We also included the Mixtral-8x7B model (Jiang et al., 2024) as our MoE model, but due to the limit of our compute capability, we used the activation-aware weight quantized (AWQ) version (Lin et al., 2024).For all 6 selected models, we used the instruction-tuned version, i.e., the chat model, for zero-shot, CoT and few-shot inference.Additionally, we experimented with TowerLLM (Alves et al., 2024) for EN-ZH via HuggingFace3 , but results are not discussed in the paper because the model output is mostly identical to the input for most instances (see Appendix A).</p>
<p>Experimental Setup</p>
<p>All our experiments were run using 1 × NVIDIA A100 40G, 1 × A40, and 2 × RTX A5000 GPUs, for different LLM variants.We used vLLM (Kwon et al., 2023) to save inference time.Detailed settings for hyperparameters, formatting and evaluation metrics are provided below.</p>
<p>Hyperparameters We chose the default hyperparameter settings in vLLM for all our experiments, i.e., 0.8 as temperature4 , 0.95 for top_p.The input sequence length was chosen as 1024 for zero-shot and CoT inference and 3000 for few-shot inference.</p>
<p>Formatting As chat models were fine-tuned on certain formats to interact with humans, it is suggested to use the specific format that was used to train the model while inferencing.As vLLM does not support formatting natively, we formatted all our prompt templates before they were fed into the models based on the format of each model in Section 4.5.</p>
<p>Evaluation Since LLMs usually output a score and some explanations about their evaluation, we used regular expression to extract the score from the LLM output.Spearman correlation5 (Spearman, You are going to evaluate the quality of machine translation given the source, machine translation and reference translation.1904) was used to evaluate how the predicted scores are correlated with the (mean of) human annotated scores.However, not all LLMs would output a score for all the instances.Sometimes, LLMs failed to score the input translation.In such cases, we dropped these instances (denoted as D in Table 3) during the process of correlation calculation, but they were noted as a metric for robustness.</p>
<p>Results and Discussion</p>
<p>This section displays our baseline and prompting results using existing QE frameworks and LLMs.</p>
<p>Baselines</p>
<p>Table 2 shows our baseline results using Tran-sQuest and COMET.Since TransQuest models were fine-tuned on data from each language pair with the exception of EN-MR, the Spearman correlation scores of these reference-less models, are higher than those of reference-based COMET models.Except EN-DE and EN-MR, the correlation scores for most language pairs are relatively high.</p>
<p>Zero-shot Inference</p>
<p>Table 3 shows our zero-shot results of Templates 1 to 6 for open-source LLMs including OpenChat3.5,Llama-2-7B, Gemma7B, Llama-2-13B, Qwen1.5-14B and Mixtral-8x7B-AWQ.</p>
<p>Comparing results among LLMs We observe that OpenChat3.5 achieved the highest Spearman correlation scores for most language pairs, despite having only 7 billion parameters -roughly half the size of Llama-2-13B and Qwen1.5-14B.It excelled not only in Spearman scores but also in consistently providing valid quality evaluation scores, with very few dropped rows.Among the 6 models, Llama-2 (both the 7 and 13 billion variants) performed poorly in generating evaluations with valid scores.Many rows were dropped, and the Spearman scores were low, indicating a weak correlation with the true scores.The MoE model, Mixtral-8x7B-AWQ, did not outperform OpenChat3.5 on most language pairs and prompt templates for our task.</p>
<p>Comparing with the baselines We find that models fine-tuned for each language pair by Tran-sQuest, performed much better than the zero-shot prompting results for all language pairs.For some language pairs like ET-EN, NE-EN and RO-EN, our best zero-shot prompting results (Template 6 of OpenChat3.5 in Table 3) were comparable to the reference-based models fine-tuned on multilingual data using COMET.For some other pairs like SI-EN, our best zero-shot prompting results were even slightly better than the COMET models.</p>
<p>Comparison among Templates When we fix the model variable as OpenChat3.5,we can compare the performance of different prompt templates.</p>
<p>Looking at the OpenChat3.5results in Table 3, we observe that LLM performance is generally better when the source and reference are included in the prompt, as in Templates 3, 5, and 6, compared to prompts without them, such as Templates 1 and 2. This pattern holds true for other LLMs such as the LLama-2 models and Gemma, as shown in the table.Notably, the Spearman scores are obviously higher when the source is incorporated into the prompt, as seen by comparing Templates 2 and 3.This suggests that the source is an essential component for evaluating MT quality using LLMs, contrary to the results in Huang et al. (2024)   ity across different language pairs when compared to using just the plain GEMBA prompt (Template 3).For most language pairs like EN-ZH, ET-EN, RU-EN, and SI-EN, Template 3 had the highest correlation with human judgments.However, removing reference translations (Template 4) clearly lowered correlation scores, highlighting their importance for accurate MT evaluation.</p>
<p>Although incorporating error words does not seem to improve performance, they are surprisingly useful in helping LLMs provide scores in their outputs.As shown in Table 3, there are fewer dropped rows when using Templates 4 and 5, which include error words.Outputs from Templates 4 and 5 are the most stable across models, unlike other templates that are more model-dependent.</p>
<p>Results among different language pairs For high-resource language pairs like EN-DE and EN-ZH, correlation scores tend to be lower than those of medium-and low-resource pairs such as NE-EN, RO-EN, and RU-EN.This pattern holds true across most models, including the fine-tuned ones from TransQuest and COMET.</p>
<p>To further investigate the reasons, we selected EN-DE and EN-ZH as high-resource language pairs, and RO-EN and SI-EN as medium-and lowresource language pairs.We plotted the distributions of the predicted (from OpenChat3.5)vs true scores (mean of all annotators) as shown in Figures 4 and 5.For high-resource language pairs, the predictions are skewed towards higher DA scores.Well-trained MT systems, due to abundant resources, tend to produce high-quality translations, leading to higher DA scores.However, LLM-based evaluation systems may amplify these imbalanced distributions and are more likely to predict scores within the high range.</p>
<p>In contrast, for medium-and low-resource language pairs, there are fewer resources for training MT systems.As a result, low-quality translations (with low DA scores) are better represented than in high-resource pairs.Quality evaluation systems can better recognize low-quality translations and produce a more balanced score distribution.This imbalance in the score representation could be the reason why predicted DA scores for high-resource languages are less correlated with true scores than for medium-and low-resource pairs.</p>
<p>CoT and Few-shot Inference</p>
<p>Tables 4 and 5 show results of CoT (Template 7) and 5-shot inference (Template 8) together with the results of Template 3 for the 6 selected LLMs.Dropped rows for the two templates are presented in Table 6.Both Templates 7 and 8 were built upon Template 3, i.e., including the source, MT output and reference.We expect the model performance to be improved when more reasoning steps or evaluation examples were given.However, for 7 billion parameter variants, CoT prompting resulted in worse performance, as Spearman correlation scores of Template 7 were obviously lower than those of Template 3.For the larger 13 billion parameter variants, results were mixed for different language pairs.For language pairs such as EN-DE and EN-MR, CoT prompting improved the performance in the prediction of DA scores.This indicates that CoT may work better on larger models than smaller models.While CoT prompting did not consistently improve model performance as measured by the Spearman correlation scores, it shows relatively more consistent output than other prompt templates.Table 6 suggests that fewer rows were dropped when using Template 7, especially for Llama-2 models.Interestingly, 5-shot inference results are not better than zero-shot results, posing a question on context utilization by LLMs.Performance varies on the LLMs and the specific language pairs.This could relate to the language data available for training these LLMs, as well as the quality of the evaluation examples chosen for different languages pairs.</p>
<p>Discussion</p>
<p>Based on our results, Template 3, which includes the source, MT output and reference, but excludes error words and detailed guidelines, performed the best in terms of Spearman correlation scores.Prompting with CoT and few-shot learning may yield better results for larger models, but more experiments are needed to confirm this.</p>
<p>While larger language models often perform better, our results show that a 7-billion parameter model outperformed other models for most language pairs.Surprisingly, even much smaller COMET models fine-tuned on multilingual data, rather than data for specific language pairs, usually outperformed our LLM prompting results.However, due to the high computational cost, we could not test models with 70 billion or more parameters.</p>
<p>Different models excel at various language pairs Table 6: Dropped rows for Template 7 (T7) and Template 8 (T8), i.e., the CoT and few-shot prompt templates, using various LLMs for each language pair (LP).</p>
<p>while struggling with others.Even for a single model, performance fluctuates across different language pairs.This variability could stem from whether a language is considered high-resource, but further research is necessary to understand the underlying causes.</p>
<p>Our experiments with prompting LLMs for translation evaluation reveal that these models are often inconsistent in generating numerical scores.In most cases, LLMs tend to generate scores accompanied by lengthy and unstructured explanations.While using regular expressions for extraction can be helpful, it is not always reliable.For models like Llama-2, we observed numerous instances where LLMs failed to produce a valid score.Our empirical findings demonstrate that employing CoT prompting or incorporating error words into the prompt can enhance the consistency of the model outputs.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we explored what translation information is needed for LLMs to evaluate MT quality.We conducted a comprehensive investigation into different prompting techniques such as zeroshot, CoT and few-shot prompting using different translation information for 8 language pairs and 6 LLMs of different sizes and types.Our findings suggest that the source, MT output and reference are essential compared to other information such as translation errors for quality evaluation.Larger models may not necessarily perform better than smaller models, but CoT prompting works better on larger than smaller model variants.We also observe that LLMs do not always provide a numerical score when generating evaluations, which makes their assessments less reliable.For future research, we plan to explore whether fine-tuning LLMs could improve their performance in quality evaluation.We also plan to thoroughly investigate error explainability of LLMs using MQM and other fine-grained error identification techniques.These future studies can inform downstream error correction through automatic post-editing, contributing to a more comprehensive evaluation and correction framework.</p>
<p>Limitations and Ethical Considerations</p>
<p>Our results were achieved on a limited number of LLMs which are mostly smaller than 14 billion parameters due to the constraints of our computational capabilities.Larger models may perform differently in this translation evaluation task.The examples used in the few-shot scenario were randomly sampled since we do not have the knowledge to prepare good-quality examples for all language pairs.Results might be different if these examples were carefully chosen by native speakers.</p>
<p>Our experiments in the paper were conducted solely on publicly available datasets as described in Section 3, requiring no ethical approval.</p>
<p>Figure 1 :
1
Figure 1: Base Prompt Template Prompt 1: You are going to evaluate the quality for {language_pair} translation.You need to think step by step.First read the following source, machine translation and reference translation.Analyze where the machine translation is different from the reference translation.Source: {source_sentence} Machine translation: {target_sentence} Reference translation: {reference_translation} Prompt 2:A large language model did an evaluation of machine translation quality for the {source_language} sentence, which is given as below: {output_from_Prompt1} Based on your analysis, score the machine translation quality on a continuous scale from 0 to 100, where score of zero means "no meaning preserved" and score of one hundred means "perfect meaning and grammar".Provide the score strictly in JSON format.</p>
<p>Figure 2 :
2
Figure 2: Prompt Template 7</p>
<p>Figure 4 :
4
Figure 4: Density plots of the predicted (red) and true (mean) DA scores (blue) for high-resource language pairs i.e., EN-DE (left) and EN-ZH (right).</p>
<p>Figure 5 :
5
Figure 5: Density plots of the predicted (red) and true (mean) DA scores (blue) for medium-and low-resource language pairs i.e., RO-EN (left) and SI-EN (right).</p>
<p>Table 2 :
2
The followings are examples of scoring translation quality.{5_examples}Figure3:Prefix for the base prompt to create Template 8 Spearman ρ achieved by models using Tran-sQuest and COMET on each language pair (LP).
LPTransQuest COMETEN-DE0.38110.3579EN-MR0.24890.5135EN-ZH0.63600.5410ET-EN0.81480.7018NE-EN0.80340.6393RO-EN0.87390.7699RU-EN0.82520.6482SI-EN0.72330.5874</p>
<p>Table 3 :
3
Spearman ρ correlation scores achieved by zero-shot inference using Templates 1-6 (T1-6) on various open-source LLMs for each language pair (LP).D -&gt; rows dropped as LLM generated output without a score.The underlined scores represent the best result among templates, while the bold represent the best among LLMs.
, who</p>
<p>Table 4 :
4
Spearman ρ correlation scores achieved using Template 7 (T7), the CoT prompt template, on various LLMs for each language pair (LP).Results of Template 3 (T3) from Table3are listed here for reference.The underlined scores represent better result between the templates, while the bold represent the best among LLMs.
LPOpenChat3.5 T7 T3Llama-2-7B T7 T3Gemma-7B T7 T3Llama-2-13B T7 T3Qwen1.5-14B T7 T3Mixtral-8x7B-AWQ T7 T3EN-DE 0.2433 0.2849 -0.0353 0.0876 0.0048 0.1624 0.0345 0.0316 0.2388 0.2182 0.22130.2631EN-MR 0.2937 0.3546 -0.0021 0.1255 0.0859 0.1479 0.0804 0.0685 0.3455 0.3131 0.19060.1825EN-ZH 0.3324 0.3995 0.0354 0.0946 0.1609 0.1805 0.0703 0.1412 0.3429 0.4131 0.24790.3720ET-EN 0.6110 0.6980 0.1459 0.3715 0.3191 0.3772 0.2558 0.4042 0.5845 0.6467 0.46280.6229NE-EN 0.5160 0.5937 0.1363 0.2207 0.3221 0.2921 0.3315 0.3178 0.4791 0.5114 0.43730.4891RO-EN 0.7175 0.7294 0.1859 0.4514 0.4550 0.4429 0.3403 0.4616 0.7019 0.7200 0.63600.6526RU-EN 0.5317 0.6066 0.1618 0.4253 0.2979 0.4399 0.2519 0.4074 0.5203 0.5597 0.51910.5831SI-EN 0.5124 0.6034 0.1818 0.2212 0.2808 0.3519 0.2854 0.2669 0.4680 0.5936 0.46910.4563LPOpenChat3.5 T8 T3Llama-2-7B T8 T3Gemma-7B T8 T3Llama-2-13B T8 T3Qwen1.5-14B T8 T3Mixtral-8x7B-AWQ T8 T3EN-DE 0.1756 0.2849 0.0327 0.0876 0.0343 0.1624 0.0655 0.0316 0.1804 0.2182 0.21550.2631EN-MR 0.2543 0.3546 0.0078 0.1255 0.1651 0.1479 0.0159 0.068 0.2706 0.3131 0.24100.1825EN-ZH 0.2801 0.3995 0.0283 0.0946 0.1831 0.1805 0.0875 0.1412 0.2946 0.4131 0.29700.3720ET-EN 0.5779 0.6980 -0.0026 0.3715 0.4134 0.3772 0.2328 0.4042 0.4320 0.6467 0.55660.6229NE-EN 0.4621 0.5937 0.1428 0.2207 0.3117 0.2921 0.1907 0.3178 0.3349 0.5114 0.51430.4891RO-EN 0.6881 0.7294 0.0405 0.4514 0.4693 0.4429 0.2574 0.4616 0.4498 0.7200 0.67120.6526RU-EN 0.5774 0.6066 0.1680 0.4253 0.2531 0.4399 0.1951 0.4074 0.4798 0.5597 0.52390.5831SI-EN 0.4277 0.6034 0.0352 0.2212 0.3048 0.3519 0.1368 0.2669 0.4207 0.5936 0.46420.4563</p>
<p>Table 5 :
5
Spearman ρ correlation scores achieved using Template 8 (T8), the few-shot prompt template, on various LLMs for each language pair (LP).Results of Template 3 (T3) from Table3are listed here for reference.The underlined scores represent better result between the templates, while the bold represent the best among LLMs.</p>
<p>https://huggingface.co/Unbabel/ TowerInstruct-7B-v0.1
We also experimented with 0 temperature, but we did not observe huge differences in terms of score distribution.
Pearson's r and Kendall's τ are in Appendix A.
AcknowledgementsWe would like to thank the European Association for Machine Translation (EAMT) for funding QE data curation of Indic languages used in this paper (UoS/RN0580).Model input (before formatting): Score the following translation from English to Chinese with respect to the human reference on a continuous scale from 0 to 100, where score of zero means "no meaning preserved" and score of one hundred means "perfect meaning and grammar".\nEnglishsource: The last conquistador then rides on with his sword drawn.\nChinesehuman reference: 最后的征服者随后举着剑前 进。\nChinese translation: 最后的征服者骑着他的剑继续前进.\nScore:Model output: &lt;|im_start|&gt;user\nScore the following translation from English to Chinese with respect to the human reference on a continuous scale from 0 to 100, where score of zero means "no meaning preserved" and score of one hundred means "perfect meaning and grammar".\nEnglish source: The last conquistador then rides on with his sword drawn.\nChinesehuman reference: 最后的征服者随 后举着剑前进。\nChinese translation: 最后的征服者骑着他的剑继续前 进.\nScore:&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n最后的征服者骑着他的剑继 续前进.
Tower: An Open Multilingual Large Language Model for Translation-Related Tasks. Miguel Duarte, José Alves, Pombal, M Nuno, Pedro Henrique Guerreiro, João Martins, Amin Alves, Ben Farajian, Ricardo Peters, Patrick Rei, Sweta Fernandes, Pierre Agrawal, Colombo, G C José, Andre De Souza, Martins, First Conference on Language Modeling. 2024</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei</p>
<p>. Binyuan Huang, Luo Hui, Mei Ji, Junyang Li, Runji Lin, Dayiheng Lin, Gao Liu, Chengqiang Liu, Keming Lu, Jianxin Lu, Rui Ma, Xingzhang Men, Xuancheng Ren, Chuanqi Ren, Sinan Tan, Jianhong Tan, Peng Tu, Shijie Wang, Wei Wang, Shengguang Wang, Benfeng Wu, Jin Xu, An Xu, Hao Yang, Jian Yang, Shusheng Yang, Yang Yang, Bowen Yao, Hongyi Yu, Zheng Yuan, Jianwei Yuan, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhang, Jingren Zhou, Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical Report</p>
<p>Findings of the WMT 2023 shared task on quality estimation. Frederic Blain, Chrysoula Zerva, Ricardo Rei, M Nuno, Diptesh Guerreiro, Kanojia, G C José, Beatriz De Souza, Tânia Silva, Yan Vaz, Fatemeh Jingxuan, Constantin Azadi, André Orasan, Martins, 10.18653/v1/2023.wmt-1.52Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingaporeAssociation for Computational Linguistics2023</p>
<p>Unsupervised cross-lingual representation learning at scale. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, 10.18653/v1/2020.acl-main.747Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Pushpak Bhattacharyya, and Constantin Orȃsan. 2023a. A multitask learning framework for quality estimation. Sourabh Deoghare, Paramveer Choudhary, Diptesh Kanojia, Tharindu Ranasinghe, 10.18653/v1/2023.findings-acl.585Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics</p>
<p>Quality estimation-assisted automatic postediting. Sourabh Deoghare, Diptesh Kanojia, Fred Blain, Tharindu Ranasinghe, Pushpak Bhattacharyya, 10.18653/v1/2023.findings-emnlp.115Findings of the Association for Computational Linguistics: EMNLP 2023. Singapore2023bAssociation for Computational Linguistics</p>
<p>The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, Orhan Firat, 10.18653/v1/2023.wmt-1.100Proceedings of the Eighth Conference on Machine Translation. the Eighth Conference on Machine TranslationSingapore2023Association for Computational Linguistics</p>
<p>. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Giuseppe Pier, Aakanksha Sessa, Adam Chowdhery, Aditya Roberts, Alex Barua, Alex Botev, Ambrose Castro-Ros, Amélie Slone, Andrea Héliou, Anna Tacchetti, Antonia Bulanova, Beth Paterson, Bobak Tsai, Charline Le Shahriari, Christopher A Lan, Clément Choquette-Choo, Daniel Crepy, Daphne Cer, David Ippolito, Elena Reid, Eric Buchatskaya, Eric Ni, Geng Noland, George Yan, George-Christian Tucker, Grigory Muraru, Henryk Rozhdestvenskiy, Ian Michalewski, Ivan Tenney, Jacob Grishchenko, James Austin, Jane Keeling, Jean-Baptiste Labanowski, Jeff Lespiau, Jenny Stanway, Jeremy Brennan, Johan Chen, Justin Ferret, Justin Chiu, Katherine Mao-Jones, Kathy Lee, Katie Yu, Lars Lowe Millican, Lisa Sjoesund, Lucas Lee, Machel Dixon, Maciej Reid, Mateo Mikuła, Michael Wirth, Nikolai Sharman, Nithum Chinaev, Olivier Thain, Oscar Bachem, Oscar Chang, Paige Wahltinez, Paul Bailey, Petko Michel, Rahma Yotov, Ramona Chaabouni, Reena Comanescu, Rohan Jana, Ross Anil, Ruibo Mcilroy, Ryan Liu, Mullins, L Samuel, Sebastian Smith, Sertan Borgeaud, Sholto Girgin, Shree Douglas, Siamak Pandya, Soham Shakeri, Ted De, Tom Klimenko, Vlad Hennigan, Wojciech Feinberg, Yu Stokowiec, Zafarali Hui Chen, Zhitao Ahmed, Tris Gong, Ludovic Warkentin, Minh Peran, Clément Giang, Oriol Farabet, Jeff Vinyals, Koray Dean, Demis Kavukcuoglu, Zoubin Hassabis, Douglas Ghahramani, Joelle Eck, Fernando Barral, Pereira, arXiv:2403.08295Alek Andreev. Technology. Preprintand Kathleen Kenealy. 2024. Gemma: Open Models Based on Gemini Research and</p>
<p>Continuous measurement scales in human evaluation of machine translation. Yvette Graham, Timothy Baldwin, Alistair Moffat, Justin Zobel, Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse. the 7th Linguistic Annotation Workshop and Interoperability with DiscourseSofia, Bulgaria2013Association for Computational Linguistics</p>
<p>Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation. Xu Huang, Zhirui Zhang, Xiang Geng, Yichao Du, Jiajun Chen, Shujian Huang, 10.18653/v1/2024.findings-acl.211Findings of the Association for Computational Linguistics ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024and virtual meeting</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Renard Lélio, Lucile Lavaud, Marie-Anne Saulnier, Pierre Lachaux, Sandeep Stock, Sophia Subramanian, Yang, arXiv:2401.04088Mixtral of Experts. Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed, 2024Preprint</p>
<p>Pushing the right buttons: Adversarial evaluation of quality estimation. Diptesh Kanojia, Marina Fomicheva, Tharindu Ranasinghe, Frédéric Blain, Constantin Orȃsan, Lucia Specia, 10.18653/v1/2023.wmt-1.64Association for Computational Linguistics. Tom Kocmi and Christian Federmann. 2023a. SingaporeAssociation for Computational Linguistics2021Proceedings of the Eighth Conference on Machine Translation</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine Translation2023b</p>
<p>ChatGPT: Jack of all trades, master of none. Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, Anna Kocoń, Bartłomiej Koptyra, Wiktoria Mieleszczenko-Kowszewicz, Piotr Miłkowski, Marcin Oleksy, Maciej Piasecki, Łukasz Radliński, 10.1016/j.inffus.2023.101861Information Fusion. Konrad Wojtasik, Stanisław Woźniak, and Przemysław Kazienko991018612023</p>
<p>Efficient Memory Management for Large Language Model Serving with PagedAttention. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, Ion Stoica, 10.1145/3600006.3613165Proceedings of the 29th Symposium on Operating Systems Principles, SOSP '23. the 29th Symposium on Operating Systems Principles, SOSP '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, Song Han, Proceedings of Machine Learning and Systems. Machine Learning and Systems20246</p>
<p>Arle Richard Lommel, Aljoscha Burchardt, Hans Uszkoreit, 10.5565/rev/tradumatica.77Multidimensional Quality Metrics: A Flexible System for Assessing Translation Quality. Tradumàtica: tecnologies de la traducció. 20140</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>TransQuest at WMT2020: Sentencelevel direct assessment. Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov, Proceedings of the Fifth Conference on Machine Translation. the Fifth Conference on Machine TranslationOnline. Association for Computational Linguistics2020a</p>
<p>TransQuest: Translation quality estimation with cross-lingual transformers. Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov, 10.18653/v1/2020.coling-main.445Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020bInternational Committee on Computational Linguistics</p>
<p>An exploratory analysis of multilingual word-level quality estimation with cross-lingual transformers. Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov, 10.18653/v1/2021.acl-short.55Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20212Short Papers)</p>
<p>COMET-22: Unbabel-IST 2022 submission for the metrics shared task. Ricardo Rei, G C José, Duarte Souza, Chrysoula Alves, Ana C Zerva, Taisiya Farinha, Alon Glushkova, Luisa Lavie, Coheur, F T André, Martins, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022a</p>
<p>COMET: A neural framework for MT evaluation. Ricardo Rei, Craig Stewart, Ana C Farinha, Alon Lavie, 10.18653/v1/2020.emnlp-main.213Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. Ricardo Rei, Marcos Treviso, M Nuno, Chrysoula Guerreiro, Ana C Zerva, Christine Farinha, Maroti, G C José, Taisiya De Souza, Duarte Glushkova, Luisa Alves, Alon Coheur, Lavie, F T André, Martins, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022b</p>
<p>BLEURT: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur Parikh, 10.18653/v1/2020.acl-main.704Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. Noam Shazeer, Azalia Mirhoseini, * , Krzysztof Maziarz, * , Andy Davis, Quoc Le, Geoffrey Hinton, Jeff Dean, International Conference on Learning Representations. 2017</p>
<p>A study of translation edit rate with targeted human annotation. Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, John Makhoul, Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers. the 7th Conference of the Association for Machine Translation in the Americas: Technical PapersCambridge, Massachusetts, USA2006Association for Machine Translation in the Americas</p>
<p>The Proof and Measurement of Association between Two Things. Charles Spearman, The American Journal of Psychology. 151904</p>
<p>Findings of the WMT 2020 shared task on quality estimation. Lucia Specia, Frédéric Blain, Marina Fomicheva, Erick Fonseca, Vishrav Chaudhary, Francisco Guzmán, F T André, Martins, Proceedings of the Fifth Conference on Machine Translation. the Fifth Conference on Machine TranslationOnline. Association for Computational Linguistics2020</p>
<p>Findings of the WMT 2021 shared task on quality estimation. Lucia Specia, Frédéric Blain, Marina Fomicheva, Chrysoula Zerva, Zhenhao Li, Vishrav Chaudhary, F T André, Martins, Proceedings of the Sixth Conference on Machine Translation. the Sixth Conference on Machine TranslationOnline. Association for Computational Linguistics2021</p>
<p>Quality Estimation for Machine Translation. Lucia Specia, Carolina Scarton, Gustavo Henrique Paetzold, 10.1007/978-3-031-02168-8_12018Spinger, Cham, Germany</p>
<p>COMET -deploying a new state-ofthe-art MT evaluation metric in production. Craig Stewart, Ricardo Rei, Catarina Farinha, Alon Lavie, Proceedings of the 14th Conference of the Association for Machine Translation in the Americas. the 14th Conference of the Association for Machine Translation in the AmericasUser Track20202Association for Machine Translation in the Americas</p>
<p>Can ChatGPT Replace Traditional KBQA Models? An In-Depth Analysis of the Question Answering Performance of the GPT LLM Family. Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, Guilin Qi, The Semantic Web -ISWC 2023. ChamSpringer Nature Switzerland2023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov ; Zheng Yan, Iliyan Zarov, Yuchen Zhang, arXiv:2307.09288Pushkar Mishra, Igor Molybog. Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien RodriguezPreprintand Thomas Scialom. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models</p>
<p>UniTE: Unified translation evaluation. Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang, Boxing Chen, Derek Wong, Lidia Chao, 10.18653/v1/2022.acl-long.558Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221Long Papers)</p>
<p>OpenChat: Advancing Open-source Language Models with Mixed-Quality Data. Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, Yang Liu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. the 36th International Conference on Neural Information Processing Systems, NIPS '22Red Hook, NY, USACurran Associates Inc2024</p>
<p>Findings of the WMT 2022 shared task on quality estimation. Chrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, G C José, Steffen Souza, Diptesh Eger, Duarte Kanojia, Constantin Alves, Marina Orȃsan, Fomicheva, F T André, Lucia Martins, Specia, Proceedings of the Seventh Conference on Machine Translation (WMT). the Seventh Conference on Machine Translation (WMT)Abu Dhabi, United Arab Emirates2022Association for Computational Linguistics</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>Benchmarking Large Language Models for News Summarization. Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen Mckeown, Tatsunori B Hashimoto, 10.1162/tacl_a_00632Transactions of the Association for Computational Linguistics. 122024</p>
<p>Large Language Models for Information Retrieval: A Survey. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zhicheng Dou, Ji-Rong Wen, arXiv:2308.071072023Preprint</p>
<p>. Openchat3, </p>
<p>Pearson's r and Kendall's τ correlation scores achieved using Templates 1-8 (T1-8) on various open-source LLMs for each language pair. LP7</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1575 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1575</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1575</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-30.html">extraction-schema-30</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-f3caa43a7016fbbf309d45112b31b20230eaf8da</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f3caa43a7016fbbf309d45112b31b20230eaf8da" target="_blank">Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A deep reinforcement learning architecture that represents the game state as a knowledge graph which is learned during exploration that is used to prune the action space, enabling more efficient exploration.</p>
                <p><strong>Paper Abstract:</strong> Text-based adventure games provide a platform on which to explore reinforcement learning in the context of a combinatorial action space, such as natural language. We present a deep reinforcement learning architecture that represents the game state as a knowledge graph which is learned during exploration. This graph is used to prune the action space, enabling more efficient exploration. The question of which action to take can be reduced to a question-answering task, a form of transfer learning that pre-trains certain parts of our architecture. In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives. We have also open-sourced our code at https://github.com/rajammanabrolu/KG-DQN.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1575.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1575.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Deep Q-Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep reinforcement learning agent that constructs and maintains a knowledge graph (RDF triples) from textual observations, uses graph attention embeddings to represent state, prunes the large natural-language action space using the graph, and estimates Q-values via dot-product between state and action encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses OpenIE + rule-based updates to build a persistent knowledge graph; embeds the graph with multi-head graph-attention; encodes observations with a Sliding Bi-LSTM; encodes candidate textual actions with an LSTM; prunes actions via simple graph-based scoring (presence and path checks) to top-k; Q(s,a) is the dot product of state and action vectors. Trained with prioritized experience replay and an epsilon1,epsilon2-greedy exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (home theme)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A grammar-driven text-adventure generator producing interactive 'home' worlds; agent receives textual observations (descriptions of rooms, objects, inventory), composes textual commands (parser-based action set), and receives textual feedback and scalar rewards. TextWorld can provide admissible actions but the agent is not allowed to use that API in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household tasks / quest procedures</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Navigate rooms (go north/east/etc.), pick up/drop/use objects, inspect inventory, insert key into lock, open/close objects — sequences of object interactions and movement to complete a quest.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Compositional: quests are multi-step procedures composed of primitive actions (movement, object manipulation, inspection); quests require combining verbs with one or two object arguments in specific sequences. Game generator produces varying room layouts and object placements so procedures are compositions of subtasks (explore, find object, use object).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Two major settings: 'small' games with 10 rooms, 20 objects, quest length 5 (branching factor ~143); 'large' games with 20 rooms, 40 objects, quest length 10 (branching factor ~562).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Not framed as curriculum but as transfer: weights (SB-LSTM and embeddings) initialized from a DrQA QA model pre-trained on oracle walkthrough traces from many generated games. This pre-training transferred to unseen test games and improved sample efficiency and solution quality when combined with pruning (see key findings and numbers below).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Incorporating a persistent knowledge graph yields markedly faster convergence to maximum reward (~40% faster on small games) compared to baselines. 2) Action pruning using the knowledge graph reduces action-space exploration and improves convergence. 3) Pre-training via a QA paradigm (DrQA) transfers useful embeddings/encoders and, when combined with pruning (Full KG-DQN), produces the best tradeoff of fast convergence and short action sequences to solve quests. 4) Ablations show pre-training alone without pruning (Unpruned, pre-trained) led to worse final step-efficiency than the full model; pruning without pre-training gave intermediate results. 5) On the small game Full KG-DQN matched LSTM-DQN in steps-to-completion (not statistically different); on large games KG-DQN variants converge far faster than LSTM-DQN (LSTM-DQN required >300 episodes to reach KG-DQN levels).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1575.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1575.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of curriculum learning approaches for teaching agents commonsense or science procedures in interactive text environments, including details about the curriculum strategy, task composition, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QA pre-training (DrQA traces)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Offline question-answering pre-training using DrQA on oracle walkthrough traces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline transfer-learning procedure that frames action selection as question answering: generate oracle (optimal) walkthrough traces from many generated games, train a DrQA QA model to map observation text (question/context) to the optimal action (answer), and use the DrQA document-encoder SB-LSTM weights and embedding initializations to initialize parts of the KG-DQN network.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reading Wikipedia to answer opendomain questions.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DrQA-based pre-training (used to initialize KG-DQN components)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Not an agent for execution, but a QA model (DrQA) trained on (observation -> oracle action) pairs; the encoded SB-LSTM weights and embedding layer (initialized from GloVe) are transferred to KG-DQN's SB-LSTM and embedding layers to provide prior knowledge for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>TextWorld (home theme) used to generate pre-training data</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same procedural TextWorld generator: many different but same-domain games (same room/item types, different spatial configurations and quests). Oracle generates optimal minimal-step walkthroughs for each generated game to create (observation, correct action) pairs for QA training.</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_type</strong></td>
                            <td>household tasks / quest procedures (used as QA training targets)</td>
                        </tr>
                        <tr>
                            <td><strong>procedure_examples</strong></td>
                            <td>Oracle actions in traces correspond to the exact commands needed at each observation to optimally advance the quest (e.g., 'go east', 'take key', 'insert key into lock', 'open chest').</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_structure</strong></td>
                            <td>Training traces span many different compositional sequences (quests) so the QA model learns mappings from diverse composed observation contexts to the immediate next action; the generator varies room layouts and object placements to avoid memorization and encourage general QA competence.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_curriculum</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_ordering_principle</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_complexity_range</strong></td>
                            <td>Pre-training dataset drawn from generated games matching the small/large regimes: small (quest length 5) and large (quest length 10); 200 generated games used per regime, with 160 for training, 20 dev, 20 test during QA pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_curriculum</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_curriculum_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>alternative_curriculum_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization</strong></td>
                            <td>Pre-training generalized to unseen test games in the same domain: QA metrics (Table 2) — Small: Exact Match 46.20%, Precision 56.57%, Recall 63.38%, F1 57.94%; Large: Exact Match 34.13%, Precision 52.53%, Recall 64.72%, F1 55.06%. When those pre-trained weights were used to initialize KG-DQN, the full model (pruning + pre-training) achieved substantially fewer steps-to-completion than some ablations and converged quickly. This indicates effective transfer of action-selection priors across different generated games.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Framing 'what action to take' as QA and pre-training on oracle traces transfers useful representations to RL agents. 2) QA pre-training produced moderate token-level QA accuracy (EM 34–46%) but still yielded helpful inductive biases when transferred. 3) Pre-training alone (without action pruning) did not produce the best final step efficiency — combining pre-training with knowledge-graph-based pruning yielded the best results (Full KG-DQN). 4) The pre-training dataset was generated from many related but distinct games to force learning general QA competence rather than memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning', 'publication_date_yy_mm': '2018-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reading Wikipedia to answer opendomain questions. <em>(Rating: 2)</em></li>
                <li>The Natural Language Decathlon : Multitask Learning as Question Answering <em>(Rating: 2)</em></li>
                <li>TextWorld : A Learning Environment for Text-based Games <em>(Rating: 2)</em></li>
                <li>Language Understanding for Textbased Games Using Deep Reinforcement Learning <em>(Rating: 1)</em></li>
                <li>Learning How Not to Act in Text-Based Games <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1575",
    "paper_id": "paper-f3caa43a7016fbbf309d45112b31b20230eaf8da",
    "extraction_schema_id": "extraction-schema-30",
    "extracted_data": [
        {
            "name_short": "KG-DQN",
            "name_full": "Knowledge Graph Deep Q-Network",
            "brief_description": "A deep reinforcement learning agent that constructs and maintains a knowledge graph (RDF triples) from textual observations, uses graph attention embeddings to represent state, prunes the large natural-language action space using the graph, and estimates Q-values via dot-product between state and action encodings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "KG-DQN",
            "agent_description": "Uses OpenIE + rule-based updates to build a persistent knowledge graph; embeds the graph with multi-head graph-attention; encodes observations with a Sliding Bi-LSTM; encodes candidate textual actions with an LSTM; prunes actions via simple graph-based scoring (presence and path checks) to top-k; Q(s,a) is the dot product of state and action vectors. Trained with prioritized experience replay and an epsilon1,epsilon2-greedy exploration.",
            "agent_size": null,
            "environment_name": "TextWorld (home theme)",
            "environment_description": "A grammar-driven text-adventure generator producing interactive 'home' worlds; agent receives textual observations (descriptions of rooms, objects, inventory), composes textual commands (parser-based action set), and receives textual feedback and scalar rewards. TextWorld can provide admissible actions but the agent is not allowed to use that API in these experiments.",
            "procedure_type": "household tasks / quest procedures",
            "procedure_examples": "Navigate rooms (go north/east/etc.), pick up/drop/use objects, inspect inventory, insert key into lock, open/close objects — sequences of object interactions and movement to complete a quest.",
            "compositional_structure": "Compositional: quests are multi-step procedures composed of primitive actions (movement, object manipulation, inspection); quests require combining verbs with one or two object arguments in specific sequences. Game generator produces varying room layouts and object placements so procedures are compositions of subtasks (explore, find object, use object).",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Two major settings: 'small' games with 10 rooms, 20 objects, quest length 5 (branching factor ~143); 'large' games with 20 rooms, 40 objects, quest length 10 (branching factor ~562).",
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "Not framed as curriculum but as transfer: weights (SB-LSTM and embeddings) initialized from a DrQA QA model pre-trained on oracle walkthrough traces from many generated games. This pre-training transferred to unseen test games and improved sample efficiency and solution quality when combined with pruning (see key findings and numbers below).",
            "key_findings": "1) Incorporating a persistent knowledge graph yields markedly faster convergence to maximum reward (~40% faster on small games) compared to baselines. 2) Action pruning using the knowledge graph reduces action-space exploration and improves convergence. 3) Pre-training via a QA paradigm (DrQA) transfers useful embeddings/encoders and, when combined with pruning (Full KG-DQN), produces the best tradeoff of fast convergence and short action sequences to solve quests. 4) Ablations show pre-training alone without pruning (Unpruned, pre-trained) led to worse final step-efficiency than the full model; pruning without pre-training gave intermediate results. 5) On the small game Full KG-DQN matched LSTM-DQN in steps-to-completion (not statistically different); on large games KG-DQN variants converge far faster than LSTM-DQN (LSTM-DQN required &gt;300 episodes to reach KG-DQN levels).",
            "uuid": "e1575.0",
            "source_info": {
                "paper_title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-12"
            }
        },
        {
            "name_short": "QA pre-training (DrQA traces)",
            "name_full": "Offline question-answering pre-training using DrQA on oracle walkthrough traces",
            "brief_description": "An offline transfer-learning procedure that frames action selection as question answering: generate oracle (optimal) walkthrough traces from many generated games, train a DrQA QA model to map observation text (question/context) to the optimal action (answer), and use the DrQA document-encoder SB-LSTM weights and embedding initializations to initialize parts of the KG-DQN network.",
            "citation_title": "Reading Wikipedia to answer opendomain questions.",
            "mention_or_use": "use",
            "agent_name": "DrQA-based pre-training (used to initialize KG-DQN components)",
            "agent_description": "Not an agent for execution, but a QA model (DrQA) trained on (observation -&gt; oracle action) pairs; the encoded SB-LSTM weights and embedding layer (initialized from GloVe) are transferred to KG-DQN's SB-LSTM and embedding layers to provide prior knowledge for action selection.",
            "agent_size": null,
            "environment_name": "TextWorld (home theme) used to generate pre-training data",
            "environment_description": "Same procedural TextWorld generator: many different but same-domain games (same room/item types, different spatial configurations and quests). Oracle generates optimal minimal-step walkthroughs for each generated game to create (observation, correct action) pairs for QA training.",
            "procedure_type": "household tasks / quest procedures (used as QA training targets)",
            "procedure_examples": "Oracle actions in traces correspond to the exact commands needed at each observation to optimally advance the quest (e.g., 'go east', 'take key', 'insert key into lock', 'open chest').",
            "compositional_structure": "Training traces span many different compositional sequences (quests) so the QA model learns mappings from diverse composed observation contexts to the immediate next action; the generator varies room layouts and object placements to avoid memorization and encourage general QA competence.",
            "uses_curriculum": false,
            "curriculum_name": null,
            "curriculum_description": null,
            "curriculum_ordering_principle": null,
            "task_complexity_range": "Pre-training dataset drawn from generated games matching the small/large regimes: small (quest length 5) and large (quest length 10); 200 generated games used per regime, with 160 for training, 20 dev, 20 test during QA pre-training.",
            "performance_with_curriculum": null,
            "performance_without_curriculum": null,
            "has_curriculum_comparison": false,
            "alternative_curriculum_performance": null,
            "transfer_generalization": "Pre-training generalized to unseen test games in the same domain: QA metrics (Table 2) — Small: Exact Match 46.20%, Precision 56.57%, Recall 63.38%, F1 57.94%; Large: Exact Match 34.13%, Precision 52.53%, Recall 64.72%, F1 55.06%. When those pre-trained weights were used to initialize KG-DQN, the full model (pruning + pre-training) achieved substantially fewer steps-to-completion than some ablations and converged quickly. This indicates effective transfer of action-selection priors across different generated games.",
            "key_findings": "1) Framing 'what action to take' as QA and pre-training on oracle traces transfers useful representations to RL agents. 2) QA pre-training produced moderate token-level QA accuracy (EM 34–46%) but still yielded helpful inductive biases when transferred. 3) Pre-training alone (without action pruning) did not produce the best final step efficiency — combining pre-training with knowledge-graph-based pruning yielded the best results (Full KG-DQN). 4) The pre-training dataset was generated from many related but distinct games to force learning general QA competence rather than memorization.",
            "uuid": "e1575.1",
            "source_info": {
                "paper_title": "Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning",
                "publication_date_yy_mm": "2018-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reading Wikipedia to answer opendomain questions.",
            "rating": 2
        },
        {
            "paper_title": "The Natural Language Decathlon : Multitask Learning as Question Answering",
            "rating": 2
        },
        {
            "paper_title": "TextWorld : A Learning Environment for Text-based Games",
            "rating": 2
        },
        {
            "paper_title": "Language Understanding for Textbased Games Using Deep Reinforcement Learning",
            "rating": 1
        },
        {
            "paper_title": "Learning How Not to Act in Text-Based Games",
            "rating": 1
        }
    ],
    "cost": 0.010466999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Playing Text-Adventure Games with Graph-Based Deep Reinforcement Learning</h1>
<p>Prithviraj Ammanabrolu<br>School of Interactive Computing<br>Georgia Institute of Technology<br>Atlanta, GA<br>raj.ammanabrolu@gatech.edu</p>
<p>Mark O. Riedl<br>School of Interactive Computing<br>Georgia Institute of Technology<br>Atlanta, GA<br>riedl@cc.gatech.edu</p>
<h4>Abstract</h4>
<p>Text-based adventure games provide a platform on which to explore reinforcement learning in the context of a combinatorial action space, such as natural language. We present a deep reinforcement learning architecture that represents the game state as a knowledge graph which is learned during exploration. This graph is used to prune the action space, enabling more efficient exploration. The question of which action to take can be reduced to a question-answering task, a form of transfer learning that pre-trains certain parts of our architecture. In experiments using the TextWorld framework, we show that our proposed technique can learn a control policy faster than baseline alternatives. We have also open-sourced our code at https://github.com/rajammanabrolu/KGDQN.</p>
<h2>1 Introduction</h2>
<p>Natural language communication can be used to affect change in the real world. Text adventure games, in which players must make sense of the world through text descriptions and declare actions through natural language, can provide a stepping stone toward more real-world environments where agents must communicate to understand the state of the world and indirectly affect change in the world. Text adventure games are also useful for developing and testing reinforcement learning algorithms that must deal with the partial observability of the world (Narasimhan et al., 2015; He et al., 2016).</p>
<p>In text adventure games, the agent receives an incomplete textual description of the current state of the world. From this information, and previous interactions with the world, a player must determine the next best action to take to achieve some quest or goal. The player must then com-
pose a textual description of the action they intend to make and receive textual feedback of the effects of the action. Formally, a text-based game is a partially observable Markov decision process (POMDP), represented as a 7-tuple of $\langle S, T, A, \Omega, O, R, \gamma\rangle$ representing the set of environment states, conditional transition probabilities between states, words used to compose text commands, observations, observation conditional probabilities, reward function, and the discount factor respectively (Côté et al., 2018).</p>
<p>In text-based games, the agent never has access to the true underlying world state and has to reason about how to act in the world based only on the textual observations. Additionally, the agent's actions must be expressed through natural language commands, ensuring that the action space is combinatorially large. Thus, text-based games pose a different set of challenges than traditional video games. Text-based games require a greater understanding of previous context to be able to explore the state-action space more effectively. Such games have historically proven to be difficult to play for AI agents, and the more complex variants such as Zork still remain firmly out of the reach of existing approaches.</p>
<p>We introduce three contributions to text-based game playing to deal with the combinatorially large state and action spaces. First, we show that a state representation in the form of a knowledge graph gives us the ability to effectively prune an action space. A knowledge graph captures the relationships between entities as a directed graph. The knowledge graph provides a persistent memory of the world over time and enables the agent to have a prior notion of what actions it should not take at a particular stage of the game.</p>
<p>Our second contribution is a deep reinforcement learning architecture, Knowledge Graph DQN (KG-DQN), that effectively uses this state rep-</p>
<p>resentation to estimate the $Q$-value for a stateaction pair. This architecture leverages recent advances in graph embedding and attention techniques (Guan et al., 2018; Veličković et al., 2018) to learn which portions of the graph to pay attention to given an input state description in addition to having a mechanism that allows for natural language action inputs. Finally, we take initial steps toward framing the POMDP as a questionanswering (QA) problem wherein a knowledgegraph can be used to not only prune actions but to answer the question of what action is most appropriate. Previous work has shown that many NLP tasks can be framed as instances of questionanswering and that we can transfer knowledge between these tasks (McCann et al., 2017). We show how pre-training certain parts of our KG-DQN network using existing QA methods improves performance and allows knowledge to be transferred from different games.</p>
<p>We provide results on ablative experiments comparing our knowledge-graph based approach approaches to strong baselines. Results show that incorporating a knowledge-graph into a reinforcement learning agent results in converges to the highest reward more than $40 \%$ faster than the best baseline. With pre-training using a questionanswering paradigm, we achieve this fast convergence rate while also achieving high quality quest solutions as measured by the number of steps required to complete the quests.</p>
<h2>2 Related Work</h2>
<p>A growing body of research has explored the challenges associated with text-based games (Bordes et al., 2010; Narasimhan et al., 2015; He et al., 2016; Fulda et al., 2017; Haroush et al., 2018; Côté et al., 2018; Tao et al., 2018). Narasimhan et al. (2015) attempts to solve parser-based text games by encoding the observations using an LSTM. This encoding vector is then used by an action scoring network that determines the scores for the action verb and each of the corresponding argument objects. The two scores are then averaged to determine $Q$-value for the state-action pair. He et al. (2016) present the Deep Reinforcement Relevance Network (DRRN) which uses two separate deep neural networks to encode the state and actions. The $Q$-value for a state-action pair is then computed by a pairwise interaction function between the two encoded representations. Both of
these methods are not conditioned on previous observations and so are at a disadvantage when dealing with complex partially observable games. Additionally, neither of these approaches prune the action space and so end up wasting trials exploring state-action pairs that are likely to have low $Q$ values, likely leading to slower convergence times for combinatorially large action spaces.</p>
<p>Haroush et al. (2018) introduce the Action Eliminating Network (AEN) that attempts to restrict the actions in each state to the top- $k$ most likely ones, using the emulator's feedback. The network learns which actions should not be taken given a particular state. Their work shows that reducing the size of the action space allows for more effective exploration, leading to better performance. Their network is also not conditioned on previous observations.</p>
<p>Knowledge graphs have been demonstrated to improve natural language understanding in other domains outside of text adventure games. For example, Guan et al. (2018) use commonsense knowledge graphs such as ConceptNet (Speer and Havasi, 2012) to significantly improve the ability of neural networks to predict the end of a story. They represent the graph in terms of a knowledge context vector using features from ConceptNet and graph attention (Veličković et al., 2018). The state representation that we have chosen as well as our method of action pruning builds on the strengths of existing approaches while simultaneously avoiding the shortcomings of ineffective exploration and lack of long-term context.</p>
<h2>3 Knowledge Graph DQN</h2>
<p>In this section we introduce our knowledge graph representation, action pruning and deep $Q$ network architecture.</p>
<h3>3.1 Knowledge Graph Representation</h3>
<p>In our approach, our agent learns a knowledge graph, stored as a set of RDF triples, i.e. 3-tuples of $\langle$ subject, relation, object $\rangle$. These triples are extracted from the observations using Stanford's Open Information Extraction (OpenIE) (Angeli et al., 2015). OpenIE is not optimized to the regularities of text adventure games and there are a lot of relations that can be inferred from the typical structure of descriptive texts. For example, from a phrase such as "There is an exit to the north" one can infer a has relation between the current</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Graph state update example given two observations
location and the direction of the exit. These additional rules fill in the information not provided by OpenIE. The resultant knowledge graph gives the agent what essentially amounts to a mental map of the game world.</p>
<p>The knowledge graph is updated after every agent action (see Figure 1). The update rules are defined such that there are portions of the graph offering short and long-term context. A special node-designated "you"-represents the agent and relations out of this node are updated after every action with the exception of relations denoting the agent's inventory. Other relations persist after each action. We intend for the update rules to be applied to text-based games in different domains and so only hand-craft a minimal set of rules that we believe apply generally. They are:</p>
<ul>
<li>Linking the current room type (e.g. "basement", "chamber') to the items found in the room with the relation "has", e.g. $\langle$ chamber, has, bed stand $\rangle$</li>
<li>Extracting information regarding entrances and exits and linking them to the current room, e.g. $\langle$ basement, has, exit to north $\rangle$</li>
<li>Removing all relations relating to the "you" node with the exception of inventory every action, e.g. $\langle$ you, have, cubical key $\rangle$</li>
<li>Linking rooms with directions based on the action taken to move between the rooms, e.g. $\langle$ chamber, east of, basement $\rangle$ after the action "go east" is taken to go from the basement to the chamber</li>
</ul>
<p>All other RDF triples generated are taken from OpenIE.</p>
<h3>3.2 Action Pruning</h3>
<p>The number of actions available to an agent in a text adventure game can be quite large: $A=$ $\mathcal{O}(|V| \times|O|^{2})$ where $V$ is the number of action verbs, and $O$ is the number of distinct objects in the world that the agent can interact with, assuming that verbs can take two arguments. Some actions, such as movement, inspecting inventory, or observing the room, do not have arguments.</p>
<p>The knowledge graph is used to prune the combinatorially large space of possible actions available to the agent as follows. Given the current state graph representation $G_{t}$, the action space is pruned by ranking the full set of actions and selecting the top- $k$. Our action scoring function is:</p>
<ul>
<li>+1 for each object in the action that is present in the graph; and</li>
<li>+1 if there exists a valid directed path between the two objects in the graph.</li>
</ul>
<p>We assume that each action has at most two objects (for example inserting a key in a lock).</p>
<h3>3.3 Model Architecture and Training</h3>
<p>Following Narasimhan et al. (2015), all actions $A$ that will be accepted by the game's parser are available to the agent at all times. When playing the game, the agent chooses an action and receives an observation $o_{t}$ from the simulator, which is a textual description of current game state. The state graph $G_{t}$ is updated according to the given observation, as described in Section 3.1.</p>
<p>We use the $Q$-Learning technique (Watkins and Dayan, 1992) to learn a control policy $\pi\left(a_{t} \mid s_{t}\right)$, $a_{t} \in A$, which gives us the probability of taking action $a_{t}$ given the current state $s_{t}$. The policy is</p>
<p>determined by the $Q$-value of a particular stateaction pair, which is updated using the Bellman equation (Sutton and Barto, 2018):</p>
<p>$$
\begin{aligned}
&amp; Q_{t+1}\left(s_{t+1}, a_{t+1}\right)= \
&amp; \quad E\left[r_{t+1}+\gamma \max <em t="t">{a \in A</em>\right]
\end{aligned}
$$}} Q_{t}(s, a) \mid s_{t}, a_{t</p>
<p>where $\gamma$ refers to the discount factor and $r_{t+1}$ is the observed reward. The policy is thus to take the action that maximizes the $Q$-value in a particular state, which will correspond to the action that maximizes the reward expectation given that the agent has taken action $a_{t}$ at the current state $s_{t}$ and followed the policy $\pi(a \mid s)$ after.</p>
<p>The architecture in Figure 2 is responsible for computing the representations for both the state $s_{t}$ and the actions $a^{(i)} \in A$ and coming to an estimation of the $Q$-value for a particular state and action. During the forward activation, the agent uses the observation to update the graph $G_{t}$ using the rules outlined in Section 3.2.</p>
<p>The graph is then embedded into a single vector $\mathbf{g}<em _mathbf_1="\mathbf{1">{\mathbf{t}}$. We use Graph Attention (Veličković et al., 2018) with an attention mechanism similar to that described in Bahdanau et al. (2014). Formally, the Multi-headed Graph Attention component receives a set of node features $H=$ $\left{\mathbf{h}</em>}}, \mathbf{h<em _mathbf_N="\mathbf{N">{\mathbf{2}}, \ldots, \mathbf{h}</em>}}\right}, \mathbf{h<em t="t">{\mathbf{i}} \in \mathbb{R}^{\mathrm{F}}$, where $N$ is the number of nodes and $F$ the number of features in each node, and the adjacency matrix of $G</em>$ applied to all the node features:}$. Each of the node features consist of the averaged word embeddings for the tokens in that node, as determined by the preceding graph embedding layer. The attention mechanism is set up using self-attention on the nodes after a learnable linear transformation $W \in \mathbb{R}^{2 \mathrm{~F} \times \mathrm{F}</p>
<p>$$
e_{i j}=\operatorname{LeakyReLU}\left(\mathbf{p} \cdot W\left(\mathbf{h}<em _mathbf_j="\mathbf{j">{\mathbf{i}} \oplus \mathbf{h}</em>\right)\right)
$$}</p>
<p>where $\mathbf{p} \in \mathbb{R}^{2 \mathrm{~F}}$ is a learnable parameter. The attention coefficients $\alpha_{i j}$ are then computed by normalizing over the choices of $k \in \mathcal{N}$ using the softmax function. Here $\mathcal{N}$ refers to the neighborhood in which we compute the attention coefficients. This is determined by the adjacency matrix for $G_{t}$ and consists of all third-order neighbors of a particular node.</p>
<p>$$
\alpha_{i j}=\frac{\exp \left(e_{i j}\right)}{\sum_{k \in \mathcal{N}} \exp \left(e_{i k}\right)}
$$</p>
<p>Multi-head attention is then used, calculating multiple independent attention coefficients. The resulting features are then concatenated and passed
into a linear layer to determine $\mathbf{g}_{\mathbf{t}}$ :</p>
<p>$$
\mathbf{g}<em g="g">{\mathbf{t}}=f\left(W</em>\left(\left|<em _in="\in" _mathcal_N="\mathcal{N" j="j">{k=1}^{K} \sigma\left(\sum</em>}} \alpha_{i j}^{(k)} \mathbf{W}^{(k)} \mathbf{h<em g="g">{j}\right)\right)+b</em>\right)\right.
$$</p>
<p>where $k$ refers to the parameters of the $k^{t h}$ independent attention mechanism, $W_{g}$ and $b_{g}$ the weights and biases of this component's output linear layer, and $|$ represents concatenation.</p>
<p>Simultaneously, an encoded representation of the observation $\mathbf{o}<em _mathbf_t="\mathbf{t">{\mathbf{t}}$ is computed using a Sliding Bidirectional LSTM (SB-LSTM). The final state representation $\mathbf{s}</em>$ is computed as:}</p>
<p>$$
\mathbf{s}<em l="l">{\mathbf{t}}=f\left(W</em>}\left(\mathbf{g<em _mathbf_t="\mathbf{t">{\mathbf{t}} \oplus \mathbf{o}</em>\right)
$$}}\right)+b_{l</p>
<p>where $W_{l}, b_{l}$ represent the final linear layer's weights and biases and $\mathbf{o}_{\mathbf{t}}$ is the result of encoding the observation with the SB-LSTM.</p>
<p>The entire set of possible actions $A$ is pruned by scoring each $a \in A$ according to the mechanism previously described using the newly updated $G_{t+1}$. We then embed and encode all of these action strings using an LSTM encoder (Sutskever et al., 2014). The dashed lines in Figure 2 denotes non-differentiable processes.</p>
<p>The final $Q$-value for a state-action pair is:</p>
<p>$$
Q\left(\mathbf{s}<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{a}</em>}}\right)=\mathbf{s<em _mathbf_t="\mathbf{t">{\mathbf{t}} \cdot \mathbf{a}</em>
$$}</p>
<p>This method of separately computing the representations for the state and action is similar to the approach taken in the DRRN (He et al., 2016).</p>
<p>We train the network using experience replay (Lin, 1993) with prioritized sampling (cf., (Moore and Atkeson, 1993)) and a modified version of the $\epsilon$-greedy algorithm (Sutton and Barto, 2018) that we call the $\epsilon_{1}, \epsilon_{2}$-greedy learning algorithm. The experience replay strategy finds paths in the game, which are then stored as transition tuples in a experience replay buffer $D$. The $\epsilon_{1}, \epsilon_{2}$-greedy algorithm explores by choosing actions randomly from $A$ with probability $\epsilon_{1}$ and from $A_{t}$ with a probability $\epsilon_{2}$. The second threshold is needed to account for situations where an action must be chosen to advance the quest for which the agent has no prior in $G_{t}$. That is, action pruning may remove actions essential to quest completion because those actions involve combinations of entities that have not been encountered before.</p>
<p>We then sample a mini-batch of transition tuples consisting of $\left\langle\mathbf{s}<em _mathbf_k="\mathbf{k">{\mathbf{k}}, \mathbf{a}</em>}}, r_{k+1}, \mathbf{s<em _mathbf_k="\mathbf{k">{\mathbf{k}+\mathbf{1}}, \mathbf{A}</em>\right\rangle$ from}+\mathbf{1}}, p_{k</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: KG-DQN architecture, blue shading (or the symbol 'B') indicates components that can be pre-trained and red (or the symbol 'R') indicates no pre-training. The solid lines indicate gradient flow for learnable components.
$D$ and compute the temporal difference loss as:</p>
<p>$$
\begin{aligned}
L(\theta)= &amp; r_{k+1}+ \
&amp; \gamma \max <em _mathbf_k="\mathbf{k">{\mathbf{a} \in \mathbf{A}</em>}+1}} Q\left(\mathbf{s<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{a} ; \theta\right)-Q\left(\mathbf{s}</em> ; \theta\right)
\end{aligned}
$$}}, \mathbf{a}_{\mathbf{t}</p>
<p>Replay sampling from $D$ is done by sampling a fraction $\rho$ from transition tuples with a positive reward and $1-\rho$ from the rest. As shown in (Narasimhan et al., 2015), prioritized sampling from experiences with a positive reward helps the deep $Q$-network more easily find the sparse set of transitions that advance the game. The exact training mechanism is described in Algorithm 1.</p>
<h2>4 Game Play as Question Answering</h2>
<p>Previous work has shown that many NLP tasks can be framed as instances of question-answering and that in doing so, one can transfer knowledge between these tasks (McCann et al., 2017). In the abstract, an agent playing a text adventure game can be thought of as continuously asking the question "What is the right action to perform in this situation?" When appropriately trained, the agent may be able to answer the question for itself and select a good next move to execute. Treating the problem as question-answering will not replace the need for exploration in text-adventure games. However, we hypothesize that it will cut down on the amount of exploration needed during testing time, theoretically allowing it to complete quests faster; one of the challenges of text adventure games is that the quests are puzzles and even after training, execution of the policy requires a significant amount of exploration.</p>
<p>To teach the agent to answer the question of what action is best to take given an observation,
we use an offline, pre-training approach. The data for the pre-training approach is generated using an oracle, an agent capable of finishing a game perfectly in the least number of steps possible. Specifically, the agent knows exactly what action to take given the state observation in order to advance the game in the most optimal manner possible. Through this process, we generate a set of traces consisting of state observations and actions such that the state observation provides the context for the implicit question of "What action should be taken?" and the oracle's correct action is the answer. We then use the DrQA (Chen et al., 2017) question-answering technique to train a paired question encoder and an answer encoder that together predict the answer (action) from the question (text observation). The weights from the SB-LSTM in the document encoder in the DrQA system are then used to initialize the weights of the SB-LSTM. Similarly, embedding layers of both the graph and the LSTM action encoder are initialized with the weights from the embedding layer of same document encoder. Since the DrQA embedding layers are initialized with GloVe, we are transferring word embeddings that are tuned during the training of the QA architecture.</p>
<p>The game traces used to train the questionanswering come from a set of games of the same domain but have different specific configurations of the environment and different quests. We use the TextWorld framework (Côté et al., 2018), which uses a grammar to generate random worlds and quests. The types of rooms are the same, but their relative spatial configuration, the types of objects, and the specific sequence of actions needed to complete the quest are different each time. This</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Small</th>
<th style="text-align: left;">Large</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Rooms</td>
<td style="text-align: left;">10</td>
<td style="text-align: left;">20</td>
</tr>
<tr>
<td style="text-align: left;">Total objects</td>
<td style="text-align: left;">20</td>
<td style="text-align: left;">40</td>
</tr>
<tr>
<td style="text-align: left;">Quest length</td>
<td style="text-align: left;">5</td>
<td style="text-align: left;">10</td>
</tr>
<tr>
<td style="text-align: left;">Branching factor</td>
<td style="text-align: left;">143</td>
<td style="text-align: left;">562</td>
</tr>
<tr>
<td style="text-align: left;">Vocab size</td>
<td style="text-align: left;">746</td>
<td style="text-align: left;">819</td>
</tr>
<tr>
<td style="text-align: left;">Average words per obs.</td>
<td style="text-align: left;">67.5</td>
<td style="text-align: left;">94.0</td>
</tr>
<tr>
<td style="text-align: left;">Average new RDF triples per obs.</td>
<td style="text-align: left;">7.2</td>
<td style="text-align: left;">10.5</td>
</tr>
</tbody>
</table>
<p>Table 1: Generated game details.
means that the agent cannot simply memorize quests. For pre-training to work, the agent must develop a general question-answering competence that can transfer to new quests. Our approach to question-answering in the context of text adventure game playing thus represents a form of transfer learning.</p>
<h2>5 Experiments</h2>
<p>We conducted experiments in the TextWorld framework (Côté et al., 2018) using their "home" theme. TextWorld uses a grammar to randomly generate game worlds and quests with given parameters. Games generated with TextWorld start with a zero-th observation that gives instructions for the quest; we do not allow our agent to access this information. The TextWorld API also provides a list of admissible actions at each state-the actions that can be performed based on the objects that are present. We do not allow our agent to access the admissible actions.</p>
<p>We generated two sets of games with different random seeds, representing different game difficulties, which we denote as small and large. Small games have ten rooms and quests of length five and large games have twenty rooms and quests of length ten. Statistics on the games are given in Table 1. Quest length refers to the number of actions that the agent is required to perform in order to finish the quest; more actions are typically necessary to move around the environment and find the objects that need to be interacted with. The branching factor is the size of the action set $A$ for that particular game.</p>
<p>The reward function provided by TextWorld is as follows: +1 for each action taken that moves the agent closer to finishing the quest; -1 for each action taken that extends the minimum number of steps needed to finish the quest from the current stage; 0 for all other situations. The maximum achievable reward for the small and large sets of games are 5 and 10 respectively. This allows for</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">EM</th>
<th style="text-align: left;">Precision</th>
<th style="text-align: left;">Recall</th>
<th style="text-align: left;">F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Small</td>
<td style="text-align: left;">46.20</td>
<td style="text-align: left;">56.57</td>
<td style="text-align: left;">63.38</td>
<td style="text-align: left;">57.94</td>
</tr>
<tr>
<td style="text-align: left;">Large</td>
<td style="text-align: left;">34.13</td>
<td style="text-align: left;">52.53</td>
<td style="text-align: left;">64.72</td>
<td style="text-align: left;">55.06</td>
</tr>
</tbody>
</table>
<p>Table 2: Pre-training accuracy.
a large amount of variance in quest quality-as measured by steps to complete the quest-that receives maximum reward.</p>
<p>The following procedure for pre-training was done separately for each set of games. Pre-training of the SB-LSTM within the question-answering architecture is conducted by generating 200 games from the same TextWorld theme. The QA system was then trained on data from walkthroughs of a randomly-chosen subset of 160 of these generated games, tuned on a dev set of 20 games, and evaluated on the held-out set of 20 games. Table 2 provides details on the Exact Match (EM), precision, recall, and F1 scores of the QA system after training for the small and large sets of games. Precision, recall, and F1 scores are calculated by counting the number of tokens between the predicted answer and ground truth. An Exact Match is when the entire predicted answer matches with the ground truth. This score is used to tune the model based on the dev set of games.</p>
<p>A random game was chosen from the test-set of games and used as the environment for the agent to train its deep $Q$-network on. Thus, at no time did the QA system see the final testing game prior to the training of the KG-DQN network.</p>
<p>We compare our technique to three baselines:</p>
<ul>
<li>Random command, which samples from the list of admissible actions returned by the TextWorld simulator at each step.</li>
<li>LSTM-DQN, developed by Narasimhan et al. (2015).</li>
<li>Bag-of-Words DQN, which uses a bag-ofwords encoding with a multi-layer feed forward network instead of an LSTM.</li>
</ul>
<p>To achieve the most competitive baselines, we used a randomized grid search to choose the best hyperparameters (e.g., hidden state size, $\gamma, \rho$, final $\epsilon$, update frequency, learning rate, replay buffer size) for the BOW-DQN and LSTM-DQN baselines.</p>
<p>We tested three versions of our KG-DQN:</p>
<ol>
<li>Un-pruned actions with pre-training</li>
</ol>
<p>Algorithm $1 \epsilon_{1}, \epsilon_{2}$-greedy learning algorithm for KG-DQN</p>
<table>
<thead>
<tr>
<th style="text-align: center;">1: for episode $=1$ to $M$ do</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2:</td>
<td style="text-align: center;">Initialize action dictionary $A$ and graph $G_{0}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3:</td>
<td style="text-align: center;">Reset the game simulator</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">4:</td>
<td style="text-align: center;">Read initial observation $o_{1}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">5:</td>
<td style="text-align: center;">$G_{1} \leftarrow$ updateGraph $\left(G_{0}, o_{1}\right) ; A_{1} \leftarrow$ pruneActions $\left(A, G_{0}\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\triangleright$ Section 3.2</td>
</tr>
<tr>
<td style="text-align: center;">6:</td>
<td style="text-align: center;">for step $t=1$ to $T$ do</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">7:</td>
<td style="text-align: center;">if random ()$&lt;\epsilon_{1}$ then</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">8:</td>
<td style="text-align: center;">if random() $&lt;\epsilon_{2}$ then</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">9:</td>
<td style="text-align: center;">Select random action $a_{t} \in A$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">10:</td>
<td style="text-align: center;">else</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">11:</td>
<td style="text-align: center;">Select random action $a_{t} \in A_{t}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">12:</td>
<td style="text-align: center;">else</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">13:</td>
<td style="text-align: center;">Compute $Q\left(\mathbf{a}_{\mathbf{t}}, \mathbf{a}^{(1)} ; \theta\right)$ for $a^{(1)} \in A$ for network parameters $\theta$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\triangleright$ Section 3.3, Eq. 6</td>
</tr>
<tr>
<td style="text-align: center;">14:</td>
<td style="text-align: center;">Select $a_{t}$ based on $\pi\left(a \mid s_{t}\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">15:</td>
<td style="text-align: center;">Execute action $a_{t}$ in the simulator and observe reward $r_{t}$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">16:</td>
<td style="text-align: center;">Receive next observation of $\pm 1$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">17:</td>
<td style="text-align: center;">$G_{t+1} \leftarrow$ updateGraph $\left(G_{t}, o_{t+1}\right) ; A_{t+1} \leftarrow$ pruneActions $\left(A, G_{t+1}\right)$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\triangleright$ Section 3.1</td>
</tr>
<tr>
<td style="text-align: center;">18:</td>
<td style="text-align: center;">Compute $\mathbf{a}<em _mathbf_t="\mathbf{t">{\mathbf{t}+1}$ and $\mathbf{A}</em>\right \in A}$}+1}=\left(\mathbf{a}^{\wedge 1)}\right.$ for all $\left.a^{\wedge 1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\triangleright$ Section 3.3</td>
</tr>
<tr>
<td style="text-align: center;">19:</td>
<td style="text-align: center;">Set priority $p_{t}=1$ if $r_{t}&gt;0$, else $p_{t}=0$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">20:</td>
<td style="text-align: center;">Store transition $\left(\mathbf{a}<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{a}</em>}}, r_{t}, \mathbf{a<em _mathbf_t="\mathbf{t">{\mathbf{t}+1}, \mathbf{A}</em>\right)$ in replay buffer $D$}+1}, p_{t</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">21:</td>
<td style="text-align: center;">Sample mini-batch of transitions $\left(\mathbf{a}<em _mathbf_k="\mathbf{k">{\mathbf{k}}, \mathbf{a}</em>}}, r_{k}, \mathbf{a<em _mathbf_k="\mathbf{k">{\mathbf{k}+1}, \mathbf{A}</em>=1$}+1}, p_{k}\right)$ from $D$, with fraction $\rho$ having $p_{k</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">22:</td>
<td style="text-align: center;">Set $y_{k}=r_{k}+\gamma \max <em _mathbf_k="\mathbf{k">{\mathbf{a} \in \mathbf{A}</em>}+1}} Q\left(\mathbf{a<em k="k">{\mathbf{t}}, \mathbf{a} ; \theta\right)$, or $y</em>$ is terminal}=r_{k}$ if $s_{k+1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">23:</td>
<td style="text-align: center;">Perform gradient descent step on loss function $L(\theta)=\left(y_{k}-Q\left(\mathbf{a}<em _mathbf_t="\mathbf{t">{\mathbf{t}}, \mathbf{a}</em>$}} ; \theta\right)\right)^{2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<ol>
<li>Pruned actions without pre-training</li>
<li>Pruned actions with pre-training (full)</li>
</ol>
<p>Our models use 50 -dimensional word embeddings, 2 heads on the graph attention layers, minibatch size of 16 , and perform a gradient descent update every 5 steps taken by the agent.</p>
<p>All models are evaluated by observing the (a) time to reward convergence, and (b) the average number of steps required for the agent to finish the game with $\epsilon=0.1$ over 5 episodes after training has completed. Following Narasimhan et al. (2015) we set $\epsilon$ to a non-zero value because text adventure games, by nature, require exploration to complete the quests. All results are reported based on multiple independent trials. For the large set of games, we only perform experiments on the best performing models found in the small set of games. Also note that for experiments on large games, we do not display the entire learning curve for the LSTM-DQN baseline, as it converges significantly more slowly than KG-DQN. We run each experiment 5 times and average the results.</p>
<p>Additionally, human performance on the both the games was measured by counting the number of steps taken to finish the game, with and without instructions on the exact quest. We modified Textworld to give the human players reward feedback in the form of a score, the reward function itself is identical to that received by the deep reinforcement learning agents. In one variation of this experiment, the human was given instructions on the potential sequence of steps that are required
to finish the game in addition to the reward in the form of a score and in the other variation, the human received no instructions.</p>
<h2>6 Results and Discussion</h2>
<p>Recall that the number of steps required to finish the game for the oracle agent is 5 and 10 for the small and large maps respectively. It is impossible to achieve this ideal performance due to the structure of the quest. The player needs to interact with objects and explore the environment in order to figure out the exact sequence of actions required to finish the quest. To help benchmark our agent's performance, we observed people unaffiliated with the research playing through the same TextWorld "home" quests as the other models. Those who did not receive instructions on how to finish the quest never finished a single quest and gave up after an average of 184 steps on the small map and an average of 190 steps on the large map. When given instructions, human players completed the quest on the large map in an average of 23 steps, finishing the game with the maximum reward possible. Also note that none of the deep reinforcement learning agents received instructions.</p>
<p>On both small and large maps, all versions of KG-DQN tested converge faster than baselines (see Figure 3 for the small game and Figure 4 for the large game). We don't show BOW-DQN because it is strictly inferior to LSTM-DQN in all situations). KG-DQN converges $40 \%$ faster than baseline on the small game; both KG-DQN and the LSTM-DQN baseline reaches the maximum reward of five. On the large game, no</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Reward learning curve for select experiments with the small games. Best viewed in color.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Command</td>
<td>319.8</td>
</tr>
<tr>
<td>BOW-DQN</td>
<td>83.1 ± 8.0</td>
</tr>
<tr>
<td>LSTM-DQN</td>
<td>72.4 ± 4.6</td>
</tr>
<tr>
<td>Unpruned, pre-trained KG-DQN</td>
<td>131.7 ± 7.7</td>
</tr>
<tr>
<td>Pruned, non-pre-trained KG-DQN</td>
<td>97.3 ± 9.0</td>
</tr>
<tr>
<td>Full KG-DQN</td>
<td>73.7 ± 8.5</td>
</tr>
</tbody>
</table>
<p>Table 3: Average number of steps (and standard deviation) taken to complete the small game.</p>
<p>agents achieve the maximum reward of 10, and the LSTM-DQN requires more than 300 episodes to converge at the same level as KG-DQN. Since all versions of KG-DQN converge at approximately the same rate, we conclude that the knowledge graph—i.e., persistent memory—is the main factor helping convergence time since it is the common element across all experiments.</p>
<p>After training is complete, we measure the number of steps each agent needs to complete each quest. Full KG-DQN requires an equivalent number of steps in the small game (Table 3) and in the large game (Table 4). Differences between LSTM-DQN and full KG-DQN are not statistically significant, <em>p</em> = 0.199 on an independent T-test. The ablated versions of KG-DQN—unpruned KG-DQN and non-pre-trained KG-DQN—require many more steps to complete quests. TextWorld's reward function allows for a lot of exploration of the environment without penalty so it is possible for a model that has converged on reward to complete quests in as few as five steps or in many hundreds of steps. From these results, we conclude that the pre-training using our question-answering paradigm is allowing the agent to find a general understanding of how to pick good actions even when the agent has never seen the final</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Reward learning curve for select experiments with the large games. Best viewed in color.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random Command</td>
<td>2054.8</td>
</tr>
<tr>
<td>LSTM-DQN</td>
<td>260.3 ± 4.5</td>
</tr>
<tr>
<td>Pruned, non-pre-trained KG-DQN</td>
<td>340 ± 6.4</td>
</tr>
<tr>
<td>Full KG-DQN</td>
<td>265.9 ± 9.4</td>
</tr>
</tbody>
</table>
<p>Table 4: Average number of steps (and standard deviation) taken to complete the large game.</p>
<p>test game. LSTM-DQN also learns how to choose actions efficiently, but this knowledge is captured in the LSTM's cell state, whereas in KG-DQN this knowledge is made explicit in the knowledge graph and retrieved effectively by graph attention. Taken together, KG-DQN converges faster without loss of quest solution quality.</p>
<h2>7 Conclusions</h2>
<p>We have shown that incorporating knowledge graphs into an deep <em>Q</em>-network can reduce training time for agents playing text-adventure games of various lengths. We speculate that this is because the knowledge graph provides a persistent memory of the world as it is being explored. While the knowledge graph allows the agent to reach optimal reward more quickly, it doesn't ensure a high quality solution to quests. Action pruning using the knowledge graph and pre-training of the embeddings used in the deep <em>Q</em>-network result in shorter action sequences needed to complete quests.</p>
<p>The insight into pre-training portions of the agent's architecture is based on converting text-adventure game playing into a question-answering activity. That is, at every step, the agent is asking—and trying to answer—what is the most important thing to try. The pre-training acts as a form of transfer learning from different, but re-</p>
<p>lated games. However, question-answering alone cannot solve the text-adventure playing problem because there will always be some trial and error required.</p>
<p>By addressing the challenges of partial observability and combinatorially large action, spaces through persistent memory, our work on playing text-adventure games addresses a critical need for reinforcement learning for language. Textadventure games can be seen as a stepping stone toward more complex, real-world tasks; the human world is one of partial understanding through communication and acting on the world using language.</p>
<h2>References</h2>
<p>Gabor Angeli, Johnson Premkumar, Melvin Jose, and Christopher D. Manning. 2015. Leveraging Linguistic Structure For Open Domain Information Extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers).</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv:1409.0473.</p>
<p>Antoine Bordes, Nicolas Usunier, Ronan Collobert, and Jason Weston. 2010. Towards understanding situated natural language. In Proceedings of the 2010 International Conference on Artificial Intelligence and Statistics.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Association for Computational Linguistics (ACL).</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Wendy Tay, and Adam Trischler. 2018. TextWorld : A Learning Environment for Text-based Games. In Proceedings of the ICML/IJCAI 2018 Workshop on Computer Games, page 29.</p>
<p>Nancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. 2017. What can you do with a rock? affordance extraction via word embeddings. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pages 1039-1045.</p>
<p>Jian Guan, Yansen Wang, and Minlie Huang. 2018. Story Ending Generation with Incremental Encoding and Commonsense Knowledge. arXiv:1808.10113v1.</p>
<p>Matan Haroush, Tom Zahavy, Daniel J Mankowitz, and Shie Mannor. 2018. Learning How Not to Act in Text-Based Games. In Workshop Track at ICLR 2018, pages 1-4.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep Reinforcement Learning with a Natural Language Action Space. In Association for Computational Linguistics (ACL).</p>
<p>Long-Ji Lin. 1993. Reinforcement learning for robots using neural networks. Ph.D. thesis, Carnegie Mellon University.</p>
<p>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2017. The Natural Language Decathlon : Multitask Learning as Question Answering. arXiv:1806.08730.</p>
<p>Andrew W. Moore and Christopher G. Atkeson. 1993. Prioritized sweeping: Reinforcement learning with less data and less time. Machine Learning, 13(1):103-130.</p>
<p>Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language Understanding for Textbased Games Using Deep Reinforcement Learning. In Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Robert Speer and Catherine Havasi. 2012. Representing General Relational Knowledge in ConceptNet 5. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC).</p>
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104-3112.</p>
<p>Richard S Sutton and Andrew G Barto. 2018. Reinforcement Learning: An Introduction. MIT Press.</p>
<p>Ruo Yu Tao, Marc-Alexandre Côté, Xingdi Yuan, and Layla El Asri. 2018. Towards solving text-based games by producing adaptive action spaces. In Proceedings of the 2018 NeurIPS Workshop on Wordplay: Reinforcement and Language Learning in Text-based Games.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph Attention Networks. International Conference on Learning Representations (ICLR).</p>
<p>Christopher J. C. H. Watkins and Peter Dayan. 1992. Q-learning. Machine Learning, 8(3):279-292.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-765 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-765</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-765</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-278904595</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.13549v2.pdf" target="_blank">EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents</a></p>
                <p><strong>Paper Abstract:</strong> Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce EscapeBench, a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals. Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15% average progress without hints, highlighting their limitations in creativity. To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks). Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence. It navigates and completes games with up to 40% fewer steps and hints, performs robustly across difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e765.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e765.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BaseAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BaseAgent (Chain-of-Thought baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline LM agent that acts in the EscapeBench text-based room-escape environments using Chain-of-Thought reasoning and a short working memory to select actions from a predefined action space (move, click, apply, input, craft). It receives textual environment feedback and updates working memory after each step.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BaseAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>The BaseAgent is an LLM-driven agent that (1) receives the current scene description, list of interactable items, tools in bag, and a working-memory of recent steps (memory length set to 10), (2) uses explicit Chain-of-Thought prompting to reason about the next action, and (3) issues one of five actions (move, click, apply, input, craft). After each action it appends the environment feedback to working memory. It is implemented as the standard evaluation baseline in EscapeBench and is instantiated with various backbone LMs (GPT-4o, Llama-3.1-70B, Qwen-2.5, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>EscapeBench (room-escape text environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A suite of 36 text-based room-escape games where each scene contains items and collectible tools; scenes are connected as a graph (adjacent scenes via doors or tunnels). Observations are local textual descriptions and environment feedback is sparse (especially in Hard mode). Challenges include uncertain goal pathways, very long required action chains (100+ steps), creative tool affordance reasoning, crafting, and sparse/noisy feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>No external web/search/calculator tools are used. The 'tools' here refer to in-environment collectible objects (keys, rag, wire, etc.) that the agent can Apply or Craft; additionally the system can provide human-annotated hints when the agent is stuck. The BaseAgent uses internal LLM reasoning (Chain-of-Thought) and working-memory as its internal 'tools'.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual environment feedback: success/failure messages, state changes for items/tools (e.g., item state updates), scene transitions (move -> new scene), and occasional human-provided hint text when provided after long inactivity.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A short textual working memory (sliding window of the last ~10 steps) that stores recent actions and environment feedback; no explicit structured world model beyond the immediate scene description. The agent also has access to the annotated scene metadata provided by the engine (scene relations, list of items/tools present).</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>After each action the game engine returns textual feedback which is appended to the working memory; this updated memory is used for the next Chain-of-Thought reasoning step. There is no separate structured belief graph in BaseAgent—belief is maintained as the sequence of recent textual observations and tool/item lists provided by the engine.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Chain-of-Thought prompting (LLM reasoning) with trial-and-error; local, greedy action selection based on recent memory and scene context. No explicit search-based or model-based planner is used in the BaseAgent.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Discrete graph navigation via explicit move(scene) actions to adjacent scenes given by the engine's scene relations; the agent does not run an explicit shortest-path algorithm (e.g., A*) but decides moves via LLM reasoning over the local scene graph and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Across evaluated backbone models, BaseAgent achieves low progress: the paper reports 'only 15% average progress without hints' in early experiments; example per-model figures include GPT-4o BaseAgent: Hints Used 9.83, Total Steps 707.33, Early Exit Progress 19.85% (Table values).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BaseAgent can collect tools through exploration but often fails to exploit them creatively; its short textual working memory and purely CoT-based decision-making lead to superficial strategies and high hint reliance. It handles local navigation via move actions on the scene graph but lacks structured belief or explicit tool-hypothesis mechanisms, resulting in poor performance on creative Apply/Input/Craft key steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e765.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e765.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EscapeAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EscapeAgent (Foresight + Reflection enhanced agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An enhanced LLM-agent architecture built on BaseAgent that adds a Reflection module (task list management) and a Foresight module (explicit hypothesis generation about tool uses) to improve creative tool use and implicit goal identification in EscapeBench.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>EscapeAgent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>EscapeAgent composes three components: (1) a BaseAgent that issues actions via Chain-of-Thought reasoning with working memory, (2) a Reflection module downstream of action execution that maintains a structured task list (entries include task index, target item, task description, and failed actions) and supports operations {new, update, delete, none}, and (3) a Foresight module upstream that triggers when a new tool is collected or a new task is created and explicitly hypothesizes possible apply/craft/input/click actions for that tool/task. Foresight can transition the agent into a 'Try Action' mode to sequentially test hypotheses (or 'Free Explore' otherwise). The integration yields an agent that proposes multiple viable actions, avoids repeated failed trials, and keeps track of unsolved subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>EscapeBench (room-escape text environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same EscapeBench as for BaseAgent: 36 text-based room-escape scenarios with items, collectible tools, and scene-graph connectivity; difficulty levels vary by description clarity and feedback granularity. The environment is partially observable (local textual observations only), contains sparse feedback (especially in Hard), and requires long action chains and creative tool affordance reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>No external search/calculator APIs; uses in-environment collectible 'tools' (game objects) and internal modules. Additionally relies on human-annotated hints supplied by the environment after prolonged inactivity. The Foresight/Reflection modules operate over these in-environment tools and textual feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual environment feedback (success/failure, item state transitions, newly available tools/items, scene transitions), and human-provided hint text when triggered; Foresight's hypotheses are internal textual proposals, not separate external tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A composite textual belief comprising (a) the BaseAgent's working memory (sliding window of recent steps and feedback), (b) a structured Reflection-maintained task list (each task: index, target item, description, failed actions, hints), and (c) the current bag inventory and annotated scene metadata. This hybrid textual+structured task-list serves as the agent's explicit belief representation.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Tool and action results from the environment (textual feedback) are first appended to working memory; Reflection then processes non-move action feedback to add 'new' tasks (when an unsolved goal is identified), 'update' tasks (when an attempted action fails—it records failed actions/hindsight), or 'delete' tasks (when success is achieved). When a new tool is collected or new task is added, Foresight reads the current task list and bag inventory to generate and justify hypothesized apply/craft/click/input actions; these hypotheses guide Try Action trials. Thus belief updates combine sequential memory appends and rule-guided task-list operations.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>LLM Chain-of-Thought reasoning combined with explicit hypothesis-driven trial planning (Foresight) and persistent task bookkeeping (Reflection). Planning is largely local and hypothesis-driven rather than exhaustive graph search; multiple concurrent candidate actions can be proposed and executed in sequence until success.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Graph-based discrete navigation using move(scene) to adjacent scenes defined by the engine. The agent uses LLM reasoning (and knowledge in memory/task list) to decide next moves; there is no explicit path-finding algorithm (A*, Dijkstra) described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>EscapeAgent substantially improves efficiency and reduces hint reliance compared to BaseAgent. Example: GPT-4o with EscapeAgent: Hints Used 5.00, Total Steps 452.00, Early Exit Progress 44.55% (Table 6/5); compared to GPT-4o BaseAgent: Hints 9.83, Steps 707.33, Early Exit Progress 19.85%. The paper reports EscapeAgent reduces hint reliance by nearly 50% and completes games with up to ~40% fewer steps and hints on average.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit hypothesis generation (Foresight) plus persistent unsolved-task bookkeeping (Reflection) increases creative tool use and reduces redundant trials: agents propose targeted Apply/Craft/Input hypotheses on new tools/tasks and systematically avoid repeated failed attempts. Incorporating tool feedback into a structured task list increases eventual success within multiple trials (EscapeAgent achieves higher overall success rates within 10 trials), reduces hint usage, and shortens steps to next key progress; however, there is no explicit path-search/navigation planner and overall performance still lags humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Textworld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback <em>(Rating: 2)</em></li>
                <li>Webarena: A realistic web environment for building autonomous agents <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 2)</em></li>
                <li>Webshop: Towards scalable real-world web interaction with grounded language agents <em>(Rating: 1)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 1)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-765",
    "paper_id": "paper-278904595",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "BaseAgent",
            "name_full": "BaseAgent (Chain-of-Thought baseline)",
            "brief_description": "A baseline LM agent that acts in the EscapeBench text-based room-escape environments using Chain-of-Thought reasoning and a short working memory to select actions from a predefined action space (move, click, apply, input, craft). It receives textual environment feedback and updates working memory after each step.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "BaseAgent",
            "agent_description": "The BaseAgent is an LLM-driven agent that (1) receives the current scene description, list of interactable items, tools in bag, and a working-memory of recent steps (memory length set to 10), (2) uses explicit Chain-of-Thought prompting to reason about the next action, and (3) issues one of five actions (move, click, apply, input, craft). After each action it appends the environment feedback to working memory. It is implemented as the standard evaluation baseline in EscapeBench and is instantiated with various backbone LMs (GPT-4o, Llama-3.1-70B, Qwen-2.5, etc.).",
            "environment_name": "EscapeBench (room-escape text environments)",
            "environment_description": "A suite of 36 text-based room-escape games where each scene contains items and collectible tools; scenes are connected as a graph (adjacent scenes via doors or tunnels). Observations are local textual descriptions and environment feedback is sparse (especially in Hard mode). Challenges include uncertain goal pathways, very long required action chains (100+ steps), creative tool affordance reasoning, crafting, and sparse/noisy feedback.",
            "is_partially_observable": true,
            "external_tools_used": "No external web/search/calculator tools are used. The 'tools' here refer to in-environment collectible objects (keys, rag, wire, etc.) that the agent can Apply or Craft; additionally the system can provide human-annotated hints when the agent is stuck. The BaseAgent uses internal LLM reasoning (Chain-of-Thought) and working-memory as its internal 'tools'.",
            "tool_output_types": "Textual environment feedback: success/failure messages, state changes for items/tools (e.g., item state updates), scene transitions (move -&gt; new scene), and occasional human-provided hint text when provided after long inactivity.",
            "belief_state_mechanism": "A short textual working memory (sliding window of the last ~10 steps) that stores recent actions and environment feedback; no explicit structured world model beyond the immediate scene description. The agent also has access to the annotated scene metadata provided by the engine (scene relations, list of items/tools present).",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "After each action the game engine returns textual feedback which is appended to the working memory; this updated memory is used for the next Chain-of-Thought reasoning step. There is no separate structured belief graph in BaseAgent—belief is maintained as the sequence of recent textual observations and tool/item lists provided by the engine.",
            "planning_approach": "Chain-of-Thought prompting (LLM reasoning) with trial-and-error; local, greedy action selection based on recent memory and scene context. No explicit search-based or model-based planner is used in the BaseAgent.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Discrete graph navigation via explicit move(scene) actions to adjacent scenes given by the engine's scene relations; the agent does not run an explicit shortest-path algorithm (e.g., A*) but decides moves via LLM reasoning over the local scene graph and memory.",
            "performance_with_tools": "Across evaluated backbone models, BaseAgent achieves low progress: the paper reports 'only 15% average progress without hints' in early experiments; example per-model figures include GPT-4o BaseAgent: Hints Used 9.83, Total Steps 707.33, Early Exit Progress 19.85% (Table values).",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "BaseAgent can collect tools through exploration but often fails to exploit them creatively; its short textual working memory and purely CoT-based decision-making lead to superficial strategies and high hint reliance. It handles local navigation via move actions on the scene graph but lacks structured belief or explicit tool-hypothesis mechanisms, resulting in poor performance on creative Apply/Input/Craft key steps.",
            "uuid": "e765.0",
            "source_info": {
                "paper_title": "EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "EscapeAgent",
            "name_full": "EscapeAgent (Foresight + Reflection enhanced agent)",
            "brief_description": "An enhanced LLM-agent architecture built on BaseAgent that adds a Reflection module (task list management) and a Foresight module (explicit hypothesis generation about tool uses) to improve creative tool use and implicit goal identification in EscapeBench.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "EscapeAgent",
            "agent_description": "EscapeAgent composes three components: (1) a BaseAgent that issues actions via Chain-of-Thought reasoning with working memory, (2) a Reflection module downstream of action execution that maintains a structured task list (entries include task index, target item, task description, and failed actions) and supports operations {new, update, delete, none}, and (3) a Foresight module upstream that triggers when a new tool is collected or a new task is created and explicitly hypothesizes possible apply/craft/input/click actions for that tool/task. Foresight can transition the agent into a 'Try Action' mode to sequentially test hypotheses (or 'Free Explore' otherwise). The integration yields an agent that proposes multiple viable actions, avoids repeated failed trials, and keeps track of unsolved subtasks.",
            "environment_name": "EscapeBench (room-escape text environments)",
            "environment_description": "Same EscapeBench as for BaseAgent: 36 text-based room-escape scenarios with items, collectible tools, and scene-graph connectivity; difficulty levels vary by description clarity and feedback granularity. The environment is partially observable (local textual observations only), contains sparse feedback (especially in Hard), and requires long action chains and creative tool affordance reasoning.",
            "is_partially_observable": true,
            "external_tools_used": "No external search/calculator APIs; uses in-environment collectible 'tools' (game objects) and internal modules. Additionally relies on human-annotated hints supplied by the environment after prolonged inactivity. The Foresight/Reflection modules operate over these in-environment tools and textual feedback.",
            "tool_output_types": "Textual environment feedback (success/failure, item state transitions, newly available tools/items, scene transitions), and human-provided hint text when triggered; Foresight's hypotheses are internal textual proposals, not separate external tool outputs.",
            "belief_state_mechanism": "A composite textual belief comprising (a) the BaseAgent's working memory (sliding window of recent steps and feedback), (b) a structured Reflection-maintained task list (each task: index, target item, description, failed actions, hints), and (c) the current bag inventory and annotated scene metadata. This hybrid textual+structured task-list serves as the agent's explicit belief representation.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Tool and action results from the environment (textual feedback) are first appended to working memory; Reflection then processes non-move action feedback to add 'new' tasks (when an unsolved goal is identified), 'update' tasks (when an attempted action fails—it records failed actions/hindsight), or 'delete' tasks (when success is achieved). When a new tool is collected or new task is added, Foresight reads the current task list and bag inventory to generate and justify hypothesized apply/craft/click/input actions; these hypotheses guide Try Action trials. Thus belief updates combine sequential memory appends and rule-guided task-list operations.",
            "planning_approach": "LLM Chain-of-Thought reasoning combined with explicit hypothesis-driven trial planning (Foresight) and persistent task bookkeeping (Reflection). Planning is largely local and hypothesis-driven rather than exhaustive graph search; multiple concurrent candidate actions can be proposed and executed in sequence until success.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Graph-based discrete navigation using move(scene) to adjacent scenes defined by the engine. The agent uses LLM reasoning (and knowledge in memory/task list) to decide next moves; there is no explicit path-finding algorithm (A*, Dijkstra) described.",
            "performance_with_tools": "EscapeAgent substantially improves efficiency and reduces hint reliance compared to BaseAgent. Example: GPT-4o with EscapeAgent: Hints Used 5.00, Total Steps 452.00, Early Exit Progress 44.55% (Table 6/5); compared to GPT-4o BaseAgent: Hints 9.83, Steps 707.33, Early Exit Progress 19.85%. The paper reports EscapeAgent reduces hint reliance by nearly 50% and completes games with up to ~40% fewer steps and hints on average.",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Explicit hypothesis generation (Foresight) plus persistent unsolved-task bookkeeping (Reflection) increases creative tool use and reduces redundant trials: agents propose targeted Apply/Craft/Input hypotheses on new tools/tasks and systematically avoid repeated failed attempts. Incorporating tool feedback into a structured task list increases eventual success within multiple trials (EscapeAgent achieves higher overall success rates within 10 trials), reduces hint usage, and shortens steps to next key progress; however, there is no explicit path-search/navigation planner and overall performance still lags humans.",
            "uuid": "e765.1",
            "source_info": {
                "paper_title": "EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Textworld: A learning environment for text-based games",
            "rating": 2,
            "sanitized_title": "textworld_a_learning_environment_for_textbased_games"
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback",
            "rating": 2,
            "sanitized_title": "webgpt_browserassisted_questionanswering_with_human_feedback"
        },
        {
            "paper_title": "Webarena: A realistic web environment for building autonomous agents",
            "rating": 2,
            "sanitized_title": "webarena_a_realistic_web_environment_for_building_autonomous_agents"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "Webshop: Towards scalable real-world web interaction with grounded language agents",
            "rating": 1,
            "sanitized_title": "webshop_towards_scalable_realworld_web_interaction_with_grounded_language_agents"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 1,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        }
    ],
    "cost": 0.014118249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents</p>
<p>Cheng Qian chengq9@illinois.edu 
University of Illinois Urbana-Champaign</p>
<p>Peixuan Han 
University of Illinois Urbana-Champaign</p>
<p>Qinyu Luo 
Johns Hopkins University</p>
<p>Bingxiang He 
Xiusi Chen 
University of Illinois Urbana-Champaign</p>
<p>Yuji Zhang 
University of Illinois Urbana-Champaign</p>
<p>Hongyi Du 
University of Illinois Urbana-Champaign</p>
<p>Jiarui Yao 
University of Illinois Urbana-Champaign</p>
<p>Xiaocheng Yang 
University of Illinois Urbana-Champaign</p>
<p>Denghui Zhang 
Stevens Institute of Technology</p>
<p>Yunzhu Li 
University of Illinois Urbana-Champaign</p>
<p>Columbia University</p>
<p>Heng Ji hengji@illinois.edu 
Siyuan Qi 
University of Illinois Urbana-Champaign</p>
<p>Shuo Chen 
Yexin Li </p>
<p>Junqi Wang, Pring Wong2024Xiangyu Kong, Bangcheng Yang, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang</p>
<p>EscapeBench: Towards Advancing Creative Intelligence of Language Model Agents
67BB89F4CBEE577667DBE119C89E19B0arXiv:2401.10568.
Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments.To address this, we introduce EscapeBench-a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals.Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15% average progress without hints, highlighting their limitations in creativity.To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks).Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence.It navigates and completes games with up to 40% fewer steps and hints, performs robustly across difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies.All the data and codes are released 1 .</p>
<p>Introduction</p>
<p>Building robust language model (LM) agents to perform planning and reasoning has always been a challenging task.Recent efforts have explored how agents could compress and utilize memory (Wang et al., 2023a;Hu et al., 2023;Liu et al., 2023b;Liang et al., 2023b;Wang et al., 2024c;Zhong et al., 2024), perform complex reasoning (Wei et al., 2022;Kojima et al., 2022;Zhou et al., 2023a;Lin et al., 2024;Yao et al., 2023), planning (Wang et al., 2023b;Liu et al., 2023a;Hao et al., 2023;Yao et al., 2024;Zhou et al., 2024a), and reflection (Madaan et al., 2024;Zhang et al., 2024a,b;Miao et al., 2024; Figure 1: An agent with creative thinking should adapt its observation (e.g.hard texture of wood stick) into a novel tool-use strategy (e.g.prying objects open).Dhuliawala et al., 2024) to improve task success rate.Integrating these capabilities, recent lines of work begin to build agents for embodied actions (Zheng et al., 2024;Huang et al., 2024a;Zhu et al., 2023) and tool use (Schick et al., 2023;Qin et al., 2023;Qian et al., 2024;Wang et al., 2025;Qian et al., 2025a) grounded in environments including the Web (Nakano et al., 2021;Furuta et al., 2024;Gur et al., 2024), games (Guo et al., 2023;Xu et al., 2023;Hu et al., 2024), and society (Park et al., 2023;Liu et al., 2023c;Li et al., 2023;Ren et al., 2024).</p>
<p>The surge of LM agent systems also accelerates the development of simulation environments, including tasks like computer-based operations (Yao et al., 2022;Deng et al., 2024;Zhou et al., 2024b;Xie et al., 2024;Liu et al., 2024b), scientific research (Wang et al., 2022;Bran et al., 2023;Boiko et al., 2023;Huang et al., 2023a), and interactive experiences in text-based (Côté et al., 2019;Urbanek et al., 2019;O'Gara, 2023;Wu et al., 2024) or vir-arXiv:2412.13549v2[cs.CL] 24 May 2025 tual sandbox game environments (Lin et al., 2023;BAAI, 2023;Wang et al., 2024a).However, most existing benchmarks are usually goal-oriented, emphasizing models' planning, reasoning, and errorhandling abilities, while overlooking their creativity: the capacity to think innovatively and adapt their observations to new, unstructured scenarios.</p>
<p>Current agents still significantly lack creativity in novel tool use (Zhang et al., 2023), as their training predominantly focuses on memorizing tooltask associations.This emphasis overshadows their ability to explore tool affordances and adapt to unstructured scenarios (Zhang et al., 2024c).Despite this, creativity is still widely recognized as a crucial component of intelligence.In cognitive science, the well-established Triarchic Theory of Intelligence (Sternberg, 1984) divides intelligence into three components: practical, analytical, and creative.While current reasoning benchmarks primarily assess analytical intelligence through problemsolving, and simulation environments focus on practical intelligence by testing knowledge application in real-world scenarios, creative intelligence remains largely unaddressed.</p>
<p>To bridge this gap, we introduce EscapeBench, a benchmark that evaluates LM's creative reasoning using scenarios inspired by room escape games.These scenarios challenge conventional thinking through unusual settings and require "thinking outside the box" skills including creative tool use and strategic problem-solving.As shown in Figure 1, a wooden stick, typically used for walking or poking, has to be repurposed to pry open a lid due to its hard texture.This demands agents to perform adaptive reasoning under customized constraints.Overall, our benchmark has several distinctive features:</p>
<p>• Creative Tool Use: The tools at hand might be repurposed for creative use in order to solve the puzzle.These innovative ways of tool use are uncommon in the LM agent's existing parametric knowledge, requiring it to reason creatively and adapt its observation into customized scenarios.</p>
<p>• Uncertain Goal Pathways: While the final goal of each game is escaping from the room, the pathways to achieving it cannot be explicitly foreseen.</p>
<p>An agent cannot devise precise, long-range plans initially and must rely on trial and error to discover viable strategies.</p>
<p>• Super-Long Reasoning Chain: Each scenario requires even an omniscient agent to perform over 100 steps, with at least 40 bottleneck actions required to achieve the goal.A human player may take up to an hour to complete one game.We benchmarked multiple models within the BaseAgent framework, which incorporates working memory and Chain-of-Thought reasoning (Wei et al., 2022).Our results show that even the best models struggle to complete the easiest game setting without hints, often requiring up to ten times the optimal steps and falling far behind human performance.These findings highlight how models tend to be constrained by conventional thinking patterns, struggling to break free and show creativity.</p>
<p>To overcome this limitation, we introduce Es-capeAgent, enhanced with Foresight for creative tool use and Reflection for implicit goal identification.Foresight enables the agent to propose and evaluate tool-use hypotheses before acting, while Reflection maintains an unsolved task list to guide future actions.Experiments show EscapeAgent reduces hint reliance by nearly 50%, lowers total action steps, and performs robustly across difficulty levels, achieving more efficient progress and higher action success rates with creative strategies.Our contributions include the following: • We identify challenges in LLM agent creative intelligence and introduce EscapeBench, a robust environment for evaluating agent creativity.• We present EscapeAgent, which boosts creative reasoning by identifying implicit goals and generating innovative hypotheses.• We propose measuring creativity through tool use and crafting, and introduce new metrics that provide a fresh dimension for agent evaluation.</p>
<p>Related Work</p>
<p>Creativity in Language Models.Creativity is a cornerstone of human intelligence and a growing focus in AI research (Legg and Hutter, 2007;Lake et al., 2017).LMs have demonstrated notable creative capabilities across domains -they excel at generating narratives and poetry (Brown et al., 2020;Akoury et al., 2020), show effectiveness in tool creation and design (Qian et al., 2023;Cai et al., 2024), model real-world challenges (Qian et al., 2025b), and augment human creativity through interactive ideation (Mialon et al., 2023).In scientific discovery, research has also found that LM-generated ideas tend to be more novel but slightly less feasible than those from human experts (Si et al., 2024;Wang et al., 2024b).However, research on LM creativity still remains nascent, emphasizing novelty, surprise, and practical value through psychological assessments like the Alternative Uses Test (AUT) (Guilford, 1967) and Torrance Tests of Creative Thinking (TTCT) (Boden, 1998).Creativity in LMs is categorized as combinatorial, exploratory, or transformational, with transformational being the most challenging (Franceschelli and Musolesi, 2023).A TTCT study found GPT-4 performing in the top 1% of human fluency and originality, but adapting such assessments to other LMs faces limitations like sample randomness and high evaluation costs (Guzik et al., 2023).Similarly, a modified Torrance Test (Zhao et al., 2024) identified strengths in elaboration and originality but highlighted gaps influenced by prompts and role-play.Notably, most research evaluates backbone models, whereas our work explores creativity within an LM agent-based setting that requires complex reasoning and planning.</p>
<p>Agent Evaluation in Simulated Environment.</p>
<p>Agent evaluation often focuses on text-based or sandbox environments for assessing cognitive and behavioral abilities in goal-oriented tasks (Zhou et al., 2023b;Chen et al., 2022Chen et al., , 2024;;Yu et al., 2024;Deng et al., 2024), with emerging work exploring LM/VLM-enabled agents in robotics for real-world challenges (Liang et al., 2023a;Huang et al., 2023bHuang et al., , 2024b;;Rana et al., 2023;Zhu et al., 2025;Yang et al., 2025).Text-based environments (Yuan et al., 2018;Côté et al., 2019), such as interactive fiction games (Lin et al., 2024) or conversational agents (Qiao et al., 2023), evaluate natural language understanding, reasoning, and decisionmaking consistency (Uludaglı and Oguz, 2023;Qi et al., 2024).Games like Zork (Infocom, 1980) and TextWorld measure narrative comprehension and problem-solving in structured contexts.In contrast, sandbox environments (Lin et al., 2023;Gan et al., 2021;Fan et al., 2022) like Minecraft (Zhu et al., 2023) andRoblox (Rospigliosi, 2022) provide more open-ended settings that test spatial reasoning, planning, and collaboration (Carroll et al., 2019;Agashe et al., 2023).The settings typically rely on task-specific metrics for goal achievement but overlook creative and proactive problemsolving in unfamiliar contexts.To address this, we introduce EscapeBench to evaluate agents' creative reasoning in navigating uncertain goal pathways, offering a novel approach to agent assessment.</p>
<p>EscapeBench Construction</p>
<p>Most agent benchmarks focus on explicit, goaloriented tasks grounded in commonsense knowledge, where agents can chart clear pathways to achieve goals using analytical and practical intelligence, but they often overlook creative intelligence.This raises our core research question: How to build an environment that benchmarks an agent's creative intelligence?Given that tool use is central to agent functionality, we propose room escape game scenarios, which naturally require creative tool use to solve complex puzzles, as an ideal environment for this evaluation.</p>
<p>Engine Design</p>
<p>Our game engine aims to simulate the room escape environment that i) receives agent actions and ii) makes corresponding environment feedback as agent action's reward.Specifically, our game engine involves three key components:</p>
<p>• Scenes: The container of tools and items, connected with each other forming a graph structure that constitutes the whole game scenario.• Items: Objects that are intractable in each scene.</p>
<p>Tools, inputs, and other interactions may be applied to trigger its state change or other effects.• Tools: Objects that could be collected in each scene, usually applied to other items to take effect or to other tools to craft new ones.The interaction of these components defines a game's basic logic.In the van example from Figure 2, scenes are connected into a graph, representing physical connectivity via doors or tunnels.The tool key chain is collected in the bag for future use, while the wire iron awaits something sharp to cut it to trigger effects.Please refer to Appendix A for more detailed examples and explanations.</p>
<p>Action Space</p>
<p>The model agent could take five different actions.While the action space is well-defined, the parameter space-regarding the scenes, items, or tools involved in these actions-is high-dimensional, thus allowing for dynamic interactions.</p>
<p>• Move (Scene): Move to an adjacent scene.</p>
<p>• Click (Item): Click to simply interact with an item in the scene.• Apply (Tool, Item): Apply a tool in the bag to an item in the scene.• Input (str, Item): Input an arbitrary string to an item in the scene.</p>
<p>Figure 2: An illustration of Scenes, Tools, and Items in the game and their relations with agent action space.Tools can be collected for "Apply" and "Craft", while items require "Input", "Click" or "Apply" of tools to trigger effects.• Craft (Tool, Tool): Use two tools in the bag to craft a new one.Figure 2 illustrates the connections between game engine components and agent action space.Among all the actions, "Apply" and "Craft" stand out as the most creativity-driven, as they require the agent to think innovatively about how to use or craft tools in an unseen way during its training.We delve into specific examples in Section 3.4.</p>
<p>Annotation and Statistics</p>
<p>Building on existing online room escape games and puzzle-solving logic 2 , we introduce EscapeBench, featuring 36 game settings spanning three difficulty levels.Each scenario includes three versions with consistent solution logic but varying in description clarity and feedback granularity.A detailed example is shown in Table 1.All scenes, items, and tools are manually annotated to ensure high quality.These scenarios emphasize creative tool use and crafting strategies, challenging agents throughout 2 https://spotlight.ee the game, making EscapeBench a robust environment for testing creativity.A detailed statistic is presented in Figure 3.</p>
<p>Preliminary Study</p>
<p>We sample scenarios from EscapeBench and test GPT-4o (Hurst et al., 2024) and LLama-3.1-70B(Dubey et al., 2024)'s creative reasoning performance through case studies in Table 2. Our results reveal that: i) EscapeBench presents diverse creative reasoning challenges, including unconventional tool use, implicit numerical puzzles, and innovative tool crafting.ii) Both closed-and opensource models struggle with creativity, especially in identifying implicit goals and forming creative strategies.These findings highlight the complexity of EscapeBench and the gaps in model creativity.</p>
<p>EscapeAgent Design</p>
<p>To address challenges identified in the preliminary study, we introduce EscapeAgent, a framework addressing two core issues from EscapeBench: • Uncertain Goal Pathways: A Reflection module dynamically manages a task list, refining goals through trial-and-error to enhance action focus and proactive task discovery (Section 4.2).• Creative Tool Use: A Foresight module enabling explicit reasoning about tool applications, allowing the agent to hypothesize and evaluate strategies before execution (Section 4.3).Integrating both modules with a BaseAgent, Es-capeAgent excels in handling Super-long Reasoning Chains and significantly boosts the model's creativity, problem-solving, and strategic thinking.The BaseAgent employs Chain-of-Thought (Wei et al., 2022) reasoning to decide its next action.Note that it serves as a strong baseline and standard evaluation method for EscapeBench, as it already incorporates all the essential components an agent needs for reasoning.For more implementation details, please refer to Appendix C.1.</p>
<p>Reflection Module</p>
<p>The Reflection Module manages a structured task list updated through three actions:</p>
<p>• New: Add a newly identified, unsolved task.</p>
<p>• Update: Record attempted but failed actions.</p>
<p>• Delete: Remove a task when its goal is achieved.Each entry includes the task name, target item, and failed actions, preventing repeated mistakes and improving efficiency.Triggered after nonmove actions, it uses feedback to update the task list.For example, in Figure 4 (right), Task 1 is deleted once the machine starts, encouraging focused problem-solving over random exploration.See Appendix C.2 for details.</p>
<p>Foresight Module</p>
<p>The Foresight Module enhances creative reasoning by explicitly evaluating tool use and problemsolving strategies.It activates in two cases:</p>
<p>• New Task Identified: The agent hypothesizes potential actions to achieve it using available tools.• New Tool Collected: The agent assesses its use for solving existing tasks or crafting new tools.If a valid hypothesis is proposed, the agent enters "Try Action" state to test it; otherwise, it stays in "Free Explore" state, operating like the BaseAgent.For example in Figure 4 (left), the agent identifies clicking the button as an action worth trying, thus guiding it into the "Try Action" state.It enables the agent to adapt flexibly under customized scenarios, make bold hypotheses, and execute targeted trials efficiently.See Appendix C.2 for details.</p>
<p>Experiments</p>
<p>We divide experiments into: i) Benchmarking model creativity within the BaseAgent, and ii) Evaluating EscapeAgent's effectiveness.</p>
<p>Settings</p>
<p>Environment.Experiments are conducted on 36 game settings.An agent is considered to be making progress if it either achieves a key step (defined in Figure 3) or collects a new tool.Agents will receive help if they fail to make progress for 50 consecutive actions (see Appendix D.1), thus ensuring full completion of the game.The working memory length is set to 10.</p>
<p>Models.We evaluate both closed-and opensource models: GPT-4o, GPT-4o-mini (Hurst et al., 2024), Claude-3.5 (Anthropic, 2024), Gemini-1.5 (Team et al., 2024), Llama-3.1 (Dubey et al., 2024), Qwen-2.5 (Team, 2024), DeepSeek-LLM (Liu et al., 2024a), Phi-3.5 (Abdin et al., 2024), Yi (Young et al., 2024), Ministral (Mis-tralAI, 2024).Models with fewer than 7B parameters are excluded due to near-random behavior.For consistency, we set sampling temperature to T = 0 and n = 1.</p>
<p>Metrics.We use two main metrics including: • Hints Used: Total hints used in a game.for key steps (normalized by total key steps).Results are micro-averaged across the 36 settings.</p>
<p>Benchmarking Results</p>
<p>We benchmark current models using the BaseAgent framework, with results in Table 3 showing that large-scale closed-source models consistently outperform smaller models.Key insights include:</p>
<p>• Most hints are used on key steps, which demand creative reasoning, while models may often collect tools through random exploration.• Models require significantly more action steps and hints than the average human, and up to 20x more steps than the most efficient action chain.We further present an ablation study of total hints and steps used in Appendix D.3.</p>
<p>Input and craft are the most challenging actions.</p>
<p>As shown in Figure 5, while "Apply" actions require the most hints in absolute terms, "Input" and "Craft" actions have the highest relative hint usage compared to the total number of key steps.This likely reflects the large parametric space of "Input" actions, where random guesses are impractical, and the creativity-demanding nature of "Craft" actions.</p>
<p>Error analysis.We observe from Table 4 that the BaseAgent often gets stuck and relies on hints due to its struggles with environment-following and    creativity.Smaller models tend to perform invalid actions in complex scenarios, while larger models excel at tool collection but fail to attempt creative actions, resorting to superficial strategies.Our Es-capeAgent addresses this by promoting more purposeful and creative actions.</p>
<p>EscapeAgent Results</p>
<p>The introduction of the Reflection and Foresight in EscapeAgent, as shown in Table 5, significantly reduces hint uses and total steps, with larger models benefiting the most.Key insights include:</p>
<p>• Larger models still outperform smaller ones, suggesting while new modules aid creative reasoning, the core model's capabilities remain crucial.</p>
<p>• Early exit progress improves across models, despite the exponentially increasing difficulty of consecutively making progress without hints, demonstrating EscapeAgent's effectiveness.</p>
<p>EscapeAgent progresses more efficiently.As shown in Figure 6, EscapeAgent demonstrates steeper progress slopes, reflecting greater efficiency.Figure 8 further shows that EscapeAgent requires fewer actions to reach the next key step, indicating stronger creative reasoning ability.The spike at 50 steps corresponds to hints provided after prolonged inactivity, so the lower pink bar here further highlights EscapeAgent's reduced hint dependency.</p>
<p>Notably, across all models, progress after 15 steps without hints is rare, underscoring a lack of spontaneous insights typically seen in humans.</p>
<p>Case study.Figure 7 illustrates agent progress relative to action steps across six game settings, with red dots marking hint provided.We observe that: i) EscapeAgent requires fewer hints and achieves steeper progress; ii) it can make consecutive progress in shorter intervals; iii) harder scenarios remain challenging, especially for BaseAgent, which heavily relies on hints; iv) Average human performs far better.Humans rarely make mistakes shown in Table 4, while agents still struggle with short memory due to context length and creative tool use strategies.These emphasize the need for further improvements and highlight EscapeBench's challenge to even the most advanced models.</p>
<p>6 Further Analysis</p>
<p>Evaluation across Diverse Models</p>
<p>To enhance the diversity of model scales and domains in our evaluation, we present additional experimental results on smaller-scale and domainspecific models.These results supplement our main findings and provide further insights into the scalability and generalizability of our framework.</p>
<p>In addition to the models presented in out main table, we also evaluated sub-7B models, including the latest Llama-3.2-1Band 3B variants.However, these models demonstrated extremely limited performance.In even the simplest game scenarios under normal difficulty, these models frequently resorted to random guessing, with over 95% of key  actions requiring hint usage.The inefficiency translated into significantly inflated action sequences and minimal task completion, leading us to our conclusion that 7B is the minimum viable scale for meaningful evaluation in our benchmark.</p>
<p>To examine domain specialization effects, we benchmarked two variants from the Qwen-2.5 model series, focused respectively on coding and mathematical tasks.As shown in Table 6, the Qwen-2.5-Coder-7Bachieved performance close to the general-purpose 7B-Instruct model, indicating effective domain adaptation without significant loss of reasoning flexibility.In contrast, the Qwen-2.5-Math-7Bmodel struggled, failing to initiate or complete key actions autonomously.This suggests that excessive specialization, such as strict math alignment, may hinder the model's ability to generalize and engage in creative, multi-step tasks.</p>
<p>Ablation Study on Key Modules</p>
<p>To evaluate the individual contributions of the Foresight and Reflection modules in EscapeAgent, we conducted an ablation study using two modified agent variants: • EscapeAgent (only Foresight): Disables the task list maintained by the Reflection module, allowing action proposals based solely on environmental observation.• EscapeAgent (only Reflection): Disables the Foresight module, relying instead on the task list to guide decision-making.</p>
<p>We benchmarked these variants alongside the full EscapeAgent and the BaseAgent across all Although EscapeAgent uses 40% fewer hints and makes significant progress independently, it still falls far short of average human performance, often requiring twice as many total steps to complete a game.game scenarios with normal difficulty.Across both GPT-4o and Llama-3.1-70B in Table 7, each module independently improves performance relative to the BaseAgent.The full EscapeAgent consistently achieves the best results, confirming that the Foresight and Reflection modules are complementary.Notably, Reflection alone often outperforms Foresight alone, likely because the Foresight module relies on the task list to avoid redundant or failed actions.These results support the conclusion that combining both modules is essential to fully realize EscapeAgent's potential.</p>
<p>7 Discussions and Future Directions LM's creativity for benchmarking.Our experiments reveal that even the most advanced language models within EscapeAgent require more hints and twice as many steps as the average human, exposing limitations in creative reasoning and tool use.The benchmark highlights that while analytical and practical intelligence is well-assessed, creative intelligence remains a critical gap.Addressing this gap may require enhancing LMs to link knowledge and objects through affordances-their properties and functions-to foster creativity.</p>
<p>Theoretical Foundations for AI Creativity.Human creativity, characterized by generating novel ideas and adapting to complexity, arises from the interplay of stochastic neuronal noise and structured, learned information (Dainys, 2024;Malach, 2024).In contrast, AI creativity relies on trained data patterns and algorithms.Boden identifies three mechanisms driving AI creativity: combining familiar ideas, exploring conceptual spaces, and enabling transformative innovations (Boden, 1998).Integrating insights from psychology and neuroscience may further enhance AI's creativity.</p>
<p>Human-AI Collaboration.Human-AI collaboration in EscapeBench may promote a new problem-solving paradigm by merging human intuition with AI's systematic reasoning.Humans bring unique insights and ideas that AI might not generate, while AI excels in tasks like information aggregation and logical organization.This synergy fosters innovative strategies, improves efficiency, and creates opportunities for deeper learning, offering a dynamic and enriched problem-solving experience that bridges human creativity with AI's structured problem-solving capabilities.</p>
<p>Conclusion</p>
<p>In this work, we introduce EscapeBench, the first benchmark for advancing LM's creativity.Our results show that while LMs still lag in creative reasoning, the EscapeAgent framework improves innovative problem-solving and implicit goal identification.Despite these advancements, enhancing the models' intrinsic creativity remains a challenge.Future work could explore integrating multi-modal perception and new reinforcement learning algorithms to foster greater creativity.Our work serves as an important first step, offering a robust environment for experimentation.Looking ahead, we believe that creative intelligence, beyond just analytical and practical capabilities, will play a key role in shaping the frontier of AI.</p>
<p>Limitations</p>
<p>Our work utilizes a text-based environment to evaluate common language models, focusing on creative reasoning within this framework.However, an Escape Room scenario inherently includes visual and auditory clues, which we have not incorporated into this benchmark.Expanding to include multi-modal inputs could be a valuable next step for future work.Additionally, while the data used in our benchmark is annotated through intensive human effort to ensure high quality, this approach limits scalability.We have explored the use of GPT-4 for automatic annotation through free exploration but found that the model sometimes overlooks important items and clues, and struggles to design environment feedback crucial for adjusting the game's difficulty.We anticipate that more powerful vision-language models may enable better automatic annotation in the future, though current model capabilities are still a limiting factor.</p>
<p>Ethical Statement</p>
<p>In this research, we consider the following ethical issues related to our benchmark and agent design:</p>
<p>• Fairness: We ensure that EscapeBench is designed to provide equal evaluation opportunities for all agents, regardless of their underlying model architectures or training methodologies.The tasks and scenarios are crafted to assess creativity and problem-solving abilities without bias, promoting fairness in the benchmarking process.Additionally, we aim to avoid overfitting to specific agent strategies, ensuring a more generalizable and inclusive evaluation framework for future AI advancements.While our environment is robust, we caution against potential misuse and strongly encourage its fair and responsible use.</p>
<p>• Transparency: Our work incorporates Chain-of-Thought reasoning in the BaseAgent framework to improve the transparency and interpretability of the agent's decision-making process.This approach makes it easier to attribute the reasoning behind each agent's action.Additionally, we will fully release the benchmarking code, EscapeAgent design, and data to promote transparency in our evaluation process, ensuring that the broader research community can benefit from and build upon our work.</p>
<p>The game engine involves scenes, tools, and items as three main components.We will introduce in detail each of them in the following.</p>
<p>Scene.A scene typically includes a description, its connections to other scenes, and the tools and items it contains.A typical scene example in the game configuration looks like the following: ... tools :</p>
<p>Scene Example</p>
<p>...</p>
<p>In this example, the name of this scene is "hallway".It leads to nearby scenes including "blocked path close-up" and "cabinet close-up", where the model could reach through action "move(To the blocked path close-up)" and "move(To the cabinet closeup)".</p>
<p>Tool.Each tool has various states and a visibility status.In each state, a tool is either awaiting the application of another tool or ready to be applied to another tool or item.A typical tool example in the game configuration looks like the following: For instance, a digital lock (item) may await the application of card (tool) for authorization and correct password input to trigger the closed cabinet door (item, state 1) to open (item, state 2).</p>
<p>B Data Annotation Details</p>
<p>We recruited eight human annotators, all with prior Room Escape game experience (offline and online) and at least a bachelor's degree.To ensure a smooth annotation process, all annotators were U.S.-based students with computer science backgrounds.Each annotator received detailed guidelines to ensure objective annotations and was tasked with extracting game logic and object descriptions (scenes, items, and tools) based on the official guide.The foundational data logic will be released with the software, and all annotators consented to the data collection.</p>
<p>C Agent Design Details</p>
<p>For both BaseAgent and EscapeAgent, we apply the same system prompt for its action-taking to ensure fairness:</p>
<p>System Instruction</p>
<p>You are in a Room Escape game .You should explore the scene and find out what to do next .There are three types of interactives : items , which are the interactable things in the scene ; tools , which are applicable tools in your bag ; scenes , whcih are interactable scenes near your current position .</p>
<p>You can perform one of the following actions : -click ( &lt; interactable item &gt;) : Click an &lt; interactable item &gt; to examine it or interact with it .For example , you can examine a door handle that is marked as interactable .</p>
<p>-apply ( &lt; applicable tool &gt; , &lt; interactable item &gt;) : Apply an &lt; applicable tool &gt; in your bag to an &lt; interactable item &gt;.For example , you can apply a key in your bag to an interactable locked door to open it .</p>
<p>-input ( string , &lt; interactable item &gt;) : Input a string ( only digits and letters ) to an &lt; interactable item &gt;.For example , you can input a string password to an interactable combination lock .</p>
<p>-move (&lt; interactable scene &gt;) : Move to a nearby &lt; interactable item &gt; to further explore .For example , you can move to the living room to explore more interactable items there .</p>
<p>-craft (&lt; applicable tool &gt; , &lt; applicable tool &gt;) : Use one &lt; applicable tool &gt; in bag to another &lt; applicable tool &gt; in bag to craft something new .For example , you can use a battery in bag to a controller in bag to craft a new charged controller .For instance , some valid actions may be : click ( microwave ) , apply ( key , silver chest ) , craft ( controller , battery ) , input ( c79a1 , combination lock ) , move ( Go to operation room ) .</p>
<p>The system prompts explicit instructs on the agent's action space with examples.In the following, we present in this section more details on EscapeAgent design, including BaseAgent, Reflection, and Foresight modules.</p>
<p>C.1 BaseAgent Details</p>
<p>At each step, the BaseAgent receives information from the game engine.This information typically includes an environment description, a list of interactable objects in the scene, and the tools available in the agent's bag.A typical environment description appears as follows:</p>
<p>Scene Description : You are in the scene ' underneath part of the van '.There is a stepladder on the right side .There is a license plate on the left side .</p>
<p>Here are the items you can see in this scene : -On the left side , there is license plate space : The license plate is currently fixed to the van , with four screws on each corner -On the right side , there is stepladder : The stepladder is unfolded , now you can reach the top of the van ,</p>
<p>Possible Actions :</p>
<p>Here are all the items in the scene that you can perform ' click ', ' apply ' or ' input ': &lt; interactable item &gt; license plate space Here are nearby scenes that you can perform ' move ' to further explore : &lt; interactable scene &gt; Back to the back of the van : It leads to back of the van Tools in Bag :</p>
<p>Here are the tools in your bag .You can perform ' craft ' to use two tools in your bag to craft a new one , or perfom ' apply ' to apply one tool in your bag to an object in the scene : &lt; applicable tool &gt; bunch of keys : a bunch of keys with a keychain and some rust on it &lt; applicable tool &gt; rag : a rag soaked with engine oil "</p>
<p>The scene typically includes a general description, while each item within the scene is accompanied by a detailed description.Possible actions specify which items or aspects of the scene the agent can interact with, and tools in the bag indicate which tools are available for use.</p>
<p>This environment description will also be coupled with working memory of previous steps.Each step's memory contains the following fields: Using this information, the BaseAgent is instructed to explicitly apply a Chain-of-Thought reasoning process to determine its next action.This action is then parsed and sent back to the game engine.</p>
<p>The game engine updates its state based on the agent's input and provides feedback to the agent.By default, this feedback indicates whether the action was successful or not.In Easy and Normal game settings, additional customized environment feedback is provided.However, in the Hard game setting, no extra information is given.</p>
<p>C.2 Reflection Module Details</p>
<p>The Reflection module is integrated as a downstream component after BaseAgent within the Es-capeAgent design.This module is responsible for maintaining a task list that is updated solely based on the agent's current actions and the feedback received.Each task in the list generally includes the following fields: The task index facilitates the identification of specific tasks during task list management operations.The target item specifies the item in the scene that the task is focused on, enhancing the agent's sense of purpose when exploring and performing trials within the scene.The task description outlines a potential strategy for solving the task, including actions the agent has previously attempted but failed.</p>
<p>The following system prompt is used to guide the model:</p>
<p>System Instruction</p>
<p>You are a helpful agent to reflect on your action and environment response , and then maintain a task list with solving suggestions .The role of this task list is that there are some tasks you currently cannot solve with the tools at hand , but you think you may need to solve them later , so write them down with some suggestions and hints for your future reference .</p>
<p>After analyzing your current action and the response from the environment , you should give an action to maintain the task list : update ,new , delete or none .Note that the operations "update" and "delete" can be implemented in a rule-based manner.An update is triggered when the model attempts an action on a specific item and fails, requiring revised feedback.A delete occurs when the model successfully performs an action that advances progress, necessitating the removal of the corresponding task, if it exists, from the task list.</p>
<p>C.3 Foresight Module Details</p>
<p>The Foresight module serves as an upstream component preceding the BaseAgent in the Es-capeAgent design.This module is activated when a new tool is collected during the last action step or a new task is added during the previous reflection step.When a new tool is collected, the agent is provided with the current task list and instructed to propose potential applications for the tool within the context of these tasks and their specific scenarios.Additionally, the agent is given a list of all existing tools in its bag and encouraged to creatively evaluate whether the new tool could be combined with others to craft something useful.The following system prompt is employed to guide the model:</p>
<p>System Instruction</p>
<p>You are in a Room Escape game .You have to use your creativity to figure out the use of the tool you have just collected .There are generally two ways about how to use the tool : 1. Combine this tool with another one in your bag to craft a new tool .In this case , use acton ' craft ( &lt; collected tool &gt; , &lt; applicable tool &gt;) ', e .g .craft ( controller , battery ) indicates use a battery in your bag you already have to the controller you just collected to craft a charged controller .2. Apply this tool to a target item in a task to try solve this task .In this case , use action ' apply ( &lt; collected tool &gt; , Target Item in a task ) ', e .g .apply ( key , locked cabinet ) indicates apply the key you just collected to a locked cabinet to open it .</p>
<p>Here are some general hints that you may follow : 1. Please especially pay attention to the description of the task and the tool , try to find the connection between them to justify your action .2. In your '-Thought : ... ' part in response , you shuold explicitly think about whether there 's item in bag for crafting , or task in the list for applying this tool .You should read and infer carefully from the tool descriptions and the task description , and evaluate one by one .3. In your '-Actions : ... ' part in response , you should give zero to multiple action calls .For each action , you should follow the format ' craft ( &lt; collected tool &gt; , &lt; applicable tool &gt;) ' or ' apply ( &lt; collected tool &gt; , Target Item in a task ) '.If it 's a craft action , you should justify why crafting here makes sense .If it 's an apply action , you should first give the task index corresponding to the target item , then justify why this tool may solve the task .</p>
<p>If a new task is created, the agent is provided with a list of all the tools currently in its bag.It is then instructed to reason creatively about which tools could be applied to address the newly created task.The following system prompt is used to guide the model:</p>
<p>System Instruction</p>
<p>You are in a Room Escape game .You have to use your creativity to figure out if you could use any tools you have now to solve a new task you have just discovered .There are generally three ways to solve a task : 1. Click the target item to simply interact with it to solve the task .In this case , use action ' click ( Target Item in current task ) ', e . g . click ( microwave ) indicates click the microwave to examine it and try solve the task .2. Use the tool in your bag to apply to the target item in the task .In this case , use action ' apply (&lt; applicable tool &gt; , Target Item in current task ) ', e.g .apply ( key , locked cabinet ) indicates apply the key in your bag to a locked cabinet to open it .3. Input a string to the target item in the task .In this case , use action ' input ( &lt; any string &gt; , Target Item in current task ) ', e . g . input (2413 , combination lock ) indicates input a string password to the combination lock to solve the task .</p>
<p>Here are some general hints that you may follow : 1. Please especially pay attention to the description of the task about what might be needed .Please always first try simple click to interact if haven ' t done so .Examine the tool description and your memory pad , try to find the connection between them and what this task needs to justify your action .2. In your '-Thought : ... ' part in response , you should explicitly think about whether there 's item to click , tool in bag for applying , or hint from memory pad and tools for string input .You should read and infer carefully from the task description , evaluate one by one .3. In your '-Actions : ... ' part in response , you should give zero to multiple action calls .For each action , you should follow the format ' click ( Target Item in current task ) ', ' apply ( &lt; applicable tool &gt; , Target Item in current task ) ', or ' input ( &lt; any string &gt; , Target Item in current task ) '.You shuold justify why this action may solve the task according to the task description , tool description , and memory pad hint .</p>
<p>Depending on whether the model proposes a valid action, the agent transitions into either the "Free Explore" or "Try Action" state.If multiple actions are proposed, the agent attempts them sequentially until a successful action is achieved.In cases where the task cannot be solved with the currently available tools, the task remains on the task list, and the newly acquired tool stays in the bag.For consistency, all action trials are included in the total count of action steps.</p>
<p>D More Experiment Details</p>
<p>D.1 Help Setting</p>
<p>We provide help to the agent through explicit instructions, focusing on two aspects: i) identifying the next action location that could help the model make progress, and ii) specifying the action the model should take.These objectives are addressed by providing the instruction below to the model during the action-taking step:</p>
<p>Action Instruction</p>
<p>Since you ' re stuck , the system will provide you with a hint .You MUST follow the hint to complete next key step .The next target location should be : &lt; target position &gt;.Your next target action should be : &lt; target action &gt;.You should go to the target position and perform the target action .If you are already at the target location , please directly perform the action .This help will remain available until the model successfully performs the target action.All the helps on how to complete the game is provided through human annotation.</p>
<p>In each game, there may be cases where multiple actions can be taken in parallel since they do not interfere with one another.As a result, there is no fixed sequence among them.However, to simplify the help provided, the actions are linearized into a single action chain that ensures the agent can complete the game.Whenever the agent requires help, we always provide the first action in this chain that the model has not completed, even if the model may have already succeeded in performing some later actions in the chain.</p>
<p>D.2 Resource Setting</p>
<p>For closed-source models, we utilize standard APIs for testing.Under the BaseAgent framework, the average number of API calls per game setting is approximately 800, resulting in a total cost of $50-60 per model test.While the EscapeAgent framework introduces two additional modules, these are not always triggered.Consequently, the total number of API calls increases to roughly 1.2 times that of BaseAgent, raising the cost to approximately $60-80 per model test.</p>
<p>For open-source models, all benchmarks are conducted using the vLLM framework on 2 A100-80G GPUs.Inference time varies based on model size: smaller-scale models complete all 36 game settings in approximately 12 hours, while larger 70B-scale models require about twice as much time for benchmarking.The average tool-calling frequency, reflected in the Total Steps metric, is reported in Table 3 for BaseAgent and Table 5 for EscapeAgent.These metrics vary significantly depending on the specific open-source models being tested.</p>
<p>D.3 Ablation Study</p>
<p>We perform an ablation study on the total steps and used hints in Figure 12 and Figure 13.Generally, harder game settings require more steps and hints for an agent to solve.Since our difficulty setting depends solely on the granularity and usefulness of descriptions and feedback (see Table 1), our results demonstrate that the way the environment is presented can impact difficulty, even when the core game logic remains unchanged.</p>
<p>D.4 Further Analysis</p>
<p>We further analyze the valid actions agents attempt on different items in scenes before successfully operating them in Figure 14.While the BaseAgent exhibits a higher first-attempt success rate, Es-capeAgent achieves greater effectiveness by making multiple attempts, leading to a significantly higher overall success rate within 10 trials.This difference can be attributed to EscapeAgent's strategy of proposing multiple viable actions simultaneously.Although this increases the likelihood of eventually succeeding by trying more actions, it does not prioritize the most-likely-to-succeed action first.As a result, the BaseAgent appears more efficient on its initial trial, but its superficial approach, as highlighted in Table 4, limits its overall performance.The EscapeAgent's design effectively addresses this limitation by leveraging a more exploratory approach, which proves advantageous in complex scenarios requiring creativity.</p>
<p>D.5 More Study Results</p>
<p>In Figure 9, we present additional results on the action step intervals with respect to two types of progress-making ways, achieving Key Step or Tool Collection.It can be observed that EscapeAgent consistently requires fewer steps to perform the   next bottleneck Key Step.In contrast, for Tool Collection, the difference between the two bars is less pronounced.Despite this, the findings still demonstrate the effectiveness of our design.Tool Collection typically occurs after the successful application or crafting of tools, meaning the reasoning challenge associated with it is significantly lower.</p>
<p>Since EscapeAgent focuses primarily on creative reasoning, it is reasonable that it excels in identifying the next Key Action more efficiently.</p>
<p>In Figure 10, we present the results of action trial counts for a specific item across four additional models.We observe that EscapeAgent achieves a higher success rate within 10 trials, even though   it does not always succeed on the first attempt.Furthermore, for smaller models like Phi-3 and Ministral, EscapeAgent occasionally outperforms BaseAgent even in terms of one-trial success rates.This highlights how our framework effectively lowers the barriers to creative reasoning, even for less capable language models.</p>
<p>In Figure 15, we showcase eight additional pairs of progress-making maps for eight more models.These case studies illustrate two key points: i) There are significant disparities in creativity among models.For instance, models like GPT-4o-mini and Qwen-2.5-72Brequire only two-thirds of the total steps than others to achieve success, while smaller models such as Qwen-2.5-7Bheavily rely on hints to make progress, even with the Es-capeAgent framework.ii) When combining these results with Table 5, we observe that EscapeAgent's performance improvement relative to BaseAgent is more pronounced for larger-scale models like GPT-4o and 70B-scale models.Conversely, while smaller models also benefit from reduced steps and hint usage, they still lag significantly behind in creative intelligence.This underscores the importance of enhancing a model's intrinsic reasoning abilities, as our method primarily mitigates the barriers to creative reasoning but does not fully address inherent limitations.</p>
<p>Lastly, Figure 11 depicts the progress curves across all 36 game settings for GPT-4o and LLama-3.1-70B-Instruct.The trends reveal that BaseAgent's total step distribution spans a broader range compared to EscapeAgent, resulting in a relatively milder and less steep progression curve.These findings further confirm the effectiveness of EscapeAgent in facilitating more efficient progress.</p>
<p>E Human Performance Details</p>
<p>To better understand the "Average Human" baseline we present in the main table, we conducted a postexperiment survey with all participants, including: • Suggestions for improving EscapeBench Results indicated that nearly all participants had prior experience, with an average of 5.6 offline sessions.However, only 20% identified as skilled players; 40% selected "Might Be" and another 40% "Not Really."Despite varying self-assessments, game logs showed no significant performance difference across groups-likely due to the trial-anderror nature of the tasks.</p>
<p>Although a large-scale user study is beyond our current scope, these findings suggest that our reported human performance offers a fair approximation of average human reasoning under novel conditions.</p>
<p>F Scalability of Benchmark Creation</p>
<p>To ensure high-quality annotations, we employ human annotators supported by an automated checker and manual cross-validation.While we experimented with GPT-4o for annotation, it faced two major issues: (i) frequent omission of interactable objects, and (ii) inability to balance difficulty across scenarios.These limitations currently require human intervention to maintain benchmark quality.</p>
<p>Specifically, we explored using GPT-4o to generate item-based scenarios.While the model can produce coherent logical sequences, it lacks the creativity and complexity required by our benchmark.For example:</p>
<p>• Example 1: Use a magnifying glass to examine an ancient scroll, revealing hidden formulas.</p>
<p>• Example 2: Clean a rusty key with water, then scrape it with a coin to unlock a cabinet (partially creative).</p>
<p>Each AI-generated scenario yields about 12 key steps, whereas our benchmark includes over 1200 steps across 36 finely tiered scenarios.Therefore, human expertise remains essential for crafting creatively challenging and balanced content.While full automation falls short, parts of the process (e.g., validation, formatting) can be scaled.Future model improvements may increase automation potential, but human oversight will remain critical.</p>
<p>G More Detailed Discussions</p>
<p>We present a more detailed version of our results discussions and future research directions.</p>
<p>Theoretical Foundations for AI Creativity.Understanding the cognitive mechanisms behind human creativity is essential for designing AI systems that emulate or surpass human creative processes.Human creativity is a multifaceted phenomenon involving the generation of novel and valuable ideas, problem-solving, and adaptation to complex situations (Dainys, 2024).Neurologically, it arises from the interplay between stochastic neuronal noise and structured, learned information, driven by spontaneous brain activity fluctuations (Malach, 2024).This contrasts with AI's reliance on data patterns and algorithmic processes.Boden identifies three AI creativity mechanisms: combining familiar ideas in novel ways, exploring conceptual spaces, and enabling transformative innovations (Boden, 1998).Our research shows that even advanced models like GPT-4o struggle with implicit goal identification and creative problemsolving, often requiring extensive prompting.The EscapeAgent framework significantly reduces this dependency and improves task-solving efficiency, indicating that these modules effectively overcome barriers to creative reasoning.However, the results also highlight the importance of the core model's capabilities, as larger models like GPT-4o benefit more from the framework than smaller models.This suggests further research is needed to address current models' limitations in generating truly novel ideas.Interdisciplinary approaches integrating psychology, neuroscience, and model architecture can advance agent creativity further.</p>
<p>Multimodal Integration.Expanding the escape room environment to include multimodal data, such as visual and voice cues, offers a promising avenue for enhancing both the agent's performance and the realism of the scenarios.While integrating visionlanguage models could enable agents to interpret visual clues more naturally, such an extension would also require robust visual understanding and reasoning capabilities to handle the complexity of tasks effectively.Additionally, incorporating multimodal interactions presents opportunities to study how agents synthesize information across modalities, such as correlating visual patterns with instructions or adapting strategies based on dynamic, multimodal feedback.Future work should explore how to seamlessly integrate and harmonize these diverse data types into the benchmark, pushing the boundaries of agents' ability to process, reason about, and act on streams of information.</p>
<p>Step RL for Creative Reasoning.Introducing reinforcement learning into the EscapeBench task is expected to enhance the accuracy and efficiency of the agent's exploration in long-chain tasks.This implies that under the guidance of rewards and penalties, the agent can explore in the correct direction more quickly.Compared with existing end-to-end reinforcement learning schemes, which rely on the final completion of the escape room task as the ultimate reward, introducing step rewards-providing immediate feedback for each step of the model's operation-could potentially accelerate convergence and foster creative advancements in the model.Specifically, the discrete steps in EscapeBench can be organized either as a continuous, logically coherent progression or as strategic, abrupt jumps reflecting non-linear reasoning.</p>
<p>Step Rewards can also further draw on the idea of task decomposition to be structured hierarchically.By constructing such a framework, Hierarchical Reinforcement Learning could manage super-long reasoning chains, decompose complex tasks into manageable subtasks, and enable a more fine-grained exploration and evaluation of AI's creative capabilities.</p>
<p>Figure 3 :
3
Figure 3: Statistics of total Scenes, Tools, and Items across all game settings."Key Steps" refer to the essential bottleneck actions required to complete the game.</p>
<p>Table 1 :
1
Rules and examples for different difficulty levels.Desc.refers to Item or Tool descriptions, while Env.represents the game engine's feedback when an unexpected action targets an Item or Tool.Table 2: Creative reasoning cases in EscapeBench and model's responses versus actual creative uses of tools.</p>
<p>Figure 4 :
4
Figure 4: Illustration of the EscapeAgent design.Building on the BaseAgent (Action), we integrate the Foresight and Reflection modules to enhance the agent's capabilities in creative reasoning and implicit goal identification.</p>
<ol>
<li>1
1
BaseAgentThe BaseAgent serves as the foundation of Es-capeAgent, as shown in the middle section of Figure 4.It takes actions based on the scenario context provided by the game engine and updates its working memory with environment feedback after each step.The working memory stores the agent's previous actions and corresponding feedback.</li>
</ol>
<p>Figure 5 :
5
Figure 5: Distribution of Key Steps Hints Used, categorized by different actions.Colored bars represent the percentage of hints used for each action type relative to the total key steps for that type (See right of Figure 3).</p>
<p>Figure 6 :
6
Figure 6: The accumulated number of completed games (in total 36) relative to total steps a game setting takes.EscapeAgent, shown in dotted lines, completes games in fewer steps, demonstrating greater efficiency.</p>
<p>Figure 7 :
7
Figure 7: Case study on Human, BaseAgent, and EscapeAgent's progress map corresponding to six game settings.Although EscapeAgent uses 40% fewer hints and makes significant progress independently, it still falls far short of average human performance, often requiring twice as many total steps to complete a game.</p>
<p>Figure 8 :
8
Figure 8: Distribution of step intervals for progress made through tool collection and key step achievement.EscapeAgent uses fewer steps to achieve the next progress and relies less on hints.</p>
<p>-</p>
<p>name : hallway desc : You are in a hallway with a blocked path straight ahead , a locked cabinet on the left , and a corridor to the right .scene_relations :To the blocked path close -up : blocked path close -up To the cabinet close -up : cabinet close -up items :</p>
<p>History : [ Step ...] Your position : &lt; How you get to that position from the beginning scene &gt; e .g .Living Room -&gt; Outside Corridor Your action : &lt; The action taken &gt; e .g .move ( Explore the blocked path ) Response from the environment : &lt; Feedback from game engine &gt; e .g .Action executed successfully .Change to another scene : blocked path close -up .</p>
<p>[</p>
<p>Task Index &lt; index &gt;] Name : &lt; brief task name &gt; , Target Item : &lt; item name &gt; -Task description : &lt; description of the task &gt; Example Task : [ Task Index 1] Name : open the chest , Target Item : chest -Task description : To open the chest wth a matched key , I have tried simple click , apply safe key but all failed .</p>
<p>-update ( updated_feedback ) : The parameter should an updated feedback about what you newly tried but failed .The updated feedback should retain the original feedback , and add one new hindsight you got from current action .-new ( task_name , feedback ) : The first parameter should be a brief name of the new task you propose , the second parameter should be what you have to do ( extract hint from environment response ) to solve this task .-delete ( index ) : If you choose delete , then the first parameter should be the index of the task in the task list that you thought you have completed or is not useful anymore .-none () : If you choose none , do not give any parameter , it indicates you believe you don 't need to perform any action on the task list in the current step For instance , valid task list maintaining action may be : update ( The door has a keyhole and needs a key .I try apply a hammer but fails .), new ( open the safe , I need a 4-digit password input to open it with a hint of sigma sign beside the safe .), delete (1) , none () .</p>
<p>Figure 9 :
9
Figure 9: More model's analysis on progress-making interval, extension of Figure 8.</p>
<p>Figure 10 :
10
Figure 10: More model's analysis on item trial times, extension of Figure 14.</p>
<p>Figure 11 :
11
Figure 11: An illustration of progress-making trend through all 36 game settings.</p>
<p>Figure 12 :
12
Figure 12: Ablation of difficulty through Hints Used.</p>
<p>Figure 13 :
13
Figure 13: Ablation of difficulty through Total Steps.</p>
<p>Figure 14 :
14
Figure 14: A comparison of action trial times distribution for a specific item before success.</p>
<p>•</p>
<p>Prior offline Room Escape game experience?[Yes/No] • Number of times played offline?[Integer] • Prior online Room Escape experience (e.g., phone/PC)?[Yes/No] • Self-assessed skill level?[Skilled / Might Be / Not Really / Not at All]</p>
<p>Figure 15 :
15
Figure 15: More model's analysis on progress with respect to action steps, extension of Figure 7.</p>
<p>Table 3 :
3
Benchmarking results of BaseAgent with different core models on EscapeBench.An oracle action chain's total step is only 107.83 on average.Both closed-and open-source models rely heavily on hints to complete the escape compared to human performance, with smaller-scale models exhibiting a particularly high dependency.</p>
<p>Table 4 :
4GPT-4o5.03↓5.27452.75↓270.8647.03↑22.280.33↓1.84 (1.19%)4.70↓3.44 (13.74%)GPT-4o-mini10.58↓4.61752.25↓250.1428.17↑12.111.14↓0.86 (4.16%)9.44↓3.75 (28.53%)Llama-3.1-70B-Instruct7.92↓6.61645.19↓337.2331.44↑12.441.42↓1.69 (5.44%)6.56↓4.86 (19.15%)Qwen2.5-72B-Instruct9.72↓6.78746.61↓355.8928.62↑16.162.72↓2.61 (10.61%)7.00↓4.17 (20.71%)DeepSeek-LLM-67b-Chat20.14↓5.361285.03↓273.4415.30↑8.6710.81↑0.31 (42.72%)9.34↓5.66 (27.20%)Yi-1.5-34B-Chat22.59↓1.411468.03↓105.3012.04↑0.0810.42↑2.31 (41.61%)12.19↓3.73 (35.71%)Phi-3-medium-128k-instruct25.75↓6.441513.69↓357.509.99↑2.6511.20↓0.91 (44.99%)14.55↓5.56 (43.35%)Llama-3.1-8B-Instruct19.81↓6.051271.53↓271.7719.22↑9.126.64↓0.17 (25.23%)13.22↓5.89 (39.34%)Ministral-8B-Instruct19.47↓5.841233.72↓323.2519.34↑10.376.61↓0.56 (25.56%)12.86↓5.33 (38.93%)Qwen2.5-7B-Instruct27.53↓4.671639.58↓310.848.56↑2.0413.00↓0.81 (52.66%)14.58↓3.89 (43.99%)
Error Analysis of BaseAgent's inefficiency or failures.All cases are selected in the same game setting.Model Name↓ Hints Used ↓ Total Steps ↑ Early Exit Progress (%)↓ Tool Hints Used (percentage)↓ Key Steps Hints Used (percentage)</p>
<p>Table 5 :
5
Benchmarking Results of EscapeAgent with different core models.Nearly all the performance raises compared to Table3, showcasing the effectiveness of EscapAgent in promoting the agent's creativity.</p>
<p>Table 6 :
6
Performance of Domain-Specific Models on EscapeAgent Benchmark
ModelQwen-2.5-7B-Instruct22.61329.85.93Qwen-2.5-Coder-7B22.41339.812.99Qwen-2.5-Math-7B45.42384.40.00Model↓ Hints Used ↓ Total Steps↑ Early Exit Progress (%)GPT-4o, EscapeAgent5.00452.0044.55GPT-4o, only Reflection6.75570.0837.00GPT-4o, only Foresight7.17593.9248.89GPT-4o, BaseAgent9.83707.3319.85Llama-3.1-70B, EscapeAgent7.17624.6728.31Llama-3.1-70B, only Reflection11.33878.6728.78Llama-3.1-70B, only Foresight11.00877.2522.29Llama-3.1-70B, BaseAgent14.67981.4217.28
↓ Hints Used ↓ Total Steps ↑ Early Exit Progress (%)</p>
<p>Table 7 :
7
Ablation results on GPT-4o and Llama-3.1-70B-Instructbackbones.</p>
<p>https://github.com/qiancheng0/EscapeBench
AcknowledgmentThis research is based upon work supported by U.S. DARPA ITM Program No. FA8650-23-C-7316 and DARPA ECOLE Program No. #HR00112390060.The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government.The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.
Jyoti Marah Abdin, Hany Aneja, Ahmed Awadalla, Ammar Awadallah, Nguyen Ahmad Awan, Amit Bach, Arash Bahree, Jianmin Bakhtiari, Harkirat Bao, Behl, arXiv:2404.14219Phi-3 technical report: A highly capable language model locally on your phone. 2024arXiv preprint</p>
<p>Saaket Agashe, Yue Fan, Xin Eric, Wang , arXiv:2310.03903Evaluating multi-agent coordination abilities in large language models. 2023arXiv preprint</p>
<p>Storium: A dataset and evaluation platform for machine-in-the-loop story generation. Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, arXiv:2010.017172020arXiv preprintNanyun Peng, and Mohit Iyyer</p>
<p>Introducing claude 3.5 sonnet. Anthropic, 2024</p>
<p>Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. Baai Pku, arXiv:2303.165632023arXiv preprint</p>
<p>Creativity and artificial intelligence. A Margaret, Boden, Artificial Intelligence. 1031-21998</p>
<p>Emergent autonomous scientific research capabilities of large language models. Robert Daniil A Boiko, Gabe Macknight, Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>Sam Andres M Bran, Oliver Cox, Carlo Schilter, Andrew D Baldassari, Philippe White, Schwaller, arXiv:2304.05376Chemcrow: Augmenting large-language models with chemistry tools. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Advances in neural information processing systems. 2020</p>
<p>Large language models as tool makers. Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou, The Twelfth International Conference on Learning Representations. 2024</p>
<p>On the utility of learning about humans for human-ai coordination. Micah Carroll, Rohin Shah, Mark K Ho, Tom Griffiths, Sanjit Seshia, Pieter Abbeel, Anca Dragan, Advances in neural information processing systems. 201932</p>
<p>Reliable: Offline reinforcement learning for tactical strategies in professional basketball games. Xiusi Chen, Jyun-Yu Jiang, Kun Jin, Yichao Zhou, Mingyan Liu, Jeffrey Brantingham, Wei Wang, Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management. the 31st ACM International Conference on Information &amp; Knowledge Management2022</p>
<p>Playbest: Professional basketball player behavior synthesis via planning with diffusion. Xiusi Chen, Wei-Yao Wang, Ziniu Hu, David Reynoso, Kun Jin, Mingyan Liu, Jeffrey Brantingham, Wei Wang, Proceedings of the 33rd ACM International Conference on Information and Knowledge Management. the 33rd ACM International Conference on Information and Knowledge Management2024</p>
<p>Textworld: A learning environment for text-based games. Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, Computer Games: 7th Workshop, CGW 2018, Held in Conjunction with the 27th International Conference on Artificial Intelligence, IJCAI. Stockholm, SwedenSpringer2019. 2018. July 13. 2018Revised Selected Papers 7</p>
<p>Human creativity versus machine creativity: Will humans be surpassed by ai?. Augustinas Dainys, ; Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, Yu Su, Advances in Neural Information Processing Systems, 36. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu. Roberta Raileanu, Xian Li, Asli Celikyilmaz, Jason E Weston, 2024. 2024. 2024ICLR 2024 Workshop on Reliable and Responsible Foundation Models</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Minedojo: Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An, Yuke Huang, Anima Zhu, Anandkumar, Advances in Neural Information Processing Systems. 202235</p>
<p>Giorgio Franceschelli, Mirco Musolesi, arXiv:2304.00008On the creativity of large language models. 2023arXiv preprint</p>
<p>Multimodal web navigation with instruction-finetuned foundation models. Hiroki Furuta, Kuang-Huei Lee, Ofir Nachum, Yutaka Matsuo, Aleksandra Faust, Shixiang Shane Gu, Izzeddin Gur, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, L K Daniel, James J Yamins, Josh Dicarlo, Antonio Mcdermott, Torralba, arXiv:2103.14025The threedworld transport challenge: A visually guided task-and-motion planning benchmark for physically realistic embodied ai. 2021arXiv preprint</p>
<p>Creativity: Yesterday, today and tomorrow. P Joy, Guilford, The Journal of Creative Behavior. 111967</p>
<p>Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, Yutaka Matsuo, arXiv:2309.17277Suspicion-agent: Playing imperfect information games with theory of mind aware gpt-4. 2023arXiv preprint</p>
<p>A real-world webagent with planning, long context understanding, and program synthesis. Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, Aleksandra Faust, The Twelfth International Conference on Learning Representations. 2024</p>
<p>The originality of machines: Ai takes the torrance test. Erik E Guzik, Christian Byrge, Christian Gilde, 10.1016/j.yjoc.2023.100065Journal of Creativity. 3331000652023</p>
<p>Reasoning with language model is planning with world model. Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, Zhiting Hu, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao, arXiv:2306.03901Chatdb: Augmenting llms with databases as their symbolic memory. 2023arXiv preprint</p>
<p>Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, Ling Liu, arXiv:2404.02039A survey on large language model-based game agents. 2024arXiv preprint</p>
<p>An embodied generalist agent in 3d world. Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, Siyuan Huang, Forty-first International Conference on Machine Learning. 2024a</p>
<p>Benchmarking large language models as ai research agents. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023a</p>
<p>Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, Li Fei-Fei, arXiv:2409.01652Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation. 2024barXiv preprint</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, arXiv:2307.05973Voxposer: Composable 3d value maps for robotic manipulation with language models. 2023barXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>. Infocom, 1980</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Building machines that learn and think like people. Brenden M Lake, Joshua B Tomer D Ullman, Samuel J Tenenbaum, Gershman, Behavioral and brain sciences. 40e2532017</p>
<p>Universal intelligence: A definition of machine intelligence. Minds and machines. Shane Legg, Marcus Hutter, 200717</p>
<p>Camel: Communicative agents for" mind" exploration of large language model society. Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, Bernard Ghanem, Advances in Neural Information Processing Systems. 202336</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023a</p>
<p>Unleashing infinite-length input capacity for largescale language models with self-controlled memory system. Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, Zhoujun Li, 2023b2304arXiv e-prints</p>
<p>Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. Yicheng Bill Yuchen Lin, Karina Fu, Faeze Yang, Shiyu Brahman, Chandra Huang, Prithviraj Bhagavatula, Yejin Ammanabrolu, Xiang Choi, Ren, Advances in Neural Information Processing Systems. 202436</p>
<p>Agentsims: An open-source sandbox for large language model evaluation. Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, arXiv:2308.040262023arXiv preprintHuqiuyue Ping, and Qin Chen</p>
<p>Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, arXiv:2405.04434Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. 2024aarXiv preprint</p>
<p>Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone, arXiv:2304.11477Llm+ p: Empowering large language models with optimal planning proficiency. 2023aarXiv preprint</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang, arXiv:2311.08719Think-in-memory: Recalling and post-thinking enable llms with long-term memory. 2023barXiv preprint</p>
<p>Training socially aligned language models in simulated human society. Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, Soroush Vosoughi, arXiv:2305.169602023carXiv preprint</p>
<p>Agentbench: Evaluating llms as agents. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>The neuronal basis of human creativity. Rafael Malach, Frontiers in Human Neuroscience. 1813679222024</p>
<p>Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste Rozière, Jane Schick, Asli Dwivedi-Yu, Celikyilmaz, arXiv:2302.07842Augmented language models: a survey. 2023arXiv preprint</p>
<p>Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. Ning Miao, Yee Whye Teh, Tom Rainforth, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Introducing ministral-8b-instruct. Mistralai, 2024</p>
<p>Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, arXiv:2112.09332Webgpt: Browser-assisted questionanswering with human feedback. 2021arXiv preprint</p>
<p>Hoodwinked: Deception and cooperation in a text-based game for language models. Aidan O' Gara, arXiv:2308.014042023arXiv preprint</p>
<p>Metaverse or simulacra? roblox, minecraft, meta and the turn to virtual reality for education, socialisation and work. Pericles 'asher' Rospigliosi, 2022</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109Can llms generate novel research ideas? a largescale human study with 100+ nlp researchers. 2024arXiv preprint</p>
<p>Toward a triarchic theory of human intelligence. J Robert, Sternberg, Behavioral and Brain Sciences. 721984</p>
<p>Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. Gemini Team, Petko Georgiev, Ian Ving, Ryan Lei, Libin Burnell, Anmol Bai, Garrett Gulati, Damien Tanzer, Zhufeng Vincent, Shibo Pan, Wang, arXiv:2403.055302024arXiv preprint</p>
<p>Qwen Team. 2024. Qwen2.5: A party of foundation models. </p>
<p>Nonplayer character decision-making in computer games. Muhtar Çagkan, Uludaglı , Kaya Oguz, Artificial Intelligence Review. 56122023</p>
<p>Learning to speak and act in a fantasy text adventure game. Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, Jason Weston, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Linxi Fan, and Anima Anandkumar. 2024a. Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Transactions on Machine Learning Research. </p>
<p>Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, Heng Ji, arXiv:2504.14870Otc: Optimal tool calls via reinforcement learning. 2025arXiv preprint</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, Proc. The 62nd Annual Meeting of the. The 62nd Annual Meeting of theAssociation for Computational Linguistics2024b. ACL2024</p>
<p>Scienceworld: Is your agent smarter than a 5th grader?. Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, Prithviraj Ammanabrolu, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Augmenting language models with long-term memory. Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, Furu Wei, Thirty-seventh Conference on Neural Information Processing Systems. 2023a</p>
<p>Describe, explain, plan and select: interactive planning with large language models enables open-world multi-task agents. Zihao Wang, and Team CraftJarvisShaofei Cai, and Team CraftJarvisGuanzhou Chen, and Team CraftJarvisAnji Liu, and Team CraftJarvisXiaojian Ma, and Team CraftJarvisYitao Liang, and Team CraftJarvisProceedings of the 37th International Conference on Neural Information Processing Systems. the 37th International Conference on Neural Information Processing Systems2023b</p>
<p>Jarvis-1: Open-world multitask agents with memory-augmented multimodal language models. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, Yitao Liang, Second Agent Learning in Open-Endedness Workshop. 2024c</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Smartplay: A benchmark for llms as intelligent agents. Yue Wu, Xuan Tang, Tom Mitchell, Yuanzhi Li, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Jing Toh, Zhoujun Hua, Dongchan Cheng, Fangyu Shin, Lei, arXiv:2404.07972Osworld: Benchmarking multimodal agents for openended tasks in real computer environments. 2024arXiv preprint</p>
<p>Exploring large language models for communication games: An empirical study on werewolf. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu, arXiv:2309.046582023arXiv preprint</p>
<p>Embodiedbench: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, arXiv:2502.095602025arXiv preprint</p>
<p>Webshop: Towards scalable realworld web interaction with grounded language agents. Shunyu Yao, Howard Chen, John Yang, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202235</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Yuan Karthik R Narasimhan, Cao, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Yi: Open foundation models by 01. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, arXiv:2403.046522024arXiv preprint</p>
<p>Fincon: A synthesized llm multi-agent system with conceptual verbal reinforcement for enhanced financial decision making. Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yupeng Cao, Zhi Chen, Jordan W Suchow, Rong Liu, Zhenyu Cui, Zhaozhuo Xu, Denghui Zhang, Koduvayur Subbalakshmi, Guojun Xiong, Yueru He, Jimin Huang, Dong Li, Qianqian Xie, arXiv:2407.065672024Preprint</p>
<p>Xingdi Yuan, Marc-Alexandre Côté, Alessandro Sordoni, Romain Laroche, Remi Tachet Des Combes, Matthew Hausknecht, Adam Trischler, arXiv:1806.11525Counting to explore and generalize in text-based games. 2018arXiv preprint</p>
<p>Creative agents: Empowering agents with imagination for creative tasks. Chi Zhang, Penglin Cai, Yuhui Fu, Haoqi Yuan, Zongqing Lu, arXiv:2312.025192023arXiv preprint</p>
<p>Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu, arXiv:2401.02009Self-contrast: Better reflection through inconsistent solving perspectives. 2024aarXiv preprint</p>
<p>Agentpro: Learning to evolve via policy-level reflection and optimization. Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu, ICLR 2024 Workshop on Large Language Model (LLM) Agents. 2024b</p>
<p>Knowledge overshadowing causes amalgamated hallucination in large language models. Yuji Zhang, Sha Li, Jiateng Liu, Pengfei Yu, Yi R Fung, Jing Li, Manling Li, Heng Ji, arXiv:2407.080392024carXiv preprint</p>
<p>Assessing and understanding creativity in large language models. Yunpu Zhao, Rui Zhang, Wenyi Li, Di Huang, Jiaming Guo, Shaohui Peng, Yifan Hao, Yuanbo Wen, Xing Hu, Zidong Du, Qi Guo, Ling Li, Yunji Chen, arXiv:2401.124912024Preprint</p>
<p>Towards learning a generalist model for embodied navigation. Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Memorybank: Enhancing large language models with long-term memory. Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, Yanlin Wang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Language agent tree search unifies reasoning, acting, and planning in language models. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, Yu-Xiong Wang, Forty-first International Conference on Machine Learning. 2024a</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, The Eleventh International Conference on Learning Representations. 2023a</p>
<p>Webarena: A realistic web environment for building autonomous agents. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023b</p>
<p>Webarena: A realistic web environment for building autonomous agents. Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, The Twelfth International Conference on Learning Representations. 2024b</p>
<p>Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian, Xiangru Tang, Heng Ji, arXiv:2503.01935Multiagentbench: Evaluating the collaboration and competition of llm agents. 2025arXiv preprint</p>
<p>Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, arXiv:2305.17144Ghost in the minecraft: Generally capable agents for open-world environments via large language models with textbased knowledge and memory. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
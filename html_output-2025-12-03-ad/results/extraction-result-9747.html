<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9747 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9747</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9747</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-274656347</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.09569v2.pdf" target="_blank">JuStRank: Benchmarking LLM Judges for System Ranking</a></p>
                <p><strong>Paper Abstract:</strong> Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available. The scale and versatility of such evaluations make the use of LLM-based judges a compelling solution for this challenge. Crucially, this approach requires first to validate the quality of the LLM judge itself. Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems. We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems. To address this gap, we conduct the first large-scale study of LLM judges as system rankers. System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking. Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9747.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9747.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instance-vs-System mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between instance-level judge accuracy and system-level ranking quality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that strong instance-level performance of an LLM judge does not necessarily imply accurate system-level rankings, because the distribution of judge errors across systems (not just the overall error rate) determines ranking quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open-domain assistant/dialogue system ranking (system-level evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Multiple generative LLMs and reward models (e.g., GPT-4o, GPT-4o-mini, Llama-3 variants, Qwen2.5-72B, several reward models)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Judges scored K×L responses using realizations (Numeric, Likert, TokenProbs, Anchor); scores aggregated by Win-rate, Mean, Median, or Bradley-Terry to form system rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Chatbot Arena human preference data (English Hard Prompts subset): ~300K battles used to compute Bradley-Terry gold ranking; 113K non-tied battles produce pairwise win-rates for overlap with system responses.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's Tau (τ) between judge-induced system ranking and Chatbot Arena gold ranking; example top τ values: Qwen2.5 Likert τ=0.83, URM-LLaMa-3.1-8B (reward) τ=0.82; Numeric mean τ=0.75, Likert τ=0.74, Anchor τ=0.71, TokenProbs τ=0.68. (See §5, Table 1, Fig.3)</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>What is lost is the guarantee that instance-level accuracy implies correct system ordering: even judges with high instance-level accuracy can produce skewed or incorrect system rankings because their mistakes cluster unevenly across systems.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Comparison to RewardBench shows judges that do well at instance-level do not always top JuStRank; the paper reports cases where judges with similar instance-level metrics yield different system rankings (see §5 and Fig.3).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Some reward models (explicitly trained for instance-level pairwise decisions) nonetheless achieved strong system-level agreement—several 8B reward models were on par with much larger LLMs (§5, Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>§3 (The Gap in Judge Benchmarking); §5 (JuStRank - Judge Performance Results); Fig.3; Table 1</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JuStRank: Benchmarking LLM Judges for System Ranking', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9747.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9747.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decisiveness / Overconfidence</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judge decisiveness (sigmoidal/overconfident win-rate predictions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM judges often produce pairwise win-rates that are more extreme than human data (push probabilities toward 0 or 1), characterized by a fitted beta-curve parameter α>1 indicating 'decisiveness' or overconfidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Pairwise system preference within assistant/dialogue evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Examples: Llama-3.1-405B (notably decisive), other LLM realizations (Likert/Numeric shown to be more decisive than TokenProbs)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Pairwise win-rates computed from judge scores matrix (Score_p(k,l)) before aggregation; beta CDF (α=β) fit to judge vs gold win-rate scatter to measure decisiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Chatbot Arena pairwise win-rates derived from user preference battles between system responses (968 overlapping system-pair comparisons used for fit).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Decisiveness quantified as α from beta CDF fit (α=1 linear/no decisiveness; α>1 sigmoidal/overconfident); correlation r=0.55 between α and ranking quality τ (§6.1, App.F).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM judges lose the calibrated, distributed uncertainty present in human aggregated preferences: they amplify differences (more extreme preferences), potentially masking subtle disagreements among humans and exaggerating separation between systems.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Llama-405B produced predicted win-rates >0.9 for pairs where human win-rate ~0.8 (Fig.5); Likert and Numeric realizations show higher α (more decisive) whereas TokenProbs can yield α<1 (indecision) (Fig.6).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Decisiveness can be beneficial: extreme win-rates increase separability and may raise the chance of correct preference decisions with fewer samples (practical benefit noted in §6.1).</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>§6.1 (Some Judges are Particularly Decisive); Fig.5; Fig.6; Appendix F</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JuStRank: Benchmarking LLM Judges for System Ranking', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9747.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9747.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>System-specific bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Judge bias toward or against specific systems (system-specific favoritism/favoritism)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Judges systematically over- or under-rate particular systems relative to human gold win-rates; bias measured as expectation of (W_R_judge − W_R_gold) per system and corrected for decisiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>System ranking / assistant evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Observed across many judges; example systems affected: Athene-70B receives strong positive bias from most judges, GPT-4-0613 receives negative bias.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Compute B_p(sa) = E_s_b[W_R_p(sa,sb) − W_R_g(sa,sb)], and decisiveness-corrected bias B'_p(sa) using judge-specific beta fit to adjust expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Gold pairwise win-rates and BT coefficients from Chatbot Arena (English Hard Prompts); same overlapping system pairs used to compute bias measures.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Bias propensity δ = standard deviation σ_s(B'_p(s)) reported per judge; δ correlates negatively with τ (r = −0.56) indicating higher bias propensity predicts worse ranking agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs as judges can introduce systematic unfairness to certain systems: a model can be consistently over-ranked or under-ranked compared to humans, distorting leaderboards and model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Most judges assign Athene-70B inflated rank (often #1) despite humans ranking it differently; GPT-4-0613 is pushed down by negative bias (median rank 38 vs gold 27) (Fig.7).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Self-bias (preference for same-architectural models) was observed only sporadically and not consistently across realizations (App.Table 3); some judges show low δ and align well with human ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>§6.2 (Bias Towards Specific Systems); Fig.7; App.Table 3; App.Table 4</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JuStRank: Benchmarking LLM Judges for System Ranking', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9747.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9747.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Score-distribution & ties</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Discrete score distributions, ties, and aggregation sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Many LLM realizations (Numeric, Likert) concentrate probability mass on a few score values, causing many ties across responses; this degrades the ability to separate systems and makes aggregation choices (e.g., median vs mean vs BT) important.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>System ranking / aggregation of judge scores</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLM realizations (Numeric and Likert especially), TokenProbs, Anchor; reward models produce continuous scores and fewer ties.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLMs asked for Numeric (0–100) or Likert (5 labels) scores; TokenProbs uses yes/no token probabilities; distributions D_p analyzed (Fig.12) and many ties observed for LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human gold data are binary preference battles aggregated into continuous BT coefficients (fewer ties due to many human votes).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Observed effect on Kendall's Tau and robustness: Median aggregation often fails to separate systems for discrete-likert judges; aggregation method had non-significant effect on τ overall (ANOVA), but ties cause instability for some realizations (App.B, Fig.12).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of granularity: LLMs' discrete scoring yields many ties, reducing separability among systems and making some aggregation functions ineffective (e.g., Median). This can degrade reliability of system rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Likert realizations concentrate mass causing many identical medians across systems (Appendix A); TokenProbs tends to be extreme (near 0/1) giving tiny score gaps and lower robustness (Fig.12; §A).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Reward models' continuous outputs avoid many of these problems and some LLM realizations (Numeric) perform well when aggregated appropriately; BT and Win-rate aggregations mitigate ties better than median.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Appendix A (Judge Score Distributions); §4.3 (Aggregations); App.B; Fig.12</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JuStRank: Benchmarking LLM Judges for System Ranking', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9747.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9747.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Realization/prompt brittleness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to judge realization and prompt phrasing (realization effects)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The specific LLM judge realization (Numeric, Likert, Anchor, TokenProbs) and prompt phrasing strongly impact system-level ranking quality—nearly as much as the choice of model itself.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>System ranking / LLM-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Multiple LLMs (Llama-3 variants, Mistral, Mixtral, Qwen2.5, GPT-4o, GPT-4o-mini) each run with multiple realizations/prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Realizations: Numeric (0–100), Likert (5-point), TokenProbs (yes/no token probability), Anchor (comparative against GPT-4-0314 anchor); prompts listed in Appendix G.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Chatbot Arena human preferences used as gold ranking across same instruction distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Three-way ANOVA on τ shows judge model partial eta-squared η^2=0.81, realization η^2=0.51, interaction η^2=0.78 (all p<0.001), indicating large realization effects; post-hoc tests show Numeric and Likert significantly outperform Anchor and TokenProbs (all p<=0.002).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using off-the-shelf LLM judgments without careful realization/prompt tuning can degrade alignment with human preferences; poor prompt/realization choices reduce ranking agreement and introduce artifacts (over/under-decisiveness, ties).</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Comparative Anchor realization underperformed compared to Numeric/Likert for most LLMs (except GPT-4o), token-prob based judgments produced extreme probabilities and larger variance (Fig.4, §5, App.D).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Some LLMs (e.g., GPT-4o) buck the trend and perform relatively well with Anchor; careful prompt engineering (Numeric/Likert) and realization choice can recover much of the lost alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>§5 (Effects of LLM Realizations); Fig.4; Appendix D; Appendix G</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JuStRank: Benchmarking LLM Judges for System Ranking', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9747.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9747.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Loss of human nuance (multi-dimensional preference)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reduction of subjective, multi-dimensional human preference to a single automated metric</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors note that human preference is subjective and multi-faceted (helpfulness, safety, style, coherence), but the automated judge comparisons and the gold reference treat preference as a single scalar concept, losing human nuance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Human preference judgment for conversational/assistant responses</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various LLM judges and reward models used to produce scalar scores, thereby collapsing multiple dimensions into one metric.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Judges asked for single-score outputs (Numeric or Likert) or pairwise preference; aggregated into single system scores and rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Chatbot Arena human preference votes aggregated into Bradley-Terry coefficients (treated as a unidimensional 'better' metric).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's Tau between judge ranking and BT gold ranking; limitations acknowledged that humans have diverse preferences not captured by a single scalar (Limitations section).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Loss of subjective nuance: automated judges compress multidimensional human judgments into a single score, potentially overlooking important axes of preference and cultural/annotator heterogeneity.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Authors explicitly state in Limitations that there is no single 'human preference'—different annotators weigh helpfulness vs brevity differently—hence treating preference as a single concept is a limitation of LLM-as-judge comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The Bradley-Terry aggregation is a practical operationalization of 'better system' (winning more often), and is a defensible gold standard for many system-comparison tasks; still, it abstracts away subdimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Limitations (paragraph on human preference subjectivity); §4.4 (Gold Ranking - Chatbot Arena Battles)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JuStRank: Benchmarking LLM Judges for System Ranking', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9747.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9747.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Comparative Anchor weakness</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparative (Anchor) realization underperformance vs absolute judgments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Contrary to some recommendations, the Anchor comparative realization (comparing each system response to an anchor GPT-4 response) frequently performed worse than absolute Numeric or Likert realizations for most LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Comparative pairwise preference judgments for assistant responses</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLMs including GPT-4o, GPT-4o-mini, Llama-3 variants, Mixtral, Mistral; Anchor uses GPT-4-0314 as anchor responses.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Comparative prompt: judge asked to prefer between Assistant A (system) and Assistant B (anchor GPT-4) and output discrete preference labels mapped to [-2,+2]; used to infer system scores via comparisons to anchor.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Chatbot Arena human pairwise battles (not anchored) used as gold for full pairwise comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Anchor realization mean τ ~0.71 vs Numeric 0.75 and Likert 0.74; post-hoc tests show Anchor significantly worse than Numeric/Likert (App.D, Tukey HSD p<=0.002).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Use of anchor-based comparative prompts can degrade alignment with human rankings, possibly due to anchor choice effects and the impracticality of full paired comparisons to all systems.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Across many LLMs Anchor-ranked system orderings had lower Kendall's Tau; only GPT-4o was a notable exception where Anchor performed well (Fig.4, §5).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Comparative judgments are sometimes recommended and can be useful; GPT-4o showed good performance with Anchor, suggesting model-specific interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>§4.2.2 (Comparative judgment - Anchor); §5 (Effects of LLM Realizations); Fig.4; Appendix D</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JuStRank: Benchmarking LLM Judges for System Ranking', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9747.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9747.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TokenProbs extremes / indecision</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Token probability (yes/no) realization yields extreme or indecisive scores</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>TokenProbs realization (asking Is this a good response? yes/no and using token probabilities) often produces probabilities very close to 0 or 1, or in some cases small gaps leading to low robustness and higher sensitivity to aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Binary/threshold-based quality judgments for responses</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLM judges across sizes (various Llama-3, Mistral, Mixtral, Qwen, GPT-4o variants) using TokenProbs prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prompted with yes/no question; judge score = normalized probability mass on tokens 'yes' vs 'no' for first generated token; scores in [0,1].</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Chatbot Arena multi-voter human preferences aggregated to continuous BT win-rates, not binary probabilities per instance.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>TokenProbs realization had lower mean τ (≈0.68) and higher variance in τ across judges; TokenProbs can lead to judge 'indecision' (α<1) or produce extreme values (Appendix A, Fig.12; App.D stats).</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Calibration loss and robustness issues: extreme probability outputs or tiny gaps between responses reduce discriminative power and cause instability in aggregated rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>TokenProbs distributions tend to cluster at 0.0 or 1.0, causing small score gaps and sensitivity to aggregation choices; TokenProbs realization also sometimes yielded indecisive behavior (α<1) for certain models (Appendix A, Fig.12; Fig.6b).</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>TokenProbs may avoid some over-decisiveness that Likert/Numeric show, and for some judges it resulted in lower decisiveness, which can be appropriate if human gold is noisy.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>§4.2.2 (TokenProbs description); Appendix A (TokenProbs distributions); §5 (Realization comparisons); Fig.6b</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'JuStRank: Benchmarking LLM Judges for System Ranking', 'publication_date_yy_mm': '2024-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RewardBench: Evaluating reward models for language modeling <em>(Rating: 2)</em></li>
                <li>Style outweighs substance: Failure modes of LLM judges in alignment benchmarking <em>(Rating: 2)</em></li>
                <li>Limits to scalable evaluation at the frontier: LLM as judge won't beat twice the data <em>(Rating: 2)</em></li>
                <li>Chatbot Arena: An open platform for evaluating LLMs by human preference <em>(Rating: 2)</em></li>
                <li>JudgeBench: A benchmark for evaluating LLM-based judges <em>(Rating: 2)</em></li>
                <li>Justice or prejudice? quantifying biases in LLM-as-a-judge <em>(Rating: 2)</em></li>
                <li>LLMs instead of human judges? a large scale empirical study across 20 NLP evaluation tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9747",
    "paper_id": "paper-274656347",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "Instance-vs-System mismatch",
            "name_full": "Mismatch between instance-level judge accuracy and system-level ranking quality",
            "brief_description": "The paper documents that strong instance-level performance of an LLM judge does not necessarily imply accurate system-level rankings, because the distribution of judge errors across systems (not just the overall error rate) determines ranking quality.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open-domain assistant/dialogue system ranking (system-level evaluation)",
            "llm_judge_model": "Multiple generative LLMs and reward models (e.g., GPT-4o, GPT-4o-mini, Llama-3 variants, Qwen2.5-72B, several reward models)",
            "llm_judge_setup": "Judges scored K×L responses using realizations (Numeric, Likert, TokenProbs, Anchor); scores aggregated by Win-rate, Mean, Median, or Bradley-Terry to form system rankings.",
            "human_evaluation_setup": "Chatbot Arena human preference data (English Hard Prompts subset): ~300K battles used to compute Bradley-Terry gold ranking; 113K non-tied battles produce pairwise win-rates for overlap with system responses.",
            "agreement_metric": "Kendall's Tau (τ) between judge-induced system ranking and Chatbot Arena gold ranking; example top τ values: Qwen2.5 Likert τ=0.83, URM-LLaMa-3.1-8B (reward) τ=0.82; Numeric mean τ=0.75, Likert τ=0.74, Anchor τ=0.71, TokenProbs τ=0.68. (See §5, Table 1, Fig.3)",
            "losses_identified": "What is lost is the guarantee that instance-level accuracy implies correct system ordering: even judges with high instance-level accuracy can produce skewed or incorrect system rankings because their mistakes cluster unevenly across systems.",
            "examples_of_loss": "Comparison to RewardBench shows judges that do well at instance-level do not always top JuStRank; the paper reports cases where judges with similar instance-level metrics yield different system rankings (see §5 and Fig.3).",
            "counterexamples_or_caveats": "Some reward models (explicitly trained for instance-level pairwise decisions) nonetheless achieved strong system-level agreement—several 8B reward models were on par with much larger LLMs (§5, Table 1).",
            "paper_reference": "§3 (The Gap in Judge Benchmarking); §5 (JuStRank - Judge Performance Results); Fig.3; Table 1",
            "uuid": "e9747.0",
            "source_info": {
                "paper_title": "JuStRank: Benchmarking LLM Judges for System Ranking",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Decisiveness / Overconfidence",
            "name_full": "Judge decisiveness (sigmoidal/overconfident win-rate predictions)",
            "brief_description": "LLM judges often produce pairwise win-rates that are more extreme than human data (push probabilities toward 0 or 1), characterized by a fitted beta-curve parameter α&gt;1 indicating 'decisiveness' or overconfidence.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Pairwise system preference within assistant/dialogue evaluation",
            "llm_judge_model": "Examples: Llama-3.1-405B (notably decisive), other LLM realizations (Likert/Numeric shown to be more decisive than TokenProbs)",
            "llm_judge_setup": "Pairwise win-rates computed from judge scores matrix (Score_p(k,l)) before aggregation; beta CDF (α=β) fit to judge vs gold win-rate scatter to measure decisiveness.",
            "human_evaluation_setup": "Chatbot Arena pairwise win-rates derived from user preference battles between system responses (968 overlapping system-pair comparisons used for fit).",
            "agreement_metric": "Decisiveness quantified as α from beta CDF fit (α=1 linear/no decisiveness; α&gt;1 sigmoidal/overconfident); correlation r=0.55 between α and ranking quality τ (§6.1, App.F).",
            "losses_identified": "LLM judges lose the calibrated, distributed uncertainty present in human aggregated preferences: they amplify differences (more extreme preferences), potentially masking subtle disagreements among humans and exaggerating separation between systems.",
            "examples_of_loss": "Llama-405B produced predicted win-rates &gt;0.9 for pairs where human win-rate ~0.8 (Fig.5); Likert and Numeric realizations show higher α (more decisive) whereas TokenProbs can yield α&lt;1 (indecision) (Fig.6).",
            "counterexamples_or_caveats": "Decisiveness can be beneficial: extreme win-rates increase separability and may raise the chance of correct preference decisions with fewer samples (practical benefit noted in §6.1).",
            "paper_reference": "§6.1 (Some Judges are Particularly Decisive); Fig.5; Fig.6; Appendix F",
            "uuid": "e9747.1",
            "source_info": {
                "paper_title": "JuStRank: Benchmarking LLM Judges for System Ranking",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "System-specific bias",
            "name_full": "Judge bias toward or against specific systems (system-specific favoritism/favoritism)",
            "brief_description": "Judges systematically over- or under-rate particular systems relative to human gold win-rates; bias measured as expectation of (W_R_judge − W_R_gold) per system and corrected for decisiveness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "System ranking / assistant evaluation",
            "llm_judge_model": "Observed across many judges; example systems affected: Athene-70B receives strong positive bias from most judges, GPT-4-0613 receives negative bias.",
            "llm_judge_setup": "Compute B_p(sa) = E_s_b[W_R_p(sa,sb) − W_R_g(sa,sb)], and decisiveness-corrected bias B'_p(sa) using judge-specific beta fit to adjust expectations.",
            "human_evaluation_setup": "Gold pairwise win-rates and BT coefficients from Chatbot Arena (English Hard Prompts); same overlapping system pairs used to compute bias measures.",
            "agreement_metric": "Bias propensity δ = standard deviation σ_s(B'_p(s)) reported per judge; δ correlates negatively with τ (r = −0.56) indicating higher bias propensity predicts worse ranking agreement.",
            "losses_identified": "Using LLMs as judges can introduce systematic unfairness to certain systems: a model can be consistently over-ranked or under-ranked compared to humans, distorting leaderboards and model selection.",
            "examples_of_loss": "Most judges assign Athene-70B inflated rank (often #1) despite humans ranking it differently; GPT-4-0613 is pushed down by negative bias (median rank 38 vs gold 27) (Fig.7).",
            "counterexamples_or_caveats": "Self-bias (preference for same-architectural models) was observed only sporadically and not consistently across realizations (App.Table 3); some judges show low δ and align well with human ranking.",
            "paper_reference": "§6.2 (Bias Towards Specific Systems); Fig.7; App.Table 3; App.Table 4",
            "uuid": "e9747.2",
            "source_info": {
                "paper_title": "JuStRank: Benchmarking LLM Judges for System Ranking",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Score-distribution & ties",
            "name_full": "Discrete score distributions, ties, and aggregation sensitivity",
            "brief_description": "Many LLM realizations (Numeric, Likert) concentrate probability mass on a few score values, causing many ties across responses; this degrades the ability to separate systems and makes aggregation choices (e.g., median vs mean vs BT) important.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "System ranking / aggregation of judge scores",
            "llm_judge_model": "LLM realizations (Numeric and Likert especially), TokenProbs, Anchor; reward models produce continuous scores and fewer ties.",
            "llm_judge_setup": "LLMs asked for Numeric (0–100) or Likert (5 labels) scores; TokenProbs uses yes/no token probabilities; distributions D_p analyzed (Fig.12) and many ties observed for LLMs.",
            "human_evaluation_setup": "Human gold data are binary preference battles aggregated into continuous BT coefficients (fewer ties due to many human votes).",
            "agreement_metric": "Observed effect on Kendall's Tau and robustness: Median aggregation often fails to separate systems for discrete-likert judges; aggregation method had non-significant effect on τ overall (ANOVA), but ties cause instability for some realizations (App.B, Fig.12).",
            "losses_identified": "Loss of granularity: LLMs' discrete scoring yields many ties, reducing separability among systems and making some aggregation functions ineffective (e.g., Median). This can degrade reliability of system rankings.",
            "examples_of_loss": "Likert realizations concentrate mass causing many identical medians across systems (Appendix A); TokenProbs tends to be extreme (near 0/1) giving tiny score gaps and lower robustness (Fig.12; §A).",
            "counterexamples_or_caveats": "Reward models' continuous outputs avoid many of these problems and some LLM realizations (Numeric) perform well when aggregated appropriately; BT and Win-rate aggregations mitigate ties better than median.",
            "paper_reference": "Appendix A (Judge Score Distributions); §4.3 (Aggregations); App.B; Fig.12",
            "uuid": "e9747.3",
            "source_info": {
                "paper_title": "JuStRank: Benchmarking LLM Judges for System Ranking",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Realization/prompt brittleness",
            "name_full": "Sensitivity to judge realization and prompt phrasing (realization effects)",
            "brief_description": "The specific LLM judge realization (Numeric, Likert, Anchor, TokenProbs) and prompt phrasing strongly impact system-level ranking quality—nearly as much as the choice of model itself.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "System ranking / LLM-based evaluation",
            "llm_judge_model": "Multiple LLMs (Llama-3 variants, Mistral, Mixtral, Qwen2.5, GPT-4o, GPT-4o-mini) each run with multiple realizations/prompts.",
            "llm_judge_setup": "Realizations: Numeric (0–100), Likert (5-point), TokenProbs (yes/no token probability), Anchor (comparative against GPT-4-0314 anchor); prompts listed in Appendix G.",
            "human_evaluation_setup": "Chatbot Arena human preferences used as gold ranking across same instruction distribution.",
            "agreement_metric": "Three-way ANOVA on τ shows judge model partial eta-squared η^2=0.81, realization η^2=0.51, interaction η^2=0.78 (all p&lt;0.001), indicating large realization effects; post-hoc tests show Numeric and Likert significantly outperform Anchor and TokenProbs (all p&lt;=0.002).",
            "losses_identified": "Using off-the-shelf LLM judgments without careful realization/prompt tuning can degrade alignment with human preferences; poor prompt/realization choices reduce ranking agreement and introduce artifacts (over/under-decisiveness, ties).",
            "examples_of_loss": "Comparative Anchor realization underperformed compared to Numeric/Likert for most LLMs (except GPT-4o), token-prob based judgments produced extreme probabilities and larger variance (Fig.4, §5, App.D).",
            "counterexamples_or_caveats": "Some LLMs (e.g., GPT-4o) buck the trend and perform relatively well with Anchor; careful prompt engineering (Numeric/Likert) and realization choice can recover much of the lost alignment.",
            "paper_reference": "§5 (Effects of LLM Realizations); Fig.4; Appendix D; Appendix G",
            "uuid": "e9747.4",
            "source_info": {
                "paper_title": "JuStRank: Benchmarking LLM Judges for System Ranking",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Loss of human nuance (multi-dimensional preference)",
            "name_full": "Reduction of subjective, multi-dimensional human preference to a single automated metric",
            "brief_description": "The authors note that human preference is subjective and multi-faceted (helpfulness, safety, style, coherence), but the automated judge comparisons and the gold reference treat preference as a single scalar concept, losing human nuance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Human preference judgment for conversational/assistant responses",
            "llm_judge_model": "Various LLM judges and reward models used to produce scalar scores, thereby collapsing multiple dimensions into one metric.",
            "llm_judge_setup": "Judges asked for single-score outputs (Numeric or Likert) or pairwise preference; aggregated into single system scores and rankings.",
            "human_evaluation_setup": "Chatbot Arena human preference votes aggregated into Bradley-Terry coefficients (treated as a unidimensional 'better' metric).",
            "agreement_metric": "Kendall's Tau between judge ranking and BT gold ranking; limitations acknowledged that humans have diverse preferences not captured by a single scalar (Limitations section).",
            "losses_identified": "Loss of subjective nuance: automated judges compress multidimensional human judgments into a single score, potentially overlooking important axes of preference and cultural/annotator heterogeneity.",
            "examples_of_loss": "Authors explicitly state in Limitations that there is no single 'human preference'—different annotators weigh helpfulness vs brevity differently—hence treating preference as a single concept is a limitation of LLM-as-judge comparisons.",
            "counterexamples_or_caveats": "The Bradley-Terry aggregation is a practical operationalization of 'better system' (winning more often), and is a defensible gold standard for many system-comparison tasks; still, it abstracts away subdimensions.",
            "paper_reference": "Limitations (paragraph on human preference subjectivity); §4.4 (Gold Ranking - Chatbot Arena Battles)",
            "uuid": "e9747.5",
            "source_info": {
                "paper_title": "JuStRank: Benchmarking LLM Judges for System Ranking",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "Comparative Anchor weakness",
            "name_full": "Comparative (Anchor) realization underperformance vs absolute judgments",
            "brief_description": "Contrary to some recommendations, the Anchor comparative realization (comparing each system response to an anchor GPT-4 response) frequently performed worse than absolute Numeric or Likert realizations for most LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Comparative pairwise preference judgments for assistant responses",
            "llm_judge_model": "LLMs including GPT-4o, GPT-4o-mini, Llama-3 variants, Mixtral, Mistral; Anchor uses GPT-4-0314 as anchor responses.",
            "llm_judge_setup": "Comparative prompt: judge asked to prefer between Assistant A (system) and Assistant B (anchor GPT-4) and output discrete preference labels mapped to [-2,+2]; used to infer system scores via comparisons to anchor.",
            "human_evaluation_setup": "Chatbot Arena human pairwise battles (not anchored) used as gold for full pairwise comparisons.",
            "agreement_metric": "Anchor realization mean τ ~0.71 vs Numeric 0.75 and Likert 0.74; post-hoc tests show Anchor significantly worse than Numeric/Likert (App.D, Tukey HSD p&lt;=0.002).",
            "losses_identified": "Use of anchor-based comparative prompts can degrade alignment with human rankings, possibly due to anchor choice effects and the impracticality of full paired comparisons to all systems.",
            "examples_of_loss": "Across many LLMs Anchor-ranked system orderings had lower Kendall's Tau; only GPT-4o was a notable exception where Anchor performed well (Fig.4, §5).",
            "counterexamples_or_caveats": "Comparative judgments are sometimes recommended and can be useful; GPT-4o showed good performance with Anchor, suggesting model-specific interactions.",
            "paper_reference": "§4.2.2 (Comparative judgment - Anchor); §5 (Effects of LLM Realizations); Fig.4; Appendix D",
            "uuid": "e9747.6",
            "source_info": {
                "paper_title": "JuStRank: Benchmarking LLM Judges for System Ranking",
                "publication_date_yy_mm": "2024-12"
            }
        },
        {
            "name_short": "TokenProbs extremes / indecision",
            "name_full": "Token probability (yes/no) realization yields extreme or indecisive scores",
            "brief_description": "TokenProbs realization (asking Is this a good response? yes/no and using token probabilities) often produces probabilities very close to 0 or 1, or in some cases small gaps leading to low robustness and higher sensitivity to aggregation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Binary/threshold-based quality judgments for responses",
            "llm_judge_model": "LLM judges across sizes (various Llama-3, Mistral, Mixtral, Qwen, GPT-4o variants) using TokenProbs prompt.",
            "llm_judge_setup": "Prompted with yes/no question; judge score = normalized probability mass on tokens 'yes' vs 'no' for first generated token; scores in [0,1].",
            "human_evaluation_setup": "Chatbot Arena multi-voter human preferences aggregated to continuous BT win-rates, not binary probabilities per instance.",
            "agreement_metric": "TokenProbs realization had lower mean τ (≈0.68) and higher variance in τ across judges; TokenProbs can lead to judge 'indecision' (α&lt;1) or produce extreme values (Appendix A, Fig.12; App.D stats).",
            "losses_identified": "Calibration loss and robustness issues: extreme probability outputs or tiny gaps between responses reduce discriminative power and cause instability in aggregated rankings.",
            "examples_of_loss": "TokenProbs distributions tend to cluster at 0.0 or 1.0, causing small score gaps and sensitivity to aggregation choices; TokenProbs realization also sometimes yielded indecisive behavior (α&lt;1) for certain models (Appendix A, Fig.12; Fig.6b).",
            "counterexamples_or_caveats": "TokenProbs may avoid some over-decisiveness that Likert/Numeric show, and for some judges it resulted in lower decisiveness, which can be appropriate if human gold is noisy.",
            "paper_reference": "§4.2.2 (TokenProbs description); Appendix A (TokenProbs distributions); §5 (Realization comparisons); Fig.6b",
            "uuid": "e9747.7",
            "source_info": {
                "paper_title": "JuStRank: Benchmarking LLM Judges for System Ranking",
                "publication_date_yy_mm": "2024-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RewardBench: Evaluating reward models for language modeling",
            "rating": 2,
            "sanitized_title": "rewardbench_evaluating_reward_models_for_language_modeling"
        },
        {
            "paper_title": "Style outweighs substance: Failure modes of LLM judges in alignment benchmarking",
            "rating": 2,
            "sanitized_title": "style_outweighs_substance_failure_modes_of_llm_judges_in_alignment_benchmarking"
        },
        {
            "paper_title": "Limits to scalable evaluation at the frontier: LLM as judge won't beat twice the data",
            "rating": 2,
            "sanitized_title": "limits_to_scalable_evaluation_at_the_frontier_llm_as_judge_wont_beat_twice_the_data"
        },
        {
            "paper_title": "Chatbot Arena: An open platform for evaluating LLMs by human preference",
            "rating": 2,
            "sanitized_title": "chatbot_arena_an_open_platform_for_evaluating_llms_by_human_preference"
        },
        {
            "paper_title": "JudgeBench: A benchmark for evaluating LLM-based judges",
            "rating": 2,
            "sanitized_title": "judgebench_a_benchmark_for_evaluating_llmbased_judges"
        },
        {
            "paper_title": "Justice or prejudice? quantifying biases in LLM-as-a-judge",
            "rating": 2,
            "sanitized_title": "justice_or_prejudice_quantifying_biases_in_llmasajudge"
        },
        {
            "paper_title": "LLMs instead of human judges? a large scale empirical study across 20 NLP evaluation tasks",
            "rating": 1,
            "sanitized_title": "llms_instead_of_human_judges_a_large_scale_empirical_study_across_20_nlp_evaluation_tasks"
        }
    ],
    "cost": 0.016028,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>JuStRank: Benchmarking LLM Judges for System Ranking
10 Jun 2025</p>
<p>Ariel Gera 
IBM Research</p>
<p>Odellia Boni 
IBM Research</p>
<p>Yotam Perlitz 
IBM Research</p>
<p>Roy Bar-Haim 
IBM Research</p>
<p>Asaf Yehudai 
IBM Research</p>
<p>JuStRank: Benchmarking LLM Judges for System Ranking
10 Jun 2025B83E2D36C4A1D3B7C4828CBE888B6664arXiv:2412.09569v2[cs.CL]
Given the rapid progress of generative AI, there is a pressing need to systematically compare and choose between the numerous models and configurations available.The scale and versatility of such evaluations make the use of LLMbased judges a compelling solution for this challenge.Crucially, this approach requires first to validate the quality of the LLM judge itself.Previous work has focused on instance-based assessment of LLM judges, where a judge is evaluated over a set of responses, or response pairs, while being agnostic to their source systems.We argue that this setting overlooks critical factors affecting system-level ranking, such as a judge's positive or negative bias towards certain systems.To address this gap, we conduct the first large-scale study of LLM judges as system rankers.System scores are generated by aggregating judgment scores over multiple system outputs, and the judge's quality is assessed by comparing the resulting system ranking to a human-based ranking.Beyond overall judge assessment, our analysis provides a fine-grained characterization of judge behavior, including their decisiveness and bias.</p>
<p>Introduction</p>
<p>The evaluation of Large Language Models (LLMs) is rapidly adopting the LLM-as-a-judge paradigm (Zheng et al., 2023), where automatic evaluations with LLMs complement the use of human annotators, or even replace them altogether.LLMbased judges are increasingly relied upon to conclude which models exhibit superior performance, whether novel training and inference approaches are beneficial, and ultimately which LLM configurations offer a better value proposition to users.</p>
<p>Since relying on an inaccurate judge will likely result in sub-optimal decisions, this trend lends an urgency to evaluating the performance of the LLM judges themselves.Indeed, recent works attempt to</p>
<blockquote>
<p>=</p>
</blockquote>
<p>"This response is better than the other"</p>
<p>Figure 1: Instance and system level judges make different calls: An instance-level judge (top) is used to make decisions about the quality of individual responses (which may be produced by different systems).</p>
<p>A system-level judge (bottom) is used to make decisions about the overall quality of systems.For clarity, in this illustration, we focus on pairwise decisions.</p>
<p>benchmark judging capabilities, compiling leaderboards of judge performance (Lambert et al., 2024;Tan et al., 2024) as well as analyzing their sensitivities and biases (Wang et al., 2023;Wei et al., 2024;Bavaresco et al., 2024;Feuer et al., 2024;Liu et al., 2024b;Xu et al., 2024;Ye et al., 2024).These works all focus on the instance-level performance of judges.A "good" instance-level judge is expected to make a correct judgment about each response, regardless of the system generating it.For example, given a specific pair of responses, the judge may be asked to determine which one is better (Figure 1,top).This approach is very much in line with prevailing paradigms for model alignment (e.g., RLHF, DPO; Lee et al., 2024b) and synthetic data generation (Yehudai et al., 2024); these often rely on LLM judges and reward models for making</p>
<p>System Ranks</p>
<p>Figure 2: System-level judge pipeline.Schematic of our data generation pipeline for judge system rankings.</p>
<p>instance-level pairwise decisions on the quality of individual responses.</p>
<p>Although judges are evaluated based on their instance-level performance, very commonly they are actually used for making system-level decisions; namely, to compare and rank different models or different configurations (Figure 1, bottom).Crucially, even very good instance-level capabilities do not guarantee accurate model ranking; and at the same time, mediocre performance on instances could still yield a very accurate overall ranking (Dorner et al., 2024, §2).Thus, the systemlevel performance of judges -that is, to what degree they can correctly decide between candidate systems, and produce accurate model performance rankings -remains largely an open question.Furthermore, system-level evaluations can unveil an entire range of under-explored judge qualities, such as being biased towards certain models or making un-calibrated model preference judgments.</p>
<p>In this work we aim to address this gap, and characterize the system-level evaluation capabilities and behaviors of LLM-based judges.To this end, we introduce a novel judge benchmark -JuStRank (Judges for System Ranking).JuStRank compares judges by their ability to correctly rank models, based on agreement with a ground-truth model ranking.JuStRank encompasses a collection of 48 state-of-the-art judges, including both generalpurpose LLMs and reward models.Our large-scale benchmark and analysis allow us to explore the performance and behavior of judges as system rankers.</p>
<p>Our contributions are as follows:</p>
<ol>
<li>
<p>We introduce JuStRank, the first large-scale benchmark of judges for ranking target systems.</p>
</li>
<li>
<p>We quantify the tendency of a judge to exhibit system bias, where some models are judged "unfairly" ( §6.2).</p>
</li>
<li>
<p>We reveal an emergent quality of a systemlevel judge, its decisiveness factor; decisive judges consistently amplify the gap between strong and weak target systems ( §6.1).</p>
</li>
<li>
<p>To facilitate further research into judge behavior, we release our data1 , comprising 1.5M judgment scores given by LLMs and reward models.</p>
</li>
</ol>
<p>The Gap in Judge Benchmarking</p>
<p>In this section, we outline why existing estimations of judge performance are insufficient to decide which judge is best at choosing between target systems (Figure 1, bottom).</p>
<p>At present, users looking for a judge for ranking models, will likely choose it according to the available instance-level judge benchmarks.Yet, from a theoretical standpoint instance-level judge performance does not directly correspond to system-level judge performance (Dorner et al., 2024).</p>
<p>More specifically, instance-level judge evaluations focus on how many errors the judge makes, and do not address the distribution of these errors across systems.</p>
<p>For system-level judge evaluation, however, the error distribution plays a key role, as judge errors may distribute unevenly across systems, impacting their induced ranking (Dorner et al., 2024;von Däniken et al., 2024).For example, a judge may exhibit an unjustifiable preference (positive bias) for responses from a particular system A. Thus, this judge will tend to give A an incorrect ranking, even if it makes very few mistakes on responses from other systems (i.e., has an overall high instancelevel accuracy).Hence, a more uniform distribution of errors -reflecting less biased judgment -is a desirable quality for system-level judges, and one that may lead to a more accurate ranking.</p>
<p>Drawing on this observation, our goal here is to construct a system-level benchmark for judges.As a benchmark tailored for system-level evaluation, it will enable reliably estimating a judge's ability to rank systems; moreover, our ranking-oriented analysis can shed light on judge behaviors and biases, as they occur in real-world data.</p>
<p>Task Formulation</p>
<p>In this work we study the use of LLM-based judges for determining the relative quality of systems2 , over a given set of user instructions (prompts).</p>
<p>Formally, we begin with a set of L systems S = {s l } L l=1 , and K user instructions I = {i k } K k=1 .Each system produces a response for each such user instruction, denoted as R = {r l k } k,l=K,L k,l=1,1 , such that s l (i k ) = r l k (see Figure 2).Judges J = {j p } P p=1 map a pair of instruction i k , and system response r l k to a scalar score that estimates the quality of the response.Each judge has a specific realization for performing this score mapping3 , of the form: j p (i k , r l k ) = Score p k,l .Once a judge j p scores all K × L responses, we can define a scores matrix j p (R) ∈ R K×L where j p (R) k,l = Score p k,l .In order to quantify system-level quality, we must apply an aggregation method, a ∈ A = {a : R K×L −→ R L }.The aggregation method a maps a scores matrix j p (R) to a system-level vector V p,a ∈ R L where each entry, V p,a l , is a single overall quality score for system s l by judge j p .In turn, ordering the system scores in V p,a induces a ranking over the systems set S.</p>
<p>We test the performance of judge j p as a ranker by checking the correlation between the ranking induced by V p,a and a golden ranking for S.</p>
<p>Experimental setup</p>
<p>To explore judge performance and behavior, we utilize responses from multiple systems ( §4.1) and run reward model judges ( §4.2.1) and LLM judges ( §4.2.2) over these responses.To obtain system rankings, we experiment with different aggregation methods ( §4.3) over the judge scores.Finally, the resulting rankings are compared against a gold system ranking, taken from a separate dataset ( §4.4).</p>
<p>System Responses Data</p>
<p>We utilize the Arena Hard v0.1 dataset (Li et al., 2024) for a diverse set of instructions and system responses.The dataset uses a curated set of K = 500 challenging instructions, I.As of September 2024, it includes responses from L = 63 systems, S, totaling about 32K pairs of instructions and their associated system responses, R.</p>
<p>Generating Judgments</p>
<p>For every judge realization, j p , we generate a judgment scores matrix, j p (R), over R. In total, we examine 48 judge realizations, yielding a total of 1.5M individual judge scores (63 systems × 500 instances × 48 judge realizations).</p>
<p>Reward Models</p>
<p>We run multiple reward models over R. While their exact architectures vary, reward models generally produce a scalar quality score for a given pair of an instruction and a system response.</p>
<p>We utilize the following reward models: ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024), Eurus-RM-7b (Yuan et al., 2024), InternLM2-7breward, InternLM2-20b-reward (Cai et al., 2024), Skywork-Reward-Llama-3.1-8B-v0.2(Liu et al., 2024a), Llama-3-OffsetBias-RM-8B (Park et al., 2024), GRM-Llama3.2-3B-ft (Yang et al., 2024), URM-LLaMa-3.1-8B(Lou et al., 2024).</p>
<p>LLM Judge Realizations</p>
<p>Unlike dedicated reward models that produce a single score, generative LLMs can be prompted to judge in multiple ways.Thus, for every LLM we examine several judge realizations.</p>
<p>Absolute judgment -Numeric score (Numeric)</p>
<p>The LLM judge is given an instruction and system response, and is asked to provide a quality score for the response between 0 and 100.</p>
<p>Absolute judgment -Textual score (Likert) The judge provides a quality score of the response on a Likert (Likert, 1932) scale with 5 labels: [Very Bad, Bad, Mediocre, Good, Very Good].We then convert the textual judgments to scores in [1 − 5].</p>
<p>Absolute judgment -Token probablities (TokenProbs) The task is framed to the judge as a yes/no question: Is this a good response?.We then extract the top log-probabilities for the first generated token, and specifically look at the probabilities for the tokens yes or no.The judgment score [0.0 − 1.0] is the sum of probabilities for yes divided by the sum of probabilities for yes and no.</p>
<p>Comparative judgment -Anchor model (Anchor) Here the judgment task is comparative, i.e., the judge is asked to state a preference between two responses rather than an absolute quality judgment of a given response.Conducting paired comparisons between a system and all other systems is unfeasible; thus, we follow Li et al. (2024) and use the responses of GPT-4-0314 as anchors to which the responses of other systems are compared.Given an anchor response and a system response, we ask the judge which one it prefers.The output is then converted to scores in [−2, +2] (where 0 indicates a tie, and +1 / +2 indicate slight/strong preference for the system response over the anchor response, respectively).</p>
<p>In total, we collect judgments from 10 LLMs and 4 realizations, yielding 40 LLM judges.Prompts for all realizations are provided in Appendix G.</p>
<p>We use the following generative LLM judges: Llama-3.1-405B-Instruct(Dubey et al., 2024), Llama-3.1-70B-Instruct,Llama-3.1-8B-Instruct,Llama-3-70B-Instruct, Mixtral-8x22B-Instruct-v0.1, Mixtral-8x7B-Instruct-v0.1 (Jiang et al., 2024), Mistral-Large-Instruct-2407, Qwen2.5-72B-Instruct,GPT-4o and GPT-4o-mini.</p>
<p>Aggregations</p>
<p>Given the raw judgment scores of each judge, j p (R), there are multiple ways to construct a rank-ing of the 63 target systems.We calculate rankings using Win-rate aggregation, Mean aggregation, Median aggregation, and BT (Bradley-Terry) aggregation.Details are provided in Appendix B.
RewardBench JuStRank 0.0 0.2 0.4 0.6 0.8 1.0 Judge Normalized Score ArmoRM-Llama3-8B Eurus-7b GRM-Llama3.2-3B Llama-3-OffsetBias-8B Skywork-Llama-3.1-8B URM-LLaMa-3.1-8B GPT-4o-mini Internlm2-20b Internlm2-7b Llama-3-1-70b</p>
<p>Gold Ranking -Chatbot Arena Battles</p>
<p>Human preference data from Chatbot Arena (Zheng et al., 2023) serve as our groundtruth reference for the relative quality of systems.</p>
<p>Chatbot Arena relies on human-annotated "battles" between system responses to produce a system 0.4 0.5 0.6 0.7 0.8</p>
<p>Agreement with Chatbot Arena Ranking ( )
Qwen2.5-72B-Instruct Mistral-large-instruct-2407 GPT-4o-2024-11-20 Llama-3-1-405b-instruct-fp8 GPT-4o-mini-2024-07-18 Llama-3-1-70b-instruct Mixtral-8x22B-instruct-v0.1 Llama-3-70b-instruct Llama-3.1-8B-Instruct Mixtral-8x7B-instruct-v0.1 Likert Numeric</p>
<p>Anchor TokenProbs</p>
<p>Figure 4: LLM judge realizations.Kendall's Tau correlations (±95% bootstrapping CI) between the system rankings produced by various LLM judge realizations ( §4.2.2) and the gold system ranking from Chatbot Arena.The plot depicts results for the BT aggregation method; for the full results, refer to App.Table 2.</p>
<p>ranking.We use the English Hard Prompts subset 4 of their data.We chose this subset as its distribution of user instructions has been shown (Li et al., 2024) to match that of our system response data ( §4.1).We extract the data and ranking following the official code (see Appendix C).</p>
<p>Given a system ranking produced by a judge, we quantify judge performance via the correlation between its ranking and the reference ranking from Chatbot Arena.Simply put, we assume that a ranking given by a good automated judge would have a high agreement with the ranking compiled from human judgments.</p>
<p>JuStRank -Judge Performance Results</p>
<p>Table 1 depicts the 10 top-performing judges on JuStRank, based on their ranking agreement (τ ) with the ground-truth human ranking from Chatbot Arena.For each judge model, the best-performing realization and aggregation method is shown.</p>
<p>As seen in the table, there are both LLMs and reward models that reach decent agreement with the gold ranking.Moreover, several 8B-parameter reward models are on par with much larger LLMs on the task of system ranking.Thus, we see that reward models, which are explicitly trained to make instance-level decisions between pairs of responses, can excel at the system-level ranking task as well.</p>
<p>Note that an identical correlation score with the ground-truth ranking does not indicate that the judges produce the same ranking; rather, each judge has a different pattern of agreement with 4 Chatbot Arena Hard Prompts the ground-truth.Correlations among the judges themselves are shown in App.Fig. 9.</p>
<p>Comparison to Instance-Level Performance In Figure 3 we compare our system-level judge leaderboard to the instance-level benchmark Reward-Bench (Lambert et al., 2024).The results demonstrate that better instance-level judges are not always better system rankers, highlighting the discrepancy between the two tasks.Thus, JuStRank offers a novel perspective on judge ability.However, there may be additional factors at play as well.</p>
<p>For LLM judges, we use a slightly different realization from the comparative prompts used for RewardBench.Moreover, since creators of reward models aim to do well on RewardBench, it is possible that some newer reward models are slightly overfitted to this test distribution.</p>
<p>Effects of LLM Realizations</p>
<p>Figure 4 depicts the performance of the LLM judge models by their realization ( §4.2.2).The plot demonstrates that the choice of realization has a considerable effect on the system ranking quality; this appears to be nearly as important as the identity of the LLM used.We confirm this finding using statistical variance analysis (Appendix D).</p>
<p>Many works recommend asking LLMs for comparative rather than absolute judgments (Zheng et al., 2023).However, in our experiments the comparative realization (Anchor) exhibits lower performance, with the notable exception of GPT-4o.The best realizations overall were Numeric and Likert, where the judge is asked to provide a ver-Figure 5: Predicted pairwise win-rates.Each point represents a win-rate between a pair of systems W R(s a , s b ) (App.E).The x-axis denotes the gold win-rate from Chatbot Arena, and the y-axis denotes the predicted win-rate as derived from the judge scores.The diagonal marks an exact match between the predicted and gold win-rate; the quadrants signify whether the predicted winning system is the same (green) or different (red) from the gold winning system for this pair.Note that every pair is represented twice (e.g., W R(s a , s b ) = 0.2, W R(s b , s a ) = 0.8).</p>
<p>balized quality score.This is in line with findings from Tian et al. (2023), who report better calibration with verbalized LLM confidence scores.The higher performance for both Numeric and Likert realizations -compared to Anchor and TokenProbs -is statistically significant (App.D).</p>
<p>We also note that each realization induces a characteristic distribution of judge scores, D p , such that Score p k,l ∼ D p .Notably, the LLM judges tend to produce particular score values more often than others.Refer to Appendix A for more details.</p>
<p>Judge Behavior</p>
<p>Next, we explore more fine-grained judge behaviors, beyond the bottom-line system rankings.</p>
<p>To that end, we focus on the judgment task of pairwise system preference, as this is the foundation of system ranking tasks.As in §5, our aim is to gain an understanding of judge performance and characteristics, by comparing judge behavior on pairwise system preference to ground-truth data.</p>
<p>Pairwise Win-Rates For every judge j p , and for every pair of systems (s a , s b ), the win-rate of s a , denoted by W R p (s a , s b ), is the number of instances where it received a higher score than s b , divided by the number of non-tied instances (cf.Appendix E).Thus, we calculate the pairwise win-rate for each system pair according to each judge.Note that the win-rates are calculated on the scores matrix j p (R), i.e., before applying an aggregation method.</p>
<p>Gold Win-Rates Similarly, we extract gold pairwise win-rates, W R g , from Chatbot Arena (App.C). 59 systems appear both in our response data ( §4.1) and in Chatbot Arena; in total, we have both judge and gold data for 968 head-to-head comparisons between pairs of systems.</p>
<p>Some Judges are Particularly Decisive</p>
<p>Figure 5 depicts the relationship between predicted win-rates and gold win-rates for several judges.The quadrants in the figure indicate whether the judge's pairwise preference decision is aligned with the gold preference.As can be expected, the judge predictions in Figure 5 are often centered around the ground-truth win-rates determined by humans.But strikingly, some judges exhibit unique prediction patterns, yielding win-rates that are consistently closer to the extremes (0.0 / 1.0) compared to the human data.For instance, for pairs with a ground-truth win-rate of ∼0.8, the predicted winrate in the judgments of Llama-405B (Fig. 5, right) tends to exceed 0.9.Put simply, when faced with a response from a strong system, the judge is very likely to prefer it over the response of a less capable system, even where human judges are less decisive.</p>
<p>This sigmoidal win-rate prediction pattern resembles behaviors previously described for classifier calibration (Silva Filho et al., 2023), where classifiers may exhibit "overconfidence" in their predicted probabilities.5 Thus, following Kull et al. (2017), we quantify judges' decisive (overconfident) behavior by fitting the cumulative beta distribution function to the win-rate prediction plots.This enables describing judge prediction behav- ior in terms of a single fit value α = β, where α ∈ [0, ∞], a value of α = 1 represents no overor under-decisiveness, and larger values represent more decisive behavior (refer to Appendix F for details).Figure 6a and App.Fig. 11 depict the beta curve fit for win-rates of various judges.</p>
<p>Figure 6b compares judge realizations in terms of their decisiveness behavior.We see that LLM judges are usually more decisive when directly asked to provide a quality score, and in particular a textual one (Likert); in contrast, the realization that relies on token probabilities (TokenProbs) does not give rise to such a pattern, and can even result in judge "indecision" (i.e., α &lt; 1).</p>
<p>This pattern can be explained from two directions.First, the human judgments ( §4.4) were collected from multiple individuals, who likely have differing preferences; this may introduce some noise that could lead to less extreme win-rates in the gold data.The other factor is the judges, who may rely on certain heuristics to identify responses from strong systems (Feuer et al., 2024), leading to more extreme win-rates in the judge data.While the variance between judges (Fig. 6b) supports the latter, we cannot determine this conclusively.</p>
<p>In practical terms, extreme win-rates can be beneficial to users, as they increase the likelihood of a correct system preference decision given a smaller set of responses (see Ashury Tahan et al., 2024).</p>
<p>Bias Towards Specific Systems</p>
<p>A major concern when using judges for system preference is judge bias -a judge may treat a specific system "unfairly", by consistently judging its responses too favorably or too harshly (see Von Däniken et al., 2024).</p>
<p>We define the bias B p sa of judge j p towards system s a by the expectation over the differences between the predicted and gold win-rates, over all systems that s a interacts with.Formally, 6 In other words, if according to j p the win-rates of system s a are (on average) higher than those in the human data, we will say that j p exhibits positive bias towards it; and if they are lower than the ground-truth, j p would be said to exhibit negative bias.
B p sa = E s b ∈S (W R p (s a , s b ) − W R g (s a , s b )).
Note that the decisiveness behavior in §6.1 directly entails a general bias pattern in some judgesnamely, a positive bias towards strong systems, and a negative bias towards weak ones.Thus, we calculate a decisiveness-corrected bias, B ′ sa p , where the gold win-rate W R g is replaced by W R g ′ p , i.e., the predicted value for the gold win-rate on the beta distribution fit for judge j p (App.F).</p>
<p>We observe some consistent trends of systemspecific bias that are common across judges.Figure 7 depicts systems for which there is high bias across judges.For instance, most judges exhibit a strong positive bias towards Athene-70B, to the extent that it is often ranked by them as the #1 system.In contrast, GPT-4-0613, which is 27th in the gold ranking, receives negative bias, resulting in a median rank of 38 among the judges.</p>
<p>We also ask whether LLM judges exhibit selfbias (Xu et al., 2024;Panickssery et al., 2024), i.e., bias towards the system that uses the same underlying LLM.While we find some instances of  self-bias, this is not a consistent effect across judge realizations (App.Table 3).</p>
<p>To quantify the overall propensity of a judge for bias, we measure the standard deviation of its bias over all systems, δ = σ s∈S (B ′ p ).The bias measure for each judge is presented in App.Table 4.</p>
<p>Characterizing Judge Behaviors</p>
<p>We have shown that beyond their overall ranking capability ( §5), judges exhibit distinct traits in their system-level judgments -in particular, they show different levels of decisiveness ( §6.1), and overall propensities for bias ( §6.2).Interestingly, each of these traits (cf.App.Table 4) is correlated to the ranking quality τ , with r = 0.55 for the α decisiveness measure, and r = −0.56 for the bias propensity δ.At the same time, these marked traits are -by design -uncorrelated with each other (r = −0.07 between α and δ).Thus, our analyses reveal global system-level judge traits, ones that remain hidden when assessing judges from an instancelevel perspective.</p>
<p>Related Work</p>
<p>Applying and assessing automatic metrics for system-level evaluation has been studied for decades, in particular for natural language generation tasks (Reiter and Belz, 2009;Louis and Nenkova, 2013;Deutsch et al., 2022).In the context of LLM-based judges, however, system-level evaluation is still under-explored.</p>
<p>Prior works on LLM-based judges have opted for an instance-level evaluation approach, curating benchmarks of responses with ground-truth quality annotations in order to evaluate judge performance.Most prominently, RewardBench (Lambert et al., 2024) compares dozens of judges (including reward models, generative LLMs, and classifiers) on the task of deciding between pairs of outputs.Reward-Bench aims to identify the most suitable judges for model alignment, e.g., for use in RLHF; in contrast, our work measures judges in terms of their ability to compare the performance of candidate systems.Another recent instance-level benchmark, JudgeBench (Tan et al., 2024), focuses on curating challenging response pairs where the judge must discern subtle errors.</p>
<p>Multiple works are dedicated to analyzing various biases (Ye et al., 2024) and undesirable behaviors exhibited by judges.These include positional bias (Wang et al., 2023), verbosity bias (Saito et al., 2023;Chen et al., 2024) and self-bias (Xu et al., 2024), as well as sensitivity to prompts (Wei et al., 2024), source datasets (Bavaresco et al., 2024), epistemic markers (Lee et al., 2024a) and style (Feuer et al., 2024;Liu et al., 2024b).</p>
<p>Several popular benchmarks rely on LLM judges to produce leaderboards of state-of-the-art systems.Such benchmarks -e.g., Arena Hard (Li et al., 2024) and AlpacaEval (Dubois et al., 2024) -do perform a system-level validation of their resulting leaderboards against other benchmark rankings (see Perlitz et al., 2024).However, such efforts are limited to validating the particular dataset and judge setup chosen for the benchmark (usually with GPT-4 as the judge), rather than comparing and analyzing the performance of different judge models and implementations.Thakur et al., 2024 conduct a task-specific system-level evaluation of judges, over the TriviaQA (Joshi et al., 2017) dataset.Compared to their work, the present study is on a larger scale and offers novel metrics and analyses on system-level judge behaviors.</p>
<p>Discussion</p>
<p>The usage of LLM-based judges is continually expanding.Moreover, many research papers -proposing novel architectures, algorithms and training methods -rely heavily on system-level evaluations using judges as evidence for the utility of their approach.But without evaluating the judges on such system-level tasks, how can one know whether to trust such evaluations, and their conclusions?</p>
<p>We are the first to investigate on a large scale the performance of LLM-based judges on the system ranking task.Our resulting benchmark, JuStRank, will assist users and researchers in choosing the judge best suited for their needs.</p>
<p>Choosing a judge requires many fine-grained decisions.A user can decide which reward model or LLM to use as the judge; opt for relative judgments or absolute scores; try various prompts; apply different aggregations to compile a ranking, etc.Furthermore, these decisions may interact in non-trivial ways (e.g., the distribution of scores a judge tends to output can dictate which aggregations will work well).Indeed, our findings confirm that such decisions substantially affect system-level judgments ( §5), and thus are quite likely to change the model selection of an end user, or flip the conclusions of an NLP research paper.</p>
<p>Our system-level approach has multiple additional benefits.First, it forces the evaluation of judges to be representative with respect to the distribution of systems that generate the responses.</p>
<p>In existing instance-level benchmarks this factor is not taken into account, and likely results in less accurate judge evaluations.Second, it affords a new perspective on what it means for a judge to be biased; on the one hand, we discover some decisiveness trends ( §6.1) that may actually be useful for making correct preference decisions, and increasing the separability between systems; and on the other, we report some problematic biases that directly distort the judgment of particular systems ( §6.2).An important avenue for future work is to connect our findings here to the existing literature on judge biases (Ye et al., 2024), and understand to what extent both of these behaviors stem from particular LLM style attributes (Feuer et al., 2024).</p>
<p>Given this vast and complex space, our work is admittedly only a first step in understanding the behavior of judges for ranking and selecting LLMs.We release our raw judgment scores data, and encourage the community to explore these issues further: for instance, by training dedicated system-level judges, exploring judge ensembles, or studying other aggregation approaches.We believe that JuStRank can facilitate such research directions, as it can be easily extended to new judges without requiring additional human annotations.</p>
<p>Our hope is that both practitioners and researchers can benefit from JuStRank, by making more informed choices of judges for their needs.</p>
<p>Conclusion</p>
<p>In this work we conducted the first comprehensive evaluation of system ranking by LLM judges.</p>
<p>We tested a wide array of judges, including reward models and different realizations of generative LLMs, over a large collection of systems.</p>
<p>We collected system responses over a diverse set of instructions.The judges scored each response, and we compiled a ranking by aggregating the judgments over all responses.Then, the quality of the judge's system ranking was compared to a human ranking, producing the JuStRank leaderboard.</p>
<p>JuStRank allows users to pick judges that are better aligned with the goal of choosing between different models and configurations.JuStRank demonstrates that judge ranking abilities are not directly tied to LLM size or overall quality, and that some dedicated reward models are on par with leading LLM judges.Moreover, our analysis reveals emergent judge traits -decisiveness and bias -that are strongly correlated with their ranking ability.</p>
<p>Limitations</p>
<p>The gold reference data -the English Hard Prompts subset of Chatbot Arena -does not include user instructions or responses.Hence, we collect judgment data over Arena Hard, which contains a large set of instructions and responses.This raises some questions regarding our ability to directly compare the LLM judges and human judges.However, given that Arena Hard was designed to match the distribution of user instructions in English Hard Prompts (see Li et al., 2024), we assume that these datasets are sufficiently similar.</p>
<p>Our analyses of LLM judge realizations are, by necessity, limited to the specific realization prompts that we used.Several studies show that LLMs (Mizrahi et al., 2024) as well as LLM judges (Wei et al., 2024) are brittle with respect to prompt phrasing, and hence this may have had an impact on the results.In addition, there can be some variations in judge responses depending on the exact API and inference implementation used.</p>
<p>As in multiple other works, here we treat human preference as a single concept.In practice, however, preference is inherently subjective, and is composed of numerous dimensions (e.g., helpfulness, safety, style, coherence etc.).For instance, one individual may prefer succinct model responses while another would prefer more detailed answers.Thus there is no single "human preference", but rather a collection of preference decisions that depend on the annotation guidelines, cultural context, and human idiosyncrasies (Conitzer et al., 2024;Kirk et al., 2024).</p>
<p>Note that following Peyrard et al. (2021), as well as Chatbot Arena (Chiang et al., 2024), we generally regard the ground-truth quality of a system in terms of the Bradley-Terry model; simply put, a better system is a system that "wins" more often.Thus, in this work we do not directly consider the quality difference in system responses per instance, i.e., beyond counting wins/losses.Still, some of the aggregation methods we use (e.g., mean) implicitly reflect other perspectives on system quality.</p>
<p>All of our analyses are performed on heterogeneous datasets of user instructions to LLMs.Thus, while we study judges through the lens of generalpurpose LLM usage, we cannot draw conclusions on judge behavior that is task-specific (or in specialized domains), nor on performance in languages other than English (Gureja et al., 2024).The issue of task, domain, and language-specific judge behavior is thus an important avenue for future work.</p>
<p>A Judge Score Distributions</p>
<p>Figure 12 depicts the score distributions, D p , of the judges in our data.</p>
<p>Reward model distributions</p>
<p>The reward models exhibit continuous score distributions.As seen in Figure 12, these distributions vary in the range of scores, as well as in the shape of the distribution.Some reward model judges have a narrow range of scores, e.g., −0.1 to 0.4, whereas in others it is much wider, e.g., −3000 to 5000.Similarly, some distributions are more symmetric while others have peaks at more extreme values.However, all distributions are uni-modal, with a single peak.Moreover, we note that the continuous nature of these judgment scores also entails an absence of ties between the judged responses.</p>
<p>LLM Numeric distributions As shown in Fig-</p>
<p>ure 12, even though the LLM judges are given a wide range of possible judgment scores ([0 − 100]), in practice they tend to prefer specific score values.This results in many ties when comparing responses from different systems.</p>
<p>LLM Likert distributions</p>
<p>Similarly to the Numeric distributions, the Likert realizations put most of their probability mass on specific scores, which leads to an even greater inclination towards ties (as here they are limited to a smaller range of scores).</p>
<p>LLM TokenProbs distributions TokenProbs scores tend to be extreme, namely very close to either 0.0 or 1.0.Thus, in many cases the score gap between responses is extremely small.This can result in low judge robustness (see the error bars in Figure 4), as well as a higher sensitivity to the choice of aggregation method.</p>
<p>LLM Anchor distributions The distribution for</p>
<p>Anchor judgments is mainly tied to the quality of the anchor system relative to the other systems.However, we see that it is also affected by the characteristics of the judge.For example, we see in Fig. 12 that Llama-3.1-8Bexhibits indecision, rating most responses as comparable to those of the anchor.In addition, for some judges, the proportion of −1 scores (i.e., the response is slightly worse than the anchor) or 1 scores (the response is slightly better than the anchor) is unusually low.</p>
<p>B Aggregation Methods</p>
<p>Given the raw judgments of each judge, j p (R), there are multiple aggregation methods, a, that con-struct a ranking over all the target systems.Here, we calculate rankings using Win-rate aggregation, BT aggregation, Mean aggregation, and Median aggregation.In the following, we provide further details on each aggregation.</p>
<p>Mean &amp; Median Aggregation These aggregation methods map a score for each system, s l , by relying solely on the scores assigned to its responses by judge j p .In other words, the mapping of V p,a l by a depends only on the column corresponding to system s l in j p (R). Accordingly, these aggregations can be viewed as an operation on the columns of the scores matrix j p (R).Specifically, for the Mean aggregation, V p,a l = 1 K Σ K k=1 Score p k,l .Similarly, Median aggregation is the median of the vector j p (R) * l .</p>
<p>We note that for realizations with discrete score distributions (see §A), many systems will likely share the same median score; in this case, the Median aggregation method fails to separate the systems.Hence, Table 2 contains only a handful of LLM judges with Median aggregation, all using the TokenProbs realization.</p>
<p>Win-rate Aggregation This aggregation maps each system based on its proportion of wins over other systems, averaged over all instructions i k ∈ I. Bradley-Terry Aggregation Following Chiang et al. (2024), we use the vector of Bradley-Terry (BT) coefficients (Bradley and Terry, 1952) as system scores.</p>
<p>For calculating the BT scores we use the implementation of the Chatbot Arena official notebook7 .Whereas Chiang et al. (2024) apply this method for battles between responses with a human judge, we apply it over our LLM-based judge data, i.e., each "battle" is a comparison between the judge scores Score p k,a , Score p k,b for a response generated by systems s a and s b .</p>
<p>When there are no ties, e.g., for the reward model judges, this aggregation produces similar rankings to the win-rate aggregation.</p>
<p>C Chatbot Arena Data</p>
<p>The data for the Chatbot Arena LLM leaderboard (https://lmarena.ai)consists of "battles" between systems over the same instructions.In these battles, users indicate a preference (or a tie) between a pair of responses generated by different LLMs (Zheng et al., 2023;Chiang et al., 2024).</p>
<p>We use their public data file from August 2024 8 , and follow the official notebook 7 to extract the raw data, deduplicate it, and calculate the overall system rankings.This dataset includes the human preference judgments and names of the participating systems, but not the instructions or system responses for the battles.</p>
<p>Here we limit the analysis to the English Hard Prompts subset of their data 9 (300K battles).Notably, Arena Hard was specifically designed to match the distribution of user instructions in the English Hard Prompts subset, as described by Li et al. (2024).We follow their code to construct a full system ranking based on these 300K battles, using Bradley-Terry coefficients.This yields a score for each system in their data, including 59 systems that are also in our system responses data ( §4.1)</p>
<p>Out of this full English Hard data, we also extract a total of 113K battles that were not judged by humans as ties, and that are between pairs of systems which appear in our responses data.We then use those to calculate win-rates between pairs of systems ( §E), yielding a total of 968 system pairwise win-rates.Note that the Chatbot Arena data does not contain battles between every possible pairing of systems, and thus we do not have winrates for all combinations of the 59 systems under consideration.In addition, we limit the analysis to system pairs with at least 10 non-tied battles.</p>
<p>D Statistical Analysis of Judge Performance</p>
<p>In §5 and Table 2 we report results of agreement with the gold ranking (τ ) for various judge pipelines.Each pipeline consists of a chosen judge model, a realization ( §4.2.2) and an aggregation method ( §4.3, App.B).We focus on the LLM judges and perform a three-way ANOVA (analysis of variance), with the ranking correlation τ as a dependent variable and the model, realization and aggregation as factors.In addition to the variance analysis estimating the effects of these factors, we perform post-hoc pairwise comparisons to ask whether certain configurations (i.e., a specific realization/aggregation) outperform the others.We conduct all analyses using 8 Chatbot Arena data 9 Chatbot Arena Hard Prompts IBM SPSS Statistics v30.0.</p>
<p>The ANOVA shows that both the judge model and the realization have a strong influence on τ , with an effect size (Partial Eta-Squared) of η 2 = 0.81 for the judge model (p &lt; 0.001; F = 36.0),η 2 = 0.51 for the realization (p &lt; 0.001; F = 26.6), and η 2 = 0.78 for the interaction effect between model and realization (p &lt; 0.001; F = 10.1).In contrast, the aggregation methods were not found to have a significant effect on τ (η 2 = 0.02; p &gt; 0.5).</p>
<p>We also perform Tukey's HSD (Tukey, 1949) post-hoc tests to compare the means of the variables.The analysis indicates that the both the Numeric (mean τ = 0.75; σ τ = 0.06) and Likert (τ = 0.74; σ τ = 0.07) realizations are significantly better than the Anchor (τ = 0.71; σ τ = 0.07) and TokenProbs (τ = 0.68; σ τ = 0.13) realizations (all p values &lt;= 0.002).The differences between aggregation methods are not statistically significant.</p>
<p>E Pairwise Win-Rates</p>
<p>We denote the win-rate of system s a over system s b as W R(s a , s b ) p where p denotes the judge upon which the win-rate was calculated, and p ∈ J ∪{g}, where g stands for human gold data.</p>
<p>The win-rate of system s a over system s b according to judge j p over the set of instances I is calculated as the proportion of instances where the score given by j p to the response generated by s a surpasses that of system s b , where ties are excluded.Namely W R p (s a , s
b ) = 1 K−|T p a,b | Σ K k=1 I(Score p k,a &gt; Score p k,b ) Where T p a,b = {i k |Score p k,a = Score p k,b }, and I(•) denotes the indicator function. Notice that W R p (s a , s b ) = 1 − W R p (s b , s a ).
To quantify the agreement between the judge and gold win-rates we also define an Accuracy metric.This measures the proportion of pairs where the judge pairwise system preference decisions are in agreement with those of the human gold-data.In other words, we want to count the pairs that appear in the first and third quadrants in Figure 5; namely, the pairs where the judge and gold win-rate are both bigger than 0.5, or the pairs where both are lower than 0.5, representing agreement on the winning system.For that, we denote all the pairs of systems we have in the gold data as {s a m , s b m } M m=1 .Now the Accuracy is defined as follows:
Acc p W R = 1 M Σ M m=1 I(I(W R p (s a m , s b m ) &gt; 0.5) = I(W R g (s a m , s b m ) &gt; 0.5))
Additionally, we define a second metric, the Mean Squared Error over all win-rate pairs.</p>
<p>M SE
m W R = 1 M Σ M m=1 (W R g (s a m , s b m ) −W R p (s a m , s b m )) 2 .
The Acc p W R scores are in high agreement with the JuStRank judge ranking quality scores τ (Pearson correlation of r = 0.96 for the BT aggregation, r = 0.79 for the Mean aggregation).This highlights the direct link between judges' ability to rank systems and their performance on pairwise system preference.</p>
<p>The M SE p W R scores have a low correlation with the JuStRank judge τ scores (r = −0.19 for the BT aggregation, r = −0.07 for the Mean aggregation).This can be explained by the decisiveness effect ( §6.1), where judges deviate substantially from the gold win-rate, but mostly toward the stronger system in the pair.</p>
<p>F Beta Distribution Fit</p>
<p>Following Kull et al. (2017), we model the relation between judge and gold win-rates using the cumulative distribution function (CDF) of the Beta distribution.We parameterize the distribution such that both shape parameters α and β are equal (α = β).</p>
<p>The CDF of the Beta distribution, defined over the interval [0, 1], for α = β ∈ [0, ∞] provides a wide range of function fits: a linear y = x fit for α = 1, a sigmoidal fit for larger α values, and approaching a step function as α → ∞.These attributes make it particularly suited for our data characteristics.</p>
<p>Given a set of data points {(W R p (s a m , s b m ), W R g (s a m , s b m )} M m=1 , where W R p (s a m , s b m ) ∈ [0, 1] represents the judge win-rate and W R g (s a m , s b m ) ∈ [0, 1] denotes the gold win-rate between system, s a m and s b m .We fit the Beta CDF by optimizing the shape parameter α.The optimization objective is minimizing the sum of absolute errors (SAE) between the judge win-rate, W R p (s a m , s b m ), and the predicted values from the Beta CDF.In order to capture the behavior across the entire range of win-rates, we weight the errors by the distance of W R p from 0.5:
SAE = M m=1 γ(W R p (s a m , s b m )) • W R p (s a m , s b m ) −F Beta (W R g (s a m , s b m ); α)
where F Beta (x; α) denotes the Beta CDF with shape parameters α = β, and γ is the distance of W R p from 0.5.</p>
<p>The optimization was performed using the scipy.optimize.minimize10function, with the parameter (α) constrained to a reasonable range [0.1, 10000].This approach efficiently identified the best-fit parameter (α).</p>
<p>The resulting Beta CDF closely captures the empirical data distribution, as validated both quantitatively, through low SAE, and qualitatively via visual inspection.Figure 11 depicts the fitted Beta CDF curve and the observed data points, demonstrating the effectiveness of this approach for modeling the judges' predicted win-rate distribution.</p>
<p>G LLM Judge Prompts</p>
<p>Below we list the prompts we use for each LLM judge realization ( §4.2.2).</p>
<p>Numeric</p>
<p>Here is a user input and a model response.On a scale of 0 to 100, to what extent is this a good response for the given input?Reply with your rating score without any preceding explanation.Input: [user instruction] Response: [system response] Rating (0-100):</p>
<p>Likert</p>
<p>Here is a user input and a model response.To what extent is this a good response for the given input?Provide a rating from one of the following choices: 'Very Bad', 'Bad', 'Mediocre', 'Good', 'Very Good'.Reply using the format of [</p>
<p>Figure 3 :
3
Figure 3: Comparison to RewardBench.The plot depicts the relative performance of judges present in both JuStRank and RewardBench(Lambert et al., 2024).For comparison, we perform Min-Max normalization over the judge performance scores (accuracy for Reward-Bench, Kendall's Tau for our results).Results shown are for the BT aggregation method; the LLM judges use the Anchor realization, which is closest to the setting in RewardBench.Plots for the different RewardBench subsets are shown in Appendix Figure8.</p>
<p>Figure 6 :
6
Figure 6: Beta distribution fit of pairwise win-rates.(a): Judge beta fit example.Each point represents the win-rate between a pair of systems, W R(s a , s b ); the curve and α value describe a fit to the beta distribution (App.F).Plots for all judges are in App.Fig. 11.(b): Decisiveness by judge realization.Cell values denote the decisiveness behaviors of different LLM judge realizations, as described by the α value for their win-rate distribution.</p>
<p>Figure 7 :
7
Figure7: System-specific judge biases.The plot depicts win-rate biases of judges towards specific systems, with respect to the ground-truth win-rates from Chatbot Arena (after correction for the beta distribution fit of each judge).This plot portrays select systems with high bias; the full heat map, including all judge realizations and all systems, is shown in App.Fig.10b.</p>
<p>where I(•) denotes the indicator function.</p>
<p>[rating]], for example: '[[Mediocre]]' Input: [user instruction] Response: [system response] Rating: TokenProbs Here is a user input and a model response.Is this a good response for the given input?Answer with only yes/no.Input: [user instruction] Response: [system response] Good response?(Yes/No): Anchor Here is a user input and responses from two assistants, A and B. Which response is better?You must output only one of the following choices as your final verdict with a label: 1. Assistant A is significantly better: [[A&gt;&gt;B]] 2. Assistant A is slightly better: [[A&gt;B]] 3. Tie, relatively the same: [[A=B]] 4. Assistant B is slightly better: [[B&gt;A]] 5. Assistant B is significantly better: [[B&gt;&gt;A] Example output: "My final verdict is tie: [[A=B]]".&lt;|User Prompt|&gt; [user instruction] &lt;|The Start of Assistant A's Answer|&gt; [system response] &lt;|The End of Assistant A's Answer|&gt; &lt;|The Start of Assistant B's Answer|&gt; [anchor system response] &lt;|The End of Assistant B's Answer|&gt; Final Verdict:</p>
<p>Figure 11 :
11
Figure 11: Beta distribution fit of pairwise win-rates (Part 4/4).Each point represents the win-rate between a pair of systems, W R(s a , s b ); the curve and α value describe a fit to the beta probability distribution.Refer to Appendix F for details.</p>
<p>Table 1 :
1
Top 10 judges by ranking performance.Judges are sorted by the Kendall's Tau correlation between their overall system ranking and the gold ranking from Chatbot Arena ( §4.4).For every judge model, only the best-performing realization and aggregation method is shown.For the full results, refer to Appendix Table2.
Realization AggregationAgreement (τ )with Gold RankingQwen2.5-72B-InstructLikertWin-Rate.83URM-LLaMa-3.1-8BRewardMean.82GPT-4o-2024-11-20AnchorMean.82Llama-3-1-405b-instruct-fp8 NumericMean.81Mistral-large-instruct-2407LikertBT.81GPT-4o-mini-2024-07-18NumericWin-Rate.81ArmoRM-Llama3-8B-v0.1RewardMean.80Llama-3-1-70b-instructNumericWin-Rate.80Skywork-Llama-3.1-8B-v0.2 RewardMean.79Llama-3.1-8B-InstructTokenProbs Mean.78
JuStRank Judge Scores data
Henceforth, we will use the term System to refer to a target model or pipeline that performs a task, and Judge for one that is asked to score (or compare) the quality of such systems. Generative LLMs can act as both systems and judges.
We note that some realizations, such as the comparative realization in §4.2.2, may incorporate a separate set of responses to perform the judgment.
Note, however, that the behavior in our case does not reflect judge probability scores, but rather the empirical ratio of instances where the responses {r l k } l=L l=1 of a system k are preferred over those of another system.
Our formulation of bias aims to reflects the practical impact of the judge bias on system preference. This is in contrast to the Favi-Score metric proposed by Von Däniken et al.(2024), which is decoupled from the overall accuracy of preference decisions.
Arena official notebook
SciPy Documentation for scipy.optimize.minimize
Figure 11: Beta distribution fit of pairwise win-rates (Part 1/4)
Figure 11: Beta distribution fit of pairwise win-rates (Part 2/4)
Figure 11: Beta distribution fit of pairwise win-rates (Part 3/4)
Figure 12: Judge score distributions (Part 1/3)
Figure 12: Judge score distributions (Part 2/3)
Figure 12: Judge score distributions (Part 3/3).
athene-70b-0725 claude-2.0claude-2.1 claude-3-5-sonnet-20240620 claude-3-haiku-20240307 claude-3-opus-20240229 claude-3-sonnet-20240229 command-r-plus command-r dbrx-instruct-preview deepseek-coder-v2 gemini-1.5-flash-api-0514gemini-1.5-pro-api-0409-previewgemini-1.5-pro-api-0514gemini-pro gemma-1.1-2b-itgemma-1.1-7b-itgemma-2-27b-it gemma-2b-it gemma-7b-it glm-4-0116 glm-4-0520 gpt-3.5-turbo-0125gpt-3.5-turbo-0314gpt-3.5-turbo-0613gpt-3.5-turbo-1106gpt-4-0125-preview gpt-4-0314 gpt-4-0613 gpt-4-1106-preview gpt-4-turbo-2024-04-09 gpt-4o-2024-05-13 gpt-4o-mini-2024-07-18 llama-2-70b-chat llama-3-70b-instruct llama-3-8b-instruct llama-3.1-70b-instructllama-3.1-8b-instructmistral-7b-instruct mistral-large-2402 mistral-large-2407 mistral-medium mistral-next mixtral-8x22b-instruct-v0.1 mixtral-8x7b-instruct-v0.1 nemotron-4-340b-instruct phi-3-medium-4k-instruct phi-3-mini-128k-instruct phi-3-small-8k-instruct qwen1.5-72b-chatqwen2-72b-instruct snowflake-arctic-instruct starling-lm-7b-alpha starling-lm-7b-beta tulu-2-dpo-70b vicuna-33b yi-34b-chat yi-large-preview yi-largeJudge Normalized Score      Table4: Judge characteristics.The table presents three measures for each judge realization: an overall ranking quality τ ( §5, Kendall's Tau correlation with the Chatbot Arena gold ranking), a decisiveness score α ( §6.1, App.F), and its propensity for system-specific biases δ ( §6.2).Correlations τ shown are for the BT aggregation method; α and δ are calculated on the judge scores before aggregation.↓: Lower is better.
Label-efficient model selection for text generation. Shir Ashury Tahan, Ariel Gera, Benjamin Sznajder, Leshem Choshen, 10.18653/v1/2024.acl-long.456Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024Liat Ein-Dor, and Eyal Shnarch</p>
<p>Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, arXiv:2406.18403LLMs instead of human judges? a large scale empirical study across 20 NLP evaluation tasks. 2024</p>
<p>Rank analysis of incomplete block designs: I. the method of paired comparisons. Ralph Allan, Bradley , Milton E Terry, Biometrika. 393/41952</p>
<p>Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, arXiv:2403.17297InternLM2 technical report. 2024</p>
<p>ODIN: Disentangled reward mitigates hacking in RLHF. Lichang Chen, Chen Zhu, Jiuhai Chen, Davit Soselia, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, Bryan Catanzaro, Forty-first International Conference on Machine Learning. 2024</p>
<p>Chatbot Arena: An open platform for evaluating LLMs by human preference. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael Jordan, Joseph E Gonzalez, Forty-first International Conference on Machine Learning. 2024</p>
<p>Social choice should guide ai alignment in dealing with diverse human feedback. Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H Holliday, Bob M Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, arXiv:2404.102712024</p>
<p>Reexamining system-level correlations of automatic summarization evaluation metrics. Daniel Deutsch, Rotem Dror, Dan Roth, 10.18653/v1/2022.naacl-main.442Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Florian E Dorner, Vivian Y Nastl, Moritz Hardt, arXiv:2410.13341Limits to scalable evaluation at the frontier: LLM as judge won't beat twice the data. 2024</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The Llama 3 herd of models. 2024</p>
<p>Yann Dubois, Balázs Galambosi, Percy Liang, Tatsunori B Hashimoto, arXiv:2404.04475Length-controlled Al-pacaEval: A simple way to debias automatic evaluators. 2024</p>
<p>Benjamin Feuer, Micah Goldblum, Teresa Datta, Sanjana Nambiar, Raz Besaleli, Samuel Dooley, Max Cembalest, John P Dickerson, arXiv:2409.15268Style outweighs substance: Failure modes of LLM judges in alignment benchmarking. 2024</p>
<p>Srishti Gureja, Lester James Validad, Shayekh Bin Miranda, Rishabh Islam, Drishti Maheshwary, Gusti Sharma, Nathan Winata, Sebastian Lambert, Ruder, arXiv:2410.15522Sara Hooker, and Marzieh Fadaee. 2024. M-RewardBench: Evaluating reward models in multilingual settings. </p>
<p>Alexandre Albert Q Jiang, Antoine Sablayrolles, Arthur Roux, Blanche Mensch, Chris Savary, Devendra Bamford, Diego Singh Chaplot, Emma Bou De Las Casas, Florian Hanna, Bressand, arXiv:2401.04088Mixtral of experts. 2024</p>
<p>TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. Mandar Joshi, Eunsol Choi, Daniel Weld, Luke Zettlemoyer, 10.18653/v1/P17-1147Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>The PRISM alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. Rose Hannah, Alexander Kirk, Paul Whitefield, Andrew Röttger, Katerina Bean, Juan Margatina, Rafael Ciro, Max Mosquera, Adina Bartolo, He Williams, Bertie He, Scott A Vidgen, Hale, arXiv:2404.16019Proceedings of the 20th International Conference on Artificial Intelligence and Statistics. the 20th International Conference on Artificial Intelligence and StatisticsPMLR2024. 201754of Proceedings of Machine Learning Research</p>
<p>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Bill Miranda, Khyathi Yuchen Lin, Nouha Chandu, Sachin Dziri, Tom Kumar, Yejin Zick, Choi, arXiv:2403.13787RewardBench: Evaluating reward models for language modeling. 2024</p>
<p>Dongryeol Lee, Yerin Hwang, Yongil Kim, Joonsuk Park, Kyomin Jung, arXiv:2410.20774Are LLM-judges robust to expressions of uncertainty? investigating the effect of epistemic markers on LLM-based evaluation. 2024a</p>
<p>Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, Sushant Prakash, arXiv:2309.00267RLAIF vs. RLHF: Scaling reinforcement learning from human feedback with ai feedback. 2024b</p>
<p>Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E Gonzalez, Ion Stoica, arXiv:2406.11939From crowdsourced data to highquality benchmarks: Arena-hard and benchbuilder pipeline. 2024</p>
<p>A technique for the measurement of attitudes. Rensis Likert, 1932Archives of Psychology</p>
<p>Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, Yahui Zhou, arXiv:2410.18451Skywork-reward: Bag of tricks for reward modeling in LLMs. 2024a</p>
<p>Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, Juanzi Li, arXiv:2410.16184RM-bench: Benchmarking reward models of language models with subtlety and style. 2024b</p>
<p>Xingzhou Lou, Dong Yan, Wei Shen, Yuzi Yan, Jian Xie, Junge Zhang, arXiv:2410.00847Uncertainty-aware reward model: Teaching reward models to know what is unknown. 2024</p>
<p>Automatically assessing machine summary content without a gold standard. Annie Louis, Ani Nenkova, 10.1162/COLI_a_00123Computational Linguistics. 3922013</p>
<p>State of what art? a call for multi-prompt LLM evaluation. Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, Gabriel Stanovsky, arXiv:2401.00595Arjun Panickssery, Samuel R. Bowman, and Shi Feng. 2024. 202437Advances in Neural Information Processing Systems</p>
<p>OffsetBias: Leveraging debiased data for tuning evaluators. Junsoo Park, Seungyeon Jwa, Ren Meiying, Daeyoung Kim, Sanghyuk Choi, Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Yotam Perlitz, Ariel Gera, Ofir Arviv, Asaf Yehudai, Elron Bandel, Eyal Shnarch, Michal Shmueli-Scheuer, Leshem Choshen, arXiv:2407.13696Do these LLM benchmarks agree? Fixing benchmark evaluation with BenchBench. 2024</p>
<p>Better than average: Paired evaluation of NLP systems. Maxime Peyrard, Wei Zhao, Steffen Eger, Robert West, 10.18653/v1/2021.acl-long.179Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Ehud Reiter, Anja Belz, 10.1162/coli.2009.35.4.35405Computational Linguistics. 3542009</p>
<p>Keita Saito, Akifumi Wachi, Koki Wataoka, Youhei Akimoto, arXiv:2310.10076Verbosity bias in preference labeling by large language models. 2023</p>
<p>Classifier calibration: a survey on how to assess and improve predicted class probabilities. Telmo Silva Filho, Hao Song, Miquel Perello-Nieto, Raul Santos-Rodriguez, Meelis Kull, Peter Flach, 10.1007/s10994-023-06336-72023Machine Learning112</p>
<p>Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y Tang, Alejandro Cuadron, Chenguang Wang, arXiv:2410.12784Raluca Ada Popa, and Ion Stoica. 2024. JudgeBench: A benchmark for evaluating LLMbased judges. </p>
<p>Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, Dieuwke Hupkes, arXiv:2406.12624Judging the judges: Evaluating alignment and vulnerabilities in LLMs-as-judges. 2024</p>
<p>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher Manning, 10.18653/v1/2023.emnlp-main.330Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Comparing individual means in the analysis of variance. John W Tukey, Biometrics. 1949</p>
<p>Pius Von Däniken, arXiv:2412.03152A measure of the system dependence of automated metrics. Jan Deriu, and Mark Cieliebak. 2024</p>
<p>Favi-Score: A measure for favoritism in automated preference ratings for generative AI evaluation. Pius Von Däniken, Jan Deriu, Don Tuggener, Mark Cieliebak, 10.18653/v1/2024.acl-long.243Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Interpretable preferences via multi-objective reward modeling and mixture-ofexperts. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, Tong Zhang, Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Large language models are not fair evaluators. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, Zhifang Sui, arXiv:2305.179262023</p>
<p>Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, Mei Han, arXiv:2408.13006Systematic evaluation of LLM-as-a-judge in LLM alignment tasks: Explainable metrics and diverse prompt templates. 2024</p>
<p>Pride and prejudice: LLM amplifies self-bias in self-refinement. Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, William Wang, 10.18653/v1/2024.acl-long.826Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Regularizing hidden states enables learning generalizable reward model for LLMs. Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, Tong Zhang, Advances in Neural Information Processing Systems. 2024</p>
<p>. Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, arXiv:2410.02736et al. 2024. Justice or prejudice? quantifying biases in LLM-as-a-judge</p>
<p>Achieving human parity in content-grounded datasets generation. Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills, Eyal Shnarch, Leshem Choshen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Advancing LLM reasoning generalists with preference trees. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun ; Lianmin, Wei-Lin Zheng, Ying Chiang, Siyuan Sheng, Zhanghao Zhuang, Yonghao Wu, Zi Zhuang, Zhuohan Lin, Dacheng Li, Eric Li, Hao Xing, Joseph E Zhang, Ion Gonzalez, Stoica, arXiv:2404.02078Advances in Neural Information Processing Systems. Curran Associates, Inc2024. 202336Judging LLM-as-a-judge with MT-bench and chatbot arena</p>
<p>GRM-Llama3.2-3B-rewardmodel-ft 4 Internlm2-20b-reward 5. </p>
<p>. Skywork-Reward, -Llama-3.1-8B-v0.2 8 URM-LLaMa-3.1-8B 9</p>
<p>Llama-3-1-405b-instruct-fp8 (Numeric) 18 Llama-3-1-405b-instruct-fp8. Likert19</p>
<p>Llama-3-1-405b-instruct-fp8. 20</p>
<p>Llama-3-1-405b-instruct-fp8 (TokenProbs) 21 Llama-3-1-70b-instruct. 22</p>
<p>Llama-3-1-70b-instruct. Likert23</p>
<p>Llama-3-1-70b-instruct (Anchor). 24</p>
<p>Llama-3-1-70b-instruct (TokenProbs) 25 Llama-3-70b-instruct. 26</p>
<p>Llama-3-70b-instruct. Likert27</p>
<p>Llama-3-70b-instruct (Anchor). 28</p>
<p>Llama-3-70b-instruct (TokenProbs). 29</p>
<p>Llama-3.1-8B-Instruct (Numeric). 30</p>
<p>Llama-3.1-8B-Instruct (Likert). 31</p>
<p>Llama-3.1-8B-Instruct (Anchor). 32</p>
<p>Llama-3.1-8B-Instruct (TokenProbs). 33</p>
<p>Mistral-large-instruct-2407 (Numeric) 34 Mistral-large-instruct-2407. Likert35</p>
<p>Mistral-large-instruct-2407 (Anchor). 36</p>
<p>Mistral-large-instruct-2407 (TokenProbs) 37 Mixtral-8x22B-instruct-v0.1 (Numeric). 38</p>
<p>Mixtral-8x22B-instruct-v0.1 (Likert). 39</p>
<p>Mixtral-8x22B-instruct-v0.1 (Anchor). 40</p>
<p>Mixtral-8x22B-instruct-v0.1 (TokenProbs) 41 Mixtral-8x7B-instruct-v0. 142</p>
<p>Mixtral-8x7B-instruct-v0.1 (Likert). 43</p>
<p>Mixtral-8x7B-instruct-v0.1 (Anchor). 44</p>
<p>Mixtral-8x7B-instruct-v0.1 (TokenProbs) 45 Qwen2.5-72B-Instruct. 46</p>
<p>Qwen2.5-72B-Instruct (Anchor). 48</p>
<p>Qwen2.5-72B-Instruct (TokenProbs). 49</p>
<p>2-3B Mixtral-8x22B (Numeric) Mistral-large (Numeric) Mixtral-8x7B (Numeric) Llama-3-1-405b (Numeric) Llama-3-1-70b (Numeric) Qwen2.5-72B (Numeric) Llama-3.1-8B (Numeric) Llama-3-70b (Numeric) GPT-4o-mini (Numeric) GPT-4o (Numeric) Mixtral-8x22B (Likert) Mistral-large (Likert) Mixtral-8x7B (Likert) Llama-3-1-405b (Likert) Llama-3-1-70b (Likert) Qwen2.5-72B (Likert) Llama-3.1-8B (Likert) Llama-3-70b (Likert) GPT-4o-mini (Likert) GPT-4o (Likert) Mixtral-8x22B (TokenProbs) Mistral-large (TokenProbs) Mixtral-8x7B (TokenProbs) Llama-3-1-405b (TokenProbs) Llama-3-1-70b (TokenProbs) Qwen2.5-72B (TokenProbs) Llama-3.1-8B (TokenProbs) Llama-3-70b (TokenProbs) GPT-4o-mini (TokenProbs) GPT-4o (TokenProbs) Mixtral-8x22B (Anchor) Mistral-large (Anchor) Mixtral-8x7B (Anchor) Llama-3-1-405b (Anchor) Llama-3-1-70b (Anchor) Qwen2. ArmoRM-Llama3-8B Skywork-Llama-3.1-8B URM-LLaMa-3.1-8B Eurus-7b Internlm2-7b Internlm2-20b Llama-3-OffsetBias-8B GRM-Llama3. Anchor)Anchor) Llama-3.1-8B (Anchor) Llama-3-70b (Anchor) GPT-4o-mini</p>
<p>Numeric) Mixtral-8x22B (Likert) Mistral-large (Likert) Mixtral-8x7B (Likert) Llama-3-1-405b (Likert) Llama-3-1-70b (Likert) Qwen2.5-72B (Likert) Llama-3.1-8B (Likert) Llama-3-70b (Likert) GPT-4o-mini (Likert) GPT-4o (Likert) Mixtral-8x22B (TokenProbs) Mistral-large (TokenProbs) Mixtral-8x7B (TokenProbs) Llama-3-1-405b (TokenProbs) Llama-3-1-70b (TokenProbs) Qwen2.5-72B (TokenProbs) Llama-3.1-8B (TokenProbs) Llama-3-70b (TokenProbs) GPT-4o-mini (TokenProbs) GPT-4o (TokenProbs) Mixtral-8x22B (Anchor) Mistral-large (Anchor) Mixtral-8x7B (Anchor) Llama-3-1-405b (Anchor) Llama-3-1-70b (Anchor) Qwen2.5-72B (Anchor) Llama-3.1-8B (Anchor) Llama-3-70b (Anchor) GPT-4o-mini (Anchor) GPT-4o (Anchor) athene-70b-0725 claude-2.0 claude-2.1 claude-3-5-sonnet-20240620 claude-3-haiku-20240307 claude-3-opus-20240229 claude-3-sonnet-20240229 command-r-plus command-r dbrx-instruct-preview deepseek-coder-v2 gemini-1.5-flash-api-0514 gemini-1. glm-4-0520 gpt-3.5-turbo-0125 gpt-3.5-turbo-0314 gpt-3.5-turbo-0613 gpt-3.5-turbo-1106 gpt-4-0125-preview gpt-4-0314 gpt-4-0613 gpt-4-1106-preview gpt-4-turbo-2024-04-09 gpt-4o-2024-05-13 gpt-4o-mini-2024-07-18ArmoRM-Llama3-8B Skywork-Llama-3.1-8B URM-LLaMa-3.1-8B Eurus-7b Internlm2-7b Internlm2-20b Llama-3-OffsetBias-8B GRM-Llama3.2-3B Mixtral-8x22B (Numeric) Mistral-large (Numeric) Mixtral-8x7B5-pro-api-0409-preview gemini-1.5-pro-api-0514 gemini-pro gemma-1.1-2b-it gemma-1.1-7b-it gemma-2-27b-it gemma-2b-it gemma-7b-it glm-4-0116llama-2-70b-chat llama-3-70b-instruct llama-3-8b-instruct llama-3.1-70b-instruct llama-3.1-8b-instruct mistral-7b-instruct mistral-large-2402 mistral-large-2407 mistral-medium mistral-next mixtral-8x22b-instruct-v0.1 mixtral-8x7b-instruct-v0</p>            </div>
        </div>

    </div>
</body>
</html>
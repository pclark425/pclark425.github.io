<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7636 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7636</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7636</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-265659239</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.02233v2.pdf" target="_blank">Medxchat: A Unified Multimodal Large Language Model Framework Towards CXRS Understanding and Generation</a></p>
                <p><strong>Paper Abstract:</strong> Multimodal Large Language Models (MLLMs) have shown promise in general image tasks, but their application in medical imaging remains limited. This study explores MLLMs for Chest X-Rays (CXRs) through MedXChat, a unified framework enabling seamless interaction for tasks such as text report generation, visual question-answering (VQA), and Text-to-CXR generation. Our MLLM, which uses natural language as input, breaks task boundaries and simplifies medical professional training to the maximum extent by allowing diverse tasks within a single environment. To understand CXRs, we employ visual encoders (e.g.,ViT) and LLMs (e.g., mPLUG-Owl), which we refine using visual adapters and delta-tuning. For CXR generation, we introduce an innovative synthesis approach that leverages the instruction-following capabilities of the Stable Diffusion (SD) architecture to produce fine-grained medical images without additional parameters. Through comprehensive experiments, our model demonstrates exceptional cross-task adaptability, showcasing proficiency across all three tasks.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7636",
    "paper_id": "paper-265659239",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0040462499999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MedXChat: A Unified Multimodal Large Language Model Framework towards CXRs Understanding and Generation
10 May 2024</p>
<p>Ling Yang ling.yang2@sydney.edu.au 
The University of Sydney
Australia</p>
<p>Zhanyu Wang zhanyu.wang@sydney.edu.au 
The University of Sydney
Australia</p>
<p>Zhenghao Chen zhenghao.chen@sydney.edu.au 
The University of Sydney
Australia</p>
<p>Xinyu Liang xinyu.liang31@gmail.com 
Guangzhou University of Chinese Medicine
China</p>
<p>Luping Zhou luping.zhou@sydney.edu.au 
The University of Sydney
Australia</p>
<p>MedXChat: A Unified Multimodal Large Language Model Framework towards CXRs Understanding and Generation
10 May 2024BF4C7673DB2AE7D88E87BAA2712279FAarXiv:2312.02233v2[cs.CV]Multimodal Large Language ModelStable DiffusionInstruction Data
Multimodal Large Language Models (MLLMs) have shown success in various general image processing tasks, yet their application in medical imaging is nascent, lacking tailored models.This study investigates the potential of MLLMs in improving the understanding and generation of Chest X-Rays (CXRs).We introduce MedXChat, a unified framework facilitating seamless interactions between medical assistants and users for diverse CXR tasks, including text report generation, visual question-answering (VQA), and Text-to-CXR generation.Our MLLMs using natural language as the input breaks task boundaries, maximally simplifying medical professional training by allowing diverse tasks within a single environment.For CXR understanding, we leverage powerful offthe-shelf visual encoders (e.g., ViT) and LLMs (e.g., mPLUG-Owl) to convert medical imagery into language-like features, and subsequently fine-tune our large pre-trained models for medical applications using a visual adapter network and a delta-tuning approach.For CXR generation, we introduce an innovative synthesis approach that utilizes instructionfollowing capabilities within the Stable Diffusion (SD) architecture.This technique integrates smoothly with the existing model framework, requiring no extra parameters, thereby maintaining the SD's generative strength while also bestowing upon it the capacity to render fine-grained medical images with high fidelity.Through comprehensive experiments, our model demonstrates exceptional cross-task adaptability, displaying adeptness across all three defined tasks.Our MedXChat model and the instruction dataset utilized in this research will be made publicly available to encourage further exploration in the field.</p>
<p>Introduction</p>
<p>Multi-modal tasks bridging image and text modalities play a pivotal role in advancing medical image analysis.Tasks such as medical report generation [29,47,30,49,11,53], medical visual question-answering (VQA) [59,47,30], and medical visual grounding [30,29] have gained increasing importance in providing comprehensive insights into complex medical data.Large Language Models (LLM), such as those based on transformer [49] architectures, can play a wide role in promoting multimodal analysis due to their excellent capability in natural language processing (NLP) and reasoning.Leveraging LLMs for multi-modal tasks offers the potential to enhance the understanding and interpretation of medical images through the fusion or transformation of textual and visual information.Recent advancements in LLMs, exemplified by successes like Llama2 [48] and ChatGPT-4 [37], highlight their effectiveness in various vision-and-language tasks [28,17,58].These pre-trained LLMs are gaining traction in the medical domain for tasks like generating medical reports and answering questions [47], yet there is a relative scarcity in text-to-medical-image generation and unified frameworks for handling diverse multimodal tasks in a single environment.</p>
<p>Though such a unified large-scale multimodal data processing framework presents a significant opportunity, there is a noticeable absence for a multimodal processing framework for medical image and text.To the best of our understanding, within the realm of conventional medical imaging, specifically chest X-rays (CXRs), there exist merely two comprehensive multimodal medical frameworks that facilitate both interpretation and creation.These are the non-LLM based method UniXGen [29] model and the LLM-based method LLM-CXR [30].Specifically, UniXGen utilizes the vector quantization (VQ) [14] technique to handle both the CXRs interpretation and generation of reports.Meanwhile, the LLM-CXR again leverages VQ strategy to merge the image-text token space using the instruction-tuning approach.Though the use of VQ strategy aids in aligning textual and visual features, it introduces several limitations, including quantization errors and a fixed codebook length.Also, this may restrict generalizability, e.g., incorporating new datasets or expanding the codebook requires full retraining, adding complexities.Therefore, there is a need for efficient and broadly applicable framework for processing large-scale multimodal medical data.</p>
<p>To address aforementioned limitations and promote multimodal medical data processing, we introduce MedXChat, a comprehensive MLLM framework designed as a proficient medical assistant.This framework facilitates seamless interactions between medical data generation and understanding, enabling essential clinical capabilities such as image-to-report generation, intelligent question answering, and report-to-image synthesis.Specifically, MedXChat is constructed on the foundation of instruction-tuning LLMs, addressing the image-text disparities from three perspectives.First, rather than relying on the local image patterns from discretely encoding such as VQ strategy as in [30,29], we adopt visual features extracted and aligned by using CLIP encoder [40], which is sufficiently pre-trained to explicitly align image and text features using contrastive learning.Such strategy establishes a more intimate mapping between the image and text tokens, which is particularly beneficial for fine-grained medical data.Second, we employ close-sourced text-only ChatGPT-4 [37] API as the instructor to fine-tune our own LLM model and devise a way to construct instruction data by creating dialogues that refer to both medical reports and images.Third, we further address the image-text disparities in the text-to-image synthesis task by directly generating CXRs from input text prompt output by our fine-tuned LLM, avoiding the conversion from text tokens to image tokens as in conventional unified multimodal models [28,17,58,30,29]; this is achieved by leveraging the instruction-following capability inherent to the stable diffusion model [43].Our approach circumvents the full updates to the image generation component, enabling us to fine-tune the diffusion model portion without the need to retrain the entire language model.We conduct comprehensive experiments to validate the efficacy of our framework on CXR-to-Report, CXR-VQA and Text-to-CXR tasks respectively.We outperform most LLM-based methods [30,29,47,57,32] and non-LLM-based methods [7,8,11] on those benchmarks.Moreover, to further validate whether our methods can contribute to clinical field, we invite a professional radiologist to assess 20 generated reports and 20 CXRs, utilizing UniXGen [29] and LLM-CXR [30] for the evaluation.The outcomes again demonstrate that our methodology surpasses existing techniques in performance.Our main contributions are summarized as follows.</p>
<p>-We present MedXChat, a unified MLLM framework that supports both interpretive and generative functions for CXR images through the interchange of information between medical texts and CXRs images.It has demonstrated superiority across three key tasks: CXR-to-Report, CXR-VQA and Text-to-CXR tasks, compared with existing benchmark multimodal methods.-In particular, we have pioneered an innovative Text-to-CXR synthesis approach by utilizing the instruction-following abilities of the off-the-shelf Stable Diffusion method.This technique effectively maintains the creation of fine-grained features crucial for CXR imaging and takes advantage of the strong generative capabilities of the extensively pre-trained model.-Our MedXChat also innovates on application side.It uses natural language as the input and breaks task boundaries.This maximally simplifies medical professional training by allowing diverse tasks within a single environment, facilitating the transformation of AI to clinic utility.Also, our Text-to-CXR generation adds essential visual context to text, particularly for nuanced details, potentially benefiting radiologist education and clinical consultation.-We have gathered a significant volume of instructional data from the MIMIC-CXR dataset, tailored specifically for Text-to-CXR generation.Furthermore, we have fine-tuned the Stable Diffusion (SD) method with a comprehensive collection of medical data.All these resources will be made available as opensource to facilitate the subsequent medical imaging generation research.</p>
<p>Related Work</p>
<p>Radiology report generation is rooted in image captioning but with specific challenges due to the fine-grained nature of medical images, the generation of long text, and the data distribution biased towards normal samples and observations.Existing methods in this field focus on addressing these challenges.For example, the works [54,63,52] enhanced the alignment of image and text features to capture and describe fine-grained changes.The works [24,62,54] utilized hierarchical recurrent neural networks or advanced language model GPT-3 [4] to improve long paragraph generation.To battle with data bias, many works [52,23,24,45,53] trained additional disease classifiers to capture diseaserelated image features.More recent methods constructed knowledge graphs to guide report generation with disease prior knowledge [31,65,34], or contrasted disease images with normal samples [60].Medical Visual Question-answering (VQA) could be categorized into the dominant classification-based approaches [36,13,12,15] and the generation-based approaches [1,26].The field is advanced by integration with LLMs.ELIXR [59] employed a language-aligned image encoder, integrated with or adapted to a fixed LLM, PaLM 2, to enhance VQA accuracy.RadFM [57] an innovative radiology foundation model, by constructing MedMD, a vast medical multimodal dataset, and introducing an architecture for visually conditioned generative pre-training to evaluation benchmark encompassing VQA task.Moreover, XrayGPT [47] aligned the medical visual encoder (MedClip) [55] with the finetuned LLM (Vicuna), showcasing remarkable capabilities in visual dialogue.Chest X-ray (CXR) Image Generation from radiology reports is very challenging and less researched despite the recent efforts [10,3,16,56,5,6] in simulating medical image data using deep generative models like variational autoencoders (VAE) [27], generative adversarial networks [18], and diffusion models [20].These simulation works did not make full use of the information contained in medical reports to generate details in CXR images.A more related work is [21], which employed a domain-specific hierarchical text encoder to extract key medical concepts from reports, facilitating the generation of diverse and relevant X-ray images.It is noteworthy that none of the above systems could produce view-specific chest X-ray images or accommodate inputs from multiple perspectives, whereas our model could achieve.</p>
<p>Multimodal Models Text and image processing evolved independently until the Transformer model [49], ushering in an era of heightened correlation between the two fields.GILL [28] translated text hidden representations into visual embedding space using unique image tokens.SEED [17] introduced a discrete image tokenizer for LLMs to see and draw simultaneously.In medical images, UniXGen [29] generated bidirectional CXR and report, augmenting capabilities with special-view inputs.LLM-CXR [30] used transformer and VQ-GAN [14], instruction-tuning a text-pretrained LLM for enhanced alignment in understanding and generating CXR images.Unlike GILL and SEED, which extend LLM to text-to-image generation through a mapping network using special image tokens to bridge image-text disparities, we directly let LLM yield the text prompt allowing SD model to generate images, bypassing the potential token conversion gap.Also, compared with our MedXchat, UniXGen lacks VQA capabilities while LLM-CXR is incapable of handling multi-view input.Neither of them could generate lateral-view CXRs.</p>
<p>Methodology</p>
<p>We propose a unified MLLM framework MedXChat, which possesses the capability to handle both multimodal medical data (e.g., image and text), while producing corresponding outputs.An overview of our framework is provided in Fig. 1 bottom right.Specifically, we select off-the-shelf LLM framework mPLUG-Owl [61] as our LLM due to its demonstrated effectiveness in numerous opensource projects focused on instructions-tuning language [46,9,39].When optimizing this framework, the majority of the model weights, including those fundamental to the LLM and Stable Diffusion (SD), are kept frozen.For optimizing the main LLM, we use delta-tuning strategy, Low-Rank Adaptation (i.e., Lora) [22].The input to our model could be composed of a CXR and the users' text instruction.The input CXR is processed by a pre-trained CLIP visual encoder ViT-L/14 [40] to extract text-aligned regional visual features to cater to the fine-grained nature of medical images and reports.These visual features are then combined with the users' instruction to form a passage of text fed into LLM.Consequently, our LLM makes responses based on the instructions.In particular, if our model parses the need for CXR generation, it will output prompts that can be used for the successive CXR synthesis.These prompts are then forwarded to our CXR-SD model, which fine-tunes the original SD on CXR images.Our training based on instruction tuning comprises a preparation stage to construct instruction data (Sec.3.1) and fine-tune SD by CXR images (Sec.3.2), and the actual instruction tuning stage for multimodal model training (Sec.3.3 &amp; 3.4).</p>
<p>Instruction Data Construction</p>
<p>It is noteworthy that most current open-source LLMs [47,32] are incapable of handling image data while our unified model aims to possess image generation capabilities.Hence, to optimize our end-to-end MLLM to conduct conversations with patients in the manner of a clinician for text-to-CXR tasks.We need to obtain the instruction ground-truth so we can use such instruction for the finetuning process.In this work, we adopt ChatGPT-4 [37] and employ prompt engineering to generate a passage of text including a task description, human characteristics, dialogue properties, report generation, and example dialogues, as shown in the top of Fig. 1.Specifically, we provide five contents of prompts: 1.Text Description provides a general guideline for the response, e.g., "Please help construct three dialogues between a person and a helpful assistant.Each chest X-ray image is represented by &lt; Xray &gt; DESCRIPTION &lt; /Xray &gt;, where DESCRIPTION is a textual description or report of the X-ray image".2. Human Characteristics allows users to instruct the assistant on their individual need in generating chest X-ray images, e.g., whether the posteroanterior (PA) view or the lateral view is preferred.3. Dialogue Properties defines the properties of a good dialogue to ensure logical flow and restrict the content to only the pertinent parts of the report.4. Report Generation involves selecting a medical report from the MIMIC-CXR dataset and completing it with the view information.5. Example Dialogues provides two typical conversation examples within our framework.</p>
<p>In this way, our LMM can then introduce special start and termination symbols &lt; Xray &gt; and &lt; /Xray &gt; in the text-to-CXR generation section to signify that the intervening text corresponds to a text-represented image.For example, "the constructed dialogues must contain questions according to the following input chest x-ray image, which contains only the relevant part instead of the whole text between &lt; Xray &gt; and &lt; /Xray &gt;: &lt; Xray &gt; {content} &lt; /Xray &gt;".The position of "{content}" will be filled by the image's corresponding report augmented with different view information.We categorize the view of CXRs by training a transformer encoder upon all CXRs in the training set of MIMIC-CXR dataset to equip our model with the ability to differentiate between PA and lateral CXRs.Subsequently, we prepend the identified view information (i.e., "PA view of the chest was obtained"), at the beginning of the relevant report.For the subsequent fine-tuning of the LLM and SD models, we utilize reports augmented with the various view information rather than solely relying on the original reports.</p>
<p>The passage of text is then input into ChatGPT-4 to produce three segments of medical dialogue.Within the MIMIC-CXR dataset, which contains paired CXRs and reports for 14 diagnostic categories, we select 200 reports from each category, adding up to 2800 reports in total.Each report is used to generate three corresponding dialogue segments by ChatGPT-4.Once the instruction data is constructed, all the generated dialogues are input back into our LLM, which is then fine-tuned using delta-tuning strategy LoRA to equip the model with dialogue analysis capabilities.</p>
<p>By fine-tuning the pre-trained LLM, we can then produce accurate dialogue content.Once there is a need to generate a CXR that corresponds to a report, our LLM will be correspondingly trained to identify the special symbols &lt; Xray &gt; and &lt; /Xray &gt;, where the content in between is then extracted as the prompt fed into our CXR-SD for text-to-image generation.To allow users to generate CXR images from user-specified views, we further fine-tune the original SD model with CXRs and our view-augmented reports, enabling the precise generation of CXR images that match the specified views.What is seen in the right lower lobe and left infrahilar areas?</p>
<p>What does the presence of opacities in the appropriate clinical setting suggest?</p>
<p>Pneumonia.</p>
<p>What is the recommended course of action for the observed findings?</p>
<p>Follow-up to resolution.</p>
<p>Is there a possibility of a pulmonary nodule being present?</p>
<p>Yes, a pulmonary nodule cannot be excluded.</p>
<p>What should be done if the finding of the nodular opacity persists after therapy?</p>
<p>It should be followed up with chest radiographs.</p>
<p>What can be noted about the pulmonary vascular engorgement?</p>
<p>Mild pulmonary vascular engorgement is present.</p>
<p>CXR SD</p>
<p>To generate CXR from a given clinical test, we utilize a pre-trained SD model [43] generation network.Similar to recent popular fine-tuning strategy [64], we finetune our SD based on MIMIC-CXR dataset using the zero-convolution module as shown in Fig. 1 bottom left.</p>
<p>For understanding the data distribution p(z), SD will progressively denoise variables that follow a normal distribution using an encoder-decoder style network, which is equivalent to mastering the reverse process of a predetermined Markov chain of time-step length T .In each time-step t, we first encode the text features using a time encoder E t employing positional encoding techniques.</p>
<p>In the beginning, with a sample CXR z 0 , SD incrementally adds noise to z 0 via a forward diffusion process denoted as q:
q (z t | z 0 ) = N z t ; α t z 0 , σ 2 t I .(1)
After T timesteps, the initial CXR will be transformed into a pure noise z t , and then sent into a encoder-decoder architecture (i.e., U-Net [44]) including 4 down-sampling blocks and 4 up-sampling blocks, respectively.The remaining core blocks each consist of four ResNet layers and two Vision Transformers (ViTs), with each ViT incorporating multiple cross-attention and self-attention mechanisms [49].Then, SD takes text prompts that undergo encoding via using the CLIP text encoder E clip [40] and then merge with the noise by forecasting and implementing the reverse process of noise addition:
p θ (z t−1 | z t ) := q (z t−1 | z t , ϵ θ (z t , t)) = N z t−1 ; µ θ (z t , t) , σ 2 t|t−1 σ 2 t−1 σ 2 t I ,(2)
where ϵ θ is the mapping function of a U-Net model predicting noise.This method systematically lessens the noise, ultimately generating the corresponding CXR.</p>
<p>We have adopted the pre-trained SD based on a large dataset, which already yields impressive outcomes in text-to-image generation.Nonetheless, when it comes to medical datasets, fine-tuning remains essential to guarantee the quality of the generated results.During our initialization stage, to preserve the inherent capabilities of the original SD without compromising its performance, we introduce a module that only trains SD encoder E sd and several 1 × 1 zeroconvolution layers, where their weights were initialized as zero values adding no impact to the deep features.Then, they will gradually increase from zero to optimized parameters during the subsequent fine-tuning process.Since we are only plugging the proposed module into the pre-trained model, this training is as fast as fine-tuning diffusion models compared to training from scratch.</p>
<p>During our fine-tuning stage, the report is initially processed through a text encoder, succeeded by a zero-convolution layer.Subsequently, it is concatenated with noise z t as part of the denoising process which is achieved by passing the data through four down-sampling blocks of the trainable SD encoder, with each block aligning with the corresponding four up-sampling blocks of the SD encoder via a zero convolution layer.Ultimately, these results are integrated with the outcomes of the fixed SD encoder from the pre-trained model.By adopting this approach, we only focus on training our specialized model for rapid fine-tuning rather than the entire SD model.This strategy maintains the efficacy of the pre-trained model while also enabling the generation of fine-grained CXR.</p>
<p>Lateral view</p>
<p>There is no pleural effusion or pneumothorax.</p>
<p>There is mild Cardiomegaly.</p>
<p>There is severe Cardiomegaly.</p>
<p>There is right</p>
<p>Edema.</p>
<p>There is left Edema.</p>
<p>There is bilateral Edema.Prompt Fig. 4. Visual examples showcasing our Text-to-CXR generation.The delineated image regions correspond to the users' prompts, expertly identified by a trained Radiologist.</p>
<p>Multi Model Tasks</p>
<p>The proposed framework is a unified framework shown in Fig. 1 bottom right, integrating three distinct tasks: CXR-to-Report, CXR-VQA and Text-to-CXR, elaborated below.CXR-to-Report In this task, a pre-trained CLIP visual encoder ViT-L/14 [40] is used to process the CXR image, providing text-aligned visual features.These visual features are then paired with a corresponding user's instruction, e.g., "Generate a report of this chest x-ray", and input into the LLM.Based on this instruction, LLM produces a concise report according to the information extracted from the CXR.CXR-VQA In this task, our model inputs a set of three conditions into the LLM: the CXR image, the accompanying medical report, and a question from the user like "Is there cardiomegaly?".The LLM evaluates these inputs and responds with an answer, for instance, "No, cardiomegaly is not present."Text-to-CXR In this task, the dialogues generated from Sec. 3.1 are sent to LLM for training.An example of the prompt input to LLM is as follows.</p>
<p>"Human: Hi there, I recently had a chest X-ray and the report mentioned "right lower lobe consolidation".I'm not sure what that means.Could you generate a lateral view of a chest X-ray with that condition for me?"</p>
<p>The output of LLM for this example is like: Assistant: Absolutely.Consolidation in the right lower lobe of your lungs typically indicates an infection, such as pneumonia.Here is an image based on the description you provided: &lt; Xray &gt; Lateral view of the chest was obtained.New right lower lobe consolidation is consistent with pneumonia.&lt; /Xray &gt;.</p>
<p>The LLM processes this textual description and extracts the content between &lt; Xray &gt; and &lt; /Xray &gt; as the prompt for CXR synthesis, which is further passed to our CXR-SD for CXR generation.Our model is able to generate a CXR image with meticulous details, enabling the LLM to respond to the user's request with a visual representation.</p>
<p>Training Objectives</p>
<p>Our training objective encompasses two components: LLM at the instruction tuning stage and CXR-SD at the preparation stage.LLM's objective is to produce comprehensive target paragraphs consisting of an explanation, input, and an autoregressive response, akin to the instruction-following fine-tuning paradigm [46] used in ChatGPT pre-training [41,4].The loss function is specifically calculated based on the tokens generated in response to the prompt, aligning with the key response mechanism of the model.Moreover, the training objective of CXR-SD follows that of the SD model, focusing on denoising [43]: it employs textual prompts and temporal training steps to forecast the noise that has been introduced to the distorted images.Through this process, the model incrementally diminishes the noise, thereby progressively restoring the clarity of the image with each step.Large Language Model.With the dialogues X 1 q , X 1 a , • • • , X T q , X T a obtained utilizing ChatGPT-4 in Sec.3.1, where X q is the question, X a is the answer, and T is the total number of dialogues, we fine-tune LLM with LoRA [22] through minimizing the loss L a :
L a = L i=1 − log p X i a | X t q , &lt; i, X t a , &lt; i(3)
for a sequence of length L. As a result, the enhanced LLM is now adept at crafting intelligent and contextually relevant medical responses.Stable Diffusion.Given an initial image z 0 , the image diffusion process incrementally introduces noise, resulting in a sequence of increasingly noisy images z t , where t denotes the timestep of noise addition.Under a set of conditions, comprising the timestep t and textual prompts c t , a network ϵ θ is trained to predict the noise pattern that has been added to the noisy image z t , enabling the reverse process of noise reduction to reconstruct the original image.The overall learning objective for the entire diffusion model is to minimize the loss L SD :
L SD = E z0,t,ct,ϵ∼N (0,1) [∥ϵ − ϵ θ (z t , t, c t )) ∥ 2 2 .(4)</p>
<p>Experiments</p>
<p>We benchmark our model against key state-of-the-art (SOTA) models for CXRrelated multimodal generation using LLMs.For CXR-to-Report, our model's performance is mainly benchmarked against UniXGen [29], XrayGPT [47], and LLM-CXR [30].For CXR-VQA, we compare with XrayGPT [47], ELIXR [59], and LLM-CXR [30].For Text-to-CXR, we evaluate our model against UniX-Gen [29] and LLM-CXR [30].It is highlighted that LLM-CXR and our model are uniquely equipped to handle all of these diverse tasks using a single integrated model.Whereas, XrayGPT [47] cannot handle Text-to-CXR generation and UniXGen cannot handle CXR-VQA task.</p>
<p>Dataset</p>
<p>MIMIC-CXR [25] stands as the largest publicly available dataset encompassing chest radiographs along with free-text reports.It covers 14 categories related to lung disease diagnosis.Part of the CXR images are obtained in both PA and lateral views.In our research, we followed the official dataset division guidelines of MIMIC-CXR to maintain consistency and enable fair comparisons.Consequently, our training dataset comprises 270,790 samples of image-report pairs, accompanied by 2,130 and 3,858 samples for validation and testing, respectively.</p>
<p>Implementation</p>
<p>Downstream Classification.To evaluate the clinical utility of our Text-to-CXR generation, we apply our generated CXRs to classifying the 14 diagnostic categories in MIMIC-CXR.We adopt a classification model proposed in [19], which gains high classification accuracy from its increased model depth via a residual learning framework, train it by all CXR images in MIMIC-CXR training set, and test it on the generated CXR images produced by different models.</p>
<p>Evaluation Metrics</p>
<p>CXR-to-Report.We include both NLP standard evaluation metrics such as BLEU-4 [38], METEOR [2], ROUGE-L [33], and CIDEr [50] to evaluate the quality of the generated diagnostic reports, and the AUROC and F1 score metrics to assess the accuracy of disease classification based on generated reports.As per [30], we report AUROC and F1 in three forms: Micro, Macro, and Weighted, derived from the frequency deviation of categories, with each category being treated as equally important and weighted average, respectively.They are calculated based on six diagnostic categories: No Findings (NoF.),Pneumothorax (Pmtx.),Edema (Edem.),Pleural Effusion (PEff.),Consolidation or Pneumonia (Csdn./Pna.), and Lung lesion (LLsn.).CXR-VQA.We follow the VQA performance evaluation framework from ELIX-R [59] to enable the comparison with other LLM-based methods3 .Specifically, from the MIMIC-CXR dataset, we randomly choose eight samples from each of the six distinct diagnostic categories.We either inquiry about specific lesions or query the presence, location, and severity of the findings in each CXR image.Text-to-CXR.We employ the Frechet Inception Distance (FID), a widely recognized metric for image generation quality assessment, in conjunction with a downstream classification task outlined in Sec.4.2, to evaluate the quality of the images produced by our model.</p>
<p>Results</p>
<p>CXR-to-Report.The results of AUROC and F1 are compared in Table 1, which are critical metrics to reflect the clinic efficacy of the generated reports.</p>
<p>The results of the comparing methods are directly quoted from the paper of LLM-CXR [30].As seen, our model produces the highest overall scores in terms of both F1 and AUROC among the compared methods.It is also the best/second performer across all diagnostic categories except "No Fundings".As also observed, the non-LLM-based UniXGen [29] underperforms when compared with LLM-based methods XrayGPT [47] and LLM-CXR [30], showing the potential of LLMs in generating clinic-relevant contents.NLP metrics are given in Table 2.In addition to large multimodal generation methods, we also include conventional methods exclusively for text generation.As seen, our model's BLEU scores (BLEU-1 to BLEU-4) surpass nearly all methods, indicating a substantial overlap of n-grams in our generation and the ground truth.Moreover, compared with LLM-CXR, the sole method capable of performing all three tasks within a single integrated framework like ours, our model exhibits indisputable superiority across all NLP metrics except CIDEr.However, the notably high CIDEr of LLM-CXR might be linked to a distinct evaluation protocol focusing on the impression section rather than the complete report 4 .Ours also outperforms UniXGen [29] in all BLEU scores and CIDEr.Notably, our model excels in generating clinically consistent reports as already seen in Table 1.Compared to conventional methods, ours leads in BLEU-1 to BLEU-3, with only a slight lag behind GSK in BLEU-4.Our performance is average in ROUGE, yet superior to a majority of models in METEOR and CIDEr.</p>
<p>Moreover, a qualitative comparison is presented in Fig. 2. As evident, LLM-CXR [30] generates only front-view reports containing a brief impression session, while failing to handle lateral view CXRs.Conversely, our generated reports not only offer impressions but also furnish detailed findings on specific diagnoses, ensuring close alignment with the content of the original report.CXR-VQA.Table 3 and Table 4 present VQA accuracies for various multimodal LLMs.In Table 3, the answers are marked based on the accuracies of predicting the presence, location, and SST (Size, Severity, Type) of the lesions or findings.Our model achieves the highest overall accuracy, excelling in all three categories of prediction.In Table 4, the answers are marked based on the accuracies of predicting the diagnostic labels.Again, our model stands out as the best performer, showing superior performance for Edema, Pleural Effusion, and Pneumothorax.An example of our VQA generation is illustrated in Fig. 3, showcasing the model's proficiency in answering questions regarding the categories and severities of the findings.Text-to-CXR.Table 5 presents the quantitative results of Text-to-CXR generation.Notably, our model yields a significantly lower FID score, indicating a much higher similarity between the distributions of real CXR images and our generated images.Crucially, to access the clinical utility of the generated CXR images, we train a classifier using authentic CXR images to classify the 14 diagnostic categories in MIMIC-CXR, as described in Sec.4.2.The results show that our generated CXRs contribute to higher classification accuracies, suggesting a superior fidelity in capturing clinically informative patterns.An interesting observation is the non-LLM-based UniXGen [29] exhibits inferiority in capturing clinical information compared to LLM-based methods, reinforcing our findings from the CXR-to-Report task.</p>
<p>Visual examples in Fig. 4 showcase our model's reliable generation of abnormalities, aligning with specified clinical conditions, severities, locations, and views.Beyond superior quantitative results, a significant advantage of our model over LLM-CXR [30] and UniXGen [29] lies in our ability to generate high-quality lateral views, whereas the other two are limited in PA views only.Our model excels in capturing intricate visual patterns, as seen in bilateral edema (Fig. 4 rightmost), where fine details of clinic conditions in the lateral view are successfully reproduced.Radiologist Evaluation.To further validate the performance of the three tasks, we facilitated a detailed comparative evaluation by a radiologist, focusing on the CXR-to-Report and Text-to-CXR tasks, as exemplified in Fig. 6.Acknowledging the impracticality of exhaustive manual evaluation due to the MIMIC-CXR dataset's extensive size, we randomly selected a representative subset of 40 generative cases for comparison.This subset included results generated by us, UniXGen [29], and LLM-CXR [30], containing 20 reports and 20 frontal CXRs.The CXRs only featured the frontal view because UniXGen and LLM-CXR are capable of generating only frontal CXRs.The evaluations predominantly favored the reports and CXRs generated by our model, with only a minority preferring the outputs from UniXGen.This trend underscores our approach's superior performance and its consistent preference in comparative analyses.</p>
<p>Conclusion</p>
<p>We leverage multimodal LLMs to introduce MedXChat, a unified intelligent dialogue model supporting the generation of CXR images and reports and intelligent answering based on users' input.MedXChat especially excels in user-prompted generation of fine-grained CXR patterns.It surpasses other SOTA models in multimodal generation across multiple medical imaging tasks.Yet, MedXChat is still limited in lacking the capacity for medical visual grounding, another essential medical multimodal task.</p>
<p>Fig. 1 .
1
Fig.1.Overview of our MedXChat framework, including a preparation stage (dashed boxes) for constructing instruction data (top row) and fine-tuning the Stable Diffusion model using CXR images (referred to as CXR-SD), followed by an instruction tuning stage (solid box) where our multimodal MedXChat is actively trained.</p>
<p>Fig. 2 .
2
Fig. 2. Visual examples for the CXR-to-Report Task</p>
<p>Fig. 3 .
3
Fig. 3. Visual examples for the CXR-VQA Task</p>
<p></Xray>" MedXChat MIMIC-CXR Lable Diagnosis Data Collection Human: "</p>
<p>I recently got a chest X-ray and the report mentioned mild cardiomegaly.Could you generate a lateral view of this for me to better understand?" Assistant: "Yes, I can.Here is the image based on your description: <Xray> Lateral view of the chest was obtained.A chest x-ray of mild cardiomegaly.
VisualVisualEncoderAdaptor"Yes, I can. Here is the imagebased on your description:<Xray> Lateral view of the chest"I recently got a chest X-ray and thewas obtained. A chest x-rayCould you generate a lateral view of this report mentioned mild cardiomegaly.LLMof mild cardiomegaly.for me to better understand?""Yes, tiny pleural effusions.""Are there any abnormalities in the lungs?""Tiny bilateral pleural effusions"Generate a report of this chest x-ray."LoRAare new. No signs of pneumonia..."</p>
<p></Xray>" Human: "This makes things clearer, thank you." Content of prompt: 1. Task Description 2. Human Characteristics 3. Dialogue properties 4. Report generation 5. Example dialogues Instruction Data Construction Instruction Response CXR-to-Report</p>
<p>Table 1 .
1
[30]to-Report: AUROC and F1.† marks quoted results from[30].
AUROC↑Micro Macro Weighted NoF. Pmtx. Edem. PEff. Csdn./Pna. LLsn.UniXGen-256 † [29] 0.577 0.533 0.541 0.564 0.530 0.542 0.5330.5160.513XrayGPT † [47]0.595 0.552 0.576 0.592 0.511 0.590 0.5950.5150.511LLM-CXR † [30]0.654 0.586 0.628 0.698 0.532 0.612 0.6350.5400.501MedXChat (Ours) 0.672 0.599 0.666 0.583 0.549 0.633 0.7830.5300.519F1↑Micro Macro Weighted NoF. Pmtx. Edem. PEff. Csdn./Pna. LLsn.UniXGen-256 † [29] 0.281 0.187 0.256 0.411 0.083 0.226 0.2150.1320.055XrayGPT † [47]0.314 0.227 0.320 0.371 0.049 0.333 0.4040.1430.058LLM-CXR † [30]0.414 0.283 0.408 0.562 0.083 0.370 0.4550.1980.030MedXChat (Ours) 0.420 0.292 0.436 0.318 0.092 0.398 0.7180.1770.049</p>
<p>Table 2 .
2
CXR-to-Report: NLP Metrics.† marks quoted results from respective papers.
MethodsBLEU-1 BLEU-2 BLEU-3 BLEU-4 ROUGE METEOR CIDErShow-Tell [51]0.3080.1900.1250.0880.2560.1220.096Att2in [42]0.3140.1980.1330.0950.2640.1220.106AdaAtt [35]0.3140.1980.1320.0940.2670.1280.131Transformer [49]0.3160.1990.1400.0920.2670.1290.134M2transformer [11] 0.3320.2100.1420.1010.2640.1340.142R2Gen † [8]0.3530.2180.1450.1030.2770.1420.141PPKED † [34]0.360.2240.1490.1060.2840.1490.237R2GenCMN † [7]0.3530.2180.1480.1060.2780.1420.143GSK † [60]0.3630.2280.156 0.1150.284-0.203LLM-CXR [30]0.1960.0950.0540.0330.2450.0810.445LLaVA-Med [32]0.2320.0860.0270.0090.1680.0820.015UniXGen-256 [29]0.3650.2270.1470.1010.2940.1560.138MedXChat (Ours) 0.367 0.235 0.158 0.1110.2640.1350.175</p>
<p>Table 3 .
3
[30]VQA: Accuracy by topic.†marksquoted results from[30]."SST" stands for Size, Severity, Type.
Accuracy↑All Presence Location SSTELIXR † [59]54.8% 64.5%41.0% 25.0%XrayGPT † [47]25.2% 27.4%21.9% 20.3%RadFM † [57]32.7% 34.5%31.3% 20.8%LLM-CXR † [30] 44.8% 41.3%50.0% 62.5%LLaVA-Med [32]53.1% 53.8%51.0% 56.3%MedXChat (Ours) 61.2% 61.5% 56.3% 68.8%</p>
<p>Table 4 .
4
[30]VQA: Accuracy by Diagnosis.†marksquoted results from[30].
Accuracy↑AllNoF. Pmtx. Edem. PEff. Csdn./Pna. LLsn.XrayGPT † [47]25.2% 42.5% 18.8% 26.3% 20.0%25.0%17.2%RadFM † [57]32.7% 61.3% 23.8% 31.3% 26.3%34.4%40.6%LLM-CXR † [30] 44.8% 71.3% 22.5% 53.8% 53.8%39.1%50.0%LLaVA-Med [32]53.1% 37.5% 54.2% 58.3% 60.4%56.3%37.5%MedXChat (Ours) 61.2% 50.0% 58.3% 72.9% 79.2%50.0%37.5%</p>
<p>Table 5 .
5
Text-to-CXR: FID and Classification accuracy
MethodFID↓ Classification ↑UniXGen [29]106.1767.2%LLM-CXR [30]73.2968.6%MedXChat (Ours) 43.4671.5%</p>
<p>Table 6 .
6
Radiologist evaluation on 40 randomly selected generation cases</p>
<h1>prefer/totalReports CXRsUniXGen [29]1/207/20LLM-CXR [30]0/200/20MedXChat (Ours) 19/20 13/20</h1>
<p>This choice is necessitated by the unavailability of CXR-VQA results for other LLMbased methods
CXR-to-Report performance evaluation of LLM-CXR using complete reports yields lower results than those reported in Table2</p>
<p>A sequence-to-sequence model approach for imageclef 2018 medical domain visual question answering. R Ambati, C R Dudyala, INDICON. 42018</p>
<p>Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. S Banerjee, A Lavie, ACL workshop. 200512</p>
<p>Melanogans: High resolution skin lesion synthesis with gans. C Baur, S Albarqouni, N Navab, arXiv:1804.0433818044arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, NeurIPS. 4102020</p>
<p>Adapting pretrained vision-language foundational models to medical imaging domains. P Chambon, C Bluethgen, C P Langlotz, A Chaudhari, NeurIPS. 42022</p>
<p>Exploiting intra-slice and inter-slice redundancy for learning-based lossless volumetric image compression. Z Chen, S Gu, G Lu, D Xu, IEEE Transactions on Image Processing. 3142022</p>
<p>Cross-modal memory networks for radiology report generation. Z Chen, Y Shen, Y Song, X Wan, ACL. 3112022</p>
<p>Generating radiology reports via memory-driven transformer. Z Chen, Y Song, T H Chang, X Wan, ACL. 3112020</p>
<p>W L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. 20235</p>
<p>How to fool radiologists with generative adversarial networks? a visual turing test for lung cancer diagnosis. M J Chuquicusma, S Hussein, J Burt, U Bagci, ISBI. 42018</p>
<p>Meshed-memory transformer for image captioning. M Cornia, M Stefanini, L Baraldi, R Cucchiara, CVPR. 2112020</p>
<p>Multiple meta-model quantifying for medical visual question answering. T Do, B X Nguyen, E Tjiputra, M Tran, Q D Tran, A Nguyen, MICCAI. 42021</p>
<p>Does clip benefit visual question answering in the medical domain as much as it does in the general domain?. S Eslami, G De Melo, C Meinel, arXiv:2112.1390620214arXiv preprint</p>
<p>Taming transformers for high-resolution image synthesis. P Esser, R Rombach, B Ommer, CVPR. 252021</p>
<p>Model-agnostic meta-learning for fast adaptation of deep networks. C Finn, P Abbeel, S Levine, ICML. 42017</p>
<p>Synthetic data augmentation using gan for improved liver lesion classification. M Frid-Adar, E Klang, M Amitai, J Goldberger, H Greenspan, ISBI. 42018</p>
<p>Making llama see and draw with seed tokenizer. Y Ge, S Zhao, Z Zeng, Y Ge, C Li, X Wang, Y Shan, arXiv:2310.01218202325arXiv preprint</p>
<p>. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, 2020ACM5</p>
<p>Deep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. 122015</p>
<p>Denoising diffusion probabilistic models. J Ho, A Jain, P Abbeel, NeurIPS. 52020</p>
<p>Diversity-preserving chest radiographs generation from reports in one stage. Z Hou, R Yan, Q Wang, N Lang, X Zhou, MICCAI. 52023</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, ICLR. 5102021</p>
<p>Promptmrg: Diagnosis-driven prompts for medical report generation. H Jin, H Che, Y Lin, H Chen, arXiv:2308.1260420234arXiv preprint</p>
<p>On the automatic generation of medical imaging reports. B Jing, P Xie, E Xing, ACL. 42017</p>
<p>Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs. A E Johnson, T J Pollard, N R Greenbaum, M P Lungren, C Y Deng, Y Peng, Z Lu, R G Mark, S J Berkowitz, S Horng, arXiv:1901.07042201911arXiv preprint</p>
<p>Mmbert: Multimodal bert pretraining for improved medical vqa. Y Khare, V Bagal, M Mathew, A Devi, U D Priyakumar, C Jawahar, ISBI. 42021</p>
<p>Auto-encoding variational bayes. D P Kingma, M Welling, ICLR. 52013</p>
<p>Generating images with multimodal language models. J Y Koh, D Fried, R Salakhutdinov, NeurIPS. 252023</p>
<p>Unified chest x-ray and radiology report generation model with multi-view chest x-rays. H Lee, W Kim, J H Kim, T Kim, J Kim, L Sunwoo, E Choi, arXiv:2302.121722023) 2, 3, 5, 10, 111314arXiv preprint</p>
<p>Llm-cxr: Instruction-finetuned llm for cxr image understanding and generation. S Lee, W J Kim, J Chang, J C Ye, ICLR. 12142023) 2, 3, 5, 10, 11</p>
<p>Knowledge-driven encode, retrieve, paraphrase for medical image report generation. C Y Li, X Liang, Z Hu, E P Xing, AAAI. 42019</p>
<p>Llava-med: Training a large language-and-vision assistant for biomedicine in one day. C Li, C Wong, S Zhang, N Usuyama, H Liu, J Yang, T Naumann, H Poon, J Gao, NIPS. 3122024</p>
<p>Rouge: A package for automatic evaluation of summaries. C Y Lin, Text summarization branches out. 200412</p>
<p>Exploring and distilling posterior and prior knowledge for radiology report generation. F Liu, X Wu, S Ge, W Fan, Y Zou, CVPR. 4112021</p>
<p>Knowing when to look: Adaptive attention via a visual sentinel for image captioning. J Lu, C Xiong, D Parikh, R Socher, CVPR. 112017</p>
<p>Overcoming data limitation in medical visual question answering. B D Nguyen, T T Do, B X Nguyen, T Do, E Tjiputra, Q D Tran, MICCAI. 42019</p>
<p>OpenAI: Gpt-4 technical report. 202326</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W J Zhu, ACL. 122002</p>
<p>Instruction tuning with gpt-4. B Peng, C Li, P He, M Galley, J Gao, arXiv:2304.0327720235arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, ICML. 292021</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, OpenAI. 102018</p>
<p>Self-critical sequence training for image captioning. S J Rennie, E Marcheret, Y Mroueh, J Ross, V Goel, CVPR. 112017</p>
<p>High-resolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, CVPR. 3102022</p>
<p>U-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, MICCAI. 82015</p>
<p>Interactive and explainable regionguided radiology report generation. T Tanida, P Müller, G Kaissis, D Rueckert, CVPR. 42023</p>
<p>R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Stanford alpaca: an instruction-following llama model. 2023. 2023510</p>
<p>Xraygpt: Chest radiographs summarization using medical vision-language models. O Thawkar, A Shaker, S S Mullappilly, H Cholakkal, R M Anwer, S Khan, J Laaksonen, F S Khan, arXiv:2306.079712023) 2, 3, 4, 6, 10, 111213arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, Lacroix, arXiv:2302.1397120232arXiv preprint</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, NeurIPS. 2112017</p>
<p>Cider: Consensus-based image description evaluation. R Vedantam, C Lawrence Zitnick, D Parikh, CVPR. 122015</p>
<p>Show and tell: A neural image caption generator. O Vinyals, A Toshev, S Bengio, D Erhan, CVPR. 112015</p>
<p>Automated radiographic report generation purely on transformer: A multicriteria supervised approach. Z Wang, H Han, L Wang, X Li, L Zhou, IEEE Transactions on Medical Imaging. 42022</p>
<p>A medical semantic-assisted transformer for radiographic report generation. Z Wang, M Tang, L Wang, X Li, L Zhou, MICCAI. 242022</p>
<p>A self-boosting framework for automated radiographic report generation. Z Wang, L Zhou, L Wang, X Li, CVPR. 42021</p>
<p>Medclip: Contrastive learning from unpaired medical images and text. Z Wang, Z Wu, D Agarwal, J Sun, ACL. 42022</p>
<p>Diffusion models for medical anomaly detection. J Wolleb, F Bieder, R Sandkühler, P C Cattin, MICCAI. 42022</p>
<p>C Wu, X Zhang, Y Zhang, Y Wang, W Xie, arXiv:2308.02463Towards generalist foundation model for radiology. 2023312arXiv preprint</p>
<p>Next-gpt: Any-to-any multimodal llm. S Wu, H Fei, L Qu, W Ji, T S Chua, arXiv:2309.05519202323arXiv preprint</p>
<p>Elixr: Towards a general purpose x-ray artificial intelligence system through alignment of large language models and radiology vision encoders. S Xu, L Yang, C Kelly, M Sieniek, arXiv:2308.013172023212arXiv preprint</p>
<p>Knowledge matters: Chest radiology report generation with general and specific knowledge. S Yang, X Wu, S Ge, S K Zhou, L Xiao, Medical image analysis. 4112022</p>
<p>Q Ye, H Xu, G Xu, J Ye, M Yan, Y Zhou, J Wang, A Hu, P Shi, Y Shi, arXiv:2304.14178mplug-owl: Modularization empowers large language models with multimodality. 20235arXiv preprint</p>
<p>Automatic generation of medical imaging diagnostic report with hierarchical recurrent neural network. C Yin, B Qian, J Wei, X Li, X Zhang, Y Li, Q Zheng, ICDM. 42019</p>
<p>Aligntransformer: Hierarchical alignment of visual regions and disease tags for medical report generation. D You, F Liu, S Ge, X Xie, J Zhang, X Wu, MICCAI. 42021</p>
<p>Adding conditional control to text-to-image diffusion models. L Zhang, A Rao, M Agrawala, ICCV. 72023</p>
<p>When radiology report generation meets knowledge graph. Y Zhang, X Wang, Z Xu, Q Yu, A Yuille, D Xu, AAAI. 42020</p>            </div>
        </div>

    </div>
</body>
</html>
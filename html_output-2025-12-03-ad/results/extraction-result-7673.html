<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7673 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7673</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7673</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-143.html">extraction-schema-143</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models are used to distill quantitative laws, equations, or functional relationships from collections of scholarly papers, including details of the models, prompting or fine‑tuning approaches, input corpora, extraction methods, types of laws, representation formats, evaluation datasets, metrics, baseline comparisons, validation procedures, and reported performance or limitations.</div>
                <p><strong>Paper ID:</strong> paper-277620950</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.04877v2.pdf" target="_blank">System Log Parsing with Large Language Models: A Review</a></p>
                <p><strong>Paper Abstract:</strong> Log data provides crucial insights for tasks like monitoring, root cause analysis, and anomaly detection. Due to the vast volume of logs, automated log parsing is essential to transform semi-structured log messages into structured representations. Recent advances in large language models (LLMs) have introduced the new research field of LLM-based log parsing. Despite promising results, there is no structured overview of the approaches in this relatively new research field with the earliest advances published in late 2023. This work systematically reviews 29 LLM-based log parsing methods. We benchmark seven of them on public datasets and critically assess their comparability and the reproducibility of their reported results. Our findings summarize the advances of this new research field, with insights on how to report results, which data sets, metrics and which terminology to use, and which inconsistencies to avoid, with code and results made publicly available for transparency.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Scaling Laws for Fact Memorization of Large Language Models <em>(Rating: 1)</em></li>
                <li>Rethinking LLM Memorization through the Lens of Adversarial Compression <em>(Rating: 1)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 1)</em></li>
                <li>Making Pre-trained Language Models Better Few-shot Learners <em>(Rating: 1)</em></li>
                <li>Calibrate Before Use: Improving Few-shot Performance of Language Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7673",
    "paper_id": "paper-277620950",
    "extraction_schema_id": "extraction-schema-143",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Scaling Laws for Fact Memorization of Large Language Models",
            "rating": 1,
            "sanitized_title": "scaling_laws_for_fact_memorization_of_large_language_models"
        },
        {
            "paper_title": "Rethinking LLM Memorization through the Lens of Adversarial Compression",
            "rating": 1,
            "sanitized_title": "rethinking_llm_memorization_through_the_lens_of_adversarial_compression"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 1,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Making Pre-trained Language Models Better Few-shot Learners",
            "rating": 1,
            "sanitized_title": "making_pretrained_language_models_better_fewshot_learners"
        },
        {
            "paper_title": "Calibrate Before Use: Improving Few-shot Performance of Language Models",
            "rating": 1,
            "sanitized_title": "calibrate_before_use_improving_fewshot_performance_of_language_models"
        }
    ],
    "cost": 0.01043325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>System Log Parsing with Large Language Models: A Review
15 May 2025</p>
<p>Viktor Beck viktor.beck@ait.ac.at 
AITMarkus Wurzenberger markus.wurzenberger@ait.ac.at 
Florian Skopik florian.skopik@ait.ac.at 
Andreas Rauber 
T U Wien 
Austria </p>
<p>AIT Austrian Institute of Technology
Austria</p>
<p>MAX LANDAUER
AIT Austrian Institute of Technology
Austria</p>
<p>AIT Austrian Institute of Technology
Austria</p>
<p>AIT Austrian Institute of Technology
Austria</p>
<p>AIT Austrian Institute of Technology
ViennaAustria</p>
<p>AIT Austrian Institute of Technology
Max Landauer, ViennaAustria</p>
<p>Austrian Institute of Technology
ViennaAustria</p>
<p>Florian Skopik
AIT Austrian Institute of Technology
ViennaAustria</p>
<p>System Log Parsing with Large Language Models: A Review
15 May 2025520223C6948E0E48B8425BDED862724CarXiv:2504.04877v2[cs.LG]CCS Concepts:Computing methodologies → Information extraction;Software and its engineering → Software performance log parsing, log data, large language models, LLMs
Log data provides crucial insights for tasks like monitoring, root cause analysis, and anomaly detection.Due to the vast volume of logs, automated log parsing is essential to transform semi-structured log messages into structured representations.Recent advances in large language models (LLMs) have introduced the new research field of LLM-based log parsing.Despite promising results, there is no structured overview of the approaches in this relatively new research field with the earliest advances published in late 2023.This work systematically reviews 29 LLM-based log parsing methods.We benchmark seven of them on public datasets and critically assess their comparability and the reproducibility of their reported results.Our findings summarize the advances of this new research field, with insights on how to report results, which data sets, metrics and which terminology to use, and which inconsistencies to avoid, with code and results made publicly available for transparency.</p>
<p>Introduction</p>
<p>Log data consists of recorded information generated by logging facilities in software applications, systems, devices, or networks during operation.It provides a chronological record of events and contains valuable insights for various tasks, such as monitoring [7,66], root cause analysis [20,50], and anomaly detection [21,30].Given that modern computer systems can generate millions of log lines per hour, manual processing is infeasible, necessitating fast and scalable automated approaches.For instance, in 2013 the Alibaba Cloud system was reported to produce approximately 100 to 200 million log lines per hour [47].However, extracting meaningful information from raw log data requires transforming it into a structured format.Since logs are typically semi-structured, following general patterns but containing variable log messages, direct analysis is challenging.To enable log analytics tasks like anomaly detection, logs must first be parsed into a structured representation, making log parsing a critical step in log analysis [69].</p>
<p>Log parsing refers to the extraction of structured data from semi-structured logs.Parsing the logs into the structured format can simply be achieved by regular expression matching if the correct log templates are given.Most methods therefore formulate log parsing as a log template extraction problem [10,19,24,57,65,73].Ground-truth templates are defined somewhere in logging statements in the source code and are therefore often not available.A wide range of log parsing techniques [13,26,34,57,58] have therefore been proposed to overcome this issue.</p>
<p>Many state-of-the-art log parsing methods require manual configuration or other manual actions at some point in the parsing process.Approaches may involve labeling of some of the logs from the training set [24,34,72], definition of log formats [38] or regular expressions for preprocessing [19,57,73], or other parameters [73] that fit the parsing algorithm to the data.From a user's perspective the configuration of parsing algorithms might seem overwhelming since it requires expertise or an analysis of the data at hand.The reason log parsing methods require manual actions is that humans have both semantic and syntactic understanding [24], but more importantly, a natural generalization capability and potentially expert knowledge about logs and log parsing.Many log parsers have already achieved semantic [34,40] and syntactic abilities [10,19,73], but many do not achieve satisfactory performance on data other than LogHub [83] as reported by [16,37,77] or require substantial amounts of labeled logs for training their models [70].While syntax-based parsers focus on characteristics like occurrence frequencies, or word or log length, to determine the static parts of logs, semantic-based parsers leverage the semantic differences between static and variable parts of log messages [24,70].Knowledge about logs and how to parse them can be learned by language models, such as BERT [12] or its descendants, by fine-tuning them with logs and their templates [34,40,63].However, these methods often lack generalization and have to be trained or fine-tuned [17] with labeled logs, preferably with logs of the exact same log type as the target logs [42].Generalization arises from pretraining on a large corpus of diverse datasets, allowing models to perform well also on novel tasks and possibly enhanced through fine-tuning [17] or in-context learning (ICL) [6].This is where large language models (LLMs) come into play.</p>
<p>With the emergence of LLMs the new research field of LLM-based log parsing arised.In late 2023, Le and Zhang [33] reported notable performance of ChatGPT1 in a naive setting, where the LLM was presented individual logs and simply asked for their templates.In the meantime, other approaches adopted LLMs for this purpose as well, building sophisticated frameworks that utilize LLMs in various ways to enhance log parsing by adopting learning paradigms for LLMs such as ICL [6] or fine-tuning [17].Given the fact that the number of unique templates is significantly lower than the number of logs and the average rate of newly discovered templates decreases as more logs are processed [24], some LLM-based parsers [24,53] are even able to achieve runtime performance comparable with the fast and well-known parser Drain [19].</p>
<p>To the best of our knowledge, a structured overview and an objective comparison of papers and methods concerning LLM-based log parsing is currently lacking, as this is a rather novel research field which only recently emerged from the popular research field of generative LLMs.So far there is only a preprint survey paper by Akhtar et al. [1], which provides an overview of using LLMs for log analysis, including log parsing.Although their discussion on log parsing is concise, it lays a valuable foundation for further innovation and development in this area.Consequently, our work aims to undertake a systematic review of the literature, identifying the commonalities and variabilities of the existing approaches, defining common terminologies and reporting schemes, highlighting performance enhancing paradigms, and deriving recommendations for future advancements in this field.Our main focus is on investigating LLM-based log parsing methods with emphasis on researchers seeking to apply or develop novel log parsing techniques.Thus, additional emphasis is placed on the application of the LLM and the manual effort required for the implementation of the various approaches.</p>
<p>The aforementioned goals are approached by answering the following research questions:</p>
<p>• RQ1: What are the main advantages and disadvantages of LLM-based log parsing approaches and non-LLM-based approaches?• RQ2: To what extent do LLM-based log parsing approaches rely on labeled data and manual configuration?• RQ3: Which techniques can enhance the efficiency or effectiveness of LLM-based log parsing?</p>
<p>• RQ4: Which experiment design choices hinder the comparability, and hence which guidelines should be adopted in terms of configuration, datasets used, evaluation metrics, and reporting to make results comparable?• RQ5: To what extent are the presented LLM-based parsing methods accessible and reproducible?</p>
<p>We summarize the main contributions of this work as follows:</p>
<p>• We provide a systematic overview of the existing methods and techniques based on a set of feature definitions that characterize LLM-based log parsing approaches, or the corresponding papers, respectively.• We derive the general process of LLM-based log parsing, encompassing each parsing approach of each reviewed paper in a single flow chart.</p>
<p>• We provide a comprehensive benchmark of seven open-source LLM-based log parsing</p>
<p>approaches on open-source datasets.• Based on the literature review and our evaluation results, we answer 5 research questions, deriving recommendations for future research and the design of (LLM-based) log parsers.• Finally, we make the source code and all the results of our evaluation publicly available in our GitHub repository [4] to ensure transparency and reproducibility.</p>
<p>The remainder is structured as follows: Section 2 describes the background and the core concepts.Section 3 describes the related work.Section 4 outlines the survey method, including the literature search strategy and the definition of the reviewed features.Section 5 presents the results of the literature review in a large table and describes the findings of each feature.Section 6 explains the evaluation setup, while Sec.7 presents the evaluation results.We discuss the findings from literature and evaluation in Sec. 8, and conclude our paper in Sec. 9.</p>
<p>Background</p>
<p>In this section, we describe the relevant background by means of the core concepts relevant to LLM-based log parsing.</p>
<p>Log Parsing</p>
<p>Log parsing is the foremost step in log analytics and denotes the extraction of structured data from logs.Logs are given as single-or multi-line events, available in textual form [30].The diverse and un-or semi-structured nature of logs makes log parsing a difficult task.It is the precondition for many log analytics tasks, hence the effectiveness of downstream tasks is inherently dependent on the effectiveness of the parser.Parsing also determines the runtime efficiency of the whole process, which is especially important for online or incremental approaches, such as live-monitoring for cybersecurity [24].Incremental approaches are designed to handle continuous data streams, updating the parsing model dynamically as new log data arrives, allowing for real-time analysis and quick responses to emerging issues.</p>
<p>In this work, we specifically focus on template extraction, since the extraction of the variables from the log messages is trivial once the (correct) templates are identified, for example, with regular expression matching.The template extraction process is therefore the key component of log parsing.</p>
<p>Log Templates</p>
<p>A log template is a predefined format used to log system events in software applications.It typically includes placeholders for essential details like timestamps, log levels (e.g., INFO, ERROR), message content, contextual metadata and more [38].Previous research differentiates between the unstructured log message part of a log and the structured log headers [18].The structured part is thereby straightforward to extract with regular expressions.It has been shown that parsing only the unstructured log message part improves the parsing accuracy for conventional (non-LLM-based) parsers [18].Many methods [13,13,19,72], therefore require the log format (≠ log template) of a dataset as an input parameter.An example of a log template from the Apache dataset [83] is given in Fig. 1.The log format parameter for this example would be " [<Time>] [<Level>] <Content>", whereas "Time" and "Level" refers to log headers and "Content" refers to the log message.In the remainder, we understand the "log format" as this parameter defining the positional indicators of the log headers and of the content within different log types, or different datasets, respectively.Fig. 1.A simple example of log parsing, from logging statement to log file to parsed log.</p>
<p>Large Language Models (LLMs)</p>
<p>Large Language Models (LLMs) have evolved significantly, transitioning from early statistical and rule-based approaches to deep learning architectures powered by Generative Pretrained Transformers (GPTs).These models leverage self-attention mechanisms and large-scale datasets to generate human-like text, enabling advancements in natural language processing (NLP) tasks such as summarization, translation, content generation, and complex problem-solving.A key enhancement in LLMs is instruction tuning which specializes the pretrained LLMs to follow instructions provided in prompts.Instruction tuning enables effective in-context learning (ICL).Instead of modifying model parameters, ICL allows LLMs to learn from prompts that include instructions, examples, and queries, making them more adaptable to specialized applications [75].The term LLM is only loosely defined, hence, for this work, we define LLMs as generative pretrained transformers (GPTs) such as ChatGPT, Mistral or Qwen.Models like BERT [12] (encoderonly) are thereby excluded, as we are only interested in generative models that excel in both understanding and generating text, with increased generalization capabilities compared to BERT, making them suitable for a broader range of tasks.</p>
<p>Log Parsing with LLMs</p>
<p>Large-scale model series like ChatGPT and many more have demonstrated potential in this domain.Some approaches fine-tune smaller LLMs [46] or BERT [34] to this specific task while reporting superior parsing performance to state-of-the-art approaches.However, fine-tuning LLMs for log parsing is resource-intensive in terms of computational costs, runtime, and labeling examples, making ICL a compelling alternative [70].Research shows that ICL enhances LLMs' performance in logical reasoning and fact retrieval, suggesting its viability for structured log analysis [6].However, LLM inference is also costly.The computational demands of LLMs contribute to inference overhead and network latency, but also energy consumption and thus, an increased carbon footprint.Using LLMs for automation therefore necessitates optimizations for practical and energy efficient deployment [24,70].Regardless of the efficiency, LLMs are not inherently designed for log parsing, leading to inconsistencies in token classification and errors in log templates.Inaccuracies in the parsed templates can negatively impact downstream tasks like anomaly detection [84].Despite the aforementioned challenges there is a variety of approaches that address these concerns and provide solutions, making LLM-based log parsing a strong alternative to conventional log parsing.</p>
<p>Related Work</p>
<p>Existing surveys and benchmarks have significantly contributed to the understanding and evaluation of log parsing.These works are particularly relevant to our work, as they provide a comprehensive overview of the current state of the field, highlighting key methodologies, challenges, and advancements: The work of Zhu et al. [84] provides a comprehensive benchmark of log parsing techniques, as well as LogHub [83], which became the common dataset collection used in many log analysis evaluations.The labels of the LogHub datasets were corrected by Khan et al. [28] to a unified template style.Jiang et al. [25] conducted a large-scale evaluation on state-of-the-art log parsing techniques while also providing the large-scale dataset collection, LogHub-2.0.It contains several million annotated logs and has been widely adopted for the evaluation of log analysis techniques [22,24,27,70,81].The large number of logs in LogHub-2.0facilitated comprehensive efficiency evaluations, which are becoming increasingly important due to the large number of logs generated by modern computer systems [47,76].Similarly to our work, Zhang et al. [76] classify existing parsing approaches by a set of features, such as the parsing mode or the required amount of preprocessing effort.The researchers contemplate the challenges of state-of-the-art log parsing techniques, more specifically, the scarcity of public datasets, limited generalizability caused by intrinsic limitations, a lack of automation, and insufficient efficiency regarding the implementation.</p>
<p>The integration of LLMs into log analysis methods has been explored in the recent survey paper by Akhtar et al. [1].It provides a comprehensive survey of LLM-based techniques for log analysis, highlighting their potential to enhance efficiency and accuracy in processing substantial volumes of log data.The authors discuss various applications, including anomaly detection, fault monitoring, root cause analysis, threat detection, but also provides an overview log parsing.The paper explores the use of fine-tuning, ICL, and retrieval-augmented generation (RAG) [35] for performance improvements in log parsing.It also reviews existing research, identifying common challenges such as the high resource consumption of LLMs, the need for frequent updates, and security concerns with closed-source models.</p>
<p>Survey Method</p>
<p>This section describes the search strategy and the taxonomy of LLM-based log parsing.</p>
<p>Search Strategy</p>
<p>This section describes our search strategy and the inclusion and exclusion criteria derived from the core concepts of Sec. 2 and early insights from LLM-based log parsing approaches.For the keyword search, we determined the following keywords:</p>
<p>(1) log(s) (in title);</p>
<p>(2) LLM(s), large-language-model(s), or large language model(s);</p>
<p>(3) parsing, parser(s), or parse.</p>
<p>The term "log" or "logs" must be included in the publication title, while the remaining keywords are entered into the default search engine of the respective database.This approach should ensure that the search results are focused on publications where log data is the central aspect.The search statistics are given in Table 1.</p>
<p>Table 1.Keyword search results (status 29-01-2025)."#R" and "#I" stand for the number of results and the number of papers that were still included after all exclusion criteria were applied.</p>
<p>Database</p>
<p>Search String #R #I IEEE Xplore</p>
<p>("Document Title": "log" OR "Document Title": "logs") AND (LLM OR LLMs OR "large language model" OR "large language models") AND (parser OR parsers OR parsing OR parse)
23 11
Science-Direct title: ("log?")AND title, abstract, keywords: ("LLM" OR "LLMs" OR "large language model" OR "large language models" OR "large-language-model" OR "large-language-models") AND ("parser" OR "parsers" OR "parsing" OR "parse") Google Scholar intitle:"log" OR intitle:"logs" "LLM" OR "LLMs" OR "large-language-model" OR "large-language-models" OR "large language model" OR "large language models" "parsing" OR "parser" OR "parsers" OR "parse" 166 29</p>
<p>Web of Science TI=("log" OR "logs") AND TS=("LLM" OR "LLMs" OR "large language model" OR "large language models" OR "large-language-model" OR "large-language-models") AND TS=("parser" OR "parsers" OR "parsing" OR "parse")
2 2
Springer Link Title:(log OR logs) AND (LLM OR LLMs OR "large language model" OR "large language models" OR "large-language-model" OR "large-language-models") AND (parser OR parsers OR parsing OR parse)
6 0
The numbers of search results per database sum up to 176, excluding duplicates found on multiple databases.The results were then filtered by exclusion criteria.A publication is excluded if,</p>
<p>• It is not written in English.</p>
<p>• It is not accessible in electronic format.</p>
<p>• It is a book, technical report, lecture note, presentation, patent, thesis, or dissertation.</p>
<p>• A more recent publication is available that presents the same study (by the same authors), ensuring that this SoK focuses on the latest versions of specific approaches.• It only applies a non-LLM-based approach without introducing novelties to LLM-based log parsing.• It does not apply log parsing or LLMs by definitions of Sec. 2.</p>
<p>By applying these exclusion criteria on both abstract and full text, we omitted 147 of the initial publications found: 1 publication was omitted because it was not written in English.5 were outdated versions of newer papers.21 were omitted because they were books, theses, dissertations, or technical reports.8 publications were omitted due to being completely unrelated to the topic.112 are related but excluded, as they do not directly cover LLM-based log parsing by the definitions of Sec. 2 or focus other log analytics tasks, but employ parsing-free methods or existing conventional parsers, such as Drain [19] or SPELL [13].</p>
<p>The final selection consists of 29 papers.One publication is from the year 2023, one from 2025, while the remaining are all from 2024.Since there is a recent hype around the usage of LLMs for log related issues, we include preprint papers to capture the newest research.There are 12 preprint papers in the final selection (∼ 41%).</p>
<p>Feature Definition</p>
<p>This section describes each feature and its classes of Table 2, which we report from the analyzed approaches.The features describe either general properties (GP), processing steps (PS), or concern the reproducibility (R) of the approaches.Features concerning the general properties of the approaches are typical classifications of machine learning approaches.The features we categorize as processing steps are taken from the work of Jiang et al. [24] and Pei at al. [53].For reproducibility, we take inspiration from the work of Olszewski et al. [51] where they analyzed the reproducibility of machine learning papers of Tier 1 security conferences and defined a set of questions to determine the reproducibility of research findings.During the review process, we found that these features occur with significant frequencies or with significant variations, validating our selection.As illustrated in Table 2, this is evident.</p>
<p>In Table 2 features are reported with "✓" (true), an empty cell (false) or a descriptive name or abbreviation.If a paper provides an unclear answer to the feature in question, we write "?".For instance, one papers provides a link to its code repository but the repository is empty, thus we write "?" for the corresponding feature.If a paper does not include the feature, we leave it blank.</p>
<p>GP-1 -Supervision.In log parsing, a labeled log is one for which a template, acting as the ground truth for that log, is available.In machine learning approaches are therefore often classified based on the requirement for labels into supervised [40,68] or unsupervised [19,70] approaches, thus we report:</p>
<p>• Supervised parsing (sup) requires at least some log instances of the training set to be labeled.</p>
<p>• Unsupervised parsing (un) does not require labels.</p>
<p>GP-2 -Parsing Mode.Many methods are described as online approaches [19,39], but their interpretations vary.Some consider online processing to be incremental processing or streaming, while others apply batch-wise processing, yet still label their method as online.Additionally, some works do not specify whether their approach is online or offline.Our initial incentive was to classify these methods accordingly, but this is often not feasible due to a lack of context or sufficient explanations.</p>
<p>As a consequence, we devised the following categories that describe how many logs are processed at once:</p>
<p>• Stream (S): The log lines are processed one-by-one.</p>
<p>• Batch (B): Multiple log lines are processed at once, whereby it is possible to apply local (within each batch) optimizations.A batch is significantly smaller than the entire dataset.• Total (T): The entire dataset is processed at once, whereby it is possible to apply global optimizations to the process.</p>
<p>GP-3 -Learning / Prompting.We identified four different types of learning or prompt engineering techniques.The type of learning can strongly influence how the prompt is designed which is why we report this in a single feature:</p>
<p>• In-context learning (ZS/FS/S): ICL [6] leverages the context provided within a single LLM call to generate responses, adapting to specific needs without any updates to the model's parameters.Thereby, we differentiate between a zero-shot setting (ZS), where the model performs a task based solely on an instruction in the prompt, and few-shot ICL (FS), where a small number of demonstrations is provided in the prompt to guide the model's behavior.Demonstrations can be retrieved randomly from the dataset or with sophisticated strategies.They can also be static (S) which we report separately.• Chain of Thought (CoT): CoT [67] refers to a series of subsequent LLM calls, where the (complex) task is broken down into (easier) subtasks or "thoughts" by the LLM itself and answered in a step-by-step manner, rather than jumping to the solution directly.The process also enhances transparency since intermediate steps can be monitored by users.• Fine-tuning (FT): In addition to ICL and CoT, which are solely modifications of the prompt, fine-tuning [17] modifies the parameters of the LLM, but mostly affects only the last few layers of the neural network, which are the most decisive ones for the outcome.• Pretraining (PT): Pretraining refers to the initial phase of training the model on a large corpus of text data to learn the structure and patterns of language.</p>
<p>PS-1 -Manual Configuration.This is true if at least one manual configuration step is required (e.g., when a parser is applied to an unseen log type).This includes manual definition of input parameters such as regular expressions, log formats or other essential parameters for different datasets, which is often required for many conventional log parsers such as Drain [19], Brain [73],</p>
<p>or Spell [13], and others featured in the LogPAI log parser repository2 [84].This feature does not include optional parameters that are generic enough to be left unchanged for a new log type.</p>
<p>PS-2 -Retrieval-Augmented Generation.Retrieval-augmented generation, or RAG, is a paradigm where the LLM is provided with information retrieval capabilities.In case of log parsing, most approaches utilize a sampling strategy to include either logs or logs and their templates in the prompt.The LLM should then use the provided context to learn the variability and commonality of logs [38] or learn parsing directly from log-template pairs.Many approaches create clusters, trees, buckets or other kinds of aggregations from which they sample their demonstrations for ICL and retrieve them based on some kind of similarity measure.We differentiate two cases:
• Random retrieval (R):
The process is a random retrieval of demonstrations from training data.</p>
<p>• Strategic retrieval (S):</p>
<p>There is a refined strategy for retrieving demonstrations from specific data storages (e.g., clusters, lists, etc.).</p>
<p>PS-3 -Caching.Some approaches increase their efficiency by storing parsed logs' templates in some kind of data structure and only call the LLM if a subsequent log does not fit any of the existing templates.These data structures can be tree-like structures, similarity-based clusters, or simply lists.</p>
<p>PS-4 -LLM Usage.The employed LLM is used in at least one of three different ways:</p>
<p>• Direct parsing (dir): The LLM receives one or more logs directly in the prompt and is asked for the corresponding template.• post-processor (post): The LLM is used to post-process the log lines, which is mostly merging similar templates or correcting templates based on new information obtained by subsequent log lines in an online approach.• Preprocessor (pre): The LLM functions as a helper in preprocessing, for example, for identifying the timestamp, the log format, or other relevant features.</p>
<p>PS-5 -Template Revision.This feature describes whether the templates are revised in a postprocessing step, such as merging similar templates or correcting templates, based on the information obtained from new logs (in an incremental approach).This step can be done with or without the help of LLMs.</p>
<p>R-1 -Evaluation Data.Since the performance of parsers is strongly dependent on the data, we report the different types of datasets or dataset collections used in the papers:</p>
<p>• LogHub (L): The well-known LogHub [83,84] repository is widely used for evaluating log analysis methods.It features 16 annotated datasets from different domains with 2000 logs each.</p>
<p>• Corrected LogHub (CL): This is a version of LogHub with corrected templates.The original templates have been observed to have inconsistencies due to inconsistent labeling styles [28].• LogHub-2.0(L2): LogHub-2.0[25] is based on Loghub and features 14 large-scale datasets from various domains with numbers of logs in the order of 10 4 to 10 7 .• Custom (CA/CU): If the parser was evaluated with custom data we report whether it is publicly available (CA) or unavailable (CU).</p>
<p>R-2 -Evaluation Metrics.We report the various evaluation metrics employed to assess the effectiveness of log parsing techniques.Each metric provides a unique perspective on the parsing process, focusing on different aspects of effectiveness:</p>
<p>• Group Accuracy (GA): A log message is considered correctly parsed if and only if its event template corresponds to the same group of log messages as the ground truth does.GA is then the number of correctly parsed log messages divided by the total number of log messages [84].GA is also known as RandIndex [55].• F1-score of Group Accuracy (FGA):   is the number of templates that are generated by a log parser, and   the number of templates that are correctly parsed by the log parser.  is the actual correct number of templates in the ground truth.The precision of the GA (PGA) is then   /  and the recall of GA (RGA) is   /  .Then, FGA is the harmonic mean of PGA and RGA [25].• Parsing Accuracy (PA): A log is considered correctly parsed if and only if all its static text and dynamic variables are correctly identified.PA is then the number of correctly parsed log messages divided by the total number of log messages [10].PA is the same as message-level accuracy (MLA) [40,72].• Edit Distance (ED): ED is the minimum number of operations, such as insertions, deletions, or substitutions, needed to convert one string into the other [49].In log parsing, it is used to calculate the minimum number of operations required to convert the parsed template into the ground-truth template [70].• Precision Template Accuracy (PTA): A template is correctly parsed from log messages if and only if it is identical (token-by-token) to the ground-truth template(s) of the log messages.PTA is the ratio of correctly identified templates to the total number of identified templates [28].• Recall Template Accuracy (RTA): Complementary to PTA, RTA is the ratio of correctly identified templates to the total number of ground-truth templates [28].• F1-score of Template Accuracy (FTA): Is the harmonic mean of PTA and RTA [25].</p>
<p>• Other: If metrics other than the above are used we write "other".</p>
<p>GA and PA are sensitive to the total number of log messages, which may be problematic since most log datasets contain a large number of logs but a much smaller number of unique templates [25].Therefore, Jiang et al. [25] proposed to use F-metrics since they are insensitive to class imbalance.</p>
<p>R-3 -Used Models.</p>
<p>A variety of different LLMs are used in the analyzed works.We report only the base models, or the name of the model series, of the used LLMs and only the ones used for the task of log parsing.R-4 -Code Availability.For a potential user, it might be essential that a parser is already implemented, thus we report the availability of code repositories.This feature is true if a link was provided to the implementation and if the link does not lead to an empty repository or a non-existent page, and false otherwise.R-5 -Preprint.This is true if the reviewed paper is a preprint paper.It is possible that the corresponding code repository to the paper, the results, findings, or interpretations are only preliminary, which is why we report this feature.</p>
<p>Literature Review Results</p>
<p>The results of our literature review are aggregated in Table 2.The following subsections discuss the results and findings of the literature review in the context of this table.</p>
<p>Process Pipeline</p>
<p>After reviewing the paper selection, we were able to develop a process pipeline that summarizes all the approaches in a single flow chart, given in Fig. 2. The dashed arrows and boxes represent optional actions or components while the continuous ones represent essential actions components.Optional components can be skipped.The user and the LLM represent key actors that can act on other components.The arrows pointing away from the user indicate steps that can require manual effort.The arrow from user to the logs represent the manual labeling effort for supervised parsers while the arrow pointing to the preprocessing component represents manual configuration (which can also affect other components).We provide detailed explanations about the components in the following sections.</p>
<p>Supervision (GP-1)</p>
<p>From the reviewed papers, 6 are fully supervised and 9 approaches describe a supervised and unsupervised setting.Supervised ones require labeled logs for ICL or fine-tuning, but the amount of templates varies from a few guiding seed examples [81] to significant proportions of a dataset's templates [24,72].The works that fine-tune LLMs [27,42,46,52,81] all require logs and their templates for the process which is why we classify these approaches as supervised.</p>
<p>Similar to earlier work [34], LILAC [24] and DivLog [72] utilize specialized algorithms to sample from the training data.The objective of the algorithms is to maximize the sample diversity of the labeled logs required for ICL.They call this process candidate sampling.LILAC has a sampling algorithm based on hierarchical clustering while DivLog uses the Determinantal Point Process (DPP) [29].It is noteworthy that the unsupervised parser LogBatcher [70] also uses DPP, but to maximize the sample diversity within the batches of logs that are prompted to the LLM for direct parsing.</p>
<p>Huang et al. (LUNAR) [22] empirically study the average FTA of LILAC [24] and the BERT-based parser LogPPT [34] based on the proportion of labeled logs they receive as input.It was found that FTA exhibited a substantial decline in performance when the proportion of labels was reduced to 5%, which, in general, remains a relatively high figure, particularly in light of the typically abundant nature of logs.The finding that the more templates are available, the better the parsers' performance  is little surprising [22,24].However, labeled logs are scarce and require substantial manual effort and expertise [70].All supervised parsers of the selection can technically be operated unsupervised at the cost of performance.This may require modifying the prompt so that it corresponds to a zero-shot setting to not confuse the LLM with announced but absent examples.
Approach GP-1 GP-2 GP-3 PS-1 PS-2 PS-3 PS-4 PS-5 R-1 R-2 R-3 R-4 R-5</p>
<p>Processing Mode (GP-2)</p>
<p>The 18 parsers classified under the "stream" category process logs in real-time as they arrive, making them suitable for online processing.On the other hand, "batch" parsers [39,43,53,65,70,71] process logs in fixed-sized chunks, meaning they can function in both online and offline settings, depending on the frequency of batch execution.When processing in batches, it is possible to optimize the process for this batch [70].Lastly, parsers under the category "total" [22,74,78] analyze an entire log dataset at once, which is inherently an offline approach, as it requires the complete dataset to be available before processing begins.For instance, LUNAR [22] and Lemur [78] group the logs based on certain features (e.g., length or most frequent shared token) into buckets.They then create clusters within that buckets that maximize the sample diversity.The concept of maximizing the sample diversity within certain similarity clusters is also used by LogBatcher [70] but for data batches.Similarly, DivLog [72] and LILAC [24] use this idea for their sampling algorithm.</p>
<p>5.4</p>
<p>Learning / Prompting (GP-3) 5.4.1 In-Context Learning.ICL [6] is a prompt-based learning paradigm.It includes learning from instructions but also from examples [6].This makes it a cheap way of learning.All approaches therefore apply ICL in some way.Among others, 16 works use zero-shot ICL, 14 employ few-shot ICL, and [2,3,46,65,71] use static examples.We report static examples separately from the few-shot setting in order to illustrate which approaches use dynamic examples.This in turn implies RAG (PS-2).[78] uses CoT [67] to revise similar templates, that may belong to the same event, in three dialogue rounds.These steps include revising the structure, the semantics and finally, a decision if the templates should be merged.The other works applying CoT [61,62,71] use it for direct parsing but do not explain in detail how.5.4.3Fine-Tuning and Pre-Training.Compared to ICL, fine-tuning is considered a rather computationally costly task [70] since it modifies the parameters of the models itself.Previous research [48] found that fine-tuning generally outperforms ICL in effectiveness.From our selection, the works of Ma et al. (LLMParser) [42] and Mehrabi et al. [46] support this finding.However, fine-tuning also requires labeled logs.As the default setting for the training, [46] used 80% of the available templates and 15 logs per template, ONLA-LLM [52] used 80% of the labeled logs of their custom dataset, LLMParser [42] used 50 labeled logs per dataset, and [81] used 32 per dataset.Furthermore, Mehrabi et al. [46] and Ma et al. [42] find that their fine-tuned LLMs struggle with new and unseen log types in terms of effectiveness and robustness.This raises the question of whether this is due to overfitting, but this is a task for future research.It remains up to the users to decide whether a costly training phase, requiring considerable quantities of labeled logs, but with improved performance on seen logs, is feasible for their use case.</p>
<p>Chain of Thought. Lemur</p>
<p>From our selection only Ji et al. (SuperLog) [23] pretrained a model.Pretraining requires vast amounts of data.To this end, Ji et al. created NLPLog, a comprehensive dataset with a quarter million question-answer pairs on log analysis tasks.The researchers report superior performance not only in parsing but also anomaly detection, failure diagnosis, and log interpretation.They also conducted an evaluation on unseen logs, but they do not report any of the common parsing metrics from Sec. 4.2.This hinders the determination of the practical benefits in settings involving unseen logs.Future research could be conducted on this aspect.</p>
<p>Manual Configuration (PS-1)</p>
<p>The datasets from LogHub and its descendants [28,83,84] have been used for the evaluation of log parsing for many years now and it is known from existing literature which regular expressions, log formats, or other parameters have to be used to achieve a certain baseline of performance.Some of the reviewed methods require multiple parameters that are tuned specifically for each dataset and claim to achieve superior performance on the evaluation metrics.These matters may lead to unrealistic expectations for real-life applications, where sometimes no previous knowledge of the versatile format of logs is given [11].For instance, Dai et al. have shown that the parsing results of (non-LLM-based) parsers like Drain [19], IPLoM [44], LenMA [58], AEL [26] and Logram [10] are strongly influenced by their parameter settings.Of the reviewed studies, 12 require certain configuration parameters.All of them require the log format of the logs to extract the log content from the raw logs and [43,78] additionally require regular expressions for preprocessing.</p>
<p>RAG (PS-2)</p>
<p>Ten approaches describe a sophisticated retrieval process, one describes random retrieval and for six approaches it is not clear how the approaches apply RAG.RAG is used for demonstration retrieval but also for retrieval of multiple target logs where a set of similar logs is chosen.The latter is only relevant for approaches parsing in batches or the whole dataset at once (GP-2).LogBatcher [70], OpenLogParser [43], and LUNAR [22] prompt batches of logs that are in the same similarity cluster but maximize the dissimilarity within that cluster to highlight the variabilities and commonalities in logs for the LLM.Demonstrations can either be only logs [27] or logs and their templates [2,24,43,53,68,70,72,81].The templates are either available through being parsed before, yet with no guaranty of correctness, or they origin from candidate sets of labeled logs for supervised settings <a href="GP-1">24,53,62,72,81</a>.</p>
<p>As mentioned in [43] the quality of demonstrations is of high importance for the parsing effectiveness since it can also introduce noise and confuse the LLM.The works [24,[70][71][72]77] support this finding in their ablation studies by comparing the performance of their RAG sampling techniques to random retrieval or static examples, which generally achieves lower scores than strategic retrieval.</p>
<p>Another noteworthy phenomenon observed in generative language models is recency bias, which refers to the tendency of the model to give disproportionate weight or attention to information that appears closer to the end of the prompt [79].Consistently, Xu et al. (DivLog) [72] observed a significant impact on parsing performance based on the order of the retrieved examples in the prompt, whereby an ascending order of examples by similarity achieved the best results.The ordering of demonstrations in ascending order is adopted by multiple works [24,68,70,72].</p>
<p>Caching (PS-3)</p>
<p>As LLM calls are costly compared to traditional algorithms, many approaches therefore create certain structures like clusters [22,24,27,53,62,70,71,78], prefix trees [24,27,27,43,68,81] or other structures containing logs and their already parsed templates.In case the target log (or logs) matches an existing template (from an already parsed log) or is at least very similar, the parsing algorithm skips the LLM call and returns the corresponding template retrieved from the structure for this log.This self-evolutionary process [53] is visualized in Fig. 2 by the arrow pointing from the templates to the cache component.This approach can greatly increase efficiency, allowing the usage of parsing with LLMs in settings where fast and cost efficient parsing is crucial, with reported runtimes comparable to runtime efficient parsers, like Drain [19].A parsing cache can also enhance effectiveness by mitigating the instability of results that often occurs with generative language models [24].Astekin et al. [3] find that even with temperature 0 some models do not answer deterministically.This emphasizes the usefulness of caches even more.However, it is possible that the first retrieved and stored template for an event type is incorrect, even if it matches some of the incoming logs.Such an event can be counteracted by a template revision step (PS-5).All but one paper [60], which described an approach with some form of caching, also described a post-processing step for correcting templates.</p>
<p>Since the total number of templates is drastically less than the number of logs, LLM parsing becomes a scalable approach with caching [81].For example, the LogHub-2.0datasets [25] contain more than 50 million logs but less than 3500 templates.14 approaches apply some form of caching, yet some do not explicitly name it "caching" and describe it simply as template matching.The structures used for caching can be efficiently and effectively used for sampling similar logs or relevant parsing demonstrations for ICL with RAG.Caching is not bound to the usage with LLMs and can be built into any other parsing approach.</p>
<p>LLM Usage (PS-4)</p>
<p>We observed three different ways an LLM can be applied: preprocessing, post-processing and direct parsing, from which we derived the main components of the flow chart in Fig. 2.</p>
<p>Three approaches employ the LLM for preprocessing.ECLIPSE [77] applies the LLM for the extraction of semantic information from the logs that is then used by an LCS-(longest common subsequence) and entropy-based parsing algorithm.Fariha et al. [15] used an LLM for extracting the regular expressions necessary for subsequent parsing with SPELL [13].LogGenius [74] uses an LLM to augment log groups with low diversity.The actual parsing task is then left to existing unsupervised parsers, like Drain [19] or SPELL [13], for which their parsing effectiveness should be enhanced by the increased diversity of the logs.</p>
<p>Five approaches perform post-processing with the LLM.In all cases, this is done to correct the already parsed logs to yield more accurate templates.How this is performed is explained in Sec 5.9.</p>
<p>Direct parsing is the most straightforward way of parsing with LLMs which is utilized in all but 6 approaches.There are differences in how the approaches prompt the LLM, yet they all have a similar structure.The prompts these approaches use consist of the following components -note that the components are not strictly separated from each other and can be interlaced:</p>
<p>• Instruction: Instructs the LLM to parse the log(s) and how.The instruction may also contain rules, such as replacement rules for the timestamp or IP address [62] or special treatment rules for logs concerning exceptions, errors or similar.• Context explanations (optional): This part contains explanations about logs or log parsing.For instance, Zhong et al. [81] state to improve the LLM's capabilities of variable identification by including explanations about variable categories as outlined in [38].Other approaches [2,22,61,62,71,81] also provide information about variable types in the prompt.</p>
<p>The categorization of the variables can also be beneficial for downstream tasks and the interpretability of the results [38,81].</p>
<p>• Context examples (optional): This part either contains multiple logs to illustrate their variabilities and commonalities or logs and their templates as parsing demonstrations.This part can be static, or dynamic using RAG.Providing examples standardizes the LLM's output format, leading to more stable results [43].• Output constraints (optional): This part instructs the LLM how to format the output.</p>
<p>Some approaches enforce JSON format, but most approaches determine one or two markers (e.g., often backticks or something like /START and /END) so that in a post-processing step the generated template can be extracted easily with regex after the marker or between markers.This improves output quality, because generative language models sometimes generate unwanted output.• Target log(s): Contains the log(s) to be parsed.The approaches [22,43,53,65,70,71] provide multiple target logs at once in the query, while the rest provide single logs.Providing multiple logs at once can help the LLM in abstracting variable and static parts from the logs [53,70].</p>
<p>Le et al. [33] find that with a simple prompt, where no context is provided nor a detailed instruction is given, GPT-3.5 is hardly able to understand the concept of log parsing.Sun et al. [61] achieve a significantly higher PA (20% to 60%) with a prompt that contains extraction rules for direct parsing (on the HDFS and BGL datasets from LogHub [83] with GPT-3.5 and GPT-4) than without extraction rules.</p>
<p>Template Revision (PS-5)</p>
<p>In total, 14 papers revise templates in a post-processing step.All works that correct their templates in a postprocessing step also use a cache (PS-3).The works [39,68,78,81] employ the LLM to revise templates per prompt: Lemur [78] uses CoT in three dialogue rounds for revising semantically similar templates for potential merge.First, the structure is revised, then the semantics, and in a final round the LLM is asked for a solution based on the first two rounds.LogParser-LLM [81] and LogBabylon [27] generate a parsing tree from the templates during parsing.In case, a loose match is identified the LLM decides if the template should be merged, leading to an update of the template tree, or the creation of a new tree root node.The works [39,43,68] re-prompt the logs to the LLM, with the same prompt they were initially parsed, to the LLM if they do not match all the related logs until all logs are matched [39] or if a certain number of re-prompts is exceeded [43].AdaParser [68] also revises templates if their variables contain certain keywords linked to exceptions, failures or interrupts, which they state should not be abstracted with a wildcard [68].</p>
<p>Updating the previously parsed templates can adapt the parser to changes in the computer system and help correct faulty templates.Templates are typically faulty because they have static parts of the log identified as a variable, or variable parts were identified as static.Faulty templates can be identified by high similarity with an existing template [24,77,81], if not all logs within a similarity cluster can be matched [70], or if the centroids of log clusters move closer together through newer incoming logs added to clusters [71].Templates can be merged with templates that partially match with an existing template, by traversing a prefix tree [24,27,81], LCS [77,80], implicitly by reentering the logs into the parsing queue [70], or by monitoring whether multiple paths (templates) of a prefix tree join together in a subsequent node [53].A previously parsed template can also be replaced with a new and more permissive template [65,80].More permissive in this context means that more variables were identified for the newer template.</p>
<p>Datasets (R-1)</p>
<p>All of the papers in our selection use one of the LogHub versions.The most popular dataset is LogHub [83] used in 14 of the them.LogHub-2.0[25] is used 8 times, and corrected LogHub [28] is used 6 times.The works [68,70,74,77,81] evaluate on two or more datasets, or dataset collections.Zhang et al. (ECLIPSE) [77] and Pang et al. [52] use datasets, other than any of the LogHub ones, that are not publicly available, while [46,65,74] use custom open-source datasets.The usage of custom datasets along with the datasets from the LogHub versions provides a more comprehensive view on the performance of the parsers.While it is positive to use a variety of datasets for the evaluation, it can also hinder the comparability of the parsing results.For instance, Khan et al. reported performance differences of the conventional parsers on LogHub [83] versus corrected LogHub [28].Fu et al. [16] and Zhang et al. (ECLIPSE) [77] showed that many state-of-the-art log parsers perform poorly on their custom datasets while performing well on the LogHub datasets [83].</p>
<p>Using custom datasets alongside LogHub datasets offers a broader view of parser performance but can affect result comparability.The studies [16,28,77] show that parsers often perform well on LogHub datasets [83] or its descendants [26,28] but poorly on custom datasets.Additionally, it is not clear whether the open-source log datasets used are known to the LLMs from their extensive pretraining phases.However, the ablative studies of [24,70,72] report significant performance improvements with ICL compared to direct zero-shot parsing, arguing that this implies a low likelihood of memorization of single log templates for GPT-3 and GPT-3.5-Turbo.</p>
<p>Metrics (R-2)</p>
<p>Since the correctness evaluation of templates is not straightforward, there is a strong variety of different evaluation metrics.The set of all used metrics is {, ,  , , ,  ,  , other}.Except for "other", these are the traditional metrics that have been widely used [25,28].If we compute the Jaccard Index of the used metrics and all available metrics and average the results we get roughly 0.33, which means that on average only 33% of the full range of metrics is used (including "other" as a metric).More specifically, 5 publications use only PA, or PA and GA, 6 use ED, 12 publications use at least one metric that is insensitive to class imbalance (FGA, FTA).Also, 5 publications use metrics other than traditional ones.For example, the creators of LogBatcher [70] report the normalized Edit Distance (NED) [45] which computes the mean ED of all template pairs compared in the data set.In [3] Astekin et al. report the number of unique templates as a means of LLM determinism, while in [2] they report variations of metics based on ED and LCS.The creators of LogParser-LLM [81] propose metrics based on Granularity Distance, the minimum number of operations necessary to transform one parsing result into the other.An operation is thereby defined as converting a static part of the parsed template to a variable or vice versa.</p>
<p>The metrics cover different characteristics of correctness.It is therefore important for evaluating parsers to cover the relevant evaluation aspects with these metrics.For instance, Jiang et al. [25] proposed using FGA and FTA since they are insensitive to class imbalance, while Khan et al. [28] recommend using GA, PA, RTA and PTA to cover all aspects (FTA is the harmonic mean of RTA and PTA).For example, the anomaly detection tool, DeepLog [14], detects anomalies solely based on the event ID.Each event ID corresponds to a unique template.Consequently, it is indifferent if the parsed templates are correct at the template level as long as they are grouped together correctly, which is indicated by a high score for GA.Since downstream tasks, like anomaly detection, can focus on a variety of data characteristics [5], it is important to report metrics that cover these characteristics.</p>
<p>Previous work [40,49,54] found that the state-of-the-art log parsers are often evaluated with insufficient selections of evaluation metrics.While their work concerns the conventional non-LLM based approaches, we find that many LLM-based approaches carry on with this manner.More specifically, we consider using only PA or the combination of the grouping metrics GA and FGA as insufficient.The highly varying scores between the metrics of different parsers in [25,28] but also in our own evaluation in Sec.7 show how important a versatile evaluation metric selection is.Focusing only on a single aspect misleads estimations of a parser's performance.</p>
<p>Model (R-3)</p>
<p>An overview of the LLMs used is given in Table 3.We list only the base models or the name of the model series, since there is an overwhelming variety of different versions and sizes of LLMs.The most widely used model series is GPT.The works [2,3,9,23,39,42,43,46,65,68,74,81] evaluate multiple LLMs and compare their performance.Since the papers use different evaluation settings (metrics, datasets) or configurations, it is not straightforward to say which LLM performs best.To this end, we refer to the aforementioned works that compare the results of multiple LLMs and specifically the comprehensive benchmark by Cui et al. (LogEval) [9] which evaluates 18 different LLMs (partly from the same series, but with different sizes).</p>
<p>Code (R-4)</p>
<p>From our selection, 16 papers make their implementation available and provide links to their code repositories.At the time of writing, two of them [22,62] lead to an empty or non-existing repository, but consider that both are preprint versions.The rest did not make their code open-source.A study by Olzewski et al. [51] analyzed the reproducibility and replicability of 750 machine learning papers and their codebases and datasets from Tier 1 security conferences between the years 2013 and 2022.</p>
<p>They found that about 59% of the papers did not provide any reproducible artifacts.In our study, 45% of the reviewed papers do not provide any reproducible artifacts (including preprints).</p>
<p>Code Quality.</p>
<p>A large proportion of the approaches do not provide code and therefore can not be used for the evaluation in Sec. 7.Even with available code, some do not provide the comprehensive functionality to replicate the processes described in their papers and comprehensively correcting the code of others is out of scope for this work.At the time of review (February 14th, 2025) the code of Lemur 3 [78] lacks a script for selecting the previously parsed templates for the subsequent CoT template merging process.The code of LogGenius 4 [74] runs into multiple errors due to missing folders and files, while there is no README file providing instructions.The LogEval repository 5 [9] does not provide the scripts for parsing nor instructions.LogPrompt 6 [39] encounters multiple errors due to simple typos (e.g. a variable that seems to be undefined but it is actually spelled differently than in its definition).Furthermore, they describe three different prompt strategies, but it is not clear which they used for their result nor do they explain it in the instruction file.Moreover, they do not provide the functionality to run the parsing process except for the self-prompt setting.SelfLog [53] proposes a self-evolutionary approach, in which they "retrieve the most similar historical logs and their templates from the data through an Approximate Nearest Neighbors (ANN) search, serving as the corpus for In-Context Learning (ICL)" [53].One could think that this means they update the database incrementally after each newly parsed log.However, in the code 7 the script provided for the effectiveness evaluation does not contain the self-evolution functionality.They provide a second file for online parsing that does provide this functionality but it does not work ad hoc due to missing files and missing function parameters.Also, by default they do not use ANN but cosine similarity for retrieval which is somewhere hidden in the code (not an adjustable parameter).The code repository of SuperLog 8 [23] only provides code for LLM training, but not the described framework for parsing.Moreover, we found that many repositories provide sparse instructions, which further impedes reproducibility and application, especially when the execution scripts do not work ad hoc.These issues appear to be widespread rather than isolated cases.Similar findings were reported in a large-scale study by Trisovic et al. [64], which examined thousands of replication code repositories containing R files published between 2010 and 2020.Their analysis revealed that a significant majority of these files failed to execute properly, even after code cleaning.Likewise, research by Olzewski et al. [51] showed that more than half of the artifacts from nearly 300 reviewed papers could not be run at all.Even among the repositories that did execute, only a fraction produced the expected results, while others either generated different outcomes or lacked crucial components such as arguments or outputs. 3https://github.com/zwpride/lemur;accessed 14-February-2025. 4https://github.com/huashengyihao/LogGenius;accessed 14-February-2025. 5https://github.com/LinDuoming/LogEval;accessed 14-Febrary-2025. 6https://github.com/lunyiliu/LogPrompt;accessed 14-February-2025. 7https://github.com/CSTCloudOps/SelfLog;accessed 14-February-2025. 8https://github.com/J-York/SuperLog;accessed 14-Febrary-2025 5.13.2 Licenses.As we focus on an application perspective, we also report the code licenses for the approaches for which the code is available.They are given in Table 4. Keep in mind that it is possible that code repositories of preprint papers might be preliminary versions or unfinished.</p>
<p>Notable Mentions</p>
<p>Besides the features we defined in Sec 4.2, we identified other promising techniques and ideas, worth mentioning in the reviewed papers.For instance, some approaches specifically focus on highlighting the commonalities and variabilities of logs.Lemur [78], LUNAR [22], OpenLogParser [43], and LogBatcher [70] cluster the logs into multilevel clusters for parsing while the supervised parsers DivLog [72] and LILAC [24] use this concept for sampling labeled logs.The coarse-grained clusters thereby capture the commonality of the logs, that, in the best case, all belong to the same log group (i.e. the same event), and the fine-grained clusters should highlight the variabilites within that group.This idea is also used by LogShrink [36] for compressing large log files.Xiao et al. (LogBatcher) [70] report to use the heuristic rules proposed by Khan et al. [28] to correct templates.This includes measures, such as combining subsequent wildcards, like &lt;<em>&gt;&lt;</em>&gt;, into a single one &lt;*&gt;.This technique, or similar, is also used by [24,71,72] and other work outside of our selection [34,40].By manually inspecting some the templates generated by the parsers used for the evaluation, we found that a significant proportion of templates would have been considered correct by the evaluation functions, if some of these steps would have been applied.</p>
<p>Evaluation Setup</p>
<p>In terms of evaluation, the existing literature regarding LLM-based log parsing misses a common ground.While there are some commonalities regarding single features, such as used datasets, used evaluation metrics, or used models, the combination of these is highly varying between the approaches -see feature R-1, R-2, and R-3 in Table 2.As a consequence, we create a benchmark comparing a subset of 7 out of the 29 approaches.To counteract randomness from sampling, LLM output and runtime fluctuations, we run each parser 3 times and take the average.</p>
<p>Selection for the Benchmark</p>
<p>For the benchmark we only select parsers which made their code publicly available -see feature R-4 in Table 2.We distinguish between approaches that solely focus on methods changing model parameters, like fine-tuning and pretraining, and approaches that apply the LLM as a tool within a framework.Prior studies showed that fine-tuned [46] or pretrained [23] models can achieve stateof-the-art accuracy scores without pre-and post-processing modules for RAG (PS-2), caching (PS-3), and template revision (PS-5).However, it is clear that improved quality of the LLM's output leads to overall improved output quality of the whole framework.It is therefore meaningful to separate model-centric and model-wrapping approaches -especially, in the rapidly evolving landscape of LLMs, where more performant models are introduced on a daily basis.For our evaluation, we limit our scope and focus on these frameworks and therefore do not include the fine-tuning or pretraining approaches LLMParser [42], SuperLog [23], and the work of Mehrabi et al. [46], even tough they provide their code or model.We also exclude Semirald [61] since it does not provide a log parsing framework and focuses rather on anomaly detection.In Sec.5.13.1, we described that some approaches provide code, but do not provide functioning parsers.They are consequently not included in the benchmark (except for LogPrompt since its typos could be resolved by exchanging single characters).</p>
<p>Given the above-stated matters the remaining approaches featured in the benchmark are LILAC [24], OpenLogParser [43], LogBatcher [70], SelfLog [53], DivLog [72], LLM-TD [65], Log-Prompt [39].To a large extent, this selection represents the state-of-the-art of LLM-based log parsing frameworks, as their approaches cover most of the aspects of this field.Unfortunately, due to the aforementioned selection criteria, this benchmark could not include any work using LLMs as pre-or post-processors.</p>
<p>Datasets and Baseline</p>
<p>Following previous work [2,3,33,34,42,70,80] we take the corrected version of LogHub from Khan et al. [28] as the default dataset for our evaluation.It consists of 16 datasets with ground-truth templates from different domains such as distributed and supercomputer systems, and server application.Note that we found minor errors in the "Content" column of the corrected version, mostly additional character spaces, that led to errors when evaluating.We exchanged this column (during runtime) with the "Content" column from the original LogHub version [83] (only the templates are different but the content is the same).In Sec.7.1, we also use the original LogHub version [83] which contains the non-corrected templates.For the runtime evaluation in Sec.7.2 we follow previous work [24] and only report the runtime of the large-scale datasets from LogHub-2.0[25].This choice is motivated by the need for a more extensive data as the 2000 samples from the original LogHub are insufficient for a comprehensive runtime analysis.The results for the runtime on the LogHub datasets are available at [4].</p>
<p>Since the LogHub datasets are so widely used for the evaluation of log parsers (see Sec. 5.5) there is a tendency that the parsers work especially well on these datasets but not on others.To simulate a use case closer to a real life example, we extend the 16 datasets by a custom dataset based on AIT Log Dataset V2 [31,32].The dataset contains 2000 audit logs (from russellmitchell/gather/intranet_server/logs/audit) with 8 unique templates.This dataset will be referred to as Audit in the following.The logs were manually annotated by ourselves, adhering to the style of the templates from the corrected LogHub version [28].Audit logs track system activities, user actions, and changes for security, compliance, and troubleshooting.The corresponding files can be found in our code repository [4].</p>
<p>For the baseline of the evaluation, we select five non-LLM-based parsers from the LogPAI logparser repository 9 , namely AEL [26], SPELL [13], Drain [19], ULP [57] and Brain [73].AEL, SPELL and Drain have been widely used in the log parsing research and their performance has been thoroughly investigated by [25,28,76].Their papers were published in 2008 (AEL), 2016 (SPELL) and 2017 (Drain).ULP and Brain are newer advancements from the years 2022 and 2023.The baseline parsers use the standard configuration of the literature [24,25,70,83].The default configuration of all these parsers contains the log format, regular expressions (but empty for some datasets), and up to two other parameters that are specific to each of the LogHub datasets.For the Audit dataset [31,32] the log format "type=<Type> msg=audit(<Time>): <Content>" was used and the regular expression parameter was left empty.The other parameters for the Audit dataset were chosen through grid search by the best score for GA.We used the full dataset for this hyperparameter tuning to show the best possible performance of the baseline.</p>
<p>Evaluation Metrics</p>
<p>Following previous work [25] we report Group Accuracy (GA), Parsing Accuracy (PA), and F1-score of Template Accuracy (FTA) which are described in Sec.4.2.GA and PA are the most used ones in the literature (cf.Sec.5.11) and FTA is recommended by Joang et al. [25].Section 4.2 also described the Edit Distance (ED), which is the recommended parsing metric of Petrescu et al. [54], from which we compute the Normalized Edit Distance (NED) [45].From our selection, NED is only used by Xiao et al. (LogBatcher) [70].Nevertheless, we report NED because normalization makes it a more intuitive indicator of effectiveness than ED, as it fits into the [0, 1] range of the other conventional metrics.</p>
<p>Implementation and Settings</p>
<p>This section describes the implementation details and the settings used.</p>
<p>6.4.1 Settings.The evaluation was performed using three LLMs of different sizes (small, medium, large):</p>
<p>(1) CodeLlama 10 (codellama:7b-instruct) is built on top of Llama2 from Meta and has been designed for code-related task.It offers strong performance while requiring significantly fewer parameters than other models.This model is employed to illustrate the performance of a relatively small and efficient LLM (7 Billion parameters).Astekin et al. [2] report that CodeLlama outperforms other LLMs (such as GPT-3.5) in parsing accuracy (PA).In a separate study, Astekin et al. [3] find that both CodeLlama and GPT-3.5 deliver stable results for the log parsing task.The model was run on a Ubuntu 24.04 LTS server with a 15-core Intel Xeon Gold 6226R CPU and a Tesla V100S (32 GB) GPU via Ollama 11 .(2) GPT-3.5 12 (gpt-3.5-turbo-0125)is a widely known closed-source model, reaching outstanding performance in the earlier stages of the LLM hype.It was chosen because it is the most used one from all reviewed papers and it is considered to be medium sized (the size is actually undisclosed but a paper by researchers of Microsoft [59] state it is 20 billion parameters).The model is proprietary and was accessed via the OpenAI API.(3) DeepSeek R1 13 utilizes a novel reinforcement learning approach for model training.It has recently received wide public attention due to its outstanding performance, while being relatively small (but still large) compared to similarly performing models like OpenAI's o1 model, in combination with its open-source release.This model is used to assess the potential gains in accuracy that can be achieved by utilizing one of the most effective models (671 billion parameters).DeepSeek R1 was called via API from TogetherAI 14 .</p>
<p>The runtime evaluation with the LogHub-2.0dataset [25] was run on an Ubuntu 24.04 LTS server with a 32-core AMD EPYC-Milan CPU and 64 GB RAM.With the computation on this hardware and LLM calls to the GPT-3.5 API.In general, we use GPT-3.5 as the default LLM for our evaluation since it is the most used one by the selected approaches.</p>
<p>Configuration and Code Changes.</p>
<p>As mentioned in Sec.5.13.1, there are some issues with some of the parsers' code which had to be resolved for the benchmark.To ensure the comparability of the output of the parsers, other changes were also necessary to the code of each parser.These changes are kept as small as possible to not disturb the original design or interfere with performance, or at least as little as possible.To ensure model or API compatibility, we updated deprecated versions of the OpenAI (python) package to the newer working ones and we added the functionality to call the OpenAI, Ollama, and TogetherAI API if not given.To have consistent output format, we ensure that the template placeholder symbols are always &lt;*&gt;.Since the parsers LLM-TD [65] and LogPrompt [39] do not only parse the log content but the entire log, we modify the input to these parsers to only parse the content as well.As found by He et al. [18], this typically improves the parsing accuracy.</p>
<p>Specific changes were made to DivLog [72], which is one of the earliest works on LLM-based log parsing.It does not feature a caching mechanism and therefore, calls the LLM for each log line.Since this is a significant burden in terms of runtime (and cost) and therefore also unrealistic for real-life adaptation, we added a simple cache consisting only of the set of already parsed templates.The cache returns the corresponding template in the event of a match instead of calling the LLM.This slightly affects the accuracy metrics due to improved output consistency.Additionally, we implemented a minor post-processing step where multiple consecutive wildcards were replaced by a single wildcard &lt;*&gt; for efficient matching, to resolve an issue with the generation of infinite consecutive wildcards for some of the logs.This measure slightly increases DivLog's accuracy.</p>
<p>LogPrompt [39] accepts a maximum prompt size parameter which is by default 3000, corresponding to roughly 25 logs parsed at once (as they state).Early experiments revealed that this performs extremely poor, hence we set the maximum prompt size to 1000.This corresponds to roughly 5 to 10 logs per prompt, which is in the recommended range of simultaneously parsed logs by Xiao et al. [70].If a template cannot be matched with a log it is again sent to the LLM.We limit the maximum number of re-prompts to 3 to avoid infinite LLM calls (which occurred in early experiments).LogPrompt also required some code corrections, other than typos.For some datasets, LogPrompt runs into an infinite loop due to a faulty template extraction process from the LLM responses.This was fixed by computing the enumeration of the logs instead of extracting it from the prompt again.From the paper it is not clear whether they used ICL, CoT or the self-prompt paradigm for their parsing performance evaluation, yet the parsing functionality is only given for the self-prompt case.</p>
<p>SelfLog [53] requires the user to set up an SQL database, yet the main script (run.py)does not contain the self-evolution functionality described in the paper (updating the database with new templates).Therefore, we removed the log-groundtruth template examples part from the prompt and operated the parser as unsupervised and without RAG (no database).6.4.3Sampling for Supervised Parsers.From the selected parsers LILAC [24] and DivLog [72] are supervised parsers and require labeled logs to achieve competitive accuracy.In this study, we emphasize minimal human effort and therefore keep the number of available templates  small, namely  = 2 and  = 4.Note that the Apache dataset from LogHub [83] contains the smallest number of templates, which is only 6.For LILAC and DivLog it has been shown that the performance increases with an increasing number of labeled logs available [24,72].This is obvious as the similarity search retrieves the exact template the target log belongs to with a higher likeliness.Multiple works from our selection [24,72] as well as [34] state that having a diverse candidate set is crucial to reduce the risk of overfitting to a specific log template, which is why they create specific sampling algorithms to sample from labeled logs.However, this demands having a large proportion of templates at hand and may require users to label logs manually, thus may hinder real-life application [22].</p>
<p>To ensure comparability, we exchanged the sampling processes of the supervised parsers DivLog [72] and LILAC [24] with a simple custom sampling approach: from all labeled logs of a dataset we randomly select  (unique) templates.For each of those templates we randomly select a matching log and thus yield a set of log-template pairs which constitutes the candidate set for the supervised parsers.We sample a new set before each run.In the further course of this paper, we indicate the two supervised parsers by the suffix "-", thus LILAC-, DivLog-, with  ∈ [2,4].</p>
<p>Evaluation Results</p>
<p>This section contains the evaluation results with focus on effectiveness and efficiency.</p>
<p>Effectiveness</p>
<p>7.1.1Baseline Performance.The performance of the baseline parsers is visualized in Fig. 3.We can see that neither of the baseline parsers achieves satisfactory performance on FTA and PA, meaning that most templates are not matched exactly nor correct when compared token-by-token.7.1.2Performance of LLM-based Parsers.The evaluation results of the parsers with the three LLMs on LogHub [83] are given in Fig. 4a, Fig. 4b, and Fig. 4c.Note that we excluded LogPrompt [39] from the evaluation with DeepSeek R1 because the LLM was barely able to extract any templates, resulting in scores of roughly 0 in experimental runs.The LLM did not provide the output in the structured format that was requested.Therefore, the template extraction process, which is based on markers positioned directly before the extracted template, was not able to properly extract the template.This resulted in long inner monologues of DeepSeek R1 and maximum reprompts due to log-template mismatches which would have resulted in an unreasonable financial expense given the cost of the LLM and the poor performance.In the boxplots of Fig. 4a, Fig. 4b, and Fig. 4c we can see that the best performance on each metric and for each LLM is achieved by LogBatcher [70], followed by LILAC [24] for 2 and 4 shots.LogBatcher and LILAC clearly outperform the conventional parsers, while the conventional parsers outperform the rest of the LLM-based parsers.In general, the performance of each LLM parser is rather stable across the different models.CodeLlama visibly performs worst, but it is also by far the smallest of the models with 7 billion parameters.Given the size difference of GPT-3.5 and DeepSeek R1, it is surprising that DeepSeek R1 does not outperform GPT-3.5.Furthermore, we report the exact numbers for the evaluation with GPT-3.5 in Table 5 to show how the parsers performed on each individual dataset.</p>
<p>7.1.3LLM General Performance.Figure 5 shows the performance for CodeLlama, GPT-3.5 and DeepSeek R1 averaged over all parsers and datasets.One can see that CodeLlama performs worst while GPT-3.5 slightly outperforms DeepSeek R1 except for PA.Consequently, the financial implications of employing DeepSeek R1 do not justify the cost.For instance, the OpenAI API for GPT-3.5 (gpt-3.5-0125)costs 0.5$ per 1 million tokens (input and output) while DeepSeek R1 from the TogetherAI API costs 3$ for input and 7$ for the output per 1 million tokens.We found that the reasoning steps of DeepSeek R1 are not always helpful and especially when a structured output is required may confuse the LLM.A possible reason could be that the process of log parsing, not being complex enough, leads to overthinking [8].However, in [8] the researchers state that DeepSeek R1 is robust against overthinking.The usefulness of reasoning models for log parsing should therefore be investigated in future research.As in the study by Khan et al. [28], we report the performance difference between the evaluation on the corrected LogHub datasets [28] and the original LogHub datasets [83] (performance of corrected minus performance of original) with GPT-3.5.In Fig. 6 we can see that the highest differences are given for LogBatcher [70], followed by LILAC [24] for both sample numbers.Naturally, LILAC's and DivLog's [72] samples were collected and evaluated with the same dataset.That the difference is rather on the positive side indicates that they achieve better performance on the corrected LogHub dataset.Interestingly LILAC was originally evaluated with LogHub-2.0whose templates correspond to the original LogHub templates and not to the corrected ones [24].In general, we can see a slight tendency that the parsers output templates that correspond more to the corrected datasets' templates, which suggests that LLMs "intuitively" prefer the corrected templates' format.7.1.5Performance on the Audit Dataset.An analysis of the performance of all parsers on the Audit dataset reveals that only the supervised parser, LILAC [24], and to some extent also DivLog [72], are able to parse the logs effectively.Despite the fact that the parameters of the baseline parsers were hyperparameter tuned, they did not achieve satisfactory results.It appears that the intended templates' format of the logs is not straightforward for conventional parsers, but neither for GPT-3.5 without supervised demonstrations.This finding underscores the efficacy of supervised parsers, demonstrating that a mere two templates are sufficient to attain a reasonable performance level, potentially even a single one demonstrating the preferred template style.</p>
<p>Efficiency</p>
<p>For the evaluation regarding efficiency we select the well-known datasets HDFS and BGL from LogHub-2.0[25].HDFS contains 11.2 million logs with 46 unique templates while BGL contains 4.6 million logs with 320 unique templates.We excluded DivLog [72] and LogPrompt [39] from this evaluation since they do not utilize caching.The computation would therefore scale linearly with the number of logs and require multiple million LLM calls.</p>
<p>Figure 8 visualizes the computation time and LLM invocation time of the parsers, Fig. 9 shows the number of LLM calls made.The invocation time for conventional parsers is, of course, zero.Outstanding runtime efficiency is achieved by the conventional parser ULP [57] on both datasets.While LLM-TD is the second fastest and also calls the LLM the least times it is rather the opposite on the BGL dataset.As mentioned in Sec.5.13.1 and 6.4.2 there were some implementation problems with SelfLog [53].They provided a faulty script for the efficiency evaluation which is why we ran the evaluation with the script they designed for the evaluation of the 2000 log lines version of LogHub [83].However, for both HDFS and BGL the processes got killed and are therefore not featured in the plots.</p>
<p>Discussion</p>
<p>This section summarizes the findings of the literature review from Sec. 5 and of the evaluation from Sec. 7 by answering the research questions from Sec. 1.</p>
<p>Answers to Research Questions</p>
<p>RQ1: What are the main advantages and disadvantages of LLM-based log parsing approaches and non-LLM-based approaches?LLM-based log parsing approaches offer key advantages such as adaptability to diverse log formats, the ability to generalize across different system logs, and robustness to unseen log templates as demonstrated by the evaluation on the Audit dataset [31,32] in Sec.7.1.5and other work [16].Approaches like the unsupervised parser LogBatcher [70] and the supervised parser LILAC [24] can handle complex and unstructured logs more effectively than conventional methods, as demonstrated in Sec.7.1.1 and Sec.7.1.2and their corresponding publications [24,70].They utilize both syntactic and semantic information within log data, outperforming methods that focus solely on syntactic (e.g.Drain [19], SPELL [13]) or semantic information (e.g.DivLog [72]).However, they also come with drawbacks.The usage of LLMs comes with a significant computational burden, implying the requirement of hardware capable of running LLMs locally or using external API services.Methods like caching can significantly reduce the number of LLM calls and therefore also the latency, but especially systems with a high number of templates or frequently changing templates (e.g.due to updates) may still use too many resources for certain use cases.The use of LLMs also introduces randomness into the process due to the randomness within the LLM's output and possibly overthinking for reasoning models [8].In contrast, approaches that do not integrate language models into their process, such as Brain [73] and ULP [57], have deterministic output, are computationally efficient, and are often easier to deploy but lack the flexibility of LLMs in handling diverse log templates.</p>
<p>RQ2:</p>
<p>To what extent do LLM-based log parsing approaches rely on labeled data and manual configuration?LLM-based log parsing approaches typically require labeled datasets for fine-tuning or evaluation, though some methods leverage self-evolutionary approaches using previously parsed templates to substitute the need for a priori labels.However, with only 2 and 4 shots of labeled logs, LILAC [24] achieves superior performance on the corrected LogHub dataset [28] and Audit [31,32], compared to the conventional parsers and other LLM-based parsers, excluding LogBatcher [70] (Sec.7).Furthermore, LogBatcher demonstrates impressive performance on both runtime and effectiveness while being unsupervised and only requiring the log format as input parameter.Fine-tuning [27,42,46,52,81] or even pretraining approaches [23] also report outstanding performance, yet their stronger reliance on labeled logs and computational power and the robust and high performance of LogBatcher and LILAC demonstrate that ICL is a cheaper but still performant alternative.</p>
<p>Compared to the baseline parsers AEL [26], SPELL [13], Drain [19], ULP [57] and Brain [73], the LLM-based parsers of our evaluation selection require on average less configuration parameters (Sec.5.5).This finding is particularly noteworthy in light of the satisfactory performance achieved by LogBatcher [70].This finding suggests that the deployment of LLMs can effectively substitute a substantial proportion of manual effort, and thus enhance the usability.</p>
<p>RQ3: Which techniques can enhance the efficiency or effectiveness of LLM-based log parsing?It is evident that ICL and fine-tuning present powerful learning paradigms for LLM-based log parsing.They are not mutually exclusive to each other and can be employed simultaneously to improve parsing accuracy (Sec.5.4).ICL can be enhanced with RAG or with previously parsed or labeled logs as demonstrations.The ablative studies of reviewed papers [24,[70][71][72]77] unanimously conclude that dynamic demonstrations and sophisticated demonstration selection algorithms improve accuracy scores (Sec.5.6).Smart caching solutions show efficiency improvements in terms of runtime but also monetary expenses since LLM calls are usually costly (Sec.5.7).Template revision methods that correct, delete, or merge related templates in postprocessing can further improve cached templates.This can also help adapt the parser to evolving logs due to updates or other behavioral changes of the computer systems without reconfiguration or retraining (Sec.5.9).Evaluations show that it is also possible to improve the effectiveness of parsers with more powerful LLMs, yet the effective application of reasoning models like DeepSeek R1 should be investigated more closely (Sec.7).It has been shown that variable-aware prompts improve the effectiveness in direct parsing with LLMs.Naturally, more explanations in the prompt mean more tokens per parsed log, which negatively affects the inference time of LLMs.Short and concise instructions and explanations could thus alleviate this issue.Furthermore, creating coarse-grained clusters to capture the commonalities of logs and fine-grained clusters to capture the variabilities of logs to support the template extraction process constitutes another promising technique, that can be applied by conventional and LLM-based parsers alike (Sec.5.8).</p>
<p>RQ4: Which experiment design choices hinder the comparability, and hence which guidelines should be adopted in terms of configuration, datasets used, evaluation metrics, and reporting to make results comparable?Consistency in preprocessing methods is crucial for fair comparisons.For instance, not all parsers require the log format (see definition in Sec. 2) as an input parameter, such as LLM-TD [65] and LUNAR [22].LUNAR explicitly includes log headers, such as IP addresses, timestamps, or levels, to grasp the full context of the log.For instance, partitioning logs into groups based on their timestamps is also possible with time intervals [82].Previous research [18] found that parsing only the extracted log message part, according to the log format, improves effectiveness, but especially LLMs with their generalization capabilities could utilize this extended context [22].In general, the choice of configuration parameters significantly impacts comparability.For example, the researchers of Lemur [78] report near-perfect scores of 0.999 and 0.996 for FGA and GA on LogHub [83], yet they require log format, regular expressions, and four other hyperparameter-tuned parameters for each dataset.This demonstrates the potential performance levels that can technically be achieved.However, the number of parameters required is impractical for real-world applications and complicates meaningful performance comparisons.</p>
<p>The datasets related to LogHub [26,28,83] are used in most publications, namely 25 out of 29.LogHub and its versions are widely used benchmarks, enabling direct comparison with a large body of existing research and providing a standardized way to evaluate parsing performance.We therefore recommend using at least one of the LogHub versions for the sake of comparability.However, relying solely on LogHub datasets may not cover all possible log structures and scenarios.Incorporating custom or other open-source datasets reveals how well parsers generalize to different types of logs, identifies their strengths and weaknesses, and avoids overfitting to a single dataset (or dataset collection) (Sec.5.10).</p>
<p>We recommend a set of metrics, namely GA, PA, FTA, and NED, to be standardized across studies to cover the relevant characteristics of templates, since each metrics maps to specific requirements of specific downstream tasks (Sec.5.11).This is important to prevent single-sided metric selection that focuses on single characteristics.Using FGA over GA should also be considered if class imbalance is a concern, but if not, there is little difference whether GA or FGA is used as long as the aspect of grouping performance is covered.The results of our evaluation in Sec.7 show high variance between these metrics for different parsers, indicating that this selection covers a variety of aspects.</p>
<p>The aforementioned experiment design choices concern all types of log parsers.Specific to LLMbased parsers, the LLM employed is also an important factor to consider regarding the comparability of evaluation results.As demonstrated, CodeLlama, GPT-3.5, and DeepSeek R1 perform significantly different on the effectiveness scores (Sec.7).Additionally, variations in API versions, underlying model updates, and differences in temperature settings further contribute to comparability issues in performance across studies.</p>
<p>RQ5:</p>
<p>To what extent are the presented LLM-based parsing methods accessible and reproducible?The accessibility and reproducibility of the presented LLM-based parsing methods are significantly hindered by issues related to code availability, documentation, and execution reliability (Sec.5.13.1).Many approaches do not provide public code, and even when code is available, it often lacks essential components, such as necessary scripts or instructions, making replication difficult.Several repositories suffer from missing files, incorrect implementations, or incomplete functionality that does not align with the descriptions in their respective papers.Furthermore, discrepancies in the code, such as missing scripts for crucial steps or incorrect default settings, complicate reproducibility.Even when modifications are made to ensure compatibility -such as fixing typos, updating libraries, or implementing minor adjustments to improve consistency (Sec.6.4.2) -these changes highlight the necessity of external intervention to make the methods functional.The need for such corrections aligns with broader findings from Trisovic et al. [64] and Olzewski et al. [51], who observed that a majority of replication code repositories contain errors that prevent immediate execution.Olzewski et al. further found that only a small proportion of the running codebases also produce the claimed results of the related papers [51].Additionally, some parsers require substantial configuration efforts, further limiting accessibility (Sec.5.5).Whilst minor improvements, such as adding caching mechanisms to DivLog [72] or modifying LogPrompt's [39] prompt size, enhance usability, they also introduce deviations from the original implementations, raising concerns about reproducibility.The discrepancy between our own evaluation results (Sec.7) and the reported results raises further concerns about the validity of the publications' evaluations but naturally also about our own evaluation.Future benchmarks are expected to address these concerns.Finally, while log parsers based on LLM show great potential, their implementations' current state significantly hinders accessibility and reliable reproduction without substantial external effort.</p>
<p>Threats to Validity</p>
<p>We identified two major threats to the validity of the results of our evaluation concerning the adoption of the code and randomness.8.2.1 Mistakes in Code Adoption.In Sec.5.13.1 we describe issues with the code of some of the parsers and in Sec.6.4.2 we describe the changes we made due to the mentioned issues and to ensure transparency.We found that a significant number of parsers do not attain the performance levels claimed in their respective papers.This raises the question whether the observed discrepancies can be attributed to our alterations in their code or the inaccuracy of their evaluation.Alternatively, it is possible that other factors, such as the utilization of different language models, are responsible for the observed variations.However, given that the alterations were predominantly minor, we anticipate that the correctness was not significantly negatively influenced.The code has been made available on GitHub for reproduction.Should any flaws in the adoption of the parsers be identified, we request to report an issue.8.2.2 Randomness.The validity of results is threatened by the randomness in LLM output and the randomness introduced by the random sampling for the supervised parsers.To mitigate the randomness in LLMs, we set the LLMs' initial temperature to 0. The temperature determines the creativity of an LLM.However, even a temperature of 0 does not guarantee a deterministic output [3].Therefore, we repeated the evaluation three times and computed the average of the measured values.</p>
<p>8.2.3 Data Leakage.Following previous work [24,70] we identify data leakage through memorized logs from the pretraining phase of the LLMs as a threat to the validity of our evaluation results.However, [24,70,72] report significant performance improvements with ICL compared to direct zero-shot parsing, suggesting a low likelihood of GPT-3.5-Turbomemorizing single log templates.In a small experiment we asked CodeLlama and DeepSeek R1 if they can output exact logs of the LogHub repository.In both cases they were not able to print exact logs, that matched any of those from the repository.The same held true when asked for templates.Diverse studies [41,56] suggest that direct fact memorization is unprobable and that the models are only likely to memorize often encountered text from diverse sources from their training [56].</p>
<p>Conclusion</p>
<p>In this work, we conducted a systematic review of LLM-based log parsing approaches, analyzed their strengths and weaknesses in comparison to non-LLM-based methods, and performed an extensive benchmark evaluation to assess their effectiveness, efficiency, and usability.Our findings demonstrate that LLM-based log parsers provide notable advantages, particularly in their adaptability to diverse log formats and their capability to generalize across unseen templates.However, they also come with challenges such as high computational costs, susceptibility to hallucinations, and issues with interpretability.</p>
<p>A key observation is that LLM-based log parsers often reduce the need for manual configuration and labeling, making them more accessible to users with limited domain expertise.Techniques such as in-context learning (ICL), fine-tuning, retrieval-augmented generation (RAG) and template revision can significantly enhance efficiency and accuracy of these methods.Sophisticated dynamic demonstration selection and caching strategies have been shown to significantly impact performance, reducing both LLM inference time and cost.However, despite these improvements, only two out of seven LLM-based parsers, namely LILAC [24] and LogBatcher [70], clearly outperform the non-LLM-based parsers that constitute our baseline.Furthermore, we identified a lack of reproducibility and comparability, as many implementations lack publicly available code or datasets, comprehensive documentation, or consistency with reported results.Our study highlights the necessity of standardized benchmarking practices for LLM-based log parsing.Differences in experimental setups, dataset preprocessing, and metric selection complicate direct comparisons between methods and impede a meaningful selection for user-specific downstream tasks.</p>
<p>Overall, LLM-based log parsing represents a promising direction for automated log analysis.Techniques like caching, RAG, template revision, especially when used in combination, have clearly proven their value in this field, but further research is required to fully realize the potential of LLMbased log parsing.We hope that our systematic review, benchmark study, and insights contribute to the development of more effective, transparent, and user-friendly log parsing solutions.To facilitate future research and reproducibility, we make our evaluation results and source code publicly available on our GitHub repository [4].</p>
<p>Fig. 2 .
2
Fig. 2. LLM-based parsing process pipeline.The dashed arrows and boxes represent optional components while the continuous ones represent essential components.The arrows pointing from the LLM to certain process elements describe where the LLM can be applied.</p>
<p>Fig. 3 .
3
Fig. 3. Performance of the baseline parsers on the corrected LogHub dataset including Audit.</p>
<p>Fig. 4 .
4
Fig. 4. Performance of the selected parsers on the corrected LogHub datasets including Audit</p>
<p>Fig. 5 .
5
Fig. 5. Averaged performance of LLM-based parsers for CodeLlama, GPT-3.5 and DeepSeek R1.</p>
<p>7.1.4Performance Difference on LogHub and corrected LogHub.</p>
<p>Fig. 6 .
6
Fig. 6.Performance of the parsers with GPT-3.5 on corrected LogHub minus the performance on the original LogHub.</p>
<p>Fig. 7 .
7
Fig. 7. Performance of the selected LLM-based parsers with GPT-3.5 and the baseline on the Audit data.</p>
<p>Table 2 .
2
Categorization of the selected papers by features.
SupervisionProcessing modeLearning / PromptingManual config.RAGCachingLLM usageTemplate corr.DatasetsMetricsModelsCode availabilityPreprintGeneral PropertiesProcessing StepsReproducibility</p>
<p>Table 3 .
3
Overview of LLMs used.
Base model / Model series CreatorAvailability Times usedGPTOpenAIAPI23LlamaMetaOpen-source8ClaudeAnthropicAPI5MistralMistral AIOpen-source5ChatGLMTsinghua University Open-source2GeminiGoogleAPI2GemmaGoogleOpen-source2QwenAlibaba CloudOpen-source2AquilaChatBAAIOpen-source1BaichuanBaichuan AIOpen-source1DeepSeekDeepSeek AIOpen-source1DevOps-ModelCodeFuseOpen-source1InternLMShanghai AI LabOpen-source1OpenChatOpenChat TeamOpen-source1OWLCamel AIOpen-source1VicunaLMSYSOpen-source1WizardLMMicrosoftOpen-source1</p>
<p>Table 4 .
4
Licenses of the approaches with open-source implementations.Most approaches do not provide a license.Please note, that especially for preprint papers the license might change or, if absent, be added in the future (status 7-March-2025).
ApproachLicensePreprint?Cui et al. [9] (LogEval)N/A✓Ji et al. [23] (SuperLog)Apache License Version 2.0, January 2004✓Jiang et al. [24] (LILAC)N/ALiu et al. [39] (LogPrompt)N/AMa et al. [42] (LLMParser)N/AMa et al. [43] (OpenLogParser)N/A✓Mehrabi et al. [46]N/APei et al. [53] (SelfLog)N/ASun et al. [61] (Semirald)N/A✓Vaarandi et al. [65] (LLM-TD)GNU General Public License version 2✓Xiao et al. [70] (LogBatcher)MIT License 2024Xu et al. [72] (DivLog)Apache License Version 2.0, January 2004Yu et al. [74] (LogGenius)N/AZhang et al. [78] (Lemur)Apache License Version 2.0, January 2004✓</p>
<p>Table 5 .
5
Performance of the selected parsers with GPT-3.5 on the corrected LogHub datasets including Audit.The best scores for each metric on each dataset are marked bold.
DatasetAndroidApacheBGLHDFSHPCHadoopHealthAppLinuxMacOpenSSHOpenStackProxifierSparkThunderbirdWindowsZookeeperAuditAverageBaselineAELGA0.77 1.00 0.96 1.00 0.90 0.87 0.57 0.40 0.76 0.54 0.25 0.97 0.90 0.94 0.69 0.92 0.21 0.74FTA0.46 0.50 0.22 0.53 0.46 0.24 0.10 0.27 0.17 0.23 0.08 0.44 0.28 0.20 0.27 0.45 0.00 0.29PA0.39 0.69 0.34 0.62 0.68 0.51 0.16 0.17 0.17 0.25 0.02 0.67 0.38 0.04 0.15 0.75 0.00 0.35NED0.90 0.92 0.84 0.93 0.97 0.70 0.56 0.74 0.71 0.92 0.60 0.97 0.71 0.71 0.90 0.92 0.49 0.79BrainGA0.86 1.00 0.99 1.00 0.94 0.95 1.00 0.35 0.94 1.00 0.49 0.53 1.00 0.97 1.00 0.99 0.21 0.84FTA0.37 0.67 0.24 0.87 0.46 0.20 0.40 0.40 0.33 0.23 0.21 0.74 0.44 0.34 0.45 0.57 0.00 0.41PA0.24 0.70 0.41 0.96 0.66 0.16 0.25 0.17 0.34 0.28 0.11 0.70 0.38 0.04 0.46 0.78 0.00 0.39NED0.84 0.92 0.84 1.00 0.88 0.64 0.88 0.82 0.86 0.95 0.96 0.97 0.96 0.82 0.92 0.94 0.41 0.86DrainGA0.83 1.00 0.96 1.00 0.89 0.95 0.78 0.42 0.79 0.79 0.22 0.76 0.92 0.96 1.00 0.97 0.21 0.79FTA0.54 0.50 0.22 0.60 0.39 0.30 0.11 0.40 0.20 0.44 0.02 0.41 0.40 0.25 0.41 0.52 0.00 0.34PA0.55 0.69 0.34 0.63 0.65 0.51 0.23 0.18 0.22 0.51 0.02 0.68 0.38 0.05 0.46 0.79 0.00 0.41NED0.90 0.92 0.84 0.93 0.96 0.79 0.61 0.78 0.76 0.97 0.51 0.96 0.96 0.81 0.86 0.94 0.43 0.82SPELLGA0.86 1.00 0.79 1.00 0.65 0.78 0.64 0.15 0.76 0.56 0.26 0.53 0.90 0.84 0.99 0.96 0.34 0.71FTA0.25 0.50 0.06 0.43 0.39 0.16 0.10 0.14 0.05 0.23 0.00 0.04 0.19 0.15 0.10 0.37 0.00 0.19PA0.15 0.69 0.20 0.30 0.53 0.11 0.15 0.09 0.03 0.19 0.00 0.48 0.32 0.03 0.00 0.75 0.00 0.24NED0.81 0.92 0.67 0.94 0.84 0.49 0.52 0.75 0.66 0.86 0.50 0.93 0.67 0.71 0.77 0.94 0.40 0.73ULPGA0.74 1.00 0.93 1.00 0.95 0.99 0.90 0.18 0.81 0.43 0.47 0.02 0.92 0.68 0.41 0.99 0.34 0.69FTA0.29 0.00 0.30 0.00 0.53 0.20 0.33 0.40 0.28 0.07 0.04 0.38 0.20 0.44 0.38 0.38 0.00 0.25PA0.23 0.00 0.38 0.00 0.85 0.16 0.07 0.20 0.24 0.15 0.02 0.51 0.02 0.07 0.39 0.50 0.00 0.22NED0.67 0.74 0.69 0.61 0.77 0.72 0.80 0.61 0.69 0.78 0.66 0.78 0.77 0.74 0.44 0.82 0.51 0.69UnsupervisedLLM_TDGA0.00 1.00 0.50 0.98 0.22 0.47 0.24 0.39 0.00 0.25 0.82 0.27 0.29 0.26 0.57 0.35 0.33 0.41FTA0.00 0.50 0.20 0.00 0.19 0.17 0.10 0.14 0.00 0.40 0.57 0.00 0.12 0.04 0.10 0.16 0.00 0.16PA0.00 0.69 0.45 0.00 0.20 0.12 0.24 0.28 0.00 0.36 0.25 0.00 0.23 0.10 0.01 0.11 0.00 0.18NED0.05 0.95 0.51 0.43 0.26 0.47 0.28 0.42 0.06 0.68 0.46 0.22 0.42 0.29 0.48 0.18 0.09 0.37LogBatcherGA0.97 1.00 0.99 1.00 0.95 0.99 1.00 0.84 0.92 1.00 0.98 1.00 1.00 0.90 1.00 0.99 0.27 0.93FTA0.78 1.00 0.83 1.00 0.75 0.74 0.95 0.75 0.50 0.82 0.79 1.00 0.76 0.67 0.62 0.81 0.13 0.76PA0.83 1.00 0.94 1.00 0.94 0.89 0.99 0.84 0.48 0.97 0.77 1.00 0.97 0.84 0.61 0.99 0.00 0.83NED0.92 1.00 0.99 1.00 1.00 0.97 1.00 0.94 0.86 0.99 0.94 1.00 0.98 0.96 0.85 1.00 0.77 0.95LogPromptGA0.13 0.19 0.14 0.00 0.17 0.07 0.06 0.12 0.20 0.00 0.01 0.00 0.01 0.07 0.12 0.45 0.00 0.10FTA0.12 0.00 0.03 0.00 0.07 0.08 0.06 0.15 0.09 0.00 0.00 0.00 0.03 0.18 0.05 0.07 0.00 0.06PA0.24 0.16 0.38 0.00 0.55 0.15 0.27 0.13 0.23 0.13 0.10 0.00 0.25 0.09 0.33 0.50 0.00 0.21NED0.78 0.85 0.83 0.49 0.85 0.70 0.71 0.61 0.77 0.73 0.43 0.54 0.82 0.69 0.78 0.86 0.54 0.70OpenLogParserGA0.85 1.00 0.96 0.85 0.93 0.96 0.82 0.42 0.81 0.86 0.57 0.76 0.87 0.94 0.99 0.96 0.14 0.81FTA0.32 0.50 0.42 0.00 0.59 0.28 0.46 0.45 0.27 0.65 0.05 0.00 0.41 0.36 0.15 0.41 0.00 0.31PA0.24 0.69 0.72 0.00 0.89 0.09 0.66 0.24 0.26 0.69 0.06 0.00 0.36 0.06 0.01 0.48 0.00 0.32NED0.17 0.46 0.37 0.27 0.30 0.21 0.26 0.27 0.16 0.39 0.24 0.37 0.21 0.19 0.44 0.27 0.48 0.30SelfLogGA0.89 1.00 0.97 1.00 0.89 0.99 0.92 0.29 0.79 0.44 0.44 0.53 0.94 0.70 0.72 0.99 0.14 0.74FTA0.39 0.33 0.29 0.00 0.16 0.27 0.48 0.31 0.24 0.17 0.54 0.00 0.38 0.35 0.31 0.24 0.00 0.26PA0.46 0.28 0.66 0.00 0.22 0.14 0.55 0.09 0.29 0.15 0.31 0.12 0.76 0.08 0.59 0.24 0.00 0.29NED0.76 0.85 0.94 0.86 0.65 0.82 0.89 0.79 0.80 0.85 0.73 0.79 0.93 0.82 0.81 0.80 0.51 0.80SupervisedDivLog-2GA0.56 0.91 0.60 0.49 0.52 0.84 0.68 0.16 0.38 0.47 0.30 0.72 0.46 0.36 0.52 0.93 0.42 0.55FTA0.39 0.48 0.32 0.06 0.23 0.31 0.57 0.32 0.19 0.33 0.11 0.39 0.23 0.31 0.19 0.59 0.48 0.32PA0.52 0.70 0.72 0.57 0.77 0.32 0.76 0.33 0.26 0.64 0.24 0.76 0.67 0.35 0.31 0.80 0.49 0.54NED0.78 0.92 0.93 0.76 0.90 0.80 0.92 0.79 0.52 0.95 0.52 0.95 0.87 0.82 0.70 0.92 0.94 0.82DivLog-4GA0.67 0.53 0.55 0.65 0.36 0.70 0.86 0.20 0.59 0.59 0.27 0.32 0.67 0.40 0.52 0.87 0.36 0.53FTA0.42 0.43 0.22 0.25 0.18 0.25 0.56 0.34 0.30 0.55 0.10 0.29 0.48 0.30 0.22 0.53 0.45 0.34PA0.50 0.70 0.67 0.71 0.53 0.32 0.76 0.38 0.36 0.65 0.25 0.85 0.77 0.27 0.63 0.79 0.56 0.57NED0.78 0.91 0.93 0.87 0.63 0.69 0.91 0.81 0.78 0.95 0.48 0.91 0.93 0.64 0.80 0.93 0.77 0.81LILAC-4GA0.95 1.00 0.98 0.94 0.95 0.97 0.91 0.30 0.81 0.75 0.49 0.95 0.97 0.94 0.79 0.99 1.00 0.86FTA0.66 1.00 0.89 0.46 0.75 0.59 0.80 0.70 0.47 0.84 0.83 0.92 0.67 0.56 0.57 0.69 0.88 0.72PA0.57 1.00 0.96 0.53 0.92 0.83 0.89 0.42 0.49 0.78 0.43 0.99 0.93 0.73 0.70 0.64 0.78 0.74NED0.85 1.00 0.99 0.82 0.99 0.95 0.96 0.92 0.86 0.97 0.90 1.00 0.97 0.93 0.85 0.82 0.99 0.93LILAC-2GA0.96 1.00 0.96 0.94 0.88 0.89 0.91 0.29 0.79 0.63 0.49 0.85 0.92 0.96 0.69 0.99 1.00 0.83FTA0.70 1.00 0.80 0.50 0.70 0.57 0.80 0.61 0.48 0.75 0.74 0.77 0.66 0.61 0.57 0.70 0.67 0.68PA0.57 1.00 0.94 0.48 0.87 0.70 0.89 0.37 0.48 0.64 0.37 0.92 0.88 0.78 0.52 0.66 0.49 0.68NED0.87 1.00 0.98 0.91 0.98 0.90 0.96 0.87 0.87 0.94 0.90 0.99 0.99 0.95 0.77 0.87 0.98 0.93</p>
<p>Computation and invocation time of parsers on HDFS and BGL with GPT-3.5.Number of LLM calls of parsers on HDFS and BGL with GPT-3.5.
0 1000 2000 3000 4000 5000 6000 Time (seconds) 5207+59 1328 1222 1142 1058 578+30 573+34 289+38 274+35 78 HDFS Computation Time Invocation Time 0 20 40 60 80 Number of LLM calls 73.0 49.3 42.0 41.3 21.0 HDFS Fig. 8. X:28 OpenLogParser SPELL Drain AEL Brain LILAC-2 LILAC-4 LogBatcher LLM_TD ULP OpenLogParser LogBatcher LILAC-4 LILAC-2 LLM_TD Fig. 9.AEL OpenLogParser LILAC-2 SPELL LLM_TD LILAC-4 Drain Brain LogBatcher ULP LLM_TD OpenLogParser LogBatcher LILAC-4 LILAC-20 47 01000 806+249 Time (seconds) 2000 1931+249 3000 2318+441 3360 1846 1610+1360 63+284 450 410 BGL Computation Time Invocation Time Beck et al. 500 1000 1500 Number of LLM calls 1534.0 416.0 380.0 326.3 325.0 BGL
https://openai.com/index/chatgpt/; accessed 10-March-2025
https://github.com/logpai/logparser; accessed 9-December-2024
AcknowledgmentsFunded by the European Union under the European Defence Fund (GA no.101121403 -NEWSROOM) and under the Horizon Europe Research and Innovation programme (GA no.101168144 -MIRANDA).Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Commission.Neither the European Union nor the granting authority can be held responsible for them.This work is co-funded by the Austrian FFG Kiras project ASOC (GA no.FO999905301).
LLM-based event log analysis techniques: A survey. Siraaj Akhtar, Saad Khan, Simon Parkinson, 10.48550/arXiv.2502.00677arXiv:2502.006772025</p>
<p>A Comparative Study on Large Language Models for Log Parsing. Merve Astekin, Max Hort, Leon Moonen, 10.1145/3674805.3686684Proceedings of the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM '24). the 18th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM '24)New York, NY, USAAssociation for Computing Machinery2024</p>
<p>An Exploratory Study on How Non-Determinism in Large Language Models Affects Log Parsing. Merve Astekin, Max Hort, Leon Moonen, 10.1145/3643661.3643952Proceedings of the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering (InteNSE '24). the ACM/IEEE 2nd International Workshop on Interpretability, Robustness, and Benchmarking in Neural Software Engineering (InteNSE '24)New York, NY, USAAssociation for Computing Machinery2024</p>
<p>GitHub repository for this paper. Viktor Beck, 2025</p>
<p>Semi-supervised Configuration and Optimization of Anomaly Detection Algorithms on Log Data. Viktor Beck, Max Landauer, Markus Wurzenberger, Florian Skopik, Andreas Rauber, 10.1109/BigData62323.2024.108252022024 IEEE International Conference on Big Data (BigData). 2024</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, 10.48550/arXiv.2005.14165arXiv:2005.14165Language Models are Few-Shot Learners. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020Ilya Sutskever, and Dario Amodei</p>
<p>An Experience Report of Generating Load Tests Using Log-Recovered Workloads at Varying Granularities of User Behaviour. Jinfu Chen, Weiyi Shang, Ahmed E Hassan, Yong Wang, Jiangbin Lin, 10.1109/ASE.2019.0006834th IEEE/ACM International Conference on Automated Software Engineering (ASE). 2019. 2019</p>
<p>The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks. Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis , Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, Joseph E Gonzalez, 10.48550/arXiv.2502.08235arXiv:2502.082352025</p>
<p>Tianyu Cui, Shiyu Ma, Ziang Chen, Tong Xiao, Shimin Tao, Yilun Liu, Shenglin Zhang, Duoming Lin, Changchang Liu, Yuzhe Cai, Weibin Meng, Yongqian Sun, Dan Pei, arXiv:2407.01896LogEval: A Comprehensive Benchmark Suite for Large Language Models In Log Analysis. 2024</p>
<p>Logram: Efficient Log Parsing Using nn-Gram Dictionaries. Hetong Dai, Heng Li, Che-Shao Chen, Weiyi Shang, Tse-Hsun Chen, 10.1109/TSE.2020.3007554Conference Name: IEEE Transactions on Software Engineering. 2022. March 202248</p>
<p>PILAR: Studying and Mitigating the Influence of Configurations on Log Parsing. Hetong Dai, Yiming Tang, Heng Li, Weiyi Shang, 10.1109/ICSE48619.2023.000772023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). 2023</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Spell: Streaming Parsing of System Event Logs. Min Du, Feifei Li, 10.1109/ICDM.2016.01032016 IEEE 16th International Conference on Data Mining (ICDM). 2016</p>
<p>DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning. Min Du, Feifei Li, Guineng Zheng, Vivek Srikumar, 10.1145/3133956.3134015Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS '17). the 2017 ACM SIGSAC Conference on Computer and Communications Security (CCS '17)New York, NY, USAAssociation for Computing Machinery2017</p>
<p>Log Anomaly Detection by Leveraging LLM-Based Parsing and Embedding with Attention Mechanism. Asma Fariha, Vida Gharavian, Masoud Makrehchi, Shahryar Rahnamayan, 10.1109/CCECE59415.2024.10667308Sanaa Alwidian, and Akramul Azim. 2024. 2024</p>
<p>Investigating and improving log parsing in practice. Ying Fu, Meng Yan, Jian Xu, Jianguo Li, Zhongxin Liu, Xiaohong Zhang, Dan Yang, 10.1145/3540250.3558947Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022). the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2022)New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Making Pre-trained Language Models Better Few-shot Learners. Tianyu Gao, Adam Fisch, Danqi Chen, 10.18653/v1/2021.acl-long.295Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. Chengqing Zong, Fei Xia, Wenjie Li, Roberto Navigli, the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline20211</p>
<p>An Evaluation Study on Log Parsing and Its Use in Log Mining. Pinjia He, Jieming Zhu, Shilin He, Jian Li, Michael R Lyu, 10.1109/DSN.2016.662016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN). 2016</p>
<p>Drain: An Online Log Parsing Approach with Fixed Depth Tree. Pinjia He, Jieming Zhu, Zibin Zheng, Michael R Lyu, 10.1109/ICWS.2017.132017 IEEE International Conference on Web Services (ICWS). 2017</p>
<p>. Shilin He, Pinjia He, Zhuangbin Chen, Tianyi Yang, Yuxin Su, Michael R Lyu, 10.1145/3460345130:1-130:37A Survey on Automated Log Analysis for Reliability Engineering. ACM Comput. Surv. 5462021. July 2021</p>
<p>Experience Report: System Log Analysis for Anomaly Detection. Shilin He, Jieming Zhu, Pinjia He, Michael R Lyu, 10.1109/ISSRE.2016.212016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE). 2016</p>
<p>Junjie Huang, Zhihan Jiang, Zhuangbin Chen, Michael R Lyu, arXiv:2406.07174LUNAR: Unsupervised LLM-based Log Parsing. 2024</p>
<p>Yuhe Ji, Yilun Liu, Feiyu Yao, Minggui He, Shimin Tao, Xiaofeng Zhao, Su Chang, Xinhua Yang, Weibin Meng, Yuming Xie, Boxing Chen, Hao Yang, 10.48550/arXiv.2412.01377arXiv:2412.01377Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge. 2024</p>
<p>LILAC: Log Parsing using LLMs with Adaptive Parsing Cache. Zhihan Jiang, Jinyang Liu, Zhuangbin Chen, Yichen Li, Junjie Huang, Yintong Huo, Pinjia He, Jiazhen Gu, Michael R Lyu, 10.1145/3643733Proc. ACM Softw. Eng. 11602024. July 2024FSE</p>
<p>A Large-Scale Evaluation for Log Parsing Techniques: How Far Are We?. Zhihan Jiang, Jinyang Liu, Junjie Huang, Yichen Li, Yintong Huo, Jiazhen Gu, Zhuangbin Chen, Jieming Zhu, Michael R Lyu, 10.1145/3650212.3652123Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2024). the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2024)New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Abstracting Execution Logs to Execution Events for Enterprise Applications (Short Paper). Ming Zhen, Ahmed E Jiang, Parminder Hassan, Gilbert Flora, Hamann, 10.1109/QSIC.2008.50The Eighth International Conference on Quality Software. 2008. 2008</p>
<p>Rabimba Karanjai, Yang Lu, Dana Alsagheer, Keshav Kasichainula, Lei Xu, Weidong Shi, Shou-Hsuan Stephen Huang, 10.1145/3672608.3707883arXiv:2412.12364LogBabylon: A Unified Framework for Cross-Log File Integration and Analysis. 2024</p>
<p>Guidelines for assessing the accuracy of log message template identification techniques. Zanis Ali Khan, Donghwan Shin, Domenico Bianculli, Lionel Briand, 10.1145/3510003.3510101Proceedings of the 44th International Conference on Software Engineering (ICSE '22). the 44th International Conference on Software Engineering (ICSE '22)New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Determinantal Point Processes for Machine Learning. Alex Kulesza, Ben Taskar, 10.1561/2200000044Foundations and Trends® in Machine Learning. 52012. Dec. 2012Now Publishers, Inc</p>
<p>Deep learning for anomaly detection in log data: A survey. Max Landauer, Sebastian Onder, Florian Skopik, Markus Wurzenberger, 10.1016/j.mlwa.2023.100470Machine Learning with Applications. 121004702023. June 2023</p>
<p>Maintainable Log Datasets for Evaluation of Intrusion Detection Systems. Max Landauer, Florian Skopik, Maximilian Frank, Wolfgang Hotwagner, Markus Wurzenberger, Andreas Rauber, 10.1109/TDSC.2022.3201582Conference Name: IEEE X:34 Beck et al. Transactions on Dependable and Secure Computing. 2023. July 202320</p>
<p>Have it Your Way: Generating Customized Log Datasets With a Model-Driven Simulation Testbed. Max Landauer, Florian Skopik, Markus Wurzenberger, Wolfgang Hotwagner, Andreas Rauber, 10.1109/TR.2020.3031317Conference Name: IEEE Transactions on Reliability. 2021. March 202170</p>
<p>Log Parsing: How Far Can ChatGPT Go?. Van-Hoang Le, Hongyu Zhang, 10.1109/ASE56229.2023.002062023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE). 2023</p>
<p>Log Parsing with Prompt-Based Few-Shot Learning. Van-Hoang Le, Hongyu Zhang, 10.1109/ICSE48619.2023.00204Proceedings of the 45th International Conference on Software Engineering (ICSE '23). the 45th International Conference on Software Engineering (ICSE '23)Melbourne, Victoria, AustraliaIEEE Press2023</p>
<p>Retrieval-augmented generation for knowledge-intensive NLP tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, Proceedings of the 34th International Conference on Neural Information Processing Systems (NIPS '20). the 34th International Conference on Neural Information Processing Systems (NIPS '20)Red Hook, NY, USACurran Associates Inc2020</p>
<p>LogShrink: Effective Log Compression by Leveraging Commonality and Variability of Log Data. Xiaoyun Li, Hongyu Zhang, Van-Hoang Le, Pengfei Chen, 10.1145/3597503.3608129Proceedings of the IEEE/ACM 46th International Conference on Software Engineering (ICSE '24). the IEEE/ACM 46th International Conference on Software Engineering (ICSE '24)New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Revisiting Log Parsing: The Present, the Future, and the Uncertainties. Zhijing Li, Qiuai Fu, Zhijun Huang, Jianbo Yu, Yiqian Li, Yuanhao Lai, Yuchi Ma, 10.1109/TR.2023.3340020IEEE Transactions on Reliability. 732024. Sept. 2024</p>
<p>Did We Miss Something Important? Studying and Exploring Variable-Aware Log Abstraction. Zhenhao Li, Chuan Luo, Weiyi Tse-Hsun Chen, Shilin Shang, Qingwei He, Dongmei Lin, Zhang, 10.1109/ICSE48619.2023.000782023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). 2023</p>
<p>Interpretable Online Log Analysis Using Large Language Models with Prompt Strategies. Yilun Liu, Shimin Tao, Weibin Meng, Jingyu Wang, Wenbing Ma, Yuhang Chen, Yanqing Zhao, Hao Yang, Yanfei Jiang, 2024 IEEE/ACM 32nd International Conference on Program Comprehension (ICPC). 2024</p>
<p>UniParser: A Unified Log Parser for Heterogeneous Log Data. Yudong Liu, Xu Zhang, Shilin He, Hongyu Zhang, Liqun Li, Yu Kang, Yong Xu, Minghua Ma, Qingwei Lin, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang, 10.1145/3485447.3511993Proceedings of the ACM Web Conference 2022 (WWW '22). the ACM Web Conference 2022 (WWW '22)New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Xuanjing Huang, and Xipeng Qiu. 2024. Scaling Laws for Fact Memorization of Large Language Models. Xingyu Lu, Xiaonan Li, Qinyuan Cheng, Kai Ding, 10.48550/arXiv.2406.15720arXiv:2406.15720</p>
<p>LLMParser: An Exploratory Study on Using Large Language Models for Log Parsing. Zeyang Ma, An Ran Chen, Dong Jae Kim, Tse-Hsun Peter, Chen , Shaowei Wang, 10.1145/3597503.36391502024 IEEE/ACM 46th International Conference on Software Engineering (ICSE). 2024</p>
<p>LibreLog: Accurate and Efficient Unsupervised Log Parsing Using Open-Source Large Language Models. Zeyang Ma, Dong Jae Kim, Tse-Hsun Chen, 10.48550/arXiv.2408.01585arXiv:2408.015852024</p>
<p>Clustering event logs using iterative partitioning. A O Adetokunbo, A Makanju, Evangelos E Nur Zincir-Heywood, Milios, 10.1145/1557019.1557154Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '09). the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD '09)New York, NY, USAAssociation for Computing Machinery2009</p>
<p>Computation of normalized edit distance and applications. A Marzal, E Vidal, 10.1109/34.232078Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence. 1993. Sept. 199315</p>
<p>The Effectiveness of Compact Fine-Tuned LLMs in Log Parsing. Maryam Mehrabi, Abdelwahab Hamou-Lhadj, Hossein Moosavi, 2024 IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE2024</p>
<p>Toward Fine-Grained, Unsupervised, Scalable Performance Diagnosis for Production Cloud Computing Systems. Haibo Mi, Huaimin Wang, Yangfan Zhou, Michael Rung-Tsong Lyu, Hua Cai, 10.1109/TPDS.2013.21Conference Name: IEEE Transactions on Parallel and Distributed Systems. 2013. June 201324</p>
<p>Few-shot Fine-tuning vs. Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, Yanai Elazar, 10.48550/arXiv.2305.16938arXiv:2305.16938-context Learning: A Fair Comparison and Evaluation. 2023</p>
<p>Self-supervised Log Parsing. Sasho Nedelkoski, Jasmin Bogatinovski, Alexander Acker, Jorge Cardoso, Odej Kao, 10.1007/978-3-030-67667-4_8Machine Learning and Knowledge Discovery in Databases: Applied Data Science Track. Dunja Dong, Craig Mladenić, Saunders, ChamSpringer International Publishing2021</p>
<p>LogRule: Efficient Structured Log Mining for Root Cause Analysis. Paolo Notaro, Soroush Haeri, Jorge Cardoso, Michael Gerndt, 10.1109/TNSM.2023.3282270Conference Name: IEEE Transactions on Network and Service Management. 2023. Dec. 202320</p>
<p>Get in Researchers; We're Measuring Reproducibility": A Reproducibility Study of Machine Learning Papers in Tier 1 Security Conferences. Daniel Olszewski, Allison Lu, Carson Stillman, Kevin Warren, Cole Kitroser, Alejandro Pascual, Divyajyoti Ukirde, Kevin Butler, Patrick Traynor, 10.1145/3576915.3623130Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security (CCS '23). the 2023 ACM SIGSAC Conference on Computer and Communications Security (CCS '23)New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Large language model-based optical network log analysis using LLaMA2 with instruction tuning. Yue Pang, Min Zhang, Yanli Liu, Xiangbin Li, Yidi Wang, Yahang Huan, Zhuo Liu, Jin Li, Danshi Wang, 10.1364/JOCN.527874Journal of Optical Communications and Networking. 16112024. Nov. 2024Journal of Optical Communications and Networking</p>
<p>Self-Evolutionary Group-wise Log Parsing Based on Large Language Model. Changhua Pei, Zihan Liu, Jianhui Li, Erhan Zhang, Le Zhang, Haiming Zhang, Wei Chen, Dan Pei, Gaogang Xie, 2024 IEEE 35th International Symposium on Software Reliability Engineering (ISSRE). IEEE2024</p>
<p>Log Parsing Evaluation in the Era of Modern Software Systems. Stefan Petrescu, Floris Den Hengst, Alexandru Uta, Jan S Rellermeyer, 10.1109/ISSRE59848.2023.000192023 IEEE 34th International Symposium on Software Reliability Engineering. 2023</p>
<p>Objective Criteria for the Evaluation of Clustering Methods. William M Rand, 10.1080/01621459.1971.10482356J. Amer. Statist. Assoc. 661971. Dec. 1971</p>
<p>Rethinking LLM Memorization through the Lens of Adversarial Compression. Avi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C Lipton, J Z Kolter, Advances in Neural Information Processing Systems. 372024. Dec. 2024</p>
<p>An Effective Approach for Parsing Large Log Files. Issam Sedki, Abdelwahab Hamou-Lhadj, Otmane Ait-Mohamed, Mohammed A Shehab, 10.1109/ICSME55016.2022.000092022 IEEE International Conference on Software Maintenance and Evolution (ICSME). 2022</p>
<p>Keiichi Shima, 10.48550/arXiv.1611.03213arXiv:1611.03213Length Matters: Clustering System Log Messages using Length of Words. 2016</p>
<p>CodeFusion: A Pre-trained Diffusion Model for Code Generation. Mukul Singh, José Cambronero, Sumit Gulwani, Carina Vu Le, Gust Negreanu, Verbruggen, 10.18653/v1/2023.emnlp-main.716Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Design and Development of a Log Management System Based on Cloud Native Architecture. Yuchen Sun, Yanpiao Chen, Haotian Zhao, Shan Peng, 10.1109/ICSAI61474.2023.104233282023 9th International Conference on Systems and Informatics (ICSAI). 2023</p>
<p>Semirald: A Semi-Supervised Hybrid Language Model for Robust Anomalous Log Detection. Yicheng Sun, Jacky Keung, Zhen Yang, Shuo Liu, Hi Kuen, Yu , 2024</p>
<p>Advancing Semi-Supervised Anomaly Detection in Software Logs with Deep Grouping and Auto-Optimization. Yicheng Sun, Jacky Keung, Zhen Yang, Shuo Liu, Jingyu Zhang, 10.2139/ssrn.49182032024</p>
<p>LogStamp: Automatic Online Log Parsing Based on Sequence Labelling. SIGMETRICS Perform. Shimin Tao, Weibin Meng, Yimeng Cheng, Yichen Zhu, Ying Liu, Chunning Du, Tao Han, Yongpeng Zhao, Xiangguang Wang, Hao Yang, 10.1145/3543146.3543168Eval. Rev. 4942022. June 2022</p>
<p>A large-scale study on research code quality and execution. Ana Trisovic, Matthew K Lau, Thomas Pasquier, Mercè Crosas, 10.1038/s41597-022-01143-6Scientific Data. 9602022. Feb. 2022Nature Publishing Group</p>
<p>Using Large Language Models for Template Detection from Security Event Logs. Risto Vaarandi, Hayretdin Bahsi, arXiv:2409.050452024</p>
<p>Would you like a quick peek? providing logging support to monitor data processing in big data applications. Zehao Wang, Haoxiang Zhang, Shaowei Tse-Hsun (peter) Chen, Wang, 10.1145/3468264.3468613Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering X:36 Beck et al. (ESEC/FSE 2021). the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering X:36 Beck et al. (ESEC/FSE 2021)New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, Advances in Neural Information Processing Systems. 2022. Dec. 202235</p>
<p>Log Parsing with Self-Generated In-Context Learning and Self-Correction. Yifan Wu, Siyu Yu, Ying Li, arXiv:2406.033762024</p>
<p>AECID-PG: A Tree-Based Log Parser Generator To Enable Log Analysis. Markus Wurzenberger, Max Landauer, Florian Skopik, Wolfgang Kastner, 2019 IFIP/IEEE Symposium on Integrated Network and Service Management (IM). 2019</p>
<p>Demonstration-Free: Towards More Practical Log Parsing with Large Language Models. Yi Xiao, Hongyu Van-Hoang Le, Zhang, 10.1145/3691620.3694994Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering (ASE '24). the 39th IEEE/ACM International Conference on Automated Software Engineering (ASE '24)New York, NY, USAAssociation for Computing Machinery2024</p>
<p>HELP: Hierarchical Embeddings-based Log Parsing. Andy Xu, Arno Gau, arXiv:2408.083002024</p>
<p>DivLog: Log Parsing with Prompt Enhanced In-Context Learning. Junjielong Xu, Ruichun Yang, Yintong Huo, Chengyu Zhang, Pinjia He, 10.1145/3597503.3639155Proceedings of the IEEE/ACM 46th International Conference on Software Engineering (ICSE '24). the IEEE/ACM 46th International Conference on Software Engineering (ICSE '24)New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Brain: Log Parsing With Bidirectional Parallel Tree. Siyu Yu, Pinjia He, Ningjiang Chen, Yifan Wu, 10.1109/TSC.2023.3270566Conference Name: IEEE Transactions on Services Computing. 2023. Sept. 202316</p>
<p>LogGenius: An Unsupervised Log Parsing Framework with Zero-shot Prompt Engineering. Xian Yu, Shengxi Nong, Dongbiao He, Weijie Zheng, Teng Ma, Ning Liu, Jianhui Li, Gaogang Xie, 10.1109/ICWS62655.2024.001592024 IEEE International Conference on Web Services (ICWS). 2024</p>
<p>Instruction Tuning for Large Language Models: A Survey. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, Guoyin Wang, 10.48550/arXiv.2308.10792arXiv:2308.107922024</p>
<p>System Log Parsing: A Survey. Tianzhu Zhang, Han Qiu, Gabriele Castellano, Myriana Rifai, Chung Shue Chen, Fabio Pianese, 10.1109/TKDE.2022.3222417Conference Name: IEEE Transactions on Knowledge and Data Engineering. 2023. Aug. 202335</p>
<p>Wei Zhang, Xianfu Cheng, Yi Zhang, Jian Yang, Hongcheng Guo, Zhoujun Li, Xiaolin Yin, Xiangyuan Guan, Xu Shi, Liangfan Zheng, Bo Zhang, arXiv:2405.13548ECLIPSE: Semantic Entropy-LCS for Cross-Lingual Industrial Log Parsing. 2024</p>
<p>Wei Zhang, Hongcheng Guo, Anjie Le, Jian Yang, Jiaheng Liu, Zhoujun Li, 10.48550/arXiv.2402.18205arXiv:2402.18205Lemur: Log Parsing with Entropy Sampling and Chain-of-Thought Merging. 2025</p>
<p>Calibrate Before Use: Improving Few-shot Performance of Language Models. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningPMLR2021</p>
<p>LLM-powered Zero-shot Online Log Parsing. Chen Zhi, Liye Cheng, Meilin Liu, Xinkui Zhao, Yueshen Xu, Shuiguang Deng, 10.1109/ICWS62655.2024.001062024 IEEE International Conference on Web Services (ICWS). 2024</p>
<p>LogParser-LLM: Advancing Efficient Log Parsing with Large Language Models. Aoxiao Zhong, Dengyao Mo, Guiyang Liu, Jinbu Liu, Qingda Lu, Qi Zhou, Jiesheng Wu, Quanzheng Li, Qingsong Wen, 10.1145/3637528.3671810Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24). the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD '24)New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Leveraging Large Language Models and BERT for Log Parsing and Anomaly Detection. Yihan Zhou, Yan Chen, Xuanming Rao, Yukang Zhou, Yuxin Li, Chao Hu, 10.3390/math12172758ID: WOS:001310957300001MATHEMATICS. 1227582024. Sept. 2024MDPI Web of Science</p>
<p>Loghub: A Large Collection of System Log Datasets for AI-driven Log Analytics. Jieming Zhu, Shilin He, Pinjia He, Jinyang Liu, Michael R Lyu, 10.1109/ISSRE59848.2023.000712023 IEEE 34th International Symposium on Software Reliability Engineering (ISSRE). 2023</p>
<p>Tools and Benchmarks for Automated Log Parsing. Jieming Zhu, Shilin He, Jinyang Liu, Pinjia He, Qi Xie, Zibin Zheng, Michael R Lyu, 10.1109/ICSE-SEIP.2019.000212019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). 2019</p>            </div>
        </div>

    </div>
</body>
</html>
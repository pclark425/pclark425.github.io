<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9657 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9657</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9657</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-279402998</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.12689v2.pdf" target="_blank">SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation</a></p>
                <p><strong>Paper Abstract:</strong> The rapid growth of scientific literature demands robust tools for automated survey-generation. However, current large language model (LLM)-based methods often lack in-depth analysis, structural coherence, and reliable citations. To address these limitations, we introduce SciSage, a multi-agent framework employing a reflect-when-you-write paradigm. SciSage features a hierarchical Reflector agent that critically evaluates drafts at outline, section, and document levels, collaborating with specialized agents for query interpretation, content retrieval, and refinement. We also release SurveyScope, a rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11 computer science domains, with strict recency and citation-based quality controls. Evaluations demonstrate that SciSage outperforms state-of-the-art baselines (LLM x MapReduce-V2, AutoSurvey), achieving +1.73 points in document coherence and +32% in citation F1 scores. Human evaluations reveal mixed outcomes (3 wins vs. 7 losses against human-written surveys), but highlight SciSage's strengths in topical breadth and retrieval efficiency. Overall, SciSage offers a promising foundation for research-assistive writing tools.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9657.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9657.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciSage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SCISAGE: A Multi-Agent Framework for High-Quality Scientific Survey Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent, LLM-driven pipeline that synthesizes citation-rich scientific surveys via modular agents (Interpreter, Organizer, Collector, Composer, Refiner) and a hierarchical Reflector that iteratively critiques and refines drafts at outline, section, and document levels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SciSage multi-agent system (experiments run with QWEN3-32B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>System-level multi-agent architecture coordinating specialized LLM agents: Interpreter (query rewrite/intent), Organizer (multi-LLM ensemble for outlines), Collector (multi-source retrieval + reranking), Composer (bottom-up synthesis of sections with citation grounding and figure/table extraction), Refiner (final polishing), and Reflector (hierarchical iterative critique employing a panel of LLM expert-personas). Retrieval-augmented generation (RAG) with multi-source APIs (arXiv, PubMed, Google Scholar) and multi-LLM ensemble for outline diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B (QWEN3-32B used for experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Retrieval from multiple scholarly sources guided per-section (e.g., arXiv, PubMed, Google Scholar); SurveyScope benchmark (46 high-impact papers, 2020–2025) used for evaluation. Collector reranks by recency, venue prestige, author influence, and citations.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>User-provided research query (possibly a paper title); the Interpreter rewrites/normalizes queries to guide outline generation and per-section search queries.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-augmented multi-agent synthesis with iterative hierarchical reflection: Organizer produces an outline via multi-LLM ensemble; Collector retrieves and reranks relevant papers; Composer synthesizes atomic, citation-rich subsection texts and extracts figures/tables; Reflector iteratively critiques outlines, sections, and full drafts, triggering additional retrievals and regenerations until quality thresholds or max trials are reached; Refiner enforces citation alignment, style, deduplication and outputs LaTeX/Markdown.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured, citation-rich scientific survey (sectioned document) with optional mindmaps and exported LaTeX/Markdown</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td>A full-document draft F_final containing introduction, thematic chapters with citation-supported paragraphs, extracted figures/tables integrated into sections, and concluding future-directions — produced iteratively via generate-reflect-regenerate cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic LLM-based metrics across Content Quality, Structural Coherence, and Reference Accuracy (TP/F1 vs. human-cited refs) plus human evaluations by CS graduate-level experts on 10 sampled topics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Automatic: document-level coherence 80.37, critical thinking 77.58, language fluency 85.65, citation F1 = 0.46 (correctly matching 1,510 references out of 3,844 in human-written papers). Compared to baselines (LLM×MapReduce-V2, AutoSurvey, OpenScholar), SciSage achieved best document coherence and much higher citation F1. Human eval: mixed — SciSage won 3 vs. lost 7 against human-written surveys (~30% win rate); strengths in topical breadth and retrieval efficiency, weaknesses in depth and mathematical precision.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Improved document-level coherence and substantially higher citation alignment (F1). Scalable, modular retrieval and synthesis across multi-source corpora. Reflector-enabled iterative critique yields measurable gains in language, critical scores, and structure. Generates mindmaps and exports for publication formats.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Limited evaluation to English; dependence on a single foundation model in experiments (QWEN3-32B); reproducibility and performance may vary with other LLMs; lacks rigorous mathematical/formula precision and stylistic nuance for some technical fields; metrics saturation reduces discriminative evaluation power.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Human evaluations show failures on deep analytical tasks, precise mathematical expression, and stylistic concision; produced longer, sometimes vague phrasing; lacked integrated detailed formulas/charts in fields needing rigorous mathematical exposition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9657.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9657.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QWEN3-32B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QWEN3-32B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The foundation model used to implement and evaluate SciSage and all baseline systems in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QWEN3-32B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 32-billion-parameter foundation language model referenced in the paper and used as the LLM backbone for all methods and baselines in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Varies by system — in experiments, each method used the title of a benchmark paper as input seed and relied on system-specific retrieval (online/offline) to fetch documents.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Each benchmark paper title served as the input query/seed for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Serves as the underlying LLM in retrieval-augmented generation pipelines, multi-agent prompting, outline-to-content synthesis, and evaluation prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated survey drafts and evaluated outputs for automatic metrics and human studies.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic LLM-based scoring pipelines and human evaluation as described for SciSage.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>All numerical automatic evaluation results reported in the paper (SciSage and baselines) were obtained using QWEN3-32B; SciSage results (reported above) used this model.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Used uniformly across methods for controlled comparison; enabled direct baseline comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Paper notes model-dependence: results may vary with other foundation models; no cross-model evaluation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not individually analyzed beyond the system-level failure modes reported for SciSage and baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9657.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9657.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior two-stage LLM-based pipeline for automated survey generation focusing on outline generation followed by subsection synthesis using an offline corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autosurvey: Large language models can automatically write surveys</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>AutoSurvey (run with QWEN3-32B in this paper's experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage pipeline: retrieval from an offline corpus, outline drafting, subsection generation, and evaluation. Does not support online retrieval in its original form.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B (QWEN3-32B used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Offline corpus provided by AutoSurvey (used for both retrieval and summarization in experiments); specific corpus size not stated.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Paper title used as the input seed for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieval-augmented generation with a two-stage outline-then-content pipeline using an offline dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Survey drafts (human-level surveys claimed by the original AutoSurvey work).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Compared via the paper's automatic LLM-based metrics (content quality, structure, reference accuracy) and human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Reported lower document coherence and citation F1 than SciSage (automatic metrics table lists AutoSurvey with moderate content/structure scores and lower citation alignment; specific numbers shown in the paper's Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Established two-stage pipeline that influenced later systems; effective for offline-corpus settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lacks online retrieval capability; in this paper, achieved lower citation recovery and structural scores compared to SciSage.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Limited retrieval scope (offline-only) leading to poorer citation alignment with human-written surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9657.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9657.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM×MapReduce-V2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM × MapReduce-V2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system for assembling coherent long-form drafts from extremely long resources using entropy-driven convolutional scaling (convolutional test-time scaling).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM × MapReduce-V2</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM×MapReduce-V2 (evaluated with QWEN3-32B in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An approach that applies entropy-driven convolutional scaling to synthesize coherent drafts from very large corpora, with a MapReduce-like divide-and-conquer assembly mechanism and built-in online retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B (QWEN3-32B used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Designed to work with extremely long resources via online retrieval; in experiments the system used its built-in online retrieval to collect relevant content.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Paper title provided as input query.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Convolutional scaling MapReduce-style synthesis over retrieved content, aggregating many retrieved fragments into a long-form coherent draft.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Long-form survey drafts intended to be coherent across extensive source material.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic LLM-based metrics and head-to-head comparisons; baseline results reported in Table 4.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>High language fluency score reported (86.14) and strong section-level coherence, but much lower citation F1 (about 0.017) compared to SciSage; document-level coherence slightly lower than SciSage (78.64 vs. 80.37).</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Effective at assembling long-form coherent drafts and high language fluency; built-in online retrieval for large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Extremely low citation alignment with human-curated references in the evaluation; lower document coherence vs. SciSage.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Failed to retrieve or align with the majority of human-cited references (very low F1 in the benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9657.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9657.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenScholar (RAG-based LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented LLM system for answering scientific queries by identifying relevant passages from open-access papers; cited as achieving citation accuracy comparable to human experts in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenScholar</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenScholar (integrated with SciSage outlines; evaluated with QWEN3-32B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Specialized RAG-based LLM that locates relevant passages from large open-access paper collections to answer scientific queries; in this paper, SciSage-generated outlines and paragraph-level queries were fed into OpenScholar's pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Open-access papers (specific sources not enumerated here); in experiments both local and online retrieval modes were enabled.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Paper titles used as seeds; SciSage outlines provided to guide paragraph-level retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>RAG pipeline where retrieved passages guide LLM generation; in this work, combined with SciSage outlines to adapt OpenScholar for survey writing.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated survey drafts with retrieved evidence passages and citations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automatic LLM-based metrics and comparison with SciSage and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Lower automatic metrics compared to SciSage (Table 4); extremely low citation F1 in this study (near zero in Table 4) despite prior claims in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>RAG architecture targeting passage-level retrieval from broad open-access corpora; prior work claimed human-level citation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>When used in this paper's experimental setup (with SciSage outlines), citation alignment remained poor compared to SciSage's multi-source reranking; precise performance depends on retrieval configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Recovered very few true-positive references relative to human-written surveys in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9657.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9657.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Research tools</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Research tools (closed-source research synthesis services)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Commercial/closed-source systems (e.g., OpenAI Deep Research, Gemini Deep Research) that claim to synthesize large amounts of online information into comprehensive scientific surveys; mechanisms are not fully disclosed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deep Research (closed-source LLM services)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Advanced closed-source LLM-driven tools that perform large-scale synthesis of online information into surveys; described as promising but with opaque mechanisms for search and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Large-scale online sources (not specified); closed-source retrieval/search mechanisms not described.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>General scientific synthesis and question answering from online literature.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Closed-source retrieval and synthesis pipelines (details unknown); likely RAG-like with proprietary search and aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comprehensive synthesized scientific surveys and answers.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed in the paper; prior reports claim promising performance but mechanisms and evaluations are unclear due to closed-source nature.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Described as promising in prior work, but the paper explicitly notes the opacity of underlying mechanisms and thus does not evaluate them in detail.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Potentially strong performance in large-scale online information synthesis according to non-detailed reports.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Closed-source; search/retrieval and synthesis mechanisms are unclear, limiting reproducibility and independent assessment.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Unavailable due to lack of transparent mechanism descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9657.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9657.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SurveyForge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SurveyForge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system for automated survey writing emphasizing outline heuristics, memory-driven generation, and multi-dimensional evaluation to bridge quality gaps between LLM outputs and human surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>SurveyForge</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SurveyForge (LLM-based pipeline referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Method that emphasizes outline heuristics and memory-driven generation to improve LLM-based survey quality; targets better structure and citation reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in this paper; referenced as prior work addressing outline heuristics and memory.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Outline heuristics plus memory-driven generation (details in the referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured survey drafts with improved outline coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Referenced as improving quality through multi-dimensional evaluation (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not reported in this paper; included in related work comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Focus on outline heuristics and memory to enhance coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9657.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9657.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InteractiveSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InteractiveSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based personalized and interactive survey generation system that allows continuous user customization of intermediate components like reference categorization, outline, and content.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interactivesurvey: An llm-based personalized and interactive survey paper generation system</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InteractiveSurvey (LLM-based interactive pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Interactive pipeline enabling personalized, iterative refinement with user-in-the-loop interventions across reference categorization, outline, and content synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified in the current paper; designed to operate with user-provided/ retrieved references.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>User-driven interactive queries and iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Human-in-the-loop RAG with stepwise customization of intermediate artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Interactive, user-tailored survey drafts.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Not detailed here; referenced as a different approach emphasizing personalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Enables user customization, improving engagement and output relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires human-in-the-loop; potential scalability trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9657.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9657.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STORM / Co-STORM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>STORM and Co-STORM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systems that model pre-writing stages for article generation and improve outline coherence via multi-perspective questioning and human-in-the-loop semantic mind-mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>STORM</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>STORM / Co-STORM (LLM-based multi-agent/interactive systems)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>STORM discovers diverse perspectives and simulates multi-perspective questioning to produce comprehensive outlines; Co-STORM adds human-in-the-loop semantic mind-mapping to enhance outline coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified here; positioned as systems for outline and draft generation.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Pre-writing exploration to discover perspectives on a topic.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Multi-perspective questioning, curation of collected information, and mind-map aided outline refinement; Co-STORM integrates human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Wikipedia-style drafts and improved outlines/mind-maps.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Referenced comparatively in related work; detailed evaluation in original works.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Emphasizes diverse perspectives and improved outline coherence via mind-mapping and human feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9657.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9657.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-RAG / Synthesizing scientific literature (Asai et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-RAG: Learning to retrieve, generate, and critique through self-reflection / Synthesizing scientific literature with retrieval-augmented LMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related works on retrieval-augmented LMs that learn retrieval, generation, and critique cycles (self-reflection) for improved literature synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-rag: Learning to retrieve, generate, and critique through self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Self-RAG / related retrieval-augmented LMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Approaches that integrate retrieval, generation, and reflective critique loops to improve factuality and synthesis when producing literature summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Not specified here; generally use scholarly corpora for retrieval-augmented tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>General literature synthesis and retrieval-augmented generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Self-reflection style loops combining retrieval, generation, and critique (RAG + reflective rewrites).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Improved, critique-informed literature syntheses.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Referenced as prior work informing SciSage's reflective design; details in cited papers.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not shown in this paper; cited as inspiration.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Informs iterative critique and retrieval-triggered regeneration design choices.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Autosurvey: Large language models can automatically write surveys <em>(Rating: 2)</em></li>
                <li>LLM × MapReduce-V2 <em>(Rating: 2)</em></li>
                <li>OpenScholar <em>(Rating: 2)</em></li>
                <li>Synthesizing scientific literature with retrieval-augmented lms <em>(Rating: 2)</em></li>
                <li>Self-rag: Learning to retrieve, generate, and critique through self-reflection <em>(Rating: 2)</em></li>
                <li>SurveyForge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing <em>(Rating: 1)</em></li>
                <li>Interactivesurvey: An llm-based personalized and interactive survey paper generation system <em>(Rating: 1)</em></li>
                <li>STORM <em>(Rating: 1)</em></li>
                <li>Co-STORM <em>(Rating: 1)</em></li>
                <li>Introducing deep research <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9657",
    "paper_id": "paper-279402998",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "SciSage",
            "name_full": "SCISAGE: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
            "brief_description": "A multi-agent, LLM-driven pipeline that synthesizes citation-rich scientific surveys via modular agents (Interpreter, Organizer, Collector, Composer, Refiner) and a hierarchical Reflector that iteratively critiques and refines drafts at outline, section, and document levels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SciSage multi-agent system (experiments run with QWEN3-32B)",
            "model_description": "System-level multi-agent architecture coordinating specialized LLM agents: Interpreter (query rewrite/intent), Organizer (multi-LLM ensemble for outlines), Collector (multi-source retrieval + reranking), Composer (bottom-up synthesis of sections with citation grounding and figure/table extraction), Refiner (final polishing), and Reflector (hierarchical iterative critique employing a panel of LLM expert-personas). Retrieval-augmented generation (RAG) with multi-source APIs (arXiv, PubMed, Google Scholar) and multi-LLM ensemble for outline diversity.",
            "model_size": "32B (QWEN3-32B used for experiments)",
            "input_corpus_description": "Retrieval from multiple scholarly sources guided per-section (e.g., arXiv, PubMed, Google Scholar); SurveyScope benchmark (46 high-impact papers, 2020–2025) used for evaluation. Collector reranks by recency, venue prestige, author influence, and citations.",
            "input_corpus_size": null,
            "topic_query_description": "User-provided research query (possibly a paper title); the Interpreter rewrites/normalizes queries to guide outline generation and per-section search queries.",
            "distillation_method": "Retrieval-augmented multi-agent synthesis with iterative hierarchical reflection: Organizer produces an outline via multi-LLM ensemble; Collector retrieves and reranks relevant papers; Composer synthesizes atomic, citation-rich subsection texts and extracts figures/tables; Reflector iteratively critiques outlines, sections, and full drafts, triggering additional retrievals and regenerations until quality thresholds or max trials are reached; Refiner enforces citation alignment, style, deduplication and outputs LaTeX/Markdown.",
            "output_type": "Structured, citation-rich scientific survey (sectioned document) with optional mindmaps and exported LaTeX/Markdown",
            "output_example": "A full-document draft F_final containing introduction, thematic chapters with citation-supported paragraphs, extracted figures/tables integrated into sections, and concluding future-directions — produced iteratively via generate-reflect-regenerate cycles.",
            "evaluation_method": "Automatic LLM-based metrics across Content Quality, Structural Coherence, and Reference Accuracy (TP/F1 vs. human-cited refs) plus human evaluations by CS graduate-level experts on 10 sampled topics.",
            "evaluation_results": "Automatic: document-level coherence 80.37, critical thinking 77.58, language fluency 85.65, citation F1 = 0.46 (correctly matching 1,510 references out of 3,844 in human-written papers). Compared to baselines (LLM×MapReduce-V2, AutoSurvey, OpenScholar), SciSage achieved best document coherence and much higher citation F1. Human eval: mixed — SciSage won 3 vs. lost 7 against human-written surveys (~30% win rate); strengths in topical breadth and retrieval efficiency, weaknesses in depth and mathematical precision.",
            "strengths": "Improved document-level coherence and substantially higher citation alignment (F1). Scalable, modular retrieval and synthesis across multi-source corpora. Reflector-enabled iterative critique yields measurable gains in language, critical scores, and structure. Generates mindmaps and exports for publication formats.",
            "limitations": "Limited evaluation to English; dependence on a single foundation model in experiments (QWEN3-32B); reproducibility and performance may vary with other LLMs; lacks rigorous mathematical/formula precision and stylistic nuance for some technical fields; metrics saturation reduces discriminative evaluation power.",
            "failure_cases": "Human evaluations show failures on deep analytical tasks, precise mathematical expression, and stylistic concision; produced longer, sometimes vague phrasing; lacked integrated detailed formulas/charts in fields needing rigorous mathematical exposition.",
            "uuid": "e9657.0",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "QWEN3-32B",
            "name_full": "QWEN3-32B",
            "brief_description": "The foundation model used to implement and evaluate SciSage and all baseline systems in the paper's experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "QWEN3-32B",
            "model_description": "A 32-billion-parameter foundation language model referenced in the paper and used as the LLM backbone for all methods and baselines in experiments.",
            "model_size": "32B",
            "input_corpus_description": "Varies by system — in experiments, each method used the title of a benchmark paper as input seed and relied on system-specific retrieval (online/offline) to fetch documents.",
            "input_corpus_size": null,
            "topic_query_description": "Each benchmark paper title served as the input query/seed for generation.",
            "distillation_method": "Serves as the underlying LLM in retrieval-augmented generation pipelines, multi-agent prompting, outline-to-content synthesis, and evaluation prompts.",
            "output_type": "Generated survey drafts and evaluated outputs for automatic metrics and human studies.",
            "output_example": null,
            "evaluation_method": "Automatic LLM-based scoring pipelines and human evaluation as described for SciSage.",
            "evaluation_results": "All numerical automatic evaluation results reported in the paper (SciSage and baselines) were obtained using QWEN3-32B; SciSage results (reported above) used this model.",
            "strengths": "Used uniformly across methods for controlled comparison; enabled direct baseline comparisons.",
            "limitations": "Paper notes model-dependence: results may vary with other foundation models; no cross-model evaluation provided.",
            "failure_cases": "Not individually analyzed beyond the system-level failure modes reported for SciSage and baselines.",
            "uuid": "e9657.1",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "AutoSurvey",
            "name_full": "AutoSurvey",
            "brief_description": "A prior two-stage LLM-based pipeline for automated survey generation focusing on outline generation followed by subsection synthesis using an offline corpus.",
            "citation_title": "Autosurvey: Large language models can automatically write surveys",
            "mention_or_use": "use",
            "model_name": "AutoSurvey (run with QWEN3-32B in this paper's experiments)",
            "model_description": "Two-stage pipeline: retrieval from an offline corpus, outline drafting, subsection generation, and evaluation. Does not support online retrieval in its original form.",
            "model_size": "32B (QWEN3-32B used in experiments)",
            "input_corpus_description": "Offline corpus provided by AutoSurvey (used for both retrieval and summarization in experiments); specific corpus size not stated.",
            "input_corpus_size": null,
            "topic_query_description": "Paper title used as the input seed for generation.",
            "distillation_method": "Retrieval-augmented generation with a two-stage outline-then-content pipeline using an offline dataset.",
            "output_type": "Survey drafts (human-level surveys claimed by the original AutoSurvey work).",
            "output_example": null,
            "evaluation_method": "Compared via the paper's automatic LLM-based metrics (content quality, structure, reference accuracy) and human evaluations.",
            "evaluation_results": "Reported lower document coherence and citation F1 than SciSage (automatic metrics table lists AutoSurvey with moderate content/structure scores and lower citation alignment; specific numbers shown in the paper's Table 4).",
            "strengths": "Established two-stage pipeline that influenced later systems; effective for offline-corpus settings.",
            "limitations": "Lacks online retrieval capability; in this paper, achieved lower citation recovery and structural scores compared to SciSage.",
            "failure_cases": "Limited retrieval scope (offline-only) leading to poorer citation alignment with human-written surveys.",
            "uuid": "e9657.2",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "LLM×MapReduce-V2",
            "name_full": "LLM × MapReduce-V2",
            "brief_description": "A system for assembling coherent long-form drafts from extremely long resources using entropy-driven convolutional scaling (convolutional test-time scaling).",
            "citation_title": "LLM × MapReduce-V2",
            "mention_or_use": "use",
            "model_name": "LLM×MapReduce-V2 (evaluated with QWEN3-32B in experiments)",
            "model_description": "An approach that applies entropy-driven convolutional scaling to synthesize coherent drafts from very large corpora, with a MapReduce-like divide-and-conquer assembly mechanism and built-in online retrieval.",
            "model_size": "32B (QWEN3-32B used in experiments)",
            "input_corpus_description": "Designed to work with extremely long resources via online retrieval; in experiments the system used its built-in online retrieval to collect relevant content.",
            "input_corpus_size": null,
            "topic_query_description": "Paper title provided as input query.",
            "distillation_method": "Convolutional scaling MapReduce-style synthesis over retrieved content, aggregating many retrieved fragments into a long-form coherent draft.",
            "output_type": "Long-form survey drafts intended to be coherent across extensive source material.",
            "output_example": null,
            "evaluation_method": "Automatic LLM-based metrics and head-to-head comparisons; baseline results reported in Table 4.",
            "evaluation_results": "High language fluency score reported (86.14) and strong section-level coherence, but much lower citation F1 (about 0.017) compared to SciSage; document-level coherence slightly lower than SciSage (78.64 vs. 80.37).",
            "strengths": "Effective at assembling long-form coherent drafts and high language fluency; built-in online retrieval for large corpora.",
            "limitations": "Extremely low citation alignment with human-curated references in the evaluation; lower document coherence vs. SciSage.",
            "failure_cases": "Failed to retrieve or align with the majority of human-cited references (very low F1 in the benchmark).",
            "uuid": "e9657.3",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "OpenScholar",
            "name_full": "OpenScholar (RAG-based LLM)",
            "brief_description": "A retrieval-augmented LLM system for answering scientific queries by identifying relevant passages from open-access papers; cited as achieving citation accuracy comparable to human experts in prior work.",
            "citation_title": "OpenScholar",
            "mention_or_use": "use",
            "model_name": "OpenScholar (integrated with SciSage outlines; evaluated with QWEN3-32B)",
            "model_description": "Specialized RAG-based LLM that locates relevant passages from large open-access paper collections to answer scientific queries; in this paper, SciSage-generated outlines and paragraph-level queries were fed into OpenScholar's pipeline.",
            "model_size": null,
            "input_corpus_description": "Open-access papers (specific sources not enumerated here); in experiments both local and online retrieval modes were enabled.",
            "input_corpus_size": null,
            "topic_query_description": "Paper titles used as seeds; SciSage outlines provided to guide paragraph-level retrieval.",
            "distillation_method": "RAG pipeline where retrieved passages guide LLM generation; in this work, combined with SciSage outlines to adapt OpenScholar for survey writing.",
            "output_type": "Generated survey drafts with retrieved evidence passages and citations.",
            "output_example": null,
            "evaluation_method": "Automatic LLM-based metrics and comparison with SciSage and other baselines.",
            "evaluation_results": "Lower automatic metrics compared to SciSage (Table 4); extremely low citation F1 in this study (near zero in Table 4) despite prior claims in literature.",
            "strengths": "RAG architecture targeting passage-level retrieval from broad open-access corpora; prior work claimed human-level citation accuracy.",
            "limitations": "When used in this paper's experimental setup (with SciSage outlines), citation alignment remained poor compared to SciSage's multi-source reranking; precise performance depends on retrieval configuration.",
            "failure_cases": "Recovered very few true-positive references relative to human-written surveys in the benchmark.",
            "uuid": "e9657.4",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Deep Research tools",
            "name_full": "Deep Research tools (closed-source research synthesis services)",
            "brief_description": "Commercial/closed-source systems (e.g., OpenAI Deep Research, Gemini Deep Research) that claim to synthesize large amounts of online information into comprehensive scientific surveys; mechanisms are not fully disclosed.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Deep Research (closed-source LLM services)",
            "model_description": "Advanced closed-source LLM-driven tools that perform large-scale synthesis of online information into surveys; described as promising but with opaque mechanisms for search and synthesis.",
            "model_size": null,
            "input_corpus_description": "Large-scale online sources (not specified); closed-source retrieval/search mechanisms not described.",
            "input_corpus_size": null,
            "topic_query_description": "General scientific synthesis and question answering from online literature.",
            "distillation_method": "Closed-source retrieval and synthesis pipelines (details unknown); likely RAG-like with proprietary search and aggregation.",
            "output_type": "Comprehensive synthesized scientific surveys and answers.",
            "output_example": null,
            "evaluation_method": "Not detailed in the paper; prior reports claim promising performance but mechanisms and evaluations are unclear due to closed-source nature.",
            "evaluation_results": "Described as promising in prior work, but the paper explicitly notes the opacity of underlying mechanisms and thus does not evaluate them in detail.",
            "strengths": "Potentially strong performance in large-scale online information synthesis according to non-detailed reports.",
            "limitations": "Closed-source; search/retrieval and synthesis mechanisms are unclear, limiting reproducibility and independent assessment.",
            "failure_cases": "Unavailable due to lack of transparent mechanism descriptions.",
            "uuid": "e9657.5",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "SurveyForge",
            "name_full": "SurveyForge",
            "brief_description": "A prior system for automated survey writing emphasizing outline heuristics, memory-driven generation, and multi-dimensional evaluation to bridge quality gaps between LLM outputs and human surveys.",
            "citation_title": "SurveyForge",
            "mention_or_use": "mention",
            "model_name": "SurveyForge (LLM-based pipeline referenced)",
            "model_description": "Method that emphasizes outline heuristics and memory-driven generation to improve LLM-based survey quality; targets better structure and citation reliability.",
            "model_size": null,
            "input_corpus_description": "Not specified in this paper; referenced as prior work addressing outline heuristics and memory.",
            "input_corpus_size": null,
            "topic_query_description": "Not specified here.",
            "distillation_method": "Outline heuristics plus memory-driven generation (details in the referenced work).",
            "output_type": "Structured survey drafts with improved outline coherence.",
            "output_example": null,
            "evaluation_method": "Referenced as improving quality through multi-dimensional evaluation (details in cited work).",
            "evaluation_results": "Not reported in this paper; included in related work comparison.",
            "strengths": "Focus on outline heuristics and memory to enhance coherence.",
            "limitations": "Not detailed in this paper.",
            "failure_cases": "Not reported here.",
            "uuid": "e9657.6",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "InteractiveSurvey",
            "name_full": "InteractiveSurvey",
            "brief_description": "An LLM-based personalized and interactive survey generation system that allows continuous user customization of intermediate components like reference categorization, outline, and content.",
            "citation_title": "Interactivesurvey: An llm-based personalized and interactive survey paper generation system",
            "mention_or_use": "mention",
            "model_name": "InteractiveSurvey (LLM-based interactive pipeline)",
            "model_description": "Interactive pipeline enabling personalized, iterative refinement with user-in-the-loop interventions across reference categorization, outline, and content synthesis.",
            "model_size": null,
            "input_corpus_description": "Not specified in the current paper; designed to operate with user-provided/ retrieved references.",
            "input_corpus_size": null,
            "topic_query_description": "User-driven interactive queries and iterative refinement.",
            "distillation_method": "Human-in-the-loop RAG with stepwise customization of intermediate artifacts.",
            "output_type": "Interactive, user-tailored survey drafts.",
            "output_example": null,
            "evaluation_method": "Not detailed here; referenced as a different approach emphasizing personalization.",
            "evaluation_results": "Not reported in this paper.",
            "strengths": "Enables user customization, improving engagement and output relevance.",
            "limitations": "Requires human-in-the-loop; potential scalability trade-offs.",
            "failure_cases": "Not reported here.",
            "uuid": "e9657.7",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "STORM / Co-STORM",
            "name_full": "STORM and Co-STORM",
            "brief_description": "Systems that model pre-writing stages for article generation and improve outline coherence via multi-perspective questioning and human-in-the-loop semantic mind-mapping.",
            "citation_title": "STORM",
            "mention_or_use": "mention",
            "model_name": "STORM / Co-STORM (LLM-based multi-agent/interactive systems)",
            "model_description": "STORM discovers diverse perspectives and simulates multi-perspective questioning to produce comprehensive outlines; Co-STORM adds human-in-the-loop semantic mind-mapping to enhance outline coherence.",
            "model_size": null,
            "input_corpus_description": "Not specified here; positioned as systems for outline and draft generation.",
            "input_corpus_size": null,
            "topic_query_description": "Pre-writing exploration to discover perspectives on a topic.",
            "distillation_method": "Multi-perspective questioning, curation of collected information, and mind-map aided outline refinement; Co-STORM integrates human feedback.",
            "output_type": "Wikipedia-style drafts and improved outlines/mind-maps.",
            "output_example": null,
            "evaluation_method": "Referenced comparatively in related work; detailed evaluation in original works.",
            "evaluation_results": "Not reported in this paper.",
            "strengths": "Emphasizes diverse perspectives and improved outline coherence via mind-mapping and human feedback.",
            "limitations": "Not detailed here.",
            "failure_cases": "Not reported here.",
            "uuid": "e9657.8",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Self-RAG / Synthesizing scientific literature (Asai et al.)",
            "name_full": "Self-RAG: Learning to retrieve, generate, and critique through self-reflection / Synthesizing scientific literature with retrieval-augmented LMs",
            "brief_description": "Related works on retrieval-augmented LMs that learn retrieval, generation, and critique cycles (self-reflection) for improved literature synthesis.",
            "citation_title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
            "mention_or_use": "mention",
            "model_name": "Self-RAG / related retrieval-augmented LMs",
            "model_description": "Approaches that integrate retrieval, generation, and reflective critique loops to improve factuality and synthesis when producing literature summaries.",
            "model_size": null,
            "input_corpus_description": "Not specified here; generally use scholarly corpora for retrieval-augmented tasks.",
            "input_corpus_size": null,
            "topic_query_description": "General literature synthesis and retrieval-augmented generation tasks.",
            "distillation_method": "Self-reflection style loops combining retrieval, generation, and critique (RAG + reflective rewrites).",
            "output_type": "Improved, critique-informed literature syntheses.",
            "output_example": null,
            "evaluation_method": "Referenced as prior work informing SciSage's reflective design; details in cited papers.",
            "evaluation_results": "Not shown in this paper; cited as inspiration.",
            "strengths": "Informs iterative critique and retrieval-triggered regeneration design choices.",
            "limitations": "Not detailed here.",
            "failure_cases": "Not reported here.",
            "uuid": "e9657.9",
            "source_info": {
                "paper_title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Autosurvey: Large language models can automatically write surveys",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "LLM × MapReduce-V2",
            "rating": 2,
            "sanitized_title": "llm_mapreducev2"
        },
        {
            "paper_title": "OpenScholar",
            "rating": 2,
            "sanitized_title": "openscholar"
        },
        {
            "paper_title": "Synthesizing scientific literature with retrieval-augmented lms",
            "rating": 2,
            "sanitized_title": "synthesizing_scientific_literature_with_retrievalaugmented_lms"
        },
        {
            "paper_title": "Self-rag: Learning to retrieve, generate, and critique through self-reflection",
            "rating": 2,
            "sanitized_title": "selfrag_learning_to_retrieve_generate_and_critique_through_selfreflection"
        },
        {
            "paper_title": "SurveyForge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing",
            "rating": 1,
            "sanitized_title": "surveyforge_on_the_outline_heuristics_memorydriven_generation_and_multidimensional_evaluation_for_automated_survey_writing"
        },
        {
            "paper_title": "Interactivesurvey: An llm-based personalized and interactive survey paper generation system",
            "rating": 1,
            "sanitized_title": "interactivesurvey_an_llmbased_personalized_and_interactive_survey_paper_generation_system"
        },
        {
            "paper_title": "STORM",
            "rating": 1
        },
        {
            "paper_title": "Co-STORM",
            "rating": 1
        },
        {
            "paper_title": "Introducing deep research",
            "rating": 1,
            "sanitized_title": "introducing_deep_research"
        }
    ],
    "cost": 0.017915,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SCISAGE: A MULTI-AGENT FRAMEWORK FOR HIGH-QUALITY SCIENTIFIC SURVEY GENERATION
21 Jul 2025</p>
<p>Xiaofeng Shi xfshi@baai.ac.cn 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Qian Kou 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Yuduo Li 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Beijing Jiaotong University (BJTU</p>
<p>Ning Tang 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Fudan University (FDU)</p>
<p>Jinxin Xie 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Longbin Yu 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Songjing Wang 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>Hua Zhou 
Beijing Academy of Artificial Intelligence (BAAI)</p>
<p>SCISAGE: A MULTI-AGENT FRAMEWORK FOR HIGH-QUALITY SCIENTIFIC SURVEY GENERATION
21 Jul 2025FAD81FB2F18C5E89222A4789F730F59CarXiv:2506.12689v2[cs.AI]
The rapid growth of scientific literature demands robust tools for automated survey-generation.However, current large language model (LLM)-based methods often lack in-depth analysis, structural coherence, and reliable citations.To address these limitations, we introduce SciSage, a multi-agent framework employing a reflect-when-you-write paradigm.SciSage features a hierarchical Reflector agent that critically evaluates drafts at outline, section, and document levels, collaborating with specialized agents for query interpretation, content retrieval, and refinement.We also release SurveyScope, a rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11 computer science domains, with strict recency and citation-based quality controls.Evaluations demonstrate that SciSage outperforms state-of-the-art baselines (LLM×MapReduce-V2, AutoSurvey), achieving +1.73 points in document coherence and +32% in citation F1 scores.Human evaluations reveal mixed outcomes (3 wins vs. 7 losses against human-written surveys), but highlight SciSage's strengths in topical breadth and retrieval efficiency.Overall, SciSage offers a promising foundation for research-assistive writing tools.Githubgithub.com/FlagOpen/SciSageBenchmark BAAI/SurveyScope</p>
<p>Introduction</p>
<p>The rapid growth of scientific literature, particularly in fast-evolving domains like artificial intelligence, poses increasing challenges for researchers to stay up-to-date [1,2].As literature accumulation outpaces human synthesis capacity, concerns emerge around research quality, redundancy, and accessibility.Survey articles help address this burden by systematically synthesizing existing work, highlighting key trends, and identifying open problems.High-quality surveys provide structured overviews, critically evaluate methodologies, and guide future research [3,4,5].However, their creation remains labor-intensive, demanding deep domain expertise, thematic abstraction, and rigorous citation management.As the scale and speed of academic papers continue to grow, scalable and robust survey generation methods have become increasingly essential.</p>
<p>With the development of large language models (LLMs) [6,7,8,9], researchers are employing them to automate scientific research survey writing.Most prior systems for automating literature surveys adhere to a two-stage pipeline-outline generation followed by content synthesis.AutoSurvey [2] employs a streamlined pipeline of retrieval, outline drafting, subsection generation, and evaluation to produce human-level surveys.STORM [10] leverages multi-agent to generate Wikipedia-style drafts, while Co-STORM [11] adds human-in-the-loop semantic mind-mapping to improve outline coherence.For large-scale survey tasks, LLM×MapReduce-V2 [12] uses convolutional scaling to synthesize coherent drafts from vast corpora.</p>
<p>Related Works</p>
<p>Scientific survey generation.The automation of scientific survey generation using Large Language Models (LLMs) has garnered significant attention in recent years.Early approaches primarily relied on retrieval-augmented generation (RAG) techniques to synthesize literature.For instance, OpenScholar [16] introduced a specialized RAG-based LLM capable of answering scientific queries by identifying relevant passages from a vast corpus of open-access papers, achieving citation accuracy on par with human experts.Despite these advancements, challenges persist in ensuring the structural coherence and depth of generated surveys.AutoSurvey [2] proposed a two-stage LLMbased method for survey generation, focusing on logical parallel generation to enhance content quality and citation accuracy.Similarly, SurveyForge [17] addressed some of these issues by emphasizing outline heuristics and memorydriven generation, aiming to bridge the quality gap between LLM-generated surveys and those written by humans.InteractiveSurvey [18] took a different approach by introducing a personalized and interactive survey paper generation system.This system allows users to customize and refine intermediate components continuously during generation, including reference categorization, outline, and survey content, thereby enhancing user engagement and output quality.In the realm of long-form article generation, STORM [10] presented a writing system that models the pre-writing stage by discovering diverse perspectives, simulating multi-perspective questioning, and curating collected information to create comprehensive outlines, while Co-STORM [11] extends this with human-in-the-loop and semantic mind-map techniques to enhance outline coherence.To handle ultra-long document synthesis, LLM×MapReduce-V2 [12] applies entropy-driven convolutional scaling to assemble coherent survey drafts from extensive corpora.In addition, Deep Research tools based on advanced closed-source LLMs [19,20] show promise performance in synthesizing large amounts of online information into comprehensive scitific surveys, whose mechanisms are still unclear.Despite the impressive performance of Deep Research tools, due to closed-source nature, their search mechanisms are still unknown.These systems demonstrate LLMs' potential in automating end-to-end survey generation, yet persistent challenges remain in guaranteeing content quality, structural comprehensive, and establishing rigorous evaluation standards.</p>
<p>Generative Agents [26] introduced a framework where 25 AI agents, each with unique identities and memories, autonomously coordinated social events and daily activities in a virtual town, demonstrating believable human-like behavior.MetaGPT [23] utilizes human-like standard operating procedures and specialized roles-product manager, architect, coder, tester-to reduce hallucinations in software generation.AutoGen [24] offers a flexible framework allowing configurable conversation patterns among LLM agents, enabling tool invocation, human-in-the-loop interventions, and multi-agent debate strategies to boost reasoning and factuality.AgentVerse [27] emphasizes group formation and emergent social behaviors, demonstrating performance gains from collaborative diversity.ChatDev [28] and its companion systems implement entire virtual development teams, validating structured role allocation on real-world code bases.DyLAN [29] introduced a dynamic LLM-agent network leveraging inference-time agent selection via an unsupervised Importance Score, flexible communication structures, and early stopping.Debate oriented frameworks [30,31] formalize structured argumentation among solver agents, mediated by aggregators, and show clear improvements on arithmetic and reasoning benchmarks.AgentNet [32] introduced a decentralized, retrieval-augmented, evolutionary coordination model over a dynamically evolving DAG network, eliminating central orchestrators and enabling scalable, privacy-aware specialization.We share the idea by establishing an LLM-based multi-agent system to facilitate academic research.</p>
<p>Research</p>
<p>Method</p>
<p>In this section, we introduce SciSage, a LLM-based Multi-Agent framework designed for automated scientific survey generation.Inspired by the cognitive and iterative behaviors of expert authors, SciSage leverages a coordinated architecture of specialized agents that unfolds through three interconnected components-Query Understanding and Rewrite, Retrieval and Content Generation, and Iterative Hierarchical Reflection.Each component comprises distinct agents that cooperate to produce high-quality scientific surveys.</p>
<p>As shown in Figure 1, SciSage operates as a dynamic, iterative workflow.The system begins with user queries and proceeds through modular stages where intermediate results are critically reviewed and enhanced.Central to this process is the Reflector agent, which simulates expert-like revision cycles to ensure coherence, depth, and informativeness across all sections of the survey.The following subsections provide an in-depth analysis of each module's architecture and operational logic.The overall pseudo-code of SciSage is summarized in Algorithm 1.</p>
<p>Algorithm 1 SCISAGE: A Multi-Agent Survey Generation System</p>
<p>Require: User query Q, research sources D, reflection trials N , outline templates T Ensure: Final refined survey document F f inal 1: Rewrite the query and get intent information
Q R , I ← INTERPRETER(Q) 2: Select a suitable outline template t ← ORGANIZER(Q R , I, T ) 3: Generate outline O ← ORGANIZER(Q R , I, t) 4: repeat 5:
Receive feedback from Reflector ∆O ← REFLECTOR(O, Q R , I)
6: if ∆O ̸ = ∅ then 7:
Refine and update O ← ORGANIZER(O, ∆O, t) Retrieve relevant papers from multiple sources P i ← COLLECTOR(S i , D)</p>
<p>Query Understanding and Rewrite</p>
<p>The efficacy of the entire review generation process is contingent upon a precise and comprehensive understanding of the user's request.The Interpreter Agent serves as the entry point of the SciSage framework.Its objective is to transform original, often ambiguous user queries into well-structured, standardized, and actionable instructions for downstream agents.First, to accommodate multilingual user queries, the Interpreter performs automatic language detection for query Q and translates it into English Q E .This standardization ensures consistency in downstream processing and leverages broader retrieval sources.Next, the Interpreter engages in a deep semantic analysis of the translated query to discern the user's core intent, scientific domain of interest, and research topic, which can be represented as intent information I.For example, given the query "The latest progress in code generation using LLM", the Interpreter infers that the user seeks recent advances in deep learning of LLMs for code generation.Finally, to maximize the precision and recall of the information retrieval phase, the initial query often requires refinement.The Interpreter evaluates whether the input query needs to be rewritten.Once ambiguity, vagueness, or informal phrasing is detected, the Interpreter generates a refined version Q R = Interpreter(Q E , I) that is semantically equivalent but structurally optimized for retrieval and generation purposes.Prompts for query understading and rewriting are shown in Appendix A.1.</p>
<p>Retrieval and Content Generation</p>
<p>The central engine of the SciSage framework executes a "bottom-up" workflow for content creation, moving from high-level planning to detailed writing and final polishing.This entire process is orchestrated across four specialized agents-Organizer, Collector, Composer, and Refiner-working in unison to produce a coherent and comprehensive survey.</p>
<p>Outline Construction</p>
<p>The Organizer Agent constructs a comprehensive, logical, and scholarly outline that reflects the user's intent, guiding high-quality content generation.It begins by selecting a suitable outline structure from a curated template library T based on the user's intent(e.g., survey, theory) detected by the Interpreter.To move beyond this initial template and foster a more innovative and robust structure, the Organizer then employs a multi-model ensemble strategy.Multiple LLMs generate varied outline candidates in parallel to promote diversity and reduce bias.These candidate outlines are synthesized into a unified structure using content-aware heuristics and the outline is represented as
O = ORGANIZER(Q R , I, t) = Merge(O LLM1 , • • • , O LLM N ).
Finally, for each section and subsection in the outline, the Organizer extracts key points and generates precise search queries {Q i S } to guide the following retrieval process, while deliberately excluding non-content sections such as conclusion and references.The ultimate output of this stage is a tree structure where each node contains a section title, its hierarchy, key points, and the corresponding search queries, which is then passed to the Collector to initiate the research phase.</p>
<p>Multi-Source Retrieval and Re-Ranking The Collector Agent serves as the research assistant and gathers high-quality references from various academic sources.Integrated with APIs from multiple scholarly sources D (e.g., arXiv, PubMed, Google Scholar), the Collector employs a multi-source adaptive retrieval strategy.Guided by the domain context provided by the Interpreter, it prioritizes sources most likely to yield relevant results, thereby improving both the efficiency and precision of the retrieval process.Once the relevant sources are identified, the Collector retrieves candidate papers and scores them, evaluating their semantic relevance and topical depth.To further ensure the credibility and currency of selected papers, the Collector reranks the retrieval results based on publication recency, venue prestige, author influence, and citation metrics, ultimately prioritizing the most authoritative and timely literature for subsequent content generation.The retrieval process for each section S i can be represented as
P i = COLLECTOR(S i , D)
, where P i is the final reranked most relevant paper list.</p>
<p>Hierarchical Content Generation</p>
<p>The Composer Agent is the central synthesis engine in the SciSage framework, tasked with transforming the Organizer's structured outline and the Collector's curated papers into a coherent and comprehensive scientific survey.It adopts a bottom-up methodology that emphasizes local coherence and factual grounding before scaling up to larger textual structures.The Composer begins with atomic content generation, producing focused, citation-rich content S i for each outlined subsection s i by synthesizing titles, abstracts, and full texts from corresponding retrieved papers P i .These atomic units are then assembled into coherent sections S i = COMPOSER(s i , P i ), each featuring an introductory overview, core discussions and a conclusion.During this process, the Composer also performs key figures and tables extraction and integrates them into section contents, scanning documents (e.g., LaTeX files from arXiv) to heuristically identify and extract visual content that best supports the topic, particularly in method or result sections.Once all sections are generated, the Composer organizes the content at both section and document levels, integrating the components into chapters and compiling them into a full-document draft F .This also includes crafting the Introduction and Conclusion/Future Work sections to ensure thematic and logical coherence.To further enhance readability, the Composer generates visual aids such as mindmaps derived from the outline and integrates them with the document to provide a high-level overview of the paper's structure and intellectual architecture.Mindmap example can be found in Appendix E.</p>
<p>Final Refinement</p>
<p>The Refiner is the final agent in the content generation process, responsible for transforming the draft into a polished document which is ready for publication.Following the Composer's draft generation, a thorough finalization process is conducted by the Refiner for both content and presentation to get the final refined survey F f inal = REFINER(F ).It improves the internal flow of paragraphs, eliminates redundancy, enforces consistent terminology, and ensures logical transitions throughout the manuscript.It starts with the content and citation, where the Refiner progressively aligns the document with the final outline based on the section titles and their corresponding content, removes the duplicated references and renumbers the citations.Next, the writing format and style are checked and standardized to meet the academic requirements, while ensuring the clarity of the topic.Lastly, as for the output, the Refiner exports the document in formats such as LaTeX and Markdown to support most publishing systems.</p>
<p>Iterative Hierarchical Reflection</p>
<p>The Reflector Agent is a critical innovation of SciSage's system, functioning as a pervasive, iterative mechanism embedded deeply within the workflows of both the Organizer and Composer.Rather than being a standalone step, it operates through a continuous "generate-reflect-regenerate" loop that drives recursive, multi-level content refinement, mirroring the self-corrective nature of expert academic writing.Its hierarchical scope of reflection spans the entire generation process.At the outline level, the Reflector evaluates outline O in completeness, logical structure, topical relevance, and alignment with academic standards, returning feedback ∆O to the Organizer for iterative refinement until a quality threshold is reached.At the section level, as the Composer produces section content S i , the Reflector gives critique ∆S i in accuracy, evidential support, structural clarity, and the balance of perspectives.If deficiencies are detected, it may trigger new literature retrieval by the Collector, followed by targeted content regeneration.At the full-text level, the Reflector deploys a panel of LLM agents simulating expert personas, such as journal editors, senior professors, and peer reviewers, to evaluate the manuscript from diverse critical views.A majority vote system identifies suboptimal sections, prompting the creation of a structured revision plan, including new key points and queries, which reactivates the Collector and Composer in a recursive improvement cycle.The Reflector also ensures that chapter introductions communicate each chapter's intent and structure.Through this dynamic process, SciSage transforms initial drafts into rigorously refined academic surveys that have withstood multiple rounds of critique and enhancement.</p>
<p>Benchmark</p>
<p>To comprehensively evaluate the quality of generated survey content, we introduce SurveyScope, a high-quality benchmark specifically designed for academic survey writing.SurveyScope significantly improves upon existing evaluation benchmarks like SURVEYEVAL_TEST [12] and AUTOSURVEY [2] by enhancing both the diversity of research topics and the quality of papers.</p>
<p>Paper quality in SurveyScope is defined by two key criteria: publication recency and citation count.</p>
<p>Given the fast-moving nature of computer science research-especially in areas such as large language models (LLMs) and AI safety-recent papers are more likely to reflect current trends, methods, and state-of-the-art advances.To ensure timeliness, all papers in SurveyScope were published between 2020 and 2025.The majority are concentrated in the 2023-2024 period, coinciding with the surge in large language model research following the advent of ChatGPT [33].</p>
<p>Citation count serves as a proxy for academic influence and recognition.A high citation count generally indicates an influential, well-received, and widely adopted paper.Papers in SurveyScope exhibit significantly higher citation metrics than those in other benchmarks, with a maximum of 2,184 and an average of approximately 322 citations.</p>
<p>These stringent metrics ensure the benchmark's high reliability and representativeness, grounding evaluation results in authoritative and influential literature.</p>
<p>Construction Methodology</p>
<p>The construction pipeline of SurveyScope is illustrated in Figure 2, and it consists the following key steps:</p>
<p>SurveyScope Completion Extraction Benchmarks</p>
<p>Seed Topics As Table 1 shows, the dataset comprises 20 surveys from SURVEYEVAL_TEST, 8 from AUTOSURVEY, and 18 manually curated surveys collected from Google Scholar and other academic platforms.This diverse sourcing strategy ensures a balanced benchmark that reflects both standardized evaluations and high-quality, real-world survey writing.</p>
<p>… …</p>
<p>Paper Selection Citation
Pub Date Content
SurveyEval_Test AutoSurvey Expand Manually Curated Table 1: Source distribution of SurveyScope Following this pipeline, we constructed a curated benchmark of 46 high-quality research papers.Each paper was manually selected by professionals with graduate-level training in computer science, based on criteria including publication recency and citation impact.</p>
<p>Characteristics</p>
<p>Thanks to a carefully designed construction pipeline, SurveyScope exhibits several key characteristics that distinguish it from existing benchmarks:</p>
<p>Broad Topic Coverage SurveyScope covers a broad range of active research areas in computer science, including natural language processing (NLP), large language models (LLMs), AI safety, robotics, and multimodal learning.This topical diversity enables systematic and cross-domain evaluation of automatic survey generation systems.Figure 3 provides an overview of the topic distribution, with 46 papers spanning 11 distinct topics.A detailed comparison of topic categories across benchmarks is provided in Appendix B.1.showing that over 52% of the papers have received more than 100 citations.
0
Summary SurveyScope stands out from existing benchmarks through its broad topical coverage, inclusion of recent high-impact publications, and emphasis on citation-based influence.These characteristics make it a comprehensive and reliable resource for evaluating academic survey generation systems.A comparative analysis across benchmarks is presented in Figure 6, where SurveyScope consistently leads across all dimensions.Comparison details can be found in Appendix B.4.</p>
<p>Category Count
Topic</p>
<p>Evaluation</p>
<p>To comprehensively evaluate the quality of generated content compared to human-written counterparts, we adopt a two-fold evaluation protocol: (1) automatic evaluation leveraging large language models (LLMs), and (2) human evaluation by domain experts.</p>
<p>Automatic Evaluation with LLM-based Metrics</p>
<p>We established an automated evaluation framework, drawing inspiration from AUTOSURVEY [2] and LLM × MAPREDUCE-V2 [12].Our evaluation assesses content across three core dimensions: Content Quality, Document Structure, and Reference Accuracy.All scores are normalized to a 0-100 scale, with higher scores indicating better performance.</p>
<p>Content Quality Assessment</p>
<p>We evaluated textual quality across the following dimensions:</p>
<p>Language Fluency and Style This metric assesses the linguistic quality of generated content, emphasizing academic formality, clarity, and fluency.Referring to the evaluation method provided by LLM × MAPREDUCE-V2 [12], we observed that directly using their original 100-point prompt template often resulted in limited score variance, with most outputs receiving uniformly high scores and thus exhibiting low discriminative capacity.To address this, we employed a 10-point scoring rubric to encourage more granular distinctions, then linearly rescaled the scores to a 0-100 range for comparability across evaluation metrics.Figure 7 presents the score distribution under different prompt templates, demonstrating that the 10-point rubric yields a broader and more informative spread.The prompt details we used can be found in Appendix A.4. Critical Thinking and Originality This dimension evaluates the depth of analysis, the originality of perspectives, and the articulation of forward-looking insights.To ensure consistency and interpretability, we designed structured prompts that elicit both numerical ratings and textual justifications from the model.Following the observation that the 100-point scale used in LLM ×MAPREDUCE-V2 [12] often results in compressed score distributions, we adopted a revised 10-point scale to enhance discriminative capacity.The evaluation prompt template is provided in Appendix A.5.</p>
<p>Topical Relevance We evaluated how well generated content aligns with the target research topic, following the approach in AUTOSURVEY [2].Our assessment focused on whether the survey maintains consistent focused on the intended subject, avoiding off-topic content.We employed a five-level scoring rubric (Table 2) that measures increasing degrees of topical coherence.We preserved the original AUTOSURVEY rubric without modification for comparability with prior work.</p>
<p>Structural Coherence Assessment</p>
<p>We evaluated the structural quality of generated content from both local and global perspectives.</p>
<p>Section-Level Structure This dimension assesses the internal coherence and logical flow of individual sections and subsections, following the rubric proposed in AUTOSURVEY.A score of 1 indicates disorganized or incoherent content, while a score of 5 denotes a tightly structured and logically consistent organization with smooth transitions.After evaluation, the score is linearly scaled to a 0-100 range.The full rubric is provided in Table 3.</p>
<p>Document-Level Structure</p>
<p>This dimension evaluates the overall structural coherence, thematic completeness, and scholarly depth of the document structure.We adopted a composite scoring scheme, assigning a score from 0 to 10 for each of the following three criteria: (1) structural coherence and narrative logic, (2) conceptual depth and thematic coverage, and (3) critical thinking and scholarly synthesis.The final score is calculated as the average of these sub-scores and is linearly scaled to a 0-100 range.Detailed prompts and scoring criteria are provided in Appendix A.6.</p>
<p>Note that Section-Level Structure focuses on local coherence between adjacent sections or subsections, while Document-Level Structure captures the document-wide organization, conceptual design, and thematic rigor.</p>
<p>Score Description
1
The content is outdated or unrelated to the field it purports to review, offering no alignment with the topic 2</p>
<p>The survey is somewhat on topic but with several digressions; the core subject is evident but not consistently adhered to.</p>
<p>3</p>
<p>The survey is generally on topic, despite a few unrelated details.</p>
<p>4</p>
<p>The survey is mostly on topic and focused; the narrative has a consistent relevance to the core subject with infrequent digressions.</p>
<p>5</p>
<p>The survey is exceptionally focused and entirely on topic; the article is tightly centered on the subject, with every piece of information contributing to a comprehensive understanding of the topic.</p>
<p>Table 2: Topical Relevance Assessment Rubric.</p>
<p>Score Description
1
The survey lacks logic, with no clear connections between sections, making it difficult to understand the overall framework.</p>
<p>2</p>
<p>The survey has weak logical flow with some content arranged in a disordered or unreasonable manner.</p>
<p>3</p>
<p>The survey has a generally reasonable logical structure, with most content arranged orderly, though some links and transitions could be improved such as repeated subsections.</p>
<p>4</p>
<p>The survey has good logical consistency, with content well arranged and natural transitions, only slightly rigid in a few parts.</p>
<p>5</p>
<p>The survey is tightly structured and logically clear, with all sections and content arranged most reasonably, and transitions between adajecent sections smooth without redundancy.</p>
<p>Table 3: Structural Coherence Evaluation Rubric.</p>
<p>Reference Accuracy Assessment</p>
<p>To evaluate the quality of reference usage in generated surveys, we compared the reference papers retrieved by models against those cited by human authors using standard information retrieval (IR) metrics.This evaluation is especially critical in retrieval-augmented generation (RAG) settings, as the quality of retrieved content directly impacts the factual accuracy and trustworthiness of the generated text.Specifically, we employed true positives (TP) and the F1 score [34] to quantify the degree of alignment between model-selected references and those curated by human experts.</p>
<p>True Positives (TP) Let A denote the set of references retrievd by the framework, and B denote the set of references cited in HUMAN WRITTEN surveys.We compute the number of correctly predicted references as:
TP = |A ∩ B|.
This metric reflects the absolute count of overlapping references between the model and the human-written baseline.</p>
<p>F1 Score We further compute:
F1 = 2 • Precision • Recall Precision + Recall , where Precision = |A ∩ B| |A| , Recall = |A ∩ B| |B| .
Precision measures the proportion of model-generated references that are also cited by human authors.Recall quantifies the proportion of human-cited references that the model successfully retrieves.The F1 score provides a harmonic mean of these two metrics, offering an overall measure of citation alignment.A higher F1 score indicates stronger agreement with human citation behavior and thus reflects superior reference retrieval quality within the RAG framework.</p>
<p>Human Evaluation by Domain Experts</p>
<p>To provide a more comprehensive evaluation of content quality beyond automatic metrics, we conducted a human study with domain experts.We randomly sampled 10 research topics and recruited graduate-level students in computer science as annotators.For each topic, annotators were presented with two documents: one generated by our SCISAGE system and one authored by human researchers.They were instructed to compare the texts across multiple dimensions, including logical coherence, academic tone, paragraph transitions, content completeness, and conciseness, among others.</p>
<p>6 Experiments</p>
<p>Baseline Configurations</p>
<p>To assess the effectiveness of SCISAGE, we compared it against three representative baselines.All methods were implemented using QWEN3-32B [9], and the title of each benchmark paper was used as the input seed for generation.Each baseline was executed using its official codebase with default or recommended configurations.A brief description of each baseline is provided below:</p>
<ol>
<li>OpenScholar (w/ SciSage) [16]: Since OpenScholar did not support outline generation natively, we incorporated outlines and paragraph-level queries generated by SCISAGE into its pipeline.The implementation was based on the official repository (https://github.com/AkariAsai/OpenScholar),and both local and online retrieval mode were enabled.</li>
</ol>
<p>AutoSurvey [2]:</p>
<p>As AUTOSURVEY lacks support for online retrieval, we use its offline corpus for both retrieval and summarization.Our implementation strictly follows the official codebase (https://github.com/AutoSurveys/AutoSurvey).</p>
<p>LLM × MapReduce-V2</p>
<p>[12]: We followed the official implementation from https://github.com/thunlp/LLMxMapReduce.The paper title was directly used as the input query, and the system employed its built-in online retrieval mechanism to collect relevant content before generation.</p>
<p>Complete hyperparameter settings for each baseline are provided in Appendix C.</p>
<p>Main Result</p>
<p>Automatic Evaluation Results</p>
<p>All evaluation results were obtained using QWEN3-32B [9].Table 4 reports the automatic evaluation scores for SCISAGE and three competitive baselines across content quality, structural coherence, reference accuracy.</p>
<p>Content Quality.SCISAGE achieves the highest score in critical thinking (77.58) while maintaining strong language fluency (85.65), slightly below LLM × MAPREDUCE-V2 (86.14).It also achieves perfect topical relevance (100).These results suggest that SCISAGE generally produces higher-quality content.</p>
<p>Structural Coherence.At both the section and document levels, SCISAGE outperformed all baselines, with the highest document coherence score (80.37).This indicates that SCISAGE demonstrates superior logical flow and structural organization.</p>
<p>Reference Accuracy.SCISAGE substantially improves citation accuracy, achieving an F1 score of 0.46 by correctly matching 1,510 references out of 3,844 cited in HUMAN WRITTEN papers.In contrast, competing baselines typically retrieve only a single overlapping reference, highlighting their limited capability in accurate citation reproduction.</p>
<p>These evaluation results demonstrate the effectiveness of SCISAGE.It consistently outperforms baselines across almost all metrics, especially in reference accuracy and document-level coherence.</p>
<p>Human Evaluaion Results</p>
<p>To evaluate the quality of content generated by SCISAGE, we conducted a human evaluation on a randomly selected set of 10 papers.These papers were assessed by professional evaluators, each holding a Master's degree in Computer Science.The evaluators performed a comprehensive analysis, contrasting the characteristics and identified shortcomings of SCISAGE's output against content authored by expert researchers on identical topics.Figure 8 shows the human evaluation results between SCISAGE and HUMAN WRITTEN, with further details provided in Appendix D.1.Strengths: Broad Coverage and Summarization SCISAGE excels at generating content that is broad in scope and performs as well as or better than human authors on summarization tasks.For example, in areas requiring extensive literature reviews and synthesis, such as the "Reasoning with Large Language Models, a Survey", SCISAGE can effectively summarize and present information.This feature makes it a valuable tool for quickly generating overviews and synthesizing large amounts of information.</p>
<p>Limitations: Depth, Precision, and Stylistic Nuance SCISAGE faces significant challenges in terms of content depth, especially when dealing with complex arguments, subtle details, and scenarios that require rigorous logical coherence or empirical support.It lacks precise and rigorous mathematical expression, which is particularly prominent in fields such as "reinforcement learning and algorithm research" that rely on precise formula descriptions.In terms of language style, SCISAGE tends to complicate sentence structure, resulting in less clear and concise writing, for example, by using vague terms such as "mitigation techniques" instead of precise academic vocabulary.Similar to existing generative models or frameworks, SCISAGE's generation also suffers from lengthy text and lacks integrated visual elements such as detailed formulas/charts, which are key carriers for conveying complex information in academic communication.</p>
<p>Conclusion: SciSage's Capabilities and Limitations in Academic Content Generation SCISAGE performs well in literature review and information integration, and can efficiently generate academic content with wide coverage, even surpassing the level of professional human authors.However, it still lacks analytical depth, mathematical expression accuracy, and academic language style, especially in fields that require complex logical reasoning, precise formulas, or rigorous terminology.</p>
<p>7 Ablation Study</p>
<p>Structural Impact of Query Understanding</p>
<p>We conduct an ablation study to investigate the structural benefits introduced by incorporating Query Understanding (Q.U.) in our framework.Specifically, we compare the following two experimental settings:</p>
<p>• Experiment A (w/ Q.U.):The complete SCISAGE pipeline, where the system first performs query understanding before generating the document structure.</p>
<p>• Experiment B (w/o Q.U.):A simplified pipeline that omits the query understanding step and directly proceeds to structure generation.</p>
<p>We evaluated the structural quality of the generated outlines along three dimensions: structural coherence, topical coverage, and critical analysis, as defined in Appendix A.6.</p>
<p>As shown in</p>
<p>Contribution of the Reflection</p>
<p>To assess the impact of iterative hierarchical reflection, we conducted an ablation study by disabling the reflection component in SCISAGE.Table 6 presents a comparison between the full system and its ablated variant.</p>
<p>Results show that reflection leads to sustained improvements in all dimensions assessed.Specifically, content quality improved significantly: Language scores increased from 82. 28</p>
<p>Limitations</p>
<p>Our study has several limitations that should be acknowledged:</p>
<p>• Language Restriction: The current evaluation is limited to English-language queries and documents.The effectiveness of our approach for other languages (e.g., Chinese) remains untested and may require additional language-specific adaptations.</p>
<p>• Domain Specificity: While we demonstrate strong performance in academic paper retrieval, the generalizability of our method to broader search scenarios (e.g., web search or enterprise document retrieval) requires further validation.</p>
<p>• Model Dependence: All reported results are based on the QWEN3-32B [9].The performance characteristics may vary when implemented with other foundation models, and comprehensive cross-model evaluation would be needed to establish broader applicability.</p>
<p>• Metric Saturation: Several systems, including SCISAGE, LLM × MAPREDUCE-V2, and AUTOSURVEY, achieved near-perfect scores in both Topical Relevance and Section Coherence.This saturation suggests that these metrics are becoming less effective in distinguishing between modern LLM-based generation systems, as they typically produce well-structured and topically relevant content.Future evaluations may require more fine-grained metrics to capture subtle differences in reasoning and factual consistency.</p>
<p>Conclusion</p>
<p>In this work, we present SciSage, a novel multi-agent framework that addresses long-standing limitations in automated scientific survey generation-specifically issues of structural coherence, content depth, and citation reliability.Guided by a reflect-when-you-write paradigm, SciSage coordinates six specialized agents across a dynamic workflow, with the Reflector Agent playing a central role in iteratively critiquing and refining outputs at the outline, section, and document levels.This reflection-driven architecture emulates expert authoring behavior and ensures end-to-end consistency and factual accuracy throughout the generation pipeline.SCISAGE significantly improves structural coherence and citation accuracy over existing methods.Rigorously evaluate the quality of an academic survey on the topic of [TOPIC] by scoring three dimensions (each on a 0-10 scale) and computing the average as the final score.</p>
<p>[Evaluation Criteria]</p>
<p>The final score is the average of the individual scores from the following three dimensions.Please evaluate each dimension rigorously based on the highest scholarly standards.</p>
<ol>
<li>Critical Analysis (10 points) Offers a deep and incisive critique of methodologies, results, and underlying assumptions.Clearly identifies significant gaps, weaknesses, and areas for improvement.Challenges assumptions with well-supported arguments and proposes concrete alternatives.</li>
</ol>
<p>Original Insights (10 points)</p>
<p>Proposes novel, well-supported interpretations or frameworks based on the reviewed literature.Demonstrates strong subject-matter understanding and contributes genuinely original perspectives.Insights are well-integrated with existing research, challenging conventional views or offering new directions.</p>
<p>Future Directions (10 points)</p>
<p>Clearly articulates promising research directions with strong justification.Suggestions are concrete, actionable, and closely tied to gaps identified in the literature.Demonstrates foresight by proposing innovative approaches or methodologies.</p>
<p>[Topic]</p>
<p>[TOPIC]</p>
<p>[Section]</p>
<p>[SECTION]</p>
<p>[Output Format] Rationale: <Provide a detailed justification for the score.Address each of the three dimensions step by step, highlighting specific strengths and weaknesses, such as the depth of critique, the originality of insights, or the clarity of proposed future directions.> Rigorously evaluate the quality of an academic survey outline on the topic of [TOPIC] by scoring three dimensions (each on a 0-10 scale) and computing the average as the final score.</p>
<p>[Evaluation Criteria]</p>
<p>Evaluate each dimension on a strict 0-10 scale, based on the following high-precision standards.The final score is the average of the three dimension scores.</p>
<p>Figure 1 :
1
Figures &amp; Charts Mind Map</p>
<p>Figure 2 :
2
Figure 2: Overview of the SurveyScope construction pipeline.</p>
<p>Figure 3 :Figure 4 :Figure 5 :
345
Figure 3: Distribution of topics in SurveyScope</p>
<p>Figure 6 :
6
Figure 6: Comparison of different benchmarks on Category Count, Topic Diversity, Data Volume, Year Span, Max Citations, Avg Citations</p>
<p>Figure 7 :
7
Figure 7: Score comparison: direct 100-point (blue) vs. scaled from 10-point rubric (purple).</p>
<p>Figure 8 :
8
Figure 8: Human evaluation results comparing SCISAGE with HUMAN WRITTEN papers</p>
<p>Final Score: <SCORE>(X+Y+Z/ 3 =
3
Final)</SCORE> Example: <SCORE>(2.5+7+5.1)/3=4.87</SCORE>Use two decimal places; do not include any other text outside the SCORE tag.A.6 Prompt for Evaluation Document Outline [Task]</p>
<p>Figure 9 :
9
Figure 9: Radar chart illustrating topic distribution across SurveyScope, SURVEYEVAL_TEST, and AutoSurvey.SurveyScope exhibits broader and more balanced domain coverage.</p>
<p>Figure 13 :
13
Figure 13: Example of generated outline</p>
<p>until ∆O = ∅ or max reflection trails N reached 10: Construct search queries for each section in final outline {Q i S } K i=1 ← ORGANIZER(O) 11: for all outlined section s i ∈ O do
8:end if9: 12:</p>
<p>13 :
13
Generate section content S i ← COMPOSER(s i , P i ) Refine and update S i ← COMPOSER(S i , P i , ∆S i ) until ∆C i = ∅ or max reflection trails N reached 20: end for 21: Integrate all sections to full survey F ← Merge(S 1 , . . ., S K ) 22: Refine and get the final survey F f inal ← REFINER(F ) 23: return F f inal
14:repeat15:Receive feedback from Reflector ∆S i ← REFLECTOR(S i , P i , O)16:if ∆S i ̸ = ∅ then17:18:end if19:</p>
<p>Table 4 :
4
Metrics of Automatic Evaluation.
MethodContent QualityStructural CoherenceReferenceLanguage Critical Relevance SectionDocumentF1TPOpenScholar (w/ SciSage)68.0953.5599--0.061 156AutoSurvey72.1360.90998565.330.14392LLM × MapReduce-V286.1476.9310010078.640.017 130SciSage85.6577.5810010080.370.46 1510</p>
<p>Table 5
5, incorporating query understanding leads to consistent improvements in both overall and aspect-level evaluation. The average and maximum document-level scores increase from 8.04 to 8.16 and from 9.00 to 9.33,respectively. Aspect-wise, improvements are observed in structure (8.74 vs. 8.64), coverage (8.32 vs. 8.20), andanalysis (7.40 vs. 7.29). These results suggest that query understanding enhances the SCISAGE's ability to generateoutlines that are more coherent, comprehensive, and analytically robust. (Full evaluation details and examples areprovided in our project repository.)MethodDocument Level StructureStructure Score DetailsAvg MaxMinStructure Coverage Analysisw/o Q.U. 8.04 9.006.338.648.207.29w/ Q.U. 8.16 9.336.008.748.327.40</p>
<p>Table 5 :
5
Comparison of SCISAGE with and without Reflection.</p>
<p>Table 6 :
6
to 85.60, and Critical scores significantly increased from 69.70 to 77.93.Structural coherence also benefited from reflection, with Document-level structure scores improving from 71.25 to 81.48.These findings suggest that repeated reflection enables SCISAGE to better revise and organize its generated content, resulting in more fluent, thoughtful, and well-structured content.Comparison of SCISAGE with and without Reflection.
MethodContent QualityStructural CoherenceLanguage Critical Relevance Section DocumentSciSage (w/o Reflection)82.2869.70100.0099.0071.25SciSage (w/ Reflection)85.6077.93100.00100.0081.48</p>
<p>To rigorously evaluate system performance, we introduce SurveyScope benchmark, curated for recency and scholarly impact, provides a robust testbed for evaluating survey-generation systems.Empirical results confirm SCISAGE's superiority: it achieves an 80.37 document-coherence score (vs.78.64 for LLM×MapReduce-V2) and 46% citation F1, outperforming all baselines.While SCISAGE still trails humanauthored surveys in analytical depth (30% win rate), it demonstrates clear advantages on relatively straightforward topics and offers substantial reductions in drafting time, highlighting its practical utility.strengths and weaknesses (e.g., academic tone consistency, clarity of sentence structure, or presence of redundancy).&gt;
Final Score:<SCORE>(X+Y+Z/3 = Final)</SCORE>Example: <SCORE>(2.5+7+5.1)/3=4.87</SCORE>Use up to two decimal places. Do not include any text outside the SCORE tags.A.5 Pormpt for Evaluation Critical Thinking Score[Task]</p>
<p>1 .
1
Structural Coherence &amp; Narrative Logic (10 points) Ideal Standard: The outline presents a well-structured, logically flowing framework.Sections and subsections are clearly organized, transitions are smooth, and the narrative progression is coherent.Scoring Guidance: Deduct points for imbalanced section lengths, disjointed transitions, or subsections that interrupt narrative clarity.A perfect score(10)requires no observable flaws.2. Conceptual Depth &amp; Thematic Coverage (10 points) Ideal Standard: The outline captures key themes, concepts, and subfields comprehensively and insightfully.There is a balance of breadth and depth, with core debates and historical development of the field clearly reflected.Scoring Guidance: Deduct points for missing major themes, excessive focus on niche areas, or shallow treatment of foundational concepts.3. Critical Thinking &amp; Scholarly Synthesis (10 points) Ideal Standard: The outline integrates perspectives critically, addressing contradictions, methodological tensions, and open research questions.It synthesizes viewpoints into a coherent scholarly vision.Scoring Guidance: Deduct points for lack of critical analysis, overlooking disagreements or critiques, or failing to propose unresolved questions.Rationale: <Provide a detailed reason for the score, considering each dimension step by step.Highlight specific strengths and weaknesses, such as structural imbalances, thematic omissions, or weak analytical synthesis.Then provide the final scores for each dimension.> <SCORE>(X+Y+Z/3 = ...)</SCORE> Example: <SCORE>(2.5+7+5.1)/3=4.87</SCORE>Use two decimal places; do not include any other text outside the SCORE tag.
Finance / Domain-specificLLMs (General)Dialogue SystemsSurveyScope SurveyEval_Test AutoSurveyLLMs EfficiencyBenchmarking / Evaluation8642LLMs Safety7. Medical / BiomedicalMedical / BiomedicalRoboticsMultimodalOtherNLP[Topic][TOPIC][Skeleton][OUTLINE][Output Format]-Structure: <X/10>-Coverage: <Y/10>-Critical Analysis: <Z/10>Final Score:
A Prompt TemplateA.1 Prompt for Query Understanding Prompt for Query Intent Chassification You are an expert in classifying user queries for academic research purposes.Your task is to analyze the given user query and extract the following information:1. Research Domain: Identify the broad academic field the query falls into.Examples: Computer Science, Medicine, Physics, Sociology, History, Linguistics.Be as specific as reasonably possible (e.g., "Machine Learning" if clearly indicated within Computer Science, otherwise "Computer Science").2. Query Type: Determine the type of information or paper the user is likely seeking.You MUST choose one of the following predefined types: survey, method, application, analysis, position, theory, benchmark, dataset, OTHER.If none of the specific types fit well, use OTHER.3.Research Topic: Pinpoint the specific subject, concept, or entities at the core of the query.This should be a concise phrase representing the main focus.For example, if the query is "latest advancements in using LLMs for code generation", the topic could be "LLMs for code generation".Prompt for Query RewritingYou are a query rewriting expert.Your task is to evaluate a given query and determine if it requires rewriting by checking for: Rigorously evaluate the quality of an academic survey on the topic of [TOPIC] by scoring three dimensions on a 0-10 scale.The final score is the arithmetic mean of the three individual scores.[Evaluation Criteria]Assign scores for each dimension based on the highest academic standards described below.The final score is calculated as the average of the three:1. Academic Formality (10 points) Demonstrates flawless academic rigor.Uses precise terminology consistently, avoids colloquial language entirely, and maintains a scholarly tone throughout.Sentence structures are sophisticated and intentionally crafted to support analytical depth.Even a single instance of informal phrasing or vague terminology disqualifies a perfect score.Clarity &amp; Readability (10 points)Writing is exceptionally clear, concise, and unambiguous.Sentences are logically structured with seamless transitions.The argument progresses smoothly with no unnecessary complexity.Any ambiguity or minor inefficiency reduces the score.Redundancy (10 points)Uniqueness: Every sentence should contribute new value.Repetition is only acceptable for structural clarity, such as reinforcing terminology or aiding transitions.Efficiency: Arguments must be logically coherent and free from unnecessary repetition.Redundant rephrasing of the same point without adding new insight leads to point deductions.[Topic][TOPIC][Section][SECTION]
Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. Robin Lutz Bornmann, Rüdiger Haunschild, Mutz, Humanities and Social Sciences Communications. 812021</p>
<p>Autosurvey: Large language models can automatically write surveys. Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Qingsong Wen, Wei Ye, Advances in Neural Information Processing Systems. 202437</p>
<p>Deep reinforcement learning: A survey. Xu Wang, Sen Wang, Xingxing Liang, Dawei Zhao, Jincai Huang, Xin Xu, Bin Dai, Qiguang Miao, IEEE Transactions on Neural Networks and Learning Systems. 3542022</p>
<p>A survey on large language models: Applications, challenges, limitations, and practical usage. Rizwan Muhammad Usman Hadi, Abbas Qureshi, Muhammad Shah, Anas Irfan, Muhammad Zafar, Naveed Bilal Shaikh, Jia Akhtar, Seyedali Wu, Mirjalili, 2023Authorea Preprints</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 1861863452024</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang ; Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, Zihan Qiu, arXiv:2505.09388Qwen3 technical report. Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren,2025arXiv preprint</p>
<p>Yijia Shao, Yucheng Jiang, Theodore A Kanell, Peter Xu, Omar Khattab, Monica S Lam, arXiv:2402.14207Assisting in writing wikipedia-like articles from scratch with large language models. 2024arXiv preprint</p>
<p>Into the unknown unknowns: Engaged human learning through participation in language model agent conversations. Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J Semnani, Monica S Lam, arXiv:2408.152322024arXiv preprint</p>
<p>Llm × mapreduce-v2: Entropy-driven convolutional test-time scaling for generating long-form articles from extremely long resources. Haoyu Wang, Yujia Fu, Zhu Zhang, Shuo Wang, Zirui Ren, Xiaorong Wang, Zhili Li, Chaoqun He, Bo An, Zhiyuan Liu, arXiv:2504.057322025arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 202336</p>
<p>Self-rag: Learning to retrieve, generate, and critique through self-reflection. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, Hannaneh Hajishirzi, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Self-reflection in llm agents: Effects on problem-solving performance. Matthew Renze, Erhan Guven, arXiv:2405.066822024arXiv preprint</p>
<p>Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D , arXiv:2411.14199Synthesizing scientific literature with retrieval-augmented lms. 2024arXiv preprint</p>
<p>Surveyforge: On the outline heuristics, memory-driven generation, and multi-dimensional evaluation for automated survey writing. Xiangchao Yan, Shiyang Feng, Jiakang Yuan, Renqiu Xia, Bin Wang, Bo Zhang, Lei Bai, arXiv:2503.046292025arXiv preprint</p>
<p>Interactivesurvey: An llm-based personalized and interactive survey paper generation system. Zhiyuan Wen, Jiannong Cao, Zian Wang, Beichen Guo, Ruosong Yang, Shuaiqi Liu, arXiv:2504.087622025arXiv preprint</p>
<p>. OpenAI. Introducing deep research. 2024</p>
<p>Gemini deep research overview. Google Deepmind, 2024</p>
<p>A survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, Yi Yang, 2024Vicinagearth19</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Meta programming for multi-agent collaborative framework. 202336arXiv preprint</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, arXiv:2308.081552023arXiv preprint</p>
<p>Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji, arXiv:2307.053002023arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, arXiv:2308.10848202326arXiv preprint</p>
<p>Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, arXiv:2307.07924Communicative agents for software development. 2023arXiv preprint</p>
<p>Dynamic llm-agent network: An llm-agent collaboration framework with agent team optimization. Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, Diyi Yang, arXiv:2310.021702023arXiv preprint</p>
<p>Encouraging divergent thinking in large language models through multi-agent debate. Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, Zhaopeng Tu, arXiv:2305.191182023arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, Forty-first International Conference on Machine Learning. 2023</p>
<p>Agentnet: Decentralized evolutionary coordination for llm-based multi-agent systems. Yingxuan Yang, Huacan Chai, Shuai Shao, Yuanyi Song, Siyuan Qi, Renting Rui, Weinan Zhang, arXiv:2504.005872025arXiv preprint</p>
<p>Chatgpt and open-ai models: A preliminary review. I Konstantinos, Nikolaos D Roumeliotis, Tselikas, Future Internet. 1561922023</p>
<p>A probabilistic interpretation of precision, recall and f-score, with implication for evaluation. Cyril Goutte, Eric Gaussier, European conference on information retrieval. Springer2005</p>
<p>. Finance / Domain-specific. 9</p>
<p>Other Paper Title: {title} SurveyScope (n=46) SurveyEval_Test (n=20) AutoSurvey (n=20). 2019 2020 2021 2022 2023 2024 2025 20262022Benchmarking / Evaluation 11. Year Med:2023 Mean:2023.3 Max:2025 Min:2020 Med:2023 Mean:2022.5 Max:2024 Min:2020 Med:2024 Mean:2023.6 Max:2025 Min</p>
<p>SurveyScope emphasizes more recent works, reflecting rapid developments in the field. min_citation: 5 Minimum citation count for reference papers. norm_cite: True Normalize citation counts. ss_retriever: True Enable Semantic Scholar online retrieval. use_feedback: True Enable feedback for iterative refinement. new_feedback_docs: 2 Documents retrieved after feedback. Figure. 10Boxplot showing publication year distributions across benchmarks. feedback_num: 4 Number of feedback items used</p>
<p>Human Evaluation Details Paper Title Evaluation Result Human Analysis Measure and Improve Robustness in NLP Models: A Survey Human is better Human version defines robustness clearly, has better structure and logic. D Experiment Result, LLM version has awkward phrasing and lacks coherence</p>
<p>A Survey on Explainability in Machine Reading Comprehension Human is better Human version uses structured benchmarks and visuals effectively; LLM version lacks clarity and has poor section design. </p>
<p>LLM offers broader metrics. The Decades Progress on Code-Switching Research in NLP Human is better Human version aligns better with survey goals using empirical analysis. Efficient Methods for Natural Language Processing: A Survey Same Both cover NLP efficiency. human is clear. LLM fails to capture research trend focus</p>
<p>A Survey of Large Language Models in Medicine Human is better Human version is structured around medical use cases; LLM version is disjointed and overly focused on technical background. </p>
<p>A Survey of Controllable Text Generation Human is better Human version is intuitive and organized by model stages; LLM version is messy and lacks strategy-method separation. </p>
<p>Neural Entity Linking: A Survey of Models Based on Deep Learning Human is better Human version follows processing pipeline; LLM version has incoherent topic grouping and surface-level analysis. Reasoning with Large Language Models, a Survey SciSage is better Human version is CoT-focused but narrow. A Survey on Detection of LLMs-Generated Content Same LLM version is well-structured and easy to follow; human version introduces more novel and timely perspectives. LLM version covers broader reasoning aspects despite typical stylistic flaws</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6601 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6601</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6601</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-1085ddc5028be0a6f517bde6c44029abc208c63f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1085ddc5028be0a6f517bde6c44029abc208c63f" target="_blank">Associative Recurrent Memory Transformer</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%.</p>
                <p><strong>Paper Abstract:</strong> This paper addresses the challenge of creating a neural architecture for very long sequences that requires constant time for processing new information at each time step. Our approach, Associative Recurrent Memory Transformer (ARMT), is based on transformer self-attention for local context and segment-level recurrence for storage of task specific information distributed over a long context. We demonstrate that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of 79.9%. The source code for training and evaluation is available on github.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6601.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6601.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Associative Recurrent Memory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer augmented with layerwise associative (fast-weight) segment-level recurrent memory that stores key-value associations in quasi-linear association matrices and recalls them via an inner-product lookup using a DPFP nonlinearity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ARMT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer with segment-level recurrence plus layerwise associative memory matrices A_s^l. At each segment, memory tokens are linearly projected to keys and values and written into an association matrix using a delta-rule-like fast-weights update (with importance beta and gamma correction); at recall, queries are projected and associations are retrieved by inner products with phi(non-linearity)-transformed keys normalized by z.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>145M (reported GPT-2 augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Layerwise associative (quasi-linear key-value / fast-weights) segment-level recurrent memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Projected memory tokens stored as key vectors, value vectors, and scalar importances (beta) in an association matrix A and a normalization vector z</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Writes: linear projection of memory token -> (k,v,beta), recall previous value bar_v, update A via outer product addition A += beta*(v - bar_v) ⊗ phi(k) and update z with gamma-weighted phi(k). Reads: project token to query q and compute y = A phi(q) / (z^T phi(q)) (inner-product based associative recall with DPFP nonlinearity).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative Retrieval (Remember & Rewrite), BABILong QA1-5, Wikitext-103 (LM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>associative retrieval / long-context QA (single-fact and multi-hop) / language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>BABILong QA1: up to 79.9% Exact Match reported on 50M tokens (best single model; averaged runs show 49.6% ± 40.4); QA1 at 10M: 89.4% ± 8.1 EM; on associative retrieval tasks: near-perfect recall and robust to 500 rewrite operations when trained on 50 pairs (10x generalization); language modeling: similar LM loss to RMT (no clear LM improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared to RMT/PRMT (no associative block): RMT (137M) QA1 at 10M: 76.4% EM; PRMT (ablation with layerwise recurrent memory but no associative matrix) did not improve RMT and performs substantially worse than ARMT on associative retrieval and long-context QA (see text & Fig.2/Fig.4).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) for BABILong; exact-match / recall for associative retrieval; cross-entropy (bits-per-byte) for LM</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Sequential-segment processing required (no efficient parallel implementation) so slower on short/medium contexts vs parallel methods (Mamba, RWKV); requires gamma-correction to avoid catastrophic forgetting in many erase-insert operations; still challenging to train for language modeling (tends to keep only last segment in memory).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Slower on short/medium-length contexts due to lack of parallelism; struggles on LM tasks (keeps mostly last segment in memory and extrapolates poorly for LM); without gamma-correction the quasi-linear fast-weights suffer catastrophic forgetting under many updates; some instability across runs at extreme lengths (large std at 50M).</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Rodkin I., Kuratov Y., Bulatov A., Burtsev M. (2024). Associative Recurrent Memory Transformer. arXiv preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6601.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6601.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Memory Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Segment-level recurrent transformer that passes a small set of memory tokens between segments to provide long-range context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent memory transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RMT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Transformer with segment-level recurrence implemented by memory tokens passed between segments; local self-attention on the segment plus recurrent memory tokens provide long-context information.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>137M (reported GPT-2 baseline augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Segment-level recurrent memory tokens (recurrent state passed between segments)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Learnable memory tokens (vectors) forming recurrent state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Memory tokens are concatenated/attended to by local self-attention in the transformer and passed to next segment (recurrence).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative Retrieval (Remember & Rewrite), BABILong QA1-5, Wikitext-103 (LM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>associative retrieval / long-context QA / language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On BABILong QA1: RMT (137M) reaches 76.4% EM at 10M tokens and degrades at larger lengths (e.g., 94.2% at 1M, 76.4% at 10M in table); on associative retrieval RMT underperforms ARMT and Mamba in storing many key-value pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) for BABILong; exact-match/recall metrics for associative retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>RMT's memory capacity limited by number of memory tokens; requires backprop-through-time for many layers during training, making training challenging; does not scale as well in associative retrieval as ARMT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower associative memory capacity compared to ARMT and Mamba; shows degradation when number of rewrite operations or context length exceed training regime.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Bulatov A., Kuratov Y., Burtsev M. S. (2022). Recurrent Memory Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6601.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6601.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRMT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel-memory Recurrent Memory Transformer (ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablation variant of RMT with layerwise recurrent memory tokens (parallel memory tokens passed to next segment in each layer) used to test the role of associative memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PRMT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RMT variant with layerwise recurrent memory tokens (memory tokens are passed to the next segment in each layer) but without associative (association-matrix) block.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Layerwise recurrent memory tokens (non-associative)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Memory tokens per layer (vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Memory tokens passed forward between segments and attended locally by transformer layers (no associative key-value matrix updates).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative Retrieval (ablation), BABILong</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>associative retrieval / long-context QA (ablation study)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>PRMT did not improve over RMT on associative retrieval capacity (Fig.2a) and generally performs similarly to RMT on evaluated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-match / recall (associative retrieval), Exact Match for BABILong</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Shows that adding layerwise recurrent tokens alone (without associative matrix) does not solve capacity issues—associative mechanism is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails to match ARMT's increased memory capacity and robustness to many rewrite operations; no notable improvement over RMT in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6601.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6601.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mamba</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mamba: Linear-time sequence modeling with selective state spaces</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computationally efficient long-context model (state-space / selective state spaces) used as a baseline; it uses recurrent state vectors as memory and is parallelizable for LM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mamba: Linear-time sequence modeling with selective state spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Mamba</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Linear-time sequence model using selective state-space components (recurrent states) providing working memory (recurrent state vectors) used for long-context processing; memory represented as recurrent states with fixed float-budget.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130M (reported runs)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Recurrent state-space memory (recurrent state vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recurrent state floats (state vectors) used as working memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>State updates via selective state-space modules (recurrent update rules); memory size compared in number of floats in recurrent states.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Associative Retrieval (Remember & Rewrite), BABILong QA1-5, Language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>associative retrieval / long-context QA / language modeling</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>On associative retrieval Remember task Mamba outperforms RMT and is comparable to ARMT in some capacity metrics; on BABILong Mamba performs well up to its implementation limits (e.g., strong on some tasks at moderate lengths, but not evaluated at extreme lengths like 10M+ due to segmentation limits). Specific reported: Mamba (130M) QA1 at 64k/128k: ~100%/99.5% EM; degrades at longer lengths (e.g., 92.3% at 500k in table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM) and exact-match/recall for associative retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>More parallel and faster on short/medium sequences; implementation segmentation limits prevented evaluation at extreme long lengths (500k+); may experience slight degradation past training lengths on rewrite tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Implementation limits prevented evaluating at extreme context lengths; experiences some degradation when exceeding training lengths on dynamic rewrite tasks compared to ARMT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Gu A., Dao T. (2023). Mamba: Linear-time sequence modeling with selective state spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6601.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6601.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RWKV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RWKV (Reinventing RNNs for the Transformer era)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RNN-like model with transformer-style performance that has been used in long-context work; the authors attempted to train RWKV but failed to obtain reasonable scores for these benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RWKV: Reinventing RNNs for the transformer era</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RWKV</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RNN-structured model with linear-time recurrence and gating designed as an alternative to transformers for long contexts; in this paper the authors attempted but failed to train RWKV for associative retrieval and BABILong evaluations with their settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>attempted 430M finetune for Babilong (reported attempt), associative-retrieval training from scratch (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Recurrent (RNN-like) state</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Recurrent hidden states (time-mixed state vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>Sequential recurrent updates (RNN-style recurrence with gating)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Attempted on Associative Retrieval and BABILong (training failed)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>associative retrieval / long-context QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>No reasonable scores achieved by authors with their training setup; training failed for both associative retrieval and BABILong (see Appendix J).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Authors note RWKV training may need careful adjustments; did not succeed under their hyperparameters; RWKV is more parallel on some regimes (per discussion of parallel methods).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Training instability or sensitivity: the authors were unable to train RWKV to solve the evaluated tasks with their settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Peng B., et al. (2023). RWKV: Reinventing RNNs for the transformer era.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6601.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6601.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (few-shot baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large pretrained LLM used as a few-shot baseline on BABILong tasks (no special long-range memory mechanism applied in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Off-the-shelf GPT-4 used in few-shot prompting as a baseline; no additional memory augmentation described in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BABILong QA1-5 (few-shot baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-context QA baseline (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>N/A (no memory augmentation in these few-shot runs). Reported few-shot EMs: e.g., QA1: 30.0% at 64k, 24.0% at 128k (table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Baseline few-shot LLM performance is low on extreme long-context queries compared to recurrent-memory models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Per-table, poor performance on long-context single/multi-hop retrieval tasks compared to recurrent/associative-memory augmented models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6601.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6601.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 + RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with Retrieval-Augmented Generation (RAG) (few-shot baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 combined with retrieval (RAG) used as a baseline that augments the model with external retrieval during few-shot evaluation on BABILong.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-4 + RAG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Few-shot GPT-4 augmented with external retrieval (RAG) used as baseline; retrieval supplies context from an external store to the model at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>Retrieval-augmented external storage (document retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Retrieved text passages/documents</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>External retrieval (RAG) pipeline providing retrieved passages to GPT-4 for generation/answering</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>BABILong QA1-5 (few-shot baseline with RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>long-context QA with external retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported EMs in table: e.g., QA1: 50.0% at 64k, 56.0% at 128k, 50.0% at 500k, 56.0% at 1M, 16.0% at 10M (few-shot + RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared to GPT-4 few-shot: RAG generally improves EM at some lengths (e.g., QA1 50.0% vs 30.0% at 64k) but still lags behind ARMT at extreme lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact Match (EM)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>RAG helps but still fails on extremely long contexts (performance drops at very long lengths, e.g., 10M), indicating retrieval alone may not suffice for multi-fact reasoning in extremely long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Performance degrades at very long context lengths (10M+); retrieval may not retrieve/combine multiple distant facts required for multi-hop reasoning in BABILong.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Associative Recurrent Memory Transformer', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Recurrent memory transformer <em>(Rating: 2)</em></li>
                <li>Mamba: Linear-time sequence modeling with selective state spaces <em>(Rating: 2)</em></li>
                <li>Linear transformers are secretly fast weight programmers <em>(Rating: 2)</em></li>
                <li>In search of needles in a 11m haystack: Recurrent memory finds what llms miss <em>(Rating: 2)</em></li>
                <li>RWKV: Reinventing RNNs for the transformer era <em>(Rating: 1)</em></li>
                <li>Memorizing Transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6601",
    "paper_id": "paper-1085ddc5028be0a6f517bde6c44029abc208c63f",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "ARMT",
            "name_full": "Associative Recurrent Memory Transformer",
            "brief_description": "A transformer augmented with layerwise associative (fast-weight) segment-level recurrent memory that stores key-value associations in quasi-linear association matrices and recalls them via an inner-product lookup using a DPFP nonlinearity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ARMT",
            "agent_description": "Transformer with segment-level recurrence plus layerwise associative memory matrices A_s^l. At each segment, memory tokens are linearly projected to keys and values and written into an association matrix using a delta-rule-like fast-weights update (with importance beta and gamma correction); at recall, queries are projected and associations are retrieved by inner products with phi(non-linearity)-transformed keys normalized by z.",
            "model_size": "145M (reported GPT-2 augmentation)",
            "memory_used": true,
            "memory_type": "Layerwise associative (quasi-linear key-value / fast-weights) segment-level recurrent memory",
            "memory_representation": "Projected memory tokens stored as key vectors, value vectors, and scalar importances (beta) in an association matrix A and a normalization vector z",
            "memory_access_mechanism": "Writes: linear projection of memory token -&gt; (k,v,beta), recall previous value bar_v, update A via outer product addition A += beta*(v - bar_v) ⊗ phi(k) and update z with gamma-weighted phi(k). Reads: project token to query q and compute y = A phi(q) / (z^T phi(q)) (inner-product based associative recall with DPFP nonlinearity).",
            "task_name": "Associative Retrieval (Remember & Rewrite), BABILong QA1-5, Wikitext-103 (LM)",
            "task_category": "associative retrieval / long-context QA (single-fact and multi-hop) / language modeling",
            "performance_with_memory": "BABILong QA1: up to 79.9% Exact Match reported on 50M tokens (best single model; averaged runs show 49.6% ± 40.4); QA1 at 10M: 89.4% ± 8.1 EM; on associative retrieval tasks: near-perfect recall and robust to 500 rewrite operations when trained on 50 pairs (10x generalization); language modeling: similar LM loss to RMT (no clear LM improvement).",
            "performance_without_memory": "Compared to RMT/PRMT (no associative block): RMT (137M) QA1 at 10M: 76.4% EM; PRMT (ablation with layerwise recurrent memory but no associative matrix) did not improve RMT and performs substantially worse than ARMT on associative retrieval and long-context QA (see text & Fig.2/Fig.4).",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM) for BABILong; exact-match / recall for associative retrieval; cross-entropy (bits-per-byte) for LM",
            "tradeoffs_reported": "Sequential-segment processing required (no efficient parallel implementation) so slower on short/medium contexts vs parallel methods (Mamba, RWKV); requires gamma-correction to avoid catastrophic forgetting in many erase-insert operations; still challenging to train for language modeling (tends to keep only last segment in memory).",
            "limitations_or_failure_cases": "Slower on short/medium-length contexts due to lack of parallelism; struggles on LM tasks (keeps mostly last segment in memory and extrapolates poorly for LM); without gamma-correction the quasi-linear fast-weights suffer catastrophic forgetting under many updates; some instability across runs at extreme lengths (large std at 50M).",
            "citation": "Rodkin I., Kuratov Y., Bulatov A., Burtsev M. (2024). Associative Recurrent Memory Transformer. arXiv preprint.",
            "uuid": "e6601.0",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RMT",
            "name_full": "Recurrent Memory Transformer",
            "brief_description": "Segment-level recurrent transformer that passes a small set of memory tokens between segments to provide long-range context.",
            "citation_title": "Recurrent memory transformer",
            "mention_or_use": "use",
            "agent_name": "RMT",
            "agent_description": "Transformer with segment-level recurrence implemented by memory tokens passed between segments; local self-attention on the segment plus recurrent memory tokens provide long-context information.",
            "model_size": "137M (reported GPT-2 baseline augmentation)",
            "memory_used": true,
            "memory_type": "Segment-level recurrent memory tokens (recurrent state passed between segments)",
            "memory_representation": "Learnable memory tokens (vectors) forming recurrent state",
            "memory_access_mechanism": "Memory tokens are concatenated/attended to by local self-attention in the transformer and passed to next segment (recurrence).",
            "task_name": "Associative Retrieval (Remember & Rewrite), BABILong QA1-5, Wikitext-103 (LM)",
            "task_category": "associative retrieval / long-context QA / language modeling",
            "performance_with_memory": "On BABILong QA1: RMT (137M) reaches 76.4% EM at 10M tokens and degrades at larger lengths (e.g., 94.2% at 1M, 76.4% at 10M in table); on associative retrieval RMT underperforms ARMT and Mamba in storing many key-value pairs.",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM) for BABILong; exact-match/recall metrics for associative retrieval",
            "tradeoffs_reported": "RMT's memory capacity limited by number of memory tokens; requires backprop-through-time for many layers during training, making training challenging; does not scale as well in associative retrieval as ARMT.",
            "limitations_or_failure_cases": "Lower associative memory capacity compared to ARMT and Mamba; shows degradation when number of rewrite operations or context length exceed training regime.",
            "citation": "Bulatov A., Kuratov Y., Burtsev M. S. (2022). Recurrent Memory Transformer.",
            "uuid": "e6601.1",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "PRMT",
            "name_full": "Parallel-memory Recurrent Memory Transformer (ablation)",
            "brief_description": "Ablation variant of RMT with layerwise recurrent memory tokens (parallel memory tokens passed to next segment in each layer) used to test the role of associative memory.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "PRMT",
            "agent_description": "RMT variant with layerwise recurrent memory tokens (memory tokens are passed to the next segment in each layer) but without associative (association-matrix) block.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Layerwise recurrent memory tokens (non-associative)",
            "memory_representation": "Memory tokens per layer (vectors)",
            "memory_access_mechanism": "Memory tokens passed forward between segments and attended locally by transformer layers (no associative key-value matrix updates).",
            "task_name": "Associative Retrieval (ablation), BABILong",
            "task_category": "associative retrieval / long-context QA (ablation study)",
            "performance_with_memory": "PRMT did not improve over RMT on associative retrieval capacity (Fig.2a) and generally performs similarly to RMT on evaluated tasks.",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact-match / recall (associative retrieval), Exact Match for BABILong",
            "tradeoffs_reported": "Shows that adding layerwise recurrent tokens alone (without associative matrix) does not solve capacity issues—associative mechanism is critical.",
            "limitations_or_failure_cases": "Fails to match ARMT's increased memory capacity and robustness to many rewrite operations; no notable improvement over RMT in experiments.",
            "citation": "",
            "uuid": "e6601.2",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Mamba",
            "name_full": "Mamba: Linear-time sequence modeling with selective state spaces",
            "brief_description": "A computationally efficient long-context model (state-space / selective state spaces) used as a baseline; it uses recurrent state vectors as memory and is parallelizable for LM.",
            "citation_title": "Mamba: Linear-time sequence modeling with selective state spaces",
            "mention_or_use": "use",
            "agent_name": "Mamba",
            "agent_description": "Linear-time sequence model using selective state-space components (recurrent states) providing working memory (recurrent state vectors) used for long-context processing; memory represented as recurrent states with fixed float-budget.",
            "model_size": "130M (reported runs)",
            "memory_used": true,
            "memory_type": "Recurrent state-space memory (recurrent state vectors)",
            "memory_representation": "Recurrent state floats (state vectors) used as working memory",
            "memory_access_mechanism": "State updates via selective state-space modules (recurrent update rules); memory size compared in number of floats in recurrent states.",
            "task_name": "Associative Retrieval (Remember & Rewrite), BABILong QA1-5, Language modeling",
            "task_category": "associative retrieval / long-context QA / language modeling",
            "performance_with_memory": "On associative retrieval Remember task Mamba outperforms RMT and is comparable to ARMT in some capacity metrics; on BABILong Mamba performs well up to its implementation limits (e.g., strong on some tasks at moderate lengths, but not evaluated at extreme lengths like 10M+ due to segmentation limits). Specific reported: Mamba (130M) QA1 at 64k/128k: ~100%/99.5% EM; degrades at longer lengths (e.g., 92.3% at 500k in table).",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM) and exact-match/recall for associative retrieval",
            "tradeoffs_reported": "More parallel and faster on short/medium sequences; implementation segmentation limits prevented evaluation at extreme long lengths (500k+); may experience slight degradation past training lengths on rewrite tasks.",
            "limitations_or_failure_cases": "Implementation limits prevented evaluating at extreme context lengths; experiences some degradation when exceeding training lengths on dynamic rewrite tasks compared to ARMT.",
            "citation": "Gu A., Dao T. (2023). Mamba: Linear-time sequence modeling with selective state spaces.",
            "uuid": "e6601.3",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "RWKV",
            "name_full": "RWKV (Reinventing RNNs for the Transformer era)",
            "brief_description": "An RNN-like model with transformer-style performance that has been used in long-context work; the authors attempted to train RWKV but failed to obtain reasonable scores for these benchmarks.",
            "citation_title": "RWKV: Reinventing RNNs for the transformer era",
            "mention_or_use": "mention",
            "agent_name": "RWKV",
            "agent_description": "RNN-structured model with linear-time recurrence and gating designed as an alternative to transformers for long contexts; in this paper the authors attempted but failed to train RWKV for associative retrieval and BABILong evaluations with their settings.",
            "model_size": "attempted 430M finetune for Babilong (reported attempt), associative-retrieval training from scratch (unspecified)",
            "memory_used": true,
            "memory_type": "Recurrent (RNN-like) state",
            "memory_representation": "Recurrent hidden states (time-mixed state vectors)",
            "memory_access_mechanism": "Sequential recurrent updates (RNN-style recurrence with gating)",
            "task_name": "Attempted on Associative Retrieval and BABILong (training failed)",
            "task_category": "associative retrieval / long-context QA",
            "performance_with_memory": "No reasonable scores achieved by authors with their training setup; training failed for both associative retrieval and BABILong (see Appendix J).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Authors note RWKV training may need careful adjustments; did not succeed under their hyperparameters; RWKV is more parallel on some regimes (per discussion of parallel methods).",
            "limitations_or_failure_cases": "Training instability or sensitivity: the authors were unable to train RWKV to solve the evaluated tasks with their settings.",
            "citation": "Peng B., et al. (2023). RWKV: Reinventing RNNs for the transformer era.",
            "uuid": "e6601.4",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 (few-shot)",
            "name_full": "GPT-4 (few-shot baseline)",
            "brief_description": "Large pretrained LLM used as a few-shot baseline on BABILong tasks (no special long-range memory mechanism applied in this paper).",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "GPT-4 (few-shot)",
            "agent_description": "Off-the-shelf GPT-4 used in few-shot prompting as a baseline; no additional memory augmentation described in this paper's experiments.",
            "model_size": null,
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_access_mechanism": null,
            "task_name": "BABILong QA1-5 (few-shot baseline)",
            "task_category": "long-context QA baseline (few-shot)",
            "performance_with_memory": "N/A (no memory augmentation in these few-shot runs). Reported few-shot EMs: e.g., QA1: 30.0% at 64k, 24.0% at 128k (table).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "Baseline few-shot LLM performance is low on extreme long-context queries compared to recurrent-memory models.",
            "limitations_or_failure_cases": "Per-table, poor performance on long-context single/multi-hop retrieval tasks compared to recurrent/associative-memory augmented models.",
            "citation": "",
            "uuid": "e6601.5",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-4 + RAG",
            "name_full": "GPT-4 with Retrieval-Augmented Generation (RAG) (few-shot baseline)",
            "brief_description": "GPT-4 combined with retrieval (RAG) used as a baseline that augments the model with external retrieval during few-shot evaluation on BABILong.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "GPT-4 + RAG",
            "agent_description": "Few-shot GPT-4 augmented with external retrieval (RAG) used as baseline; retrieval supplies context from an external store to the model at inference.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "Retrieval-augmented external storage (document retriever)",
            "memory_representation": "Retrieved text passages/documents",
            "memory_access_mechanism": "External retrieval (RAG) pipeline providing retrieved passages to GPT-4 for generation/answering",
            "task_name": "BABILong QA1-5 (few-shot baseline with RAG)",
            "task_category": "long-context QA with external retrieval",
            "performance_with_memory": "Reported EMs in table: e.g., QA1: 50.0% at 64k, 56.0% at 128k, 50.0% at 500k, 56.0% at 1M, 16.0% at 10M (few-shot + RAG).",
            "performance_without_memory": "Compared to GPT-4 few-shot: RAG generally improves EM at some lengths (e.g., QA1 50.0% vs 30.0% at 64k) but still lags behind ARMT at extreme lengths.",
            "has_comparative_results": true,
            "performance_metric": "Exact Match (EM)",
            "tradeoffs_reported": "RAG helps but still fails on extremely long contexts (performance drops at very long lengths, e.g., 10M), indicating retrieval alone may not suffice for multi-fact reasoning in extremely long contexts.",
            "limitations_or_failure_cases": "Performance degrades at very long context lengths (10M+); retrieval may not retrieve/combine multiple distant facts required for multi-hop reasoning in BABILong.",
            "citation": "",
            "uuid": "e6601.6",
            "source_info": {
                "paper_title": "Associative Recurrent Memory Transformer",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Recurrent memory transformer",
            "rating": 2
        },
        {
            "paper_title": "Mamba: Linear-time sequence modeling with selective state spaces",
            "rating": 2
        },
        {
            "paper_title": "Linear transformers are secretly fast weight programmers",
            "rating": 2
        },
        {
            "paper_title": "In search of needles in a 11m haystack: Recurrent memory finds what llms miss",
            "rating": 2
        },
        {
            "paper_title": "RWKV: Reinventing RNNs for the transformer era",
            "rating": 1
        },
        {
            "paper_title": "Memorizing Transformers",
            "rating": 1
        }
    ],
    "cost": 0.01580625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Associative Recurrent Memory Transformer</h1>
<p>Ivan Rodkin ${ }^{1}$ Yuri Kuratov ${ }^{2,1}$ Aydar Bulatov ${ }^{1}$ Mikhail Burtsev ${ }^{3}$<br>${ }^{1}$ Neural Networks and Deep Learning Lab, MIPT, Dolgoprudny, Russia<br>${ }^{2}$ AIRI, Moscow, Russia ${ }^{3}$ London Institute for Mathematical Sciences, London, UK<br>{rodkin.id,yurii.kuratov,bulatov.as}@phystech.edu, mb@lims.ac.uk</p>
<h4>Abstract</h4>
<p>This paper addresses the challenge of creating a neural architecture for very long sequences that requires constant time for processing new information at each time step. Our approach, Associative Recurrent Memory Transformer (ARMT), is based on transformer self-attention for local context and segment-level recurrence for storage of task specific information distributed over a long context. We demonstrate that ARMT outperfors existing alternatives in associative retrieval tasks and sets a new performance record in the recent BABILong multi-task long-context benchmark by answering single-fact questions over 50 million tokens with an accuracy of $79.9 \%$. The source code for training and evaluation is available on github.</p>
<h2>1. Introduction</h2>
<p>Memory plays a crucial role in creating models capable of processing extremely long contexts and utilizing remote past information. Starting from RNNs and evolving through LSTM [13] and Memory Networks [24, 28], we are now in the era of Transformer-based [27] Large Language Models [2, 17, 26]. Various methods for extending transformers context length have emerged [4, 19, 31], including approaches based on transformer segment-level recurrence [3, 5, 6, 20] and novel architectures that combine the efficiency of transformer parallelization during training with recurrence at inference [7, 8, 10, 11, 18]. Alternatively, Retrieval-Augmented Generation (RAG) focuses on retrieving information from external storage [1, 12, 23] or self-retrieving from past inputs [21, 29]. However, retrieval fails on complex tasks that require reasoning over multiple pieces of information [16].</p>
<p>In this work we propose the Associative Recurrent Memory Transformer (ARMT) as an extension of the segment-level recurrent model RMT [3] with associative memory. Compared to RWKV [18] and Mamba [10], which use association-based techniques, ARMT benefits from full local selfattention and has constant time and space complexity of processing new segment, similar to RMT. To study ARMT performance we use the BABILong [16] benchmark, because it allows to generate test samples up to 50 million tokens and beyond, compared to other methods. Additionally, we use Associative Retrieval task with multiple key-value pairs to estimate memory capacity of models.</p>
<p>Main contributions of this work include: (1) a novel ARMT architecture for long context processing with segment-level recurrence and associative memory; (2) demonstration that ARMT outcompetes existing memory based models like RMT [3] and Mamba [10] on associative retrieval and long context processing tasks, achieving $80 \%$ accuracy of single fact QA on unprecedented input</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ARMT augments the transformer's layers with associative memory. (a) RMT architecture. (b) ARMT adds associative memory processing to each layer. (c) Associative memory is updated with layerwise memory representations.
size of 50 million tokens; (3) an original method to evaluate memory capacity in associative retrieval task.</p>
<h1>2. Associative Recurrent Memory Transformer</h1>
<p>We extend RMT <a href="Fig. 1a">3</a> by addition of layerwise associative memory $A_{s}^{l}$ over segmented input $X_{s}^{l}$ (Fig. 1b). At every input segment $s$ for each layer $l$ memory tokens $M_{s-1}^{l+1}$ generated for preceding segment are added to $A_{s}^{l}$ (Fig. 1c) used to update input sequence and memory embeddings:</p>
<p>$$
\left[X_{s}^{l+1} ; M_{s}^{l+1}\right]=\operatorname{TrBlock}\left(\operatorname{AssocBlock}\left(\left[X_{s}^{l} ; M_{s}^{l}\right], A_{s}^{l}\right)\right) ; \quad A_{s}^{l}=\operatorname{MemUpdate}\left(A_{s-1}^{l} ; M_{s-1}^{l+1}\right)
$$</p>
<p>The mechanism of associative block (Fig. 1c) is similar to linear transformers [15], but attends only to special memory tokens and is calculated differenty. After each segment, memory tokens are converted to keys and values via linear mapping and then stored in quasi-linear key-value memory [22] using non-linearity $\phi$. Given a memory token $m_{i} \in M_{s}^{l+1}$, we calculate the keys, values, and importance scalars $\beta_{i}$. We then recall the previous association $\bar{v}<em i="i">{i}$ with this key, add the new value $v</em>}$ to the memory, erase the previous value $\bar{v<em i="i">{i}$ associated with $k</em>$, and update the normalization vector.</p>
<p>$$
\begin{gathered}
k_{i}, v_{i}=W_{K} m_{i}, W_{V} m_{i} ; \quad \beta_{i}=\sigma\left(W_{\beta} m_{i}\right) ; \quad A_{0}^{l}=\mathbf{0} ; \quad z_{0}^{l}=\mathbf{0} \
\bar{v}<em s-1="s-1">{i}=\frac{A</em> \
A_{s}^{l}=A_{s-1}^{l}+\sum_{i} \beta_{i}\left(v_{i}-\bar{v}}^{l} \phi\left(k_{i}\right)}{\left(z_{s-1}\right)^{T} \phi\left(k_{i}\right)} ; \quad \gamma_{i}=1-\frac{\left(z_{s-1}\right)^{T} \phi\left(k_{i}\right)}{\left|\phi\left(k_{i}\right)\right|^{2}<em i="i">{i}\right) \otimes \phi\left(k</em>\right)
\end{gathered}
$$}\right) ; \quad z_{s}^{l}=z_{s-1}^{l}+\sum_{i} \gamma_{i} \phi\left(k_{i</p>
<p>Once we updated $A_{s}^{l}$ with information from previous segment, we recall an association $y_{j}$ for a token $x_{j}$. Associations $y_{j}$ for each token in the segment are then passed to the next transformer layer:</p>
<p>$$
q_{j}=W_{Q} x_{j} ; \quad y_{j}=\frac{A_{s}^{l} \phi\left(q_{j}\right)}{\left(z_{s}^{l}\right)^{T} \phi\left(q_{j}\right)}
$$</p>
<p>For the non-linearity function $\phi$, we used the proposed in [22] DPFP-3 function because, in this paper, it has shown significant superiority over other methods, which is also consistent with our findings.</p>
<p>Note that without $\gamma_{i}\left(\gamma_{i}=1\right)$ this approach suffers from catastrophic forgetting on some tasks. The reason is that while we erase the information $\bar{v}_{i}$ from the $A$-matrix, the corresponding keys</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: ARMT demonstrates strong performance on associative memory tasks. (a) The estimated number of pairs, stored in memory after processing the context with key-value pairs. (b) ARMT is more accurate at operations in memory. Being trained only on 50 key-value pairs from Associative Retrieval Rewrite task, ARMT performs accurate even on 500 memory updates. So the observed generalization factor is 10 (500 pairs / 50 pairs). All data are averaged over 3 runs except RMT and PRMT with 2 runs.
remain in the normalization vector $z_{s}$. As shown in our experiments (Fig. 4(a)), this problem becomes significant when performing hundreds of erase-insert operations with associative memory. To overcome this, we propose to take into account the previous keys in $z_{s}$ when updating it (details are in Appendix F.1).</p>
<p>To determine which part contributes the most to ARMT performance, we also studied RMT with layerwise recurrent memory without associative memory block (Parallel Memory RMT, or PRMT; see Fig. 5 in Appendix F.2).</p>
<h1>3. Evaluation of Associative Retrieval and Long Context Memory Retention</h1>
<p>We test memory capacity of ARMT in comparison to recent computationally efficient long-context models Mamba [10] and RMT [3] on the following two variants of associative retrieval. ${ }^{1}$</p>
<p>Remember task requires memorization of all key-value pairs with unique keys from the prior context with subsequent recalling a value corresponding to one of the keys (Fig. 2a). We estimate the total number of key-value pairs stored in memory, based on the exact-match metric (details are in Appendix B). Since ARMT has the same recurrent memory size as Mamba, calculated as the number of floats in recurrent states, it can be concluded that ARMT makes better use of its internal associative memory. Both ARMT and Mamba outperform RMT on this task. PRMT does not improve RMT performance (Fig. 2a). This indicates that the associative memory plays a critical role in ARMT performance compared to RMT. Additionally, we ablated ARMT on normalization correction, as detailed in Appendix F.</p>
<p>In Rewrite task the keys are not unique and the goal is to recall the latest value that corresponds to one of the keys from the prior context. This task evaluates the model's ability to dynamically change the memory storage. The results, shown in Fig. 2b, indicate that ARMT is robust to the number of memory rewrite operations, while RMT and Mamba experience slight degradation after exceeding their training lengths. Notably, ARMT maintains perfect memory recall on lengths exceeding 10 times those used in training.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: ARMT sets a record in long-context processing with reasonable performance on 50 million tokens. Accuracy of models on different lengths from Babilong benchmark: panels a-e represent QA1-5 tasks.</p>
<p>We augment the GPT-2 (137M) model with ARMT to solve the BABILong tasks from a recently introduced benchmark for long context processing in a question-answering form [16]. To answer BABILong questions correctly, models have to find multiple relevant facts distributed across long natural contexts with distractor facts, and combine information from relevant facts. We use the exact match metric to evaluate models' performance. As shown in Fig. 3, ARMT outperforms the competitors in the majority of tasks, especially on long sequences. Being trained on 16 k tokens only, it strongly performs up to 50 million tokens on QA1 single supporting fact (Fig. 3a) and up to 10 million tokens on more complex tasks requiring multi-hop reasoning (Fig. 3b-e). We observe 60x length-generalization on these tasks ( $1 \mathrm{M} / 16 \mathrm{k}$ ), while Mamba has 8 x length-generalization (128k / 16k). For Mamba-130m we consider only the lengths up to 128 k due to its implementation limitations (see Appendix H, and Appendices C and D for training details).</p>
<h1>4. Conclusion</h1>
<p>In this work, we propose and evaluate recurrent memory transformer augmented with associative memory mechanism for long-context processing and find that it scales up to an unprecedented 50 million tokens on the BABILong benchmark. ARMT architecture adds an associative memory mechanism for segment-level recurrent model RMT. Based on our evaluation on associative retrieval tasks ARMT demonstrates significant advantage in memory capacity and generalisation compared to</p>
<p>Mamba and RMT. On BABILong benchmark ARMT dominates alternatives on medium sizes up to 500 K tokens and the only approach with high performance across all five tasks in the range of $500 \mathrm{~K}-10 \mathrm{M}$ tokens.</p>
<p>We conclude that ARMT holds great promise for long-range tasks because of its improved memory capacity, its ability to efficiently handle large numbers of rewrite operations with memory, its ability to extract only relevant information from memory during inference, and its generalization to much longer sequences than it was trained on. We also assume that the ARMT can be used for language modeling (Appendices $G$ and $K$ ). Despite the current results, we believe there is potential to enhance its performance on LM task through further research and optimization.</p>
<p>Since all of the results in this study are obtained on relatively small (137M) models, we also assume that the scaling of our methodology and its combination with other techniques can reveal the significant potential for modern large language models. We believe that investigating the properties of recurrent associative memory remains an exciting area of research.</p>
<h1>Acknowledgements</h1>
<p>We are thankful to SberDevices for granting us access to additional computational resources. This work was supported by a grant for research centers, provided by the Analytical Center for the Government of the Russian Federation in accordance with the subsidy agreement (agreement identifier 000000D730324P540002) and the agreement with the Moscow Institute of Physics and Technology dated November 1, 2021 No. 70-2021-00138.</p>
<h2>References</h2>
<p>[1] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206-2240. PMLR, 2022.
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
[3] Aydar Bulatov, Yuri Kuratov, and Mikhail S. Burtsev. Recurrent memory transformer, 2022.
[4] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In The Twelfth International Conference on Learning Representations, 2023.
[5] Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts, 2023.
[6] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, 2019 .</p>
<p>[7] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024.
[8] Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher Ré. Hungry hungry hippos: Towards language modeling with state space models, 2023.
[9] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800gb dataset of diverse text for language modeling, 2020.
[10] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.
[11] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021.
[12] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language model pre-training. In International conference on machine learning, pages 3929-3938. PMLR, 2020.
[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9 (8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735.
[14] Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying, 2024.
[15] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention, 2020.
[16] Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. In search of needles in a 11m haystack: Recurrent memory finds what llms miss, 2024.
[17] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,</p>
<p>Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024.
[18] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Jiaju Lin, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Bolun Wang, Johan S. Wind, Stanislaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. Rwkv: Reinventing rnns for the transformer era, 2023.
[19] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on</p>
<p>Learning Representations, 2023.
[20] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507.
[21] Ohad Rubin and Jonathan Berant. Long-range language modeling with self-retrieval. arXiv preprint arXiv:2306.13421, 2023.
[22] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers, 2021.
[23] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprint arXiv:2301.12652, 2023.
[24] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks, 2015.
[25] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive network: A successor to transformer for large language models, 2023.
[26] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[27] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in neural information processing systems, pages 5998-6008, 2017. URL http://papers.nips. cc/paper/7181-attention-is-all-you-need.
[28] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916.
[29] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=TrjbxzRcnf-.
[30] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks, 2023.
[31] Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, and Zhicheng Dou. Soaring from 4 k to 400k: Extending llm's context with activation beacon, 2024.</p>
<h1>Appendix A. Related Work</h1>
<p>AutoCompressor [5] is a strategy that stacks memory to minimize information loss at the price of quadratic computation cost.</p>
<p>Recurrent Memory Transformers Recent challenges in long-context processing tasks demonstrated recurrent memory superiority over attention mechanism [16, 31] (Fig. 1 (a)). It was shown that this type of memory performs well even in contexts of size 11M [16]. But still, this memory has some issues with capacity and training. Capacity remains limited as the suggested memory states are limited to a small number of memory tokens. The training is still challenging, as the whole training process requires backpropagation through time for hundreds of layers. Our approach is supposed to mitigate these problems by leveraging the association matrix as a connector for different segments. In contrast to RMT, it has different parameters for memory (linear attention projections described in Section 2) and makes this memory hierarchical by creating different association matrices for different layers.</p>
<p>Context explosion prevention In the attention sinks paper [30], authors demonstrated the need for some sinking tokens for attention for efficient extrapolation in long contexts. Recurrent Memory in RMT [3] as well as our model successfully perform this function. In RMT the memory tokens can act as attention sinks while in our model the very association matrix can play this role.</p>
<p>The ARMT can also be thought of as a kind of compressed-memory RMT [3], that attends to all previous memory tokens with layerwise memory.</p>
<p>Recurrent Sequence Models The problem of the quadratic cost of attention mechanism led to the development of recurrent architectures with transformer-like performance [7]. The vast majority of them are at least related to the State-Space Models (SSMs) [8, 10, 18, 25]. Despite comparable performance with transformers on LM tasks, SSMs are known to be less efficient in memorization tasks, especially when the question is asked after the information [14]. Our model performs well even on these types of tasks, because it has the large and flexible storage for keeping the associations in memory, simultaneously having the direct access to the local context via the vanilla attention.</p>
<h2>Appendix B. Memory capacity estimation</h2>
<h2>Theorem:</h2>
<p>Given:</p>
<p>$$
\text { exact_match }=\alpha ; \quad n=\text { number of pairs; } \quad v=\text { number of possible values }
$$</p>
<p>Then the number of memorized pairs can be estimated with the formula:</p>
<p>$$
k=\frac{n v \alpha-n}{v-1}
$$</p>
<h2>Proof:</h2>
<p>We can precisely predict the associated value if we remember k pairs and then extract the key from these pairs. We output the random value if we obtain the key from any other pair. As a result, the following is the mathematical expectation of an exact match:</p>
<p>$$
\alpha=\frac{k}{n} \cdot 1+\frac{n-k}{n} \cdot \frac{1}{v}=\frac{k}{n}\left(1-\frac{1}{v}\right)+\frac{1}{v}=\frac{k(v-1)+n}{n v}
$$</p>
<p>$$
k=\frac{n v \alpha-n}{v-1}
$$</p>
<h1>Additionally:</h1>
<p>$$
k=n \alpha \frac{v-\frac{1}{\alpha}}{v-1}=n \alpha \frac{v \alpha-1}{v \alpha-\alpha}=n \frac{v \alpha-1}{v-1}
$$</p>
<h2>Appendix C. Curriculum learning</h2>
<p>We train all models with curriculum learning. This means we incrementally increase the complexity of the task during the training. In particular, we train all models on short sequences first and then increase the length of the sequences until it reaches the maximum length ( 16 k tokens for babilong experiments, 200 pairs for Associative Retrieval Remember, 50 pairs for Associative Retireval Rewrite, and 1024 tokens for language modeling experiments ( 8 segments, 128 each)).</p>
<h2>Appendix D. Babilong training details</h2>
<p>We consider segments of size 512 for RMT and ARMT to process the long sequences. The curriculum learning process uses the following number of sequences consecutively: $2,3,5,8,16,32$. So the training ends when we finish training on 32 segments, 512 tokens each. We also randomly sample the number of segments during training, as we find it helps the model generalize better.</p>
<h2>Appendix E. Associative Retrieval training details</h2>
<p>Due to the task's simplicity and training efficiency, we are considering small models (about 500k parameters each) for the Associative Retrieval dataset studies. Every model that we compare has four layers. 128 is the hidden dimension. If the memory dimension parameter (state size in Mamba and memory dimension in ARMT) is present in the model, it is assumed to be 32 ; if the contrary is not indicated.</p>
<p>Moreover, if the model supports segmentation (like RMT and ARMT), we use different segments for different key-value pairs. Thus, if we have, for instance, 200 pairs, 200 segments are passed through the model, and after that, in the 201st segment, we expect the model to generate the value. Both keys and values consist of several integers from 0 to 15 .</p>
<p>We also use the curriculum with the following number of key-value pairs: $1,2,3,5,10,20,40$, 50 , and 200. We increase the key size if necessary, so the final key size for remember task is 3 (so we have $16^{3}$ unique keys) and for rewrite task it remains 1 (16 unique keys). For the Remember task, we also consider sampling different numbers of pairs during training for better generalization.</p>
<h2>Appendix F. Ablation</h2>
<h2>F.1. Gamma-correction</h2>
<p>Due to an improper normalization vector $z$ update, the proposed fast-weights technique [22] (also known as delta-rule) does not have the length generalization (Fig. 4(a)). The information in the association matrix $A$ is erased, but not from $z$, which is the source of the issue.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">64k</th>
<th style="text-align: center;">128k</th>
<th style="text-align: center;">500k</th>
<th style="text-align: center;">1M</th>
<th style="text-align: center;">10M</th>
<th style="text-align: center;">50M</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">QA1 - SINGLE SUPPORTING FACT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">30.0</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 + RAG (Few-shot)</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">16.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">94.2</td>
<td style="text-align: center;">76.4</td>
<td style="text-align: center;">- $\left(64,8^{*}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$100 \pm 0.0$</td>
<td style="text-align: center;">$99.5 \pm 0.2$</td>
<td style="text-align: center;">$92.3 \pm 1.1$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$100 \pm 0.0$</td>
<td style="text-align: center;">$99.9 \pm 0.2$</td>
<td style="text-align: center;">$99.3 \pm 0.9$</td>
<td style="text-align: center;">$98.5 \pm 1.0$</td>
<td style="text-align: center;">$89.4 \pm 8.1$</td>
<td style="text-align: center;">$49,6 \pm 40.4\left(79,9^{*}\right)$</td>
</tr>
<tr>
<td style="text-align: center;">QA2 - TWO SUPPORTING FACTS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">72.7</td>
<td style="text-align: center;">56.3</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">71,6</td>
<td style="text-align: center;">54,9</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">13.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$95.0 \pm 4.2$</td>
<td style="text-align: center;">$86.7 \pm 6.2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$100 \pm 0.0$</td>
<td style="text-align: center;">$99.8 \pm 0.2$</td>
<td style="text-align: center;">$99.4 \pm 0.3$</td>
<td style="text-align: center;">$99.4 \pm 0.3$</td>
<td style="text-align: center;">$84.4 \pm 4.0$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">QA3 - THREE SUPPORTING FACTS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">51.9</td>
<td style="text-align: center;">42.9</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">24.8</td>
<td style="text-align: center;">21.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">52.9</td>
<td style="text-align: center;">41.9</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$91.8 \pm 0.3$</td>
<td style="text-align: center;">$81.4 \pm 1.0$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$90.4 \pm 2.2$</td>
<td style="text-align: center;">$86.0 \pm 4.8$</td>
<td style="text-align: center;">$79.7 \pm 10.8$</td>
<td style="text-align: center;">$72.1 \pm 14.2$</td>
<td style="text-align: center;">$37.0 \pm 10.3$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">QA4 - TWO ARG RELATIONS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">36.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">51.2</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">17.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">58.8</td>
<td style="text-align: center;">50.1</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">26.0</td>
<td style="text-align: center;">14.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$99.7 \pm 0.2$</td>
<td style="text-align: center;">$97.6 \pm 2.8$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$100 \pm 0.1$</td>
<td style="text-align: center;">$100 \pm 0.1$</td>
<td style="text-align: center;">$99.9 \pm 0.2$</td>
<td style="text-align: center;">$99.8 \pm 0.3$</td>
<td style="text-align: center;">$91.5 \pm 1.7$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">QA5 - THREE ARG RELATIONS</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (Few-shot)</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT (137M)</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">48.0</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">RMT-R</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">77.4</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">49.9</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Mamba (130M)</td>
<td style="text-align: center;">$98.7 \pm 0.1$</td>
<td style="text-align: center;">$97.5 \pm 1.1$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">ARMT (145M)</td>
<td style="text-align: center;">$99.0 \pm 0.3$</td>
<td style="text-align: center;">$98.7 \pm 0.4$</td>
<td style="text-align: center;">$98.4 \pm 0.3$</td>
<td style="text-align: center;">$97.3 \pm 0.6$</td>
<td style="text-align: center;">$80.9 \pm 7.6$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 1: Exact match metric on QA1-5 Babilong subsets. Each column corresponds to some constant context length. Context includes both noise sentences and facts. * The 50M exact-match on QA1 is measured on 1 best model. ARMT rows are 3 runs averaged. Mamba rows are 2 runs averaged. The metric is marked bold if its $\pm$ std interval intersects the $\pm$ std interval of the best model.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Segmentation</th>
<th style="text-align: center;">Memory Capacity</th>
<th style="text-align: center;">Working Memory</th>
<th style="text-align: center;">LM</th>
<th style="text-align: center;">Length extrapolation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Mamba</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">RMT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">ARMT</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$?$</td>
<td style="text-align: center;">$\checkmark \checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 2: Models abilities.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) $\gamma$-correction cures the quasi-linear attention memory. Without correction, the quasi-linear attention with delta-rule struggles to extrapolate on unseen amounts of memory updates. (b) Parallel memory doesn't solve the capacity issue. This means that the associative memory plays an important role in increasing the capacity of the memory.</p>
<p>Note that $z$ is the sum of $\phi\left(k_{i}\right)$. Moreover, we recall the information from previous segments using the inner product of $\phi\left(k_{i}\right)$ and $\phi\left(q_{i}\right)$. This means that for accurate recall, all $\phi\left(k_{i}\right)$ should be orthogonal to each other. Therefore, we can expect $z$ to be a sum of approximately orthogonal vectors.</p>
<p>In this sense, we simplify our task to the task of removing the $\phi\left(k_{i}\right)$ from the sum of vectors orthogonal to $\phi\left(k_{i}\right)$, with the exception of the very $\phi\left(k_{i}\right)$. This means that the presence of $\phi\left(k_{i}\right)$ can be measured by computing the inner product between this sum and $\phi\left(k_{i}\right)$ and dividing it by the length of $\phi\left(k_{i}\right)$ (just taking an orthogonal basis component).</p>
<p>After the insertion of the new information into our memory, we expect this sum to include only one $\phi\left(k_{i}\right)$. Therefore, our $\gamma$-coefficient can be computed with the following formula:</p>
<p>$$
\begin{aligned}
&amp; \gamma_{i}=1-\frac{\left(z_{s-1}\right)^{T} \phi\left(k_{i}\right)}{\left|\phi\left(k_{i}\right)\right|^{2}} \
&amp; z_{s}=z_{s-1}+\sum_{i} \gamma_{i} \phi\left(k_{i}\right)
\end{aligned}
$$</p>
<p>The inner product $\left(z_{s-1}\right)^{T} \phi\left(k_{i}\right)$ is divided by the square of $\left|\phi\left(k_{i}\right)\right|$ because the gamma will be further multiplied by $\phi\left(k_{i}\right)$.</p>
<p>We also consider detaching the gamma during training, because it seems to converge better in this case.</p>
<h1>F.2. Associative memory ablation</h1>
<p>To understand, if the associative memory important for memorization tasks, we consider another architecture: Parallel-memory RMT (PRMT) Fig. 5</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Parallel recurrent memory transformer. In contrast to RMT, in PRMT memory tokens are passed to the next segment in each layer.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: ARMT performs similarly to RMT on the language modeling task. Wikitext-103 results. The loss on each of the 128 -sized segments on the test dataset (Normalized with Bits-per-byte [9]). The model is trained for a language modeling task on 8 segments, 128 tokens each. Despite the larger estimated capacity, ARMT struggles to solve the LM task well.</p>
<p>It is different from RMT in the hierarchical memory approach, considering the memory shifts layerwise, just like in ARMT. So this architecture can be thought of as RMT with layerwise memory, while ARMT is RMT with layerwise memory organized in association matrices.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Current mamba implementation allows only the first segment to be long. Other tokens have to be processed consecutively one by one.</p>
<h1>Appendix G. Language Modeling experiments</h1>
<p>We utilized the Wikitext-103 dataset to train ARMT and RMT models in order to assess our architecture's performance on real texts. Next, we examined the cross-entropy losses derived from various model segments on the test dataset, as illustrated in Figure 6. In this manner, we can estimate the amount of language data that can be stored in memory.</p>
<p>Nevertheless, we demonstrate that despite having a larger theoretical capacity than RMT, ARMT still performs similarly to RMT in language modeling.</p>
<p>We use the GPT-2 model as the base model for our architecture changes. We consider the RMT and ARMT models' segment sizes to be equal to 128 tokens and train these models to solve the language modeling task on 8 segments, so in total, we train the model to autoregressively predict 1024 tokens. Then we evaluate the models' performance on each of the 15 segments of test texts (1920 tokens).</p>
<h2>Appendix H. Why is mamba slow for long contexts?</h2>
<p>We faced some difficulties in evaluating mamba on long-context (500k+) due to it's specific segmentation abilities shown in Figure 7.</p>
<h2>Appendix I. Associative Retrieval sample structure</h2>
<p>A sample of Remember dataset contains a concatenated context, query, and answer. The context is a set of key-value pairs $(k, v)$, separated by a special token. All keys are sequences of tokens. Tokens in this sequence can intersect, but the whole sequence corresponding to any key is unique in this particular sample. The query is one of the keys in the context. And the answer is a value corresponding to the key from the query. Thus, we can control the number of pairs in the sample and check how many pairs fit in our memory.</p>
<p>This is how the dataset's sample appears:
<key1>:<value1>,<key2>:<value2>, <key3>:<value3>,<key2>-<value2>
The model is thought to be trained to produce the value following the " - " character.</p>
<h1>Appendix J. RWKV-5 model</h1>
<p>We also tried to train the RWKV-v5 [18] model to slove both associative retrieval and babilong tasks (training from scratch for AR tasks and finetuning 430M model for babilong task). Unfortunately, the model was training poorly and hadn't achieved reasonable scores. We used the very same parameters as for training other models. Perhaps the RWKV training process requires accurate adjustments. However, we haven't succeeded.</p>
<h2>Appendix K. Limitations</h2>
<p>The proposed architecture, however, has several drawbacks. The first is the lack of efficient parallel implementation: you have to process all segments consecutively. This doesn't mean that it is slow. It's fast enough to process millions of tokens in a reasonable time. However, on short and medium-length sequences (less than 300k tokens), it is much slower than, for instance, Mamba and RWKV, which have the parallel form. Moreover, as we have shown in Appendix G, it's still challenging to train ARMT to solve the language modeling task well. However, we believe that this problem is not in the very architecture, but in training process: we observe that ARMT tends to keep in memory only the last segment, and therefore struggles to extrapolate on longer sequences.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>We did our best but failed to train RWKV model for associative retrieval and BABILong benchmarks, Appendix J.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9160 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9160</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9160</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-274305736</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.18071v1.pdf" target="_blank">Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities</a></p>
                <p><strong>Paper Abstract:</strong> Do horror writers have worse childhoods than other writers? Though biographical details are known about many writers, quantitatively exploring such a qualitative hypothesis requires significant human effort, e.g. to sift through many biographies and interviews of writers and to iteratively search for quantitative features that reflect what is qualitatively of interest. This paper explores the potential to quickly prototype these kinds of hypotheses through (1) applying LLMs to estimate properties of concrete entities like specific people, companies, books, kinds of animals, and countries; (2) performing off-the-shelf analysis methods to reveal possible relationships among such properties (e.g. linear regression); and towards further automation, (3) applying LLMs to suggest the quantitative properties themselves that could help ground a particular qualitative hypothesis (e.g. number of adverse childhood events, in the context of the running example). The hope is to allow sifting through hypotheses more quickly through collaboration between human and machine. Our experiments highlight that indeed, LLMs can serve as useful estimators of tabular data about specific entities across a range of domains, and that such estimations improve with model scale. Further, initial experiments demonstrate the potential of LLMs to map a qualitative hypothesis of interest to relevant concrete variables that the LLM can then estimate. The conclusion is that LLMs offer intriguing potential to help illuminate scientifically interesting patterns latent within the internet-scale data they are trained upon.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9160.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9160.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zoo-domain simulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-driven Simulation of UCI Zoo Dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This experiment used an LLM to simulate binary biological features for 101 animal entities (the UCI Zoo Dataset) and to predict animal class labels, evaluating how well relationships in simulated tabular data match ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4o-mini (used in the paper as the primary simulator for this experiment); described as a high-capability OpenAI LLM (no parameter count reported in-paper).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>computational ecology / taxonomy (bioinformatics-style tabular biological datasets)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>For each animal name, predict 16 binary features (e.g., hair, feathers, teeth) and the categorical animal type; generate a simulated tabular dataset to support downstream predictive modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Per-feature accuracy (binary accuracy) for independent variables; perfect-label accuracy for class labels; logistic regression generalization accuracy; Matthews correlation coefficient absolute-difference when LLM asked to output correlations directly; Simulation Error Gap (difference in generalization error between models trained on simulated vs ground-truth data).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Average per-feature accuracy = 0.923; class-label (animal type) simulation achieved perfect accuracy; logistic regression trained on simulated data achieved 0.833 accuracy on real validation vs 0.933 accuracy when trained on real data → Simulation Error Gap = 0.10; when LLM was asked to directly output correlation coefficients, average absolute difference = 0.321 (vs true coefficients).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Ambiguous or underspecified property descriptions (e.g., 'catsize') reduced accuracy; clarity of property semantics improves simulation; (paper-wide) model scale/capability tends to improve fidelity (discussed across domains); structured output format and direct prompting used in this experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Ground-truth UCI Zoo dataset and models trained on it (upper-bound). The LLM-trained simulated-data model (logistic regression) underperformed the real-data-trained model by 0.10 accuracy; direct LLM correlation estimates performed substantially worse (abs diff 0.321) than simulating data then analyzing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Errors concentrated on ambiguously-named features; direct estimation of relationships (asking LLM to output correlation coefficients) gave poor agreement with true correlations; fidelity depends on clarity of property definitions and likely presence of information in model training data.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Provide clear semantic descriptions for properties to simulate; prefer simulating structured data and running external analysis (simulate-then-analyze) rather than asking LLMs to directly estimate relationships; use structured output formats (Pythonic dictionary) for consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9160.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9160.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Countries-domain simulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-driven Simulation of Country Demographics and Egalitarian Democracy Index (EDI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLMs were used to estimate continuous demographic and governance indicators for countries and to simulate EDI scores, enabling rapid hypothesis exploration about demographic correlates of egalitarian democracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-8B, LLaMA-3-70B, GPT-4o-mini (compared)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Comparative use of three LLMs: LLaMA-3-8B (8B parameters), LLaMA-3-70B (70B parameters) and GPT-4o-mini (high-capability OpenAI model used in this paper); exact training corpora not detailed in-paper for each model but LLaMA-3 family and GPT-4o-mini are identified.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>political science / comparative politics / demography / computational social science</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate continuous country-level features (demographic percentages, governance percentile ranks, mortality rates, etc.) for sampled countries and predict Egalitarian Democracy Index (EDI) values from simulated predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average correlation between simulated and ground-truth predictor variables; Mean Absolute Error (MAE) for simulated EDI; Median Absolute Error (MedAE) for regression generalization error; Simulation Error Gap (difference in generalization error between models trained on simulated vs real data).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported per-model aggregated results (Table 1): LLaMA-3-8B average correlation = 0.221, EDI MAE = 0.134, Sim. Error Gap = 0.064; LLaMA-3-70B average correlation = 0.644, EDI MAE = 0.189, Sim. Error Gap = 0.036; GPT-4o-mini average correlation = 0.738, EDI MAE = 0.119, Sim. Error Gap = 0.036. Prompting-strategy best-case (Table 2): direct-structured average correlation = 0.770, EDI MAE = 0.132, Sim. Error Gap = 0.011.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model scale/capability strongly affects fidelity (larger models yield higher correlation and lower error gaps); prompting strategy and output format (direct querying with structured/Pythonic dictionary output outperformed report-style prompts); property complexity and ambiguity (complex constructs like percentile upper bounds are harder); attempts like conditioning EDI on previously simulated properties and few-shot grounding produced inconsistent benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparison across LLaMA-3-8B, LLaMA-3-70B, and GPT-4o-mini (GPT-4o-mini performed best overall). Ground-truth-trained model performance serves as upper-bound baseline; direct LLM estimation of correlations (no simulation) produced large average error (0.483 difference) compared to simulation-then-analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Difficulty handling highly specialized or ambiguous features; no single augmentation (e.g., conditioning, few-shot, outlier conditioning) consistently improved results; estimation of continuous, narrowly ranged or confidence-interval-based features remains challenging; fidelity depends on presence of feature information in LLM training data.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Use larger/more capable models for higher fidelity; prefer direct-structured prompts requesting values in a structured format (Pythonic dictionary) over report-style prompts; simulate structured datasets and run external analyses rather than asking LLMs to directly output relationship summaries; further grounding (e.g., retrieval augmentation) could improve accuracy but was not implemented here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9160.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9160.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Athletes / Hypothesis-driven simulation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hypothesis-driven Dataset Simulation for Athletes: Injuries and Peak Performance Age</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM pipeline mapped a qualitative hypothesis about sport type, injuries, and peak performance age into concrete properties and entities, then simulated those properties for soccer and tennis players to evaluate alignment with real-world data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-4o-mini used for both hypothesis mapping (generating properties and entities) and property-value simulation; model training details not specified in-paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>sports science / applied sports analytics / computational social science</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Given a hypothesis and a list of 40 athletes (20 soccer, 20 tennis), automatically generate and simulate numeric properties: number of major injuries (lasting >2 months) and peak performance age for each athlete; produce a simulated dataset for regression analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Pearson correlations between simulated and actual properties; Mean Absolute Error (MAE) used for simulation error gap and for regression generalization error on held-out simulated data.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>With self-correction: correlation for peak performance age = 0.625, for total number of major injuries = 0.581; Simulation Error Gap (MAE) = 1.325. Without self-correction: age correlation = 0.570, injuries correlation = 0.557; Simulation Error Gap = 1.791 MAE. (Validation sources: Tennis Explorer and Transfermarkt for injuries; Perplexity for peak-age ground truth.)</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Use of self-correction (iterative self-refinement) improved correlations and reduced simulation error gap; quality and availability of external ground-truth sources for validation (Perplexity RAG vs direct LLM queries) affect measured fidelity; the representativeness of chosen properties to explain labels (features may inadequately explain target labels) limits improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Comparison made to real-world collected values (Tennis Explorer, Transfermarkt, Perplexity-retrieved values) — simulated correlations reported against these real values; self-correction improved performance vs no self-correction. Perplexity (RAG-enabled engine) used as a validation data source and shown in spot checks to outperform a standard ChatGPT query for factual extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although self-correction improved correlations, improvements translated to only modest reductions in Simulation Error Gap; overlapping error bars indicate uncertainty in generalization-error improvements; simulated properties may not fully explain the outcome variables, constraining downstream modeling effectiveness; reliance on LLM internal knowledge can miss sparse or obscure facts.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Apply self-correction (iterative refinement) to improve simulated property fidelity; consider retrieval-augmented grounding (RAG) for validation; treat LLM-simulated datasets as exploratory prototypes that should prompt manual curation or further data collection when strong signals are found.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9160.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9160.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Argyle et al. 2023 (background)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Out of One, Many: Using Language Models to Simulate Human Samples</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work demonstrating that LLMs can represent diverse human subpopulations and simulate survey-result probabilities conditioned on demographic attributes (used as motivating background that LLMs can act as human-behavior simulators).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Out of One, Many: Using Language Models to Simulate Human Samples</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>computational social science / survey simulation / behavioral simulation</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Represent diverse human subpopulations and simulate probabilities of survey responses given demographic inputs (e.g., predicting voting behavior given race, gender, political affiliation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Not specified in this paper's text beyond reference to replication of survey probabilities; this paper only cites the prior work as demonstration of potential.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9160.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9160.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aher et al. 2023 (background)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior work showing LLMs can replicate established findings from human-subject experiments by simulating human responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>experimental psychology / computational behavioral science</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate human subject experiment responses to reproduce well-established findings from prior studies.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9160.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9160.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hu et al. 2023 (background)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited example where LLMs are used to simulate user satisfaction scores and optimize dialogue systems, demonstrating application of LLMs as user simulators in conversational AI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>dialogue systems / human-computer interaction / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate user satisfaction scores and user interactions to optimize a dialogue system.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9160.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9160.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cohen et al. 2023 (background)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Crawling the Internal Knowledge-Base of Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work showing extraction of structured knowledge from LLMs (knowledge graph construction), supporting the idea that LLMs can be sources for simulated structured datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Crawling the Internal Knowledge-Base of Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>knowledge extraction / information retrieval / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Extract structured knowledge from LLMs to build knowledge graphs and structured datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Out of One, Many: Using Language Models to Simulate Human Samples <em>(Rating: 2)</em></li>
                <li>Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies <em>(Rating: 2)</em></li>
                <li>Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System <em>(Rating: 1)</em></li>
                <li>Crawling the Internal Knowledge-Base of Language Models <em>(Rating: 1)</em></li>
                <li>Self-Refine: Iterative Refinement with Self-Feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9160",
    "paper_id": "paper-274305736",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "Zoo-domain simulation",
            "name_full": "LLM-driven Simulation of UCI Zoo Dataset",
            "brief_description": "This experiment used an LLM to simulate binary biological features for 101 animal entities (the UCI Zoo Dataset) and to predict animal class labels, evaluating how well relationships in simulated tabular data match ground truth.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini",
            "model_description": "GPT-4o-mini (used in the paper as the primary simulator for this experiment); described as a high-capability OpenAI LLM (no parameter count reported in-paper).",
            "scientific_subdomain": "computational ecology / taxonomy (bioinformatics-style tabular biological datasets)",
            "simulation_task": "For each animal name, predict 16 binary features (e.g., hair, feathers, teeth) and the categorical animal type; generate a simulated tabular dataset to support downstream predictive modeling.",
            "evaluation_metric": "Per-feature accuracy (binary accuracy) for independent variables; perfect-label accuracy for class labels; logistic regression generalization accuracy; Matthews correlation coefficient absolute-difference when LLM asked to output correlations directly; Simulation Error Gap (difference in generalization error between models trained on simulated vs ground-truth data).",
            "simulation_accuracy": "Average per-feature accuracy = 0.923; class-label (animal type) simulation achieved perfect accuracy; logistic regression trained on simulated data achieved 0.833 accuracy on real validation vs 0.933 accuracy when trained on real data → Simulation Error Gap = 0.10; when LLM was asked to directly output correlation coefficients, average absolute difference = 0.321 (vs true coefficients).",
            "factors_affecting_accuracy": "Ambiguous or underspecified property descriptions (e.g., 'catsize') reduced accuracy; clarity of property semantics improves simulation; (paper-wide) model scale/capability tends to improve fidelity (discussed across domains); structured output format and direct prompting used in this experiment.",
            "comparison_baseline": "Ground-truth UCI Zoo dataset and models trained on it (upper-bound). The LLM-trained simulated-data model (logistic regression) underperformed the real-data-trained model by 0.10 accuracy; direct LLM correlation estimates performed substantially worse (abs diff 0.321) than simulating data then analyzing.",
            "limitations_or_failure_cases": "Errors concentrated on ambiguously-named features; direct estimation of relationships (asking LLM to output correlation coefficients) gave poor agreement with true correlations; fidelity depends on clarity of property definitions and likely presence of information in model training data.",
            "author_recommendations_or_insights": "Provide clear semantic descriptions for properties to simulate; prefer simulating structured data and running external analysis (simulate-then-analyze) rather than asking LLMs to directly estimate relationships; use structured output formats (Pythonic dictionary) for consistency.",
            "uuid": "e9160.0",
            "source_info": {
                "paper_title": "Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Countries-domain simulation",
            "name_full": "LLM-driven Simulation of Country Demographics and Egalitarian Democracy Index (EDI)",
            "brief_description": "LLMs were used to estimate continuous demographic and governance indicators for countries and to simulate EDI scores, enabling rapid hypothesis exploration about demographic correlates of egalitarian democracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-8B, LLaMA-3-70B, GPT-4o-mini (compared)",
            "model_description": "Comparative use of three LLMs: LLaMA-3-8B (8B parameters), LLaMA-3-70B (70B parameters) and GPT-4o-mini (high-capability OpenAI model used in this paper); exact training corpora not detailed in-paper for each model but LLaMA-3 family and GPT-4o-mini are identified.",
            "scientific_subdomain": "political science / comparative politics / demography / computational social science",
            "simulation_task": "Simulate continuous country-level features (demographic percentages, governance percentile ranks, mortality rates, etc.) for sampled countries and predict Egalitarian Democracy Index (EDI) values from simulated predictors.",
            "evaluation_metric": "Average correlation between simulated and ground-truth predictor variables; Mean Absolute Error (MAE) for simulated EDI; Median Absolute Error (MedAE) for regression generalization error; Simulation Error Gap (difference in generalization error between models trained on simulated vs real data).",
            "simulation_accuracy": "Reported per-model aggregated results (Table 1): LLaMA-3-8B average correlation = 0.221, EDI MAE = 0.134, Sim. Error Gap = 0.064; LLaMA-3-70B average correlation = 0.644, EDI MAE = 0.189, Sim. Error Gap = 0.036; GPT-4o-mini average correlation = 0.738, EDI MAE = 0.119, Sim. Error Gap = 0.036. Prompting-strategy best-case (Table 2): direct-structured average correlation = 0.770, EDI MAE = 0.132, Sim. Error Gap = 0.011.",
            "factors_affecting_accuracy": "Model scale/capability strongly affects fidelity (larger models yield higher correlation and lower error gaps); prompting strategy and output format (direct querying with structured/Pythonic dictionary output outperformed report-style prompts); property complexity and ambiguity (complex constructs like percentile upper bounds are harder); attempts like conditioning EDI on previously simulated properties and few-shot grounding produced inconsistent benefits.",
            "comparison_baseline": "Comparison across LLaMA-3-8B, LLaMA-3-70B, and GPT-4o-mini (GPT-4o-mini performed best overall). Ground-truth-trained model performance serves as upper-bound baseline; direct LLM estimation of correlations (no simulation) produced large average error (0.483 difference) compared to simulation-then-analysis.",
            "limitations_or_failure_cases": "Difficulty handling highly specialized or ambiguous features; no single augmentation (e.g., conditioning, few-shot, outlier conditioning) consistently improved results; estimation of continuous, narrowly ranged or confidence-interval-based features remains challenging; fidelity depends on presence of feature information in LLM training data.",
            "author_recommendations_or_insights": "Use larger/more capable models for higher fidelity; prefer direct-structured prompts requesting values in a structured format (Pythonic dictionary) over report-style prompts; simulate structured datasets and run external analyses rather than asking LLMs to directly output relationship summaries; further grounding (e.g., retrieval augmentation) could improve accuracy but was not implemented here.",
            "uuid": "e9160.1",
            "source_info": {
                "paper_title": "Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Athletes / Hypothesis-driven simulation",
            "name_full": "Hypothesis-driven Dataset Simulation for Athletes: Injuries and Peak Performance Age",
            "brief_description": "An LLM pipeline mapped a qualitative hypothesis about sport type, injuries, and peak performance age into concrete properties and entities, then simulated those properties for soccer and tennis players to evaluate alignment with real-world data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4o-mini",
            "model_description": "GPT-4o-mini used for both hypothesis mapping (generating properties and entities) and property-value simulation; model training details not specified in-paper.",
            "scientific_subdomain": "sports science / applied sports analytics / computational social science",
            "simulation_task": "Given a hypothesis and a list of 40 athletes (20 soccer, 20 tennis), automatically generate and simulate numeric properties: number of major injuries (lasting &gt;2 months) and peak performance age for each athlete; produce a simulated dataset for regression analysis.",
            "evaluation_metric": "Pearson correlations between simulated and actual properties; Mean Absolute Error (MAE) used for simulation error gap and for regression generalization error on held-out simulated data.",
            "simulation_accuracy": "With self-correction: correlation for peak performance age = 0.625, for total number of major injuries = 0.581; Simulation Error Gap (MAE) = 1.325. Without self-correction: age correlation = 0.570, injuries correlation = 0.557; Simulation Error Gap = 1.791 MAE. (Validation sources: Tennis Explorer and Transfermarkt for injuries; Perplexity for peak-age ground truth.)",
            "factors_affecting_accuracy": "Use of self-correction (iterative self-refinement) improved correlations and reduced simulation error gap; quality and availability of external ground-truth sources for validation (Perplexity RAG vs direct LLM queries) affect measured fidelity; the representativeness of chosen properties to explain labels (features may inadequately explain target labels) limits improvements.",
            "comparison_baseline": "Comparison made to real-world collected values (Tennis Explorer, Transfermarkt, Perplexity-retrieved values) — simulated correlations reported against these real values; self-correction improved performance vs no self-correction. Perplexity (RAG-enabled engine) used as a validation data source and shown in spot checks to outperform a standard ChatGPT query for factual extraction.",
            "limitations_or_failure_cases": "Although self-correction improved correlations, improvements translated to only modest reductions in Simulation Error Gap; overlapping error bars indicate uncertainty in generalization-error improvements; simulated properties may not fully explain the outcome variables, constraining downstream modeling effectiveness; reliance on LLM internal knowledge can miss sparse or obscure facts.",
            "author_recommendations_or_insights": "Apply self-correction (iterative refinement) to improve simulated property fidelity; consider retrieval-augmented grounding (RAG) for validation; treat LLM-simulated datasets as exploratory prototypes that should prompt manual curation or further data collection when strong signals are found.",
            "uuid": "e9160.2",
            "source_info": {
                "paper_title": "Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Argyle et al. 2023 (background)",
            "name_full": "Out of One, Many: Using Language Models to Simulate Human Samples",
            "brief_description": "Cited work demonstrating that LLMs can represent diverse human subpopulations and simulate survey-result probabilities conditioned on demographic attributes (used as motivating background that LLMs can act as human-behavior simulators).",
            "citation_title": "Out of One, Many: Using Language Models to Simulate Human Samples",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "computational social science / survey simulation / behavioral simulation",
            "simulation_task": "Represent diverse human subpopulations and simulate probabilities of survey responses given demographic inputs (e.g., predicting voting behavior given race, gender, political affiliation).",
            "evaluation_metric": "Not specified in this paper's text beyond reference to replication of survey probabilities; this paper only cites the prior work as demonstration of potential.",
            "simulation_accuracy": null,
            "factors_affecting_accuracy": null,
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9160.3",
            "source_info": {
                "paper_title": "Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Aher et al. 2023 (background)",
            "name_full": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
            "brief_description": "Cited prior work showing LLMs can replicate established findings from human-subject experiments by simulating human responses.",
            "citation_title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "experimental psychology / computational behavioral science",
            "simulation_task": "Simulate human subject experiment responses to reproduce well-established findings from prior studies.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": null,
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9160.4",
            "source_info": {
                "paper_title": "Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Hu et al. 2023 (background)",
            "name_full": "Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System",
            "brief_description": "Cited example where LLMs are used to simulate user satisfaction scores and optimize dialogue systems, demonstrating application of LLMs as user simulators in conversational AI.",
            "citation_title": "Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "dialogue systems / human-computer interaction / NLP",
            "simulation_task": "Simulate user satisfaction scores and user interactions to optimize a dialogue system.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": null,
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9160.5",
            "source_info": {
                "paper_title": "Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Cohen et al. 2023 (background)",
            "name_full": "Crawling the Internal Knowledge-Base of Language Models",
            "brief_description": "Cited work showing extraction of structured knowledge from LLMs (knowledge graph construction), supporting the idea that LLMs can be sources for simulated structured datasets.",
            "citation_title": "Crawling the Internal Knowledge-Base of Language Models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "knowledge extraction / information retrieval / NLP",
            "simulation_task": "Extract structured knowledge from LLMs to build knowledge graphs and structured datasets.",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": null,
            "comparison_baseline": null,
            "limitations_or_failure_cases": null,
            "author_recommendations_or_insights": null,
            "uuid": "e9160.6",
            "source_info": {
                "paper_title": "Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Out of One, Many: Using Language Models to Simulate Human Samples",
            "rating": 2,
            "sanitized_title": "out_of_one_many_using_language_models_to_simulate_human_samples"
        },
        {
            "paper_title": "Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies",
            "rating": 2,
            "sanitized_title": "using_large_language_models_to_simulate_multiple_humans_and_replicate_human_subject_studies"
        },
        {
            "paper_title": "Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System",
            "rating": 1,
            "sanitized_title": "unlocking_the_potential_of_user_feedback_leveraging_large_language_model_as_user_simulators_to_enhance_dialogue_system"
        },
        {
            "paper_title": "Crawling the Internal Knowledge-Base of Language Models",
            "rating": 1,
            "sanitized_title": "crawling_the_internal_knowledgebase_of_language_models"
        },
        {
            "paper_title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        }
    ],
    "cost": 0.01310675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities</p>
<p>Miguel Zabaleta mzabaletasar@gmail.com 
Stochastic Labs</p>
<p>Joel Lehman lehman.154@gmail.com 
Stochastic Labs</p>
<p>Simulating Tabular Datasets through LLMs to Rapidly Explore Hypotheses about Real-World Entities
3F23704BBF843CF3183B9EBBDD4CEB3C
Do horror writers have worse childhoods than other writers?Though biographical details are known about many writers, quantitatively exploring such a qualitative hypothesis requires significant human effort, e.g. to sift through many biographies and interviews of writers and to iteratively search for quantitative features that reflect what is qualitatively of interest.This paper explores the potential to quickly prototype these kinds of hypotheses through (1) applying LLMs to estimate properties of concrete entities like specific people, companies, books, kinds of animals, and countries; (2) performing off-the-shelf analysis methods to reveal possible relationships among such properties (e.g.linear regression); and towards further automation, (3) applying LLMs to suggest the quantitative properties themselves that could help ground a particular qualitative hypothesis (e.g.number of adverse childhood events, in the context of the running example).The hope is to allow sifting through hypotheses more quickly through collaboration between human and machine.Our experiments highlight that indeed, LLMs can serve as useful estimators of tabular data about specific entities across a range of domains, and that such estimations improve with model scale.Further, initial experiments demonstrate the potential of LLMs to map a qualitative hypothesis of interest to relevant concrete variables that the LLM can then estimate.The conclusion is that LLMs offer intriguing potential to help illuminate scientifically interesting patterns latent within the internet-scale data they are trained upon.</p>
<p>Introduction</p>
<p>While science often involves generating new data to explore hypotheses, we likely underappreciate what possible insights hide in plain sight within the vast expanse of already-existing data.One reason we might expect this is that it takes significant human effort to unearth and clarify hypotheses from diverse data sources.For example, there exist many written biographies, which in aggregate may speak to important patterns of the human condition, e.g.how and if aspects of childhood experience relate to adult life choices or relationship, or how personality and mental health interact.However, such information is unstructured and potentially spread across many different texts; for many questions of interest no one has yet made the effort to curate a from such diverse sources a specific dataset.</p>
<p>To explore these kinds of questions quantitatively within existing data requires: (1) seeking quantitative variables that are indicative of more qualitative properties of interest (e.g.how many adverse childhood experiences, or ACEs (Boullier and Blair 2018) a specific person experienced, or estimating their OCEAN personality traits (Roccas et al. 2002)); and (2) sifting through diverse unstructured collections of text to ground or estimate those quantities (e.g.reading several biographies of a figure to count their ACEs).To approach both steps manually often requires significant labor, domain expertise, and trial and error.As a result of these costs, we do not thoroughly mine what lies latent within existing data.</p>
<p>Interestingly, large language models (LLMs) are trained over an enormous corpus of human cultural output, and continue to advance in their capabilities to inexpensively answer arbitrary queries about specific entities.Thus, the main idea in this paper is to leverage LLMs for quick-and-dirty explorations of hypotheses about real-world entities (like people, countries, books, and activities).In particular, given a highlevel hypothesis (such as "Do horror writers have worse childhoods than other authors?"), an LLM can (1) suggest quantitative variables to ground such a hypothesis that are plausibly within its training corpus (e.g."Did this person's parents get a divorce"), (2) generate a list of concrete entities (e.g. 100 well-known horror writers and 100 well-known writers of other genres), and (3) estimate the concrete variables for each entity (e.g."Did Steven King's parents get a divorce?").</p>
<p>In this way, from an initial rough idea, an LLM can generate an approximate artisanal dataset, providing a preliminary way of exploring the hypothesis.The hope is that this estimation, while not perfect, could serve as an accelerant for active brainstorming, and could fit into a larger pipeline of science.For example, correlations between variables could also be automatically calculated in the simulated dataset, and if a strong and interesting correlation is found, it could motivate the effort to curate by hand a validated dataset, or to gather new data in service of the hypothesis (e.g. a more controlled survey of aspiring writers and their ACE scores).Because this kind of LLM generation (for a moderate-sized dataset) is inexpensive and fast, it can enable faster iteration arXiv:2411.18071v1[cs.AI] 27 Nov 2024 cycles of hypothesis brainstorming and debugging.</p>
<p>The experiments in this paper focus mainly on step (3) above (e.g.estimating concrete variables for concrete entities), although they also touch on steps (1) and (2).In particular, we find across several domains that indeed, LLMs can generate useful datasets about real-world entities, and that such datasets increase in fidelity with model scale.We also show in a preliminary experiment that LLMs can also translate high-level hypotheses into concrete variables, and that (perhaps unsurprisingly) they are adept at creating lists of entities appropriate for exploring a hypothesis (e.g.like horror writers).To enable the explorations of others, we release code here: https://github.com/mzabaletasar/llm hypoth simulation.</p>
<p>The conclusion is that LLM pipelines may provide novel ways for quickly exploring hypotheses related to real-world entities, helping us to better leverage and understand the oceanic data already generated by humanity.</p>
<p>Background</p>
<p>LLMs as simulators.Several previous studies have demonstrated the potential for LLMs to act as simulators, often focusing on human behaviors or responses.For instance, (Argyle et al. 2023) demonstrate that LLMs can represent diverse human subpopulations and simulate survey result probabilities based on demographic information, such as predicting voting behavior given race, gender, and political affiliation.Similarly, other works leverage LLMs to replicate human behavior experiments, showing that LLMs can reproduce well-established findings from prior human subject studies (Aher, Arriaga, and Kalai 2023); and others simulate user satisfaction scores to optimize a dialogue system (Hu et al. 2023).</p>
<p>Our work aims to generalize beyond human-centered applications by focusing on simulating the properties of any class of specific entities, such as animals and countries (although we also include an experiment about athletes).Our focus is different as well: most previous studies explore simulations of human behavior for experimental replication, while we aim to use LLMs as a tool for quickly simulating datasets that can inform the exploration of broader scientific hypotheses in an efficient, exploratory manner.</p>
<p>Similarly, (Cohen et al. 2023) demonstrate the potential for extracting structured knowledge from LLMs to build knowledge graphs, which supports the idea that LLMs will be useful tools for simulating datasets on the fly.We build upon this idea to generate synthetic data for exploring novel relationships and hypotheses.</p>
<p>More broadly, synthetic data generation has been widely studied for its ability to improve machine learning models, address privacy concerns, and augment datasets (Lu et al. 2024b).However, most applications focus on tasks like model enhancement or privacy-preserving data generation, rather than on hypothesis-driven exploration.Recent work has begun to explore the use of LLMs to generate synthetic datasets, but most often with the aim to increase the performance of LLMs rather than to enable rapid hypothesis testing.</p>
<p>Hypothesis Generation.LLMs are increasingly being applied for hypothesis generation, with approaches generally falling into three categories: text-based, data-driven, and hybrid methods.</p>
<p>Text-based approaches leverage LLMs to synthesize hypotheses directly from given textual data.For example, (Tong et al. 2024) explore generating psychological hypotheses from academic articles.Their method relies on extracting a causal graph from the corpus of literature for hypothesis generation.Data-driven approaches focus on uncovering patterns in structured datasets.For instance, (Zhou et al. 2024) extracts hypotheses from labeled data, enabling automated discovery of insights.However, this reliance on existing datasets poses challenges when suitable labeled data is unavailable, restricting its scope in exploratory or novel domains.Hybrid approaches combine insights from both literature and data.(Xiong et al. 2024) demonstrates how LLMs can integrate knowledge from text and structured data to propose hypotheses.</p>
<p>In contrast to these approaches, our work focuses not on generating hypotheses directly, but on simulating datasets from which hypotheses can be explored.By leveraging LLMs as simulators of the properties of concrete entities, we enable a structured and data-driven pathway to hypothesis prototyping, mitigating the pitfalls of forgetting and compounding errors observed in direct hypothesis generation (Liu et al. 2024).Furthermore, in domains where hallucination poses a significant challenge, we apply a selfcorrection mechanism (Madaan et al. 2023) to improve simulation quality, which in future work could be further addressed with retrieval-augmented generation.</p>
<p>Approach</p>
<p>The overarching ambition in this paper is to move towards automating more of the process of exploring interesting and important patterns latent within existing internet-scale data, to advance our scientific understanding of the world and make the most of the data we have already generated as a society.One significant obstacle to prototyping a hypothesis within society-scale data is to curate a dataset by hand that can reveal evidence about the hypothesis, which requires sifting through many data sources and carefully translating unstructured data into structured, quantitative tables.</p>
<p>The general approach in this paper to avoid that cost, is to generate approximate tabular datasets by querying LLMs.Such tabular data is a powerful way of exploring patterns, where we can consider each row as entity, and each column as a property of that entity.The idea is that training data for LLMs implicitly includes many properties of real-world entities of scientific interest, like people, animals, activities, and countries.Information about a particular entity may be spread across many different documents and contexts, and usefully centralized into the weights of the LLM through the training process (Cohen et al. 2023).</p>
<p>This naturally leads to a simple approach to simulate an artisanal tabular dataset fit to explore a particular hypothesis.First, we consider the case where an experimenter provides a list of entities (e.g.like particular animals) and properties Figure 1: LLM-driven Dataset Simulation.Given a list of entities and properties, the method is to call an LLM for each combination of entity and property to simulate the value of the property for that entity.</p>
<p>(e.g.whether they lay eggs, or have wings): Then, the approach is to simply query the LLM to estimate each property for each entity (see Figure 1).We call this method LLMdriven Dataset Simulation.Our first set of experiments explores the ability of LLMs to simulate datasets with reasonable fidelity, i.e. whether the relationships among variables in the simulated dataset reflect those in a human-validated dataset.</p>
<p>To further automate the process of exploring hypotheses, we can use LLM-driven Dataset Simulation as a building block, and also use LLMs to help translate a qualitative highlevel hypothesis into the rows and columns of the tabular dataset (e.g. for it to also create the list of real-world entities, and the list of properties of those entities relevant to explore the hypothesis).We call this method Hypothesisdriven Dataset Simulation, and this broader pipeline is shown in Figure 2. The idea is that an experimenter can describe the hypothesis they want to explore, and a chain of LLM calls can orchestrate creating the rows and columns of the dataset, as well as to simulate the dataset itself.In more detail about the components of this pipeline:</p>
<p>Prompt Generation.The first stage involves generating the system and user prompts required to simulate property values.An LLM is prompted with the experimenterprovided hypothesis description (and optionally the desired number of properties) along with a one-shot example, and is directed to produce (1) a system prompt defining the role the LLM should adopt, and (2) a user prompt specifying the task for the LLM (i.e. to generate a list of key properties given the hypothesis description).These generated prompts are used to guide the subsequent stages of property generation.Note that examples of each of the prompts in this and following sections can be found in the appendix.</p>
<p>Property Simulation.Using the generated prompts, another LLM call simulates property descriptions in free-form text, such as Property Name: "Average Happiness Level", Description: "The average self-reported happiness level of individuals in this entity.",Possible Values: [0-10].</p>
<p>Property Parsing.To structure the simulated properties, an LLM extractor parses the free-form text into a consis-tent format: property-name: [description, possible values or ranges].This structured format is then combined with the list of entities to prompt the LLM for property values.</p>
<p>Self-Correction.After simulating property values, an optional self-correction step ensures robustness (Madaan et al. 2023).The LLM is prompted to evaluate the accuracy of each value given the property description and range, provide reasoning for its assessment, and output a corrected (or confirmed) value.The aim is to improve the reliability and consistency of the simulated dataset.</p>
<p>LLM-driven Dataset Simulation Experiments</p>
<p>The first set of experiments explore LLMs' ability to simulate useful tabular datasets, given a list of entities and properties.We begin with a simple dataset of binary characteristics of animals as a didactic toy example that we expect to be well-within the capabilities of LLMs; we also explore a more difficult domain that involves specific demographic properties of countries, and complicated constructed indicators (e.g. of how egalitarian a country is), where it is less clear that LLMs would be adept, as a way of probing the limits of this technique.Note that in these experiments we use existing ground-truth datasets as a grounded proxy for the situation of real interest, e.g. to simulate novel datasets; while there is some risk of LLMs memorizing these datasets (as LLMs are at least aware of the Zoo dataset), we find in later experiments that the method does indeed generalize to novel datasets.</p>
<p>Zoo Domain</p>
<p>Description.In this experiment, we assess the ability of LLMs to simulate the well-known "Zoo Dataset" from the UCI Machine Learning Repository (Forsyth 1990).This dataset consists of 101 animal instances (e.g.vampire bat, aardvark), each characterized by 16 binary features (e.g., hair, feathers, teeth) and a categorical target variable representing the animal's type (e.g., mammal, insect).Our aim is to determine whether LLMs can replicate this dataset accurately.Note that the LLM is conditioned on the plain-text names of the animals and features.</p>
<p>Motivation.We choose this dataset as a first exploration because of its intuitive simplicity.It is clear that LLM training should include simple biological features of animals within it, and thus this provides a toy environment in which to sanity check the approach.The Zoo domain also illustrates how LLMs can be applied to biological or ecological datasets, offering potential for hypothesis generation in specialized fields.</p>
<p>Experiment setting.To assess the accuracy of individual simulated binary properties, we compared the outputs of the LLMs to the ground-truth dataset.The quality of properties was evaluated using accuracy as the primary metric for both animal features (independent variables) and animal type (dependent variable).We used GPT-4o-mini and the prompting strategy of directly querying property values in a Pythonic dictionary format.</p>
<p>Figure 2: Architecture of the hypothesis-driven simulation.The pipeline starts with a description of the hypothesis to explore, followed by the prompts that will generate the raw properties.After extracting the properties, the list of entities produces simulated data, which goes under a self-correction prompt for the final value.</p>
<p>We also evaluated the utility of simulated datasets for exploratory data analysis and hypothesis testing.This process emulated a typical scientific workflow: a standard analysis model, such as linear or logistic regression, was trained on the simulated training data and then run on unseen simulated validation data.The predictions on the (simulated) validation set were then compared to real-world validation labels to assess performance.The idea is to get a sense of how well an analysis method applied on simulated data captures the same patterns as in the real data.</p>
<p>To quantify how closely the simulated data approximates real-world patterns, we introduce a Simulation Error Gap metric.This metric measures the difference in generalization error between models trained on simulated data and the upper-bound performance achieved by fitting models on ground-truth training data.A smaller Simulation Error Gap reflects a higher fidelity of the simulated data in capturing the underlying relationships within the real-world dataset.In this domain, a logistic regression model was trained on 70% of the data, and generalization error was measured by accuracy.</p>
<p>Simulation Fidelity of Properties.Overall, the results indicate that the simulator effectively models binary properties in the domain.As shown in Figure 3, the average accuracy across all properties is 0.923, suggesting that the simulated data closely approximates the characteristics of the real data.Some of the remaining error is due to an ambiguous property called "catsize," which highlights that an LLM requires a clear semantic description of the property to be simulated.</p>
<p>Asking the LLM to Instead Directly Output Correlation</p>
<p>Coefficients.As a control experiment, we tested the direct generation of hypotheses by asking the LLM to estimate correlations between each independent variable and each class (e.g.animal type).In particular, the Matthews correlation coefficient, which is appropriate for binary variables and multi-class output.Interestingly, we found an average ab-Figure 3: Simulation accuracy for properties in the Zoo domain.Shown are how accurately the LLM is able to simulate each property in the Zoo domain across all the animals in the dataset.Accuracy is generally high, although the LLM understandably struggles with the ambigious variable name "catsize."The conclusion is that the approach is viable, although it is important to give the model sufficient context about the property it is to simulate.solute difference of 0.321 between the LLM's estimations and the real coefficients, highlighting the limited capabilities of LLMs to be used as direct estimators of relationships between variables even in quite simple domains.</p>
<p>Training Classifiers on Simulated Data.Perhaps unsurprisingly, in simulating class labels in this dataset (e.g.mapping an animal name to its type-such as mammals, birds, and reptiles), the simulator performs very well, achieving perfect accuracy.</p>
<p>To further assess the simulator's utility for predictive modeling, we trained a logistic regression model on the simulated data and evaluated it on real validation data.The model achieved an accuracy of 0.833 when trained on the simulated data, compared to an accuracy of 0.933 on real data.This resulted in a simulation error gap of 0.1, indicating a modest difference between the simulated and real data for this particular predictive task.</p>
<p>In summary, these results serve as some grounding evidence that LLMs can simulate datasets with reasonable fidelity.The next experiment explores a more difficult domain.</p>
<p>Countries Domain</p>
<p>Description.The dataset in this experiment is designed to explore how demographic features of countries correlate with their Egalitarian Democracy Index (EDI) scores; the EDI is an index that combines information on voting rights, the freedom and fairness of elections, freedoms of association and expression, as well as the extent to which the protection of rights, access to power, and distribution of resources is equal (Sigman and Lindberg 2019).It ranges from 0 to 1 (most democratic).Our reference dataset combines various indicators, such as population statistics across age groups and genders from the World Bank (The World Bank 2024), with EDI scores from Our World in Data (V-Dem 2022), all from 2022.For example, properties include metrics like 'Population ages 60-64, male (% of male population)', 'Regulatory Quality: Percentile Rank, Upper Bound of 90% Confidence Interval', and 'Political Stability and Absence of Violence/Terrorism: Number of Sources.'The goal is to test whether LLMs can simulate tabular data that reflects realworld patterns, facilitating rapid hypothesis testing in more complicate dsettings.</p>
<p>Motivation.This dataset was chosen because it is more specialized and requires estimating continuous variables with various ranges, and requires the LLM to handle ambiguous property names (e.g.'Regulatory Quality: Percentile Rank, Upper Bound of 90% Confidence Interval').In contrast to the simplicity of the Zoo domain, this provides a more challenging environment to further develop and test dataset simulation techniques.It also highlights how dataset simulation may be useful for hypothesis generation in areas of economics and policy.</p>
<p>Experiment setting.After pre-processing (see Appendix A.1), a random sample of 50 countries is selected from a pool of 155 countries, and 10 random properties are chosen from a pool of 120 properties.In the Countries Domain, the quality of predictor variables was evaluated using correlation with actual values, while the Egalitarian Democracy Index was assessed using Mean Absolute Error (MAE).</p>
<p>For the analysis method, we trained a linear regression model on 80% of the simulated data, and the generalization error of the model was measured by Median Absolute Error (MedAE), as in contrast to the Zoo domain, the dependent variable is continuous.</p>
<p>Explorations to Increase Simulation Fidelity.In this more chalelnging domain, we explored several techniques to improve simulation performance.One approach was to condition the dependent variable (EDI) on the previously simulated property values for a particular country.Another approach was to take certain complex properties, such as demographic percentages, and use few-shot learning strategies to help ground out the variable's range.Interestingly, despite extensive experimentation (e.g., conditioning on outliers or randomly-chosen data points), no consistently superior approach was identified.</p>
<p>Impact of Model size.We tested three model architectures: Llama3-8B, Llama3-70B, and GPT-4o-mini.GPT-4omini consistently outperformed the others, producing the most accurate and contextually relevant simulations.As a result, GPT-4o-mini was used for all subsequent experiments in other domains.Table 1 shows the impact of model size on simulation quality, measured by average correlation between simulated and real data points, Mean Absolute Error in simulated EDI (EDI MAE), and simulation error gap.The models compared include LLaMA-3-8b, LLaMA-3-70b, and GPT-4o-mini.As seen in Table 1, performance improves across all metrics as model size increases.Further, Figure 4, visualizes the fidelity of predictive models increasing across different model sizes.In summary, these results highlight that larger models significantly improve simulation quality.The increased model size leads to higher correlation with real-world data, reduced error gaps, and more accurate predictions of EDI values, making larger models more reliable for hypothesis generation.</p>
<p>Model</p>
<p>Impact of Prompting strategy.We examined two key factors in prompting: prompt style and output format.For prompt style, we explored prompts that were direct queries for specific data points ("Make your best guess about this value..."), with prompts that told the LLM it was an expert and was tasked to complete a report about the property at hand ("You are an expert historian.Complete the following document...").Output format compared structured formats (e.g., Python dictionaries) with instead outputting an answer The conclusion is that the fidelity of the simulations improves with model scale and capability. in natural language.This analysis reveals how different presentation styles affect data consistency and realism in simulations.</p>
<p>The prompting strategy that proved most effective in this experiment involved directly querying property values and using a Pythonic dictionary format to structure the data.This strategy was adopted for all subsequent experiments.Table 2 shows the effect of different prompting strategies on simulation quality.The direct-structured strategy, where values are requested in a structured, Pythonic dictionary format, consistently outperforms the other strategies, achieving the highest average correlation (0.770) and the lowest simulation error gap (0.011).The direct-descriptive strategy, which asks for values as free-text words, also performs well, achieving an average correlation of 0.738, an EDI MAE of 0.119, and a simulation error gap of 0.036.These results suggest that asking for data in a structured format (as in direct-structured) leads to more precise simulations, while the direct-descriptive strategy still provides reliable results.In contrast, the reportdescriptive and report-structured strategies show significantly weaker performance.Both report-based strategies involve completing partial data rather than requesting full values, leading to less accurate and more inconsistent simulations.These patterns are further illustrated in Figure A.1, which compares the average correlation metrics across the different prompting strategies.</p>
<p>Asking the LLM to Instead Directly Output Correlation Coefficients.Similar to the Zoo domain, we also tasked an LLM with directly estimating the correlations between some of the properties and the EDI.The results were that on average, there was a 0.483 difference in the correlation suggested by the LLM and the real correlation in the data, again highlighting the benefits from simulating data before analyzing patterns.See the Appendix A.3 for a plot of those correlations for various properties.</p>
<p>Towards Hypothesis-Driven Dataset Simulation</p>
<p>The previous experiments explored the ability of LLMs to simulate datasets in a controlled setting where ground-truth data was available (e.g. by having the LLM simulate existing datasets).In this section, we move more towards the setting of direct interest, where we want to explore a hypothesis but do not have a pre-existing dataset.We also experiment here with greater LLM autonomy: In addition to having the LLM simulate the data, we also have it map from a high-level hypothesis to the properties worth simulating to explore it.Further experiments explore having the LLM also generate the list of entities of interest (e.g.particular sports figures in this case).In this way, we move more towards having an LLM assistant that can help an experimenter quickly brainstorm and explore potential hypotheses.</p>
<p>Description.In this section, we evaluate the ability of LLMs to generate datasets based on qualitative hypotheses.Specifically, we explore the relationship between an athlete's sport type (team vs. individual), the number of major injuries (lasting over two months), and peak performance age.The system receives a prompt outlining the hypothesis along with a list of 40 athletes (20 soccer players and 20 tennis players).The simulator was provided only with the hypothesis and a list of entities, from which it generated data corresponding to the key properties mentioned.Real values for the number of injuries were collected from Tennis Explorer for tennis players and Transfermarkt for soccer players, while the peak performance age was sourced using Perplexity (Perplexity 2024) (as a proxy for exhaustive Google searches).</p>
<p>To justify our use of Perplexity for sourcing the peak performance age, we conducted spot checks comparing it to direct LLM queries (e.g., asking ChatGPT).Specifically, we asked both systems for the place of birth of 20 lesser-known soccer players from the Spanish soccer league.While Chat-GPT accurately identified only 10 out of 20, Perplexity correctly retrieved all 20 places of birth.This significant difference (Fischer's exact test; p &lt; 0.001)) is likely due to Perplexity's use of Retrieval-Augmented Generation (RAG), which enhances factual accuracy by grounding the inferences of the LLM in externally retrieved data (Shuster et al. 2022;Ren et al. 2023).</p>
<p>Motivation.This task tests whether LLMs can simulate data for hypotheses that would be time-consuming to collect in the real world.Information such as the number of injuries or peak performance age is often scarce, so gener-ating these values synthetically could accelerate hypothesis testing.This experiment demonstrates the potential of LLMs to convert high-level qualitative ideas into structured, usable data, making it easier to explore relationships in data-sparse fields.</p>
<p>Experiment setting.The quality of the simulated properties were evaluated using correlation and MAE with the real data points as metrics.A linear regression model was fitted as analysis method, and MAE was used to measure the generalization error (on 20% of the simulated data).GPT-4omini and directly querying for property values in a Pythonic dictionary format were used in this experiment.</p>
<p>LLMs for Hypothesis Mapping.The results indicate that the LLM-based simulator was successful in identifying and generating relevant quantitative properties implied in the hypothesis.For instance, the simulator accurately mapped the age of peak performance and the total number of major injuries to corresponding simulated values that were highly aligned with real-world data.The correlation coefficients between simulated and actual data were 0.625 for age of peak performance and 0.581 for total number of major injuries, suggesting that the LLM effectively captured the underlying relationships specified in the hypothesis.</p>
<p>To assess the accuracy of the simulated data, we also calculated the simulation error gap, which quantifies the discrepancy between the simulated data and actual data.The error gap was found to be 1.325 MAE, indicating that the LLM's output was relatively close to the actual data, but there was still some room for improvement in accuracy.Impact of Self-Correction.The introduction of selfcorrection (Madaan et al. 2023) into the simulation pipeline led to measurable improvements in the simulator's performance.Specifically, the correlation coefficients for the two key variables-age of peak performance and total number of major injuries-were higher when self-correction was applied.Without self-correction, the correlation for age was 0.570, and for injuries, it was 0.557.However, with selfcorrection, these correlations improved to 0.625 for age and 0.581 for injuries, demonstrating the effectiveness of selfcorrection in refining the LLM's output.</p>
<p>Additionally, the simulation error gap modestly decreased with the application of self-correction.Without selfcorrection, the error gap was 1.791 MAE, but with selfcorrection, it was reduced to 1.325 MAE.While this reduction highlights some benefit, the overall improvement is relatively small.Moreover, as illustrated in  improvement is likely due to the inherent limitations in the model fitted on real data.Specifically, the properties used in the simulations may not adequately explain the labels, thereby constraining the potential impact of self-correction.This aligns with the observation that, despite larger improvements in the correlations between simulated and real properties, these gains did not translate into a correspondingly significant reduction in the Simulation Error Gap.</p>
<p>LLMs to generate lists of entities To further evaluate the ability of LLMs to generate entities themselves, we performed an additional experiment.We prompted the LLM to generate two lists: one of 20 well-known soccer players and another of 20 less-known soccer players.For validation, we randomly selected 20 pairs of players (one from each list) and compared their relative popularity using Google Trends scores as a proxy for notoriety.The results clearly showed that the LLM was capable of differentiating between wellknown and less-known soccer players, with significantly higher trend scores for the well-known list (p &lt; 0.001).This finding highlights the LLM's ability not only to simulate data but also to generate entities that align with specific qualitative criteria.</p>
<p>In summary, the combination of LLMs for hypothesis mapping, entity generation, and dataset simulation provides a viable framework for using AI to generate reasonably accurate dataset prototypes for hypothesis testing and exploration.</p>
<p>Discussion and Conclusion</p>
<p>The experiments in this paper highlight the potential for LLMs to translate high-level descriptions of hypotheses into approximate datasets, ones that can be used to quickly iterate towards interesting latent patterns in existing data.The hope is to empower experimenters to more easily sift through the space of hypotheses by lowering the cost of gathering a dataset by hand.In practice, after discovering an interesting hypothesis, the experimenter will still likely need to either curate a grounded dataset, or perform a real-world experiment, to generate a scientifically validated result.</p>
<p>This kind of method of course has its limitations, as it depends on the estimation abilities of LLMs, which will vary with how well the LLMs' dataset covers the entities as properties of interest, as well as the overall capabilities of the LLM itself.One interesting phenomenon to note is that in the Countries domain, simulating data and then analyzing the relationships among that data performed better than asking the LLM directly to estimate relationships among variables (without simulating the data).In other words, while the information about the variables was latent within the LLM (as it could be simulated), externalizing that information to run outside analysis upon it, yielded further insights.Such improvement relates to the general idea of LLMs iterating upon their own outputs as a way of generating further useful synthetic data.</p>
<p>While the approach here directly queries an LLM, another interesting direction is to employ a more agentic pipeline to actively construct a grounded dataset.That is, LLMs that can browse the web and write code could do things like piece together existing datasets, or attempt to actively ground each data point in reliable sources (e.g.similar to how Perplexity was used to approximate ground truth in the final domain).Such an approach, if it worked well, might present another point in the trade-off between (1) cost and speed, and (2) dataset fidelity: e.g. it would gain fidelity but require more complex chaining of LLM calls.</p>
<p>More broadly, a grander ambition is to create an openended system (Stanley, Lehman, and Soros 2017) that could continually discover new, interesting patterns in data.The second set of experiments represents a step in this direction, where the experimenter supplies the high-level hypothesis, which is then translated into the rows and columns of a dataset, which is then simulated.But this could be taken further, where a user instead supplies a more broad question of interest, e.g."What are interesting patterns of human behavior that can be discerned from biographies of historical figures," and the system itself continually searches for unexpected and interesting patterns by simulating and analyzing datasets.This is related to other directions that attempt to apply LLMs towards open-ended creativity (Lu et al. 2024a;Lehman et al. 2023).</p>
<p>While the approach here works with simulating specific real-world entities (like countries, athletes, and animals), it is also interesting to consider automated creation of datasets that relate to simulations of people through LLMs (Argyle et al. 2023; Aher, Arriaga, and Kalai 2023).Indeed, the work here started with that direction (to explore hypotheses related to whether people with different e.g.OCEAN personality scores would benefit from different leisure activities).There are interesting technical challenges to consider, such as modeling distributions of people and their responses (e.g. the distribution of people with a high openness score, and the distribution of their favorite activities), rather than discrete properties of singular entities as in this paper.Such research is an interesting direction of future work that can build off the foundation established in this paper.</p>
<p>Finally, it is interesting to consider the possibilities for novel kinds of ML algorithms opened up by the ability to simulate new features and datasets on the fly.That is, classic tabular learning algorithms (like decision trees) are typically applied to fixed datasets; yet, LLMs open up the possibility of dynamically expanding the feature set as learning progresses.Future work will explore extensions of decision trees that start from a minimal dataset (perhaps only consisting of entities and the dependent variable), and through human-computer interaction, gradually build the dataset as the learning algorithm proceeds; the decision tree algorithm itself becomes more open-ended in its unfolding.</p>
<p>In conclusion, this paper described the potential of using LLMs to simulate datasets about real-world entities, in service of accelerating the exploration of hypotheses about them.Overall, this research points towards the possibility of fully automated systems for automated discovery of knowledge aggregated from the vast cultural output of humans: What exciting patterns (about us, and about the world) lie waiting for us to distill from the ever-growing ocean of civilization-scale data?</p>
<p>Figure 4 :
4
Figure 4: Correlation coefficients by model size for Countries domain.Shown are how well the simulated properties correlate with the ground-truth properties across all entities.The conclusion is that the fidelity of the simulations improves with model scale and capability.</p>
<p>Figures 5, B.2, and B.3 further illustrate the performance of the simulator.Figure 5a and Figure 5b show scatter plots comparing simulated and actual values for the key properties (age of peak performance and total amount of injuries).The alignment between the two datasets is strong, especially for age.Figures B.2 and B.3 present line plots comparing simulated and actual values over a range of data points, with both plots showing that the LLM's simulated values are closely aligned with actual data.</p>
<p>Figure B.1, the error bars for the generalization errors-both with and without self-correction-overlap significantly.This limited (a) Scatter plot for peak performance age.(b) Scatter plot for total major injuries.</p>
<p>Figure 5 :
5
Figure 5: Scatter plots comparing simulated and real values for peak performance age and total major injuries.Dashed red line indicates perfect correspondence between real and simulated values.</p>
<p>Table 1 :
1
Simulators' performance by model size
Average Corr. EDI MAE Sim. Error GapLLaMA-3-8b0.2210.1340.064LLaMA-3-70b0.6440.1890.036GPT-4o-mini0.7380.1190.036</p>
<p>Table 2 :
2
Simulators' performance by prompting strategy in the Countries domain.
PromptingAverage Corr. EDI MAE Sim. Error Gapdirect-descriptive0.7380.1190.036direct-structured0.7700.1320.011report-descriptive0.3940.1710.087report-structured0.2530.1600.153
A Countries DomainA.1 Pre-processing steps 1. Filtering feature database for year 2022 2. Remove features that contain 'Standard Error'.For example, 'Rule of Law: Standard Error' seems a bit too unclear what it means 3. Filter for common countries (egalitarian index and demographic features come from different data sources) 4. Remove demographic features that are not present in all countries 5. Randomly sample N countries and K features from that (N=50, K=10)A. • Direct style, Descriptive format 1 sys_prompt = You will be asked to make your best guess about the value a country had for a particular feature in 2022.Respond in the following json format: {feature: value}.Where feature is the characteristic about the country, and value is your numeric guess.If you don't know, make your best guess 2 user_prompt = What was the value of the 'Population ages 45-49, male (% of male population)' in Namibia for 2022?• Report style, Structured format      The LLM-suggested method consistently underperforms compared to our simulation approach in accurately capturing complex relationships between demographic variables.B Athletes Domain
Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies. G Aher, R I Arriaga, A T Kalai, arXiv:2208.102642023</p>
<p>Out of One, Many: Using Language Models to Simulate Human Samples. L P Argyle, E C Busby, N Fulda, J R Gubler, C Rytting, D Wingate, Political Analysis. 3132023</p>
<p>Adverse childhood experiences. Paediatrics and Child Health. M Boullier, M Blair, 201828</p>
<p>Crawling the Internal Knowledge-Base of Language Models. R Cohen, M Geva, J Berant, A Globerson, arXiv:2301.128102023</p>
<p>R Forsyth, 10.24432/C5R59VZoo. UCI Machine Learning Repository. 1990</p>
<p>Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System. Z Hu, Y Feng, A T Luu, B Hooi, A Lipani, Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM '23. the 32nd ACM International Conference on Information and Knowledge Management, CIKM '23ACM2023</p>
<p>Evolution through large models. J Lehman, J Gordon, S Jain, K Ndousse, C Yeh, K O Stanley, Handbook of Evolutionary Machine Learning. Springer2023</p>
<p>H Liu, Y Zhou, M Li, C Yuan, C Tan, arXiv:2410.17309Literature Meets Data: A Synergistic Approach to Hypothesis Generation. 2024</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024aarXiv preprint</p>
<p>Y Lu, M Shen, H Wang, X Wang, C Van Rechem, T Fu, W Wei, arXiv:2302.04062Machine Learning for Synthetic Data Generation: A Review. 2024b</p>
<p>A Madaan, N Tandon, P Gupta, S Hallinan, L Gao, S Wiegreffe, U Alon, N Dziri, S Prabhumoye, Y Yang, S Gupta, B P Majumder, K Hermann, S Welleck, A Yazdanbakhsh, P Clark, arXiv:2303.17651.Perplexity.2024Self-Refine: Iterative Refinement with Self-Feedback. 2023Perplexity AI: Explore Answers and Insights with AI-powered Search</p>
<p>Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation. R Ren, Y Wang, Y Qu, W X Zhao, J Liu, H Tian, H Wu, J Rong Wen, H Wang, S Roccas, L Sagiv, S H Schwartz, A Knafo, ArXiv, abs/2307.11019Personality and social psychology bulletin. 2862023. 2002The big five personality factors and personal values</p>
<p>Language Models that Seek for Knowledge: Modular Search &amp; Generation for Dialogue and Prompt Completion. K Shuster, M Komeili, L Adolphs, S Roller, A Szlam, J Weston, R Sigman, S I Lindberg, Conference on Empirical Methods in Natural Language Processing. 2022. 20197Democracy for all: conceptualizing and measuring egalitarian democracy</p>
<p>Openendedness: The last grand challenge you've never heard of. While open-endedness could be a force for discovering intelligence, it could also be a component of AI itself. The World Bank. 2024. World Development Indicators. Data filtered to include only 2022 values. K O Stanley, J Lehman, L Soros, 2017</p>
<p>Automating psychological hypothesis generation with AI: when large language models meet causal graph. S Tong, K Mao, Z Huang, Y Zhao, K Peng, Humanities and Social Sciences Communications. 1112024</p>
<p>Processed by Our World in Data. Original data: V-Dem Country-Year (Full + Others) v14. V-Dem , 2022. August 04. 2024Egalitarian Democracy Index -(best estimate, aggregate: average)</p>
<p>Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models. G Xiong, E Xie, A H Shariatmadari, S Guo, S Bekiranov, A Zhang, arXiv:2411.023822024</p>
<p>Y Zhou, H Liu, T Srivastava, H Mei, C Tan, arXiv:2404.04326Hypothesis Generation with Large Language Models. 2024</p>
<p>List of features 1 Population ages 00-04, male (% of male population). </p>
<p>. Population. male (% of total population</p>
<p>Population ages 65 and above, female (% of female population). </p>
<p>Regulatory Quality: Percentile Rank 5 Population ages 40-44, male (% of male population. </p>
<p>10 Population ages 15-64, total 11 Rule of Law: Estimate 12 Government Effectiveness: Percentile Rank 13 Population ages 15-64. Survival to age 65, female (% of cohort. male (% of male population</p>
<p>Population ages 65-69, male (% of male population). </p>
<p>Population ages 65 and above, total 20 Adolescent fertility rate (births per 1,000 women ages. </p>
<p>Sex ratio at birth. male births per female births</p>
<p>birth, male (years). </p>
<p>Number of Sources 25 Number of deaths ages 20-24 years 26 Number of deaths ages 10-14 years 27 Population ages 70-74. Regulatory Quality, male (% of male population</p>
<p>Population ages 40-44, female (% of female population) 29 Probability of dying among adolescents ages 15-19 years. 10</p>
<p>Voice and Accountability: Percentile Rank 32 Mortality rate, neonatal (per 1,000 live births) 33 Population ages 50-54. male (% of male population</p>
<p>Population ages 75-79, male (% of male population). </p>
<p>Population ages 0-14, total 37 Population ages 45-49, male (% of male population). </p>
<p>Population ages 55-59, male (% of male population). </p>
<p>Voice and Accountability: Percentile Rank, Upper Bound of 90% Confidence Interval 41 Mortality rate. under-5, female (per 1,000 live births</p>
<p>Population ages 0-14, male (% of male population). </p>
<p>Population ages 65 and above, male (% of male population). </p>
<p>44 Mortality rate. infant (per 1,000 live births</p>
<p>Population ages 45-49, female (% of female population). </p>
<p>Population ages 30-34, male (% of male population). </p>
<p>Regulatory Quality: Percentile Rank, Upper Bound of 90% Confidence Interval 49 Rule of Law: Number of Sources 50 Population ages 15-19. female (% of female population</p>
<p>Control of Corruption: Estimate 52 Population ages 80 and above, male (% of male population. </p>
<p>Control of Corruption: Percentile Rank, Upper Bound of 90% Confidence Interval 54 Statistical performance indicators (SPI) : Pillar 1 data use score. </p>
<p>Political Stability and Absence of Violence/Terrorism: Estimate 56 Political Stability and Absence of Violence/Terrorism: Percentile Rank, Upper Bound of 90% Confidence Interval 57 Government Effectiveness: Number of Sources 58 Probability of dying among. youth ages 20-24 years (per 1,000</p>
<p>Government Effectiveness: Percentile Rank, Upper Bound of 90% Confidence Interval. </p>
<p>male 61 Voice and Accountability: Percentile Rank, Lower Bound of 90% Confidence Interval 62 Population ages 65 and above. Population. female 63 Mortality rate, under-5, male (per 1,000 live births</p>
<p>Population ages 15-64, male 65 Population ages 15-19, male (% of male population). </p>
<p>Population ages 35-39, male (% of male population). </p>
<p>Rule of Law: Percentile Rank 69 Population ages 80 and above. female (% of female population</p>
<p>Population ages 15-64, female (% of female population). </p>
<p>Political Stability and Absence of Violence/Terrorism: Number of Sources 72 Net migration 73 Population ages 35-39, female (% of female population). </p>
<p>Number of infant deaths 75 Number of deaths ages 15-19 years 76 Probability of dying among adolescents ages 10-14 years (per 1,000) 77 Fertility rate, total (births per woman) 78 Life expectancy at birth, female (years) 79 Population ages. male (% of male population</p>
<p>Voice and Accountability: Number of Sources 81 Age dependency ratio, young (% of working-age population. </p>
<p>Rule of Law: Percentile Rank. Lower Bound of 90% Confidence Interval 83 Age dependency ratio (% of working-age population. </p>
<p>. Population. female (% of total population</p>
<p>Population ages 30-34, female (% of female population). </p>
<p>Population ages 15-64 (% of total population). </p>
<p>Population ages 65 and above (% of total population). </p>
<p>Population ages 60-64, male (% of male population). </p>
<p>Population ages 65-69, female (% of female population). </p>
<p>Political Stability and Absence of Violence/Terrorism: Percentile Rank, Lower Bound of 90% Confidence Interval 91 Regulatory Quality: Percentile Rank, Lower Bound of 90% Confidence Interval 92 Government Effectiveness: Percentile Rank, Lower Bound of 90% Confidence Interval 93 Survival to age. 65male (% of cohort</p>
<p>Death rate, crude (per 1,000 people) 95 Probability of dying among children ages 5-9 years. 10</p>
<p>Control of Corruption: Percentile Rank, Lower Bound of 90% Confidence Interval 97 Population ages 20-24. female (% of female population</p>
<p>Political Stability and Absence of Violence/Terrorism: Percentile Rank 99 Population ages. female (% of female population</p>
<p>Age dependency ratio, old (% of workingage population). </p>
<p>Number of deaths ages 5-9 years 103 Population ages. male 104 Population ages 10-14, male (% of male population</p>
<p>105 Mortality rate. infant, female (per 1,000 live births</p>
<p>female 107 Control of Corruption: Number of Sources 108 Voice and Accountability: Estimate 109 Population ages. Population. % of total population</p>
<p>110 Mortality rate. infant, male (per 1,000 live births</p>
<p>Number of under-five deaths 114 Government Effectiveness: Estimate 115 Control of Corruption: Percentile Rank 116 Population ages 65 and above, male 117 Rule of Law: Percentile Rank, Upper Bound of 90% Confidence Interval 118 Life expectancy at birth, total (years) 119 Number of neonatal deaths 120 Population ages. Population. total 113</p>            </div>
        </div>

    </div>
</body>
</html>
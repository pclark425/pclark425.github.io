<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6456 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6456</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6456</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-127.html">extraction-schema-127</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <p><strong>Paper ID:</strong> paper-269214526</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.12045v2.pdf" target="_blank">RAM: Towards an Ever-Improving Memory System by Learning from Communications</a></p>
                <p><strong>Paper Abstract:</strong> We introduce an innovative RAG-based framework with an ever-improving memory. Inspired by humans'pedagogical process, RAM utilizes recursively reasoning-based retrieval and experience reflections to continually update the memory and learn from users' communicative feedback, namely communicative learning. Extensive experiments with both simulated and real users demonstrate significant improvements over traditional RAG and self-knowledge methods, particularly excelling in handling false premise and multi-hop questions. Furthermore, RAM exhibits promising adaptability to various feedback and retrieval methods, showcasing its potential for advancing AI capabilities in dynamic knowledge acquisition and lifelong learning.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6456.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6456.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval Augmented Memory (RAM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ever‑improving RAG-based memory system that interleaves recursive reasoning (R^3) and a memory‑reflection module to update an external vector memory from communicative human feedback, enabling continual learning without re-training the base LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A RAG-based agent that runs recursive reasoning-based retrieval (R^3) (iterative Reason → Retrieval → Inference trials) over an external vector store, solicits human feedback when stuck, generates a 'reflected memory' from ground truth + trial scratchpads, and locally edits the stored document/sentence to replace the most relevant memory entry with the reflected memory (thus keeping memory compact and continually updated).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>evaluated with 7B and 13B LLaMA variants and Vicuna-7B; also compared to GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector store / retrieval‑augmented generation (RAG) with local semantic edits (ChromaDB vector DB + BM25 for sentence localization)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text passages/documents encoded as sentence embeddings (all-MiniLM-L6-v2) and stored as documents/sentences in the vector DB; reflected memories are textual sentences used to replace specific sentences in stored documents.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>iterative similarity search (L2 / embedding cosine) over vector DB (top-1 by default) during R^3; localization for update uses BM25 sentence-level similarity to select and replace the most relevant sentence (local edit).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FreshQA and MQuAKE-T (QA datasets with fresh/ever-changing knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>knowledge‑intensive question answering / multi‑hop reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>FreshQA GPT4_score = 60.17 (RAM with feedback and memory updates); MQuAKE GPT4_score = 48.96 (RAM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Self-knowledge baseline FreshQA GPT4_score = 36.36; RAG-only (no memory update, using outdated M_old) FreshQA GPT4_score = 33.77; MQuAKE self-knowledge = 12.66; RAG-only MQuAKE = 38.51</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>GPT4_score (automated evaluator) and BERTScore (semantic similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Increased inference time due to multi-step trials (reported average per-step ~8s and per-trial ~30s for LLaMA-2-7B); potential memory growth and retrieval slowdown if naive accumulation of feedback is used (mitigated by local sentence-level edits); hints vs direct-answer tradeoff (hints induce better long-term reasoning improvements while direct answers give faster but more 'spoon-fed' gains).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less helpful for fast‑changing facts beyond teacher/external knowledge scope; sometimes lower BERTScore despite improved GPT4_score (surface similarity vs correctness); failure modes include bad retrieval (~40% of sampled failures), model hallucination (~32%), question misunderstanding (~12%), knowledge deficiency due to outdated memory (~12%), and occasional bad reasoning (~4%). Memory currently implemented as vector DB only — authors note potential improvements with structured graphs or relational DBs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Li, J., Wang, X., Ding, W., Wang, Z., Kang, Y., Jia, Z., Zheng, Z., Meloni, G., 2024. RAM: Towards an Ever-Improving Memory System by Learning from Communications. arXiv:2404.12045.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAM: Towards an Ever-Improving Memory System by Learning from Communications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6456.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6456.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>R^3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursive Reasoning-based Retrieval (R^3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recursive chain‑of‑thought style retrieval loop used inside RAM that iteratively Reason → Retrieval → Inference to refine retrieval queries and gather multi-step evidence from the vector memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>R^3 (component of RAM)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A retrieval-and-reasoning controller that decomposes the query into actions (Search steps), forms reflected keywords from intermediate reasoning, retrieves top relevant documents from the vector memory, and repeats until a satisfactory inference or max trials; triggers human 'ask-for-help' when stuck on repeated identical inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>evaluated with same LLaMA/Vicuna/GPT3.5 models as RAM experiments (7B, 13B, Vicuna-7B, GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector store (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text passages/documents encoded as embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>iterative similarity-based retrieval (top-1 L2 / embedding similarity) using progressively updated retrieval keys derived from reasoning trace</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FreshQA and MQuAKE-T (as the retrieval/reasoning component)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>multi-hop reasoning / retrieval-guided QA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When used (RAM-R^3) without memory updates: FreshQA GPT4_score = 45.98; MQuAKE GPT4_score = 27.41</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared with self-knowledge (no retrieval): FreshQA self-knowledge = 36.36; RAG-only (single retrieval, no R^3) = 33.77, showing R^3 improves retrieval-guided reasoning even without memory updates.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>GPT4_score, BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Requires multiple steps/trials (higher latency) and intermediate retrievals can sometimes decrease similarity temporarily (retrieved contexts may contain overlapping keywords but not relevant facts), but overall provides better evidence and reduces hallucinations in multi-hop questions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Intermediate retrieval steps sometimes retrieve noisy or less relevant context; performance depends on quality of retrieval and underlying model reasoning capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Li, J., Wang, X., Ding, W., Wang, Z., Kang, Y., Jia, Z., Zheng, Z., Meloni, G., 2024. RAM: Towards an Ever-Improving Memory System by Learning from Communications. arXiv:2404.12045.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAM: Towards an Ever-Improving Memory System by Learning from Communications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6456.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6456.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-only</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation (RAG-only baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A standard RAG baseline that answers questions by retrieving from an external memory M_old without any iterative recursive retrieval or memory update.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG-only</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Conventional retrieval-augmented pipeline that fetches top-k documents by embedding similarity from the static memory and conditions the LLM to generate an answer; in this paper used as baseline (no memory updates, no R^3 or feedback-driven edits).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>evaluated with LLaMA-2-7B/13B, Vicuna-7B, GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector store / retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text passages encoded as embeddings (documents retrieved as context)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>single-shot similarity search (L2 / embedding) retrieving top-1 document by default (chain type 'stuff')</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FreshQA and MQuAKE-T</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>knowledge‑intensive QA / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>FreshQA GPT4_score = 33.77; MQuAKE GPT4_score = 38.51</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Self-knowledge (no retrieval) FreshQA GPT4_score = 36.36 (RAG-only underperforms self-knowledge on some settings); MQuAKE self-knowledge = 12.66</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>GPT4_score, BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Single-shot retrieval is faster but less capable for multi-hop or false‑premise questions; retrieval quality critically affects performance and hallucination risk.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Fails on many false-premise and multi-hop questions; single retrieval often insufficient for complex reasoning or when memory is outdated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Li, J., Wang, X., Ding, W., Wang, Z., Kang, Y., Jia, Z., Zheng, Z., Meloni, G., 2024. RAM: Towards an Ever-Improving Memory System by Learning from Communications. arXiv:2404.12045.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAM: Towards an Ever-Improving Memory System by Learning from Communications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6456.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6456.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-upd</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RAG with updated memory (RAG-upd)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant where the memory is updated using direct answers (ground truth) for all learning traces and subsequent RAG-only inference is performed over the updated memory M_upd.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG-upd</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RAG pipeline that uses a memory which has been updated offline/naively by inserting direct-answer feedback (the direct ground-truth answers) into the memory, then runs standard retrieval and generation on M_upd.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>evaluated with LLaMA-2 models and others as in paper</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external vector store / RAG with memory augmentation via appended direct answers</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw text passages and direct answer sentences added to memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>similarity search over the updated memory (top-1); updates performed by inserting direct answers as memory items</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FreshQA and MQuAKE-T</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>knowledge‑intensive QA / retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>FreshQA GPT4_score = 63.85 (RAG-upd reported); MQuAKE GPT4_score = 36.10</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Compared to non-updated RAG-only FreshQA = 33.77 and self-knowledge 36.36, showing large gains for some datasets; for MQuAKE RAM outperforms RAG-upd despite RAG-upd having ground-truth memory.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>GPT4_score, BERTScore</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Naive insertion of all feedback into memory causes unbounded memory growth, which authors argue is infeasible (slower retrieval, less accurate retrieval due to noise) — motivating local edits instead of full accumulation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Does not always outperform RAM (e.g., MQuAKE multi-hop dataset) because multi-hop problems may require interleaved retrieval and reasoning rather than just storing direct answers; naive accumulation can introduce noise and slow retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Li, J., Wang, X., Ding, W., Wang, Z., Kang, Y., Jia, Z., Zheng, Z., Meloni, G., 2024. RAM: Towards an Ever-Improving Memory System by Learning from Communications. arXiv:2404.12045.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAM: Towards an Ever-Improving Memory System by Learning from Communications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6456.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6456.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TeachMe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TeachMe (Dalvi et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A teachable QA system that allows users to supply corrections which are stored in a dynamic memory for future retrieval; cited as related work on teachable RAG systems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TeachMe</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A system that accepts user corrections and stores them as dynamic memory to be retrieved in future turns; intended to prevent repeated mistakes by saving user-supplied corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dynamic memory (user-provided corrections stored for retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>user-supplied correction text entries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval (semantic matching) from stored corrections</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>teachable QA / deployed QA correction tasks (as reported in the referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>question answering / continual learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Paper notes TeachMe only appends corrections and does not maintain well-updated, fused memory — cited as a limitation compared to RAM's localized editing.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Described in this paper as limited because it simply adds user corrections without knowledge fusion or continual maintenance, leading to less effective long-term memory management.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dalvi, B., Tafjord, O., Clark, P., 2022. Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement. (cited in Li et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAM: Towards an Ever-Improving Memory System by Learning from Communications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6456.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6456.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion (Shinn et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-agent framework that uses verbal reinforcement (self-reflection) to learn from errors and improve via in‑context continual learning; cited as related work combining self-reflection and memory concepts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent that records past experiences and self-reflective verbal reinforcement to guide future behavior (verbal memory), enabling in-context continual improvement without re-training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>verbal / scratchpad-based episodic memory of past trials</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>textual scratchpads / dialogs reflecting past trials</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>in-context retrieval of prior reflections; not a vector DB in this paper's description</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>language-agent tasks and sequential decision problems (as per original Reflexion paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>agent learning / in-context continual learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Cited as a complementary approach to RAM's communicative learning; no experimental numbers in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Shinn, N., Cassano, F., Gopinath, A., Narasimhan, R., 2023. Reflexion: Language agents with verbal reinforcement learning. (cited in Li et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAM: Towards an Ever-Improving Memory System by Learning from Communications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6456.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6456.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FB-NET / Learning-to-Repair</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning to repair / FB-NET (Tandon et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systems that use a dynamic memory of feedback to prevent repeated mistakes and repair model outputs after deployment; cited in related work on memory-assisted repair.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>FB-NET / Learning-to-Repair</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Approach that records feedback and uses dynamic memory of past corrections to prevent repeating errors (applied to script generation / repair tasks in the referenced literature).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>dynamic memory of feedback</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>past corrections / repair records (text)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval of prior corrections to guide future output repair</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>script generation & deployed repair tasks (as per referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>generation error repair / continual improvement</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td>Cited as relevant prior work; Li et al. note general lack of delicate data acquisition and maintenance in such memories motivating RAM design.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Tandon, N., Madaan, A., Clark, P., Yang, Y., 2022. Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. (cited in Li et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAM: Towards an Ever-Improving Memory System by Learning from Communications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6456.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6456.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Jarvis-1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Jarvis-1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-world multitask agent that leverages memory-augmented multimodal language models for planning and action; cited as related work using memory for agent experience.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Jarvis-1: Open-world multitask agents with memory-augmented multimodal language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Jarvis-1</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent that uses memory of historical action sequences and multimodal contexts to support open-world planning and task execution; mentioned as relevant prior work on memory-augmented agents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>memory-augmented multimodal store (historical action sequences as experience)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>historical action sequences and multimodal context data</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval of relevant past action sequences / contexts to inform planning</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>open-world multi-task planning and execution</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>planning / memory-augmented agents</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Wang, Z., Cai, S., Liu, A., et al., 2023. Jarvis-1: Open-world multitask agents with memory-augmented multimodal language models. (cited in Li et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAM: Towards an Ever-Improving Memory System by Learning from Communications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6456.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6456.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lift-Yourself-Up</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Lift-Yourself-Up: Retrieval-augmented text generation with self memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented text generation system that incorporates self-memory; cited as related work for self memory approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lift-Yourself-Up: Retrieval-augmented text generation with self memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Lift-Yourself-Up</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retrieval-augmented generation approach that stores and uses self-memory to improve generation; cited among related retrieval/memory augmentation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>self memory (RAG with self-memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>stored self-statements / generated memory entries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval from stored self-memory</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>retrieval-augmented text generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>text generation / memory augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Cheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., Yan, R., 2023. Lift-Yourself-Up: Retrieval-augmented text generation with self memory. (cited in Li et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAM: Towards an Ever-Improving Memory System by Learning from Communications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6456.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6456.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents that employ a memory mechanism to solve tasks, including details of the memory type, how it is accessed or updated, the tasks/benchmarks evaluated, performance with and without the memory, and any reported trade‑offs or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory-assisted prompt editing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-assisted prompt editing to improve GPT-3 after deployment (Madaan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that leverages stored memory to edit prompts post-deployment to improve model behavior; cited among memory-editing approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memory-assisted prompt editing to improve gpt-3 after deployment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memory-assisted prompt editing</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Technique using stored memory / examples to edit prompts dynamically to correct or improve deployed LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external memory for prompt editing</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>example snippets / corrective prompts</td>
                        </tr>
                        <tr>
                            <td><strong>memory_access_mechanism</strong></td>
                            <td>retrieval and prompt edit</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>post-deployment prompt correction</td>
                        </tr>
                        <tr>
                            <td><strong>task_category</strong></td>
                            <td>prompt engineering / continual improvement</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_reported</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Madaan, A., Tandon, N., Clark, P., Yang, Y., 2023. Memory-assisted prompt editing to improve gpt-3 after deployment. (cited in Li et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAM: Towards an Ever-Improving Memory System by Learning from Communications', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback <em>(Rating: 2)</em></li>
                <li>Jarvis-1: Open-world multitask agents with memory-augmented multimodal language models <em>(Rating: 2)</em></li>
                <li>Lift-Yourself-Up: Retrieval-augmented text generation with self memory <em>(Rating: 2)</em></li>
                <li>Memory-assisted prompt editing to improve gpt-3 after deployment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6456",
    "paper_id": "paper-269214526",
    "extraction_schema_id": "extraction-schema-127",
    "extracted_data": [
        {
            "name_short": "RAM",
            "name_full": "Retrieval Augmented Memory (RAM)",
            "brief_description": "An ever‑improving RAG-based memory system that interleaves recursive reasoning (R^3) and a memory‑reflection module to update an external vector memory from communicative human feedback, enabling continual learning without re-training the base LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAM",
            "agent_description": "A RAG-based agent that runs recursive reasoning-based retrieval (R^3) (iterative Reason → Retrieval → Inference trials) over an external vector store, solicits human feedback when stuck, generates a 'reflected memory' from ground truth + trial scratchpads, and locally edits the stored document/sentence to replace the most relevant memory entry with the reflected memory (thus keeping memory compact and continually updated).",
            "model_size": "evaluated with 7B and 13B LLaMA variants and Vicuna-7B; also compared to GPT-3.5-turbo",
            "memory_used": true,
            "memory_type": "external vector store / retrieval‑augmented generation (RAG) with local semantic edits (ChromaDB vector DB + BM25 for sentence localization)",
            "memory_representation": "raw text passages/documents encoded as sentence embeddings (all-MiniLM-L6-v2) and stored as documents/sentences in the vector DB; reflected memories are textual sentences used to replace specific sentences in stored documents.",
            "memory_access_mechanism": "iterative similarity search (L2 / embedding cosine) over vector DB (top-1 by default) during R^3; localization for update uses BM25 sentence-level similarity to select and replace the most relevant sentence (local edit).",
            "task_name": "FreshQA and MQuAKE-T (QA datasets with fresh/ever-changing knowledge)",
            "task_category": "knowledge‑intensive question answering / multi‑hop reasoning",
            "performance_with_memory": "FreshQA GPT4_score = 60.17 (RAM with feedback and memory updates); MQuAKE GPT4_score = 48.96 (RAM)",
            "performance_without_memory": "Self-knowledge baseline FreshQA GPT4_score = 36.36; RAG-only (no memory update, using outdated M_old) FreshQA GPT4_score = 33.77; MQuAKE self-knowledge = 12.66; RAG-only MQuAKE = 38.51",
            "has_comparative_results": true,
            "performance_metric": "GPT4_score (automated evaluator) and BERTScore (semantic similarity)",
            "tradeoffs_reported": "Increased inference time due to multi-step trials (reported average per-step ~8s and per-trial ~30s for LLaMA-2-7B); potential memory growth and retrieval slowdown if naive accumulation of feedback is used (mitigated by local sentence-level edits); hints vs direct-answer tradeoff (hints induce better long-term reasoning improvements while direct answers give faster but more 'spoon-fed' gains).",
            "limitations_or_failure_cases": "Less helpful for fast‑changing facts beyond teacher/external knowledge scope; sometimes lower BERTScore despite improved GPT4_score (surface similarity vs correctness); failure modes include bad retrieval (~40% of sampled failures), model hallucination (~32%), question misunderstanding (~12%), knowledge deficiency due to outdated memory (~12%), and occasional bad reasoning (~4%). Memory currently implemented as vector DB only — authors note potential improvements with structured graphs or relational DBs.",
            "citation": "Li, J., Wang, X., Ding, W., Wang, Z., Kang, Y., Jia, Z., Zheng, Z., Meloni, G., 2024. RAM: Towards an Ever-Improving Memory System by Learning from Communications. arXiv:2404.12045.",
            "uuid": "e6456.0",
            "source_info": {
                "paper_title": "RAM: Towards an Ever-Improving Memory System by Learning from Communications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "R^3",
            "name_full": "Recursive Reasoning-based Retrieval (R^3)",
            "brief_description": "A recursive chain‑of‑thought style retrieval loop used inside RAM that iteratively Reason → Retrieval → Inference to refine retrieval queries and gather multi-step evidence from the vector memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "R^3 (component of RAM)",
            "agent_description": "A retrieval-and-reasoning controller that decomposes the query into actions (Search steps), forms reflected keywords from intermediate reasoning, retrieves top relevant documents from the vector memory, and repeats until a satisfactory inference or max trials; triggers human 'ask-for-help' when stuck on repeated identical inferences.",
            "model_size": "evaluated with same LLaMA/Vicuna/GPT3.5 models as RAM experiments (7B, 13B, Vicuna-7B, GPT-3.5-turbo)",
            "memory_used": true,
            "memory_type": "external vector store (RAG)",
            "memory_representation": "raw text passages/documents encoded as embeddings",
            "memory_access_mechanism": "iterative similarity-based retrieval (top-1 L2 / embedding similarity) using progressively updated retrieval keys derived from reasoning trace",
            "task_name": "FreshQA and MQuAKE-T (as the retrieval/reasoning component)",
            "task_category": "multi-hop reasoning / retrieval-guided QA",
            "performance_with_memory": "When used (RAM-R^3) without memory updates: FreshQA GPT4_score = 45.98; MQuAKE GPT4_score = 27.41",
            "performance_without_memory": "Compared with self-knowledge (no retrieval): FreshQA self-knowledge = 36.36; RAG-only (single retrieval, no R^3) = 33.77, showing R^3 improves retrieval-guided reasoning even without memory updates.",
            "has_comparative_results": true,
            "performance_metric": "GPT4_score, BERTScore",
            "tradeoffs_reported": "Requires multiple steps/trials (higher latency) and intermediate retrievals can sometimes decrease similarity temporarily (retrieved contexts may contain overlapping keywords but not relevant facts), but overall provides better evidence and reduces hallucinations in multi-hop questions.",
            "limitations_or_failure_cases": "Intermediate retrieval steps sometimes retrieve noisy or less relevant context; performance depends on quality of retrieval and underlying model reasoning capability.",
            "citation": "Li, J., Wang, X., Ding, W., Wang, Z., Kang, Y., Jia, Z., Zheng, Z., Meloni, G., 2024. RAM: Towards an Ever-Improving Memory System by Learning from Communications. arXiv:2404.12045.",
            "uuid": "e6456.1",
            "source_info": {
                "paper_title": "RAM: Towards an Ever-Improving Memory System by Learning from Communications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "RAG-only",
            "name_full": "Retrieval-Augmented Generation (RAG-only baseline)",
            "brief_description": "A standard RAG baseline that answers questions by retrieving from an external memory M_old without any iterative recursive retrieval or memory update.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "RAG-only",
            "agent_description": "Conventional retrieval-augmented pipeline that fetches top-k documents by embedding similarity from the static memory and conditions the LLM to generate an answer; in this paper used as baseline (no memory updates, no R^3 or feedback-driven edits).",
            "model_size": "evaluated with LLaMA-2-7B/13B, Vicuna-7B, GPT-3.5-turbo",
            "memory_used": true,
            "memory_type": "external vector store / retrieval-augmented generation",
            "memory_representation": "raw text passages encoded as embeddings (documents retrieved as context)",
            "memory_access_mechanism": "single-shot similarity search (L2 / embedding) retrieving top-1 document by default (chain type 'stuff')",
            "task_name": "FreshQA and MQuAKE-T",
            "task_category": "knowledge‑intensive QA / retrieval",
            "performance_with_memory": "FreshQA GPT4_score = 33.77; MQuAKE GPT4_score = 38.51",
            "performance_without_memory": "Self-knowledge (no retrieval) FreshQA GPT4_score = 36.36 (RAG-only underperforms self-knowledge on some settings); MQuAKE self-knowledge = 12.66",
            "has_comparative_results": true,
            "performance_metric": "GPT4_score, BERTScore",
            "tradeoffs_reported": "Single-shot retrieval is faster but less capable for multi-hop or false‑premise questions; retrieval quality critically affects performance and hallucination risk.",
            "limitations_or_failure_cases": "Fails on many false-premise and multi-hop questions; single retrieval often insufficient for complex reasoning or when memory is outdated.",
            "citation": "Li, J., Wang, X., Ding, W., Wang, Z., Kang, Y., Jia, Z., Zheng, Z., Meloni, G., 2024. RAM: Towards an Ever-Improving Memory System by Learning from Communications. arXiv:2404.12045.",
            "uuid": "e6456.2",
            "source_info": {
                "paper_title": "RAM: Towards an Ever-Improving Memory System by Learning from Communications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "RAG-upd",
            "name_full": "RAG with updated memory (RAG-upd)",
            "brief_description": "A variant where the memory is updated using direct answers (ground truth) for all learning traces and subsequent RAG-only inference is performed over the updated memory M_upd.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "RAG-upd",
            "agent_description": "RAG pipeline that uses a memory which has been updated offline/naively by inserting direct-answer feedback (the direct ground-truth answers) into the memory, then runs standard retrieval and generation on M_upd.",
            "model_size": "evaluated with LLaMA-2 models and others as in paper",
            "memory_used": true,
            "memory_type": "external vector store / RAG with memory augmentation via appended direct answers",
            "memory_representation": "raw text passages and direct answer sentences added to memory",
            "memory_access_mechanism": "similarity search over the updated memory (top-1); updates performed by inserting direct answers as memory items",
            "task_name": "FreshQA and MQuAKE-T",
            "task_category": "knowledge‑intensive QA / retrieval",
            "performance_with_memory": "FreshQA GPT4_score = 63.85 (RAG-upd reported); MQuAKE GPT4_score = 36.10",
            "performance_without_memory": "Compared to non-updated RAG-only FreshQA = 33.77 and self-knowledge 36.36, showing large gains for some datasets; for MQuAKE RAM outperforms RAG-upd despite RAG-upd having ground-truth memory.",
            "has_comparative_results": true,
            "performance_metric": "GPT4_score, BERTScore",
            "tradeoffs_reported": "Naive insertion of all feedback into memory causes unbounded memory growth, which authors argue is infeasible (slower retrieval, less accurate retrieval due to noise) — motivating local edits instead of full accumulation.",
            "limitations_or_failure_cases": "Does not always outperform RAM (e.g., MQuAKE multi-hop dataset) because multi-hop problems may require interleaved retrieval and reasoning rather than just storing direct answers; naive accumulation can introduce noise and slow retrieval.",
            "citation": "Li, J., Wang, X., Ding, W., Wang, Z., Kang, Y., Jia, Z., Zheng, Z., Meloni, G., 2024. RAM: Towards an Ever-Improving Memory System by Learning from Communications. arXiv:2404.12045.",
            "uuid": "e6456.3",
            "source_info": {
                "paper_title": "RAM: Towards an Ever-Improving Memory System by Learning from Communications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "TeachMe",
            "name_full": "TeachMe (Dalvi et al., 2022)",
            "brief_description": "A teachable QA system that allows users to supply corrections which are stored in a dynamic memory for future retrieval; cited as related work on teachable RAG systems.",
            "citation_title": "Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement",
            "mention_or_use": "mention",
            "agent_name": "TeachMe",
            "agent_description": "A system that accepts user corrections and stores them as dynamic memory to be retrieved in future turns; intended to prevent repeated mistakes by saving user-supplied corrections.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "dynamic memory (user-provided corrections stored for retrieval)",
            "memory_representation": "user-supplied correction text entries",
            "memory_access_mechanism": "retrieval (semantic matching) from stored corrections",
            "task_name": "teachable QA / deployed QA correction tasks (as reported in the referenced work)",
            "task_category": "question answering / continual learning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Paper notes TeachMe only appends corrections and does not maintain well-updated, fused memory — cited as a limitation compared to RAM's localized editing.",
            "limitations_or_failure_cases": "Described in this paper as limited because it simply adds user corrections without knowledge fusion or continual maintenance, leading to less effective long-term memory management.",
            "citation": "Dalvi, B., Tafjord, O., Clark, P., 2022. Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement. (cited in Li et al., 2024)",
            "uuid": "e6456.4",
            "source_info": {
                "paper_title": "RAM: Towards an Ever-Improving Memory System by Learning from Communications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion (Shinn et al., 2023)",
            "brief_description": "A language-agent framework that uses verbal reinforcement (self-reflection) to learn from errors and improve via in‑context continual learning; cited as related work combining self-reflection and memory concepts.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "An agent that records past experiences and self-reflective verbal reinforcement to guide future behavior (verbal memory), enabling in-context continual improvement without re-training.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "verbal / scratchpad-based episodic memory of past trials",
            "memory_representation": "textual scratchpads / dialogs reflecting past trials",
            "memory_access_mechanism": "in-context retrieval of prior reflections; not a vector DB in this paper's description",
            "task_name": "language-agent tasks and sequential decision problems (as per original Reflexion paper)",
            "task_category": "agent learning / in-context continual learning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Cited as a complementary approach to RAM's communicative learning; no experimental numbers in this paper.",
            "limitations_or_failure_cases": null,
            "citation": "Shinn, N., Cassano, F., Gopinath, A., Narasimhan, R., 2023. Reflexion: Language agents with verbal reinforcement learning. (cited in Li et al., 2024)",
            "uuid": "e6456.5",
            "source_info": {
                "paper_title": "RAM: Towards an Ever-Improving Memory System by Learning from Communications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "FB-NET / Learning-to-Repair",
            "name_full": "Learning to repair / FB-NET (Tandon et al., 2022)",
            "brief_description": "Systems that use a dynamic memory of feedback to prevent repeated mistakes and repair model outputs after deployment; cited in related work on memory-assisted repair.",
            "citation_title": "Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback",
            "mention_or_use": "mention",
            "agent_name": "FB-NET / Learning-to-Repair",
            "agent_description": "Approach that records feedback and uses dynamic memory of past corrections to prevent repeating errors (applied to script generation / repair tasks in the referenced literature).",
            "model_size": null,
            "memory_used": true,
            "memory_type": "dynamic memory of feedback",
            "memory_representation": "past corrections / repair records (text)",
            "memory_access_mechanism": "retrieval of prior corrections to guide future output repair",
            "task_name": "script generation & deployed repair tasks (as per referenced work)",
            "task_category": "generation error repair / continual improvement",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": "Cited as relevant prior work; Li et al. note general lack of delicate data acquisition and maintenance in such memories motivating RAM design.",
            "limitations_or_failure_cases": null,
            "citation": "Tandon, N., Madaan, A., Clark, P., Yang, Y., 2022. Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. (cited in Li et al., 2024)",
            "uuid": "e6456.6",
            "source_info": {
                "paper_title": "RAM: Towards an Ever-Improving Memory System by Learning from Communications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Jarvis-1",
            "name_full": "Jarvis-1",
            "brief_description": "An open-world multitask agent that leverages memory-augmented multimodal language models for planning and action; cited as related work using memory for agent experience.",
            "citation_title": "Jarvis-1: Open-world multitask agents with memory-augmented multimodal language models",
            "mention_or_use": "mention",
            "agent_name": "Jarvis-1",
            "agent_description": "An agent that uses memory of historical action sequences and multimodal contexts to support open-world planning and task execution; mentioned as relevant prior work on memory-augmented agents.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "memory-augmented multimodal store (historical action sequences as experience)",
            "memory_representation": "historical action sequences and multimodal context data",
            "memory_access_mechanism": "retrieval of relevant past action sequences / contexts to inform planning",
            "task_name": "open-world multi-task planning and execution",
            "task_category": "planning / memory-augmented agents",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": null,
            "citation": "Wang, Z., Cai, S., Liu, A., et al., 2023. Jarvis-1: Open-world multitask agents with memory-augmented multimodal language models. (cited in Li et al., 2024)",
            "uuid": "e6456.7",
            "source_info": {
                "paper_title": "RAM: Towards an Ever-Improving Memory System by Learning from Communications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Lift-Yourself-Up",
            "name_full": "Lift-Yourself-Up: Retrieval-augmented text generation with self memory",
            "brief_description": "A retrieval-augmented text generation system that incorporates self-memory; cited as related work for self memory approaches.",
            "citation_title": "Lift-Yourself-Up: Retrieval-augmented text generation with self memory",
            "mention_or_use": "mention",
            "agent_name": "Lift-Yourself-Up",
            "agent_description": "Retrieval-augmented generation approach that stores and uses self-memory to improve generation; cited among related retrieval/memory augmentation methods.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "self memory (RAG with self-memory)",
            "memory_representation": "stored self-statements / generated memory entries",
            "memory_access_mechanism": "retrieval from stored self-memory",
            "task_name": "retrieval-augmented text generation",
            "task_category": "text generation / memory augmentation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": null,
            "citation": "Cheng, X., Luo, D., Chen, X., Liu, L., Zhao, D., Yan, R., 2023. Lift-Yourself-Up: Retrieval-augmented text generation with self memory. (cited in Li et al., 2024)",
            "uuid": "e6456.8",
            "source_info": {
                "paper_title": "RAM: Towards an Ever-Improving Memory System by Learning from Communications",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Memory-assisted prompt editing",
            "name_full": "Memory-assisted prompt editing to improve GPT-3 after deployment (Madaan et al.)",
            "brief_description": "A method that leverages stored memory to edit prompts post-deployment to improve model behavior; cited among memory-editing approaches.",
            "citation_title": "Memory-assisted prompt editing to improve gpt-3 after deployment",
            "mention_or_use": "mention",
            "agent_name": "Memory-assisted prompt editing",
            "agent_description": "Technique using stored memory / examples to edit prompts dynamically to correct or improve deployed LLM outputs.",
            "model_size": null,
            "memory_used": true,
            "memory_type": "external memory for prompt editing",
            "memory_representation": "example snippets / corrective prompts",
            "memory_access_mechanism": "retrieval and prompt edit",
            "task_name": "post-deployment prompt correction",
            "task_category": "prompt engineering / continual improvement",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "performance_metric": null,
            "tradeoffs_reported": null,
            "limitations_or_failure_cases": null,
            "citation": "Madaan, A., Tandon, N., Clark, P., Yang, Y., 2023. Memory-assisted prompt editing to improve gpt-3 after deployment. (cited in Li et al., 2024)",
            "uuid": "e6456.9",
            "source_info": {
                "paper_title": "RAM: Towards an Ever-Improving Memory System by Learning from Communications",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement",
            "rating": 2,
            "sanitized_title": "towards_teachable_reasoning_systems_using_a_dynamic_memory_of_user_feedback_for_continual_system_improvement"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback",
            "rating": 2,
            "sanitized_title": "learning_to_repair_repairing_model_output_errors_after_deployment_using_a_dynamic_memory_of_feedback"
        },
        {
            "paper_title": "Jarvis-1: Open-world multitask agents with memory-augmented multimodal language models",
            "rating": 2,
            "sanitized_title": "jarvis1_openworld_multitask_agents_with_memoryaugmented_multimodal_language_models"
        },
        {
            "paper_title": "Lift-Yourself-Up: Retrieval-augmented text generation with self memory",
            "rating": 2,
            "sanitized_title": "liftyourselfup_retrievalaugmented_text_generation_with_self_memory"
        },
        {
            "paper_title": "Memory-assisted prompt editing to improve gpt-3 after deployment",
            "rating": 1,
            "sanitized_title": "memoryassisted_prompt_editing_to_improve_gpt3_after_deployment"
        }
    ],
    "cost": 0.019433,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RAM: Towards an Ever-Improving Memory System by Learning from Communications
15 January 1977</p>
<p>Jiaqi Li lijiaqi@bigai.ai 
Beijing Institute for General Artificial Intelligence (BIGAI)
BeijingChina</p>
<p>Xiaobo Wang wangxiaobo@bigai.ai 
Beijing Institute for General Artificial Intelligence (BIGAI)
BeijingChina</p>
<p>Wentao Ding dingwentao@bigai.ai 
Beijing Institute for General Artificial Intelligence (BIGAI)
BeijingChina</p>
<p>Zihao Wang wangzihao@bigai.ai 
Beijing Institute for General Artificial Intelligence (BIGAI)
BeijingChina</p>
<p>Yipeng Kang kangyipeng@bigai.ai 
Beijing Institute for General Artificial Intelligence (BIGAI)
BeijingChina</p>
<p>Zixia Jia jiazixia@bigai.ai 
Beijing Institute for General Artificial Intelligence (BIGAI)
BeijingChina</p>
<p>Zilong Zheng zlzheng@bigai.ai 
Beijing Institute for General Artificial Intelligence (BIGAI)
BeijingChina</p>
<p>Giorgia Meloni 
Beijing Institute for General Artificial Intelligence (BIGAI)
BeijingChina</p>
<p>RAM: Towards an Ever-Improving Memory System by Learning from Communications
15 January 197782B2624C3CC0903358F71609317D1A6DarXiv:2404.12045v2[cs.AI]
We introduce an innovative RAG-based framework with an ever-improving memory.Inspired by humans' pedagogical process, RAM utilizes recursively reasoning-based retrieval and experience reflections to continually update the memory and learn from users' communicative feedback, namely communicative learning.Extensive experiments with both simulated and real users demonstrate significant improvements over traditional RAG and selfknowledge methods, particularly excelling in handling false premise and multi-hop questions.Furthermore, RAM exhibits promising adaptability to various feedback and retrieval methods, showcasing its potential for advancing AI capabilities in dynamic knowledge acquisition and lifelong learning.</p>
<p>Introduction</p>
<p>"Learning, flexibility, and attention to the partner are obviously fundamental characteristics of the human way of communicating, and things simply could not move in the human direction until they were present."- Tomasello (2010) Human learning, extended as a lifelong process, typically operates in a communicative and cooperative framework among people via different forms of interactions within the physical and social world, as evidenced by the above quotes of Tomasello (2010).From toddlers to academic graduates, the learning process (referred to as pedagogy; Shulman (1987)) often involves two communicative roles: a student that perceives, reasons and learns over the communicative messages (e.g., conversations) to update his/her internal knowledge (a.k.a.belief); and a teacher that delivers messages and provides feedback based on his/her professionalism.Such a learning paradigm, named communicative learning (CL; Yuan and Zhu (2023)) or interactive learning (IL; De Raedt and Bruynooghe (1992)), is considered natural and transparent along with numerous advantages that today's artificial intelligence (AI) systems seek to obtain, e.g., explainability (Lakkaraju et al., 2022), life-long grown skills (Dalvi et al., 2022), etc.</p>
<p>Empowered by the recent surge of large language models (LLMs), many research works have attempted to build complicated AI agents to perform a spectrum of tasks with emergent capacities, such as Human-AI conversations, in-context reasoning (Wei et al., 2022), situated planning (Wang et al., 2023a,b), etc.Despite the compelling success, the immutability and uninterpretability of pre-trained language models yield non-trivial challenges in building communicatively-learned AI (CLAI) agents: (i) the implicit knowledge representation makes it hard to revise or edit pre-trained memory and may result in "hallucinations" (Lewis et al., 2020); (ii) the limited context window size constrains the potential to take in the entire context history as memory (Li et al., 2023).More recently, retrieval-augmented generation (RAG; Lewis et al. (2020)) is proposed to enable accessing and precisely manipulating the memory with a disentangled knowledge storage system; refer to §4 for details.However, conventional RAG augments LLMs with a static and exterior knowledge to address knowledge-intensive tasks.Fundamentally, the main challenge of building CLAI agents lies in determining when and how to update dynamic and internal knowledge given communicative feedback.</p>
<p>In this work, we introduce an innovative ever-improving memory system RAM (namely Retrieval Augmented Memory), in analogy to the fast-updated memory cache in computer systems (see Fig. 1).Without periodically re-training with a huge number of parameters, RAM enables LLMs to obtain fresh knowledge and historical experience</p>
<p>Reflected Memory</p>
<p>Inference ？ Geppettos child Pinocchio, is a citizen of Italy.Giuseppe Conte is the head of government of Italy before 2019 and now it is Giorgia Meloni.</p>
<p>The current head of state can change due to elections or other political events.Giuseppe Conte term ended in 2019.</p>
<p>Human Feedback (Hints)</p>
<p>Multi-step Reflection Continual Update</p>
<p>Prompts</p>
<p>Solve a question answering task with interleaving Thought, Action, Observation steps.[…] You will be given a previous reasoning trial in which you had access to an external database and a question to answer.[…] Given the latest relevant fact, please update/edit the existing memory based on the fact.[…] 1 2 3 Question Who is the head of government in the country which is the citizenship of Mister Geppetto's child?</p>
<p>Large Language Models</p>
<p>RAM Trial 1: Geppettos child is a citizen of Italy, therefore the head of government is the President of the Italian Republic.</p>
<p>Trial 2: Giuseppe Conte is the head of government in the country ...</p>
<p>Trial 3:</p>
<p>Giuseppe Conte is the head of Italy where Mister Geppetto's child ... (same as history)</p>
<p>Figure 1: Learning framework of RAM.Blue boxes indicate LLMs' in-context reasoning and the green box indicates feedback from external users.➊ Given a new question, LLMs take multi-step reasoning and inference through self-reflection.If the current inference is the same as in previous trials, the human will provide additional hints as feedback to help LLMs better answer.➋ Relevant knowledge is recursively retrieved from memory based on LLMs' reasoning.➌ LLMs generate a reflected memory learning from the feedback and the ground truth to update the memory.All prompts are shortened for simplicity; refer to Appendix F for complete templates.</p>
<p>by dynamically improving and growing a continually updated memory through human communications ( §2).Specifically, RAM is composed of a recursive reasoning-based retrieval and a memory-reflection module: the former enables a faithful and selfrefined reasoning trajectory throughout a recursively retrieval-based reasoning process (R 3 ; §2.1); the latter enriches the dynamic memory with current observations and user feedback for further selfimprovement.To be close to real-world human teaching, we investigate different forms of common human feedback ( §2.3) to ablate RAM's performance.One close work to us is TeachMe (Dalvi et al., 2022), which aims to create a teachable QA system.However, TeachMe is restricted by solely adding user-supplied corrections to erroneous model beliefs for further retrieval, in which memory is far from well-updated and continually maintained.</p>
<p>In §3, comprehensive experiments with both simulated and real users empirically show that RAM largely promotes the performance on various baselines, backbones and categories of knowledge.Specifically, under the evaluation metric of GPT4_score on two datasets, RAM achieves an average improvement of 30% over self-knowledge and 40% over RAG-only.Notably, RAM exceeds methods with ground-truth updated memory.Moreover, RAM excelled in false premise and multi-hop questions which current LLMs still struggle with.It is worth noting that for novel questions, the model gains 10-20% with self-reflected memory, allowing learning ever-changing knowledge and improving the reasoning capabilities in the long run.Finally, we make robust ablation studies to demonstrate the generalization of RAM to different ways of teaching, feedback and retrieval chain types in practice.</p>
<p>The RAM Framework</p>
<p>In this section, we provide detailed descriptions of each key component of RAM and how they collaborate in a unified framework.Fig. 1 depicts the RAM architecture.Detailed implementations are introduced in Appendix A.</p>
<p>R 3 : Recursive Reasoning-based Retrieval</p>
<p>Many previous methods have attempted to interleave reasoning and acting (Yao et al., 2023;Shinn et al., 2023) in a chain-of-thought (CoT) paradigm.More recently, such strategies have also been integrated with retrieval-based tools during the rea-Algorithm 1: Reasoning process of R 3 Input: Query Q, memory buffer M, ground truth G, action A = ∅, prompts ➊ and ➋ as in Fig. 1.Get feedback with hints:
1 for tri = 1 to N do 2 while A ̸ = Finish do /*F B ← Feedback(Q, G) 8 if sim(Inf, G) &gt; accept_threshold then 9 break 10 return (m * i , Infi, F Bi), i ∈ [1, ..., N ]
soning trace (Luo et al., 2024).However, these prompting methods, simply taking the query or its variants for RAG, failed to consider the dynamic semantics during the reasoning process.For instance in Fig. 1, the initial key semantics of the query is "Mister Geppetoś child".Due to the nature of semantic matching, most likely the RAG engine will produce information w.r.t."Mister Geppeto" or his child.However, by deeply going further along the reasoning trace, more extra information (e.g., "head of Italy") has to be taken into consideration.To this end, we propose R 3 , a recursive CoT paradigm that prompts the model to iteratively retrieve and reason step-by-step to solve the question with a vector-based memory.Given a query Q and a memory buffer
M initialized with K outdated knowledge M = {m 1 , • • • , m K }. The whole R 3 process runs in a trial loop {tr i , 1 ≤ i ≤ N }. At tr i , R 3 runs a sequential steps of Reason-Retrieval-Inference.
Reason: R 3 reasons on Q and decomposes it into a few plausible actions A, e.g., "I need to search ... Then search ... ".We then formalize the reasoning results to a sequence of actions Search using self-reflection.Retrieval: The action Search with a reflected keyword or phrase r (e.g., "Minster Geppetos child") is the result of reasoning on Q. R 3 retrieves the most relevant memory w.r.t.m * = arg max m∈M sim(r, m).R 3 continues the Reason-Retrieval process until it finishes retrieval with an inference result.Inference: The model inferences on all the retrieved memory to obtain an inference result Inf (e.g., "President of the Italian Republic" in Trial.If Inf is judged wrong, a new trial tr i+1 for R 3 starts for more attempts.Otherwise, the model starts to update M ( §2.2).Inferences and feedback in all the trials are stored in the scratchpad for memory update.The overall process can be seen in Algorithm 1.</p>
<p>It has been shown from previous works (Gao et al., 2024;Yan et al., 2024) that retrieval based on text similarity is far from enough to cope with complex tasks.Instead of a single call to retrieve the answer directly, R 3 proposed in RAM produces a promising paradigm to discover a faithful reasoning trace leading to a probable correct answer through multiple rounds of inference, reflection and interaction with external user/environment.Iterative retrieval provides sufficient contexts and situated evidence for forward reasoning, especially when coupled with restricted knowledge.In the meantime, interweaving reasoning on the track of retrieval helps to clarify the search direction, decompose the complex multi-retrieval task through planning, and narrow down the retrieval objective to get the final answer.</p>
<p>Ask for help In RAM, we maintain a list of historical inferences for each trial, i.e., Inf = {Inf 1 , • • • , Inf N }.During R 3 , if the current Inf i is semantically the same as any historical inference result, an "ask-for-help" mechanism is activated by querying human users for more hints to assist the reasoning process.It is possibly due to a lack of knowledge or a confined mindset on retrieval of the model which needs external help.In §2.3, we propose different categories of human feedback in RAM.</p>
<p>An Ever-Improving Memory</p>
<p>Existing RAG-based methods (Tandon et al., 2022a;Madaan et al., 2023;Sarch et al., 2023) suppose the knowledge is up-to-date (Du et al., 2023;Zhong et al., 2023) or can be acquired directly from a search engine (Vu et al., 2023).A naive solution is to add all feedback F B to M without dealing with knowledge fusion and alignment.The infinitely enlarging memory, however, makes the retrieval process time-consuming and inaccurate and is therefore infeasible for real-world and complicated contexts.</p>
<p>Let M old denote the initial memory buffer and M cur denote the current memory buffer, the memory update process in RAM goes as follows.Given a new query Q, after the R 3 process ends with a cor-rect Inf or reaches tr N , we localize the most relevant m * ∈ M cur and locally edit it.Specifically, we start by collecting inferences and feedback in all the trials as context and prompting the LLM with the ground truth G to generate a reflected memory
m R = reflect(G, Inf 1 , F B 1 , Inf 2 , • • • ),
where reflect(•) denotes step ➌ in Fig. 1.Then we utilize the semantic similarity to localize the most relevant memory piece m * = arg max m∈M cur sim(m, m R ) and update the memory buffer to M upd by replacing m * as m R .The updated memory M upd is extensive and adaptive to ever-changing knowledge in the real world as the latest information is absorbed and outdated data is modified or discarded.</p>
<p>Knowledge From Human Feedback</p>
<p>Interactively learning from feedback is crucial for agents to avoid repeated errors from historical trials and accelerate the learning process with limited knowledge and capabilities.Closer to how humans learn, there are various types of feedback, that can benefit the model to get a reward/signal for its current performance, an explanation of past behaviors, instructions for future behavior learning through iterative interactions.We describe three different categories of human feedback in RAM as below.Feedback without explanation It serves as an automatic evaluator for Inf i in tr i for further retrieval and self-reflection.We compute the semantic similarity between embeddings of Inf with G based on the pre-defined threshold.Using automatic similarity as F B provides more flexibility than traditional n-gram matching while remaining comparatively accurate with lower costs than employing LLM itself as an evaluator.Feedback with hints Instead of offering ground truth directly (or statements semantically the same), it is expected to conditionally provide either additional knowledge or a new direction for better retrieval based on previous scratchpads and given ground truth.It not only allows the model to learn the association among multiple relevant pieces of knowledge in a single problem but also teaches the model the way of thinking to continually promote its intricate reasoning capability from historical trials.Feedback with direct answers It retrievals can provide clear and explicit correct responses.It is more efficient to expedite the learning process and eliminate ambiguity from being caught in a dilemma after several rounds of recursive thinking and actions.However, it cannot necessarily foster the problem-solving capabilities of the model as it predominantly relies on human supervision, seeming more like a "spoon-fed" approach; refer to §3.2.1 for empirical results.</p>
<p>Experiments</p>
<p>Setup</p>
<p>Datasets &amp; Preprocessing We evaluate the performance of RAM with two QA datasets: FreshQA (Vu et al., 2023) and MQuAKE-T (Zhong et al., 2023), both of which are newly constructed and mostly contain the latest knowledge in 2023 to avoid data leakage (Liu et al., 2023;Zhou et al., 2023).To be consistent with our memory updating setting, we carefully select 462 QA pairs (118/100/187/175 for false-premise/fastchanging/slow-changing/never-changing world knowledge, respectively) from FreshQA whose knowledge comes only from Wikipedia articles; for MQuAKE-T, we extract all the 96 1-hop questions, based on which 386 multi-hop questions are further sampled with the same distribution as the original dataset to compose the subset (a total of 482 QAs).It is worth noting that in the continual knowledge learning setting, the training set is identical to the testing set, i.e., whether the knowledge has been learned or memorized by the model.Models We use the chat version of LLaMA-2-7B and LLaMA-2-13B (Touvron et al., 2023), which are commonly used in open-sourced LLM evaluation.We also involve Vicuna (Chiang et al., 2023) which is instruction-tuned on LLaMA for comparison.For the commercial model, we utilize GPT-3.5turbo (OpenAI, 2023) from OpenAI with its default parameters, which is deemed to have much larger parameters and a stronger reasoning capability.Evaluation metrics For each dataset, we follow Baktash and Dawodi (2023) to use GPT4_score and the semantic similarity based BERTScore (Zhang et al., 2020;Zhu et al., 2021) as the major evaluation metric widely used for open-domain question answering.By setting GPT4's temperature to 0 and top_p to 1, we aim for more deterministic predictions.We randomly selected 400 questions (200 from each dataset) and evaluated the accuracy from both GPT4's and the human perspective.Tab. 1 validates the agreement between the GPT4 evaluator and human evaluation with a high consistency score.To further assess the relative performance variations under different</p>
<p>Main Results</p>
<p>We measure the model's ability to answer fresh questions under five retrieval-based methods: 1. Self-knowledge: directly answering the questions with pre-trained self-knowledge;</p>
<ol>
<li>
<p>RAG-only: answering the questions based on retrieval from M old ;</p>
</li>
<li>
<p>RAM-R 3 : answering the questions only using R 3 based on M old ; 4. RAM: answering each question using RAM process to obtain M cur .We use "Feedback with hints" to provide simulated human feedback.We fix the order of questions to produce consistent memory update results.</p>
</li>
<li>
<p>RAG-upd (RAG with updated memory): using the direct answer as feedback for all learning traces and providing RAG-only results based on  M upd with the latest knowledge.The average similarity between m R with the ground truth of each question is 0.95 (FreshQA) and 0.91 (MQuAKE) indicating that the memory contains all knowledge of corresponding questions learned from RAM. Tab. 2 illustrates the main performance of RAM.As seen, RAM demonstrates outstanding performance from GPT4_score, exceeding around 30% compared to self-knowledge and RAG-only with limited knowledge.With the help of feedback with hints and partially updated memory, RAM largely improves RAM-R 3 (up to 20%) evaluated by GPT4_score while RAM falls behind in BERTScore.It is probably due to the computed sentence similarity being higher in sentences with more common words with the ground truth although it may be a wrong answer.</p>
</li>
</ol>
<p>Notably, the performance of RAM is even better than RAG-upd on MQuAKE.We hypothesize that the dataset requires multi-retrieval knowledge from different documents which is not realized under the default retrieval setting.Tab. 3 and Fig. 2 provide more in-depth analysis on settings.We summarize our main observations as follows.RAM benefits largely when answering false premise questions.There are plenty of false premise questions (which include questions whose premises are factually incorrect and thus have to be rebutted) in FreshQA and it has been shown that Table 5: Effect of the ever-improving memory in RAM.In each subset, we select one question for memory update and test the rest questions with M upd using RAG-only (RAG-rel) and RAM (RAM-rel).The results using M old are in the parentheses as baselines.</p>
<p>current LLMs struggle with these questions that are not pre-trained on (Vu et al., 2023).In Tab. 3, RAM significantly contributes to the false premise questions (over 40% accuracy improvements) and shows impressive performance above RAG-upd, even when the ground truth is in the memory.Interleaving R 3 helps to dramatically diminish the presence of unreasonable and hallucinated answers to questions proposed without valid premises.It leverages deeper step-by-step thinking rather than answering directly without sufficient evidence to obtain a reliable answer.RAM boosts the learning of slow/neverchanging knowledge.Tab. 3 reveals that RAM especially promotes the learning of slow-changing and never-changing questions to a large extent.It is mainly because the recent knowledge in these two categories probably has close associations with existing memory and can be further deduced through multi-hop reasoning or computations.Additionally, questions before and after 2022 can benefit from RAM even with outdated knowledge.We find that the model still suffers on questions involving fastchanging information beyond their knowledge cutoff date.The feedback with hints from GPT4 (with knowledge before April 2023) has minimal gains on these fast-changing questions, which indicates the outcomes of communicative learning are also limited by the scope of knowledge and skills of teaching.RAM is helpful for multi-hop questions with deeper depths.Fig. 2 shows that the model with RAM produces notable gains on multi-hop questions, especially for MQuAKE.RAM offers larger improvements on 3-hop questions than others, leveraging recursive reasoning-based retrieval on more complex QA tasks.The performance of RAM in multi-hop questions for MQuAKE dramatically improves from 7% to 50% on 2-hop and 6% to 53% on 3-hop, which substantially exceeds the accuracy of RAG-upd.</p>
<p>Ablation Studies</p>
<p>Recursive reasoning based retrieval (R 3 ) As depicted in Fig. 3, for trials ended with a different number of steps in RAM, the average text similarity gradually gets higher as the step increases.R 3 advances the learning process in the right direction to acquire a probable correct answer.However, there are intermediate steps with a similarity decrease, which indicates that the pieces of knowledge retrieved as context are not always similar in texts with common phrases/keywords.It's far from enough for QA using RAG only, particularly for questions that need complicated reasoning.The effect of using the most commonly used retrieval methods (Zhao et al., 2024)   combined with BM25 achieve high by GPT4_score while the ensemble with rerank performs the best by BERTScore.It is desirable that different retrieval methods are carefully designed in various application scenarios for better use.</p>
<p>Continual knowledge in improving memory</p>
<p>In order to validate the effectiveness of M upd , we manually check and group relevant questions into several question subsets.In FreshQA, relevant questions refer to the same topic/event/entities while in MQuAKE, those multi-hop questions using the same single-hop facts are grouped w.r.t.assessment in Tab. 5.In FreshQA, each question in the group can be the selected question while in MQuAKE, the selected question should be a single-hop question only.To our expectation, RAG-rel and RAM-rel increase their performance respectively by leveraging the additional knowledge from M upd in historical trials.This suggests that maintaining a growing memory is helpful to continuously learn from experience across different questions/tasks in the long run.Besides, the performance of RAM-rel in MQuAKE surprisingly raises a lot due to the explicit relevance among constructed multi-hop questions with sharing 1-hop facts than questions from FreshQA.The performance delta in each setting is computed as a comparison with its corresponding baseline.</p>
<p>Feedback We first run RAM to get M upd under different feedback strategies.Later we evaluate RAG-upd on the same question set with the M upd and the results are in Tab. 6.The average text similarity between both settings is extremely low showing their discrepancy.RAM with hints yields a +8% and +20% higher accuracy than the other in each dataset respectively.Relevant knowledge in In RAM, instead of answering directly, users provide hints to help LLM develop their own way of thinking.</p>
<p>Conclusion</p>
<p>In this work, we propose RAM, an effective system to learn fresh knowledge using RAG and communicative learning from human feedback.It is a training-free and RAG-based system that builds continually updated memory with fresh knowledge and historical experience via communicative learning.The compelling results show that RAM applies to learning fresh knowledge in the ever-changing world, which delivers a more natural and effective learning paradigm for AI agents.</p>
<p>Limitations</p>
<p>In this study, we utilize a vector embedding database to store relevant documents as external memory.We encourage future work to extend the memory component of RAM with more advanced structures such as other structured knowledge graphs or traditional relational databases.Considering the limited context window of LLMs, future work may study how the RAM performs with varying learning capacities from RAG.Despite our thorough experiments with different backbones, it may not be precisely reflected due to several factors, including the limitation of the model size and diversity of models, a lack of adequate hardware resources, and the potential for more efficient prompts and techniques to further stimulate the model's abilities.Limited by the human/model expertise and biases, potentially leading to inconsistent results.</p>
<p>Ethics Statement</p>
<p>Various reasoning strategies such as Chain-of-Thought (CoT), ReAct and others can be applied in this pipeline.Meanwhile, plenty of retrieval methods are supported and can be customized for RAM under specific memory configurations.These diverse methods allow us to provide valuable insights into their performance and effectiveness within this system.</p>
<p>There is always limited memory capacity for learning the ever-changing knowledge in the real world.Similar to humans, the continually learned memory can be reorganized and induced to highlevel rulesskills for later use instead of memorizing all the naive facts directly learned from scratch.Building an abstractive memory continually induced from existing facts helps better and quicker retrieval as well as fewer storage resources needed for memory.</p>
<p>In the future, we will further explore the way of Teach and its impact on learning.Apart from how the feedback is generated (hints or direct answers) in Section 3, mechanisms such as replay, exercise, and induction can be involved and delicately designed with well-organized tasks and knowledge.RAM has demonstrated its interaction with both GPT4 and real users in our experiments.We believe that the lack of GPT4 knowledge and user experience also limits the contribution of human feedback in RAM, which could be further evaluated.</p>
<p>Another key challenge for feedback is to deter-mine the stages in the task-solving process where human intervention is most beneficial and effective, aligning to minimize human involvement while maximizing task performance.The model is expected to learn to ask for feedback proactively when it meets self-knowledge deficiency or stucks in the loop.It takes less manual cost to provide as little feedback as possible to avoid redundancy since the essential feedback is customized and provided on needs.</p>
<p>We are also looking forward to the performance and generality of RAM not only in knowledgeintensive QA tasks but also in other tasks like planning, code generation, etc.Although we believe RAM is easy to instantiate on different tasks, we acknowledge and leave the evaluation on other tasks as future work.</p>
<p>A Implementation Details</p>
<p>A.1 Implementation Details for RAM Recursive reasoning-based retrieval In this paper, we employ ReAct (Yao et al., 2023) as the reasoning strategy to choose steps of thought, action, and observation.We replace the lookup/search action in the original ReAct using RAG from the existing memory with limited knowledge.The max number of trails and steps is set to 4 and 6, respectively.Prompts used in this process can be found in Appendix F.</p>
<p>Since the facts in both datasets are mostly after 2023, we affiliate the model with old knowledge for RAG to test its ability of continual learning.We manually crawled outdated Wikipedia articles (before April 2021) from the official website3 as the source to corresponding QAs.Each article is encoded as an embedding in the vector database ChromaDB4 (supported in LangChain (Pandya and Holia, 2023)) using the sentence transformers model all-MiniLM-L6-v25 with default parameters.The default chain type for retrieval is "stuff", which retrieves the top-1 relevant document as context based on L2 similarity in each call.The model generates the observation through in-context learning based on the context.The document will be truncated if its length is larger than the limited context window of the model.Feedback For feedback without explanation, the text similarity used in the paper is computed by cosine similarity using sentence transformers bert-base-nli-mean-tokens6 to encode the inference result and the ground truth into text embeddings Emb P red , Emb G .Per a predefined threshold of 0.9, the feedback can be generated based on the cosine similarity between both embeddings: F eedback = The answer is correct if sim &gt; 0.9; otherwise wrong, where:
sim = Emb P red • Emb G Emb P red × Emb G(1)
For feedback with hints, we exploit GPT4 as the teacher using its strong in-context learning capability to provide feedback situated on the known ground truth G and previous scratchpads S: F eedback = P rompting(G, S) Updated memory After RAM ends with a correct prediction or reaches max number of trials, we utilize semantic similarity to localize the most relevant knowledge and locally edit it.There are two steps to update the memory: 1) We first collect the inference result and feedback in all the trials as context.Based on the ground truth, we prompt the model to generate a reflected memory; 2) Then we compute BM25 (Askari et al., 2023) similarity between the reflected memory and each sentence for each document.The most relevant sentence among those whose similarity scores are above the predefined threshold is extracted and replaced.</p>
<p>A.2 Experiment Settings</p>
<p>In evaluation, the implementation parameters of models and metrics are in default settings.Due to the inference-only nature of RAM, all the implementations for LLaMA-2-7B and 13B can be run on a single Nvidia A100 80GB GPU, 32GB memory, 128 Core AMD CPU.The average time cost(second) for LLaMA-2-7B for each step and each trial is 8s and 30s respectively per question.</p>
<p>For each dataset, the increased number of questions predicted from wrong to right is F T .Reversely, the increased number of questions predicted from right to wrong is T F .T otal is the total number of questions in the dataset.The True Positive Rate (TPR) and False Negative Rate (FNR) can be computed using the formula below:
T P R = F T T otal , F N R = T F T otal (2)</p>
<p>B Further analysis and results</p>
<p>We provide further ablation studies in different configurations in RAM using LLaMA2-7B mainly on FreshQA as below:  Efficiency on Different Base Models In terms of efficiency in RAM, it is mainly evaluated by the number of steps and trials used to answer the question.Considering the performance in Tab. 12, we find that to get the correct answer, LLaMA-13B needs 0.2 steps per trial fewer than LLaMA-7B with higher efficiency.This is mainly due to the stronger reasoning ability of the 13B model.When compared with GPT3.5, the external feedback to the smaller model LLaMA-13B could compensate for the deficiency in its self-knowledge and lack of reasoning ability achieving a better result with comparative efficiency.</p>
<p>Thresholds for feedback without explanation We evaluate the effect of thresholds defined for feedback without explanation and the result is in the Tab.11.RAM exhibits notably superior performance for both Bert_score and GPT4_score at 0.9, surpassing other selected thresholds for comparison.The result proves the threshold value selected in this paper is reasonable to some extent.Similarity computation methods for memory update We compare different text similarity computation methods used for memory update in Tab. 9.The results indicate that using BM25 employed in RAM shows a slight advancement in both metrics than other traditional similarity computation methods like LDA (Blei et al., 2001) and Bert (Devlin et al., 2018).This demonstrates the design choice in RAM that only the sentence with the highest relevance is used for updating to avoid interfering with other existing knowledge in the memory.</p>
<p>Retrieval chain types We also evaluate and compare the performance in Tab. 10 using different retrieval chain types7 for reference.It can be seen that the default type in RAM performs slightly better than the other types on the selected two datasets by keeping the original content of the retrieved documents.Moreover, the type "Map_rerank" ranks behind by selecting the answer with the highest score to locate the most relevant answer with minimum trials.Number of documents for retrieval in R 3 We undertake the experiment on the number of documents for retrieval in R 3 in Tab. 13.The result indicates that retrieval for top-1 documents employed in RAM shows more advancement in both metrics.This demonstrates that answers to FreshQA mostly lie in the top document with the highest text similarity using one single retrieval.Our implementation in RAM also helps to avoid noise caused by irrelevant context retrieved with lower similarity.</p>
<p>C Failure Cases Analysis from User Interaction and Potential Improvements</p>
<p>We identify the underlying cause of failure cases in RAM enabling further improvement.We randomly chose 25 cases from user simulation and distinct categories of causes are as below: 40% (10/25) bad retrieval: cases where the model either retrieves the incorrect document or hits the right document without finding the exact answer.</p>
<p>Since there is outdated knowledge in the current memory for retrieval, semantic-based RAG is no longer enough to pursue the correct answer.We have evaluated the performance using different retrieval chain types in Tab. 10 and show the advancement of reasoning-based retrieval in RAM in Tab. 2. Meanwhile, there are various works on better retrieval proposed in both research studies and engineering implementations for future enhancement.</p>
<p>32% (8/25) model hallucination: Evidence and facts of predictions are not shown up in the memory and generated by LLM itself from nowhere.</p>
<p>There are a few solutions to alleviate the model hallucination.We are encouraged to have more advanced structured knowledge in memory of RAM such as knowledge graphs or traditional relational databases.While carefully designed prompts for controlled inference could provide guidance to outputs, the continual learning from up-to-date knowledge dynamically from the external user/environment allows for self-verification and correction to reduce hallucination.</p>
<p>12% (3/25) question misunderstanding: The model deviates in comprehending the intention of queries and fails to decompose the complicated multi-hop questions, leading to irrelevant facts retrieved from the context.</p>
<p>One way to reduce the problem could be prompting the LLM to rewrite the question into a few subquestions for iterative retrieval and inference.It can decompose the question into subgoals and better figure out the intention, especially for long-horizon multi-step question reasoning.Besides, the model can also make post-verification after each step of reasoning together with the original question/task.12% (3/25) knowledge deficiency: Since the memory in RAM only contains knowledge before April 2021, the lack of essential context would be one potential cause even with enhanced retrieval methods.</p>
<p>It is predictable in the setting of RAM with outdated knowledge in the current memory for the latest QA.Many studies have shown that it's extremely challenging for LLM in the ever-changing world with pre-trained knowledge.RAM proposes insightful inspiration to continual learning paradigm with dynamic memory and we will move on to apply it to generalized tasks in practical applications in future works.</p>
<p>4% (1/25) bad reasoning: There are still a few instances due to the limited reasoning and mathematical computation capability of LLM leading to degraded performance.</p>
<p>The reasoning capability of LLM is crucial in RAM for in-context learning the retrieval/user-feedback knowledge for further improvement: In general, a larger size base model could provide strong reasoning capability and more self-knowledge in RAM.Besides traditional paradigms like CoT, and ReAct, many iterative interaction systems through prompting can be validated to promote LLM reasoning.</p>
<p>D RAM Interactive Interface</p>
<p>The graphical interactive interfaces for RAM are presented in Fig. 4 and Fig. 5. Users are prompted to determine the accuracy of the current observation.If deemed correct, the current trial concludes; otherwise, the model persists in reasoning and retrieval.In cases where the model generates and detects the same observation, the user is solicited for feedback to aid in addressing the question.This iterative process continues until the observation is validated as correct or the maximum number of steps is reached.Based on the information provided in the article, the best-selling video game franchise of all time is Mario, with over 380 million copies sold worldwide.Ground truth: The best-selling video game franchise of all time is Mario.Reflected memory: The best-selling video game franchise of all time is Mario, with over 380 million copies sold worldwide.</p>
<p>Question: The longest unbeaten streak of all time in the Premier League is how many matches?Ground truth: The longest unbeaten streak of all time in the Premier League is 49 matches.Question: Who is the head of state of the country that The Fireballs originated from?Ground truth: The Fireballs originated from the United States, and the head of state of the United States is Joe Biden.</p>
<p>Recursive reasoning based retrieval</p>
<p>Recursive reasoning based retrieval</p>
<p>Trial 1 Thought 1: I need to search The Fireballs, find the country of origin and the head of state.Action 1: Search [The_Fireballs] Observation 1: I don't know the answer to your question.The Fireballs were an American rock and roll group, so they did not originate from any particular country.Thought 2: Maybe The Fireballs were formed in multiple countries.I need to search more.Action 2: Search [Deep_Purple] Observation 2: The head of state of the country that The Fireballs originated from is Queen Elizabeth II.Thought 3: Queen Elizabeth II is the head of state of the United Kingdom, but The Fireballs did not originate from the United Kingdom.Action 3: Finish [I could not find the country of origin of The Fireballs or the head of state.]</p>
<p>Reflecting: I tried searching for the country of origin of The Fireballs but could not find it.Then I thought maybe they were formed in multiple countries, but I could not find any information on that either.Finally, I realized that The Fireballs were an American rock and roll group and did not originate from any particular country.</p>
<p>Trial 2 Thought 1: I need to search The Fireballs, find their country of origin, then find the head of state of that country.Action 1: Search [The_Fireballs] (Same generated observation detected) Human Feedback: You already said it is an American group, so it is originated form America. Thought 2: Head of state of United States is Joe Biden.Action 2: Finish [Joe Biden is the head of state of the country where The Fireballs originated from.]</p>
<p>The answer is correct!</p>
<p>Memory update</p>
<p>Existing memory: I don't know the answer to your question.The Fireballs were an American rock and roll group, so they did not originate from any particular country.The head of state of the country that The Fireballs originated from is Queen Elizabeth II.You already said it is an American group, so it is originated form America. Ground truth: The Fireballs originated from the United States, and the head of state of the United States is Joe Biden.Reflected memory: The Fireballs originated from the United States, and the head of state of the United States is Joe Biden.</p>
<p>Question: Who is the head of government in the country where Genichiro Tenryu's sport originated?Ground truth: The head of government in the country where Genichiro Tenryu's sport originated, which is Japan, is Fumio Kishida.</p>
<p>Recursive reasoning based retrieval</p>
<p>Trial 1 Thought 1: I need to search Genichiro Tenryu and find the country where his sport originated, then find the head of government of that country.Action 1: Search [Genichiro_Tenryu] Observation 1: I dont know the answer to this question because it doesnt provide any information about Genichiro Tenryu or his sport.It appears to be a completely unrelated question about the head of government in some unspecified country.Please clarify or provide additional context so I can assist you.Thought 2: This question seems nonsensical because Genichiro Tenryu is a fictional character and there is no real person or sport associated with him.Therefore, it is impossible to determine the head of government in the country where his sport originated.Action 2: Search [Yasushi_Akimoto] Observation 2: I don't know the answer to your question because Genichiro Tenryu is not a real person and there is no country where his sport originated.Thought 3: You may want to check the spelling or formatting of your question, as it appears to contain typos or inconsistencies.If you meant something else, please clarify or provide more context so I can assist you better.Action 3: Finish [I cannot answer your question as it is based on false assumptions and lacks clarity.]</p>
<p>Reflecting: I realized that the question was nonsensical because Genichiro Tenryu is a fictional character and there is no real person or sport associated with him.I tried to search for more information about him but couldnt find anything.I also noticed that the question contained typos and inconsistencies, which may have contributed to my confusion.Human Feedback: You have already find who is Genichiro Tenryu, then the next step you should do is to find what his sport is, and find out where it is originated.Thought 2: Since Genichiro Tenryus sport originated in Japan, the head of government in Japan is the one responsible for the sport.Action 2: Search [Fumio_Kishida] Observation 2: The head of government in Japan is Fumio Kishida.</p>
<p>The answer is correct!</p>
<p>Memory update</p>
<p>Existing memory: I don't know the answer to this question because it doesn't provide any information about Genichiro Tenryu or his sport.It appears to be a completely unrelated question about the head of government in some unspecified country.Please clarify or provide additional context so I can assist you.I don't know the answer to your question because Genichiro Tenryu is not a real person and there is no country where his sport originated.You have already found who is Genichiro Tenryu, so the next step you should do is to find what his sport is, and find out where it originated.The head of government in Japan is Fumio Kishida.Ground truth: The head of government in the country where Genichiro Tenryu's sport originated, which is Japan, is Fumio Kishida.Reflected memory: Genichiro Tenryus sport originated in Japan, and the head of government in Japan is Fumio Kishida.</p>
<p>F Prompts</p>
<p>F.1 QA Task Evaluation by GPT4 Instruction: Given one question, there is a ground truth and a predicted answer.Please decide whether they are the same or not in semantics.Please only output True or False.Question: {Question} ground truth = {Reference answer} predicted answer = {Generated output} F.2 Recursive Reasoning-based Retrieval Instruction: Solve a question-answering task with interleaving Thought, Action, and Observation steps.You will be given a previous reasoning trial in which you were given access to an external database and a question to answer.</p>
<p>(1) Thought can reason about the current situation, and Action can be two types:</p>
<p>(2) Search[keywords or phrases], which retrieve the relevant knowledge from the external database as context.</p>
<p>(3) Finish[answer], which returns the answer and finishes the task.You may take as many steps as necessary.</p>
<p>Here are some examples: {examples} Question: {question}{scratchpad}</p>
<p>Figure 2 :
2
Figure 2: Evaluation on multi-hop questions using RAM.</p>
<p>Figure 3 :
3
Figure 3: Evaluation of multi-step trials.Each sub-figure indicates step-wise average text similarity between the Inf and G.</p>
<p>Figure 4 :
4
Figure 4: User feedback without explanation</p>
<p>I need to search Genichiro Tenryu and find the country where his sport originated.Action 1: Search [Genichiro_Tenryu] (Same generated observation detected)</p>
<p>F. 3
3
FeedbackInstruction: There are two roles (Student and Teacher) in the question-answering task below.The Student is unsuccessful in answering the question because it has limited relevant context.You are the Teacher who is an expert in rich world knowledge and can provide additional facts as feedback for the Student.You will be given the reasoning steps of Student in previous trials and the Ground truth as a direct answer.You will be punished if the feedback is semantically similar to Ground truth or contains the same knowledge as Ground truth in different expressions.Here are some examples: {examples} Question: {question} Ground truth: {ground truth} Student: {scratchpad} Teacher feedback: F.4 Memory Update Instruction: Given the latest relevant fact, please generate a reflected memory to update/edit the existing memory based on the ground truth.If the given fact has nothing to do with the existing memory and there is no need to update/edit, then output 'None'.Here are some examples:{examples} Existing memory: {existing memory} Ground truth: {Ground truth} Reflected memory: F.5 Inference Question: {question} Feedback: {feedback} Retrieval memory: {retrieval document} Answer:</p>
<p>Table 1 :
1
Evaluation agreement.
DatasetHuman GPT4 AgreementFreshQA58.059.598.5MQuAKE44.046.596.5DatasetMethodGPT4_scoreSelf-knowledge36.36FreshQARAG-only RAM-R 333.77 45.98RAM60.17RAG-upd63.85Self-knowledge12.66MQuAKERAG-only RAM-R 38.51 27.41RAM48.96RAG-upd36.10</p>
<p>Table 2 :
2
Evaluation</p>
<p>(Touvron et al., 2023)hods on FreshQA and MQuAKE.From GPT4_score, RAM's performance improves with feedback and growing memory over self-knowledge and RAG-only with 30%.It comes close (3%) to or surpasses (12%) RAG-upd with R 3 .settings,weadoptautomaticmetrics True Positive Rate (TPR), False Negative Rate (FNR)(Riehl et al., 2023), refer to Appendix A.2 for computation.Implementation and User Simulation We mainly use LLaMA-2-7B(Touvron et al., 2023)as the backbone for all experiments, refer to Appendix A.2, Tab. 7 and Tab. 12 for more settings.Refer to Sec. 3.3 for detailed results and Appendix D for implementation details.</p>
<p>Table 3 :
3
Evaluation (GPT4_score) on various categories of questions in FreshQA.
MethodfastVaild Premise slow never &lt; 2022 ≥ 2022 &lt; 2022 ≥ 2022 False PremiseMethod EmbeddingBERTScore GPT4_score 82.33 36.84Self-knowledge 14.14 29.84 53.72 RAG-only 7.07 15.32 60.3356.64 58.0417.41 7.9647.31 53.7632.00 28.00BM25 Ensemble79.39 77.7242.05 42.11RAM14.14 46.77 81.8281.1127.3694.6276.00Embedding+Rerank82.7734.21RAG-upd53.54 55.65 80.1775.5255.2265.5960.00Ensemble+Rerank83.0734.21</p>
<p>Table 4 :
4
Performance using different retrieval methods in FreshQA.
DatasetMethodGPT4_scoreTPRFNRFreshQARAG-rel 22.56 (20.12) ∆=+6.70 ∆=+4.20 RAM-rel 62.20 (56.70) ∆=+6.10 ∆=+0.60MQuAKERAG-rel RAM-rel 71.51 (43.60) ∆=+31.40 ∆=+3.50 10.48 (6.48) ∆=+5.20 ∆=+1.00</p>
<p>Table 6 :
6
Performance using feedback with hints and direct answers.The text similarity is computed by cosine similarity between embeddings of the direct answer and hints.
DatasetFeedbackText similarityGPT4 _scoreTPRFNRFreshQADirect answer Hints0.2883.11 ∆=+11.90 ∆=+3.60 91.34MQuAKEDirect answer Hints0.2453.52 ∆=+26.80 ∆=+6.00 74.27</p>
<p>Table 7 :
7
Performance of different models.
ModelBERTScore Self-knowledge RAM Self-knowledge RAM GPT4_scoreLLaMA-7B80.2783.2536.3660.17LLaMA-13B79.9783.4440.5670.09Vicuna-7B79.9580.2435.7159.68GPT3.5-turbo81.8783.2146.5458.87The relative TPR and FPR gains for hints com-pared with the direct answer empirically validatethat feedback with hints effectively teaches andstimulates the model to better use self-knowledgeand inner capability to deal with upcoming newknowledge.Performance on different base models Tab. 7presents that all the tested models gain from RAMto a great degree to show the generalization ofRAM on different models. For open-sourced mod-els, LLaMA2-13B is much more accurate thanLLaMA2-7B with a larger knowledge base andstronger reasoning capability. Meanwhile, Vicuna-7B underperforms the others due to degraded per-formance in in-context few-shot learning based onRAG after instruct-tuning. Compared with GPT3.5-turbo, it can be induced that a smaller modelwith fewer advantages in self-knowledge is able toachieve more advanced performance using RAM.3.3 Results with Real Users
hints (although it is not the exact ground truth to any other question) provides further gains across questions that have implicitly shared knowledge.In Tab. 8, we empirically show the effectiveness of communicative learning with real users.Compared with Tab. 2, it proves that feedback in RAM largely contributes to LLMs' performance in both tables than the other retrieval-based methods.Besides,</p>
<p>Table 8 :
8
Accuracy evaluated by real users with interaction.The Human Acc.indicates the averaged accuracy of QA evaluated by different users.
using human-written references as a memory forRAG to enhance text generation. However, thesemodels rely solely on the semantic similarity be-tween the question and external corpus, making itdifficult to pursue a suitable and precise answer.Continual learning for LLM-as-agent Exist-ing methods to update LLMs with fresh knowl-edge are always costly and temporary. ContinualLearning for LLM-as-agent (van de Ven and To-lias, 2019; De Lange et al., 2022; Buzzega et al.,2020; Wang et al., 2020) is driven to cope with theever-changing world knowledge and continuouslyadapt to evolving tasks and environments in a life-long time. Reflexion (Shinn et al., 2023) employsRAM with feedback from real users increases by 20% than GPT4, indicating the customized feed-back conditionally provided by users contributes more than LLM-as-teacher. Divergent content and methodologies of feedback have a great influence on the effectiveness of learning from communi-cations, which needs to be further explored in fu-ture works. With deeper analysis, 32% users pro-vided feedback by decomposing questions into a sequence of actions and 22% provided relevant facts as hints. Further, 18% users only responded the given answer was wrong without explanation. Some 13% users provide direct answers as feed-back and 5% make further clarification on the query. Above 90% users combine multiple ways of teach-ing during interactions. Although there is no limi-tation on the feedback, most users first choose to provide hints to assist the model thinking rather than telling the ground truth directly for one hit to our expectation. More analysis and examples can be seen in Appendix C and Appendix E.self-reflection in "verbal" reinforcement, guiding agents to learn from errors and achieve in-context continual learning from historical experience. FB-NET (Tandon et al., 2022b) prevents similar mis-takes in script generation using dynamic memory. Critically, there is still a lack of delicately designed data acquisition and maintenance in memory. Learning from human feedback Human feed-back (Christiano et al., 2017; Bai et al., 2022; Casper et al., 2023; Lin et al., 2020) has been applied in various domains for LLM enhance-ment through iterative interactions with humans. RLHF (Ziegler et al., 2019) presents a method for fine-tuning language models using human pref-erences, applied to tasks such as summarization and continuing text generation. Eureka (Ma et al., 2023) generates and refines the reward functions for reinforcement learning, enabling dexterous ma-nipulation and gradient-free learning from human feedback. In situ bidirectional human-robot value alignment (Yuan et al., 2022) uses human feedback to align with the user's values in decision-making.4 Related WorkRetrieval augmented generation (RAG) Re-trieval Augmented Generation (Lewis et al., 2020;Mao et al., 2020; Liu et al., 2021; Parvez et al.,2021; Jiang et al., 2023; Trivedi et al., 2023) lever-ages the external knowledge base and helps LLMswith limited knowledge to solve dynamic prob-lems without the need for repeated fine-tuning.TeachMe (Dalvi et al., 2022) allows real users toprovide correct beliefs and store them in dynamicmemory, which can be retrieved to avoid previousmistakes in the next turn. Jarvis-1 (Wang et al.,2023b) leverage memory with historical action se-quences as experience for RAG for better planning.Lift-Yourself-Up (Cheng et al., 2023) focuses on</p>
<p>Table 9 :
9
Similarity methods.
Method BERTScore GPT4_scoreRetrieval chain type BERTScore GPT4_score Trials / StepsBM2591.1178.95Default81.0948.962.38 / 9.34LDA BERT90.32 90.6065.79 65.79Map_reduce Refine Map_rerank93.93 91.65 91.9841.42 45.89 46.882.44 / 9.58 2.27 / 9.96 2.23 / 8.76</p>
<p>Table 10 :
10
Retrieval chain types on MQuAKE.
Threshold BERTScore GPT4_score0.8091.0665.790.8590.3665.790.9091.1178.950.9591.0565.79</p>
<p>Table 11 :
11
Feedback thresholds.
ModelAvg. no. of step Avg. no. of trialLLaMA-7B4.981.39LLaMA-13B4.721.39Vicuna-7B6.231.40GPT3.5-turbo4.641.22</p>
<p>Table 12 :
12
Efficiency of different models in RAM.
Top k BERTScore GPT4_score191.1178.95290.3760.53390.3063.16</p>
<p>Table 13 :
13
TopN documents in R 3 .</p>
<p>I need to search Premier League, find the longest unbeaten streak, and determine how many matches it is.Action 1: Search [Premier_League_records_and_statistics] Observation 1: According to the provided text, the longest unbeaten streak of all time in the Premier League is 38 matches, achieved by Manchester United F.C. during 1998-99 season.Thought 2: Manchester United F.C. had an unbeaten streak of 38 matches in the Premier League.I need to find out when this happened.The information you found is incorrect, The longest unbeaten streak in the Premier League was not held by Manchester United.Thought 3: Ah, my mistake!Thank you for correcting me.Based on the provided context, the longest unbeaten streak in the Premier League is actually held by Arsenal, with 49 matches without defeat between October 1997 and April 2004.Action 3: Finish [The longest unbeaten streak in the Premier League is 49 matches, held by Arsenal.]According to the provided text, the longest unbeaten streak of all time in the Premier League is 38 matches, achieved by Manchester United F.C. during 1998-99 season.The information you found is incorrect, The longest unbeaten streak in the Premier League was not held by Manchester United.Ground truth: The longest unbeaten streak of all time in the Premier League is 49 matches.Reflected memory: The longest unbeaten streak of all time in the Premier League is 49 matches, achieved by Arsenal.Manchester United did not hold this record.
Trial 1Thought 1: Action 2: Search <a href="Same generated observation detected">Premier_League_records_and_statistics</a>GPT4 Feedback: The answer is correct!Memory updateExisting memory:
https://python.langchain.com/docs/integrations/retrievers
https://python.langchain.com/docs/modules/data_connect ion/retrievers/ensemble
https://en.wikipedia.org/wiki/Main_Page
https://github.com/chroma-core/chroma
https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens
https://www.langchain.com.cn/modules/chains/index_examples/qa_with_sources</p>
<p>Injecting the BM25 score as text improves bert-based re-rankers. Arian Askari, Amin Abolghasemi, Gabriella Pasi, Wessel Kraaij, Suzan Verberne, 10.1007/978-3-031-28244-7_5Advances in Information Retrieval -45th European Conference on Information Retrieval, ECIR 2023. Dublin, IrelandSpringer2023. April 2-6, 202313980Proceedings, Part I</p>
<p>Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dassarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, 10.48550/arXiv.2204.05862arXiv.2204.05862Training a helpful and harmless assistant with reinforcement learning from human feedback. Chris Olah, Ben Mann, Jared Kaplan, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish2022arXiv preprint</p>
<p>Gpt-4: A review on advancements and opportunities in natural language processing. Ahmad Jawid, Mursal Baktash, Dawodi, arXiv:2305.031952023Preprint</p>
<p>Latent dirichlet allocation. Advances in neural information processing systems. David Blei, Andrew Ng, Michael Jordan, 200114</p>
<p>Dark experience for general continual learning: a strong, simple baseline. Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, Simone Calderara, Conference on Neural Information Processing Systems. 2020</p>
<p>Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J Michaud, Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, 10.48550/arXiv.2307.15217arXiv.2307.15217Open problems and fundamental limitations of reinforcement learning from human feedback. Erdem Bıyık, Anca Dragan, David Krueger, Dorsa Sadigh, Dylan Hadfield-Menell, Lauro Langosco, Peter Hase2023arXiv preprint</p>
<p>Lift yourself up: Retrieval-augmented text generation with self memory. Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, Rui Yan, Conference on Neural Information Processing Systems. 2023</p>
<p>Wei - Chiang, Lin, Li, Zhuohan, Lin, Zi, Sheng, Ying, Wu, Zhanghao, Zhang, Hao, Zheng, Lianmin, Zhuang, Siyuan, Zhuang, Yonghao, Joseph E Gonzalez, Stoica, Ion, Eric P Xing, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. 2023</p>
<p>Deep reinforcement learning from human preferences. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, Dario Amodei, Conference on Neural Information Processing Systems. 2017</p>
<p>Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement. Bhavana Dalvi, Oyvind Tafjord, Peter Clark, Annual Conference on Empirical Methods in Natural Language Processing (EMNLP). 2022</p>
<p>A continual learning survey: Defying forgetting in classification tasks. Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, Tinne Tuytelaars, 10.1109/TPAMI.2021.3057446IEEE Transactions on Pattern Analysis and Machine Intelligence. 4472022</p>
<p>Interactive concept-learning and constructive induction by analogy. Luc De, Raedt , Maurice Bruynooghe, Machine Learning. 19928</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>From static to dynamic: A continual learning framework for large language models. Mingzhe Du, Anh Tuan Luu, Bin Ji, See Ng, arXiv:2310.142482023Preprint</p>
<p>Retrievalaugmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang, ; Jiang, Luyu Gao, Frank F Xu, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig, arXiv:2312.10997Annual Conference on Empirical Methods in Natural Language Processing (EMNLP). 2024. 2023PreprintActive retrieval augmented generation</p>
<p>Rethinking explainability as a dialogue: A practitioner's perspective. Himabindu Lakkaraju, Dylan Slack, Yuxin Chen, Chenhao Tan, Sameer Singh, arXiv.2202.018752022arXiv preprint</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, Conference on Neural Information Processing Systems. 2020</p>
<p>Loogle: Can long-context language models understand long contexts?. Jiaqi Li, Mengmeng Wang, Zilong Zheng, Muhan Zhang, arXiv:2311.049392023arXiv preprint</p>
<p>A review on interactive reinforcement learning from human social feedback. Jinying Lin, Zhen Ma, Randy Gomez, 10.1109/ACCESS.2020.3006254IEEE Access. 82020Keisuke Nakamura, Bo He, and Guangliang Li</p>
<p>Retrieval-augmented generation for code summarization via hybrid gnn. Shangqing Liu, Yu Chen, Xiaofei Xie, Jingkai Siow, Yang Liu, International Conference on Learning Representations (ICLR). 2021</p>
<p>Llm360: Towards fully transparent open-source llms. Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, Eric P Xing, arXiv:2312.065502023Preprint</p>
<p>Reasoning on graphs: Faithful and interpretable large language model reasoning. Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, Shirui Pan, International Conference on Learning Representations. 2024</p>
<p>Jason Yecheng, William Ma, Guanzhi Liang, De-An Wang, Osbert Huang, Dinesh Bastani, Yuke Jayaraman, Zhu, 10.48550/arXiv.2310.12931arXiv.2310.12931Linxi Fan, and Anima Anandkumar. 2023. Eureka: Human-level reward design via coding large language models. arXiv preprint</p>
<p>Memory-assisted prompt editing to improve gpt-3 after deployment. Aman Madaan, Niket Tandon, Peter Clark, Yiming Yang, arXiv:2201.060092023Preprint</p>
<p>Generation-augmented retrieval for opendomain question answering. Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, Weizhu Chen, Annual Meeting of the Association for Computational Linguistics (ACL). 2020</p>
<p>arXiv:2303.08774Gpt-4 technical report. 2023OpenAIarXiv preprint</p>
<p>Automating customer service using langchain: Building custom open-source gpt chatbot for organizations. Keivalya Pandya, Mehfuza Holia, arXiv:2310.054212023Preprint</p>
<p>Retrieval augmented code generation and summarization. Md Rizwan Parvez, Uddin Wasi, Saikat Ahmad, Baishakhi Chakraborty, Kai-Wei Ray, Chang, Annual Conference on Empirical Methods in Natural Language Processing (EMNLP). 2021</p>
<p>Hierarchical confusion matrix for classification performance evaluation. Kevin Riehl, Michael Neunteufel, Martin Hemberg, arXiv:2306.094612023Preprint</p>
<p>Open-ended instructable embodied agents with memory-augmented large language models. Gabriel Sarch, Yue Wu, Michael J Tarr, Katerina Fragkiadaki, arXiv:2310.151272023Preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Conference on Neural Information Processing Systems. 2023</p>
<p>Knowledge and teaching: Foundations of the new reform. Lee Shulman, Harvard educational review. 5711987</p>
<p>Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. Niket Tandon, Aman Madaan, Peter Clark, Yiming Yang, arXiv:2112.097372022aPreprint</p>
<p>Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback. Niket Tandon, Aman Madaan, Yiming Yang, Peter Clark, North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). 2022b</p>
<p>Origins of human communication. Michael Tomasello, 2010MIT press</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, 10.48550/arXiv.2302.13971arXiv.2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Interleaving retrieval with chain-of-thought reasoning for knowledgeintensive multi-step questions. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, Ashish Sabharwal, Annual Meeting of the Association for Computational Linguistics (ACL). 2023</p>
<p>M Gido, Andreas S Van De Ven, Tolias, arXiv.1904.07734Three scenarios for continual learning. 2019arXiv preprint</p>
<p>Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, Thang Luong, 10.48550/arXiv.2310.03214arXiv.2310.03214Freshllms: Refreshing large language models with search engine augmentation. 2023arXiv preprint</p>
<p>Learning to prompt for continual learning. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, Tomas Pfister, Conference on Computer Vision and Pattern Recognition (CVPR). 2020</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang, 10.48550/arXiv.2302.01560arXiv.2302.015602023aarXiv preprint</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, Yitao Liang, 10.48550/arXiv.2311.05997arXiv.2311.05997Jarvis-1: Open-world multitask agents with memory-augmented multimodal language models. 2023barXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Corrective retrieval augmented generation. Jia-Chen Shi-Qi Yan, Yun Gu, Zhen-Hua Zhu, Ling, arXiv:2401.158842024Preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, R Karthik, Yuan Narasimhan, Cao, International Conference on Learning Representations (ICLR). 2023OpenReview.net</p>
<p>Luyao Yuan, Xiaofeng Gao, Zilong Zheng, Mark Edmonds, Ying Nian Wu, Federico Rossano, Hongjing Lu, Yixin Zhu, Song-Chun Zhu, situ bidirectional human-robot value alignment. 2022</p>
<p>Communicative learning: A unified learning formalism. Luyao Yuan, Song-Chun Zhu, 2023Engineering</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 2020</p>
<p>Retrievalaugmented generation for ai-generated content: A survey. Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Bin Cui, arXiv:2402.194732024arXiv preprint</p>
<p>Mquake: Assessing knowledge editing in language models via multi-hop questions. Zexuan Zhong, Zhengxuan Wu, Christopher D Manning, Christopher Potts, Danqi Chen, Annual Conference on Empirical Methods in Natural Language Processing (EMNLP). 2023</p>
<p>Don't make your llm an evaluation benchmark cheater. Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, Jiawei Han, arXiv:2311.019642023Preprint</p>
<p>Retrieving and reading: A comprehensive survey on open-domain question answering. Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, Tat-Seng Chua, arXiv:2101.007742021Preprint</p>
<p>Nisan Daniel M Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv.1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
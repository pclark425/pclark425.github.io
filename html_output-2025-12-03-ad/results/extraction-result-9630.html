<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9630 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9630</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9630</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-167.html">extraction-schema-167</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-262084452</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.09683v3.pdf" target="_blank">PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence</a></p>
                <p><strong>Paper Abstract:</strong> Biomedical research yields a wealth of information, much of which is only accessible through the literature. Consequently, literature search is an essential tool for building on prior knowledge in clinical and biomedical research. Although recent improvements in artificial intelligence have expanded functionality beyond keyword-based search, these advances may be unfamiliar to clinicians and researchers. In response, we present a survey of literature search tools tailored to both general and specific information needs in biomedicine, with the objective of helping readers efficiently fulfill their information needs. We first examine the widely used PubMed search engine, discussing recent improvements and continued challenges. We then describe literature search tools catering to five specific information needs: 1. Identifying high-quality clinical research for evidence-based medicine. 2. Retrieving gene-related information for precision medicine and genomics. 3. Searching by meaning, including natural language questions. 4. Locating related articles with literature recommendation. 5. Mining literature to discover associations between concepts such as diseases and genetic variants. Additionally, we cover practical considerations and best practices for choosing and using these tools. Finally, we provide a perspective on the future of literature search engines, considering recent breakthroughs in large language models such as ChatGPT. In summary, our survey provides a comprehensive view of biomedical literature search functionalities with 36 publicly available tools.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9630.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9630.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-augmented LLMs (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Large Language Models for Literature Question Answering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Class of systems that first retrieve relevant scholarly documents with traditional/semantic search and then prompt an LLM (e.g., GPT-3) to synthesize answers or summaries conditioned on the retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 / unspecified LLMs (as used in retrieval-augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer language models prompted to generate answers conditioned on external retrieved documents (retrieval-augmented generation). The paper describes this approach at a conceptual level and cites GPT-3 as an example of the LLM used by such systems.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Documents retrieved at query time from biomedical literature search engines (e.g., PubMed, PubMed Central, CORD-19 or other corpora returned by semantic retrieval); selection is driven by the user question and the retrieval component.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Natural language research or clinical questions phrased by users (including PICO-style clinical questions), where the goal is to synthesize evidence from multiple returned articles.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Two-stage pipeline: (1) semantic or traditional retrieval returns a set of relevant articles/snippets; (2) prompt an LLM (retrieval-augmented generation) with the retrieved text and the user question to produce a synthesized answer or summary. The systems return both the supporting articles and the LLM-generated answer.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Narrative answers / synthesized summaries tied to retrieved documents (often presented alongside citations or evidence snippets).</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Preliminary evaluations described qualitatively in the survey and references to early evaluations in the literature; concerns noted about precision and recall of LLM outputs and the need for verification against source documents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Survey reports that these LLM-generated answers are susceptible to precision and recall errors and therefore must be carefully verified; no rigorous quantitative results are provided in this paper itself.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Makes literature more directly accessible by producing concise answers from multiple documents; can accept natural language questions and reduce user effort to synthesize evidence; integrates retrieval and generation for user convenience.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Susceptible to hallucination and precision/recall errors; outputs require careful verification; survey notes RAG tools are experimental and that LLM answers can be inaccurate or incomplete.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Instances where LLM-generated answers omit relevant retrieved evidence (recall errors), assert incorrect facts not supported by the sources (hallucinations/precision errors), or fail to capture nuanced methodological quality of evidence; concrete failure cases are described qualitatively rather than with examples in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9630.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9630.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Elicit (example RAG tool)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Elicit (experimental literature search tool using LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A literature search/QA system that retrieves relevant articles for a user question and uses an LLM (the paper cites GPT-3 as an example) to generate answers based on the retrieved literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (as cited example)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 is referenced in the paper as an example LLM used by Elicit to synthesize answers from retrieved articles; the survey does not provide architecture or training specifics beyond this citation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Relevant biomedical articles retrieved by Elicit via semantic search across literature sources (the survey notes Elicit retrieves relevant articles and then prompts an LLM). The paper does not specify the exact indexed corpora or counts.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>User-provided natural language research questions (potentially broad literature synthesis or targeted clinical/research questions).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Semantic retrieval followed by prompting an LLM (GPT-3 cited) with retrieved documents to generate an answer; the system returns both retrieved documents and the LLM-generated answer (retrieval-augmented generation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>LLM-generated answers/summaries alongside lists of retrieved supporting articles.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>The survey references preliminary concerns and evaluation studies in the literature generally, but does not present a formal evaluation of Elicit within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not quantified in this survey; the paper warns that such LLM outputs are prone to precision and recall errors and should be verified against source documents.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Provides direct, synthesized answers which can speed literature triage and initial evidence synthesis; combines retrieval and generation to present both sources and summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey highlights general limitations of LLM-based answers (hallucinations, precision/recall issues); Elicit-specific vetting results are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No Elicit-specific failure cases detailed in this survey; general failure modes include generation of unsupported claims, omission of relevant evidence, and potential misrepresentation of study quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9630.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9630.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>scite (example RAG tool)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>scite (experimental literature search system augmented by LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experimental literature search system listed in the survey that retrieves relevant articles for a user's question and integrates LLM-based answering capabilities (survey groups it with other experimental LLM-augmented search systems).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM (survey groups scite with other LLM-augmented systems)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Survey lists scite as an experimental system that accepts natural language questions, retrieves relevant articles, and invokes an LLM to provide answers; the specific LLM and architecture are not detailed in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Biomedical articles retrieved in response to user queries; the survey does not supply corpus size or exact source lists for scite.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Natural language research or clinical questions posed by users.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Semantic retrieval of relevant literature followed by LLM prompting to synthesize answers; outputs include both the retrieved articles and generated answers (retrieval-augmented generation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated answers/summaries with supporting article lists and evidence snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>No scite-specific evaluation presented in the survey; the paper cites general preliminary evaluations and literature noting verification issues.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not reported in this survey for scite specifically; general statement that LLM-generated answers can suffer from precision/recall problems.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Enables users to get concise synthesized responses to literature questions and see supporting evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Survey cautions about hallucinations and inaccurate or incomplete synthesis; scite-specific limitations or failure cases are not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not specified in this paper for scite; general RAG failure modes apply (omitted evidence, hallucinated claims, misattributed findings).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9630.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9630.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for Evidence Synthesis & PICO extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models for Evidence Synthesis (including Boolean-query suggestion and PICO extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Surveyed potential uses of LLMs to aid systematic evidence synthesis, including generating Boolean queries for literature screening and extracting structured PICO elements from articles to accelerate evidence review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / GPT-3 / other LLMs (as referenced generally)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General-purpose large pre-trained transformer LLMs (ChatGPT and GPT-3 are named) that can generate text, suggest search strategies, and extract structured elements from text when prompted; specific training/architecture details are not provided in the survey beyond naming these models.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Literature sets relevant to a systematic review or clinical question (the survey does not specify sizes); LLMs would operate on retrieved articles or on batches of article text to extract PICO elements or summarize findings.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Systematic-review-style clinical questions (PICO structured) or other evidence synthesis topics where users need to screen, extract, and synthesize results across many studies.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompting LLMs to (a) construct Boolean search queries from a natural language synthesis of the question, (b) extract PICO elements from article texts, and (c) summarize/synthesize findings across retrieved articles into narrative summaries. The survey also references external preliminary studies exploring these capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Boolean query suggestions, extracted structured PICO elements, and synthesized narrative summaries of evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Survey cites preliminary external evaluations and arXiv/medRxiv preprints that have tested these capabilities (e.g., generating boolean queries, summarizing medical evidence), but does not provide primary evaluation data itself.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>The survey notes mixed results in preliminary studies (e.g., 'varying success' in GPT-3 based evidence summarization and specific studies testing boolean query generation), concluding that more validation is required before widespread use.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Can speed parts of the evidence-synthesis workflow such as query formulation and extraction of structured elements; may reduce manual labor and accelerate screening and initial synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Variable and sometimes poor reliability reported in preliminary studies; LLM outputs can be incorrect, incomplete, or misleading; cannot yet replace careful manual screening and verification for high-stakes systematic reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported cases in cited preliminary work include incorrectly constructed boolean queries, omission or mis-extraction of PICO components, and generated summaries that misrepresent study findings or overstate evidence (the survey summarizes such concerns qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9630.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9630.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to distill or synthesize theories or knowledge from large numbers of scholarly papers, including details about the models, input corpora, methods, outputs, evaluations, strengths, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs for Literature Mining / Knowledge Graph Interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models for Interpretation of Literature-derived Knowledge Graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Surveyed proposal that LLMs could be used to interpret knowledge graphs assembled from literature mining (entities and relations) to surface higher-level insights and previously unknown associations, while recognizing LLMs are not necessarily superior for core NER/RE tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT / PaLM / other LLMs (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General LLMs (ChatGPT, PaLM are named in the survey) used as interpretation engines that accept structured graph outputs and produce narrative interpretations or hypotheses; the survey cautions that for low-level extraction NER/RE, smaller task-specific models often perform better.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Knowledge graphs constructed from literature mining pipelines (nodes: biomedical entities; edges: extracted relations) built from PubMed/PMC and similar sources; exact corpus sizes are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_query_description</strong></td>
                            <td>Exploratory knowledge-discovery tasks such as finding indirect associations among diseases, genes, variants, or other biomedical concepts across the literature.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Use the outputs of automated literature-mining (NER + RE) to build knowledge graphs, then prompt LLMs to interpret the graph, summarize patterns, and suggest possible missing links or hypotheses. The paper frames this as a potential use rather than a mature, evaluated method.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Narrative interpretations, hypothesized associations, and human-readable explanations of graph structure and prominent connections.</td>
                        </tr>
                        <tr>
                            <td><strong>output_example</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>No formal evaluations presented in the survey for this application; the paper notes a lack of research confirming the utility of knowledge-graph-based assistance and of LLM-based interpretation in literature discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Not established in this paper; authors call for future studies to confirm utility and to assess performance.</td>
                        </tr>
                        <tr>
                            <td><strong>strengths</strong></td>
                            <td>Could provide high-level, human-friendly interpretations of complex, structured literature-derived data and generate novel hypotheses from aggregated relations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>LLMs generally do not outperform fine-tuned smaller models on core NER/RE tasks; reliability of LLM-generated interpretations is unverified and may include hallucinated associations; lack of standardized evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Potential to propose spurious associations not grounded in the extracted evidence, misinterpretation of graph topology, or over-generalization from incomplete or noisy extraction outputs; no concrete examples provided in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success) <em>(Rating: 2)</em></li>
                <li>Can chatgpt write a good boolean query for systematic review literature search? <em>(Rating: 2)</em></li>
                <li>Evaluating Large Language Models on Medical Evidence Summarization <em>(Rating: 2)</em></li>
                <li>Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information <em>(Rating: 2)</em></li>
                <li>How Will ChatGPT Affect Information Seeking from the Medical Literature? <em>(Rating: 2)</em></li>
                <li>Large Language Models Encode Clinical Knowledge <em>(Rating: 1)</em></li>
                <li>Can large language models reason about medical questions? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9630",
    "paper_id": "paper-262084452",
    "extraction_schema_id": "extraction-schema-167",
    "extracted_data": [
        {
            "name_short": "Retrieval-augmented LLMs (RAG)",
            "name_full": "Retrieval-Augmented Large Language Models for Literature Question Answering",
            "brief_description": "Class of systems that first retrieve relevant scholarly documents with traditional/semantic search and then prompt an LLM (e.g., GPT-3) to synthesize answers or summaries conditioned on the retrieved documents.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3 / unspecified LLMs (as used in retrieval-augmentation)",
            "model_description": "Large pre-trained transformer language models prompted to generate answers conditioned on external retrieved documents (retrieval-augmented generation). The paper describes this approach at a conceptual level and cites GPT-3 as an example of the LLM used by such systems.",
            "model_size": null,
            "input_corpus_description": "Documents retrieved at query time from biomedical literature search engines (e.g., PubMed, PubMed Central, CORD-19 or other corpora returned by semantic retrieval); selection is driven by the user question and the retrieval component.",
            "input_corpus_size": null,
            "topic_query_description": "Natural language research or clinical questions phrased by users (including PICO-style clinical questions), where the goal is to synthesize evidence from multiple returned articles.",
            "distillation_method": "Two-stage pipeline: (1) semantic or traditional retrieval returns a set of relevant articles/snippets; (2) prompt an LLM (retrieval-augmented generation) with the retrieved text and the user question to produce a synthesized answer or summary. The systems return both the supporting articles and the LLM-generated answer.",
            "output_type": "Narrative answers / synthesized summaries tied to retrieved documents (often presented alongside citations or evidence snippets).",
            "output_example": "",
            "evaluation_method": "Preliminary evaluations described qualitatively in the survey and references to early evaluations in the literature; concerns noted about precision and recall of LLM outputs and the need for verification against source documents.",
            "evaluation_results": "Survey reports that these LLM-generated answers are susceptible to precision and recall errors and therefore must be carefully verified; no rigorous quantitative results are provided in this paper itself.",
            "strengths": "Makes literature more directly accessible by producing concise answers from multiple documents; can accept natural language questions and reduce user effort to synthesize evidence; integrates retrieval and generation for user convenience.",
            "limitations": "Susceptible to hallucination and precision/recall errors; outputs require careful verification; survey notes RAG tools are experimental and that LLM answers can be inaccurate or incomplete.",
            "failure_cases": "Instances where LLM-generated answers omit relevant retrieved evidence (recall errors), assert incorrect facts not supported by the sources (hallucinations/precision errors), or fail to capture nuanced methodological quality of evidence; concrete failure cases are described qualitatively rather than with examples in this paper.",
            "uuid": "e9630.0",
            "source_info": {
                "paper_title": "PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Elicit (example RAG tool)",
            "name_full": "Elicit (experimental literature search tool using LLMs)",
            "brief_description": "A literature search/QA system that retrieves relevant articles for a user question and uses an LLM (the paper cites GPT-3 as an example) to generate answers based on the retrieved literature.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3 (as cited example)",
            "model_description": "GPT-3 is referenced in the paper as an example LLM used by Elicit to synthesize answers from retrieved articles; the survey does not provide architecture or training specifics beyond this citation.",
            "model_size": null,
            "input_corpus_description": "Relevant biomedical articles retrieved by Elicit via semantic search across literature sources (the survey notes Elicit retrieves relevant articles and then prompts an LLM). The paper does not specify the exact indexed corpora or counts.",
            "input_corpus_size": null,
            "topic_query_description": "User-provided natural language research questions (potentially broad literature synthesis or targeted clinical/research questions).",
            "distillation_method": "Semantic retrieval followed by prompting an LLM (GPT-3 cited) with retrieved documents to generate an answer; the system returns both retrieved documents and the LLM-generated answer (retrieval-augmented generation).",
            "output_type": "LLM-generated answers/summaries alongside lists of retrieved supporting articles.",
            "output_example": "",
            "evaluation_method": "The survey references preliminary concerns and evaluation studies in the literature generally, but does not present a formal evaluation of Elicit within this paper.",
            "evaluation_results": "Not quantified in this survey; the paper warns that such LLM outputs are prone to precision and recall errors and should be verified against source documents.",
            "strengths": "Provides direct, synthesized answers which can speed literature triage and initial evidence synthesis; combines retrieval and generation to present both sources and summaries.",
            "limitations": "Survey highlights general limitations of LLM-based answers (hallucinations, precision/recall issues); Elicit-specific vetting results are not provided in the paper.",
            "failure_cases": "No Elicit-specific failure cases detailed in this survey; general failure modes include generation of unsupported claims, omission of relevant evidence, and potential misrepresentation of study quality.",
            "uuid": "e9630.1",
            "source_info": {
                "paper_title": "PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "scite (example RAG tool)",
            "name_full": "scite (experimental literature search system augmented by LLMs)",
            "brief_description": "An experimental literature search system listed in the survey that retrieves relevant articles for a user's question and integrates LLM-based answering capabilities (survey groups it with other experimental LLM-augmented search systems).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLM (survey groups scite with other LLM-augmented systems)",
            "model_description": "Survey lists scite as an experimental system that accepts natural language questions, retrieves relevant articles, and invokes an LLM to provide answers; the specific LLM and architecture are not detailed in the paper.",
            "model_size": null,
            "input_corpus_description": "Biomedical articles retrieved in response to user queries; the survey does not supply corpus size or exact source lists for scite.",
            "input_corpus_size": null,
            "topic_query_description": "Natural language research or clinical questions posed by users.",
            "distillation_method": "Semantic retrieval of relevant literature followed by LLM prompting to synthesize answers; outputs include both the retrieved articles and generated answers (retrieval-augmented generation).",
            "output_type": "Generated answers/summaries with supporting article lists and evidence snippets.",
            "output_example": "",
            "evaluation_method": "No scite-specific evaluation presented in the survey; the paper cites general preliminary evaluations and literature noting verification issues.",
            "evaluation_results": "Not reported in this survey for scite specifically; general statement that LLM-generated answers can suffer from precision/recall problems.",
            "strengths": "Enables users to get concise synthesized responses to literature questions and see supporting evidence.",
            "limitations": "Survey cautions about hallucinations and inaccurate or incomplete synthesis; scite-specific limitations or failure cases are not detailed here.",
            "failure_cases": "Not specified in this paper for scite; general RAG failure modes apply (omitted evidence, hallucinated claims, misattributed findings).",
            "uuid": "e9630.2",
            "source_info": {
                "paper_title": "PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "LLMs for Evidence Synthesis & PICO extraction",
            "name_full": "Large Language Models for Evidence Synthesis (including Boolean-query suggestion and PICO extraction)",
            "brief_description": "Surveyed potential uses of LLMs to aid systematic evidence synthesis, including generating Boolean queries for literature screening and extracting structured PICO elements from articles to accelerate evidence review.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / GPT-3 / other LLMs (as referenced generally)",
            "model_description": "General-purpose large pre-trained transformer LLMs (ChatGPT and GPT-3 are named) that can generate text, suggest search strategies, and extract structured elements from text when prompted; specific training/architecture details are not provided in the survey beyond naming these models.",
            "model_size": null,
            "input_corpus_description": "Literature sets relevant to a systematic review or clinical question (the survey does not specify sizes); LLMs would operate on retrieved articles or on batches of article text to extract PICO elements or summarize findings.",
            "input_corpus_size": null,
            "topic_query_description": "Systematic-review-style clinical questions (PICO structured) or other evidence synthesis topics where users need to screen, extract, and synthesize results across many studies.",
            "distillation_method": "Prompting LLMs to (a) construct Boolean search queries from a natural language synthesis of the question, (b) extract PICO elements from article texts, and (c) summarize/synthesize findings across retrieved articles into narrative summaries. The survey also references external preliminary studies exploring these capabilities.",
            "output_type": "Boolean query suggestions, extracted structured PICO elements, and synthesized narrative summaries of evidence.",
            "output_example": "",
            "evaluation_method": "Survey cites preliminary external evaluations and arXiv/medRxiv preprints that have tested these capabilities (e.g., generating boolean queries, summarizing medical evidence), but does not provide primary evaluation data itself.",
            "evaluation_results": "The survey notes mixed results in preliminary studies (e.g., 'varying success' in GPT-3 based evidence summarization and specific studies testing boolean query generation), concluding that more validation is required before widespread use.",
            "strengths": "Can speed parts of the evidence-synthesis workflow such as query formulation and extraction of structured elements; may reduce manual labor and accelerate screening and initial synthesis.",
            "limitations": "Variable and sometimes poor reliability reported in preliminary studies; LLM outputs can be incorrect, incomplete, or misleading; cannot yet replace careful manual screening and verification for high-stakes systematic reviews.",
            "failure_cases": "Reported cases in cited preliminary work include incorrectly constructed boolean queries, omission or mis-extraction of PICO components, and generated summaries that misrepresent study findings or overstate evidence (the survey summarizes such concerns qualitatively).",
            "uuid": "e9630.3",
            "source_info": {
                "paper_title": "PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "LLMs for Literature Mining / Knowledge Graph Interpretation",
            "name_full": "Large Language Models for Interpretation of Literature-derived Knowledge Graphs",
            "brief_description": "Surveyed proposal that LLMs could be used to interpret knowledge graphs assembled from literature mining (entities and relations) to surface higher-level insights and previously unknown associations, while recognizing LLMs are not necessarily superior for core NER/RE tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "ChatGPT / PaLM / other LLMs (unspecified)",
            "model_description": "General LLMs (ChatGPT, PaLM are named in the survey) used as interpretation engines that accept structured graph outputs and produce narrative interpretations or hypotheses; the survey cautions that for low-level extraction NER/RE, smaller task-specific models often perform better.",
            "model_size": null,
            "input_corpus_description": "Knowledge graphs constructed from literature mining pipelines (nodes: biomedical entities; edges: extracted relations) built from PubMed/PMC and similar sources; exact corpus sizes are not provided.",
            "input_corpus_size": null,
            "topic_query_description": "Exploratory knowledge-discovery tasks such as finding indirect associations among diseases, genes, variants, or other biomedical concepts across the literature.",
            "distillation_method": "Use the outputs of automated literature-mining (NER + RE) to build knowledge graphs, then prompt LLMs to interpret the graph, summarize patterns, and suggest possible missing links or hypotheses. The paper frames this as a potential use rather than a mature, evaluated method.",
            "output_type": "Narrative interpretations, hypothesized associations, and human-readable explanations of graph structure and prominent connections.",
            "output_example": "",
            "evaluation_method": "No formal evaluations presented in the survey for this application; the paper notes a lack of research confirming the utility of knowledge-graph-based assistance and of LLM-based interpretation in literature discovery.",
            "evaluation_results": "Not established in this paper; authors call for future studies to confirm utility and to assess performance.",
            "strengths": "Could provide high-level, human-friendly interpretations of complex, structured literature-derived data and generate novel hypotheses from aggregated relations.",
            "limitations": "LLMs generally do not outperform fine-tuned smaller models on core NER/RE tasks; reliability of LLM-generated interpretations is unverified and may include hallucinated associations; lack of standardized evaluation.",
            "failure_cases": "Potential to propose spurious associations not grounded in the extracted evidence, misinterpretation of graph topology, or over-generalization from incomplete or noisy extraction outputs; no concrete examples provided in the survey.",
            "uuid": "e9630.4",
            "source_info": {
                "paper_title": "PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success)",
            "rating": 2,
            "sanitized_title": "summarizing_simplifying_and_synthesizing_medical_evidence_using_gpt3_with_varying_success"
        },
        {
            "paper_title": "Can chatgpt write a good boolean query for systematic review literature search?",
            "rating": 2,
            "sanitized_title": "can_chatgpt_write_a_good_boolean_query_for_systematic_review_literature_search"
        },
        {
            "paper_title": "Evaluating Large Language Models on Medical Evidence Summarization",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_on_medical_evidence_summarization"
        },
        {
            "paper_title": "Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
            "rating": 2,
            "sanitized_title": "augmenting_large_language_models_with_domain_tools_for_improved_access_to_biomedical_information"
        },
        {
            "paper_title": "How Will ChatGPT Affect Information Seeking from the Medical Literature?",
            "rating": 2,
            "sanitized_title": "how_will_chatgpt_affect_information_seeking_from_the_medical_literature"
        },
        {
            "paper_title": "Large Language Models Encode Clinical Knowledge",
            "rating": 1,
            "sanitized_title": "large_language_models_encode_clinical_knowledge"
        },
        {
            "paper_title": "Can large language models reason about medical questions?",
            "rating": 1,
            "sanitized_title": "can_large_language_models_reason_about_medical_questions"
        }
    ],
    "cost": 0.014456249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence</p>
<p>Qiao Jin qiao.jin@nih.gov 
National Center for Biotechnology Information
National Library of Medicine National Institutes of Health</p>
<p>Robert Leaman robert.leaman@nih.gov 
National Center for Biotechnology Information
National Library of Medicine National Institutes of Health</p>
<p>Zhiyong Lu zhiyong.lu@nih.gov 
National Center for Biotechnology Information
National Library of Medicine National Institutes of Health</p>
<p>PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence
68276D73A2198D767898228B65D74815
Biomedical research yields a wealth of information, much of which is only accessible through the literature.Consequently, literature search is an essential tool for building on prior knowledge in clinical and biomedical research.Although recent improvements in artificial intelligence have expanded functionality beyond keyword-based search, these advances may be unfamiliar to clinicians and researchers.In response, we present a survey of literature search tools tailored to both general and specific information needs in biomedicine, with the objective of helping readers efficiently fulfill their information needs.We first examine the widely used PubMed search engine, discussing recent improvements and continued challenges.We then describe literature search tools catering to five specific information needs: 1. Identifying high-quality clinical research for evidence-based medicine.2. Retrieving gene-related information for precision medicine and genomics.3. Searching by meaning, including natural language questions.4. Locating related articles with literature recommendation.5. Mining literature to discover associations between concepts such as diseases and genetic variants.Additionally, we cover practical considerations and best practices for choosing and using these tools.Finally, we provide a perspective on the future of literature search engines, considering recent breakthroughs in large language models such as ChatGPT.In summary, our survey provides a comprehensive view of biomedical literature search functionalities with 36 publicly available tools.This article differs from previous surveys on biomedical literature search tools 11-14 in three important aspects: (1) We organize the literature search tools into a novel, scenario-</p>
<p>Introduction</p>
<p>In biomedicine, literature serves as the primary means of disseminating new findings and knowledge.Much of the information accumulated by biomedical research remains accessible only through the literature 1 .Consequently, literature search, the process of retrieving scientific articles to satisfy specific information needs, is integral to all aspects of biomedical research and patient care.For example, to practice evidence-based medicine, clinicians must locate relevant literature that depicts similar scenarios 2 .Similarly, for knowledge discovery, biomedical researchers rely on insights from prior publications as new knowledge often builds on prior knowledge 3 .</p>
<p>The exponential growth of biomedical literature makes identifying the information relevant to a given information need challenging.PubMed, the most widely used biomedical literature search engine, currently contains nearly 36 million articles, with the addition of more than 1 million annually.A typical PubMed query retrieves hundreds to thousands of articles, yet fewer than 20% of the articles past the top 20 results are ever reviewed 4,5 .This motivated a shift in PubMed's approach from recency-based ranking to a relevancebased ranking 6 , to better prioritize the most relevant and significant articles.</p>
<p>However, PubMed primarily serves as a general-purpose biomedical literature search engine.Despite significant improvements over the past decades 5 , PubMed mainly processes short keyword-based queries, returning a list of raw articles without further analysis.Consequently, it might not optimally serve specialized information needs, which require alternative query types or have specific requirements for ranking articles or displaying results.A notable example is the unprecedented upsurge of publications addressing the COVID-19 pandemic 7,8 .While the pandemic made quickly disseminating new findings critical, obtaining comprehensive results from traditional search engines requires complex querying syntax that is unfamiliar to most users and manual topic indexing requires several months post-publication.Addressing the COVID-19 pandemic, therefore, required a specialized literature search engine capable of automatically collecting and classifying relevant articles 9,10 .While various web-based literature search tools have been proposed over the past two decades to complement PubMed for specific literature search needs, they remain underutilized and unfamiliar to clinicians and researchers.This survey aims to acquaint readers with available tools, discuss best practices, identify functionality gaps for different search scenarios, and ultimately facilitate biomedical literature retrieval.Literature search tools included in this study meet the following criteria: they must be web-based, freely available, regularly maintained, and designed for searching the biomedical literature.</p>
<p>Consequently, general-domain literature search engines such as Web of Science, Scopus, Google Scholar, and Semantic Scholar, are not included.</p>
<p>Table 1 enumerates the web-based literature search tools introduced in this survey, categorized by the unique information needs they fulfill.Specifically, literature search tools are classified into five areas: (1) Evidence-based medicine (EBM), for identifying high-quality clinical evidence; (2) Precision medicine (PM) and genomics, for retrieving information related to genes or variants; (3) Semantic search, for finding textual units semantically related to the input query; (4) Literature recommendation, for suggesting related articles; and (5) Literature mining, for extracting biomedical concepts and their relations for literature-based discovery.Figure 1 presents a high-level overview of the search scenarios.Search tools catering to different information needs differ in the types of queries they accept, their methods for processing articles and matching them to the input query, and how they present search results to users.oriented, classification system, providing users with a straightforward and instructive guide for selecting the most suitable tool for their information needs; (2) Our study includes many newly-introduced systems not covered by previous surveys; (3) Beyond surveying current systems, we also cover practical considerations and best practices of choosing and using these tools for addressing biomedical information needs.Finally, we share our perspective on the development of next-generation biomedical literature search engines.Specifically, we discuss how large language models (LLM), such as ChatGPT, could be integrated with traditional literature retrieval to iteratively refine information needs into queries, retrieve relevant articles, then summarize the information retrieved or provide answers directly.Our goal is to provide a comprehensive overview of specialized literature search tools for clinicians and researchers to consider for different use cases, which enables more effective exploration of biomedical information and higher-quality care for their patients.PubMed, a widely used search engine for biomedical literature, is developed and maintained by the US National Library of Medicine (NLM).In 2021, it averaged approximately 2.5 million queries daily.When a query is entered, PubMed expands it to include additional Medical Subject Headings (MeSH) terms1 via automatic term mapping.The search engine then seeks exact matches for this expanded query in the indexed fields of each article, including the title, abstract, author list, keywords, and MeSH terms.Traditionally, all matching articles were returned in reverse chronological order.A new machine-learning-based retrieval model -Best Match -was introduced in 2017 to better assist users by returning the most relevant articles among the top results 6 .Best Match considers multiple ranking signals such as an article's type, age, and usage, and is trained with past user search logs.As of 2020, Best Match has become the default sorting option in the new PubMed 15 .Beyond relevance search for biomedical topics, PubMed also supports various other search functionalities.These include matching single citations2 through bibliographic information such as title and journal names, as well as Boolean operators that are usually used when conducting systematic reviews.</p>
<p>However, since PubMed does not index full-text articles, those that match the query in the full-text but not in the abstract or the title will not be retrieved.Such queries are accommodated by PubMed Central (PMC), which provides access to more than 7 million freely available full-text articles.Unfortunately, PMC does not support searching the other 27 million PubMed articles that lack full-text availability.Europe PMC 16 , a PMC partner, contains both 42.7 million abstracts and 9.0 million full-text articles as of July 2023.</p>
<p>While PubMed and PMC might be an ideal starting point for keyword queries, their utility beyond keyword-based searches is limited.For example, Shariff et al. demonstrate that the results for unfiltered PubMed queries are much less efficient and comprehensive than filtered queries when retrieving clinical evidence 17 .Allot et al. report that searches for genomic variants such as "rs121913527" often return zero results in PubMed 18 despite synonymous variants being mentioned in many articles.Additionally, Fiorini et al. find that queries exceeding five words tend to retrieve less satisfactory results in PubMed 5 .These findings suggest a need for specialized search engines to meet specific information needs.</p>
<p>Best Practice: PubMed should be the first choice for three types of literature search practices: (1) exploring biomedical topics via keyword query, with PMC enabling keyword search within the full text, when available; (2) searching for a single citation, i.e., a specific article; (3) reproducible literature screening with Boolean queries.</p>
<p>Evidence-based medicine</p>
<p>Evidence-based medicine (EBM) 19 requests clinical practitioners follow high-quality evidence, primarily derived from peer-reviewed articles of clinical studies.Efficient retrieval of this evidence is crucial for implementing EBM.Accordingly, clinical questions should be structured effectively, incorporating at least the "PICO" elements 20 (Population, Intervention, Comparison, and Outcome).For example, in "Does remdesivir reduce inhospital mortality for COVID-19 patients compared to placebo?", the PICO elements are COVID-19 patients (Population), remdesivir (Intervention), placebo (Comparison), and inhospital mortality (Outcome), respectively.EBM search engines should be equipped to process both PICO and natural language clinical questions.Clinical evidence spans a broad spectrum of literature, with significant variability in quality.For example, systematic reviews are generally considered as higher-quality evidence than randomized controlled trials (RCTs), which, in turn represent higher quality than individual case reports.Consequently, an ideal EBM search engine should consider the quality of evidence for filtering or ranking the articles.Figure 2 depicts the architecture of an ideal EBM search engine, which allows PICO-style input and ranks results based on evidence quality.</p>
<p>Systems accepting PICO queries:</p>
<p>Several EBM search engines, such as Trip Database, the Cochrane PICO search, and Embase, accommodate PICO-based queries.PubMed for Handhelds 21 , a lightweight platform designed for handheld devices in clinical settings, also supports PICO-based search.The search interfaces for these systems typically contain text boxes corresponding the four primary PICO elements.In general, these systems provide more precise results since the search intent is explicitly stated in the query.For example, entering "diabetes" as the "Population" term, prompts EBM search engines to only return clinical studies on diabetes patients.In contrast, keywordbased search engines would return any article that mentions "diabetes," regardless of its relevance to patient studies.</p>
<p>Systems with filtered retrieval results: PubMed Clinical Queries search employs predefined filters 22,23 for clinically-relevant studies of various types, such as therapy and diagnosis.Users can also select broad (general) or narrow (specific) scopes for the filters.Clinical practitioners should use the narrow scope for a quick overview of the important studies at the point of care, while researchers synthesizing evidence should employ the broad scope for exhaustive searches.Several EBM search engines prioritize retrieval of secondary evidence, such as systematic reviews and critically-appraised topics, which typically have higher quality than primary evidence.A notable example is the Cochrane Database, which hosts over 11 thousand high-quality systematic reviews and protocols.Critically-appraised topics summarize the evidence on a specific topic, such as prevention of type 2 diabetes mellitus, using short, templated, titles to simplify retrieval.As a result, they provide convenient clinical decision support in systems like UpToDate, a commercial evidence-based clinical resource 3 .</p>
<p>Assisting evidence synthesis: Compared to evidence retrieval, fewer systems facilitate evidence synthesis, the systematic collection, analysis and combination of results from multiple research studies to reach a comprehensive conclusion about a specific question or topic 24 .Evidence synthesis culminates in the creation of high-quality publications such as systematic reviews.However, the user conducting an evidence synthesis would need to manually screen all related literature to address a clinical question without bias, an extremely time-consuming process due to the vast number of articles likely to be relevant across multiple databases 25 .Despite efforts to use machine learning to automate this screening process 26 , these features are not yet integrated into web-based EBM search engines.</p>
<p>Best practice: Literature search is a vital step in evidence-based medicine.To optimize this process, users should: (1) formulate clinical questions in the format of PICO elements; (2) utilize a system that ranks relevant studies by their evidence quality.</p>
<p>Precision medicine and genomics</p>
<p>Precision medicine (PM) is an emerging approach that tailors disease treatment and prevention based on individual variations in genes, environment, and lifestyle 27 .The rapid development of high-throughput sequencing techniques have precipitated a sharp decline in the cost of obtaining individual genomic data, surpassing the reduction predicted by Moore's Law 28 .Human genomes, with their high heterogeneity, contain a large number of genomic variants.It is estimated, for example, that there are 4 to 5 million singlenucleotide polymorphisms (SNPs) in a person's genome 29 .Understanding the biological function and clinical significance of these genomic variants is essential for the advancement of precision medicine.These data are typically stored in manually curated databases 30 such as UniProt 31 , dbSNP 32 , ClinVar 33 , and Gene Ontology 34 .These databases manually summarize and maintain primary findings from the literature about each data entry (e.g., variant, gene, or protein).However, the growth of the biomedical literature, with an average of 3,000 new articles per day 1 , outpaces the speed of manual curation, leaving a knowledge gap.To supplement these databases, search engines capable of extracting gene or variantrelated information directly from raw literature are needed.This section primarily discusses such systems.</p>
<p>A significant challenge for PM and genomics search engines is the presence of multiple representations or synonyms for the same genomic variant.For instance, the variant "V600E" could also be referred to as "Val600Glu," "1799T&gt;A," or "rs113488022."This synonymy causes retrieval challenges for keyword-based search engines.In response, many specialized literature retrieval tools have been proposed; their core functionality is shown in Figure 3, where the search engine should be able to retrieve all articles that mention the exact variant query as well as its synonyms.Recognizing synonymous mentions: Some tools, such as LitVar 18,35 , focus on normalizing variant synonyms in the literature.LitVar uses text mining tool tmVar 36,37 to recognize variant names and convert them to standardized form.LitVar indexes both abstracts from PubMed and full-texts from PubMed Central and is updated regularly to ensure retrieval of all current literature containing synonyms of the query.Another tool, variant2literature 38 , provides a structured query interface that allows users to specify a chromosome location .Unique to variant2literature is the ability to extracts variants from figures and tables in addition to the article text.</p>
<p>Linking genes and other information: Several systems go beyond recognizing synonymous gene mentions and explore genomic-related information.DigSee 39 accepts a triplet of gene, disease, and biological processes as input and finds sentences in PubMed abstracts that link the gene to the disease through the given biological processes.Evaluation studies have shown that DigSee covers more gene-disease relations than manually curated databases like Online Mendelian Inheritance in Man 40 , and that the findings are also reliable 41 .OncoSearch 42 specializes in retrieving literature evidence for gene expression changes and cancer progression status.Specifically, it annotates sentences from the literature to indicate whether the input gene is up-regulated or down-regulated, whether the input cancer progresses or regresses with the expression change, and the expected role of the gene in the cancer.Other tools such as DISEASES 43 and DisGeNet 44 offer a search interface to gene-disease relations extracted from literature but do not return the raw evidence.</p>
<p>Best practice: To find relevant information about a gene or a variant, we recommend first querying curated databases such as UniProt and ClinVar.For more recent findings or when these databases lack sufficient data to contextualize a specific variant, the use of search engines specialized for precision medicine and genomics is recommended.For example, LitVar can assist in finding information within the literature about the role of certain genomic variants in an emerging disease, which might not have been curated into structured databases yet.</p>
<p>Semantic search</p>
<p>Unlike the keyword-based search that seek exact matches for the input query, semantic search locates texts that are semantically related to the query.For example, "renal" and "kidney" are semantically very similar.Figure 4 outlines semantic search, where text units such as concepts and sentences that match the query semantically are returned, such as mentioning the same diseases and discussing possible treatments.These texts do not necessarily contain the exact terms from the query, making their retrieval by traditional literature search engines unlikely.While there are various forms of semantic relevance, semantic search engines typically focus on one type.We introduce search engines for two common types of semantic relevance: similar sentences and question-answer pairs.Similar sentence search: Article-level searches often overlook finer-grained information in sentences.Sentence-level searches are important for precise knowledge retrieval.For example, one can search for a particular finding and compare it with relevant findings from other articles.LitSense 45 , a web-based system for sentence retrieval from PubMed and PMC, utilizes an embedding-based retrieval system in addition to traditional keyword matching.Results in LitSense can be filtered by sections, such as Conclusions.While LitSense searches for all types of similar sentences, several literature search engines have also been proposed for more specific types of sentences [46][47][48] .Lahav et al. present a search engine for sentences that describe challenges and future directions in COVID-19 46 .SciRide Finder 47 finds cited statements describing the in-line references.BioNOT 48 indexes and searches sentences that contain negated events.While separate semantic search engines for different sentence types can provide valuable functionality, a unified similar search engine integrated with filters provides greater flexibility and a consistent user interface.Question answering: Biomedical inquiries are often naturally expressed as questions, such as the PICO-based clinical questions in EBM.However, traditional keyword-based search engines may not efficiently handle natural language questions because questions and answers often lack high lexical overlap.Biomedical question answering (QA) is an active research area 49 , but user-friendly web tools remain sparse.The askMEDLINE 50 system evolved from PubMed PICO search and enables direct input to the clinical questions, e.g., "Is irrigation with tap water an effective way to clean simple laceration before suturing?".askMEDLINE displays results as a list of relevant articles.AskHERMES 51 , a clinical QA system, performs semantic analysis on complex questions and extracts summaries from the relevant articles to directly answer the question, which is more convenient than a list of relevant articles which must be searched.COVID-19 Research Explorer and BioMed Explorer are experimental semantic search engines for biomedical literature developed by Google AI.The former focuses on COVID-19 articles in CORD-19 52 , and the latter encompasses all PubMed articles.Both explorers are presumably based on Google's systems 53,54 designed for BioASQ 55 , a challenge for biomedical information retrieval and QA, and have a modern search interface.Users ask natural language questions, and the answers are highlighted in the text snippets in the results.Users can also pose follow-up questions to further investigate the research topic.</p>
<p>Best practice: Users should consider using semantic search engines if their information needs are better expressed by natural language instead of keywords.Available tools include LitSense for finding relevant sentences and BioMed Explorer for answering biomedical questions with evidence from the literature.</p>
<p>Literature recommendation</p>
<p>Biomedical research often requires comprehensive exploration of related literature.Traditional keyword-based search engines are typically inefficient for this purpose due to the difficulty of formulating queries to exhaustively capture all relevant work.Literature recommendation engines instead allow users to explore articles relevant to a specific research topic or similar to a list of articles known to be relevant.This section mainly introduces two types of literature recommendation tools: topic-based and article-based, as depicted in Figure 5.Additional forms, such as passage-based literature recommenders or citation recommenders 56 are still experimental and have not been implemented as web applications.Article-based literature recommendation systems, on the other hand, generate a list of articles related to one or more initial (seed) articles.Modern literature search engines often provide a list of articles related to individual articles, such as the "similar articles" section in PubMed 59 .A few systems have been proposed, however, which support identifying articles related to a list of articles instead of individual ones.LitSuggest 60 , a literature recommendation system based on machine learning, rates candidate articles on their similarity to a user-supplied list of positive articles and dissimilarity to a list of negative articles.The list of negative articles is optional, with random articles used if not supplied by the user.Users can also provide human-in-the-loop feedback by annotating a subset of the scored candidate articles and re-training the recommendation model.BioReader 61 offers similar functionality, but it requires a list of negative articles.Several commercial literature search tools like Connected Papers6 and Litmaps7 provide visual representations of articles related to seed articles on a citation graph, thus aiding in the navigation of the academic literature and guiding focused research.A significant shortcoming of current systems is their inability to explain their recommendations 62 , which is particularly important in high-stakes fields like biomedicine.</p>
<p>Best practice: Recommendation systems primarily assist in literature exploration.Users can find articles related to a topic of interest, such as COVID-19, using a curated literature database, or locate articles similar to a specific list of articles through article-based literature recommenders like LitSuggest.</p>
<p>Literature mining</p>
<p>Literature mining aims to uncover novel insights from scientific publications through natural language processing (NLP) techniques 63 .NLP tasks include named entity recognition (NER), the task of recognizing named entities (biomedical concepts) such as genes and diseases 64 , and relation extraction (RE), which classifies relations between the entities identified 65 .For example, an NER tool could identify a genetic variant and a disease name in a sentence, and an RE tool might classify their relation as mutationcausing-disease. Extracted concepts and their relations can be organized into a graph, referred to as a knowledge graph, which structurally summarizes the knowledge encoded in the publications related to the given query.By displaying a knowledge graph, literature search engines provide users with an overview of the knowledge discovered, thereby facilitating new knowledge discovery by predicting potential missing links.This process is visualized in Figure 6. Figure 6.The architecture of a system for mining entity associations from the biomedical literature.The system retrieves articles relevant to a given query, extracts biomedical entities and their relationships (e.g., variant-causing-disease), and presents the search as a knowledge graph that visualizes the extracted entities and their relationships.</p>
<p>Entity-augmented literature search: Several literature search engines enhance the retrieved results with biomedical concepts.PubTator 66,67 highlights six types of concepts recognized by state-of-the-art NER tools, including genes, diseases, chemicals, mutations, species, and cell lines.PubTator has also made its annotations publicly available via bulk download and an application programming inference, allowing other search engines to augment the search results with PubTator concepts.Notably, PubTator has been integrated into platforms such as LitVar, LitSense, and LitCovid.Anne O'Tate 68,69 provides options to rank concepts, such as important words, important phrases, topics, authors, MeSH pairs, etc., that are extracted from the retrieved articles.</p>
<p>Relation-augmented literature search: Some systems further process the extracted concepts and show the search results using associated concepts.FACTA+ 70 finds concepts associated with the given concept and the supporting sentences and can uncover indirectly associated concepts through certain types of "pivot concepts" as the bridge.BEST 71 is a biomedical concept search tool that returns a list of biomedical concepts, including genes, diseases, targets, and transcription factors.Evidence sentences for concept relevance are also displayed in BEST.Semantic MEDLINE 72 extracts SemRep 73 predications, comprising two Unified Medical Language System 74 (UMLS) concepts and one UMLS semantic relation, from the retrieved articles and provides a graph visualization of the predications.SciSight 75 , an exploratory search system for COVID-19, can present a graph of biomedical concepts associated with the given concept.PubMedKB 76 extracts and visualizes semantic relations between variants, genes, diseases, and chemicals, offering a user interface with interactive semantic graphs for the input query.The LION literature-based discovery system 77 also presents the search results as a graph that contains biomedical concepts and their relations extracted from the literature.While many systems for constructing biomedical knowledge graphs automatically have been proposed, there has been less research on how these systems can assist users in literature-based knowledge discovery.The utility of knowledge graphs in this context remains to be confirmed in future studies.</p>
<p>Best practice: Literature mining tools can be employed to study the associations between biomedical concepts in the literature.Users should consider the concept and relation types of interest and choose the literature mining tools that incorporate such information.For example, PubTator provides annotations for six general concept types, but concepts beyond these types and concept relations are better supported in other literature search tools, such as SciSight for COVID-19 concepts and relations.</p>
<p>Looking Ahead: The Role of ChatGPT and Other Large Language Models in Literature Search</p>
<p>ChatGPT 78 and other large language models (LLMs) such as PaLM 79 have recently demonstrated considerable performance improvements on both general and biomedical domain-specific NLP tasks.There is a rising belief that these models could significantly change how users interact with information online, potentially including the biomedical literature 80 .In this section, we explore the potential application of LLMs to the biomedical literature search scenarios presented in this review.</p>
<p>Evidence-based medicine: LLMs can accelerate evidence synthesis in two key ways.First, they can suggest Boolean queries to aid literature screening for systematic reviews 81 .Following the retrieval of results, LLMs could potentially be used to summarize and synthesize the resulting articles 82,83 .However, preliminary evaluations have exposed various issues which must be addressed before widespread use.Apart from evidence synthesis, LLMs can also enhance the extraction of PICO elements from the medical literature 84 , thereby improving PICO-based EBM search engines.</p>
<p>Precision medicine and Genomics: Most genomics information resides in curated databases, which are not easily accessible due to their keyword-centric search functions and less modern user interfaces.LLMs can alleviate these access difficulties by autonomously utilizing tools such as the application programming interface (API) of specialized databases 85 .</p>
<p>Semantic search: LLMs have achieved state-of-the-art performance on several biomedical QA datasets 86,87 , which require clinical knowledge and biomedical reasoning capabilities 88,89 .This suggests that LLMs can provide direct responses to users' natural language questions using relevant documents returned from a traditional search engine.This feature, called retrieval augmentation, is already supported by experimental literature search engines such as scite8 , Elicit9 , and statpearls semantic search10 .These tools accept a natural language research question as input and retrieve relevant articles through semantic search.They further prompt LLMs such as GPT-3 90 to answer the user's question based on the retrieved relevant articles.The systems return both the relevant articles and the LLM-generated answers.This is commonly known as retrieval augmentation and has also been integrated into general web search engines such as the new Bing.However, these LLM-generated answers are susceptible to precision and recall errors [91][92][93] and should be carefully verified before use.</p>
<p>Literature recommendation:</p>
<p>The potential role of LLMs in literature recommendation remains largely unexplored.One possibility involves using LLMs to explain literature recommendations, i.e., describing why a recommended article is similar to the input article.This capability could be used to create a dataset for training smaller generative models, enabling more flexible and cost-effective and literature recommendation explanations.</p>
<p>Literature mining: Unlike other literature search scenarios that benefit from the generative capabilities of LLMs (e.g., summarization for semantic search), literature mining predominantly depends on NLP tasks such as NER and RE.LLMs generally do not outperform smaller task-specific models such as BERT 94 , fine-tuned specifically for these tasks 95 .However, LLMs may offer superior interpretations of the constructed knowledge graphs, revealing previously unknown associations between biomedical concepts.</p>
<p>Discussion</p>
<p>In this article, we introduced five specific use cases of biomedical literature search and available tools for each scenario.Two of these scenarios are application-oriented (EBM, PM), while three are technique-oriented (semantic search, literature recommendation and mining).Our classification, while practical, is not mutually exclusive, and the advantages of different systems can be combined to better meet diverse biomedical information needs.For instance, an EBM search engine might also process queries where the specified Population is associated with certain genomic variants, necessitating recognition of variant synonyms for comprehensive literature retrieval.Another instance is biocuration, the practice of converting literature data into database entries.This threestep process 96 , involves document selection, indexing documents with biomedical concepts, and extracting their specific relations or interactions.A system to support biocuration should be equipped with both literature recommendation and mining functionality to assist biocurators by suggesting relevant publications and highlighting the relevant biomedical concepts.</p>
<p>Users might be expected to prefer a single search portal that can fulfill multiple information needs.As current literature search systems are specialized and scattered, we believe a unified portal integrating all specific functionalities would facilitate access to biomedical literature.At a high-level, biomedical literature search systems comprise three key components, as depicted in Figure 1: the input interface, the ranking algorithm, and the output display.The search systems we have introduced differ primarily in these components, which we discuss below.</p>
<p>Input interface: Analogous to web search, literature search queries generally comprise several words 4,5 .Consequently, most literature search engines accept short text inputs, typically representing biomedical concepts or concepts, such as an author's name or a disease.In this review, we broadly denote such systems as keyword-based.However, more complex or specialized information needs require interfaces capable of processing semi-structured information or even non-text modalities.Semi-structured search interfaces accept separate texts for multiple pre-defined fields, akin to the advanced search interface in modern literature search engines and PICO-based EBM search.Some information needs defy expression in text, such as finding articles that are similar to one set but dissimilar to another set of articles, requiring interfaces designed specifically for the task.Although modern search interfaces consisting of one text box are simple and easy to use, the resulting queries can be ambiguous or overly general.As a result, taskoriented search interfaces should be designed for different biomedical literature search purpose, while a unified portal can be employed to triage the user's information needs into these task-oriented interfaces.</p>
<p>Ranking algorithms:</p>
<p>In literature search engines, the ranking algorithms assess article relevance for a given query, thereby determining which articles are returned to the user.PubMed employs the Best Match 6 ranking model, a machine learning approach trained via user click logs.Many other ranking algorithms, such as the BM25 algorithm 97 , rank articles based on the importance of the terms which overlap between the article and the query.These algorithms calculate general text-based relevance without domain-specific requirements, while certain biomedical subdomains have specific article ranking requirements.For example, in EBM, articles with higher quality clinical evidence should be ranked higher.In semantic search, articles with text units that are semantically related to the input query should be returned, irrespective of term overlap.For article-based literature recommendation, the system should rank articles similar to the positive seed articles and dissimilar to the negative seed articles.In addition to performing purposespecific ranking, future literature search engines should incorporate transparent and interpretable ranking algorithms.</p>
<p>Output display: search results are most commonly displayed as a list of article metadata, including titles, authors, publication types and dates, abstract snippets, and so forth, mimicking the general web search engines familiar to users.Though list-based display has been almost unchanged in general search engines for two decades, additional modules have been introduced to serve specific information needs.For example, many web search engines directly display the answer to a question query at the top of the results page, mirroring the goal of QA-based semantic search in biomedical literature.Certain literature mining systems construct and visualize a knowledge graph from the articles retrieved, aiding exploration and knowledge discovery.Given the remarkable text generation capabilities of LLMs such as ChatGPT, we anticipate future literature search engines will include high-level overviews of returned articles generated by LLMs.</p>
<p>Conclusion</p>
<p>Our aim has been to assist biomedical researchers and clinicians in finding the most suitable literature search tool to fulfill their specialized information needs.Specialized search engines may serve specific information needs in the biomedical literature more effectively than general-purpose systems.We characterized search scenarios for five specific information needs: evidence-based medicine, precision medicine and genomics, semantic search, literature recommendation, and literature mining.We included 36 systems for biomedical literature search and classified each according to the unique information needs they fulfill.All tools discussed are web-based, freely available, regularly maintained, and designed for searching the biomedical literature.Finally, we discussed the future of biomedical literature search, especially considering the potential impacts of large language models (LLMs) such as ChatGPT.</p>
<p>Figure 1 .
1
Figure 1.Overview of five specialized search scenarios in biomedicine: evidence-based medicine, precision medicine &amp; genomics, semantic search, literature recommendation, and literature mining.Each search scenario is characterized by its unique input interface, search or ranking algorithm, and output display.</p>
<p>Figure 2 .
2
Figure 2. The architecture of a search engine for evidence-based medicine (EBM).EBM search engines should incorporate PICO elements (Population, Intervention, Comparison, and Outcome) within the input query and rank the articles returned based on the quality of the evidence.</p>
<p>Figure 3 .
3
Figure 3. Illustration of the functionality of a search engine for precision medicine and genomics.Search engines for precision medicine and genomics should handle queries containing genomic variants and identify all synonymous references to these variants in the literature.</p>
<p>Figure 4 .
4
Figure 4. Depiction of semantic search.Unlike traditional keyword-based search engines, semantic search engines process words and phrases according to their meaning rather than the literal text.For instance, "heart attack", "AMI", and "myocardial infarction" share similar meanings.</p>
<p>Figure 5 .
5
Figure 5. Illustration of topic-based and article-based literature recommendation systems.Topic-based systems provide articles relevant to a specific topic, while articlebased systems return articles similar to a group of initial (seed) articles and dissimilar to a group of irrelevant articles.</p>
<p>Table 1 .
1
Web-based biomedical literature search tools.Literature search tools included in this study are web-based, freely available, regularly maintained, and designed for searching the biomedical literature.
ResourceWebsiteBrief descriptionGeneralPubMedhttps://pubmed.ncbi.nlm.nih.gov/ General-purpose biomedicalliterature search engine.PubMed Centralhttps://www.ncbi.nlm.nih.gov/pmcSupporting full-text search./Europe PMChttps://europepmc.org/Searching both abstractsand full-texts.Evidence-based Medicine (EBM)PubMed PICOhttps://pubmedhh.nlm.nih.gov/picSearching clinical studiesSearcho/index.phpwith PICO elements.PubMed Clinicalhttps://pubmed.ncbi.nlm.nih.gov/Searching clinical studiesQueriesclinical/with various type and scopefilters.Cochrane Libraryhttps://www.cochranelibrary.com/ Searching high-qualitysystematic reviews.Trip Databasehttps://www.tripdatabase.com/General EBM searchengine.Precision Medicine (PM) and GenomicsLitVarhttps://www.ncbi.nlm.nih.gov/research/litvar</p>
<p>Experimental literature search systems augmented by large language models (LLMs)
Scitehttps://hippocratic-medical-Finding relevant articles toquestions.herokuapp.com/users' question and thenElicithttps://elicit.org/using LLMs to answer theConsensushttps://consensus.app/question with the retrievedStatpearls semantichttps://hippocratic-medical-articlessearchquestions.herokuapp.com/PubMed &amp; PMC: the first stop
https://www.nlm.nih.gov/mesh/meshhome.html
https://pubmed.ncbi.nlm.nih.gov/citmatch/
https://www.wolterskluwer.com/en/solutions/uptodate
https://www.who.int/emergencies/diseases/novel-coronavirus-2019/global-research-on-novel-coronavirus-2019-ncov
5 https://icite.od.nih.gov/covid19/search/
6 https://www.connectedpapers.com
/ 7 https://www.litmaps.com/
https://scite.ai/
https://elicit.org/
https://hippocratic-medical-questions.herokuapp.com/
AcknowledgmentThis research was supported by the NIH Intramural Research Program, National Library of Medicine.
Manual curation is not sufficient for annotation of genomic databases. W A Baumgartner, Jr, K B Cohen, L M Fox, G Acquaah-Mensah, L Hunter, 10.1093/bioinformatics/btm229Bioinformatics. 232007</p>
<p>Answering physicians' clinical questions: obstacles and potential solutions. J W Ely, J A Osheroff, M L Chambliss, M H Ebell, M E Rosenbaum, 10.1197/jamia.M1608J Am Med Inform Assoc. 122005</p>
<p>A survey on literature based discovery approaches in biomedical domain. V Gopalakrishnan, K Jha, W Jin, A Zhang, 10.1016/j.jbi.2019.103141J Biomed Inform. 931031412019</p>
<p>Understanding PubMed user search behavior through log analysis. Database (Oxford). R Islamaj Dogan, G C Murray, A Neveol, Z Lu, 10.1093/database/bap0182009. 200918</p>
<p>How user intelligence is improving PubMed. N Fiorini, R Leaman, D J Lipman, Z Lu, 10.1038/nbt.4267Nat Biotechnol. 2018</p>
<p>Best Match: New relevance search for PubMed. N Fiorini, 10.1371/journal.pbio.2005343PLoS Biol. 16e20053432018</p>
<p>The coronavirus pandemic in five powerful charts. E Callaway, D Cyranoski, S Mallapaty, E Stoye, J Tollefson, 10.1038/d41586-020-00758-2Nature. 5792020</p>
<p>Surging publications on the COVID-19 pandemic. G Li, 10.1016/j.cmi.2020.09.010Clin Microbiol Infect. 272021</p>
<p>Keep up with the latest coronavirus research. Q Chen, A Allot, Z Lu, 10.1038/d41586-020-00694-1Nature. 5791932020</p>
<p>LitCovid: an open database of COVID-19 literature. Q Chen, A Allot, Z Lu, 10.1093/nar/gkaa952Nucleic Acids Res. 492021</p>
<p>PubMed and beyond: a survey of web tools for searching biomedical literature. Database (Oxford). Z Lu, 10.1093/database/baq0362011. 201136</p>
<p>PubMed alternatives to search MEDLINE: an environmental scan. A Keepanasseril, 10.4103/0970-9290.142562Indian J Dent Res. 252014</p>
<p>Advancing PubMed? A comparison of third-party PubMed/Medline tools. L E Wildgaard, H Lund, 10.1108/LHT-06-2016-0066Library Hi Tech. 342016</p>
<p>BIOMedical Search Engine Framework: Lightweight and customized implementation of domain-specific biomedical search engines. A G Jacome, F Fdez-Riverola, A Lourenco, 10.1016/j.cmpb.2016.03.030Comput Methods Programs Biomed. 1312016</p>
<p>. N Fiorini, D J Lipman, Lu, 10.7554/eLife.28801Z. Towards PubMed. 62017</p>
<p>Europe PMC: a full-text literature database for the life sciences and platform for innovation. P M C C Europe, 10.1093/nar/gku1061Nucleic Acids Res. 432015</p>
<p>Impact of PubMed search filters on the retrieval of evidence by physicians. S Z Shariff, 10.1503/cmaj.101661CMAJ. 1842012</p>
<p>LitVar: a semantic search engine for linking genomic variant data in PubMed and PMC. A Allot, 10.1093/nar/gky355Nucleic Acids Res. 462018</p>
<p>Evidence-based medicine. D L Sackett, 10.1016/s0146-0005(97)80013-4Semin Perinatol. 211997</p>
<p>The well-built clinical question: a key to evidence-based decisions. W S Richardson, M C Wilson, J Nishikawa, R S Hayward, ACP J Club. 1231995</p>
<p>Utilization of the PICO framework to improve searching PubMed for clinical questions. C Schardt, M B Adams, T Owens, S Keitz, P Fontelo, 10.1186/1472-6947-7-16BMC Med Inform Decis Mak. 7162007</p>
<p>Optimal search strategies for retrieving scientifically strong studies of treatment from Medline: analytical survey. R B Haynes, 10.1136/bmj.38446.498542.8FBMJ. 33011792005</p>
<p>Developing optimal search strategies for detecting clinically sound studies in MEDLINE. R B Haynes, N Wilczynski, K A Mckibbon, C J Walker, J C Sinclair, 10.1136/jamia.1994.95153434J Am Med Inform Assoc. 11994</p>
<p>Cochrane handbook for systematic reviews of interventions. J P Higgins, 2019John Wiley &amp; Sons</p>
<p>Semiautomated screening of biomedical citations for systematic reviews. B C Wallace, T A Trikalinos, J Lau, C Brodley, C H Schmid, 10.1186/1471-2105-11-55BMC Bioinformatics. 11552010</p>
<p>Toward systematic review automation: a practical guide to using machine learning tools in research synthesis. I J Marshall, B C Wallace, 10.1186/s13643-019-1074-9Syst Rev. 81632019</p>
<p>A new initiative on precision medicine. F S Collins, H Varmus, 10.1056/NEJMp1500523N Engl J Med. 3722015</p>
<p>Technology: The $1,000 genome. E C Hayden, 10.1038/507294aNature. 5072014</p>
<p>A global reference for human genetic variation. C Genomes Project, 10.1038/nature15393Nature. 5262015</p>
<p>Database resources of the national center for biotechnology information. E W Sayers, Nucleic acids research. 49D102021</p>
<p>UniProt: the universal protein knowledgebase in 2021. C Uniprot, 10.1093/nar/gkaa1100Nucleic Acids Res. 492021</p>
<p>dbSNP: the NCBI database of genetic variation. S T Sherry, 10.1093/nar/29.1.308Nucleic Acids Res. 292001</p>
<p>ClinVar: public archive of relationships among sequence variation and human phenotype. M J Landrum, 10.1093/nar/gkt1113Nucleic Acids Res. 422014</p>
<p>Gene ontology: tool for the unification of biology. The Gene Ontology Consortium. M Ashburner, 10.1038/75556Nat Genet. 252000</p>
<p>Tracking genetic variants in the biomedical literature using LitVar 2.0. A Allot, 10.1038/s41588-023-01414-xNat Genet. 552023</p>
<p>tmVar: a text mining approach for extracting sequence variants in biomedical literature. C H Wei, B R Harris, H Y Kao, Z Lu, 10.1093/bioinformatics/btt156Bioinformatics. 292013</p>
<p>tmVar 2.0: integrating genomic variant information from literature with dbSNP and ClinVar for precision medicine. C H Wei, 10.1093/bioinformatics/btx541Bioinformatics. 342018</p>
<p>variant2literature: full text literature search for genetic variants. Y.-H Lin, bioRxiv. 5834502019</p>
<p>DigSee: Disease gene search engine with evidence sentences (version cancer). J Kim, 10.1093/nar/gkt531Nucleic Acids Res. 412013</p>
<p>Online Mendelian Inheritance in Man (OMIM(R)), an online catalog of human genes and genetic disorders. J S Amberger, C A Bocchini, F Schiettecatte, A F Scott, A Hamosh, Omim, Org, 10.1093/nar/gku1205Nucleic Acids Res. 432015</p>
<p>An analysis of disease-gene relationship from Medline abstracts by. J Kim, J J Kim, H Lee, 10.1038/srep40154DigSee. Sci Rep. 7401542017</p>
<p>OncoSearch: cancer gene search engine with literature evidence. H J Lee, T C Dang, H Lee, J C Park, 10.1093/nar/gku368Nucleic Acids Res. 422014</p>
<p>DISEASES: text mining and data integration of disease-gene associations. S Pletscher-Frankild, A Palleja, K Tsafou, J X Binder, L J Jensen, 10.1016/j.ymeth.2014.11.020Methods. 742015</p>
<p>DisGeNET: a discovery platform for the dynamical exploration of human diseases and their genes. Database (Oxford). J Pinero, 10.1093/database/bav0282015. 201528</p>
<p>LitSense: making sense of biomedical literature at sentence level. A Allot, 10.1093/nar/gkz289Nucleic Acids Res. 472019</p>
<p>A Search Engine for Discovery of Scientific Challenges and Directions. D Lahav, 10.1609/aaai.v36i11.21456Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>SciRide Finder: a citation-based paradigm in biomedical literature search. A Volanakis, K Krawczyk, 10.1038/s41598-018-24571-0Sci Rep. 861932018</p>
<p>BioNOT: a searchable database of biomedical negated sentences. S Agarwal, H Yu, I Kohane, 10.1186/1471-2105-12-420BMC Bioinformatics. 122011</p>
<p>Biomedical question answering: A survey of approaches and challenges. Q Jin, ACM Computing Surveys (CSUR). 552022</p>
<p>askMEDLINE: a free-text, natural language query tool for MEDLINE/PubMed. P Fontelo, F Liu, M Ackerman, 10.1186/1472-6947-5-5BMC Med Inform Decis Mak. 52005</p>
<p>AskHERMES: An online question answering system for complex clinical questions. Y Cao, 10.1016/j.jbi.2011.01.004J Biomed Inform. 442011</p>
<p>CORD-19: The COVID-19 Open Research Dataset. L L Wang, ArXiv. 2020</p>
<p>J Ma, I Korotkov, Y Yang, K Hall, R Mcdonald, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterMain</p>
<p>. J Ma, I Korotkov, K B Hall, R T Mcdonald, in CLEF (Working Notes</p>
<p>An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition. G Tsatsaronis, 10.1186/s12859-015-0564-6BMC Bioinformatics. 161382015</p>
<p>Citation recommendation: approaches and datasets. M Frber, A Jatowt, 10.1007/s00799-020-00288-2International Journal on Digital Libraries. 212020</p>
<p>LitCovid in 2022: an information resource for the COVID-19 literature. Q Chen, 10.1093/nar/gkac1005Nucleic Acids Res. 512023</p>
<p>Analyzing the vast coronavirus literature with CoronaCentral. J Lever, R B Altman, 10.1073/pnas.2100766118Proc Natl Acad Sci U S A. 1182021</p>
<p>PubMed related articles: a probabilistic topic-based model for content similarity. J Lin, W J Wilbur, 10.1186/1471-2105-8-423BMC Bioinformatics. 84232007</p>
<p>LitSuggest: a web-based system for literature recommendation and curation using machine learning. A Allot, K Lee, Q Chen, L Luo, Z Lu, 10.1093/nar/gkab326Nucleic Acids Res. 492021</p>
<p>BioReader: a text mining tool for performing classification of biomedical literature. C Simon, 10.1186/s12859-019-2607-xBMC Bioinformatics. 192019</p>
<p>Explainable recommendation: A survey and new perspectives. Y Zhang, X Chen, Foundations and Trends in Information Retrieval. 142020</p>
<p>Recent advances in biomedical literature mining. S Zhao, C Su, Z Lu, F Wang, 10.1093/bib/bbaa057Brief Bioinform. 222021</p>
<p>. R Leaman, G Gonzalez, Biocomputing, 2008. 2008World Scientific</p>
<p>Assessing the state of the art in biomedical relation extraction: overview of the BioCreative V chemical-disease relation (CDR) task. C.-H Wei, Database. 2016. 2016</p>
<p>PubTator central: automated concept annotation for biomedical full text articles. C H Wei, A Allot, R Leaman, Z Lu, 10.1093/nar/gkz389Nucleic Acids Res. 472019</p>
<p>PubTator: a web-based text mining tool for assisting biocuration. C H Wei, H Y Kao, Z Lu, 10.1093/nar/gkt441Nucleic Acids Res. 412013</p>
<p>Value-added PubMed search engine for analysis and text mining. N R Smalheiser, D P Fragnito, E E Tirk, O' Anne, Tate, 10.1371/journal.pone.0248335PLoS One. 16e02483352021</p>
<p>A tool to support userdriven summarization, drill-down and browsing of PubMed search results. N R Smalheiser, W Zhou, V I Torvik, O' Anne, Tate, 10.1186/1747-5333-3-2J Biomed Discov Collab. 32008</p>
<p>Discovering and visualizing indirect associations between biomedical concepts. Y Tsuruoka, M Miwa, K Hamamoto, J Tsujii, S Ananiadou, 10.1093/bioinformatics/btr214Bioinformatics. 272011</p>
<p>BEST: Next-Generation Biomedical Entity Search Tool for Knowledge Discovery from Biomedical Literature. S Lee, 10.1371/journal.pone.0164680PLoS One. 11e01646802016</p>
<p>Semantic MEDLINE: An advanced information management application for biomedicine. T C Rindflesch, H Kilicoglu, M Fiszman, G Rosemblat, D Shin, 10.3233/ISU-2011-0627Information Services &amp; Use. 312011</p>
<p>The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text. T C Rindflesch, M Fiszman, 10.1016/j.jbi.2003.11.003J Biomed Inform. 362003</p>
<p>The Unified Medical Language System: an informatics research collaboration. B L Humphreys, D A Lindberg, H M Schoolman, G O Barnett, 10.1136/jamia.1998.0050001J Am Med Inform Assoc. 51998</p>
<p>T Hope, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations2020</p>
<p>pubmedKB: an interactive web server for exploring biomedical entity relations in the biomedical literature. P H Li, 10.1093/nar/gkac310Nucleic Acids Res. 2022</p>
<p>LBD: a literature-based discovery system for cancer biology. S Pyysalo, 10.1093/bioinformatics/bty845Bioinformatics. 352019</p>
<p>Optimizing Language Models for Dialogue. Openai, Chatgpt, 2022</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, arXiv:2204.023112022arXiv preprint</p>
<p>How Will ChatGPT Affect Information Seeking from the Medical Literature?. Q Jin, R Leaman, Z Lu, Retrieve, Summarize, Verify, 10.1681/ASN.0000000000000166J Am Soc Nephrol. 2023</p>
<p>Can chatgpt write a good boolean query for systematic review literature search?. S Wang, H Scells, B Koopman, G Zuccon, arXiv:2302.034952023arXiv preprint</p>
<p>Summarizing, Simplifying, and Synthesizing Medical Evidence Using GPT-3 (with Varying Success). C Shaib, arXiv:2305.062992023arXiv preprint</p>
<p>Evaluating Large Language Models on Medical Evidence Summarization. medRxiv. L Tang, 2023.2004. 2022. 202323288967</p>
<p>S Wadhwa, J Deyoung, B Nye, S Amir, B C Wallace, arXiv:2305.03642Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. 2023arXiv preprint</p>
<p>Q Jin, Y Yang, Q Chen, Z Lu, Genegpt, arXiv:2304.09667Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information. 2023arXiv preprint</p>
<p>Large Language Models Encode Clinical Knowledge. K Singhal, arXiv:2212.131382022arXiv preprint</p>
<p>Can large language models reason about medical questions?. V Livin, C E Hother, O Winther, arXiv:2207.081432022arXiv preprint</p>
<p>Q Jin, B Dhingra, Z Liu, W Cohen, X Lu, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP</p>
<p>What disease does this patient have? a large-scale open domain question answering dataset from medical exams. D Jin, Applied Sciences. 1164212021</p>
<p>Language models are few-shot learners. T Brown, Advances in neural information processing systems. 332020</p>
<p>N F Liu, T Zhang, P Liang, arXiv:2304.09848Evaluating Verifiability in Generative Search Engines. 2023arXiv preprint</p>
<p>Audit AI search tools now, before they skew research. M Gusenbauer, Nature. 6174392023</p>
<p>AI science search engines are exploding in number-are they any good?. K Sanderson, Nature. 2023</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter. Long and Short Papers. the 2019 Conference of the North American ChapterHuman Language Technologies1</p>
<p>Thinking about gpt-3 in-context learning for biomedical ie? think again. B J Gutirrez, arXiv:2203.084102022arXiv preprint</p>
<p>Text mining for the biocuration workflow. Database (Oxford). L Hirschman, 10.1093/database/bas0202012. 201220</p>
<p>Okapi at TREC-3. S E Robertson, S Walker, S Jones, M M Hancock-Beaulieu, M Gatford, Nist Special Publication Sp. 1091091995</p>            </div>
        </div>

    </div>
</body>
</html>
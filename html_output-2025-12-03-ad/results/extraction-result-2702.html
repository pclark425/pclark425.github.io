<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2702 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2702</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2702</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-53225181</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1811.02872v2.pdf" target="_blank">Baselines for Reinforcement Learning in Text Games</a></p>
                <p><strong>Paper Abstract:</strong> The ability to learn optimal control policies in systems where action space is defined by sentences in natural language would allow many interesting real-world applications such as automatic optimisation of dialogue systems. Text-based games with multiple endings and rewards are a promising platform for this task, since their feedback allows us to employ reinforcement learning techniques to jointly learn text representations and control policies. We argue that the key property of AI agents, especially in the text-games context, is their ability to generalise to previously unseen games. We present a minimalistic text-game playing agent, testing its generalisation and transfer learning performance and showing its ability to play multiple games at once. We also present pyfiction, an open-source library for universal access to different text games that could, together with our agent that implements its interface, serve as a baseline for future research.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2702.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2702.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSAQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Siamese State-Action Q-Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimalistic siamese LSTM-based deep Q-network that jointly embeds state and action texts, uses a cosine-similarity interaction to estimate Q(s,a), and is trained with DQN-style learning including experience replay and prioritized sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SSAQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Siamese network with shared word-embedding and LSTM layers for both state and action inputs, per-branch dense layers (8-dim) and a cosine-similarity output producing Q-values; trained with a DQN variant using experience replay and prioritized sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>pyfiction (six IF games: Saving John, Machine of Death, Cat Simulator 2016, Star Court, The Red Hair, Transit)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Choice-/hypertext-based interactive fiction text games with natural-language state descriptions and action texts; tasks are to maximise cumulative reward and reach good endings across multiple deterministic and non-deterministic games.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>replay buffer (experience replay) and episodic visit counts (simple per-run history); internal recurrent state (LSTM hidden state) for sequence memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Experience replay: buffer of (s,a,r,s') tuples (prioritised sampling of positive-reward tuples); history: per-episode count table h(s,a) mapping state-action pair to integer visit count; LSTM: internal sequential hidden state vector</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Experience tuples (observations, actions, rewards, next observations); history stores counts of how many times action a was chosen in state s during current episode; LSTM hidden state implicitly stores recent word/observation sequence information</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Replay: prioritized sampling (fraction p of batch are positive-reward samples); History: direct lookup by current (s,a); LSTM: implicit sequential read via hidden state propagation</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Replay: new experience tuples appended to replay buffer each step (standard DQN-style); History: increment count h(s,a) each time action a is selected in state s during the current run (reset at episode end); LSTM: updated every time step through recurrent dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Replay: stabilise and de-correlate training updates and prioritise rewarding transitions; History: intrinsic motivation / loop avoidance by penalising revisiting same state-action within an episode; LSTM: capture word-order and short-term context for state/action representations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>SSAQN achieves near-optimal final rewards on deterministic games (e.g., Saving John final reward 19.4, The Red Hair 19.3, Transit 19.5) and substantially outperforms DRRN on Saving John and Machine of Death (SSAQN individual-game Saving John 19.4 vs DRRN lower; Machine of Death SSAQN 15.4 individual, ~21.0 when training on multiple games vs DRRN lower — exact DRRN numeric scores not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The paper reports that the per-episode history function helps avoid infinite loops and acts as simple intrinsic motivation, improving exploration; prioritized experience replay (including prioritising positive-reward tuples) is used to speed learning and stabilise training; using an LSTM (sequential memory) for representations is argued to be superior to bag-of-words for resolving state aliasing and capturing word order.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No detailed ablation quantifying memory limits is provided; replay buffer size is unspecified. The agent still fails to generalise to unseen games in the tested small corpus, which the authors attribute mainly to lack of overlapping phrases/semantics between games rather than replay/history mechanisms themselves.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td>SSAQN's combination of shared LSTM embeddings (for sequential representation), prioritized experience replay for training, and a simple per-episode history function is presented as the effective configuration; it outperforms BOW-based DRRN and matches or exceeds prior LSTM-DQN behavior in experiments described.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baselines for Reinforcement Learning in Text Games', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2702.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2702.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LSTM Deep Q-Network (LSTM-DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agent from prior work that uses an LSTM to produce text state representations followed by a DQN variant to estimate Q-values for verb-object action pairs; designed to capture sequential sentence properties.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Language Understanding for Text-based Games Using Deep Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LSTM-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses an LSTM for representation generation and a DQN-style value estimator; treats actions as (verb,object) tuples and computes separate Q-values for verbs and objects which are averaged to form action Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>parser-based / choice-converted text games (prior work evaluated on a subset of IF games)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Text-based interactive fiction where actions are verb-object tuples; the model reduces parser-based games to choice-based by enumerating a subset of verb-object combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>working memory implemented as recurrent hidden state (LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Sequential recurrent hidden state (LSTM internal state) that propagates across input tokens/timesteps</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Implicit encoding of recent word sequence and sentence-level ordering in LSTM hidden units</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Implicit via the LSTM hidden state; no explicit retrieval mechanism described in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>Updated at every timestep as the LSTM processes input tokens</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To capture sentence-level dependencies and word order (mitigate state aliasing) when forming state and action representations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The paper notes that LSTM-based representation can capture word order and thus address state aliasing issues that bag-of-words cannot; however, the particular LSTM-DQN prior architecture had limitations such as only accepting two-word actions and averaging verb/object Q-values which can be problematic.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Architectural limits: action representation constrained to verb-object pairs; averaging verb and object Q-values may be inappropriate when verb-object values are interdependent. No detailed capacity or retrieval limitations are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baselines for Reinforcement Learning in Text Games', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2702.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2702.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A network for hypertext-based text games that uses separate embeddings for states and actions and a bag-of-words representation; interaction via inner product of state and action vectors to estimate Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep Reinforcement Learning with an Unbounded Action Space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses simple bag-of-words embeddings for state and action texts with separate embedding networks and computes Q(s,a) as an inner product between state and action representations; trained with a DQN variant.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>hypertext-based IF games (Saving John, Machine of Death in referenced experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Hypertext interactive fiction games where available actions are textual hyperlinks; task is to select actions that lead to better endings and higher cumulative rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Paper criticises DRRN's bag-of-words representation for being unable to handle state aliasing and subtle word-order distinctions; authors report that SSAQN (which uses LSTM and shared embeddings) outperforms DRRN on Saving John and Machine of Death.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No sequence memory: BOW loses word order and hence cannot distinguish states that differ only by word order or subtle phrasing; this is presented as a core limitation for generalisation and fine distinctions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Baselines for Reinforcement Learning in Text Games', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language Understanding for Text-based Games Using Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Deep Reinforcement Learning with an Unbounded Action Space <em>(Rating: 2)</em></li>
                <li>Prioritized Experience Replay <em>(Rating: 2)</em></li>
                <li>Human-level control through deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2702",
    "paper_id": "paper-53225181",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "SSAQN",
            "name_full": "Siamese State-Action Q-Network",
            "brief_description": "A minimalistic siamese LSTM-based deep Q-network that jointly embeds state and action texts, uses a cosine-similarity interaction to estimate Q(s,a), and is trained with DQN-style learning including experience replay and prioritized sampling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SSAQN",
            "agent_description": "Siamese network with shared word-embedding and LSTM layers for both state and action inputs, per-branch dense layers (8-dim) and a cosine-similarity output producing Q-values; trained with a DQN variant using experience replay and prioritized sampling.",
            "base_model_size": null,
            "game_benchmark_name": "pyfiction (six IF games: Saving John, Machine of Death, Cat Simulator 2016, Star Court, The Red Hair, Transit)",
            "game_description": "Choice-/hypertext-based interactive fiction text games with natural-language state descriptions and action texts; tasks are to maximise cumulative reward and reach good endings across multiple deterministic and non-deterministic games.",
            "uses_memory": true,
            "memory_type": "replay buffer (experience replay) and episodic visit counts (simple per-run history); internal recurrent state (LSTM hidden state) for sequence memory",
            "memory_structure": "Experience replay: buffer of (s,a,r,s') tuples (prioritised sampling of positive-reward tuples); history: per-episode count table h(s,a) mapping state-action pair to integer visit count; LSTM: internal sequential hidden state vector",
            "memory_content": "Experience tuples (observations, actions, rewards, next observations); history stores counts of how many times action a was chosen in state s during current episode; LSTM hidden state implicitly stores recent word/observation sequence information",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Replay: prioritized sampling (fraction p of batch are positive-reward samples); History: direct lookup by current (s,a); LSTM: implicit sequential read via hidden state propagation",
            "memory_update_strategy": "Replay: new experience tuples appended to replay buffer each step (standard DQN-style); History: increment count h(s,a) each time action a is selected in state s during the current run (reset at episode end); LSTM: updated every time step through recurrent dynamics",
            "memory_usage_purpose": "Replay: stabilise and de-correlate training updates and prioritise rewarding transitions; History: intrinsic motivation / loop avoidance by penalising revisiting same state-action within an episode; LSTM: capture word-order and short-term context for state/action representations",
            "performance_with_memory": "SSAQN achieves near-optimal final rewards on deterministic games (e.g., Saving John final reward 19.4, The Red Hair 19.3, Transit 19.5) and substantially outperforms DRRN on Saving John and Machine of Death (SSAQN individual-game Saving John 19.4 vs DRRN lower; Machine of Death SSAQN 15.4 individual, ~21.0 when training on multiple games vs DRRN lower — exact DRRN numeric scores not provided in this paper).",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "The paper reports that the per-episode history function helps avoid infinite loops and acts as simple intrinsic motivation, improving exploration; prioritized experience replay (including prioritising positive-reward tuples) is used to speed learning and stabilise training; using an LSTM (sequential memory) for representations is argued to be superior to bag-of-words for resolving state aliasing and capturing word order.",
            "memory_limitations": "No detailed ablation quantifying memory limits is provided; replay buffer size is unspecified. The agent still fails to generalise to unseen games in the tested small corpus, which the authors attribute mainly to lack of overlapping phrases/semantics between games rather than replay/history mechanisms themselves.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": "SSAQN's combination of shared LSTM embeddings (for sequential representation), prioritized experience replay for training, and a simple per-episode history function is presented as the effective configuration; it outperforms BOW-based DRRN and matches or exceeds prior LSTM-DQN behavior in experiments described.",
            "uuid": "e2702.0",
            "source_info": {
                "paper_title": "Baselines for Reinforcement Learning in Text Games",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "LSTM-DQN",
            "name_full": "LSTM Deep Q-Network (LSTM-DQN)",
            "brief_description": "An agent from prior work that uses an LSTM to produce text state representations followed by a DQN variant to estimate Q-values for verb-object action pairs; designed to capture sequential sentence properties.",
            "citation_title": "Language Understanding for Text-based Games Using Deep Reinforcement Learning",
            "mention_or_use": "mention",
            "agent_name": "LSTM-DQN",
            "agent_description": "Uses an LSTM for representation generation and a DQN-style value estimator; treats actions as (verb,object) tuples and computes separate Q-values for verbs and objects which are averaged to form action Q-values.",
            "base_model_size": null,
            "game_benchmark_name": "parser-based / choice-converted text games (prior work evaluated on a subset of IF games)",
            "game_description": "Text-based interactive fiction where actions are verb-object tuples; the model reduces parser-based games to choice-based by enumerating a subset of verb-object combinations.",
            "uses_memory": true,
            "memory_type": "working memory implemented as recurrent hidden state (LSTM)",
            "memory_structure": "Sequential recurrent hidden state (LSTM internal state) that propagates across input tokens/timesteps",
            "memory_content": "Implicit encoding of recent word sequence and sentence-level ordering in LSTM hidden units",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Implicit via the LSTM hidden state; no explicit retrieval mechanism described in this paper",
            "memory_update_strategy": "Updated at every timestep as the LSTM processes input tokens",
            "memory_usage_purpose": "To capture sentence-level dependencies and word order (mitigate state aliasing) when forming state and action representations",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "The paper notes that LSTM-based representation can capture word order and thus address state aliasing issues that bag-of-words cannot; however, the particular LSTM-DQN prior architecture had limitations such as only accepting two-word actions and averaging verb/object Q-values which can be problematic.",
            "memory_limitations": "Architectural limits: action representation constrained to verb-object pairs; averaging verb and object Q-values may be inappropriate when verb-object values are interdependent. No detailed capacity or retrieval limitations are reported here.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2702.1",
            "source_info": {
                "paper_title": "Baselines for Reinforcement Learning in Text Games",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Relevance Network",
            "brief_description": "A network for hypertext-based text games that uses separate embeddings for states and actions and a bag-of-words representation; interaction via inner product of state and action vectors to estimate Q-values.",
            "citation_title": "Deep Reinforcement Learning with an Unbounded Action Space",
            "mention_or_use": "mention",
            "agent_name": "DRRN",
            "agent_description": "Uses simple bag-of-words embeddings for state and action texts with separate embedding networks and computes Q(s,a) as an inner product between state and action representations; trained with a DQN variant.",
            "base_model_size": null,
            "game_benchmark_name": "hypertext-based IF games (Saving John, Machine of Death in referenced experiments)",
            "game_description": "Hypertext interactive fiction games where available actions are textual hyperlinks; task is to select actions that lead to better endings and higher cumulative rewards.",
            "uses_memory": false,
            "memory_type": null,
            "memory_structure": null,
            "memory_content": null,
            "memory_capacity": null,
            "memory_retrieval_strategy": null,
            "memory_update_strategy": null,
            "memory_usage_purpose": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_memory_ablation": null,
            "memory_effectiveness_findings": "Paper criticises DRRN's bag-of-words representation for being unable to handle state aliasing and subtle word-order distinctions; authors report that SSAQN (which uses LSTM and shared embeddings) outperforms DRRN on Saving John and Machine of Death.",
            "memory_limitations": "No sequence memory: BOW loses word order and hence cannot distinguish states that differ only by word order or subtle phrasing; this is presented as a core limitation for generalisation and fine distinctions.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2702.2",
            "source_info": {
                "paper_title": "Baselines for Reinforcement Learning in Text Games",
                "publication_date_yy_mm": "2018-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language Understanding for Text-based Games Using Deep Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "language_understanding_for_textbased_games_using_deep_reinforcement_learning"
        },
        {
            "paper_title": "Deep Reinforcement Learning with an Unbounded Action Space",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_with_an_unbounded_action_space"
        },
        {
            "paper_title": "Prioritized Experience Replay",
            "rating": 2,
            "sanitized_title": "prioritized_experience_replay"
        },
        {
            "paper_title": "Human-level control through deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "humanlevel_control_through_deep_reinforcement_learning"
        }
    ],
    "cost": 0.011114249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Baselines for Reinforcement Learning in Text Games</p>
<p>Mikuláš Zelinka zelinka@ktiml.mff.cuni.cz 
Faculty of Mathematics and Physics Prague
Charles University
Czech Republic</p>
<p>Baselines for Reinforcement Learning in Text Games
Index Terms-Text gamesreinforcement learningneural net- works
The ability to learn optimal control policies in systems where action space is defined by sentences in natural language would allow many interesting real-world applications such as automatic optimisation of dialogue systems. Text-based games with multiple endings and rewards are a promising platform for this task, since their feedback allows us to employ reinforcement learning techniques to jointly learn text representations and control policies. We argue that the key property of AI agents, especially in the text-games context, is their ability to generalise to previously unseen games. We present a minimalistic text-game playing agent, testing its generalisation and transfer learning performance and showing its ability to play multiple games at once. We also present pyfiction, an open-source library for universal access to different text games that could, together with our agent that implements its interface, serve as a baseline for future research.</p>
<p>I. INTRODUCTION</p>
<p>In text games, which are also known as Interactive Fiction (IF) [1], the player is given a description of the game state in natural language and then chooses one of the actions which are also given by textual descriptions. The executed action results in a change of the game state, producing new state description and waiting for the player's input again. This process repeats until the game is over.</p>
<p>The engine that is responsible for running IF games is called simply a game simulator and it provides an interface for player (human or bot) interaction.</p>
<p>While the output of the game simulator is almost always a text description, the form of the input that the game expects -the game-player interface -does vary. This is one of the most common criteria for classifying IF games (for examples, see Figure 1):</p>
<p>• parser-based, where the player types in any text input freely, • choice-based, where multiple actions to choose from are typically available, in addition to the state description, • hypertext-based, where multiple actions are present as clickable links inside the state description. The largest source of information on IF games and their variants is the Interactive Fiction Database 1 which features useful filters, tags and lists for finding specific kinds of games. For our purposes, though, the differences in game genre or storytelling elements largely do not matter. 1 IFDB: http://ifdb.tads.org/.</p>
<p>Text games come in virtually all genres, often with different minigames or puzzles incorporated into them. We mostly deal with games without any additional special parts that do not fit into the general and simple input-output loop or we omit them from the selected games.</p>
<p>In this work, we only deal with choice-based and hypertextbased games but both variants are referred to as choice-based, as the hyperlinks are simply considered additional choices. In fact, all parser-based games with finite number of actions can be converted to choice-based games by simply enumerating all the actions accepted by the interpreter at different time steps. This trick was employed by [3] where a subset of all possible action combinations was presented as choices to the agent.</p>
<p>More formally speaking, text games are sequential decision making tasks with both state and action spaces given in natural language. In fact, a text game can be seen as a dialogue between the game and the player and the task is to find an optimal policy (a mapping from states to actions) that would maximise the player's total reward. Consequently, the task of learning to play text games is very similar to learning how to correctly answer in a dialogue, making this task relevant for real-world applications.</p>
<p>Usually, text games have multiple endings and countless paths that the player can take. It is often clear that some of the paths or endings are better than others and different rewards can be assigned to them. Availability of these feedback signals makes text games an interesting platform for using reinforcement learning (RL), where one can make use of the feedback provided by the game in order to try and infer the optimal strategy of responding in the given game, and potentially, in a dialogue.</p>
<p>Our aim is to: 1) Provide a platform which would enable researchers to easily add new text games. Importantly, this library unifies the interface of different types of text games, enabling researchers to conduct large-scale experiments. 2) Design a minimalistic agent capable of playing basic text games well. This agent will also implement the interface of the text-game library and could be used as a starting point for more complex models in the future.</p>
<p>II. RELATED WORK</p>
<p>RL algorithms have a long history of being successfully applied to solving games, for example to Backgammon [4] or the more recent general AlphaZero algorithm that achieved superhuman performance in Chess and Go [5]. arXiv:1811.02872v2 [cs.AI] 12 Nov 2018 There have also been successful attempts [3], [2] at playing IF games using RL agents. However, the selected games were quite limited in terms of their difficulty and even more importantly, the resulting models had mostly not been tested on games that had not been seen during learning.</p>
<p>While being able to learn to play a text game is a success in itself, the resulting model must generalise to previously unseen data in order to be useful. In other words, we can merely hypothesise that a successful IF game agent can at least partly understand the underlying game state and potentially transfer the knowledge to other, previously unseen, games, or even natural dialogues. And for the most part, it remains to be seen how the RL agents presented in [3] and [2] perform in terms of generalisation in the domain of IF games.</p>
<p>Prior relevant work includes learning to play the game Civilization by leveraging text manuals [6] and achieving human-like performance on Atari video games [7].</p>
<p>As our agent is partly inspired by the architectures employed in [3] and [2], these models are next described in more detail.</p>
<p>The LSTM-DQN [3] agent uses an LSTM network [8] for representation generation, followed by a variant of a Deep Q-Network (DQN) [7] used for scoring the generated state and action vectors. The underlying task of playing parser-based games is effectively reduced to playing choice-based games by presenting a subset of all possible verb-object tuples as available actions to the agent.</p>
<p>In the framework, a single action consists of a verb and an object (e.g. eat apple) and the model computes Q-values for objects -Q(s, o) -and actions -Q(s, a) -separately. The final Q-value is obtained by averaging these two values.</p>
<p>The Deep Reinforcement Relevance Network (DRRN) [2] was introduced for playing hypertext-based games. The agent learns to play Saving John and Machine of Death games and used a simple bag-of-words (BOW) representation of the input texts.</p>
<p>The learning algorithm is also a variant of DQN; DRRN refers to the network that estimates the Q-value, using separate embeddings for states and actions. DRRN then uses a variable number of hidden layers and makes use of softmax actionselection. The final Q-value is obtained by computing an inner product of the inner representation vectors of states and actions.</p>
<p>III. BACKGROUND</p>
<p>We formally state the problem of learning to play the games as solving a Markov decision process. Then, we briefly review common machine learning methods that will be employed, most notably neural networks and Q-learning.</p>
<p>A. Formulating the learning task</p>
<p>Text game is a sequential decision-making task with both input and output spaces given in natural language. 
→ (S × 2 A ), • T is a transition function, T : (H × A) → H, • R is a reward function, R : (S t , A t , S t+1 ) → R.
Generally speaking, both the transition function T and the description function D may be stochastic and the properties of the game and its functions have a great impact on the task difficulty 2 .</p>
<p>In particular, if both D and T are deterministic, the whole game is deterministic. Consequently the problem is then reduced to a simple graph search problem that can be solved by graph search techniques and there is no need -from the perspective of finding optimal rewards -to employ reinforcement learning methods. However, from the generalisation perspective, it still might be useful to attempt to learn simple games using RL techniques as the agents could potentially be able to generalise or transfer their knowledge to other, more difficult problems.</p>
<p>We define the task of learning to play a text game as the process of finding an optimal policy for the respective text game Markov decision process (MDP) [10], where
M = S M , A M , P, R, γ M .
States, actions and the reward function S, A, R of the game correspond to the MDP states, actions and the reward function S M , A M , R. The MDP probability function P(s |s, a) is realised by the game transition function T and is crucially unknown to the agent.</p>
<p>Notice that not all IF games necessarily have the Markov property, i.e. in text games, there can be long-term dependencies. In fact, it is even difficult to determine whether a text game is or is not Markovian. However, even problems with non-Markovian characteristics are commonly represented and modelled as MDPs while still giving good results [10].</p>
<p>Following the common notation, the agent tries to maximise the discounted cumulative reward ∞ t=0 γ t R(s t , a t , s t+1 ) while following the policy π(s) by selecting actions a ← π(s). The goal is to learn the optimal policy π * by maximising the following term:
∞ t=0 γ t R(s t , a t , s t+1 ), where a t = π * (s t ).
(1)</p>
<p>Using this approach, we have now formally reduced the task of learning to play the text-based games to estimating an optimal policy in a text-game MDP. The Q-learning algorithm and its neural network estimator for solving the problem are described next.</p>
<p>B. Deep Reinforcement Learning</p>
<p>For finding the optimal policy in the text-game MDP, we employ Deep Reinforcement Learning (DRL) [7]. This term is commonly used to describe RL algorithms that use (deep) neural networks as a part of their value function estimation.</p>
<p>Reinforcement learning is an area of machine learning methods and problems based on the notion of learning from numerical rewards obtained by an agent through interacting with an environment [10].</p>
<p>In any given step, the agent observes a state of the environment and receives a reward signal. Based on the current state and the agent's behaviour function -the policy -the agent chooses an action to take. The action is then sent to the environment which is updated and the loop repeats. See Figure  2 for illustration of the agent-environment interface. There are different approaches to learning the optimal policy. Here, we focus on a method for model-free control that has been shown to perform well on a variety of gamerelated tasks [4], [7], Q-learning.</p>
<p>Q-learning makes use of the concept of a value function which evaluates how good a given state under a given policy is -what reward we can expect to obtain in the long run if we follow the policy from the specific state. Action value function that determines the value of taking action a in state s using policy π is defined as Q π (s, a) = E π [ ∞ t=0 γ t r t+1 | s, a]. Now the optimal policy, denoted π * , can be also characterised by the optimal action-value function Q * (s, a) = max π Q π (s, a). In other words, if we know the optimal action-value function Q * , we can obtain the optimal policy π * by simply choosing the actions with maximum Q-values: π * (s) = max a Q * (s, a).</p>
<p>Typically, the agent's policy is also -greedy with relation to the Q-function, where 0 ≤ ≤ 1 is a parameter that corresponds to the probability of choosing a random action.</p>
<p>Since Q-learning attempts to find the optimal Q-values which obey the Bellman equation [11], the update rule for improving the estimate of the Q-function is as follows [10]:
Q(s t , a t ) ← Q(s t , a t ) + α t · r t+1 + γ · max at+1 Q(s t+1 , a t+1 ) − Q(s t , a t ) ,(2)
where 0 ≤ α t ≤ 1 is the learning rate parameter. The Q-learning algorithm is guaranteed to converge towards the optimal solution [10]. In simpler problems and by default, it is assumed that the Q-values for all state-action pairs are stored in a table.</p>
<p>This approach is, however, not feasible for more complex problems such as the Atari platform games task [7] or our text-game task, where the state and action spaces are simply too large to store. In text games, for example, the spaces are infinite. We address this problem by approximating the optimal Q-function by a function approximator in the form of a neural network. The Q-function is parametrised as
Q * (s t , a t ) ≈ Q(s t , a t , θ t ) = θ t (s t , a t ),(3)
where the θ function is realised by a neural network. The advantage of this non-tabular approach is that even in infinite spaces, the neural network can generalise to previously unobserved inputs and consequently cover the whole search space with reasonable accuracy. In contrast to linear function approximators, though, non-linear approximators such as neural networks do not guarantee convergence in this context.</p>
<p>IV. GOALS</p>
<p>Our goal is to introduce a minimalistic architecture serving as a proof of concept, with the ability to capture important sentence-level features and ideally capable of reasonable generalisation to previously unseen data. First, we highlight some of the aspects of the LSTM-DQN and DRRN models that could be improved upon in terms of these requirements.</p>
<p>The main drawback of DRRN is its use of BOW for representation generation. Consequently, the model is incapable of properly handling state aliasing and differentiating simple yet important nuances in the input, such as the difference between "There is a treasure chest to your left and a dragon to your right." and "There is a treasure chest to your right and a dragon to your left.".</p>
<p>Moreover, He et al. [2] claim that separate embeddings for state and action spaces lead to faster convergence and to a better solution. However, since both state and action spaces really contain the same data -at least in most games and especially in hypertext games where actions are a subset of states -we aim to employ a joint embedding representation of states and actions.</p>
<p>We also believe that a joint representation of states and actions should eventually lead to stronger generalisation capabilities of the model, since such model should be able to transfer knowledge between state and action descriptions as their representation would be shared.</p>
<p>The LSTM-DQN agent, on the other hand, utilises an LSTM network that can theoretically capture more complex sentence properties such as the word order. However, its architecture only accepts actions consisting of two words.</p>
<p>Additionally, the two action Q-values are finally averaged, which would arguably be problematic if the verbs and objects were not independent. For example, the value of the verb "drink" varies highly based on the object; consider the difference between the values of "drink" when followed by either "water" or "poison" objects.</p>
<p>We thus aim to utilise a minimalistic architecture that should:</p>
<p>• be able to capture dependencies on sentence level such as word order, • accept text input of any length for both states and action descriptions, • accept and evaluate any number of actions, • use a powerful interaction function between states and actions.</p>
<p>V. METHODS</p>
<p>We present the pyfiction platform and specify the relevant learning tasks. Then we describe the architecture of our agent capable of learning the games, leveraging the general game interface of the platform. Finally, we describe our agent architecture in detail.</p>
<p>A. Platform</p>
<p>The pyfiction 3 platform is a library for universal access to different kinds of IF games. Its interface is identical to the general RL interface (see Figure 2). Currently, it supports the Glulxe, Z-machine and general HTML simulators.</p>
<p>There are eight games present as of now, however, adding new games is straightforward. Apart from the game files, it is only necessary to provide a mapping from states to actions and to numerical rewards for the game to be playable by AI agents.</p>
<p>Any agent compatible with the simple RL interface can directly play all games present in pyfiction. Pyfiction can also integrate to OpenAI Gym [12].</p>
<p>B. The SSAQN architecture</p>
<p>Our neural network model is inspired by both LSTM-DQN and DRRN. For the sake of clarity, it is referred to as SSAQN (Siamese State-Action Q-Network).</p>
<p>Similarly to [3] and [2], we employ a variant of DQN [7] with experience replay and prioritised sampling which uses a neural network (SSAQN) for estimating the DQN's Q-function.</p>
<p>SSAQN uses a siamese network architecture [13], where two branches -a state branch and an action branch -3 pyfiction: https://github.com/MikulasZelinka/pyfiction. share most of the layers that effectively generate useful representation features. This is best illustrated by visualising the network's computational graph; see Figure 3.</p>
<p>As we are using a siamese architecture, the weights of the embedding and LSTM layers are shared between state and action data passing through. States and actions are only differentiated in the dense layers whose outputs are then fed into the similarity interaction function.</p>
<p>The most important differences to LSTM-DQN and DRRN are:</p>
<p>• the network accepts two text descriptions (state and action) of arbitrary length as an input, • the embedding and LSTM layers are the same for states and action, i.e. their weights are shared, • the interaction function of inner vectors of states and actions is realised by a normalised dot product, commonly called cosine similarity (see Section V-B4).</p>
<p>The output of the SSAQN is the estimated Q-value for the given state-action pair, i.e. the network with parameters θ realises a function θ(s, a) = Q(s, a, θ) ≈ Q * (s, a) where the input variables s and a contain preprocessed text 4 .</p>
<p>To compute the Q(s, a i ) for different i -for multiple actions -we simply run the forward pass of the action branch multiple times.</p>
<p>Next, the SSAQN architecture is described layer-by-layer in more detail.</p>
<p>1) Word embeddings: After preprocessing the text, we convert the words to their vector representation using word embeddings [14]. Since our dataset is comparatively small, we use a relatively low dimensionality for the word representations and work with embedding_dim of 16. The weights of the word embeddings are initialised to small random values and trained as a part of the gradient descent algorithm. Using pre-trained vector models is also supported.</p>
<p>As the training is done in batches, we pad the input sequences of words for each batch so that they are all aligned to the same length. Still, note that in general, the length of the output of the embedding layer is variable in length.</p>
<p>2) LSTM layer: The inputs of the LSTM layer are the word embedding vectors of variable length. Similarly to embeddings, the weights of the LSTM units are also initialised to small random values and their weights are shared between states and actions.</p>
<p>The role of the LSTM layer is to successively look at the input words, changing its inner state in the process and thus detecting time-based features in its input. It is also at this layer that we go from having a data shape of arbitrary length to having a fixed-length output vector. The output size is equal to the number of LSTM units in the layer and in our experiments, we use lstm_dim of 32.</p>
<p>3) Dense layer: Following the shared LSTM layer, we now have two dense (fully-connected) layers, one for states and one for actions. Again, we initialise the weights randomly and we also apply the hyperbolic tangent activation function tanh(x) = 1−e −2x 1+e −2x . As the dense layers for states and actions are the only layers to not necessarily share weights between the two network branches, they do play an important role in building differentiated representations for state and action input data.</p>
<p>Note that as the interaction layer uses a dot product, we require both outputs of the dense layers to be of the same dimension and we set dense_dim of both branches to 8. However, theoretically, it would be both possible and interesting to use different layer dimensions for states and actions at this level, as usually, the original state text descriptions carry more information than action descriptions in IF games. Thus, a possible extension of the network would be to use two or more hidden dense layers in the state branch of the network and to only reduce the dimension to the action dimension in the last hidden dense state layer. 4) Interaction function: Lastly, we apply the cosine similarity interaction function to the state and action dense activations, resulting in the final Q-value. If the input are two vectors x and y of dense_dim = n elements, we define their cosine similarity as a dot product of their L2-normalised (and consequently unit-sized) equivalents:
cs(x, y) = x · y x 2 y 2 = n i=1 x i y i n i=1 x 2 i n i=1 y 2 i ,(4)
which corresponds to the cosine of the angle between the input vectors.</p>
<p>Cosine similarity is commonly used for determining document similarity [15]. Here, we apply it to the two hidden vectors of dense layer values that should meaningfully represent the condensed information that was originally received as a text input by the network and we interpret the resulting value as an indicator of mutual compatibility of the original stateaction pair. Since the range of values of the cos function is [−1, 1] and since the original rewards that we aim to estimate have arbitrary values, we scale the approximated Q-values accordingly.</p>
<p>5) Loss function and gradient descent:</p>
<p>Recall the Qlearning rule (see equation 2). We define the loss function at time t as
L t = r t + γ · max a Q(s t+1 , a) − Q(s t , a t ) 2 ,(5)
which is simply a mean squared error (MSE) of the last estimated Q-value and the target Q-value. For gradient descent, we make use of the RMSProp optimiser [16] that has been shown to perform well in numerous practical applications, especially when applied in LSTM networks [17].</p>
<p>C. Action selection</p>
<p>Given an SSAQN θ, where Q(s, a) ← θ(s, a), the agent selects an action by following the -greedy policy π (s) [10] realised by the following algorithm:</p>
<p>The is the exploration parameter. For sampling in training phase (see Algorithm 2), is gradually decayed from the starting value of 1, i.e. at first, the agent's policy is completely random. In testing phase, the agent is greedy, i.e. is set to 0 and the agent always chooses the action with the maximum Q-value for the given state.</p>
<p>The only important difference between the standardgreedy control algorithm and our action selection policy is that we additionally employ a history function, h(s, a). The scope of the history function is a single run of the agent on a single game, i.e. it is reset every time a game ends.</p>
<p>The function h(s, a) returns a value equal to the number of times the agent selected action a in state s in the current run. That is, if the agent never selects an action twice in the same state during a run, the history function has no impact on The history function serves as a very simple form of intrinsic motivation [18]. It is similar to optimistic initialisation [10] in that it leads the agent to select previously unexplored state-action tuples. Additionally, note that the history function is not Markovian in the sense that it takes the whole game episode into account. In practice, the history function greatly helps the agent to avoid infinite loops, since for many games, it is likely to get stuck in an infinite loop when following a randomly chosen deterministic Markovian policy.</p>
<p>D. Training loop</p>
<p>Putting together all the parts introduced above, we can now formally describe the agent's learning algorithm.</p>
<p>We use a variant of DQN [7] with experience replay and prioritised sampling of experience tuples with positive rewards [19]. For more details, see Algorithm 2. Note that the agent inherently supports playing and learning multiple games at once.</p>
<p>VI. EXPERIMENTS</p>
<p>We conduct experiments on six games, Saving John (SJ), Machine of Death (MoD), Cat Simulator 2016 (CS), Star Court (SC), The Red Hair (TRH) and Transit (TT). All of these games are available as a part of pyfiction. Since SJ and MoD are also present in [2], results on these two games for both agents are directly comparable.</p>
<p>More details about the games including the annotated endings, are available in the pyfiction repository. For basic statistics, see Table I.</p>
<p>A. Setup</p>
<p>In all experiments, we use the following SSAQN layer dimensions: Embedding: 16, LSTM: 32, Dense: 8. The RMS-Prop optimiser is used with learning rate between 0.001 and 0.00001, batch size of 256 with the prioritised fraction of 25%, γ of 0.95, of 1 and -decay of 0.99. We run each experiment five times.</p>
<p>The different testing scenarios are as follows. Initialise the neural network θ with random weights 4: Initialise all game simulators and load the vocabulary 5: for e ∈ 0, . . . , episodes − 1 do 6:</p>
<p>Sample each game once using π , store experiences into D 7: batch ← b tuples (s t , a t , r t , s t+1 , a t+1 ) from D, where a fraction of p have r t &gt; 0 8:
for i, (s i t , a i t , r i t , s i t+1 , a i t+1 ) in batch do 9: target i ← r i t 10:
if a i t+1 then s i t+1 is not terminal 11: Define loss as L e (θ) ← (target i − θ(s i t , a i t )) 2
target i += γ · max a i t+1 θ(s i t+1 , a i</p>
<p>15:</p>
<p>Perform gradient descent on L e (θ) 16: ← · decay 17: end for 18: end function 1) Single-game task: The agent is trained and evaluated on the same game.</p>
<p>2) Transfer learning: We pre-train the agent on five games except the one it is then trained and evaluated on.</p>
<p>3) Generalisation: Same as transfer learning, except the agent is not trained on the final game but only evaluated on the game instead. 4) Playing multiple games at once: We train and evaluate a single instance of the agent on all six games at once. In each training step, all games are presented successively to the agent.</p>
<p>In all of the scenarios, the agents are mainly compared to the random agent baseline. For SJ and MoD, we can also compare the results with the DRRN agent as well as the baselines given in [2].</p>
<p>B. Results</p>
<p>Final rewards from all tasks can be seen in Table I and the learning progress is depicted in Figure 5 for most of the tasks.</p>
<p>In the single-game task, the SSAQN agent learns to play all deterministic games optimally. MoD is not learned optimally, but the agent outperforms the DRRN agent (see Figure 4). The agent doesn't significantly outperform the random baseline on SC, however, it is unknown if it is possible to do so.</p>
<p>Transfer learning resulted in a slower convergence rate but similar results on deterministic games and slightly worse results on MoD and SC. The agent unfortunately but unsurprisingly didn't generalise to unseen games in the generalisation   task and a much larger scale of experiments would be needed to safely conclude if it's capable of doing so. We attribute it mostly to the lack of relevant words and formulations between the games. Table I also depicts how many words from a given game are also present in other games.</p>
<p>Even if this percentage is comparatively high, note that it is just a statistic of single words, meaning it is very unlikely that a word phrases, which are actually important, match between the games. At any rate, more experiments in different scales are needed to verify this hypothesis.</p>
<p>In the multiple-games tasks, the agent also learned deterministic games optimally, this time however, it was able to also play MoD in an almost optimal way, significantly outperforming DRRN. Curiously, the result on MoD is better than in the single-game setting, suggesting that information learned from other games might have been useful.</p>
<p>VII. CONCLUSION AND FUTURE WORK</p>
<p>We presented a minimalistic text-game playing agent capable of learning to play multiple games at once. The agent uses a twin-like SSAQN architecture and outperforms the previously suggested DRRN architecture while also being considerably simpler.</p>
<p>We also test the transfer learning and generalisation capabilities of the agent, concluding that it unfortunately doesn't transfer its knowledge or generalise in the limited scope of six games.</p>
<p>To this end, however, we present pyfiction, an easily extensible library for universal access to various text games that could, together with our agent, serve as a baseline for future research.</p>
<p>Future work should mainly focus on expanding the textgame domain and on conducting experiments at a much larger scale. This should be made much easier thanks to the presented library and we hypothesise that events agents as simple as the SSAQN agent should show some generalisation capabilities given enough data.</p>
<p>ACKNOWLEDGEMENT</p>
<p>The author would like to thank Rudolf Kadlec for his ideas and guidance during author's work on this topic as a part of Master studies, and to Martin Pilát for helpful suggestions and comments. </p>
<p>Fig. 1 :
1Parser, choice and hypertext-based games[2].</p>
<p>Definition 1 (
1Text game). Let us define a text game as a tuple G = H, H t , S, A, D, T , R , where • H is a set of game states, H t is a set of terminating game states, H t ⊆ H, • S is a set of possible state descriptions, • A is a set of possible action descriptions, • D is a function generating text descriptions, D : H</p>
<p>Fig. 2 :
2Interaction between the agent and the environment[10].</p>
<p>Fig. 3 :
3Architecture of the SSAQN model and its data flow. Grey boxes represent data values in different layers; the bold text corresponds to the shape of the data tensors.</p>
<p>a i with max q i 9: end function action selection -and it penalises the already visited stateaction pairs, as seen on line 6 of Algorithm 1.</p>
<p>Fig. 4 :
4Comparison of the DRRN and the SSAQN agent Saving John (left) and Machine of Death (right). SSAQN converges considerably faster on both games and achieves higher performance but it has higher variance on MoD.</p>
<p>Fig. 5 :
5Results of the SSAQN agent on all six games. Blue: random agent. Orange: train and evaluate on the same game X. Green: pre-train on all but X, then train and evaluate on X. Purple: train and evaluate on all games at once. Standard deviation is shown for the random agent and random games.</p>
<p>Saving John Machine of Death Cat Simulator 2016 Star Court The Red Hair TransitPROPERTIES </p>
<h1>tokens</h1>
<p>1119 
2055 
364 
3929 
155 
575 </p>
<h1>states</h1>
<p>70 
≥ 200 
37 
≥ 420 
18 
76 </p>
<h1>endings</h1>
<p>5 
≥ 14 
8 
≥ 17 
7 
10 
Avg. words/description 
73.9 
71.9 
74.4 
66.7 
28.7 
87.0 
Deterministic transitions 
Yes 
No 
Yes 
No 
Yes 
Yes 
Deterministic descriptions 
Yes 
Yes 
Yes 
No 
Yes 
Yes 
% of tokens present in other games 
68.4 
56.0 
79.4 
33.7 
92.3 
72.7 </p>
<p>FINAL REWARD </p>
<p>Random agent (average) 
-8.6 
-10.8 
-0.6 
-11.6 
-11.4 
-10.1 
Individual game 
19.4 
15.4 
19.4 
-2.2 
19.3 
19.5 
Generalisation 
-11.2 
-15.1 
5.7 
-13.2 
-10.0 
-10.2 
Transfer learning 
19.4 
8.7 
19.4 
-13.3 
19.3 
19.5 
Multiple games 
19.4 
21.0 
19.4 
-8.2 
19.3 
19.5 
Optimal 
19.4 
≈ 21.4 
19.4 
? 
19.3 
19.5 </p>
<p>TABLE I :
ISummary of game statistics and performance comparison of the SSAQN agent on different tasks.(a) DRRN on SJ [2] 
(b) DRRN on MoD [2] </p>
<p>For a more thorough discussion, please refer to[9].
For more details, see the pyfiction library.
This research was supported by Charles University under SVV (project number 260 453) and by the Czech Science Foundation (project number 17-17125Y).
Twisty Little Passages: An Approach to Interactive Fiction. N Montfort, MIT PressCambridge, MA, USAN. Montfort, Twisty Little Passages: An Approach to Interactive Fiction. Cambridge, MA, USA: MIT Press, 2005.</p>
<p>Deep Reinforcement Learning with an Unbounded Action Space. J He, J Chen, X He, J Gao, L Li, L Deng, M Ostendorf, abs/1511.04636CoRR. J. He, J. Chen, X. He, J. Gao, L. Li, L. Deng, and M. Ostendorf, "Deep Reinforcement Learning with an Unbounded Action Space," CoRR, vol. abs/1511.04636, 2015. [Online]. Available: http://arxiv.org/ abs/1511.04636</p>
<p>Language Understanding for Text-based Games Using Deep Reinforcement Learning. K Narasimhan, T D Kulkarni, R Barzilay, abs/1506.08941CoRR. K. Narasimhan, T. D. Kulkarni, and R. Barzilay, "Language Understanding for Text-based Games Using Deep Reinforcement Learning," CoRR, vol. abs/1506.08941, 2015. [Online]. Available: http://arxiv.org/abs/1506.08941</p>
<p>Temporal difference learning and td-gammon. G Tesauro, Commun. ACM. 383G. Tesauro, "Temporal difference learning and td-gammon," Commun. ACM, vol. 38, no. 3, pp. 58-68, 1995.</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. D Silver, T Hubert, J Schrittwieser, I Antonoglou, M Lai, A Guez, M Lanctot, L Sifre, D Kumaran, T Graepel, T P Lillicrap, K Simonyan, D Hassabis, abs/1712.01815CoRR. D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, T. P. Lillicrap, K. Si- monyan, and D. Hassabis, "Mastering chess and shogi by self-play with a general reinforcement learning algorithm," CoRR, vol. abs/1712.01815, 2017.</p>
<p>Learning to Win by Reading Manuals in a Monte-Carlo Framework. S R K Branavan, D Silver, R Barzilay, abs/1401.5390CoRR. S. R. K. Branavan, D. Silver, and R. Barzilay, "Learning to Win by Reading Manuals in a Monte-Carlo Framework," CoRR, vol. abs/1401.5390, 2014. [Online]. Available: http://arxiv.org/abs/1401.5390</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M A Riedmiller, A Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, 10.1038/nature14236Nature. 5187540V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. A. Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, "Human-level control through deep reinforcement learning," Nature, vol. 518, no. 7540, pp. 529-533, 2015. [Online]. Available: https://doi.org/10.1038/nature14236</p>
<p>Untersuchungen zu dynamischen neuronalen netzen. S Hochreiter, Diploma, Technische Universität München. 91S. Hochreiter, "Untersuchungen zu dynamischen neuronalen netzen," Diploma, Technische Universität München, vol. 91, 1991.</p>
<p>Using reinforcement learning to learn how to play textbased games. M Zelinka, abs/1801.01999CoRR. M. Zelinka, "Using reinforcement learning to learn how to play text- based games," CoRR, vol. abs/1801.01999, 2018. [Online]. Available: http://arxiv.org/abs/1801.01999</p>
<p>Reinforcement Learning -An Introduction, ser. Adaptive computation and machine learning. R S Sutton, A G Barto, MIT PressR. S. Sutton and A. G. Barto, Reinforcement Learning -An Introduction, ser. Adaptive computation and machine learning. MIT Press, 1998. [Online]. Available: http://www.worldcat.org/oclc/37293240</p>
<p>Dynamic programming. Courier Corporation. R Bellman, R. Bellman, Dynamic programming. Courier Corporation, 2013.</p>
<p>OpenAI Gym. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, abs/1606.01540CoRR. G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, "OpenAI Gym," CoRR, vol. abs/1606.01540, 2016. [Online]. Available: http://arxiv.org/abs/1606.01540</p>
<p>Signature verification using a siamese time delay neural network. J Bromley, I Guyon, Y Lecun, E Säckinger, R Shah, Advances in Neural Information Processing Systems 6, [7th NIPS Conference, Denver. J. D. Cowan, G. Tesauro, and J. AlspectorColorado, USAMorgan KaufmannJ. Bromley, I. Guyon, Y. LeCun, E. Säckinger, and R. Shah, "Signature verification using a siamese time delay neural network," in Advances in Neural Information Processing Systems 6, [7th NIPS Conference, Den- ver, Colorado, USA, 1993], J. D. Cowan, G. Tesauro, and J. Alspector, Eds. Morgan Kaufmann, 1993, pp. 737-744.</p>
<p>Distributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in Neural Information Processing Systems. C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. WeinbergerLake Tahoe, Nevada, United States26T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, "Distributed representations of words and phrases and their composi- tionality," in Advances in Neural Information Processing Systems 26, 2013, Lake Tahoe, Nevada, United States., C. J. C. Burges, L. Bottou, Z. Ghahramani, and K. Q. Weinberger, Eds., 2013, pp. 3111-3119.</p>
<p>Similarity measures for text document clustering. A Huang, Proceedings of the sixth new zealand computer science research student conference. the sixth new zealand computer science research student conferenceChristchurch, New ZealandA. Huang, "Similarity measures for text document clustering," in Pro- ceedings of the sixth new zealand computer science research student conference, Christchurch, New Zealand, 2008, pp. 49-56.</p>
<p>Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. T Tieleman, G Hinton, 4COURSERA: Neural networks for machine learningT. Tieleman and G. Hinton, "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude," COURSERA: Neural networks for machine learning, vol. 4, no. 2, pp. 26-31, 2012.</p>
<p>RMSProp and equilibrated adaptive learning rates for non-convex optimization. Y N Dauphin, H Vries, J Chung, Y Bengio, abs/1502.04390CoRR. Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio, "RMSProp and equilibrated adaptive learning rates for non-convex optimization," CoRR, vol. abs/1502.04390, 2015. [Online]. Available: http://arxiv.org/ abs/1502.04390</p>
<p>Intrinsically motivated reinforcement learning. S P Singh, A G Barto, N Chentanez, Advances in Neural Information Processing Systems. Vancouver, British Columbia, Canada17S. P. Singh, A. G. Barto, and N. Chentanez, "Intrinsically motivated reinforcement learning," in Advances in Neural Information Processing Systems 17, 2004, Vancouver, British Columbia, Canada], 2004, pp. 1281-1288.</p>
<p>Prioritized Experience Replay. T Schaul, J Quan, I Antonoglou, D Silver, abs/1511.05952CoRR. T. Schaul, J. Quan, I. Antonoglou, and D. Silver, "Prioritized Experience Replay," CoRR, vol. abs/1511.05952, 2015. [Online].</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5013 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5013</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5013</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-269330143</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.15522v2.pdf" target="_blank">LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really"reason"over the natural language? This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic. Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics. To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule. We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting. Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations. Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion. We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs. Data and code are available at https://github.com/Mihir3009/LogicBench.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5013.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5013.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model from OpenAI evaluated in this paper using chain-of-thought prompting on a new logical-reasoning benchmark (LogicBench); shows the strongest performance among tested models but still exhibits systematic logical failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source large pretrained transformer language model by OpenAI. Evaluated via API in zero-shot chain-of-thought and few-shot settings in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicBench (BQA & MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>LogicBench: a newly introduced natural-language QA dataset designed to evaluate single-inference-rule logical reasoning across 25 reasoning patterns spanning propositional logic (PL), first-order logic (FOL), and non-monotonic (NM) reasoning. Two tasks: Binary QA (BQA: yes/no entailment) and Multiple-Choice QA (MCQA: pick correct conclusion among four).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot chain-of-thought prompting (three distinct CoT prompts averaged), plus some few-shot experiments; manual analysis of generated reasoning chains. Also compared to models fine-tuned on LogicBench(Aug) (LogicT5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On LogicBench BQA: average accuracy for positive entailment (A(Yes)) = 63.98% (average across inference rules and three CoT prompts); accuracy on negative label (A(No)) substantially higher (reported ~86.4%). On LogicBench MCQA: overall accuracy ~91.5% (average across rules). Qualitative findings: best performing model among those tested but still fails many single-rule instances, especially those involving negation and longer inference templates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles when premises include negations (Modus Tollens, Destructive Dilemma, etc.), with longer/more complex inference rules (CD, BD, DD), and occasionally overlooks or hallucinates contextual information necessary for correct logical derivation; performance on single-step logical entailment (BQA) remains below human performance (~85% human accuracy on subset), particularly for positive-entailment questions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms smaller models tested (ChatGPT / GPT-3.5, Llama-2-7B, Mistral-7B, Yi-34B, etc.) on both BQA and MCQA. Larger model capacity correlates with higher logical-reasoning performance; GPT-4 tops MCQA and has highest A(Yes) among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Analyses in paper show (1) model-size effect (larger models do better), (2) sensitivity to negation and rule length, (3) chain-of-thought prompt variability (results averaged across three CoT prompts), and (4) few-shot behavior: some improvements in particular rules/tasks; detailed error analysis of reasoning chains identifies ~1% of chains where model introduces unrelated facts (hallucination).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5013.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5013.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-3.5-Turbo family (ChatGPT) evaluated zero-shot with chain-of-thought prompting on LogicBench; shows moderate performance but substantially weaker than GPT-4, especially on positive entailments (A(Yes)).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI conversational model (GPT-3.5 family) accessed via API and evaluated in zero-shot CoT and some few-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicBench (BQA & MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same LogicBench BQA (yes/no entailment) and MCQA (4-choice conclusion selection) covering PL, FOL, and NM single-inference-rule problems.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot chain-of-thought prompting (three CoT prompts averaged); few-shot experiments reported in appendix for selected cases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>BQA: average A(Yes) ≈ 48.04% (models perform noticeably worse on positive entailments than on negative); MCQA: overall accuracy ≈ 82.4% (averaged across rules). Performance is uneven across inference rules and poor on negation-heavy rules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Poorer ability to draw correct positive entailments (lower A(Yes)), major failures on rules containing negation and longer rule templates; sometimes ignores relevant contextual sentences or hallucinates. Overall BQA performance below human annotators (~85%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms GPT-4 substantially on both BQA and MCQA; outperforms smaller open models in some MCQA settings but is weaker on many PL inference rules.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Paper reports that few-shot exemplars help some models more than others (e.g., Llama-2 benefits strongly), and that ChatGPT's performance improves in NM reasoning with in-context exemplars; average across three CoT prompts was reported to reduce sensitivity to prompt phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5013.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5013.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's Gemini-Pro model evaluated zero-shot with chain-of-thought prompting on LogicBench; shows strong MCQA performance but mixed BQA results and specific weaknesses similar to other LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-Pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google's Gemini-Pro large language model evaluated via API in this study using CoT prompting (zero-shot and few-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicBench (BQA & MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluates single-rule logical entailment (PL, FOL, NM) in binary and multiple-choice formats.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot chain-of-thought prompting (three distinct CoT prompts averaged); few-shot experiments included for analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MCQA: high overall accuracy (reported among the better-performing models on MCQA; e.g., MCQA average ~88.7% in the reported results). BQA: shows mixed results across inference rules; lower A(Yes) than top model(s) and struggles with negations/long rules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Similar failure modes to other LLMs: trouble with negated premises, longer inference templates, and occasional omission of contextual constraints. Performance gaps between MCQA and BQA suggest reliance on option elimination strategies in MCQA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Performs comparably to GPT-4 on some MCQA categories but generally behind GPT-4 in overall robustness on BQA; outperforms smaller models (7B class) in MCQA.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Paper observes model-size trends and prompt sensitivity; Gemini and GPT-4 show improvements in some logic types under few-shot prompting, indicating sensitivity to in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5013.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5013.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-7B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source 7B-parameter Llama-2 chat model evaluated with chain-of-thought prompting; performs substantially worse than larger models on BQA but benefits strongly from few-shot in-context examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-7B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Meta's Llama-2 family, 7-billion-parameter chat model variant used from Hugging Face for zero-shot and few-shot evaluation in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicBench (BQA & MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluated on LogicBench's single-inference-rule BQA and MCQA covering PL, FOL, and NM logic.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot chain-of-thought prompting (three CoT prompts averaged); few-shot experiments showed substantial gains by providing in-context exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MCQA: moderate accuracy (~57.9% overall). BQA: notably lower A(Yes) (paper reports markedly worse performance than larger models; quantitative A(Yes) substantially below GPT-4/ChatGPT), often failing many PL rules.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggles with positive entailments, negations, and longer inference rules; sensitive to prompt and benefits greatly from few-shot exemplars (i.e., poor zero-shot generalization but strong in-context learning).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Underperforms larger closed models (GPT-4, Gemini) on both tasks but shows the largest relative improvement when shifted from zero-shot to few-shot among tested models, indicating strong in-context imitation abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Few-shot (in-context examples) experiments show that Llama-2 consistently and markedly improves A(Yes) compared to zero-shot, implying that exemplar format/prompting is a key factor for smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5013.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5013.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-Instruct-v0.2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B instruct-tuned model evaluated on LogicBench; shows uneven performance and in some cases degraded behavior compared to other 7B models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct-v0.2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mistral's 7B instruct-tuned model used from Hugging Face, evaluated in zero-shot CoT and few-shot experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicBench (BQA & MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same LogicBench BQA and MCQA assessments across 25 inference patterns in PL, FOL, NM.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot chain-of-thought prompting; few-shot analysis included.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>MCQA: moderate performance (reported around mid-50s to high-50s % accuracy in MCQA averaged across rules in tables); BQA: variable and often degraded compared to other models — Mistral 'consistently shows degraded performance' on some logic types per the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Consistent degradation observed across many logic types; fails many negation and longer-rule cases; less responsive to few-shot exemplars relative to Llama-2 and some larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Per-paper Mistral performed worse than Gemini, ChatGPT, and GPT-4 in many categories; relative performance was mixed versus Llama-2 depending on the task and few-shot setup.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Paper notes consistent degraded performance for Mistral across logic types and that few-shot does not rescue it as strongly as for some other models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5013.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5013.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Yi-34B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Yi-34B-chat (Yi 34B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An intermediate-size (34B) chat model tested as an additional datapoint; shows improvement over 7B models but still below GPT-4, illustrating a positive correlation between size and logical-reasoning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Yi-34B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open foundation model (Yi) 34B chat variant evaluated in the Appendix/analysis as an intermediate-size model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>34B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>LogicBench (BQA & MCQA)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same LogicBench BQA and MCQA tasks; Yi-34B used to probe model-size scaling trends on logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Zero-shot CoT prompting (analysis reported in App.I); performance compared against 7B and larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported to improve over Llama-2-7B by a substantial margin across logic types; still outperformed by GPT-4. Exact aggregated numbers are reported in appendix tables (improvement trend rather than claiming top accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Retains many failure modes seen in other models: difficulty with negation, longer rules, and some NM cases. Noted to be better than 7B-class models but not state-of-the-art.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Better than 7B models (e.g., Llama-2-7B) on average; worse than GPT-4. Supports the paper's conclusion that increasing model size tends to improve logical reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Used to support size-vs-performance analysis; adding the 34B point showed monotonic improvement over 7B but diminishing returns compared to GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5013.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5013.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogicT5 (T5-large fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogicT5 (T5-large fine-tuned on LogicBench(Aug))</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A T5-large model fine-tuned on the synthetically augmented LogicBench(Aug) (created by the authors) to improve logical reasoning; used to demonstrate that training on LogicBench improves performance on logic datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large (LogicT5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>T5-large (encoder-decoder transformer) fine-tuned on a synthetically augmented version of LogicBench (LogicBench(Aug)) to create LogicT5 for evaluating the effect of supervised training on logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5-large (~700M-1B class, exact parameter count not specified in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Trained on LogicBench(Aug) and evaluated on LogicBench(Eval), plus transfer evaluations on other logic datasets (LogicNLI, FOLIO, LogiQA, ReClor).</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Fine-tuned to perform BQA/MCQA style logical entailment tasks derived from single-inference-rule narratives across PL, FOL, and NM patterns; tested for transfer to existing logic benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Supervised fine-tuning on LogicBench(Aug) synthetic data (150 unique samples per rule, resulting in an augmented training set); training hyperparameters: 10 epochs, batch size 16, max input length 1024, adaptive LR 5e-05, AdamW optimizer (on NVIDIA RTX A6000 GPUs).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Fine-tuning T5-large on LogicBench(Aug) (LogicT5) produced improvements in logical reasoning ability: reported average improvements of ≈2% on other logic datasets (LogicNLI and FOLIO) and competitive results on LogiQA and ReClor compared to baseline T5-large. Exact per-task numbers and tables are in App.I / main text summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Training on single-inference-rule, single-step data; paper notes scope limitations (no multi-step / multi-rule reasoning covered). Improvements are modest (~2% average on some downstream logic datasets) and do not fully close the gap to top LLMs; LogicT5 remains sensitive to the diversity and coverage of logic patterns in the augmented training set.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>LogicT5 (fine-tuned on LogicBench(Aug)) outperforms baseline T5-large on logic-focused datasets (notably LogicNLI and FOLIO). However, on more mixed-reasoning datasets (LogiQA, ReClor) the gains are modest; closed large LLMs (GPT-4) still outperform a fine-tuned T5 in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Paper reports transfer and multi-task fine-tuning experiments (single-task vs multi-task) showing stronger gains when LogicBench-like data is included; ablation on dataset composition and amount is discussed qualitatively and in App.I (showing that logic-focused augmentation helps FOLIO/LogicNLI more than mixed reasoning datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5013.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5013.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LogicBench (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LogicBench (evaluation and augmented variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A new natural-language dataset introduced in this paper to systematically evaluate single-inference-rule logical reasoning abilities of LLMs across 25 inference rules spanning propositional, first-order, and non-monotonic logics; supports BQA and MCQA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LogicBench (dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Dataset consisting of LogicBench(Eval) (binary QA: 1520 BQA instances (520 yes, 1000 no) and 500 MCQA instances) and a synthetic augmentation LogicBench(Aug) (12908 total samples with variations) covering 25 logical inference patterns; contexts are narrative natural-language renditions of single logical templates.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>Binary QA (BQA) and Multiple-Choice QA (MCQA) for single-inference-rule logical entailment across PL, FOL, NM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>BQA: determine whether a conclusion is logically entailed by a narrative context (yes/no). MCQA: select correct conclusion from four options. Each instance follows exactly one inference rule to allow fine-grained evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Data created by a three-stage pipeline using GPT-3.5 for sentence generation and narrative conversion (templatized-to-narrative) followed by automatic generation of question/option variations and manual validation to remove leakage/hallucinated conclusions; evaluated via zero-shot CoT and few-shot prompting across multiple LLMs and via fine-tuning experiments with T5-large.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Used as the primary benchmark in the paper: human annotators achieve ≈85% accuracy on a sampled subset (BQA/MCQA), while evaluated LLMs show substantially lower performance on BQA (A(Yes) often < 64% even for GPT-4) and mixed but higher performance on MCQA (GPT-4 ~91.5%, Gemini and ChatGPT high but lower than GPT-4).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Covers only single-inference-rule, single-step reasoning; does not include multi-step combinations of inference rules. Limited to English. Some initial synthetic generation errors required manual correction (identified leakage of conclusions in ~15% of a 500-sample subset and missing premises in ~8% before manual mitigation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Compared in the paper to existing logic datasets (ProofWriter, FOLIO, ProntoQA, LogicNLI, LogiQA, ReClor) and claimed to cover more inference rules (25) and include non-monotonic reasoning, which many prior benchmarks omit.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Analyses using LogicBench included: per-inference-rule performance breakdowns, label-wise BQA (A(Yes)/A(No)), effect of negation and rule length on accuracy, human vs model comparison, few-shot vs zero-shot impacts, and transfer/fine-tuning experiments with T5-large showing utility of LogicBench(Aug) for improving downstream logical-reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>FOLIO: Natural language reasoning with first-order logic <em>(Rating: 2)</em></li>
                <li>ProntoQA: Language models are greedy reasoners: A systematic formal analysis of chain-of-thought <em>(Rating: 2)</em></li>
                <li>LogicNLI: Diagnosing the first-order logical reasoning ability through LogicNLI <em>(Rating: 2)</em></li>
                <li>LogiQA: a challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 1)</em></li>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5013",
    "paper_id": "paper-269330143",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A state-of-the-art large language model from OpenAI evaluated in this paper using chain-of-thought prompting on a new logical-reasoning benchmark (LogicBench); shows the strongest performance among tested models but still exhibits systematic logical failure modes.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Closed-source large pretrained transformer language model by OpenAI. Evaluated via API in zero-shot chain-of-thought and few-shot settings in this work.",
            "model_size": null,
            "logical_reasoning_task": "LogicBench (BQA & MCQA)",
            "task_description": "LogicBench: a newly introduced natural-language QA dataset designed to evaluate single-inference-rule logical reasoning across 25 reasoning patterns spanning propositional logic (PL), first-order logic (FOL), and non-monotonic (NM) reasoning. Two tasks: Binary QA (BQA: yes/no entailment) and Multiple-Choice QA (MCQA: pick correct conclusion among four).",
            "method_or_approach": "Zero-shot chain-of-thought prompting (three distinct CoT prompts averaged), plus some few-shot experiments; manual analysis of generated reasoning chains. Also compared to models fine-tuned on LogicBench(Aug) (LogicT5).",
            "performance": "On LogicBench BQA: average accuracy for positive entailment (A(Yes)) = 63.98% (average across inference rules and three CoT prompts); accuracy on negative label (A(No)) substantially higher (reported ~86.4%). On LogicBench MCQA: overall accuracy ~91.5% (average across rules). Qualitative findings: best performing model among those tested but still fails many single-rule instances, especially those involving negation and longer inference templates.",
            "limitations_or_failure_cases": "Struggles when premises include negations (Modus Tollens, Destructive Dilemma, etc.), with longer/more complex inference rules (CD, BD, DD), and occasionally overlooks or hallucinates contextual information necessary for correct logical derivation; performance on single-step logical entailment (BQA) remains below human performance (~85% human accuracy on subset), particularly for positive-entailment questions.",
            "comparison": "Outperforms smaller models tested (ChatGPT / GPT-3.5, Llama-2-7B, Mistral-7B, Yi-34B, etc.) on both BQA and MCQA. Larger model capacity correlates with higher logical-reasoning performance; GPT-4 tops MCQA and has highest A(Yes) among evaluated models.",
            "ablation_or_analysis_results": "Analyses in paper show (1) model-size effect (larger models do better), (2) sensitivity to negation and rule length, (3) chain-of-thought prompt variability (results averaged across three CoT prompts), and (4) few-shot behavior: some improvements in particular rules/tasks; detailed error analysis of reasoning chains identifies ~1% of chains where model introduces unrelated facts (hallucination).",
            "uuid": "e5013.0",
            "source_info": {
                "paper_title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (GPT-3.5-Turbo)",
            "brief_description": "OpenAI's GPT-3.5-Turbo family (ChatGPT) evaluated zero-shot with chain-of-thought prompting on LogicBench; shows moderate performance but substantially weaker than GPT-4, especially on positive entailments (A(Yes)).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5-Turbo)",
            "model_description": "OpenAI conversational model (GPT-3.5 family) accessed via API and evaluated in zero-shot CoT and some few-shot experiments.",
            "model_size": null,
            "logical_reasoning_task": "LogicBench (BQA & MCQA)",
            "task_description": "Same LogicBench BQA (yes/no entailment) and MCQA (4-choice conclusion selection) covering PL, FOL, and NM single-inference-rule problems.",
            "method_or_approach": "Zero-shot chain-of-thought prompting (three CoT prompts averaged); few-shot experiments reported in appendix for selected cases.",
            "performance": "BQA: average A(Yes) ≈ 48.04% (models perform noticeably worse on positive entailments than on negative); MCQA: overall accuracy ≈ 82.4% (averaged across rules). Performance is uneven across inference rules and poor on negation-heavy rules.",
            "limitations_or_failure_cases": "Poorer ability to draw correct positive entailments (lower A(Yes)), major failures on rules containing negation and longer rule templates; sometimes ignores relevant contextual sentences or hallucinates. Overall BQA performance below human annotators (~85%).",
            "comparison": "Underperforms GPT-4 substantially on both BQA and MCQA; outperforms smaller open models in some MCQA settings but is weaker on many PL inference rules.",
            "ablation_or_analysis_results": "Paper reports that few-shot exemplars help some models more than others (e.g., Llama-2 benefits strongly), and that ChatGPT's performance improves in NM reasoning with in-context exemplars; average across three CoT prompts was reported to reduce sensitivity to prompt phrasing.",
            "uuid": "e5013.1",
            "source_info": {
                "paper_title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Gemini-Pro",
            "name_full": "Google Gemini-Pro",
            "brief_description": "Google's Gemini-Pro model evaluated zero-shot with chain-of-thought prompting on LogicBench; shows strong MCQA performance but mixed BQA results and specific weaknesses similar to other LLMs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-Pro",
            "model_description": "Google's Gemini-Pro large language model evaluated via API in this study using CoT prompting (zero-shot and few-shot).",
            "model_size": null,
            "logical_reasoning_task": "LogicBench (BQA & MCQA)",
            "task_description": "Evaluates single-rule logical entailment (PL, FOL, NM) in binary and multiple-choice formats.",
            "method_or_approach": "Zero-shot chain-of-thought prompting (three distinct CoT prompts averaged); few-shot experiments included for analysis.",
            "performance": "MCQA: high overall accuracy (reported among the better-performing models on MCQA; e.g., MCQA average ~88.7% in the reported results). BQA: shows mixed results across inference rules; lower A(Yes) than top model(s) and struggles with negations/long rules.",
            "limitations_or_failure_cases": "Similar failure modes to other LLMs: trouble with negated premises, longer inference templates, and occasional omission of contextual constraints. Performance gaps between MCQA and BQA suggest reliance on option elimination strategies in MCQA.",
            "comparison": "Performs comparably to GPT-4 on some MCQA categories but generally behind GPT-4 in overall robustness on BQA; outperforms smaller models (7B class) in MCQA.",
            "ablation_or_analysis_results": "Paper observes model-size trends and prompt sensitivity; Gemini and GPT-4 show improvements in some logic types under few-shot prompting, indicating sensitivity to in-context examples.",
            "uuid": "e5013.2",
            "source_info": {
                "paper_title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Llama-2-7B",
            "name_full": "Llama-2-7B-Chat",
            "brief_description": "Open-source 7B-parameter Llama-2 chat model evaluated with chain-of-thought prompting; performs substantially worse than larger models on BQA but benefits strongly from few-shot in-context examples.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-7B-Chat",
            "model_description": "Meta's Llama-2 family, 7-billion-parameter chat model variant used from Hugging Face for zero-shot and few-shot evaluation in this work.",
            "model_size": "7B",
            "logical_reasoning_task": "LogicBench (BQA & MCQA)",
            "task_description": "Evaluated on LogicBench's single-inference-rule BQA and MCQA covering PL, FOL, and NM logic.",
            "method_or_approach": "Zero-shot chain-of-thought prompting (three CoT prompts averaged); few-shot experiments showed substantial gains by providing in-context exemplars.",
            "performance": "MCQA: moderate accuracy (~57.9% overall). BQA: notably lower A(Yes) (paper reports markedly worse performance than larger models; quantitative A(Yes) substantially below GPT-4/ChatGPT), often failing many PL rules.",
            "limitations_or_failure_cases": "Struggles with positive entailments, negations, and longer inference rules; sensitive to prompt and benefits greatly from few-shot exemplars (i.e., poor zero-shot generalization but strong in-context learning).",
            "comparison": "Underperforms larger closed models (GPT-4, Gemini) on both tasks but shows the largest relative improvement when shifted from zero-shot to few-shot among tested models, indicating strong in-context imitation abilities.",
            "ablation_or_analysis_results": "Few-shot (in-context examples) experiments show that Llama-2 consistently and markedly improves A(Yes) compared to zero-shot, implying that exemplar format/prompting is a key factor for smaller models.",
            "uuid": "e5013.3",
            "source_info": {
                "paper_title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral-7B-Instruct-v0.2",
            "brief_description": "A 7B instruct-tuned model evaluated on LogicBench; shows uneven performance and in some cases degraded behavior compared to other 7B models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct-v0.2",
            "model_description": "Mistral's 7B instruct-tuned model used from Hugging Face, evaluated in zero-shot CoT and few-shot experiments.",
            "model_size": "7B",
            "logical_reasoning_task": "LogicBench (BQA & MCQA)",
            "task_description": "Same LogicBench BQA and MCQA assessments across 25 inference patterns in PL, FOL, NM.",
            "method_or_approach": "Zero-shot chain-of-thought prompting; few-shot analysis included.",
            "performance": "MCQA: moderate performance (reported around mid-50s to high-50s % accuracy in MCQA averaged across rules in tables); BQA: variable and often degraded compared to other models — Mistral 'consistently shows degraded performance' on some logic types per the paper.",
            "limitations_or_failure_cases": "Consistent degradation observed across many logic types; fails many negation and longer-rule cases; less responsive to few-shot exemplars relative to Llama-2 and some larger models.",
            "comparison": "Per-paper Mistral performed worse than Gemini, ChatGPT, and GPT-4 in many categories; relative performance was mixed versus Llama-2 depending on the task and few-shot setup.",
            "ablation_or_analysis_results": "Paper notes consistent degraded performance for Mistral across logic types and that few-shot does not rescue it as strongly as for some other models.",
            "uuid": "e5013.4",
            "source_info": {
                "paper_title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Yi-34B-chat",
            "name_full": "Yi-34B-chat (Yi 34B)",
            "brief_description": "An intermediate-size (34B) chat model tested as an additional datapoint; shows improvement over 7B models but still below GPT-4, illustrating a positive correlation between size and logical-reasoning ability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Yi-34B-chat",
            "model_description": "Open foundation model (Yi) 34B chat variant evaluated in the Appendix/analysis as an intermediate-size model.",
            "model_size": "34B",
            "logical_reasoning_task": "LogicBench (BQA & MCQA)",
            "task_description": "Same LogicBench BQA and MCQA tasks; Yi-34B used to probe model-size scaling trends on logical reasoning.",
            "method_or_approach": "Zero-shot CoT prompting (analysis reported in App.I); performance compared against 7B and larger models.",
            "performance": "Reported to improve over Llama-2-7B by a substantial margin across logic types; still outperformed by GPT-4. Exact aggregated numbers are reported in appendix tables (improvement trend rather than claiming top accuracy).",
            "limitations_or_failure_cases": "Retains many failure modes seen in other models: difficulty with negation, longer rules, and some NM cases. Noted to be better than 7B-class models but not state-of-the-art.",
            "comparison": "Better than 7B models (e.g., Llama-2-7B) on average; worse than GPT-4. Supports the paper's conclusion that increasing model size tends to improve logical reasoning performance.",
            "ablation_or_analysis_results": "Used to support size-vs-performance analysis; adding the 34B point showed monotonic improvement over 7B but diminishing returns compared to GPT-4.",
            "uuid": "e5013.5",
            "source_info": {
                "paper_title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LogicT5 (T5-large fine-tuned)",
            "name_full": "LogicT5 (T5-large fine-tuned on LogicBench(Aug))",
            "brief_description": "A T5-large model fine-tuned on the synthetically augmented LogicBench(Aug) (created by the authors) to improve logical reasoning; used to demonstrate that training on LogicBench improves performance on logic datasets.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "T5-large (LogicT5)",
            "model_description": "T5-large (encoder-decoder transformer) fine-tuned on a synthetically augmented version of LogicBench (LogicBench(Aug)) to create LogicT5 for evaluating the effect of supervised training on logical reasoning.",
            "model_size": "T5-large (~700M-1B class, exact parameter count not specified in paper)",
            "logical_reasoning_task": "Trained on LogicBench(Aug) and evaluated on LogicBench(Eval), plus transfer evaluations on other logic datasets (LogicNLI, FOLIO, LogiQA, ReClor).",
            "task_description": "Fine-tuned to perform BQA/MCQA style logical entailment tasks derived from single-inference-rule narratives across PL, FOL, and NM patterns; tested for transfer to existing logic benchmarks.",
            "method_or_approach": "Supervised fine-tuning on LogicBench(Aug) synthetic data (150 unique samples per rule, resulting in an augmented training set); training hyperparameters: 10 epochs, batch size 16, max input length 1024, adaptive LR 5e-05, AdamW optimizer (on NVIDIA RTX A6000 GPUs).",
            "performance": "Fine-tuning T5-large on LogicBench(Aug) (LogicT5) produced improvements in logical reasoning ability: reported average improvements of ≈2% on other logic datasets (LogicNLI and FOLIO) and competitive results on LogiQA and ReClor compared to baseline T5-large. Exact per-task numbers and tables are in App.I / main text summary.",
            "limitations_or_failure_cases": "Training on single-inference-rule, single-step data; paper notes scope limitations (no multi-step / multi-rule reasoning covered). Improvements are modest (~2% average on some downstream logic datasets) and do not fully close the gap to top LLMs; LogicT5 remains sensitive to the diversity and coverage of logic patterns in the augmented training set.",
            "comparison": "LogicT5 (fine-tuned on LogicBench(Aug)) outperforms baseline T5-large on logic-focused datasets (notably LogicNLI and FOLIO). However, on more mixed-reasoning datasets (LogiQA, ReClor) the gains are modest; closed large LLMs (GPT-4) still outperform a fine-tuned T5 in many settings.",
            "ablation_or_analysis_results": "Paper reports transfer and multi-task fine-tuning experiments (single-task vs multi-task) showing stronger gains when LogicBench-like data is included; ablation on dataset composition and amount is discussed qualitatively and in App.I (showing that logic-focused augmentation helps FOLIO/LogicNLI more than mixed reasoning datasets).",
            "uuid": "e5013.6",
            "source_info": {
                "paper_title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "LogicBench (dataset)",
            "name_full": "LogicBench (evaluation and augmented variants)",
            "brief_description": "A new natural-language dataset introduced in this paper to systematically evaluate single-inference-rule logical reasoning abilities of LLMs across 25 inference rules spanning propositional, first-order, and non-monotonic logics; supports BQA and MCQA tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LogicBench (dataset)",
            "model_description": "Dataset consisting of LogicBench(Eval) (binary QA: 1520 BQA instances (520 yes, 1000 no) and 500 MCQA instances) and a synthetic augmentation LogicBench(Aug) (12908 total samples with variations) covering 25 logical inference patterns; contexts are narrative natural-language renditions of single logical templates.",
            "model_size": null,
            "logical_reasoning_task": "Binary QA (BQA) and Multiple-Choice QA (MCQA) for single-inference-rule logical entailment across PL, FOL, NM.",
            "task_description": "BQA: determine whether a conclusion is logically entailed by a narrative context (yes/no). MCQA: select correct conclusion from four options. Each instance follows exactly one inference rule to allow fine-grained evaluation.",
            "method_or_approach": "Data created by a three-stage pipeline using GPT-3.5 for sentence generation and narrative conversion (templatized-to-narrative) followed by automatic generation of question/option variations and manual validation to remove leakage/hallucinated conclusions; evaluated via zero-shot CoT and few-shot prompting across multiple LLMs and via fine-tuning experiments with T5-large.",
            "performance": "Used as the primary benchmark in the paper: human annotators achieve ≈85% accuracy on a sampled subset (BQA/MCQA), while evaluated LLMs show substantially lower performance on BQA (A(Yes) often &lt; 64% even for GPT-4) and mixed but higher performance on MCQA (GPT-4 ~91.5%, Gemini and ChatGPT high but lower than GPT-4).",
            "limitations_or_failure_cases": "Covers only single-inference-rule, single-step reasoning; does not include multi-step combinations of inference rules. Limited to English. Some initial synthetic generation errors required manual correction (identified leakage of conclusions in ~15% of a 500-sample subset and missing premises in ~8% before manual mitigation).",
            "comparison": "Compared in the paper to existing logic datasets (ProofWriter, FOLIO, ProntoQA, LogicNLI, LogiQA, ReClor) and claimed to cover more inference rules (25) and include non-monotonic reasoning, which many prior benchmarks omit.",
            "ablation_or_analysis_results": "Analyses using LogicBench included: per-inference-rule performance breakdowns, label-wise BQA (A(Yes)/A(No)), effect of negation and rule length on accuracy, human vs model comparison, few-shot vs zero-shot impacts, and transfer/fine-tuning experiments with T5-large showing utility of LogicBench(Aug) for improving downstream logical-reasoning datasets.",
            "uuid": "e5013.7",
            "source_info": {
                "paper_title": "LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "FOLIO: Natural language reasoning with first-order logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "ProntoQA: Language models are greedy reasoners: A systematic formal analysis of chain-of-thought",
            "rating": 2,
            "sanitized_title": "prontoqa_language_models_are_greedy_reasoners_a_systematic_formal_analysis_of_chainofthought"
        },
        {
            "paper_title": "LogicNLI: Diagnosing the first-order logical reasoning ability through LogicNLI",
            "rating": 2,
            "sanitized_title": "logicnli_diagnosing_the_firstorder_logical_reasoning_ability_through_logicnli"
        },
        {
            "paper_title": "LogiQA: a challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 1,
            "sanitized_title": "logiqa_a_challenge_dataset_for_machine_reading_comprehension_with_logical_reasoning"
        },
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 1,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        }
    ],
    "cost": 0.023011999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models
6 Jun 2024</p>
<p>Mihir Parmar mparmar3@asu.edu 
Arizona State University</p>
<p>Arizona State University</p>
<p>Nisarg Patel nppatel7@asu.edu 
Arizona State University</p>
<p>PhaniNeeraj Varshney 
Arizona State University</p>
<p>Arizona State University</p>
<p>Mutsumi Nakamura 
Arizona State University</p>
<p>Man Luo 
Arizona State University</p>
<p>Santosh Mashetty 
Arizona State University</p>
<p>Arindam Mitra 
Arizona State University</p>
<p>Chitta Baral chitta@asu.edu 
Arizona State University</p>
<p>Yizhong Wang 
Arizona State University</p>
<p>Swaroop Mishra 
Arizona State University</p>
<p>Pegah Alipoormo- Labashi 
Arizona State University</p>
<p>Yeganeh Kordi 
Arizona State University</p>
<p>Amirreza Mirzaei 
Arizona State University</p>
<p>Atharva Naik 
Arizona State University</p>
<p>ArutArjun Ashok 
Arizona State University</p>
<p>Selvan Dhanasekaran 
Arizona State University</p>
<p>Anjana Arunkumar 
Arizona State University</p>
<p>David Stap 
Arizona State University</p>
<p>Eshaan Pathak 
Arizona State University</p>
<p>Giannis Karamanolakis 
Arizona State University</p>
<p>Haizhi Lai 
Arizona State University</p>
<p>Ishan Puro- Hit 
Arizona State University</p>
<p>Ishani Mondal 
Arizona State University</p>
<p>Jacob Anderson 
Arizona State University</p>
<p>Kirby Kuznia 
Arizona State University</p>
<p>Krima Doshi 
Arizona State University</p>
<p>Kuntal Kumar Pal 
Arizona State University</p>
<p>Maitreya Patel 
Arizona State University</p>
<p>Mehrad Moradshahi 
Arizona State University</p>
<p>Mirali Purohit 
Arizona State University</p>
<p>Rohitha Kaza 
Arizona State University</p>
<p>RavsehajPulkit Verma 
Arizona State University</p>
<p>Singh Puri 
Arizona State University</p>
<p>Rushang Karia 
Arizona State University</p>
<p>Savan Doshi 
Arizona State University</p>
<p>Shailaja Keyur Sampat 
Arizona State University</p>
<p>Siddhartha Mishra 
Arizona State University</p>
<p>Sujan Reddy 
Arizona State University</p>
<p>Sumanta Patro 
Arizona State University</p>
<p>Tanay Dixit 
Arizona State University</p>
<p>Xudong Shen 
Arizona State University</p>
<p>LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models
6 Jun 2024DC313305A84D5DDB09F5D66382BD2145arXiv:2404.15522v2[cs.CL]
Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks.But, can they really "reason" over the natural language?This question has been receiving significant research attention and many reasoning skills such as commonsense, numerical, and qualitative have been studied.However, the crucial skill pertaining to 'logical reasoning' has remained underexplored.Existing work investigating this reasoning ability of LLMs has focused only on a couple of inference rules (such as modus ponens and modus tollens) of propositional and first-order logic.Addressing the above limitation, we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics.To enable systematic evaluation, we introduce LogicBench, a natural language question-answering dataset focusing on the use of a single inference rule.We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting.Experimental results show that existing LLMs do not fare well on LogicBench; especially, they struggle with instances involving complex reasoning and negations.Furthermore, they sometimes overlook contextual information necessary for reasoning to arrive at the correct conclusion.We believe that our work and findings facilitate future research for evaluating and enhancing the logical reasoning ability of LLMs 1 .</p>
<p>Introduction</p>
<p>Large language models such as GPT-4, ChatGPT, Google Gemini, Llama-2 (Touvron et al., 2023), and Mistral (Jiang et al., 2023) have made remarkable progress in NLP research enabling machines to perform a variety of language tasks that were previously thought to be exclusive to humans (OpenAI, 2023;Brown et al., 2020;Zhao et al., 2023).However, the ability of these LLMs to reason "logically" over natural language text remains under-explored, even though logical reasoning is a fundamental aspect of intelligence and a crucial requirement for many practical applications, such as questionanswering systems (Khashabi, 2019) and conversational agents (Beygi et al., 2022).Although several datasets have been proposed (Clark et al., 2021;Tian et al., 2021;Joshi et al., 2020;Saeed et al., 2021) to evaluate the logical reasoning capabilities of LLMs, these datasets are limited in their scope by (1) not evaluating logical reasoning independently of other forms of reasoning such as LogiQA (Liu et al., 2021a) and ReClor (Yu et al., 2020); and (2) evaluating only a single type of logic and covering only few logical inference rules as done in FOLIO (Han et al., 2022) and ProntoQA (Saparov and He, 2023).Thus, our aim in this work is to address the lacuna of having a more comprehensive set of inference rules for evaluating the logical reasoning ability of LLMs.</p>
<p>To this end, we introduce a systematically created question-answering dataset for the evaluation of logical reasoning ability using a single inference rule, called LogicBench.Besides evaluating the logical reasoning ability of LLMs, by evaluating models on single inference rules, we can also gain insights into the frequency of text sequences corresponding to these rules in the pre-training data and their impact on model performance.As illustrated in Figure 1, LogicBench includes a total of 25 reasoning patterns across 'propositional, first-order, and non-monotonic' logics.To the best of the authors' knowledge, this is the first work to study non-monotonic reasoning, as well as various inference rules in propositional and first-order logics including hypothetical and disjunctive syllogism; and bidirectional, constructive, and destructive dilemmas in NLP domain.To evaluate LLMs using Log-icBench, we formulate two different tasks: (i) a Binary Question-Answering (BQA) task in which the context comprises logical statements and the models have to determine whether a conclusion given in the question is logically entailed by the context, and (ii) a Multiple-Choice Questions-Answering (MCQA) task where models must select the most appropriate logical conclusion from four distinct options, based on the provided context.The rationale behind having BQA and MCQA tasks is that it provides systematic standard metric-based evaluation (i.e., direct comparison of LLMs' performance in terms of accuracy), which could be more challenging with open-ended question-answer formats.Examples instances of various reasoning patterns are presented in Table 4 and App.C.</p>
<p>To construct LogicBench, we use a three-stage procedure (refer to §3).In the first stage, we prompt GPT-3.5 to generate a variety of coherent natural language sentences having different 'ontologies' (i.e., a collection of concepts as car, person, and animals) and their corresponding negations (refer to §3.2.1).In the second stage, we generate (context, question) pairs where the context represents a natural language narrative consisting of logical statements, and the question is formulated to exhibit the logical conclusion derived from the context.In the third stage, we generate task-specific data instances (i.e., (context, question, answer) triplets).</p>
<p>We conduct a comprehensive evaluation with a range of LLMs on LogicBench including GPT-4, ChatGPT (GPT-3.5-Turbo),Gemini-Pro, Llama-2-7B-Chat, and Mistral-7B-Instruct using chainof-thought (Wei et al., 2022).In particular, we measure the accuracy of LLMs predictions on both BQA and MCQA tasks.Our experiments result in several interesting findings such as LLMs often
✗ ✓ ✗ ✗ Semi-automated ProofWriter ✓ ✓ ✗ ✗ ✗ FOLIO ✗ ✓ ✗ ✗ Human-annotated SimpleLogic ✓ ✗ ✗ ✗ ✓ ProntoQA ✗ ✓ ✗ ✓ ✓ LogicBench ✓ ✓ ✓ ✓ ✓
Table 1: LogicBench's comparison with current datasets struggle to reason over complex logical contexts that involve only a single inference rule and encounter difficulties with inference rules involving negations.Experimental results reveal that these models struggle with respect to many of the inference rules and patterns, suggesting significant room for improvement in their logical reasoning abilities.To further demonstrate the use of Log-icBench, we synthetically augment it and fine-tune T5-large.Our preliminary results (App.I) show that this improves the logical reasoning ability of existing models leading to performance improvement on other logic datasets, LogicNLI, and FO-LIO (∼ 2% on an average), and shows competitive performance on LogiQA and ReClor.</p>
<p>Related Work</p>
<p>As LLMs continue to evolve rapidly, it becomes increasingly crucial to evaluate their diverse reasoning capabilities, as well as those of forthcoming LLMs.LogiQA (Liu et al., 2021a) and ReClor (Yu et al., 2020) have made notable contributions by compiling multichoice questions from standardized examinations that demand diverse forms of logical reasoning.In contrast to LogicBench, these datasets involve mixed forms of reasoning and do not focus on assessing logical reasoning in isolation.</p>
<p>A few past attempts have been made to evaluate only logical reasoning while excluding other forms of reasoning.For example, CLUTTER (Sinha et al., 2019) covers inductive reasoning, (Hahn et al., 2021) covers temporal logic, and Ruletaker (Clark et al., 2021) evaluates whether a transformerbased model emulates deductive reasoning over synthetically generated statements in a limited setting.LogicNLI (Tian et al., 2021) introduced a diagnostic benchmark for FOL reasoning, with the dataset constructed by automatically generating logic expressions and replacing the entity and attribute placeholders.</p>
<p>Our proposed dataset is similar (in terms of task formulation) to ProofWriter (Tafjord et al., 2021),</p>
<p>Names Propositional Logic</p>
<p>Extension to a (restricted) First-order Logic MP ((p → q) ∧ p) ⊢ q (∀x(p(x) → q(x)) ∧ p(a)) ⊢ q(a) MT ((p → q) ∧ ¬q) ⊢ ¬p (∀x(p(x) → q(x)) ∧ ¬q(a)) ⊢ ¬p(a) HS ((p → q)) ∧ (q → r)) ⊢ (p → r) (∀x((p(x) → q(x)) ∧ (q(x) → r(x))) ⊢ (p(a) → r(a)) DS ((p ∨ q) ∧ ¬p) ⊢ q (∀x(p(x) ∨ q(x)) ∧ ¬p(a)) ⊢ q(a) CD ((p → q) ∧ (r → s) ∧ (p ∨ r)) ⊢ (q ∨ s) (∀x((p(x) → q(x)) ∧ (r(x) → s(x))) ∧ (p(a) ∨ r(a))) ⊢ (q(a) ∨ s(a)) DD ((p → q) ∧ (r → s) ∧ (¬q ∨ ¬s)) ⊢ (¬p ∨ ¬r) (∀x((p(x) → q(x)) ∧ (r(x) → s(x))) ∧ (¬q(a) ∨ ¬s(a))) ⊢ (¬p(a) ∨ ¬r(a)) BD ((p → q) ∧ (r → s) ∧ (p ∨ ¬s)) ⊢ (q ∨ ¬r) (∀x((p(x) → q(x)) ∧ (r(x) → s(x))) ∧ (p(a) ∨ ¬s(a))) ⊢ (q(a) ∨ ¬r(a)) FOLIO (Han et al., 2022), andProntoQA (Saparov andHe, 2023) which are QA datasets designed to test reasoning ability.ProofWriter provides multihop proofs for each example, while FOLIO gives diverse and complex logical expressions, however, it is only limited to FOL.ProntoQA (Saparov and He, 2023) provides explanation and reasoning steps but is limited to modus ponens in FOL.Nevertheless, several crucial attributes motivated us to create LogicBench (see Table 1 for comparison).
CT (p ∨ q) ⊢ (q ∨ p) - MI (p → q) ⊢ (¬p ∨ q) - EG - P (a) ⇒ ∃xP (x) UI - ∀x A ⇒ A{x → a}
Additional datasets for evaluating logical reasoning also exist such as SimpleLogic (Zhang et al., 2023) provides a class of logical reasoning problems, TaxiNLI (Joshi et al., 2020)  Ponens'.In our study, we explore nine distinct inference rules of propositional logic, extensions of seven of them with one-variable and a universal quantifier, and two axioms of first-order logic as shown in Table 2.These inference rules provide a proper framework for deriving valid conclusions.</p>
<p>First-order Logic (FOL) In this work, we consider a restricted set of logical axioms for FOL that utilize quantifiers, ∀ (universal quantifier) and ∃ (existential quantifier).The universal quantifier (∀) denotes that a statement holds true for all instances within a specific category.In contrast, the existential quantifier (∃) indicates that a statement is true for at least one instance within its scope.</p>
<p>For instance, a simple extension of propositional 'Modus Ponens' is an inference rule where given the premises ∀(p(x) → q(x)) and p(a), we conclude q(a) (e.g., given "All kings are greedy" and "Sam is a king", we can conclude "Sam is greedy").</p>
<p>Here, we explore two axioms (EG and UI -in detail in App.C.3) and various inference rules that incorporate the quantifiers (shown in Table 2).</p>
<p>Non-monotonic (NM) Reasoning</p>
<p>In this work, we analyze a range of logical reasoning templates in NM logics involving "Default Reasoning," "Reasoning about Unknown Expectations," and "Reasoning about Priorities."These templates are inspired by the compilation (Lifschitz, 1989) made in 1989 to evaluate the abilities of various non-  (Lifschitz, 1989).monotonic logics that were being developed at that time.Below Table 3 shows examples of NM reasoning.Additional examples are given in App.C.4.</p>
<p>A key aspect of NM logics is to formalize notions such as "normally," "typically," and "usually" that are not directly formalizable using classical quantifiers in the first-order setting.The general rule "Heavy blocks are normally located on the table" does not imply that "All heavy blocks are always located on the table".Rather, this rule allows for exceptions.Our work explores various NM reasoning patterns, as depicted in Figure 1, to delve deeper into the nuances of this type of reasoning.</p>
<p>Data Creation</p>
<p>Our data creation procedure, illustrated in Figure 2, consists of three stages:</p>
<ol>
<li>
<p>Sentence Generation: Starting with a given prompt, we generate coherent sentences and their negations that incorporate different ontologies.</p>
</li>
<li>
<p>NL Conversion: Pairs of (context, question) are generated using pre-defined templates from which context is then converted to a natural language narrative using the prompt.</p>
</li>
<li>
<p>Task Instance Generation: Task-specific (context, question, answer) triplets are generated.BQA requires answers in the form of "yes" or "no", whereas MCQA involves selecting one correct option from a set of four.We generate semantically preserving and inverting variations of these triplets to add more diversity for BQA.</p>
</li>
</ol>
<p>Examples of generated data corresponding to each logic type and reasoning patterns are presented in App. C.</p>
<p>Sentence Generation</p>
<p>Here, the first step is to generate sentences with diverse ontologies.An ontology represents a col-lection of concepts (e.g.car, person, animals, etc.) along with their corresponding associated properties.To generate these sentences, we prompt the GPT-3.5 model with instructions tailored for each inference rule (more details in App.A).</p>
<p>An example of a prompt corresponding to the 'Modus Tollens' from PL is presented in App.A for better illustration.Note that our objective at this stage is not to generate logical sentences but rather to generate a diverse and coherent set of sentences that encompass various concepts.We also create a negation sentence corresponding to each generated sentence2 .In this work, the scope of generating negations is simple (refer to App.C for examples), however, negations can be more complicated in the case of logic.These generated sentences will be combined with logical connectives in a later stage to form context and questions.</p>
<p>NL Conversion</p>
<p>Here, the NL conversion is accomplished using two steps.First, we leverage the formal expressions of reasoning patterns to create templates that establish the desired NL formulation for each logical connective (i.e., templatized context).Second, we prompt GPT-3.5 to transform the templatized context into a story/narrative-based context, enhancing its naturalness.For instance, implication: "p → q" is expressed as "If p, then q", conjunction: "p ∧ q" as "p and q.", and disjunction: "p ∨ q" as "At least one of the following is true: (1) p and (2) q.Note that we do not know which of (1) and ( 2) is true.It is possible that only (1) is true, or only (2) is true, or both are true."since understanding the logical implication of 'or' when integrated into logical formulations posed challenges to both humans and  Then, the templatized context is converted into a narrative-based context using a prompt-based converter, enhancing its naturalness.The promptbased converter (essentially, prompting GPT-3.5)ensures that the context is no longer templatized yet follows the logical connection between sentences as mentioned in the logical rule (further details are presented in App.D).For instance, let's consider the "Modus Tollens" from PL (((p → q) ∧ ¬q) ⊢ ¬p), and the "Bidirectional Dilemma" from FOL (∀x((p(x) → q(x))∧(r(x) → s(x)))∧ (p(a) ∨ ¬s(a))) ⊢ (q(a) ∨ ¬r(a))).For these rules, Table 4 presents examples of logical templatized and narrative-based context, question, and task instances for both, BQA and MCQA.App.D showcases further examples corresponding to each inference rule and patterns from LogicBench.</p>
<p>Task Instance Generation</p>
<p>After generating the context and questions in §3.2.2, we generate (context, question, answer) triplets for both tasks: (i) BQA, and (ii) MCQA.</p>
<p>Here, narrative-based context is similar for both BQA and MCQA tasks, only the format of (question, answer) pairs are different.</p>
<p>BQA We generate semantically preserving and inverting variations of questions.Let's consider the example of "Modus Tollens" from Table 4, having question as: "Does this imply that Liam didn't finish his work early?"In this question, we observe one proposition: s 1 , representing the statement "Liam didn't finish his work early," can be used to create another question that did not follow the logical rule MT.We can create two possible tuples: &lt; ¬s 1 , yes &gt;, &lt; s 1 , no &gt;.Each tuple has a question-answer combination using proposition s 1 .Moreover, we do not generate variations for the context since it offers no substantial diversity in the dataset.For question variations, we replace the variation of proposition &lt; s 1 &gt; in the original question with the corresponding tuples to add diversity to LogicBench.The process allows us to create more variations of the question for BQA tasks, as illustrated in Figure 2 (Step 3 -Task 1).</p>
<p>MCQA A prompt-based approach is used to create different incorrect options.For rule MT, as shown in Table 4, option 1 is a correct option that follows the rule MT logically (&lt; ¬s 1 &gt;), while the other three options are generated using prompting in a way that does not follow the rule being incorrect options.In addition to the options, the question is also replaced by a randomly selected question from the set of five questions.As seen in Figure 2 (Step 3 -Task 2), there is only one correct option out of four given options.Inference rule: BD p(x): someone drinks lots of water q(x): they will feel hydrated r(x): they eat too much sugar s(x): they will experience a sugar crash p(a): Jane drinks lots of water ¬p(a): Jane does not drink lots of water q(a): she will feel hydrated ¬q(a): she will not feel hydrated r(a): she eats too much sugar ¬r(a): she does not eat too much sugar s(a): she will experience a sugar crash ¬s(a): she will not experience a sugar crash</p>
<p>Templatized Context: If someone drinks lots of water, then they will feel hydrated.If they eat too much sugar, then they will experience a sugar crash.We know that at least one of the following is true (1) Jane drinks lots of water and (2) she won't experience a sugar crash.Note that we do not know which ones of (1) and (2) are true.It might be the case that only (1) is true, or only (2) is true or both are true.</p>
<p>NL Context: If someone consumes a significant amount of water, they will experience a state of hydration.conversely, if excessive amounts of sugar are ingested by them, a sugar crash will ensue.it is known that at least one of the following statements is true: either the Jane consumes ample water or she will not experience a sugar crash.however, the actual veracity of either statement remains ambiguous, as it could be the case that only the first statement is true, only the second statement is true, or both statements are true.</p>
<p>Question 1: Can we say at least one of the following must always be true?(a) she will feel hydrated and (b) she doesn't eat too much sugar (Yes)   Data Validation Throughout the data generation phase of LogicBench(Eval), the authors conduct a review of the logical formations to ensure they follow the intended logical structure.We examine each narrative for any potential discrepancies, ensuring that they are logically sound and correctly represent the intended relationships between propositions.In addition to the logical formation, we also dedicated considerable effort to eliminating typos and validating the grammar.We also analyze the diversity in terms of different ontology and the logical nature of the LogicBench(Eval) (presented in App.C.1).We mitigate errors encountered during the validation step (presented in App.F).</p>
<p>Results and Analysis</p>
<p>Experimental Setup</p>
<p>Task Formulation For BQA, let us consider a set of data instances I r,L corresponding to the inference rule r and logic type L. In this set, i th instance is represented as
I i r,L = {(c i , Q i )}
where c i represents narrative context and Q i = {q 1 , q 2 , ..., q n } represents set of question and its variations corresponding to i th instance.As discussed in §3, each context (c) represents logical rules (e.g., All cats have fur.Tom is a cat.) and question (q) represents the conclusion (e.g., Does Tom have fur?).To each context and question pair, i.e., &lt; c, q &gt;, we assign a label from the set Y = {Y es, N o}.We assign a label Y es if the conclusion logically entails the context, otherwise, assign a label N o.To evaluate any LLMs on this setup, we provide &lt; p, c, q &gt; as input to predict a label from Y where p is a natural language prompt.In the set I r,L for MCQA, i th instance is represented as I i r,L = {(c i , q i , O i )} where c i represents narrative context and q i represents question and O i = {o 1 , o 2 , o 3 , o 4 } represents four option choices.To each context and question pair, i.e., &lt; c, q &gt;, we assign a label from the set Y = {o 1 , o 2 , o 3 , o 4 }.We assign a label o 1 if the correct conclusion is presented in the first option, and likewise for other labels.To evaluate any LLMs on this setup, we provide &lt; p, c, q, o &gt; as input to predict a label from Y.</p>
<p>Experiments We evaluate a range of prompting models including GPT-4, ChatGPT (GPT-3.5-Turbo),Google Gemini-Pro, Llama-2-7B-Chat, and Mistral-7B-Instruct-v0.2.Each model is evaluated in a zero-shot setting where the chain-ofthought prompt is provided to the model without any in-context examples.This approach allows us to determine LLM's inherent ability to do logical reasoning (based on pre-training), as we can not expect that various logical inference rules/patterns will always be made part of prompts.However, we  do evaluate these models in a few-shot setting, and present the results in App.G.</p>
<p>Metrics Here, we evaluate performance in terms of accuracy for both tasks, BQA and MCQA.For the BQA, we measure accuracy corresponding to each label, i.e., A(Y es) and A(N o).We evaluate each model on three different chain-of-thought prompts and report average results across these prompts.All prompts used for experiments are described in App.H.</p>
<p>Main Results</p>
<p>Table 6 and Table 7 represent inference rule-wise performance corresponding to each LLMs for the BCQ and MCQA tasks, respectively.Specifically, Table 6 provides label-wise accuracy (A(Y es) and A(N o)) for the BQA task, and Table 7 provides overall accuracy for the MCQA task.Both tables provide valuable insights into the performance of different models on various logic types and lead to several interesting findings.From Table 6, we can observe that ChatGPT achieves 48.04%, and GPT-4 shows a performance of 63.98% A(Y es) on average which indicates the challenge of classical logical reasoning (PL) even for larger LLMs such as ChatGPT and GPT-4.Furthermore, we can observe that models struggle more with inference rules of PL compared to FOL and NM.In addition, it is noticeable that each model performs relatively better on questions with a negative response (i.e., N o) compared to questions with a positive response (i.e., Y es).This observation suggests that the models struggle to fully comprehend the logical relationship between the context and the conclusion (i.e., lower A(Y es)).However, they demonstrate a relatively stronger understanding when the relationship is contradictory in nature (i.e., higher A(N o)).From Table 7, we can observe that larger models exhibit superior performance in selecting the correct choice to arrive at a logical conclusion.Interestingly, the performance of models decreases when the inference rules are longer or include nega- tions.In contrast to Table 6, for MCQA, LLMs show superior performance for PL and FOL compared to NM.To further investigate these findings and provide a detailed analysis, we perform a thorough study of reasoning chains generated by LLMs and present our insights in the subsequent section.</p>
<p>Analysis and Discussion</p>
<p>Human Performance We conduct a human evaluation on a subset of LogicBench(Eval) for both tasks, BQA and MCQA.Specifically, we selected 50 unique instances covering all 25 reasoning patterns from LogicBench(Eval).This selection resulted in total instances of 153 <context, ques-tion> pairs for BQA, and 50 <context, question, choices> pairs for MCQA.We hired three graduate student volunteers to provide the evaluations.</p>
<p>The task instructions given to all three annotators closely resemble the prompts provided to models (App.H).Each instance pair is answered/annotated by three different annotators with 0.785 interannotator agreement (measured with raw/observed agreement) for BQA and 0.813 for MCQA.</p>
<p>From the results (Table 8) for BQA, we see that humans achieve more than ∼ 85% accuracy on various logic types on LogicBench(Eval) which indicates the capability of humans to comprehend single-step logical reasoning effectively.From Ta-  ble 6, we observe that the average performance of all models is below human performance indicating room for improvement in their reasoning capabilities.From Table 7, we make similar observations for MCQA.However, we can see that the performance of NM for the MCQA task remains a challenge for both humans and LLMs.</p>
<p>Lower performance of LLMs on PL as compared to NM and FOL for BQA.In the development of AI, NM logic was partly developed to formalize natural language constructs, such as "normally birds fly", that were not formalizable in a straightforward manner using classical mathematical logics.Thus, while it was difficult for researchers to come up with non-monotonic logics and formalize non-monotonic reasoning, the fact that they were usually motivated by natural language examples, suggests that many of the nonmonotonic reasoning aspects are present in the NL text in the wild that is used in the pre-training of the ultra-large LLMs such as GPT4.While from human experience and complexity theory, FOL is harder than PL in general; in the LLM context, the crucial factor becomes what kind of logical sentences LLMs are pre-trained on.It seems that LLMs are pre-trained more on simple FOL sentences than on simple PL sentences (see Appendix I for further discussion).On the other hand, some PL features are perhaps less prevalent in human writing (on which LLMs are pre-trained) -such as Modes Tollens.Table 6 shows that GPT-4 achieves ∼ 85% accuracy (A(Y es)) for simple inference rules such as HS(PL).However, GPT-4 performance dropped to ∼ 59% A(Y es) for PL(MT).</p>
<p>Negations are hard to understand when embedded with logical rules.Regarding PL and FOL, it is apparent that the models struggle more with the DD and MT inference rules.A closer look at Table 2 reveals that all of these rules include examples where the models need to draw conclusions based on negated premises.This indicates that the models encounter difficulties when negated premises are introduced.We also analyze the effect of negations on the reasoning chain (see App. I).</p>
<p>Longer inference rules are still challenging.Table 6 indicates that the models face challenges when handling longer rules, such as BD, CD, and DD, both in PL and FOL.Hence, it can be concluded that these models struggle with longer logical dependencies in the premise, particularly when a higher number of propositions are present.In the case of NM reasoning, the models exhibit lower performance in DRS, indicating that a higher number of premises often leads to more frequent mistakes.</p>
<p>LLMs sometimes overlook contextual information.We investigate the LLMs' logical reasoning ability in natural language, not in artificial logical formulations.Hence, we note that LLMs sometimes hallucinate information and overlook contextual information, leading to incorrect conclusions.</p>
<p>To analyze this, we manually examine all the reasoning chains generated for instances sharing the same contexts in both BQA and MCQA tasks.We observe that, although this pattern is not dominant, it affects BQA more than MCQA.For a more indepth analysis, please refer to App.I.</p>
<p>Large models are better logical reasoners.We analyze the results of both smaller (Llama-2-7B and Mistral-7B) and larger (ChatGPT, GPT-4, and Gemini) models.Table 6 and Table 7 show that larger models tend to exhibit higher performance across different types of logic.We further investigate an additional model with an intermediate size: Yi-34B-chat (Young et al., 2024) (results are presented in App.I).When compared to the Llama-2-7B, the Yi-34B model (5x larger than Llama-7B) shows improvement in average performance across three logic types.Similarly, GPT-4 outperforms Yi-34B.This suggests that increasing the model size leads to substantial gains in performance, indicating the influence of larger model capacities on carrying out better logical reasoning.</p>
<p>Performance of BQA vs. MCQA From Table 6 and Table 7, we can see the overall performance of LLMs is higher on P L M CQA compared to P L BQA .Conversely, the performance is lower on N M M CQA compared to N M BQA .For PL, the performance gaps between the CT and DD inference rules primarily contributed to this trend, and DRO, RAP, and DRD for NM.We analyze the reasoning chains associated with these inference rules and presented our detailed observations in App.I.</p>
<p>Effect on other logic datasets We trained the T5-large model on the LogicBench(Aug) resulting in a model named LogicT5.Furthermore, we performed fine-tuning on four other logical reasoning datasets: LogiQA, Reclor, LogicNLI, and FOLIO.Further discussion is presented in App.I.</p>
<p>Conclusions</p>
<p>In this work, we evaluated the logical reasoning ability of LLMs on 25 distinct inference rules and reasoning patterns covering PL, FOL, and NM logics.To this end, we introduced LogicBench, a natural language question-answering dataset focusing on evaluating a single inference rule.We devised two tasks using LogicBench: (i) BQA, and (ii) MCQA.We evaluated a range of LLMs including GPT-4, ChatGPT, Gemini-Pro, Llama-2, and Mistral on both tasks.Experimental results showed that LLMs do not perform well on LogicBench, even though they require the application of only a single inference rule.Furthermore, we also augmented LogicBench to LogicBench(Aug), which can be utilized for training purposes.Using Log-icBench(Aug), we demonstrated that LLMs trained using it showcase an improved understanding of logical reasoning, resulting in a better performance on existing logic datasets.</p>
<p>Limitations</p>
<p>While LogicBench encompasses 25 distinct inference rules spanning three logic types (significantly more than any previous study) to comprehensively evaluate the logical reasoning capabilities of LLMs, it can be further extended by incorporating additional inference rules and logic types.However, with respect to first-order logic and logics with quantified variables, there can be an infinite number of such rules.In this study, we focused solely on evaluating model performance using a single inference rule; however, an interesting future direction can be enhancing the depth of reasoning complexity (i.e., multi-step reasoning) by incorporating combinations of inference rules to derive conclusions.We also note that this research is limited to the English language and can be extended to multilingual scenarios for evaluating the logical reasoning ability of LLMs.</p>
<p>A Example Prompt for Sentence Generation</p>
<p>Below is the general prompt structure prompted to generate data.The prompt schema, as depicted in Figure 3 Definition provides a detailed explanation of the task and offers a natural language representation of the reasoning pattern for which we are generating sentences.</p>
<p>Examples provide sample sentences that need to be generated.We also illustrate how these sentences will be utilized in later stages, emphasizing the importance of coherence and the inclusion of relevant ontological concepts.</p>
<p>Format We provide specific formatting instructions to guide the generation of sentences.Figure 4 illustrates an example prompt for the inference rule, namely, 'modus tollens' from propositional logic (PL).Modus tollens is formally represented as ((p → q) ∧ ¬q) ⊢ ¬p, which can be understood in natural language as "If p implies q, and we know ¬q, then we can conclude ¬p."In this prompt, the definition provides a comprehensive description of the inference rule in natural language.To encourage the generation of more relevant and coherent sentences, the prompt includes an examples section that demonstrates how the generated sentences will be utilized in a later stage.This serves, as an illustration, to guide GPT-3 in producing suitable outputs.In Figure 4, we present three examples involving sentences p and q, along with their respective contexts and questions.The prompt also includes instructions on how the generated sentences should be formatted.</p>
<p>B Extended Related Work</p>
<p>As LLMs such as GPT-4, and Bard continue to evolve rapidly, it becomes increasingly crucial to</p>
<p>C Examples of Data Instances</p>
<p>This section provides examples of (context, question, answer) tuples corresponding to each inference rule and reasoning pattern.Additionally, it highlights the diverse range of question variations within the dataset associated with each inference rule and reasoning pattern.</p>
<p>C.1 Word Cloud</p>
<p>Figure 5 provides a word cloud derived from the LogicBench(Eval).This word cloud highlights the logical nature and diversity of our evaluation dataset.Words such as 'if', 'normally', 'usually', and 'then' are prominently featured, suggesting their frequent use in the dataset, and suggesting the logical nature of the dataset.Moreover, we can also observe several words consisting of different ontologies such as 'cat', 'car', 'garden', and many more, suggesting diversity in the dataset.</p>
<p>C.2 Propositional Logic (PL)</p>
<p>Here, we discuss examples of each inference rule present in the PL of the LogicBench as shown in Table 9.Table 9 has context related to the inference rule and different variations of the question according to the rule.For instance, the first row of Table 9 shows the example for inference rule, Hypothetical Syllogism (HS), formally expressed as ((p → q)) ∧ (q → r)) ⊢ (p → r).The context represents the premise, i.e., ((p → q)) ∧ (q → r)), and the first question (Q1) represents the conclusion, i.e., p → r.Hence, Q1 is labeled as "Yes" since it supports the conclusion given the logical context.Furthermore, Q2 to Q4 represent different variations of the question by utilizing the variables (p, ¬p, r, ¬r).For the HS, given the provided context, Q2 to Q4 contain the variations ¬p → r, p → ¬r, and ¬p → ¬r, respectively, and are labeled as "No" since they do not support the conclusion.</p>
<p>C.3 First-Order Logic (FOL)</p>
<p>Here, we discuss examples of each inference rule and two axioms (i.e., Existential Instantiation and Universal Instantiation) present in the FOL from the LogicBench as shown in Table 10.Existential Generalization (EG), formally expressed as P (a) ⇒ ∃xP (x) indicates that there is an element a in the domain for which P (a) is true, then we know that ∃xP (x) is true.Universal Instantiation formally expressed as ∀x A ⇒ A{x → a} indicates that a statement holds true for all instances (x) within a specific category A, hence it is also true for specific instance a.</p>
<p>Table 10 represents context related to the inference rule and variations of the question.The process of generating data instances for FOL follows a similar approach to that of PL.For example, the first row of Table 10 shows the example for axiom, Existential Instantiation (EI), formally expressed as ∃xP (x) ⇒ P (a).The context represents the initial premise ∃xP (x) and the first question (Q1) represents the conclusion, i.e., P (a).Hence, Q1 is labeled as "Yes" since it supports the conclusion given the logical context.Furthermore, we generate the only variant of the question based on ¬P (a) and labeled it as N o since it does not support the conclusion.</p>
<p>C.4 Non-Monotonic (NM) Reasoning</p>
<p>Here, we discuss examples of each reasoning pattern present in the NM reasoning from the Log-icBench as shown in Table 11.Table 11 has context related to the reasoning pattern and different variants of the question.For example, the first row of Table 11 shows the example for Default Reasoning with Irrelevant Information (DRI).For this reasoning, based on the given context, there are also two possible variations of the question where one with a correct conclusion labeled as Y es and another with an incorrect conclusion labeled as N o.</p>
<p>D Examples of NL Conversion</p>
<p>This section illustrates the way natural language logical context and questions are created using the generated sentence in Stage 1 in addition to prompt-based templatized-context to narrative conversion.Table 13 shows examples of how context and question are generated from sentences corresponding to each inference rule for PL and FOL.Similarly, Table 14 shows examples of NM reasoning.From Table 13, we can see an example of sentence pairs (p, q) and their corresponding negation pairs (¬p, ¬q) for the 'modus tollens' inference rule from PL.These pairs are utilized to generate templatized logical context which is converted to narrative using a specific prompt[1D] designed for PL.In addition to the narrative, the question and its variations are also created using these sentence pairs.Similarly, in the second row, we have four generic rules with variable x (p(x), q(x), r(x), s(x)) and their specific cases (i.e., x = a), along with their respective negative sentence pairs [(p(a), ¬p(a)), (q(a), ¬q(a)), (r(a), ¬r(a)), (s(a), ¬s(a))].In the same manner, as PL, these examples demonstrate the generation of narrative and questions for the FOL inference rule called 'Bidirectional Dilemma (BD)', as shown in Table 13 using specific prompt[2D] designed for FOL.From Table 14, the first row presents an example of narrative and questions generated from a sentence pair for the 'Default Reasoning with Irrelevant Information (DRI)' from NM reasoning.In this specific instance, the generated sentences are (p, q, r, s, t), and the negation is only required for the sentence t.Therefore, there is a single negation pair (t, ¬t), which is used to generate questions specific to the 'DRI'.Same as PL and FOL, templatized context is also converted to narrative using a prompt[3D] created for NM.</p>
<p>CD</p>
<p>If I decide to go for a walk, I will be able to breathe in some fresh air and revitalize myself.On the other hand, if I choose to stay home, I will have the opportunity to enjoy a movie.One thing is certain, either I go for a walk or I stay home.It remains uncertain which of the two options I will ultimately choose.It is entirely possible that I might opt for the walk, or perhaps I will find myself drawn to staying home, or even both possibilities might come to fruition.</p>
<p>Can we say at least one of the following must always be true?Q1: (a) I will get some fresh air or (b) I will watch a movie (Yes) Q2: (a) I won't get some fresh air and (b) I will watch a movie (No) Q3: (a) I will get some fresh air and (b) I won't watch a movie (No) Q4: (a) I won't get some fresh air and (b) I won't watch a movie (No) DD If I decide to order takeout, it means I will save time.On the other hand, if I choose to cook a meal, it means I will save money.The interesting thing is that I am in a situation where I won't be able to save time or money.It is uncertain whether I won't save time or I won't save money, or it could even be both scenarios.The only thing that is clear is that at least one of these possibilities is true.</p>
<p>Can we say at least one of the following must always be true?Q1:</p>
<p>BD</p>
<p>If it is sunny outside, then I will go for a walk.However, if it rains, I will stay inside.Currently, it is uncertain whether it is raining or not, but I do know that at least one of the following is true: either it is raining or I will not go for a walk.It is possible that only one of these statements is true, or perhaps both are true.</p>
<p>Can we say at least one of the following must always be true?Q1: (a) we will stay inside or (b) it is not sunny (Yes) Q2: (a) we will not stay inside and (b) it is sunny (No) Q3: (a) we will stay inside and (b) it is sunny (No) Q4: (a) we will not stay inside and (b)it is not sunny (No)</p>
<p>MT</p>
<p>If Mason decides to leave his job, he will not receive any salary.However, against all odds, Mason still receives his salary.He finds himself receiving his regular paycheck.</p>
<p>Q1: Does this infer that Mason didn't leave his job?(Yes) Q2: Does this infer that Mason left his job?(No) MI Rohan woke up in the morning and realized that he had forgotten his lunch.Knowing that if he forgets his lunch, he will not eat at school, he felt disappointed.</p>
<p>Based on context, can we say, at least one of the following must always be true?Q1: (a) Rohan didn't forget his lunch and (b) he will not eat at school (Yes) Q2: (a) Rohan forgot his lunch and (b) he will eat at school (No) Q3: (a) Rohan forgot his lunch and (b) he will not eat at school (No) Q4: (a) Rohan didn't forget his lunch and (b) he will eat at school (No) CT At least one of two things is true about Tom -he is either an avid reader or he devours books of all genres.We are unsure which one of these statements is true or if both are true.It could be that only the first statement is true, or only the second statement is true, or even that both are true.</p>
<p>Can we say at least one of the following must always be true?All students are required to take an examination in order to fulfill the requirements for their degree.Reema, being a student, is also expected to fulfill the requirements.</p>
<p>Q1: Does Reema need to take an exam to complete her degree?(Yes) Q2: Does Reema need not to take an exam to complete her degree?(No)</p>
<p>EG</p>
<p>The marathon race was won by James, who emerged as the champion.</p>
<p>Q1: Does this imply that someone won the marathon race?(Yes) Q2: Does this mean that no one won the marathon race?(No) MP If someone is extremely tired, then they will seek some rest and relaxation.Today, Jack finds himself utterly exhausted.</p>
<p>Q1: Does this entail that he will take rest?(Yes) Q2: Does this entail that he won't take rest?(No) HS If all the necessary supplies have been purchased by someone, then they can initiate the project.Once the project is started by someone, they will complete it within the expected time-frame.</p>
<p>Q1: If Lily bought all the necessary supplies, does this mean that she will finish it on time?(Yes) Q2: If Lily didn't buy all the necessary supplies, does this imply that she won't finish it on time?(No) Q3: If Lily bought all the necessary supplies, does this entail that she won't finish it on time?(No) Q4: If Lily didn't buy all the necessary supplies, does this imply that she will finish it on time?(No)</p>
<p>DS</p>
<p>It is known that one of the following options is true: someone goes to a museum or someone visits a park.The specific scenario could involve only the option to go to a museum being true, or only the option to visit a park being true, or both options being true.However, it is stated that Jill is unable to go to a museum.</p>
<p>Q1: Does this imply that she can visit a park? (Yes) Q2: Does this entail that she can't visit a park? (No)</p>
<p>CD</p>
<p>If someone is painting a picture, then they will frame it.</p>
<p>similarly, the natural course of action for a writer would be to publish their completed story. in this scenario, it is certain that at least one of the following statements holds true: (1) john is currently engrossed in painting a picture, or (2) john is currently immersed in the act of writing a story.it should be emphasized that we are unaware of which statement specifically applies, as there is a possibility that either (1) alone is true, or (2) alone is true, or even that both (1) and</p>
<p>(2) are simultaneously true.</p>
<p>Can we say at least one of the following must always be true?Q1: (a) he will frame it and (b) he will publish it.</p>
<p>DD</p>
<p>If someone is taking care of their health, then they will be fit.However, indulging in unhealthy habits can make individuals susceptible to various diseases.The truth is, we can be certain about at least one of the following possibilities: either Jenny won't be fit and healthy, or she won't be prone to diseases.It is important to note that we are unaware of which statement is accurate.It could be the case that only the first statement is true, only the second statement is true, or both statements hold validity.</p>
<p>Can we say at least one of the following must always be true?Q1: (a) Jenny doesn't take care of her health and (b) she doesn't indulge in unhealthy habits (Yes) Q2: (a) Jenny takes care of her health and (b) she indulges in unhealthy habits (No) Q3: (a) Jenny doesn't take care of her health and (b) she indulges in unhealthy habits (No) Q4: (a) Jenny takes care of her health and (b) she doesn't indulge in unhealthy habits (No)</p>
<p>BD</p>
<p>If an individual consumes a significant amount of water, they will experience a state of hydration.Conversely, if excessive amounts of sugar are ingested, a sugar crash will ensue.It is known that at least one of the following statements is true: either the Jane consumes ample water or she will not experience a sugar crash.However, the actual veracity of either statement remains ambiguous, as it could be the case that only the first statement is true, only the second statement is true, or both statements are true.</p>
<p>Can we say at least one of the following must always be true?Q1: (a) she will feel hydrated and (b) she doesn't eat too much sugar (Yes) Q2: (a) she won't feel hydrated and (b) she eats too much sugar (No) Q3: (a) she will feel hydrated and (b) she eats too much sugar (No) Q4: (a) she won't feel hydrated and (b) she doesn't eat too much sugar (No)</p>
<p>MT</p>
<p>If someone decides to go to the park, it is required that they wear a mask.However, in this particular situation, John does not wear a face covering.</p>
<p>Q1: Does this imply that John doesn't visit the park? (Yes) Q2: Does this entail that John visits the park? (No) Once upon a time, in a land filled with animals, there were two popular mammalian creatures, cats and dogs.Mammals typically possessed a coat of fur, which kept them warm and protected.However, cats were an exception to this rule, as their bodies lacked fur.Nonetheless, both cats and dogs were beloved by many for their unique traits.Dogs, known for their loyalty, were particularly cherished by humans.</p>
<p>Q1: Does this imply that dogs have fur?(Yes) Q2: Does this entail that dogs don't have fur?(No)</p>
<p>DRS</p>
<p>John and Mary were expecting their first child, filled with the anticipation and excitement that all parents feel.Parents are usually loving and supportive.Parents are normally responsible.However, something seemed amiss in their relationship.Mary, usually affectionate and caring, seemed distant and uninvolved.On the other hand, John, known for his responsible nature, started neglecting his duties and became unreliable.</p>
<p>Q1: Does this imply that Mary is responsible and John is loving and supportive?(Yes) Q2: Does this entail that Mary isn't responsible and John is loving and supportive?(No) Q3: Does this imply that Mary is responsible and John isn't loving and supportive?(No) Q4: Does this entail that Mary isn't responsible and John isn't loving and supportive?(No) DRD Jenny and Anna are known for their tall stature, which is often associated with playing basketball.However, Anna might be an exception to this norm.</p>
<p>Q1: Does this entail that Jenny plays basketball?(Yes) Q2: Does this mean that Jenny doesn't play basketball?(No)</p>
<p>DRO</p>
<p>In the bird kingdom, there are many different species that possess unique characteristics.One such species is the hummingbird, known for its ability to hover in mid-air and its vibrant colors.While most birds engage in the annual migration south for the winter, the hummingbird chooses to stay put and brave the cold weather.This decision sets the hummingbird apart from its fellow avian companions, as it relies on its resilience and resourcefulness to survive the harsh conditions.</p>
<p>Q1: Does this mean that all other birds than hummingbirds migrate south for the winter?(Yes) Q2: Does this mean that all other birds than hummingbirds don't migrate south for the winter?(No)</p>
<p>RE1</p>
<p>In a world where animals are often regarded as intelligent creatures, there is a captivating tale that revolves around cats, dogs, and horses.It is commonly believed that most animals possess a level of intellect.However, there is an intriguing twist to this belief as it is known that either cats or dogs are not considered particularly intelligent.As the story unfolds, we delve into the lives of these remarkable creatures, their interactions, and the unique qualities that each of them possesses.</p>
<p>Q1: Does this entail that horses are considered to be intelligent creatures and exactly one of the cats or dogs is not considered intelligent?(Yes) Q2: Does this mean that horses aren't considered to be intelligent creatures and exactly one of cats or dogs is not considered intelligent?(No) Q3: Does this mean that horses are considered to be intelligent creatures and exactly one of cats or dogs is considered intelligent?(No) Q4: Does this implies that horses aren't considered to be intelligent creatures and exactly one of cats or dogs is considered intelligent?(No)</p>
<p>RE2</p>
<p>In the realm of cat communication, meowing serves as a fundamental aspect of their vocal repertoire.However, intriguingly enough, there exists a distinct species of cat that deviates from this conventional norm.This peculiar feline defies the expectations associated with its kind by refraining from emitting any meows whatsoever.</p>
<p>Q1: Does this entail that exactly one species of cat doesn't meow?(Yes) Q2: Does this imply that exactly one species of cat meows?(No)</p>
<p>RE3</p>
<p>In a world where cars were known for having four wheels, it was considered a common fact that wheels typically came equipped with spokes.However, amidst this widespread understanding, there was an exception.At least one wheel defied this norm and stood out from the rest by not having any spokes at all.</p>
<p>Q1: Does this imply that cars have four wheels with spokes?(Yes) Q2: Does this mean that cars don't have four wheels with spokes?(No)</p>
<p>RAP</p>
<p>In the midst of a heated argument, John adamantly claims that sally was present at the store.However, Jane strongly opposes John's assertion, insisting that Sally was indeed absent from the store.</p>
<p>Q1: If John's evidence is more reliable than Jane's, does this mean that Sally was in the store?(Yes) Q2: If John's evidence is more reliable than Jane's, does this mean that Sally wasn't in the store?(No) Q3: If John's evidence is less reliable than Jane's, does this entail that Sally was in the store?(No) Q4: If John's evidence is less reliable than Jane's, does this imply that Sally wasn't in the store?(Yes) Each prompt is shown below for PL, FOL, and NM templatized context to narrative conversion.</p>
<p>Prompt for templatized to narrative conversion for PL(MT):</p>
<p>Rule: Condition: If p then q; Situation: not q Context: Templatized context Improve the context in human-like language and make story with rephrased sentences.Instructions for generating a good story: 1.While generating a story, Use the rephrased sentences from the context of the story.2. Make sure to include sentences corresponding to the condition and situation from the rule in the story.3. Do not add any other extra information.4. To generate a story, DO NOT change the name of the main character from the context, if there's any.If the main character within the context is 'I', please generate a story with 'I' as the main character.5. Generate only one paragraph with rephrased sentences.</p>
<p>Prompt for templatized to narrative conversion for FOL:</p>
<p>Context: Templatized context Improve the context in human-like language and make a rephrase sentences around it.Instructions for generating a good story: 1. Make sure to include only rephrased sentences in the story.2. Do not add any other extra information and do not reach any conclusion based on context.3. To generate a story, use only generic sentences without mentioning any main character.4. Extract a specific case sentence from the context, rephrase it, and append it at the end of the context only. 5. Generate only one paragraph with rephrased sentences.6. DO NOT add any prefixes.</p>
<p>Prompt for templatized to narrative conversion for NM:</p>
<p>Context: Templatized context Improve the context in human-like language and rephrase sentences around it.Instructions for generating a good story: 1. Make sure to include only rephrased sentences in the story.2. Do not add any other extra information, but add some story-like context.3. To generate a story, DO NOT change the name of the main character from the context, if there's any.Use the same names/words for main character.If the main character within the context is 'I', please generate a story with 'I' as the main character.4. Generate only one paragraph with rephrased sentences.</p>
<p>To ensure the quality of the narrative in the Log-icBench(Eval) for task 1 and task 2, we have created category-specific prompts to convert the templatized context to convert more human-like narrative.In total, we have created three different prompts each for PL[1D], FOL[2D], and NM[3D.The prompts are designed to ensure that the logical connection is established in the narrative.Each rule of PL, FOL, and NM has a unique logical progression that should be followed in the narrative is the motivation for us to go with three different instruction-based prompts.In the prompt, used for PL rules, the logical rule is also mentioned in two parts "Condition" and "Situation".Consider the example of logical rule MT -"If p then q; not q; therefore not p", the templatized context will have "If p then q; not q;" so condition will be the first part "If p then q" while the situation will be the second part "not q".In other terms, the specific conditions are the rules to be followed while the situation is the case present in the logical rule.In contrast, for FOL, we do not need to focus more on the specific condition rather we have to make sure a generalized case is present in the narrative with 1-2 specific sentences related to the rule.For NM, we do not have such a rule instead we have a logical connection between sentences and hence we go with the instruction-based prompting.</p>
<p>E Task Instance Generation</p>
<p>This section discusses the task instance generation step, which is the last step in Fig. 2 in detail.The overall section is comprised of two subsections for BQA and MCQA.The subsection related to BQA discusses the variation generation step in detail with question generation while the MCQA subsection provides details of prompts used for incorrect option generation along with five pre-defined sets of questions.</p>
<p>E.1 Task 1-BQA (variation generation)</p>
<p>As seen in Fig. 2, BQA has a narrative along with question, and answer pairs with variation in question and corresponding answer.As discussed in the main paper Data creation3.2,the narrative is created using the pre-defined rules for PL, FOL, and NM while a question is asked based on what can be entailed from the given context.For example, HS can be defined as "If p then q; if q then r; therefore, if p then r" and the narrative will have "If p then q; if q then r" and the question is asked as "Can we conclude if p then r?" having answer as "yes".Now, other variations in the question are asked in the following ways by negating the sentence (p and r) comb: Variation 1: "Can we conclude if ¬p then r?", Variation 2: "Can we conclude if p then ¬r?", Variation 3: "Can we conclude if ¬p then ¬r?".If there's only one axiom (p) present in the question then there are only a 2 variations that can be made asking about (p) and (¬p).</p>
<p>E.2 Task 2-MCQA (Question selection and incorrect option generation)</p>
<p>We have MCQA as task 2 in LogicBench.In this, we have one correct option from the four options, and three options are incorrectly generated using prompting.The question is a bit different from the BQA question formation as we have MCQA and have to identify which conclusion can be derived from the narrative.The question is randomly selected from the pre-defined set of questions.Here, the five different questions are as follows:</p>
<ol>
<li>What would be the most appropriate conclusion based on the given context?The narrative for MCQA is the same as the BQA's narrative and there's no change.Comparing BQA methodology with MCQA, MCQA's correct option is the question asked in the BQA which can be concluded from the logical rule present in the narrative.For example, HS can be defined as "If p then q; if q then r; therefore, if p then r" and the narrative will have "If p then q; if q then r" and the question is randomly chosen from the set of questions as mentioned above and the correct option from the different multiple choice will be "if p then r".Based on the information present, we have generated an incorrect option using prompting</li>
</ol>
<p>F Mitigation of Errors in LogicBench</p>
<p>While validating LogicBench(Eval), we encountered errors within the synthetically generated narrative-based context.We mitigate these errors manually, categorizing them into two groups: (i) eliminating leakage of logical conclusions and (ii) ensuring the inclusion of intended logical premises.</p>
<p>In the first category, we found ∼ 15% of the narrative-based contexts (out of 500 total instances) were found to explicitly present the logical conclusion as a response to the question, bypassing the logical derivation process.This enables the model to extract the final logical conclusion from the context rather than derive it logically.In the second category, we found ∼ 8% of the narrative-based contexts (out of 500 total instances) where narration lacked some necessary premise sentences crucial for reaching a logical conclusion.To address this issue, we manually incorporated those sentences to ensure the quality of the data.For the MCQA task, during the generation of three incor-rect options, we found instances where the model produces two semantically similar options, resulting in the creation of ambiguous choices in ∼ 11% of the cases (out of a total of 500 instances).We manually mitigated all these errors from the data instances ensuring the high quality of our validation data.We believe that these two versions aim to accommodate different evaluations to explore the logical reasoning capabilities of LLMs.</p>
<p>G Few-shot Experiments</p>
<p>This section discusses the performance of the different LLMs in a few-shot setting on the Log-icBench(Eval) BQA .Here, we only present a case study on the BQA task.For the fair comparison with Table 6, we analyze an average performance across A(Y es).15, we can observe that incontext examples are helpful for Llama-2 since it consistently outperforms zero-shot baselines by large margins in terms of A(Y es).Llama-2 is remarkably good at following the in-context exemplars and mimicking the process to reach the correct conclusions.Thus, leveraging the in-context exemplars, Llama-2 achieves high accuracy in a few-shot setting.Specifically, Mistral consistently shows degraded performance for all logic types.However, ChatGPT improves performance on NM reasoning.Improved performance in NM reasoning demonstrates that the inclusion of in-context examples enhances the ability of these models to comprehend the nuanced meanings of logical terms such as "usually" and "typically".In particular, we see that Gemini and GPT-4 improve performance on PL and FOL, respectively, but show competitive performance on NM.</p>
<p>H Experimental Setup</p>
<p>H.1 Extended Discussion on Experiments Zero-shot setting We evaluate GPT-4, and Chat-GPT (GPT-3.5-Turbo)by utilizing their APIs provided by OpenAI3 .We evaluate Google Gemini-Pro by utilizing its API provided by Google4 The evaluation is conducted on the versions of GPT-4, ChatGPT, and Gemini released in January 2024.It's important to note that these models are regularly updated, so when reproducing the results presented in Table 6 and Table 7 (main paper), there is a possibility of variations.For Llama-2 and Mistral, we utilize the 7B-Chat, and 7B-Instruct-v0.2versions, respectively, from the huggingface model repository5 .</p>
<p>Experiments on other logic datasets In single and multi-task experiments on other logic datasets, we fine-tune the T5-large model for 10 epochs with a batch size of 16, 1024 maximum input length, an adaptive learning rate of 5e − 05, and an AdamW optimizer for each experiment.All experiments are performed using NVIDIA RTX A6000 GPUs.</p>
<p>H.2 Prompts</p>
<p>All the experiments conducted in the zero-shot setting were performed using three distinct prompts.The reported results in Table 6 (main paper) represent the average performance across these prompts.All the prompts follow the common pattern which includes task description and formatting instructions.The following are the three different prompts utilized in the experiments:</p>
<p>Prompt 1:</p>
<p>Given the context and question, think stepby-step logically to answer the question.Effect on other logic datasets Our experiments were carried out in two settings: single-task (finetuning and evaluation on one dataset) and multitask (fine-tuning on all four datasets combined, with separate evaluations for each dataset).App.H describes a detailed experimental setup.Table 12 represents the accuracy comparison between Log-icT5 and baseline T5-large in both single-task and multi-task settings.</p>
<p>The results indicate that training LLMs on Log-icBench(Aug) has a greater impact on logic datasets that primarily focus on logical reasoning, such as FOLIO and LogicNLI.Hence, we can observe that LogicT5 consistently outperforms the baseline for LogicT5 and FOLIO.However, LogiQA and Re-Clor encompass other forms of reasoning in addition to logical reasoning, hence, LogicT5 demonstrates competitive performance on them.</p>
<p>LLMs sometimes overlook contextual information In this case study, we manually examined all reasoning chains for PL and NM generated by GPT-4.The example below illustrates the observed Context: If an individual consumes a significant amount of water, they will experience a state of hydration.Conversely, if excessive amounts of sugar are ingested, a sugar crash will ensue.It is known that at least one of the following statements is true: either the Jane consumes ample water or she will not experience a sugar crash.However, the actual veracity of either statement remains ambiguous, as it could be the case that only the first statement is true, only the second statement is true, or both statements are true.</p>
<p>Question: Can we say at least one of the following must always be true?(a) she will feel hydrated and (b) she doesn't eat too much sugar MP p(x): someone is exhausted.q(x): they will take a rest.p(a): Jack is exhausted.∼p(a): Jack is not exhausted.q(a): he will take a rest.∼q(a): he will not take a rest.</p>
<p>Context: If someone is extremely tired, then they will seek some rest and relaxation.Today, Jack finds himself utterly exhausted.</p>
<p>Question: Does this entail that he will take rest?DS p: Levi is not studying for his exams ∼p: Levi is studying for his exams q: Maya is writing a book ∼q: Maya is not writing a book Context: Either Levi is not studying for his exams or Maya is writing a book, or maybe both.It was unclear which choice to make as he didn't know if either of the options was true.However, it turned out that Levi decided to prioritize his exams and focus on studying.He knew that in order to succeed, he needed to dedicate his time and energy to preparing for the upcoming tests.</p>
<p>Question: Does this mean that Maya is writing a book?HS p(x): someone buys all the necessary supplies.q(x): they can start the project.r(x): they will finish it on time.p(a): Lily bought all the necessary supplies.∼p(a): Lily did not buy all the necessary supplies.q(a): She can start the project.∼q(a): She can not start the project.s(a): She will finish it on time.∼s(a): She will not finish it on time.</p>
<p>Context: If all the necessary supplies have been purchased by someone, then they can initiate the project.Once the project is started by someone, they will complete it within the expected timeframe.</p>
<p>Question: If lily bought all the necessary supplies, does this mean that she will finish it on time?CD p: Harry goes to the park.∼p: Harry does not go to the park.q: he will have a picnic with his family ∼q: he will not have a picnic with his family r: he goes to the beach ∼r: he does not go to the beach s: he will swim in the ocean ∼s: he will not swim in the ocean Context: Harry had two options for his day off: going to the park or going to the beach.If he decided to go to the park, it meant that he would have a lovely picnic with his family.On the other hand, if he chose the beach, he would be able to enjoy a refreshing swim in the ocean.The only thing that was certain was that either Harry would go to the park or he would go to the beach.It was unclear which option he would ultimately choose, as it could be that he would go to the park alone, go to the beach alone, or even do both activities.</p>
<p>Question: Can we say at least one of the following must always be true?(a) he will have a picnic with his family and (b) he will swim in the ocean DD p: I order takeout ∼p: I did not order takeout q: I will save time ∼q: I will not save time r: I cook a meal ∼r: I did not cook a meal s: I will save money ∼s: I will not save money Context: If I decide to order takeout, it means I will save time.On the other hand, if I choose to cook a meal, it means I will save money.The interesting thing is that I am in a situation where I won't be able to save time or money.It is uncertain whether I won't save time or I won't save money, or it could even be both scenarios.The only thing that is clear is that at least one of these possibilities is true.</p>
<p>Question: Can we say at least one of the following must always be true?(a) i don't order takeout and (b) i don't cook a meal CT p: Tom is an avid reader ∼p: Tom is not an avid reader q: he devours books of all genres ∼q: he does not devour books of all genres Context: At least one of two things is true about Tom -he is either an avid reader or he devours books of all genres.We are unsure which one of these statements is true or if both are true.It could be that only the first statement is true, or only the second statement is true, or even that both are true.</p>
<p>Question: Can we say at least one of the following must always be true?(a) he devours books of all genres or (b) tom is an avid reader MI p: he is not eating healthy ∼p: he is eating healthy q: he will not gain weight ∼q: he will gain weight Context: Once upon a time, there was a guy named John who was very conscious about his weight.He knew that if he didn't eat healthy, he wouldn't be able to maintain his desired weight.John understood the condition that if he didn't eat healthy, he wouldn't be able to gain weight.</p>
<p>Question: Based on context, can we say, at least one of the following must always be true?(a) he is eating healthy and (b) he will not gain weight EG p(x): someone has coding skills a: Sheila p(a): Sheila is a proficient programmer Context: Sheila possesses exceptional coding skills and is highly proficient in programming.She has mastered various programming languages and is known for her ability to write efficient and error-free code.Sheila's expertise in programming allows her to solve complex problems with ease.Her colleagues often seek her assistance when facing technical challenges as she consistently delivers high-quality solutions.</p>
<p>Question: does this mean that someone has coding skills?UI p(x): students need to take an exam to complete their degree a: Reema p(a): Reema is a student.</p>
<p>Context: All students are required to take an examination in order to fulfill the requirements for their degree.Reema, being a student, is also expected to fulfill the requirements.</p>
<p>Question: Does reema need to take an exam to complete her degree?</p>
<p>Table 13: Illustrative examples of NL logical context and questions created using sentences that are generated in §3.2.1 for rules covered in PL and FOL for BQA. 4. The absence of the result (catching a cold) does not necessarily mean the condition (forgetting to wear a sweater) did not occur, as there could be other factors at play that prevented James from catching a cold. 5. Therefore, from the information given, we cannot infer with certainty that James remembered to wear a sweater.</p>
<p>FOL(DD)</p>
<p>Context: if an individual operates a vehicle with caution, they will prevent any accidents from occurring.however, if they decide to accelerate and exceed the speed limit, they will be issued a fine.there is a certainty that at least one of the following statements is true: either john will not be able to avoid an accident or he will not face a penalty.it is unclear which of these statements is accurate, as it could be only one of them, or it is possible that both of them hold true.Question: can we say at least one of the following must always be true?(a) john doesn't drive carefully and (b) he doesn't speed up Yes/No 1.According to the context, if an individual operates a vehicle with caution, they will prevent any accidents from occurring.2. Also, if they decide to accelerate and exceed the speed limit, they will be issued a fine.3. It is stated that there is a certainty that at least one of the following statements is true: either John will not be able to avoid an accident or he will not face a penalty.4. It is unclear which of these statements is accurate, as it could be only one of them, or it is possible that both of them hold true. 5.The question asks if we can say at least one of the following must always be true: (a) John doesn't drive carefully and (b) he doesn't speed up.6.Based on the given information, we cannot say that at least one of the statements (a) and (b) must always be true.</p>
<p>FOL(DD)</p>
<p>Context: if someone is taking care of thier health, then they will be fit.however, indulging in unhealthy habits can make individuals susceptible to various diseases.the truth is, we can be certain about at least one of the following possibilities: either jenny won't be fit and healthy, or she won't be prone to diseases.it is important to note that we are unaware of which statement is accurate.it could be the case that only the first statement is true, only the second statement is true, or both statements hold validity.Question: can we say at least one of the following must always be true?(a) jenny takes care of her health and (b) she doesn't indulge in unhealthy habits</p>
<p>No/Yes 1.According to the context, if someone takes care of their health, they will be fit.2. Indulging in unhealthy habits can make individuals susceptible to various diseases.</p>
<ol>
<li>The context states that we can be certain about at least one of the following possibilities: either Jenny won't be fit and healthy or she won't be prone to diseases.4. It is important to note that we are unaware of which statement is accurate.5.The possibilities are: a) Jenny takes care of her health and is fit.b) Jenny doesn't indulge in unhealthy habits and is not prone to diseases.c) Both statements hold validity.6.Therefore, at least one of the following must always be true: (a) Jenny takes care of her health and (b) she doesn't indulge in unhealthy habits.</li>
</ol>
<p>Figure 1 :
1
Figure 1: Comprehensive representation of different inference rules and reasoning patterns covered by propositional, first-order, and non-monotonic logics.Exp.indicates Expectation</p>
<p>∧ ¬S 2 ) ⊢ ¬S 1 Context: If S 1 , then S 2 .We know that ¬S 2 .Question: Does this context imply ¬S 1 ?Prompt-based NL Converter Context: NL story comprising MT Question: Does this context imply ¬S 1 ?Question: Does this context imply ¬S 1 ?Answer: Yes Context: NL story comprising MT Question: Does this context imply S 1 ?Liam finished his work early.¬S: Liam didn't finish his work early.</p>
<p>Figure 2 :
2
Figure 2: Schematic representation of three-stage procedure for data creation.NL: Natural Language</p>
<p>Figure 3 :
3
Figure 3: Schematic representation of prompt.</p>
<p>Figure 5 :
5
Figure 5: Word cloud of context present in the LB(eval)</p>
<p>(a) I don't order takeout or (b) I don't cook a meal (Yes) Q2: (a) I order takeout and (b) I cook a meal (No) Q3: (a) I don't order takeout and (b) I cook a meal (No) Q4: (a) I order takeout and (b) I don't cook a meal (No)</p>
<p>Q1: (a) he devours books of all genres or (b) Tom is an avid reader (Yes) Q2: (a) he doesn't devour books of all genres and (b) Tom is an avid reader (No) Q3: (a) he devours books of all genres and (b) Tom isn't an avid reader (No) Q4: (a) he doesn't devour books of all genres and (b) Tom isn't an avid reader (No) Table 9: Examples of context and question-answer pairs for each rule of Proportional logic from the LogicBench; HS: Hypothetical Syllogism, DS: Disjunctive Syllogism, CD: Constructive Dilemma, DD: Destructive Dilemma, BD: Bidirectional Dilemma, MT: Modus Tollens, MI: Material Implication, CT: Commutation Rule Context Question UI</p>
<p>(Yes) Q2: (a) he won't frame it and (b) he will publish it.(No) Q3: (a) he will frame it and (b) he won't publish it.(No) Q4: (a) he won't frame it and (b) he won't publish it.(No)</p>
<p>Table 2 :
2
Inference rules and (two) axioms that establish the relationship between premises and conclusions.MP: Modus Ponens, MT: Modus Tollens, HS: Hypothetical Syllogism, DS: Disjunctive Syllogism, CD: Constructive Dilemma, DD: Destructive Dilemma, BD: Bidirectional Dilemma, CT: Commutation, MI: Material Implication, EG: Existential Generalization, UI: Universal Instantiation</p>
<p>Table 3 :
3
Illustrative examples of non-monotonic reasoning adapted from</p>
<p>Table 4
4: Illustrative examples of logical context and questions created using sentences that are generated in the firststage ( §3.2.1) for both tasks, i.e., BQA and MCQA.sions are presented in Table 5. For Log-icBench(Eval) BQA , out of 1520, 520 samples arefor 'yes' and 1000 samples are for 'no' labels. ForLogicBench(Eval) M CQA , there are 20 unique sam-ples present for each rule, thus in total 500 uniquesamples. Furthermore, we synthetically augmentedLogicBench(Eval) for training purposes (i.e., Log-icBench(Aug)) which consists of 150 unique datasamples for each rule for BQA, resulting in a totalof 12908 data samples including variations.Dataset# of Instances per AxiomTotal # of InstancesTotal # of Instances (Including Variations)LogicBench(Eval)BQA205001520LogicBench(Eval)MCQA20500500</p>
<p>Table 5 :
5
Statistics of LogicBench(Eval)</p>
<p>Avg 84.42 0.07 50.39 0.03 82.44 0.02 73.66 0.06 86.63 0.05 67.17 0.04 84.24 0.04 69.20 0.05 84.75 0.02 83.19 0.02 Avg 57.78 0.09 43.74 0.04 71.00 0.03 67.490.05 67.78 0.04 69.71 0.07 61.47 0.03 65.46 0.10 74.43 0.02 84.75 0.02
Type RulesLlama-2MistralGeminiChatGPTGPT-4A(N o)A(Y es)A(N o)A(Y es)A(N o)A(Y es)A(N o)A(Y es)A(N o)A(Y es)HS100 0.0047.35 0.0398.63 0.0164.24 0.0799.31 0.0159.60 0.01100 0.0066.82 0.04100 0.0084.75 0.06DS44.81 0.0556.82 0.0250.77 0.0377.66 0.0568.21 0.0491.65 0.0451.26 0.0480.71 0.0979.96 0.06100 0.00CD79.94 0.0325.47 0.0183.37 0.0372.30 0.1285.56 0.0428.15 0.0190.75 0.0638.63 0.0492.85 0.0166.12 0.05PLDD BD82.22 0.16 48.89 0.4325.22 0.01 25.13 0.0171.26 0.02 79.68 0.0114.16 0.03 47.36 0.1175.48 0.04 86.81 0.0225.22 0.02 29.51 0.0171.28 0.05 84.91 0.0523.47 0.03 33.47 0.0584.51 0.02 87.86 0.0442.40 0.04 59.06 0.13MT70.54 0.0371.46 0.0547.96 0.0844.96 0.1075.99 0.0581.47 0.0355.63 0.0266.11 0.0755.28 0.0459.05 0.07MI78.57 0.2625.34 0.0175.36 0.0125.51 0.0474.69 0.0624.84 0.0481.60 0.0231.79 0.0391.84 0.0139.72 0.03CT70.00 0.1224.98 0.0287.38 0.0171.99 0.0388.31 0.0835.11 0.0789.88 0.0543.33 0.0398.59 0.0160.71 0.06Avg71.87 0.13 37.72 0.02 74.30 0.03 52.27 0.07 81.79 0.04 46.94 0.03 78.16 0.04 48.04 0.05 86.36 0.03 63.98 0.05EG100 0.0071.67 0.03100 0.00100 0.0085.68 0.0697.97 0.0496.74 0.0396.75 0.0398.41 0.03100 0.00UI80.24 0.0556.04 0.0285.24 0.0485.24 0.0490.69 0.0890.29 0.0485.31 0.0296.19 0.03 90.16 0.002 91.58 0.03MP97.44 0.0473.01 0.0496.28 0.0395.24 0.0598.25 0.0396.97 0.05100 0.0091.03 0.0494.74 0.00100 0.00HS100 0.0036.83 0.0198.02 0.0065.00 0.0597.70 0.0052.46 0.0495.14 0.0046.74 0.0489.03 0.0161.21 0.01FOLDS58.33 0.1452.08 0.0461.69 0.0392.59 0.1375.96 0.0785.82 0.0772.46 0.1094.71 0.0563.34 0.04100 0.00CD100 0.0026.36 0.0182.21 0.0282.02 0.0994.71 0.0233.62 0.0287.87 0.0639.37 0.0692.65 0.0174.72 0.05DD54.37 0.1923.36 0.0173.78 0.016.67 0.1182.70 0.0228.96 0.0172.70 0.0826.42 0.1185.54 0.0454.55 0.08BD92.31 0.1325.78 0.0175.76 0.0144.44 0.1082.82 0.0828.93 0.0585.50 0.0338.82 0.0284.22 0.0266.67 0.10MT77.11 0.0588.41 0.0969.01 0.0691.71 0.0171.18 0.1089.58 0.0462.46 0.0392.80 0.0664.69 0.04100 0.00DRI29.06 0.0540.98 0.0461.51 0.0572.22 0.0565.45 0.0378.50 0.0255.77 0.0088.89 0.0981.29 0.05100 0.00DRS66.56 0.0318.40 0.0170.13 0.016.20 0.0168.05 0.017.20 0.0669.06 0.010.00 0.0077.59 0.0239.32 0.06DRD49.21 0.0650.47 0.0477.64 0.0297.78 0.0468.10 0.0597.43 0.0455.11 0.02100 0.0086.96 0.00100 0.00NMDRO RE153.25 0.01 94.07 0.0653.60 0.02 27.55 0.0168.30 0.04 78.79 0.0289.56 0.11 44.39 0.0357.85 0.04 84.04 0.0474.03 0.08 41.98 0.1250.85 0.01 75.82 0.0066.67 0.58 31.74 0.0355.56 0.00 85.44 0.02100 0.00 100 0.00RE254.17 0.4753.62 0.0581.31 0.0485.67 0.0358.48 0.0382.22 0.1764.67 0.1160.68 0.0259.42 0.00100 0.00RE339.05 0.0338.05 0.1078.61 0.0483.59 0.0164.67 0.0879.68 0.0259.05 0.0483.57 0.0883.12 0.0289.08 0.01RAP76.82 0.0567.22 0.0351.73 0.0260.48 0.1175.58 0.0496.67 0.0661.44 0.0292.16 0.0366.07 0.0598.72 0.02</p>
<p>Table 6 :
6
Evaluation of LLMs in terms of label-wise accuracy on LogicBench(Eval) BQA , where A(Y es) and A(N o) denote the accuracy for the Y es and N o labels, respectively.DRI: Default Reasoning with Irrelevant Information, DRS: Default Reasoning with Several Defaults, DRD: Default Reasoning with a Disabled Default, DRO: Default Reasoning in an Open Domain, RE1: Reasoning about Unknown Expectations I, RE2: Reasoning about Unknown Expectations II, RE3: Reasoning about Unknown Expectations III, RAP: Reasoning about Priorities</p>
<p>Table 7 :
7
Evaluation of LLMs in terms of accuracy on LogicBench(Eval) M CQA .
Type Rules Llama-2MistralGeminiChatGPTGPT-4HS86.670.0893.330.061000.0091.670.031000.00DS63.330.1260.000.0986.670.1096.670.0695.000.00CD80.000.0570.000.1396.670.0390.000.001000.00PLDD BD43.330.03 51.670.0330.000.10 53.330.0390.000.05 86.670.0373.330.06 68.330.0688.330.03 83.330.03MT31.670.0360.000.0578.330.0873.330.0876.670.03MI33.330.1335.000.0571.670.0663.330.0873.330.14CT73.330.1068.330.081000.0098.330.031000.00Avg57.920.07 58.750.07 88.750.04 81.880.05 89.580.03EI80.000.0085.000.0595.000.0093.330.031000.00UI63.330.0375.000.0598.330.0391.670.0398.330.03MP85.000.0998.330.031000.001000.001000.00HS61.670.0670.000.0581.670.0373.330.0676.670.03FOLDS43.330.0636.670.0370.000.0578.330.1095.000.05CD75.000.0561.670.0693.330.0380.000.0591.670.03DD36.670.0646.670.0385.000.0571.670.0693.330.03BD35.000.0543.330.0678.330.0666.670.1391.670.06MT41.670.0366.670.0381.670.0686.670.0686.670.03Avg57.960.05 64.810.04 87.040.03 82.410.06 91.510.04DRI38.330.0328.330.0658.330.0866.670.0690.000.00DRS41.670.0816.670.1045.000.1041.670.1055.000.05DRD55.000.0050.000.0548.330.0371.670.1080.000.05NMDRO RE121.670.03 51.670.0321.670.03 31.670.0853.330.03 70.000.0038.330.08 65.000.0545.000.00 95.000.05RE265.000.0575.000.0068.330.0861.670.0366.670.06RE331.670.0333.330.0361.670.0870.000.0568.330.03RAP46.670.0835.000.0933.330.0355.000.0551.670.03Avg43.960.04 36.460.05 54.790.05 58.750.07 68.960.03</p>
<p>Table 8 :
8
Human performance on three logic types averaged across three annotators for both tasks.</p>
<p>Arkil Patel, Satwik Bhattamishra, and Navin Goyal.2021.Are NLP models really able to solve simple math word problems?In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080-2094, Online.Association for Computational Linguistics.Oyvind Tafjord, Bhavana Dalvi, and Peter Clark.2021.ProofWriter: Generating implications, proofs, and abductive statements over natural language.In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621-3634, Online.Association for Computational Linguistics.
Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaella Bernardi, and Roberto Zampar-elli. 2014. The SICK (Sentences Involving Com-positional Knowledge) dataset for relatedness andentailment.R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2019.Right for the wrong reasons: Diagnosing syntactic Oyvind Tafjord, Matt Gardner, Kevin Lin, and PeterPruthvi Patel, Swaroop Mishra, Mihir Parmar, andheuristics in natural language inference. Clark. 2019b. QuaRTz: An open-domain dataset ofChitta Baral. 2022. Is a question decomposition unit all we need? In Proceedings of the 2022 Confer-ence on Empirical Methods in Natural Language Processing, pages 4553-4569, Abu Dhabi, Unitedqualitative relationship questions. In Proceedings of Sewon Min, Mike Lewis, Luke Zettlemoyer, and Han-the 2019 Conference on Empirical Methods in Natu-naneh Hajishirzi. 2021. Metaicl: Learning to learn in ral Language Processing and the 9th International context. arXiv preprint arXiv:2110.15943. Joint Conference on Natural Language ProcessingArab Emirates. Association for Computational Lin-guistics.(EMNLP-IJCNLP), pages 5941-5946, Hong Kong, Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2022a. Reframing China. Association for Computational Linguistics.Mohammed Saeed, Naser Ahmadi, Preslav Nakov, and Paolo Papotti. 2021. RuleBERT: Teaching soft rules to pre-trained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natu-ral Language Processing, pages 1460-1476, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-instructional prompts to GPTk's language. In Find-ings of the Association for Computational Linguistics: ACL 2022, pages 589-612, Dublin, Ireland. Associa-tion for Computational Linguistics. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Cross-task generaliza-tion via natural language crowdsourcing instructions. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A ques-tion answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Association for arXiv preprint arXiv:2104.08773. Computational Linguistics.ula, and Yejin Choi. 2021. Winogrande: An adver-sarial winograd schema challenge at scale. Commu-nications of the ACM, 64(9):99-106.Victor Sanh, Albert Webson, Colin Raffel, Stephen HBach, Lintang Sutawika, Zaid Alyafeai, AntoineChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,et al. 2021. Multitask prompted training enablesJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2021a. Logiqa: a challenge dataset for machine reading comprehen-zero-shot task generalization. ICLR. Abulhair Saparov and He He. 2023. Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. In The Eleventh International Conference on Learning Representations.Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-Swaroop Mishra and Elnaz Nouri. 2023. HELP ME THINK: A simple prompting strategy for non-experts to create customized content with models. In Find-ings of the Association for Computational Linguistics: ACL 2023, pages 11834-11890, Toronto, Canada. As-sociation for Computational Linguistics. bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open founda-tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288.sion with logical reasoning. In Proceedings of the Twenty-Ninth International Conference on Interna-tional Joint Conferences on Artificial Intelligence, pages 3622-3628. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2021b. Pre-train, prompt, and predict: A systematic survey of Timo Schick and Hinrich Schütze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Asso-ciation for Computational Linguistics: Main Volume, pages 255-269, Online. Association for Computa-tional Linguistics.Mutsumi Nakamura, Santosh Mashetty, Mihir Parmar, Neeraj Varshney, and Chitta Baral. 2023. LogicAt-tack: Adversarial attacks for evaluating logical con-Bertie Vidgen, Dong Nguyen, Helen Margetts, Patri-cia Rossini, and Rebekah Tromble. 2021. Cad: the contextual abuse dataset. sistency of natural language inference. In Findings Liwen Wang, Rumei Li, Yang Yan, Yuanmeng Yan, of the Association for Computational Linguistics: Sirui Wang, Wei Wu, and Weiran Xu. 2022a. In-EMNLP 2023, pages 13322-13334, Singapore. Asso-structionner: A multi-task instruction-based gener-ciation for Computational Linguistics. ative framework for few-shot ner. arXiv preprintprompting methods in natural language processing. arXiv preprint arXiv:2107.13586. Koustuv Sinha, Shagun Sodhani, Jin Dong, Joelle Pineau, and William L. Hamilton. 2019. CLUTRR:OpenAI. 2023. Gpt-4 technical report.Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In Proceedings of the A diagnostic benchmark for inductive reasoning from text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages 60th Annual Meeting of the Association for Compu-4506-4515, Hong Kong, China. Association for Com-tational Linguistics (Volume 1: Long Papers), pages putational Linguistics.Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instruc-tions with human feedback. Advances in neural in-formation processing systems, 35:27730-27744.8086-8098, Dublin, Ireland. Association for Compu-tational Linguistics.
Man Luo, Sharad Saxena, Swaroop Mishra, Mihir Parmar, and Chitta Baral.2022.Biotabqa: Instruction learning for biomedical table question answering.arXiv preprint arXiv:2207.02419.Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan.2022b.NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks.In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3505-3523, Dublin, Ireland.Association for Computational Linguistics.Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man Luo, Murad Mohammad, and Chitta Baral.2022.In-BoXBART: Get instructions into biomedical multitask learning.In Findings of the Association for Computational Linguistics: NAACL 2022, pages 112-128, Seattle, United States.Association for Computational Linguistics.Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih, and Ashish Sabharwal.2019a.Quarel: A dataset and models for answering questions about qualitative relationships.In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 7063-7071.Jidong Tian, Yitian Li, Wenqing Chen, Liqiang Xiao, Hao He, and Yaohui Jin.2021.Diagnosing the firstorder logical reasoning ability through LogicNLI.In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3738-3747, Online and Punta Cana, Dominican Republic.Association for Computational Linguistics.</p>
<p>for Formatting 1. Generate only one pair of p, and q based on the above understanding. 2. Generate each sentence in a new line. 3. Do not generate p, and q prefixes.</p>
<dl>
<dt>He will order pizza for dinner.Context: If Liam finished his work early, then he will order pizza for dinner.He won't order pizza for dinner Question: Does this imply that Liam didn't finish his work early?If Benjamin is going to the beach, then he will go swimming.He won't go swimming Question: Does this imply that Benjamin isn't going to the beach?Lisa gets an A in the class.q:She will reward herself with a trip.Context: If Lisa gets an A in the class, then she will reward herself with a trip.She won't reward herself with a trip Question: Does this imply that Lisa gets A in the class?Answer</dt>
<dt>Answer: yesp:Answer: yes</dt>
<dt>p: Liam finished his work early.q:p: Benjamin is going to the beach.q:He will go swimming.Context:</dt>
<dd>
<p>no Definition rule: If p then q; not q, therefore not p Instructions:</p>
</dd>
</dl>
<p>Understand the below examples to learn the connection between sentences and how they havebeen used.Use that understanding to generate coherent sentences.
(NLI) (Schick and Schütze, 2021), and Question(Marelli et al., 2014) dataset, with its foundationAnswering (QA) (Jiang et al., 2020), Informationin 10,000 English sentence pairs, is designed toExtraction (IE) (Chen et al., 2021; Cui et al., 2021),rigorously evaluate semantic relatedness and en-to name a few. In a recent development, the T0tailment, leveraging crowdsourced annotations formodel employs prompts to achieve zero-shot gen-precision. HANS, or Heuristic Analysis for NLIeralization across various NLP tasks (Sanh et al.,Systems (McCoy et al., 2019), stands out by rigor-2021). Le Scao and Rush (2021) suggested that theExamples: ously scrutinizing the dependability of NLI models,use of prompts could be as valuable as hundreds ofputting the spotlight on potential pitfalls tied todata points on average.syntactic heuristics such as lexical overlap. Lastly,Instruction Learning Efrat and Levy (2020) was focused on whether existing LLMs understand in-structions. The same work in the field of instruction by Hase and Bansal (2022); Ye and Ren (2021); Gupta et al. (2021); Zhong et al. (2021) has been proposed to show that models follow natural lan-guage instructions. In addition, Weller et al. (2020) developed a framework focusing on NLP systems that solve challenging new tasks based on their description. Mishra et al. (2021) have proposed natural language instructions for cross-task gener-CAD (Vidgen et al., 2021) introduces a meticu-lously crafted dataset from Reddit entries, target-ing the detection of online abuse. Nakamura et al. (2023) introduced LogicAttack, a method for per-forming adversarial attacks on NLI models using PL inference rules and proposed dataset of ∼ 9k attack samples derived from the SNLI dataset. In contrast to these works, LogicBench evaluates the logical reasoning capabilities of LLMs beyond the NLI task, focusing on the application of individual inference rules.alization of LLMs. Similarly, PromptSource (Sanhet al., 2021) and FLAN (Wei et al., 2021) were builtfor leveraging instructions and achieving zero-shotgeneralization on unseen tasks. Moreover, Parmaret al. (2022) shows the effectiveness of instructionsin multi-task settings for the biomedical/clinical do-main. Furthermore, Mishra et al. (2022a) discussedthe impact of task instruction reframing. Min et al.(2021) introduced a framework to better understand in-context learning. Ouyang et al. (2022) proposedFigure 4: Example prompt for Modus Tollens inference rule from PL.the InstructGPT model that is fine-tuned with hu-man feedback to follow instructions. Wang et al. (2022a) has developed an instruction-based multi-task framework for few-shot Named Entity Recog-nition (NER) tasks. In addition, many approaches have been proposed to improve model performance using instructions (Wu et al., 2022; Lin et al., 2021; Wang et al., 2022b; Luo et al., 2022; Kuznia et al., 2022; Patel et al., 2022; Mishra and Nouri, 2023).evaluate their diverse language capabilities, as well as those of forthcoming LLMs. Recently, many datasets have been created that evaluate different language understanding skills such as pronoun res-olution (Sakaguchi et al., 2021; Levesque et al., 2012), commonsense reasoning (Talmor et al., 2019), numerical reasoning (Dua et al., 2019; Patel et al., 2021; Mishra et al., 2022b), qualitative rea-Logic and NLI Datasets FraCas (Bernardy andsoning (Tafjord et al., 2019b,a), temporal reasoningChatzikyriakidis, 2020) offers a unique approach(Zhou et al., 2019), and feasibility reasoning (Guptato temporal semantics by converting syntax treeset al., 2023). Now, we present the advancements ininto logical formulas tailored for inference, em-prompt and instruction tuning using LLMs.phasizing temporal elements such as references,adverbs, aspectual classes, and progressives. ThePrompt Learning The introduction of LLMs hasMonotonicity Entailment Dataset (MED) (Yanakasignificantly shifted the research trend in NLP toet al., 2019) dives deep into monotonicity rea-prompt-based learning methodologies (Liu et al.,soning within NLI, probing the synergy between2021b). Many studies have been conducted tolexical and syntactic structures and spotlightinginvestigate the efficacy of prompt-based learninginherent challenges in both upward and down-in various applications including Text classifica-ward monotonic reasoning trajectories. The SICKtion (Yin et al., 2019), Natural Language Inference</p>
<p>studying for her exams, or Mila is going on vacation, or both scenarios are unfolding.It is unclear which of the two options is true at this point.However, one thing is certain -Chloe is not studying for her exams.Q1: Does this entail that Mila is going on vacation?(Yes)Q2: Does this mean that Mila isn't going on vacation?(No)
Rule ContextQuestionQ1: If Jim cleaned his room, does this imply that he will buy aIf Jim cleans his room, he will receive a reward and if henew toy? (Yes)receives a reward, he will use it to buy a new toy. So,Q2: If Jim didn't clean his room, does this entail that he won'tHSJim decided to tidy up his room, hoping to earn a reward. He diligently gathered his clothes, organized his toys, andbuy a new toy? (No) Q3: If Jim cleaned his room, does this imply that he won't buy adusted every surface. After a few hours of hard work, Jim'snew toy? (No)room was spotless.Q4: If Jim didn't clean his room, does this imply that he will buya new toy? (No)Either Chloe isDS</p>
<p>Table 10 :
10
Examples of context and question-answer pairs for each rule of First order logic from the LogicBench.
Rule ContextQuestionDRI</p>
<p>Table 11 :
11
Examples of context and question-answer pairs for each rule of Non-monotonic logic from the LogicBench.</p>
<p>Table 15 shows the performance for each inference rule and reasoning patterns achieved by Llama-2, Mistral, Gemini, ChatGPT, and GPT-4.As suggested in (Lu et al., 2022), prompting models are sensitive to in-context examples.Hence, we see mixed performance in Table 15 across all models.From Table</p>
<p>Table 14 :
14
Illustrative examples of NL logical context and questions created using sentences that are generated in §3.2.1 for NM logic for BQA.
Type RuleLlama-2MistralGeminiChatGPTGPT-4A(N o) A(Y es) A(N o) A(Y es) A(N o) A(Y es) A(N o) A(Y es) A(N o) A(Y es)HS100.025.374.316.783.326.984.227.993.853.1DS66.726.976.666.795.232.887.533.392.293.8CD100.055.697.142.298.173.1100.057.1100.080.0PLDD BD93.8 76.979.2 25.461.1 76.459.1 37.594.7 77.890.0 24.664.3 82.483.3 30.464.3 85.183.3 76.9MT100.025.689.332.792.928.1100.030.391.548.5MI55.651.657.957.166.766.764.073.390.594.7CT84.752.697.445.297.448.785.736.896.790.0Avg84.742.878.844.788.348.983.546.689.277.5EG90.927.576.836.488.927.677.429.690.453.6UI75.025.879.783.388.031.585.733.392.188.2MP100.069.0100.064.595.2100.0100.0100.0100.095.2HS100.0100.0100.083.3100.0100.0100.0100.0100.0100.0FOLDS100.051.397.344.287.565.0100.050.092.966.7CD90.594.761.963.281.084.281.084.274.1100.0DD80.025.374.70.088.030.978.329.485.176.9BD72.758.677.858.181.894.179.293.890.594.7MT85.769.257.764.382.473.986.494.466.7100.0Avg88.357.980.755.288.167.587.568.388.086.2DRI86.270.662.562.580.490.974.090.095.2100.0DRS47.447.677.383.361.970.660.775.090.090.0DRD 100.066.7100.062.578.668.063.655.270.492.3NMDRO RE158.1 60.914.3 64.766.7 58.67.7 72.762.8 66.79.1 85.764.1 60.614.6 100.076.5 87.033.3 100.0RE20.025.088.030.984.832.677.825.886.546.4RE352.652.468.080.082.471.462.181.861.571.4RAP50.050.065.590.962.5100.055.6100.087.0100.0Avg56.948.973.361.372.566.064.867.881.879.2</p>
<p>Table 15 :
15
Performance of LLMs in few-shot setting in terms of label-wise accuracy on LogicBench(Eval), where A(Y es) and A(N o) denote the accuracy for the Y es and N o labels, respectively.The context provides a conditional statement: If Elijah has a new job, then he will get a raise.2. The context also states that Elijah would not be receiving the raise he had anticipated.3. The second statement does not negate the first; it only states that the raise is not as expected.4. The lack of the anticipated raise does not necessarily mean Elijah doesn't have a new job; it could mean that the raise with the new job is different from what he expected.5. Therefore, the information provided does not allow us to infer that Elijah doesn't have a new job based solely on the raise situation.Context: james had always been forgetful, especially when it came to wearing a sweater.his mother had repeatedly told him, 'if he forgets to wear a sweater, then he will catch a cold.' throughout the day, he went about his activities, oblivious to the dropping temperature.hours later, he returned home, but james didn't catch a cold.Question: does this infer that james didn't forget to wear a sweater?Yes/No 1.The mother's rule establishes a conditional relationship: If James forgets to wear a sweater (condition), then he will catch a cold (result).2. It is stated that James did not catch a cold upon returning home.3.According to the mother's rule, if the result (catching a cold) did not occur, we cannot conclusively determine whether the condition (forgetting to wear a sweater) happened or not.
RuleContext/QuestionActual/Pred Reasoning ChainsPL(MT) 1. PL(MT) Context: if elijah has a new job, then he will get a raise. however, he would not be receiving the raise he had anticipated. Question: does this infer that elijah doesn't have a new job? Yes/No</p>
<p>Table 16 :
16
Case study of reasoning chains for PL(MT) generated from ChatGPT, and FOL(DD) generated from0.0192.13 0.02 96.67 0.03 DS 63.61 0.06 87.25 0.07 90.00 0.05 CD 88.88 0.02 41.04 0.05 86.67 0.03 DD 73.69 0.01 19.53 0.01 63.33 0.08 BD 83.62 0.03 43.38 0.04 65.00 0.05 MT 46.88 0.03 31.55 0.05 58.33 0.06 MI 84.49 0.01 31.570.01 71.67 0.06 CT 95.48 0.04 59.15 0.01 70.00 0.13 Avg 79.43 0.03 50.7 0.03 75.21 0.06 FOL EG 100.0 0.0 98.33 0.03 88.33 0.03 UI 90.32 0.00 94.64 0.00 88.33 0.08 MP 84.87 0.02 100.0 0.0 93.33 0.08 HS 96.67 0.02 94.52 0.00 76.67 0.03 DS 77.42 0.03 90.28 0.09 55.00 0.05 CD 90.41 0.01 40.39 0.02 81.67 0.08 DD 72.77 0.02 22.20 0.03 50.00 0.20 BD 79.60 0.03 30.05 0.06 60.00 0.05 MT 50.95 0.02 56.11 0.21 68.33 0.19 Avg 82.56 0.02 69.61 0.05 73.52 0.09 NM DRI 86.10 0.08 98.25 0.03 60.00 0.09 DRS 73.79 0.00 10.37 0.10 30.00 0.05 DRD 85.76 0.05 100.0 0.0 55.00 0.09 DRO 70.37 0.04 100.0 0.0 33.33 0.06 REI 83.80 0.03 44.31 0.09 73.33 0.08 REII 63.27 0.02 76.03 0.06 63.33 0.08 REIII 63.89 0.03 88.33 0.13 56.67 0.03 RAP 68.15 0.06 87.12 0.03 38.33 0.08 Avg 74.39 0.04 75.55 0.05 51.25 0.07
Logic RuleBQAMCQATypeA(No)A(Yes)Acc.HS98.81PL</p>
<p>Table 17 :
17
Evaluation of Yi-34B in terms of accuracy on LogicBench(Eval) BQA and LogicBench(Eval) M CQA .</p>
<p>We use https://github.com/dmlls/negate to generate negated sentences
https://platform.openai.com/docs/guides/gpt
https://ai.google.dev/
https://huggingface.co/models
AcknowledgementWe thank the anonymous reviewers for constructive suggestions, and the computer science graduate students of Arizona State University (ASU) who helped with the human annotations.We extend our gratitude to the Research Computing (RC) at ASU for providing computing resources for experiments.We acknowledge support by a 2023 Spring Amazon Research Award (ARA), and an award by Cisco via Silicon Valley Foundation.Ethics StatementWe have used AI assistants (Grammarly and ChatGPT) to address the grammatical errors and rephrase the sentences.MethodsModelsLogiQA pattern.Specifically, in step 5 of the reasoning chain, the model deviates from the provided context and introduces unrelated information, resulting in an incorrect prediction.Although these patterns are not prevalent, we found that ∼ 1% of the total analyzed reasoning chains exhibited this pattern in PL and NM for BQA, and ∼ 0.8% for MCQA.Rule: Modus Tollens (PL) Context: James had always been forget- Negations are hard to understand when embedded with logical rules.Here, Table16presents reasoning chains corresponding to PL and FOL supporting the effect of negation on their predictions.Performance of BQA vs. MCQA By analyzing the reasoning chains of CT and DD for PL, we observe that LLMs often employ the elimination method in MCQA.They prioritize options to select the most appropriate conclusion, thereby improving performance.Conversely, for NM, the models typically struggle to differentiate between two options and tend to hallucinate by assuming information, resulting in lower performance.We present reasoning chains corresponding to PL and NM supporting their behavior on both BQA and MCQA tasks at https://github.com/Mihir3009/LogicBench. q: He will order pizza for dinner.∼q: He will not order pizza for dinner.Results on Yi-34BContext: Liam had finished his work early for the day, which meant that he would typically have ordered pizza for dinner.However, on this particular day, he decided against ordering pizza and opted for something else instead.Question: Does this imply that liam didn't finish his work early?BD p(x): someone drinks lots of water q(x): they will feel hydrated r(x): they eat too much sugar s(x): they will experience a sugar crash p(a): Jane drinks lots of water ∼p(a): Jane does not drink lots of water q(a): she will feel hydrated ∼q(a): she will not feel hydrated r(a): she eats too much sugar ∼r(a): she does not eat too much sugar s(a): she will experience a sugar crash ∼s(a): she will not experience a sugar crash Context: In the bird kingdom, there are many different species that possess unique characteristics.One such species is the hummingbird, known for its ability to hover in mid-air and its vibrant colors.While most birds engage in the annual migration south for the winter, the hummingbird chooses to stay put and brave the cold weather.This decision sets the hummingbird apart from its fellow avian companions, as it relies on its resilience and resourcefulness to survive the harsh conditions.Question: Does this mean that all other birds than hummingbirds migrate south for the winter?RE1 p: Cats, dogs, and horses are animals.q: Animals are usually considered to be intelligent creatures.r: At least one of the cats or dogs is not considered intelligent.s: Horses are considered to be intelligent creatures.∼s: Horses aren't considered to be intelligent creatures.t: Exactly one of the cats or dogs is not considered intelligent.∼t: Exactly one of the cats or dogs is considered intelligent.Context: In a world where animals are often regarded as intelligent creatures, there is a captivating tale that revolves around cats, dogs, and horses.It is commonly believed that most animals possess a level of intellect.However, there is an intriguing twist to this belief as it is known that either cats or dogs are not considered particularly intelligent.As the story unfolds, we delve into the lives of these remarkable creatures, their interactions, and the unique qualities that each of them possesses.Question: Does this entail that horses are considered to be intelligent creatures and exactly one of the cats or dogs is not considered intelligent?RE2 p: cats normally meow.q: At least one species of cat doesn't meow.r: Exactly one species of cat doesn't meow.∼r: Exactly one species of cat meows.Context: In the realm of cat communication, meowing serves as a fundamental aspect of their vocal repertoire.However, intriguingly enough, there exists a distinct species of cat that deviates from this conventional norm.This peculiar feline defies the expectations associated with its kind by refraining from emitting any meows whatsoever.Question: Does this entail that exactly one species of cat doesn't meow?RE3 p: Cars have four wheels.q: wheels normally have spokes.r: at least one wheel does not have spokes.s: Cars have four wheels with spokes.∼s: Cars don't have four wheels with spokes.Context: In a world where cars were known for having four wheels, it was considered a common fact that wheels typically came equipped with spokes.However, amidst this widespread understanding, there was an exception.At least one wheel defied this norm and stood out from the rest by not having any spokes at all.Question: Does this imply that cars have four wheels with spokes?RAP p: John asserts that Sally was in the store.q: Jane asserts that Sally was not in the store.r: John's evidence is more reliable than Jane's.∼r: John's evidence is less reliable than Jane's.s: Sally was in the store.∼s: Sally wasn't in the store.Context: In the midst of a heated argument, John adamantly claims that Sally was present at the store.However, Jane strongly opposes John's assertion, insisting that Sally was indeed absent from the store.Question: If John's evidence is more reliable than Jane's, does this mean that Sally was in the store?
Jean-Philippe Bernardy, Stergios Chatzikyriakidis, Fracas: Temporal analysis. 2020</p>
<p>Logical reasoning for task oriented dialogue systems. Sajjad Beygi, Maryam Fazel-Zarandi, Alessandra Cervone, Prakash Krishnan, Siddhartha Jonnalagadda, 10.18653/v1/2022.ecnlp-1.10Proceedings of the Fifth Workshop on e-Commerce and NLP (ECNLP 5). the Fifth Workshop on e-Commerce and NLP (ECNLP 5)Dublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Adaprompt: Adaptive promptbased finetuning for relation extraction. Xiang Chen, Xin Xie, Ningyu Zhang, Jiahuan Yan, Shumin Deng, Chuanqi Tan, Fei Huang, Luo Si, Huajun Chen, 20212104arXiv eprints</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Template-based named entity recognition using BART. Leyang Cui, Yu Wu, Jian Liu, Sen Yang, Yue Zhang, 10.18653/v1/2021.findings-acl.161Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, 10.18653/v1/N19-1246Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinnesotaAssociation for Computational Linguistics20191Minneapolis</p>
<p>Avia Efrat, Omer Levy, arXiv:2010.11982The turking test: Can language models understand instructions?. 2020arXiv preprint</p>
<p>john is 50 years old, can his son be 65?" evaluating NLP models' understanding of feasibility. Himanshu Gupta, Neeraj Varshney, Swaroop Mishra, Kuntal Kumar Pal, Arjun Saurabh, Kevin Sawant, Siddharth Scaria, Chitta Goyal, Baral, 10.18653/v1/2023.eacl-main.30Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. the 17th Conference of the European Chapter of the Association for Computational LinguisticsDubrovnik, CroatiaAssociation for Computational Linguistics2023</p>
<p>Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi, Derek Hoiem, arXiv:2104.00743Towards general purpose vision systems. 2021arXiv preprint</p>
<p>Teaching temporal logics to neural networks. Christopher Hahn, Frederik Schmitt, Jens U Kreber, Markus Norman Rabe, Bernd Finkbeiner, 2021In International Conference on Learning Representations</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840FOLIO: Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>When can models learn from explanations? a formal framework for understanding the roles of explanation data. Peter Hase, Mohit Bansal, 10.18653/v1/2022.lnls-1.4Proceedings of the First Workshop on Learning with Natural Language Supervision. the First Workshop on Learning with Natural Language SupervisionDublin, IrelandAssociation for Computational Linguistics2022</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>How can we know what language models know? Transactions of the. Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020Association for Computational Linguistics8</p>
<p>TaxiNLI: Taking a ride up the NLU hill. Pratik Joshi, Somak Aditya, Aalok Sathe, Monojit Choudhury, 10.18653/v1/2020.conll-1.4Proceedings of the 24th Conference on Computational Natural Language Learning. the 24th Conference on Computational Natural Language LearningOnline. Association for Computational Linguistics2020</p>
<p>Reasoning-Driven Question-Answering for Natural Language Understanding. Daniel Khashabi, 2019University of Pennsylvania</p>
<p>Less is more: Summary of long instructions is better for program synthesis. Kirby Kuznia, Swaroop Mishra, Mihir Parmar, Chitta Baral, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>How many data points is a prompt worth?. Le Teven, Alexander M Scao, Rush, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2021</p>
<p>The Winograd Schema Challenge. Hector J Levesque, Ernest Davis, Leora Morgenstern, Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR'12. the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR'12Rome, ItalyAAAI Press2012</p>
<p>Benchmark problems for formal nonmonotonic reasoning: Version 2.00. Vladimir Lifschitz, Non-Monotonic Reasoning: 2nd International Workshop Grassau, FRG. Springer1989. June 13-15. 19882</p>
<p>Few-shot learning with multilingual language models. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, arXiv:2112.10668.tasksProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2021arXiv preprint</p>
<p>Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Du, ICLR</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Learning from task descriptions. Orion Weller, Nicholas Lourie, Matt Gardner, Matthew E Peters, 10.18653/v1/2020.emnlp-main.105Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>
<p>Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts. Tongshuang Wu, Michael Terry, Carrie , Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing SystemsJun Cai. 2022</p>
<p>Can neural networks understand monotonicity reasoning?. Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Satoshi Sekine, Lasha Abzianidze, Johan Bos, 10.18653/v1/W19-4804Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPFlorence, Italy2019Association for Computational Linguistics</p>
<p>Learning to generate task-specific adapters from task description. Qinyuan Ye, Xiang Ren, 10.18653/v1/2021.acl-short.82Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20212Short Papers)</p>
<p>Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. Wenpeng Yin, Jamaal Hay, Dan Roth, 10.18653/v1/D19-1404Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>
<p>Yi: Open foundation models by 01. Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, arXiv:2403.046522024arXiv preprint</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, International Conference on Learning Representations. 2020</p>
<p>On the paradox of learning to reason from data. Honghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, Guy Van Den, Broeck, Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence. the Thirty-Second International Joint Conference on Artificial Intelligence2023</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections. Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein, 10.18653/v1/2021.findings-emnlp.244Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>going on a vacation" takes longer than "going for a walk": A study of temporal commonsense understanding. Ben Zhou, Daniel Khashabi, Qiang Ning, Dan Roth, 10.18653/v1/D19-1332Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2019</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-677 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-677</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-677</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-150d340d36b64c30c2aba69ebb1d27d9c2234687</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/150d340d36b64c30c2aba69ebb1d27d9c2234687" target="_blank">A statistical definition for reproducibility and replicability</a></p>
                <p><strong>Paper Venue:</strong> bioRxiv</p>
                <p><strong>Paper TL;DR:</strong> This work provides formal and informal definitions of scientific studies, reproducibility, and replicability that can be used to clarify discussions around these concepts in the scientific and popular press.</p>
                <p><strong>Paper Abstract:</strong> Everyone agrees that reproducibility and replicability are fundamental characteristics of scientific studies. These topics are attracting increasing attention, scrutiny, and debate both in the popular press and the scientific literature. But there are no formal statistical definitions for these concepts, which leads to confusion since the same words are used for different concepts by different people in different fields. We provide formal and informal definitions of scientific studies, reproducibility, and replicability that can be used to clarify discussions around these concepts in the scientific and popular press.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e677.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e677.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Incomplete study specification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Missing or incomplete reporting of scientific study components</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that many published studies report only high-level elements (e.g., question and claim) while omitting key components (population, experimental design, analysis plan, code, data, analysts), creating a mismatch between the natural-language publication and the full experimental implementation needed for replication/reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>scientific study / publication pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The end-to-end scientific study as communicated by publications and reports (population, hypothesis, experimental design, experimenter, data, analysis plan, analyst, code, estimates, and claims).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper (publication) / public report</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>not applicable (missing code or analysis artifacts)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / missing components</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Publications often include only a question and a claim while omitting the rest of the experiment's components (population, experimental design, analysis plan, raw data, code, analyst identity). This leaves the natural-language description insufficient to reconstruct the actual implemented experiment or analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>overall experimental specification (documentation stage prior to execution); affects data collection, analysis, and interpretation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>review of published reports and replication reports noting absent components (qualitative assessment of what was reported versus what is required by the authors' formal model)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative counting/assessment of reported components; specific example cited where the replication report provided only a hypothesis and a claim and lacked other components (no systematic numeric metric provided in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Prevents meaningful replication or reproducibility checks because critical implementation details are unavailable; undermines interpretation of replication claims (paper cites the 47/53 claim as uninterpretable because study components were missing).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Portrayed as common in the domains discussed; the paper highlights cases (e.g., many pre-clinical reports) where most components were not available, but provides no systematic prevalence statistic.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Omission in natural-language reporting (insufficient methodological transparency); cultural and incentive structures that reward claims over sharing full methods/artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Adopt the paper's formal framework: require explicit documentation of all study components (population, design, analysis plan, code, data, analysts) and public availability of code/data when possible.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively evaluated in this paper; proposed framework argued to improve interpretability and enable reproducibility but no empirical effectiveness metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>general scientific research (examples in preclinical cancer research, psychology, genomics)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A statistical definition for reproducibility and replicability', 'publication_date_yy_mm': '2016-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e677.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e677.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Population mismatch in replication</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Replication performed on a different population than described</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper identifies cases where a replication study changed the sampled population (e.g., US college students → Italians), creating a discrepancy between the population specified in the natural-language description and the population actually used in the replication implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>replication experiment pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Independent replication efforts that aim to recollect data and repeat analyses described in an original publication.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods/population specification</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>replication study experimental implementation (data collection choices)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous/changed population (incomplete or different population specification)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Replicators altered the population (e.g., geographic, demographic differences) relative to what the original study described, violating the formal definition of replication because the natural-language description specified a different population than the one used in implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>experimental design / data collection stage (population selection)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>comparison of the original paper's population specification with the reported population in the replication study (documented by commentary and critiques of the Reproducibility Project: Psychology)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative comparison; identification of specific mismatches (the paper cites the example of a U.S. undergraduate sample vs an Italian sample); no numeric metric reported for effect of population change in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potentially invalidates claims of replication because differences in population can produce legitimately different outcomes; cited as the core reason a purported replication was not a true replication.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Mentioned as occurring in notable high-profile replication attempts (e.g., some studies in the Reproducibility Project); prevalence not quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Insufficiently specified or ambiguous population definitions in the natural-language description; practical constraints or choices by replicators leading to different sampling frames.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Explicitly specify population in the study documentation and enforce matching population criteria for replications; use the formal framework to check whether replication criteria are met before declaring a replication.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not empirically evaluated in this paper; recommended as a conceptual remedy to prevent mislabeling of replications.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>psychology / general empirical sciences</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A statistical definition for reproducibility and replicability', 'publication_date_yy_mm': '2016-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e677.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e677.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wrong code/data artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Provided implementation artifacts (code and data) that do not match the described analysis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors describe a case (genomic chemosensitivity predictor) where the original authors released code and data but those artifacts were incorrect; forensic reanalysis found the correct code/data that reproduced the original reported results, revealing a discrepancy between provided artifacts and the intended/claimed analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>analysis artifact distribution / reproducible research pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The process by which original researchers provide code and data to enable reproduction of analyses described in publications.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods + claimed analysis description</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>released code and data artifacts (files released by original authors)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incorrect / mismatched implementation artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The code and data released by the original authors were the wrong artifacts (e.g., not the code/data that actually produced the claimed results or contained errors), so the provided implementation did not faithfully implement the described analysis; forensic re-creation produced correct code/data that reproduced the reported results, indicating the provided artifacts were misleading or erroneous.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>analysis implementation artifacts (code and data distribution / provenance)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>forensic bioinformatics / inspection and re-execution of released code and data; independent teams attempted reproduction and discovered inconsistencies and incorrect artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>reproducibility check via re-running artifacts — initial provided artifacts failed to correspond to the claimed analysis; after producing corrected artifacts the original results were reproduced; no numeric performance metric reported beyond binary outcome (original artifacts wrong vs reproduction successful after correction).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Created major controversy and misinterpretation: although the study's reported results could ultimately be reproduced with correct artifacts, the wrong released artifacts misled investigators and undermined trust; led to lawsuits and major career consequences for authors.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Presented as a high-profile but illustrative case; paper does not provide a prevalence estimate of this failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Errors in artifact release (wrong files, poor provenance/packaging), lack of verification that released artifacts match described analyses, and insufficient transparency/documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Require sharing of correct, provenance-traced code and raw data; independent verification of released artifacts; adoption of standards and forensic checks to ensure provided artifacts reproduce reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not quantitatively assessed in this paper; authors report that when corrected artifacts were produced and validated reproduction was possible, indicating high potential effectiveness if correctly applied.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>bioinformatics / high-throughput biology (genomic signatures / chemosensitivity)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A statistical definition for reproducibility and replicability', 'publication_date_yy_mm': '2016-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e677.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e677.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Garden of forking paths / P-hacking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Post-hoc analytic flexibility: divergence between pre-specified analysis plan and executed code</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper formalizes 'garden of forking paths' and 'p-hacking' as mechanisms by which analysts change code or analysis choices in response to observed data, creating a gap between the stated analysis plan in natural language and the actual implemented code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>data analysis pipeline / analyst decision process</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The sequence of choices and code modifications made during data analysis, potentially influenced by observed data.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>analysis plan (pre-specified methods) / research protocol</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>analysis code (scripts/notebooks) that may be modified during analysis</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation diverges from pre-specified plan / analysis flexibility (code changes conditioned on data)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Given a specified population, hypothesis, experimental design, analysis plan, and analyst, the code is changed after observing the data (garden of forking paths) or changed intentionally to produce desired statements (p-hacking). This results in the executed implementation not matching the pre-specified natural-language plan.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>analysis execution / code development (decision points and branching during analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>conceptual identification via the statistical framework and critique of undisclosed analytic flexibility; can be detected in practice by comparing pre-registered analysis plans to final code or via forensic review of analysis histories.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Not measured in this paper; the paper references existing literature that quantifies false positives from undisclosed flexibility (e.g., Simmons et al.) but does not report new numeric measures here.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Increases false-positive rates and undermines validity of claims; described as a central mechanism causing irreproducible or non-replicable findings when analytic choices are data-dependent and not disclosed.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Implied to be widespread as a general risk across empirical research; no prevalence numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Analyst degrees of freedom, lack of pre-registration or detailed analysis-plan documentation, incentives to obtain publishable results, and insufficient linkage between natural-language plans and versioned code artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Encourage pre-specification (pre-registration) of analysis plans, version control and archival of analysis code, explicit linking of code to analysis plans, and transparency about analytic choices; the paper's formal framework helps clarify when garden-of-forking-paths issues arise.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not empirically evaluated in this paper; literature cited suggests pre-registration and transparency reduce undisclosed flexibility but no quantitative results reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>general empirical sciences / statistics</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A statistical definition for reproducibility and replicability', 'publication_date_yy_mm': '2016-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>DERIVING CHEMOSENSITIVITY FROM CELL LINES: FORENSIC BIOINFORMATICS AND REPRODUCIBLE RESEARCH IN HIGH-THROUGHPUT BIOLOGY <em>(Rating: 2)</em></li>
                <li>Drug development: Raise standards for preclinical cancer research <em>(Rating: 2)</em></li>
                <li>Estimating the reproducibility of psychological science <em>(Rating: 2)</em></li>
                <li>Repeatability of published microarray gene expression analyses <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-677",
    "paper_id": "paper-150d340d36b64c30c2aba69ebb1d27d9c2234687",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Incomplete study specification",
            "name_full": "Missing or incomplete reporting of scientific study components",
            "brief_description": "The paper documents that many published studies report only high-level elements (e.g., question and claim) while omitting key components (population, experimental design, analysis plan, code, data, analysts), creating a mismatch between the natural-language publication and the full experimental implementation needed for replication/reproduction.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "scientific study / publication pipeline",
            "system_description": "The end-to-end scientific study as communicated by publications and reports (population, hypothesis, experimental design, experimenter, data, analysis plan, analyst, code, estimates, and claims).",
            "nl_description_type": "research paper (publication) / public report",
            "code_implementation_type": "not applicable (missing code or analysis artifacts)",
            "gap_type": "incomplete specification / missing components",
            "gap_description": "Publications often include only a question and a claim while omitting the rest of the experiment's components (population, experimental design, analysis plan, raw data, code, analyst identity). This leaves the natural-language description insufficient to reconstruct the actual implemented experiment or analysis.",
            "gap_location": "overall experimental specification (documentation stage prior to execution); affects data collection, analysis, and interpretation",
            "detection_method": "review of published reports and replication reports noting absent components (qualitative assessment of what was reported versus what is required by the authors' formal model)",
            "measurement_method": "qualitative counting/assessment of reported components; specific example cited where the replication report provided only a hypothesis and a claim and lacked other components (no systematic numeric metric provided in the paper)",
            "impact_on_results": "Prevents meaningful replication or reproducibility checks because critical implementation details are unavailable; undermines interpretation of replication claims (paper cites the 47/53 claim as uninterpretable because study components were missing).",
            "frequency_or_prevalence": "Portrayed as common in the domains discussed; the paper highlights cases (e.g., many pre-clinical reports) where most components were not available, but provides no systematic prevalence statistic.",
            "root_cause": "Omission in natural-language reporting (insufficient methodological transparency); cultural and incentive structures that reward claims over sharing full methods/artifacts.",
            "mitigation_approach": "Adopt the paper's formal framework: require explicit documentation of all study components (population, design, analysis plan, code, data, analysts) and public availability of code/data when possible.",
            "mitigation_effectiveness": "Not quantitatively evaluated in this paper; proposed framework argued to improve interpretability and enable reproducibility but no empirical effectiveness metrics reported.",
            "domain_or_field": "general scientific research (examples in preclinical cancer research, psychology, genomics)",
            "reproducibility_impact": true,
            "uuid": "e677.0",
            "source_info": {
                "paper_title": "A statistical definition for reproducibility and replicability",
                "publication_date_yy_mm": "2016-07"
            }
        },
        {
            "name_short": "Population mismatch in replication",
            "name_full": "Replication performed on a different population than described",
            "brief_description": "The paper identifies cases where a replication study changed the sampled population (e.g., US college students → Italians), creating a discrepancy between the population specified in the natural-language description and the population actually used in the replication implementation.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "replication experiment pipeline",
            "system_description": "Independent replication efforts that aim to recollect data and repeat analyses described in an original publication.",
            "nl_description_type": "research paper methods/population specification",
            "code_implementation_type": "replication study experimental implementation (data collection choices)",
            "gap_type": "ambiguous/changed population (incomplete or different population specification)",
            "gap_description": "Replicators altered the population (e.g., geographic, demographic differences) relative to what the original study described, violating the formal definition of replication because the natural-language description specified a different population than the one used in implementation.",
            "gap_location": "experimental design / data collection stage (population selection)",
            "detection_method": "comparison of the original paper's population specification with the reported population in the replication study (documented by commentary and critiques of the Reproducibility Project: Psychology)",
            "measurement_method": "qualitative comparison; identification of specific mismatches (the paper cites the example of a U.S. undergraduate sample vs an Italian sample); no numeric metric reported for effect of population change in this paper.",
            "impact_on_results": "Potentially invalidates claims of replication because differences in population can produce legitimately different outcomes; cited as the core reason a purported replication was not a true replication.",
            "frequency_or_prevalence": "Mentioned as occurring in notable high-profile replication attempts (e.g., some studies in the Reproducibility Project); prevalence not quantified.",
            "root_cause": "Insufficiently specified or ambiguous population definitions in the natural-language description; practical constraints or choices by replicators leading to different sampling frames.",
            "mitigation_approach": "Explicitly specify population in the study documentation and enforce matching population criteria for replications; use the formal framework to check whether replication criteria are met before declaring a replication.",
            "mitigation_effectiveness": "Not empirically evaluated in this paper; recommended as a conceptual remedy to prevent mislabeling of replications.",
            "domain_or_field": "psychology / general empirical sciences",
            "reproducibility_impact": true,
            "uuid": "e677.1",
            "source_info": {
                "paper_title": "A statistical definition for reproducibility and replicability",
                "publication_date_yy_mm": "2016-07"
            }
        },
        {
            "name_short": "Wrong code/data artifacts",
            "name_full": "Provided implementation artifacts (code and data) that do not match the described analysis",
            "brief_description": "The authors describe a case (genomic chemosensitivity predictor) where the original authors released code and data but those artifacts were incorrect; forensic reanalysis found the correct code/data that reproduced the original reported results, revealing a discrepancy between provided artifacts and the intended/claimed analysis.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "analysis artifact distribution / reproducible research pipeline",
            "system_description": "The process by which original researchers provide code and data to enable reproduction of analyses described in publications.",
            "nl_description_type": "research paper methods + claimed analysis description",
            "code_implementation_type": "released code and data artifacts (files released by original authors)",
            "gap_type": "incorrect / mismatched implementation artifacts",
            "gap_description": "The code and data released by the original authors were the wrong artifacts (e.g., not the code/data that actually produced the claimed results or contained errors), so the provided implementation did not faithfully implement the described analysis; forensic re-creation produced correct code/data that reproduced the reported results, indicating the provided artifacts were misleading or erroneous.",
            "gap_location": "analysis implementation artifacts (code and data distribution / provenance)",
            "detection_method": "forensic bioinformatics / inspection and re-execution of released code and data; independent teams attempted reproduction and discovered inconsistencies and incorrect artifacts",
            "measurement_method": "reproducibility check via re-running artifacts — initial provided artifacts failed to correspond to the claimed analysis; after producing corrected artifacts the original results were reproduced; no numeric performance metric reported beyond binary outcome (original artifacts wrong vs reproduction successful after correction).",
            "impact_on_results": "Created major controversy and misinterpretation: although the study's reported results could ultimately be reproduced with correct artifacts, the wrong released artifacts misled investigators and undermined trust; led to lawsuits and major career consequences for authors.",
            "frequency_or_prevalence": "Presented as a high-profile but illustrative case; paper does not provide a prevalence estimate of this failure mode.",
            "root_cause": "Errors in artifact release (wrong files, poor provenance/packaging), lack of verification that released artifacts match described analyses, and insufficient transparency/documentation.",
            "mitigation_approach": "Require sharing of correct, provenance-traced code and raw data; independent verification of released artifacts; adoption of standards and forensic checks to ensure provided artifacts reproduce reported results.",
            "mitigation_effectiveness": "Not quantitatively assessed in this paper; authors report that when corrected artifacts were produced and validated reproduction was possible, indicating high potential effectiveness if correctly applied.",
            "domain_or_field": "bioinformatics / high-throughput biology (genomic signatures / chemosensitivity)",
            "reproducibility_impact": true,
            "uuid": "e677.2",
            "source_info": {
                "paper_title": "A statistical definition for reproducibility and replicability",
                "publication_date_yy_mm": "2016-07"
            }
        },
        {
            "name_short": "Garden of forking paths / P-hacking",
            "name_full": "Post-hoc analytic flexibility: divergence between pre-specified analysis plan and executed code",
            "brief_description": "The paper formalizes 'garden of forking paths' and 'p-hacking' as mechanisms by which analysts change code or analysis choices in response to observed data, creating a gap between the stated analysis plan in natural language and the actual implemented code.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "system_name": "data analysis pipeline / analyst decision process",
            "system_description": "The sequence of choices and code modifications made during data analysis, potentially influenced by observed data.",
            "nl_description_type": "analysis plan (pre-specified methods) / research protocol",
            "code_implementation_type": "analysis code (scripts/notebooks) that may be modified during analysis",
            "gap_type": "implementation diverges from pre-specified plan / analysis flexibility (code changes conditioned on data)",
            "gap_description": "Given a specified population, hypothesis, experimental design, analysis plan, and analyst, the code is changed after observing the data (garden of forking paths) or changed intentionally to produce desired statements (p-hacking). This results in the executed implementation not matching the pre-specified natural-language plan.",
            "gap_location": "analysis execution / code development (decision points and branching during analysis)",
            "detection_method": "conceptual identification via the statistical framework and critique of undisclosed analytic flexibility; can be detected in practice by comparing pre-registered analysis plans to final code or via forensic review of analysis histories.",
            "measurement_method": "Not measured in this paper; the paper references existing literature that quantifies false positives from undisclosed flexibility (e.g., Simmons et al.) but does not report new numeric measures here.",
            "impact_on_results": "Increases false-positive rates and undermines validity of claims; described as a central mechanism causing irreproducible or non-replicable findings when analytic choices are data-dependent and not disclosed.",
            "frequency_or_prevalence": "Implied to be widespread as a general risk across empirical research; no prevalence numbers provided in this paper.",
            "root_cause": "Analyst degrees of freedom, lack of pre-registration or detailed analysis-plan documentation, incentives to obtain publishable results, and insufficient linkage between natural-language plans and versioned code artifacts.",
            "mitigation_approach": "Encourage pre-specification (pre-registration) of analysis plans, version control and archival of analysis code, explicit linking of code to analysis plans, and transparency about analytic choices; the paper's formal framework helps clarify when garden-of-forking-paths issues arise.",
            "mitigation_effectiveness": "Not empirically evaluated in this paper; literature cited suggests pre-registration and transparency reduce undisclosed flexibility but no quantitative results reported here.",
            "domain_or_field": "general empirical sciences / statistics",
            "reproducibility_impact": true,
            "uuid": "e677.3",
            "source_info": {
                "paper_title": "A statistical definition for reproducibility and replicability",
                "publication_date_yy_mm": "2016-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "DERIVING CHEMOSENSITIVITY FROM CELL LINES: FORENSIC BIOINFORMATICS AND REPRODUCIBLE RESEARCH IN HIGH-THROUGHPUT BIOLOGY",
            "rating": 2
        },
        {
            "paper_title": "Drug development: Raise standards for preclinical cancer research",
            "rating": 2
        },
        {
            "paper_title": "Estimating the reproducibility of psychological science",
            "rating": 2
        },
        {
            "paper_title": "Repeatability of published microarray gene expression analyses",
            "rating": 1
        }
    ],
    "cost": 0.00846625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Title: A statistical definition for reproducibility and replicability Authors: Prasad Patil, Roger D. Peng, Jeffrey T. Leek</p>
<h4>Abstract</h4>
<p>Everyone agrees that reproducibility and replicability are fundamental characteristics of scientific studies. These topics are attracting increasing attention, scrutiny, and debate both in the popular press and the scientific literature. But there are no formal statistical definitions for these concepts, which leads to confusion since the same words are used for different concepts by different people in different fields. We provide formal and informal definitions of scientific studies, reproducibility, and replicability that can be used to clarify discussions around these concepts in the scientific and popular press.</p>
<h1>Main text</h1>
<p>Reproducibility and replicability are at the center of heated debates in psychology (1), genomics (2), climate change (3), economics (4), and medicine (5). The conversation about reproducibility and replicability of science has spilled over into the popular press $(6,7)$ with major political consequences including new governmental policies and initiatives (8) focusing on the reliability of science. These studies have generated responses (9), responses to responses (10), and ongoing discussion (11),(12).</p>
<p>Everyone agrees that scientific studies should be reproducible and replicable. The problem is almost no one agrees upon what those terms mean. A major initiative in psychology used the term "reproducibility" to refer to completely re-doing experiments including data collection (1). In cancer biology "reproducibility" has been used to refer to the re-calculation of results using a fixed set of data and code (13). "Replication" is often used to refer to finding two independent studies that produce a result with similar levels of statistical significance in human genetics(14). The same word has been used to refer to re-doing experiments (15) and recreating results from fixed data and code(16).</p>
<p>These disagreements in terminology seem purely semantic, but they have major scientific and political implications. The recent back-and-forth in the pages of Science mentioned above hinged critically on the definition of "replication" with disagreement between the authors about what those terms meant. The press, government officials, and even late night comedy hosts have pointed out "irreproducibility" - defined as the inability to re-create statistical results using fixed data and code - as the fundamental problem with the scientific process. But they use this term to encompass all of the more insidious problems of false discoveries, missed discoveries, scientific errors, and scientific misconduct (17). Others have suggested conceptual frameworks to help define these terms (18) but have stopped short of a statistical model - which is critical for being precise in discussions of often complicated terms.</p>
<p>To address this major difficulty we need a statistical framework for the scientific process. Typically statistical and machine learning models only formally define the outputs of the scientific process with random variables. The outcome is often designated by $\mathbf{Y}$ and the covariates of interest by $\mathbf{X}$. This framework is limited because it does not allow us to model variation in what the scientists in question intended to study, how they performed the experiment, who performed the experiment, what data they collected, what analysis they intended to perform, who analyzed the data, and what analysis was actually performed. We have created a formal statistical model that includes terms for all of these components of the scientific process (see Supplemental Material). The key steps in this process can also be represented using a simple visual model (Figure 1).
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. A graphic representation of the statistical model for the scientific process $A$. Reproducibility is defined as re-performing the same analysis with the same code using a different analyst; replicability is defined as re-performing the experiment and collecting new data. B. The paper that only 6 out of 53 pre-clinical studies replicated (5) only reported a question and a claim, but not the rest of the scientific components of a study. C. The disagreement over the Reproducibility Project Psychology $(9,19)$ is because a replication was not performed since the population changed. D. In the case of the controversy over genomic signatures for chemosensitivity (2), reproducibility wasn't the main issue - the issue was that the original study did not have correct code and data.</p>
<p>Here we provide informal definitions for key scientific terms and provide detailed statistical definitions in the Using this modeling framework we provide formal definitions for the following terms (see Supplementary Material for formal statistical definitions and additional definitions):</p>
<p>A scientific study: consists of document(s) specifying a population, question, hypothesis, experimental design, experimenter, data, analysis plan, analyst, code, parameter estimates, and claims about the parameter estimates.
Publication: Making a public claim on the basis of a scientific study.
Reproducible: Given a population, hypothesis, experimental design, experimenter, data, analysis plan, and code you get the same parameter estimates in a new analysis (Figure 1A).
Strongly replicable study: Given a population, hypothesis, experimental design, analysis plan, and code you get consistent estimates when you recollect data and perform the analysis using the original code.
Replicable study: Given a population, hypothesis, experimental design, and analysis plan you get consistent estimates when you recollect data and redo the analysis (Figure 1A).
Strongly replicable claim: Given a population, hypothesis, experimental design, analysis plan, and code, you make an equivalent claim based on the results of the study.
Replicable claim: Given a population, hypothesis, experimental design, and analysis plan, you make an equivalent claim based on the results of the study.
Conceptually replicable: A population and a question relate two hypotheses. A scientific study is performed for the first hypothesis and a claim is made. Then a scientific study is performed for a second hypothesis and a claim is made. The claims from the two studies provide consistent answers to the question.
False discovery: The claim at the conclusion of a scientific study is not equal to the claim you would make if you could observe all data from the population given your hypothesis, experimental design, and analysis plan.
Garden of forking paths: Given a population, hypothesis, experimental design, experimenter, data, analysis plan, and analyst, the code changes given the data you observe.
P-hacking: Given a population, hypothesis, experimental design, experimenter, data, analysis plan, and analyst, the code changes to match a desired statement.
File drawer effect: The probability of publication depends on the claim made at the conclusion of a scientific study.</p>
<p>We can use our statistical framework to resolve arguments and misconceptions around some of the most controversial discussions of reproducibility and replicability. Consider the case of the claim that many of 53 pre-clinical studies were not replicable when performed by scientific teams at a pharmaceutical company (5). Under our formal model, the paper describing this replication effort reported a hypothesis - that most studies do not replicate. It also reported a claim - that 47 out of the 53 studies could not be replicated by scientists at the company. However, the population, hypothesis, experimental design, experimenter, data, analysis plan, analysts, code, and estimates are not available (Figure 1B). This makes it clear that the published report is missing most of the components of a scientific study.</p>
<p>Later, three replication studies [(20), (21)/(22), (23)] were reported from the same pharmaceutical company - though it was unclear if they were part of the originally reported 53. It was pointed out that some of the reported studies included experiments with different</p>
<p>populations - violating the definition of a replication. A similar issue was at the heart of a disagreement over several of the studies in the Reproducibility Project: Psychology (1). In this project, 100 studies were replicated by independent investigators. In one case, a study originally performed in the United States on US college students was evaluated among a group of Italians. It was pointed out that this change in population violates the definition of replication (9) using our framework it is clear the reason is that the population changed (Figure 1C).</p>
<p>Finally consider one of the earliest and most egregious debates over reproducibility - the case of a predictor of chemosensitivity that ultimately fell apart - leading to a lawsuits, an Institute of Medicine conference and report, and ultimately the end of the lead author's scientific career (2). In this case, both the code and the data produced by the original authors were made available; however, they were the wrong code and data. A team from MD Anderson was able to investigate and ultimately produce data and code that reproduced the original results (Figure 1D). Ultimately, the study was reproducible, which is surprising given the focus on this study being a violation of reproducibility. The problem with the study was not that the data and code could not be produced, it was that these items, when produced, were wrong (24).</p>
<p>Our statistical framework can be used to address other issues that are central to the debate over the validity of science including p-hacking(25), the garden of forking paths(26), conceptual replication, and other scientific and data analytic problems that arise before the tidy data are processed at the end of the experiment. Using a proper and statistically rigorous framework for the scientific process we can help resolve arguments and provide a solid foundation for journal and public policy around these complicated issues.</p>
<h1>References</h1>
<ol>
<li>O. S. Collaboration, Others, An open, large-scale, collaborative effort to estimate the reproducibility of psychological science. Perspect. Psychol. Sci. 7, 657-660 (2012).</li>
<li>K. A. Baggerly, K. R. Coombes, DERIVING CHEMOSENSITIVITY FROM CELL LINES: FORENSIC BIOINFORMATICS AND REPRODUCIBLE RESEARCH IN HIGH-THROUGHPUT BIOLOGY. Ann. Appl. Stat. 3, 1309-1334 (2009).</li>
<li>R. E. Benestad et al., Learning from mistakes in climate research. Theor. Appl. Climatol., $1-5(2015)$.</li>
<li>T. Herndon, M. Ash, R. Pollin, Does high public debt consistently stifle economic growth? A critique of Reinhart and Rogoff. Cambridge J. Econ. 38, 257-279 (2014).</li>
<li>C. G. Begley, L. M. Ellis, Drug development: Raise standards for preclinical cancer research. Nature. 483, 531-533 (2012).</li>
<li>
<p>T. Economist, Unreliable research: Trouble at the lab (2013).</p>
</li>
<li>
<p>J. Van Bavel, Why Do So Many Studies Fail to Replicate? The New York Times (2016), (available at
http://www.nytimes.com/2016/05/29/opinion/sunday/why-do-so-many-studies-fail-to-replicat e.html).</p>
</li>
<li>Netherlands starts major campaign against research misconduct, (available at https://www.insidehighered.com/news/2016/06/23/netherlands-starts-major-campaign-again st-research-misconduct).</li>
<li>D. T. Gilbert, G. King, S. Pettigrew, T. D. Wilson, Comment on "Estimating the reproducibility of psychological science." Science. 351, 1037 (2016).</li>
<li>C. J. Anderson et al., Response to Comment on "Estimating the reproducibility of psychological science." Science. 351, 1037-1037 (2016).</li>
<li>D. T. Gilbert, G. King, S. Pettigrew, T. D. Wilson, A RESPONSE TO THE REPLY TO OUR TECHNICAL COMMENT ON "ESTIMATING THE REPRODUCIBILITY OF PSYCHOLOGICAL SCIENCE" (2016), (available at http://projects.iq.harvard.edu/files/psychology-replications/files/gkpw_response_to_osc_reb utal.pdf).</li>
<li>D. T. Gilbert, G. King, S. Pettigrew, T. D. Wilson, More on "Estimating the Reproducibility of Psychological Science." Available at projects. iq. harvard. edu/files/psychology-replications/files/gkpw_post_publication_response. pdf (2016) (available at
http://projects.iq.harvard.edu/files/psychology-replications/files/gkpw_post_publication_resp onse.pdf).</li>
<li>R. D. Peng, Reproducible research in computational science. Science. 334, 1226-1227 (2011).</li>
<li>E. Zeggini et al., Meta-analysis of genome-wide association data and large-scale replication identifies additional susceptibility loci for type 2 diabetes. Nat. Genet. 40, 638-645 (2008).</li>
<li>B. Birmaher et al., Psychometric properties of the Screen for Child Anxiety Related Emotional Disorders (SCARED): a replication study. J. Am. Acad. Child Adolesc. Psychiatry. 38, 1230-1236 (1999).</li>
<li>J. P. A. Ioannidis et al., Repeatability of published microarray gene expression analyses. Nat. Genet. 41, 149-155 (2009).</li>
<li>J. T. Leek, L. R. Jager, Is most published research really false? bioRxiv (2016), p. 050575.</li>
<li>S. N. Goodman, D. Fanelli, J. P. A. Ioannidis, What does research reproducibility mean? Sci. Transl. Med. 8, 341ps12 (2016).</li>
<li>Open Science Collaboration, Estimating the reproducibility of psychological science. Science. 349 (2015), doi:10.1126/science.aac4716.</li>
<li>P. E. Cramer et al., ApoE-directed therapeutics rapidly clear $\beta$-amyloid and reverse deficits</li>
</ol>
<p>in AD mouse models. Science. 335, 1503-1506 (2012).
21. J. Gardner et al., G-protein-coupled receptor GPR21 knockout mice display improved glucose tolerance and increased insulin response. Biochem. Biophys. Res. Commun. 418, $1-5(2012)$.
22. O. Osborn et al., G protein-coupled receptor 21 deletion improves insulin sensitivity in diet-induced obese mice. J. Clin. Invest. 122, 2444-2453 (2012).
23. B.-H. Lee et al., Enhancement of proteasome activity by a small-molecule inhibitor of USP14. Nature. 467, 179-184 (2010).
24. J. T. Leek, R. D. Peng, Opinion: Reproducible research can still be wrong: Adopting a prevention approach. Proceedings of the National Academy of Sciences. 112, 1645-1646 (2015).
25. J. P. Simmons, L. D. Nelson, U. Simonsohn, False-positive psychology: undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychol. Sci. 22, 1359-1366 (2011).
26. A. Gelman, E. Loken, The garden of forking paths: Why multiple comparisons can be a problem, even when there is no "fishing expedition" or "p-hacking" and the research hypothesis was posited ahead of time. Downloaded January. 30, 2014 (2013).</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1947 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1947</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1947</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-281315107</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.11417v1.pdf" target="_blank">Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations</a></p>
                <p><strong>Paper Abstract:</strong> Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments. However, direct fine-tuning on robot data often disrupts these representations and limits generalization. We present a framework that better preserves pretrained features while adapting them for robot manipulation. Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances. Evaluations in simulation and on real robots show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1947.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1947.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA (open-source vision-language-action model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source vision-language-action (VLA) baseline that adapts pretrained VLM backbones for robotic manipulation; in the paper it is used as the primary baseline demonstrating that direct fine-tuning and discretized action remapping can degrade pretrained representations and hurt generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openvla: An open-source vision-languageaction model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A VLA baseline that uses pretrained vision-language backbones (visual encoders + language model) to predict robot actions; in prior recipes it remaps rarely-used language tokens to discretized action bins (discrete action head) and fine-tunes the VLM backbones end-to-end for action prediction, which the paper reports breaks alignment with pretrained semantics and degrades visual/language representations.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>DINO + SigLIP visual backbone (single encoder in baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained VLM visual backbones (DINO, SigLIP) used as VLM components (pretraining details referenced from VLM literature but not quantified in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Baseline grounding via fine-tuned VLM representations with a discretized action mapping (remapping rarely-used language tokens to action bins), effectively using learned alignment between language context and global visual features but breaking token semantics by remapping</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene/global image-level features (VLM-level features), not explicitly object-centric or per-pixel</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>implicit spatial cues via pretrained visual representations (no explicit 3D or explicit region/bbox tokens used in baseline grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>SimplerEnv tasks (PickCan, OpenDrawer, CloseDrawer, MoveNear) and real-robot Bridge dataset evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>photorealistic simulation (SimplerEnv Visual Matching) and real-world robot deployment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (task completion rate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>35.03% average success rate (Visual Matching evaluation, Table II)</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Baseline OpenVLA (as above) serves as the non-grounded/remapped baseline: 35.03% avg success rate (Visual Matching).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Compared to OpenVLA+, +43.43 percentage points (35.03% -> 78.46% avg success rate) when using the paper's grounding recipe (dual encoder + string tokenizer + co-training).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Using a partially-frozen dual encoder (freeze one copy, finetune the other) improves OpenVLA from 35.03% to 55.55% avg success rate (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Direct fine-tuning on robot action data causes representation degradation / 'collapse' of pretrained visual and language features, leading to sensitivity to background changes, distractors, viewpoint shifts, and paraphrased instructions; this is identified as a primary bottleneck limiting generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Reported failure modes include: strong performance drops with small instruction paraphrases and background variations (e.g., PickCan OpenVLA: orig 36.70% -> paraphrase 12.12%, Table IV), and tendency under distractors to reach for closest object instead of instructed target (qualitative real-robot failures reported).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Baseline handles domain shift poorly (substantial performance drops under Visual Variant Aggregation and background masking); paper introduces partial freezing, string tokenizer, and co-training to mitigate this.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Paper shows partially-frozen dual encoder (one copy frozen, one trainable) is superior to naively finetuning the single encoder: OpenVLA -> OpenVLA+D: 35.03% -> 55.55% (Table II). Fully freezing only was reported in prior work to hurt performance; this mixed strategy is preferred.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Concatenation of frozen encoder features and trainable encoder features (z_t = phi_frozen(o_t) || phi_train(o_t)), then language-conditioned autoregressive generation over a unified string output space (string tokenizer).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not quantified numerically; paper claims approaches reduce overfitting in low-data regimes and that co-training with V-L data helps generalization when robot data is scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Naive fine-tuning and discretized action remapping harm pretrained VLM representations and grounding; preserving pretrained features (partially freezing), aligning action outputs with language (string tokens), and co-training with vision-language data substantially improve grounded performance and robustness in manipulation tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1947.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1947.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>π0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>π0 (pi-zero): a vision-language-action flow model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VLA flow-matching model for general robot control used in the paper as a second baseline; the paper evaluates replacing its flow-matching action head with the proposed string-based tokenizer and applying the partially-frozen encoder + co-training recipe.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>π 0 : A vision-language-action flow model for general robot control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>π0 (baseline and π0+ with proposed recipe)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Flow-matching VLA model (π0) initialized from a pretrained VLM (PaliGemma in experiments); baseline uses a flow-matching continuous action head, while the paper experiments with replacing that head by the string-based tokenizer and adding the partially-frozen dual encoder and co-training strategy to create π0+.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>PaliGemma VLM backbone (as used by π0 in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>PaliGemma pretrained VLM (referenced as the initialization for π0 in the experiments; the paper does not re-specify pretraining dataset sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Baseline π0: flow-matching continuous action head mapping language-conditioned visual features to continuous trajectories; π0+ (in-paper) replaces the head with string-based autoregressive action tokenization to align action outputs with language embeddings and enable co-training.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>global/trajectory-level visual features used in flow head baseline; in π0+ the concatenated VLM features are token-conditioned for autoregressive string output.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit spatial cues encoded in VLM features and action deltas; no explicit 3D scene module described beyond action parameterization (∆x,∆y,∆z, rotations, gripper).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>SimplerEnv tasks (PickCan, OpenDrawer, CloseDrawer, MoveNear) and real-robot evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>photorealistic simulation (SimplerEnv) and real-world robot deployment</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>π0 baseline (with instruction augmentation in the paper) avg 62.64% (Table II); π0+ (full recipe) avg 69.19% (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Baseline π0 (flow head, finetuned VLMs) avg 62.64% (Visual Matching); replacing head + recipe yields 69.19% (π0+).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>+6.55 percentage points average success rate improvement for π0 -> π0+ (62.64% -> 69.19%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Applying the partially-frozen dual encoder and string tokenizer improves π0 performance when compared to the baseline flow head; exact per-component breakdown reported primarily for OpenVLA but overall π0+ outperforms π0 by ~6.5 points.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Same representation degradation issues as other baselines: fine-tuning on robot data can disrupt pretrained representations causing sensitivity to background/distractors and paraphrased instructions; π0 required instruction augmentation to improve baseline results in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Baselines including π0 sometimes misinterpret tasks under distractors, e.g., reaching for the nearest object; language paraphrase robustness is limited (paraphrase performance substantially lower than original instructions), though exact per-task paraphrase numbers for π0 are reported in Table IV for language robustness comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Paper applies the same remedies (partially-frozen encoder, string tokenizer, co-training) to π0 to reduce domain-shift sensitivity; results show improved OOD robustness for π0+.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Paper shows benefit of partially freezing visual encoder in the dual-encoder setup; π0 experiments use the same idea but detailed per-component numbers emphasized for OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Concatenation of frozen and trainable encoder outputs then language-conditioned prediction; in π0 baseline fusion is via features into a flow-matching head, while π0+ uses the string tokenizer head.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not numerically specified for π0; overall claims that co-training and string tokenization help make better use of pretrained representations in low-data robot regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Replacing flow-matching continuous heads with a language-aligned string tokenizer and preserving pretrained visual features via a partially-frozen encoder plus co-training yields modest but consistent gains for flow-based VLAs (π0 -> π0+), improving robustness and generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1947.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1947.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA+ (Dual encoder + String tokenizer + Co-training)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA with Partially-Frozen Dual Encoder, String-Based Action Tokenizer, and Vision-Language Co-training</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's proposed VLA recipe combining (1) a partially-frozen dual visual encoder (one frozen anchor + one trainable copy), (2) string-based autoregressive tokenization of continuous actions to align with language pretraining, and (3) co-training on robot demonstrations plus vision-language datasets emphasizing spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA+ (paper's full method); similar recipe applied to π0 to create π0+</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Architecture: duplicate the pretrained visual encoder and freeze one copy to retain pretrained features while finetuning the other for task adaptation; concatenate frozen and trainable encoder outputs to form z_t passed to a language-conditioned autoregressive tokenizer that emits actions as character strings (e.g., '0','.','0','3','1','2'), enabling shared output space with language tasks; co-train on mixed batches (50% robot demonstrations, 50% vision-language data focused on spatial reasoning/affordances) to regularize features and prevent catastrophic forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>Duplicated pretrained visual encoder; experiments use DINO + SigLIP backbones for OpenVLA variants (one copy frozen, one trainable).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained VLM visual backbones (DINO, SigLIP) and language models (e.g., LLaMA) were used to initialize models; exact pretraining datasets/sizes are from referenced VLM literature (not re-specified numerically in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Grounding achieved by (a) preserving pretrained visual-linguistic alignment via a frozen encoder, (b) concatenating preserved and task-adapted visual features, and (c) expressing actions as language-character sequences predicted autoregressively by a language-aligned head so that pretrained language embeddings and spatial reasoning capabilities are reused for action prediction; co-training on language-vision data enforces preservation of VLM grounding abilities.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Multi-level: preserves pretrained global/semantic visual representations (via frozen encoder) while allowing task-specific adaptations (trainable encoder); not explicitly object-centric but leverages spatial reasoning capabilities of VLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>No explicit 3D scene graph or bounding-box output; spatial information encoded implicitly in pretrained VLM features and in the string-based parameterization of continuous deltas (action parameters include ∆x,∆y,∆z and rotations).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation / instruction following</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>SimplerEnv tasks (PickCan, OpenDrawer, CloseDrawer, MoveNear) and real-robot Bridge dataset trials on ViperX 300s2</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>photorealistic simulation (SimplerEnv Visual Matching and Visual Variant Aggregation) and real-world robot deployment (ViperX 300s2 with Logitech C920 camera)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate (task completion)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>OpenVLA+ average success rate 78.46% (Visual Matching evaluation, Table II); π0+ average success rate 69.19% (Table II). Real-world trials show consistent outperformance of baselines across reported tasks (Table V, qualitative Fig.6).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablations in Table II: baseline OpenVLA 35.03% avg; OpenVLA + D (dual encoder only) 55.55% avg; OpenVLA + S (string tokenizer only) 50.25% avg; OpenVLA + C (co-training only) 51.05% avg; OpenVLA + DS (dual + string) 69.03% avg; OpenVLA + SC (string + co-training) 78.17% avg; OpenVLA+ (D+S+C) 78.46% avg.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Full recipe (D+S+C) improves OpenVLA baseline by +43.43 percentage points (35.03% -> 78.46% avg) and improves π0 by +6.55 points (62.64% -> 69.19% avg). String tokenizer + co-training alone yields +43.14 points over baseline (35.03 -> 78.17), showing the string-token + co-training combination is a major contributor.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>Partially-frozen dual encoder alone (D) improves OpenVLA from 35.03% -> 55.55% avg (Table II), and combining with string tokenizer/co-training yields further gains; fully freezing previously reported in prior work can degrade performance, so the paper recommends partial freezing (one frozen copy as anchor + one trainable copy).</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>The paper identifies representation degradation (visual and language) when fine-tuning on robot data as a core perception/grounding bottleneck; symptoms include sensitivity to background masking/randomization, paraphrased instructions, and distractors causing misidentification of targets.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Quantitative and qualitative failure analyses: language paraphrase failures (e.g., OpenVLA PickCan: 36.70% -> 12.12% on paraphrases), background masking causes large drops in baseline success, and under distractors baselines often reach for nearest object; OpenVLA+ and π0+ substantially reduce these failure modes per reported simulation and real-robot evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Domain shift is handled by: (1) freezing an encoder copy to preserve pretrained visual grounding, (2) string-based tokenizer to align actions with language pretraining enabling co-training, and (3) mixed-batch co-training (50% robot, 50% vision-language datasets) to prevent catastrophic forgetting; empirical results show much smaller performance drops under Visual Variant Aggregation and background masking compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td>Not reported as detailed numeric breakdown for novel objects; qualitative results indicate improved robustness to distractors and novel instructions, but explicit novel-object success rates are not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Partially-frozen dual encoder (one frozen, one trainable) outperforms naive full finetuning and (according to prior work cited) fully freezing: example OpenVLA -> OpenVLA+D: 35.03% -> 55.55% avg (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Early fusion of vision encodings via concatenation (frozen || trainable), then language-conditioned autoregressive decoding of action tokens (string tokenizer); co-training enforces shared output space between language and action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not reported as exact counts in the paper; authors claim the approach improves robustness and generalization in low-data robot regimes (co-training and preserved pretrained features reduce overfitting), but no explicit sample-efficiency numbers are given.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Preserving pretrained VLM representations (via a frozen encoder), aligning action outputs with language (string-based tokenization), and co-training with vision-language data that emphasizes spatial reasoning together enable much stronger vision-language grounding for embodied manipulation: they substantially restore pretrained spatial and language reasoning capabilities, yield large gains in success rate (+~43 points for OpenVLA), and reduce failures under visual and linguistic distribution shifts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1947.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1947.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>String Tokenizer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>String-Based Action Tokenizer (language-aligned action tokenizer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A component that encodes continuous low-level robot action parameters as character sequences (strings) and predicts them autoregressively with the language model head, aligning action prediction with language pretraining and enabling co-training on language-vision data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>String-Based Action Tokenizer (component used in OpenVLA+ and π0+)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Converts each continuous action dimension (e.g., ∆x = 0.0312) into a sequence of character tokens (e.g., '0','.','0','3','1','2') drawn from the language vocabulary; actions are generated autoregressively by the language head, allowing a single autoregressive head to be trained both on language tasks (V-L datasets) and action prediction (robot data), thereby reusing pretrained language embeddings and enabling step-by-step refinement of action values.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (tokenizer is an output/head component; used with duplicated visual encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Aligns action space with language by making actions native sequences in the language token space; this enables grounding because the model can reuse language-conditioned spatial reasoning and pretrained token semantics to produce actions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Action-level output but tightly coupled to language-level representations; indirectly leverages global and spatial visual features through language conditioning on concatenated encoder features.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>No explicit geometric representation; encodes continuous spatial deltas numerically as character strings which preserves fine-grained spatial values while leveraging language semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation (used to predict low-level end-effector trajectories parametrized by deltas and rotations)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>SimplerEnv manipulation tasks; also applied in real-robot experiments</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>simulation and real-world robot</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Using the string tokenizer alone raises OpenVLA from 35.03% -> 50.25% (OpenVLA+S in Table II); combined with co-training (SC) yields 78.17% avg (OpenVLA+SC), nearly matching the full recipe (78.46%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Without string tokenizer (but with baseline discretized/remapped action head) OpenVLA is 35.03% avg; with string tokenizer only OpenVLA+S is 50.25% avg (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>String tokenizer alone gives +15.22 percentage points over baseline OpenVLA (35.03% -> 50.25%); when combined with co-training it contributes to ~+43.14 points improvement (35.03% -> 78.17%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>The tokenizer addresses language-output misalignment bottlenecks caused by remapping language tokens to discretized action bins which discard pretrained semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>By aligning action outputs to language tokens and co-training, the tokenizer reduces language-paraphrase sensitivity and improves action precision via autoregressive refinement; quantitative ablations show large improvements when tokenizer is enabled plus co-training.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Enables co-training with vision-language datasets (mixed batches) because action outputs share the same token space as language, which helps prevent catastrophic forgetting and improves OOD robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td>Not applicable directly; tokenizer complements partially-frozen encoder strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Unifies output modality by using language-head autoregression for actions; fuses via concatenated vision features conditioned into that head.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not numerically specified; claimed to help in low-data regimes by leveraging language pretraining and enabling co-training with large V-L datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Casting continuous robot actions as language-character sequences and predicting them autoregressively greatly increases the ability to reuse pretrained language and spatial reasoning capabilities, enabling effective co-training and producing large empirical gains in robustness and success rate when combined with partial encoder freezing and V-L co-training.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Openvla: An open-source vision-languageaction model. <em>(Rating: 2)</em></li>
                <li>π 0 : A vision-language-action flow model for general robot control. <em>(Rating: 2)</em></li>
                <li>RT-1: Robotics transformer for real-world control at scale. <em>(Rating: 2)</em></li>
                <li>RoboPoint: A vision-language model for spatial affordance prediction in robotics. <em>(Rating: 2)</em></li>
                <li>SpatialVLM: Endowing vision-language models with spatial reasoning capabilities. <em>(Rating: 2)</em></li>
                <li>SimplerEnv: Evaluating real-world robot manipulation policies in simulation. <em>(Rating: 2)</em></li>
                <li>VIMA: general robot manipulation with multimodal prompts. <em>(Rating: 1)</em></li>
                <li>RT-trajectory: Robotic task generalization via hindsight trajectory sketches. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1947",
    "paper_id": "paper-281315107",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA (open-source vision-language-action model)",
            "brief_description": "An open-source vision-language-action (VLA) baseline that adapts pretrained VLM backbones for robotic manipulation; in the paper it is used as the primary baseline demonstrating that direct fine-tuning and discretized action remapping can degrade pretrained representations and hurt generalization.",
            "citation_title": "Openvla: An open-source vision-languageaction model.",
            "mention_or_use": "use",
            "model_name": "OpenVLA (baseline)",
            "model_description": "A VLA baseline that uses pretrained vision-language backbones (visual encoders + language model) to predict robot actions; in prior recipes it remaps rarely-used language tokens to discretized action bins (discrete action head) and fine-tunes the VLM backbones end-to-end for action prediction, which the paper reports breaks alignment with pretrained semantics and degrades visual/language representations.",
            "visual_encoder_type": "DINO + SigLIP visual backbone (single encoder in baseline)",
            "visual_encoder_pretraining": "Pretrained VLM visual backbones (DINO, SigLIP) used as VLM components (pretraining details referenced from VLM literature but not quantified in this paper)",
            "grounding_mechanism": "Baseline grounding via fine-tuned VLM representations with a discretized action mapping (remapping rarely-used language tokens to action bins), effectively using learned alignment between language context and global visual features but breaking token semantics by remapping",
            "representation_level": "scene/global image-level features (VLM-level features), not explicitly object-centric or per-pixel",
            "spatial_representation": "implicit spatial cues via pretrained visual representations (no explicit 3D or explicit region/bbox tokens used in baseline grounding)",
            "embodied_task_type": "object manipulation / instruction following",
            "embodied_task_name": "SimplerEnv tasks (PickCan, OpenDrawer, CloseDrawer, MoveNear) and real-robot Bridge dataset evaluations",
            "visual_domain": "photorealistic simulation (SimplerEnv Visual Matching) and real-world robot deployment",
            "performance_metric": "success rate (task completion rate)",
            "performance_value": "35.03% average success rate (Visual Matching evaluation, Table II)",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Baseline OpenVLA (as above) serves as the non-grounded/remapped baseline: 35.03% avg success rate (Visual Matching).",
            "grounding_improvement": "Compared to OpenVLA+, +43.43 percentage points (35.03% -&gt; 78.46% avg success rate) when using the paper's grounding recipe (dual encoder + string tokenizer + co-training).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Using a partially-frozen dual encoder (freeze one copy, finetune the other) improves OpenVLA from 35.03% to 55.55% avg success rate (Table II).",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Direct fine-tuning on robot action data causes representation degradation / 'collapse' of pretrained visual and language features, leading to sensitivity to background changes, distractors, viewpoint shifts, and paraphrased instructions; this is identified as a primary bottleneck limiting generalization.",
            "failure_mode_analysis": "Reported failure modes include: strong performance drops with small instruction paraphrases and background variations (e.g., PickCan OpenVLA: orig 36.70% -&gt; paraphrase 12.12%, Table IV), and tendency under distractors to reach for closest object instead of instructed target (qualitative real-robot failures reported).",
            "domain_shift_handling": "Baseline handles domain shift poorly (substantial performance drops under Visual Variant Aggregation and background masking); paper introduces partial freezing, string tokenizer, and co-training to mitigate this.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Paper shows partially-frozen dual encoder (one copy frozen, one trainable) is superior to naively finetuning the single encoder: OpenVLA -&gt; OpenVLA+D: 35.03% -&gt; 55.55% (Table II). Fully freezing only was reported in prior work to hurt performance; this mixed strategy is preferred.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Concatenation of frozen encoder features and trainable encoder features (z_t = phi_frozen(o_t) || phi_train(o_t)), then language-conditioned autoregressive generation over a unified string output space (string tokenizer).",
            "sample_efficiency": "Not quantified numerically; paper claims approaches reduce overfitting in low-data regimes and that co-training with V-L data helps generalization when robot data is scarce.",
            "key_findings_grounding": "Naive fine-tuning and discretized action remapping harm pretrained VLM representations and grounding; preserving pretrained features (partially freezing), aligning action outputs with language (string tokens), and co-training with vision-language data substantially improve grounded performance and robustness in manipulation tasks.",
            "uuid": "e1947.0"
        },
        {
            "name_short": "π0",
            "name_full": "π0 (pi-zero): a vision-language-action flow model",
            "brief_description": "A VLA flow-matching model for general robot control used in the paper as a second baseline; the paper evaluates replacing its flow-matching action head with the proposed string-based tokenizer and applying the partially-frozen encoder + co-training recipe.",
            "citation_title": "π 0 : A vision-language-action flow model for general robot control.",
            "mention_or_use": "use",
            "model_name": "π0 (baseline and π0+ with proposed recipe)",
            "model_description": "Flow-matching VLA model (π0) initialized from a pretrained VLM (PaliGemma in experiments); baseline uses a flow-matching continuous action head, while the paper experiments with replacing that head by the string-based tokenizer and adding the partially-frozen dual encoder and co-training strategy to create π0+.",
            "visual_encoder_type": "PaliGemma VLM backbone (as used by π0 in this paper)",
            "visual_encoder_pretraining": "PaliGemma pretrained VLM (referenced as the initialization for π0 in the experiments; the paper does not re-specify pretraining dataset sizes)",
            "grounding_mechanism": "Baseline π0: flow-matching continuous action head mapping language-conditioned visual features to continuous trajectories; π0+ (in-paper) replaces the head with string-based autoregressive action tokenization to align action outputs with language embeddings and enable co-training.",
            "representation_level": "global/trajectory-level visual features used in flow head baseline; in π0+ the concatenated VLM features are token-conditioned for autoregressive string output.",
            "spatial_representation": "Implicit spatial cues encoded in VLM features and action deltas; no explicit 3D scene module described beyond action parameterization (∆x,∆y,∆z, rotations, gripper).",
            "embodied_task_type": "object manipulation / instruction following",
            "embodied_task_name": "SimplerEnv tasks (PickCan, OpenDrawer, CloseDrawer, MoveNear) and real-robot evaluation",
            "visual_domain": "photorealistic simulation (SimplerEnv) and real-world robot deployment",
            "performance_metric": "success rate",
            "performance_value": "π0 baseline (with instruction augmentation in the paper) avg 62.64% (Table II); π0+ (full recipe) avg 69.19% (Table II).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Baseline π0 (flow head, finetuned VLMs) avg 62.64% (Visual Matching); replacing head + recipe yields 69.19% (π0+).",
            "grounding_improvement": "+6.55 percentage points average success rate improvement for π0 -&gt; π0+ (62.64% -&gt; 69.19%).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Applying the partially-frozen dual encoder and string tokenizer improves π0 performance when compared to the baseline flow head; exact per-component breakdown reported primarily for OpenVLA but overall π0+ outperforms π0 by ~6.5 points.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Same representation degradation issues as other baselines: fine-tuning on robot data can disrupt pretrained representations causing sensitivity to background/distractors and paraphrased instructions; π0 required instruction augmentation to improve baseline results in the paper.",
            "failure_mode_analysis": "Baselines including π0 sometimes misinterpret tasks under distractors, e.g., reaching for the nearest object; language paraphrase robustness is limited (paraphrase performance substantially lower than original instructions), though exact per-task paraphrase numbers for π0 are reported in Table IV for language robustness comparisons.",
            "domain_shift_handling": "Paper applies the same remedies (partially-frozen encoder, string tokenizer, co-training) to π0 to reduce domain-shift sensitivity; results show improved OOD robustness for π0+.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Paper shows benefit of partially freezing visual encoder in the dual-encoder setup; π0 experiments use the same idea but detailed per-component numbers emphasized for OpenVLA.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Concatenation of frozen and trainable encoder outputs then language-conditioned prediction; in π0 baseline fusion is via features into a flow-matching head, while π0+ uses the string tokenizer head.",
            "sample_efficiency": "Not numerically specified for π0; overall claims that co-training and string tokenization help make better use of pretrained representations in low-data robot regimes.",
            "key_findings_grounding": "Replacing flow-matching continuous heads with a language-aligned string tokenizer and preserving pretrained visual features via a partially-frozen encoder plus co-training yields modest but consistent gains for flow-based VLAs (π0 -&gt; π0+), improving robustness and generalization.",
            "uuid": "e1947.1"
        },
        {
            "name_short": "OpenVLA+ (Dual encoder + String tokenizer + Co-training)",
            "name_full": "OpenVLA with Partially-Frozen Dual Encoder, String-Based Action Tokenizer, and Vision-Language Co-training",
            "brief_description": "The paper's proposed VLA recipe combining (1) a partially-frozen dual visual encoder (one frozen anchor + one trainable copy), (2) string-based autoregressive tokenization of continuous actions to align with language pretraining, and (3) co-training on robot demonstrations plus vision-language datasets emphasizing spatial reasoning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OpenVLA+ (paper's full method); similar recipe applied to π0 to create π0+",
            "model_description": "Architecture: duplicate the pretrained visual encoder and freeze one copy to retain pretrained features while finetuning the other for task adaptation; concatenate frozen and trainable encoder outputs to form z_t passed to a language-conditioned autoregressive tokenizer that emits actions as character strings (e.g., '0','.','0','3','1','2'), enabling shared output space with language tasks; co-train on mixed batches (50% robot demonstrations, 50% vision-language data focused on spatial reasoning/affordances) to regularize features and prevent catastrophic forgetting.",
            "visual_encoder_type": "Duplicated pretrained visual encoder; experiments use DINO + SigLIP backbones for OpenVLA variants (one copy frozen, one trainable).",
            "visual_encoder_pretraining": "Pretrained VLM visual backbones (DINO, SigLIP) and language models (e.g., LLaMA) were used to initialize models; exact pretraining datasets/sizes are from referenced VLM literature (not re-specified numerically in this paper).",
            "grounding_mechanism": "Grounding achieved by (a) preserving pretrained visual-linguistic alignment via a frozen encoder, (b) concatenating preserved and task-adapted visual features, and (c) expressing actions as language-character sequences predicted autoregressively by a language-aligned head so that pretrained language embeddings and spatial reasoning capabilities are reused for action prediction; co-training on language-vision data enforces preservation of VLM grounding abilities.",
            "representation_level": "Multi-level: preserves pretrained global/semantic visual representations (via frozen encoder) while allowing task-specific adaptations (trainable encoder); not explicitly object-centric but leverages spatial reasoning capabilities of VLMs.",
            "spatial_representation": "No explicit 3D scene graph or bounding-box output; spatial information encoded implicitly in pretrained VLM features and in the string-based parameterization of continuous deltas (action parameters include ∆x,∆y,∆z and rotations).",
            "embodied_task_type": "object manipulation / instruction following",
            "embodied_task_name": "SimplerEnv tasks (PickCan, OpenDrawer, CloseDrawer, MoveNear) and real-robot Bridge dataset trials on ViperX 300s2",
            "visual_domain": "photorealistic simulation (SimplerEnv Visual Matching and Visual Variant Aggregation) and real-world robot deployment (ViperX 300s2 with Logitech C920 camera)",
            "performance_metric": "success rate (task completion)",
            "performance_value": "OpenVLA+ average success rate 78.46% (Visual Matching evaluation, Table II); π0+ average success rate 69.19% (Table II). Real-world trials show consistent outperformance of baselines across reported tasks (Table V, qualitative Fig.6).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablations in Table II: baseline OpenVLA 35.03% avg; OpenVLA + D (dual encoder only) 55.55% avg; OpenVLA + S (string tokenizer only) 50.25% avg; OpenVLA + C (co-training only) 51.05% avg; OpenVLA + DS (dual + string) 69.03% avg; OpenVLA + SC (string + co-training) 78.17% avg; OpenVLA+ (D+S+C) 78.46% avg.",
            "grounding_improvement": "Full recipe (D+S+C) improves OpenVLA baseline by +43.43 percentage points (35.03% -&gt; 78.46% avg) and improves π0 by +6.55 points (62.64% -&gt; 69.19% avg). String tokenizer + co-training alone yields +43.14 points over baseline (35.03 -&gt; 78.17), showing the string-token + co-training combination is a major contributor.",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "Partially-frozen dual encoder alone (D) improves OpenVLA from 35.03% -&gt; 55.55% avg (Table II), and combining with string tokenizer/co-training yields further gains; fully freezing previously reported in prior work can degrade performance, so the paper recommends partial freezing (one frozen copy as anchor + one trainable copy).",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "The paper identifies representation degradation (visual and language) when fine-tuning on robot data as a core perception/grounding bottleneck; symptoms include sensitivity to background masking/randomization, paraphrased instructions, and distractors causing misidentification of targets.",
            "failure_mode_analysis": "Quantitative and qualitative failure analyses: language paraphrase failures (e.g., OpenVLA PickCan: 36.70% -&gt; 12.12% on paraphrases), background masking causes large drops in baseline success, and under distractors baselines often reach for nearest object; OpenVLA+ and π0+ substantially reduce these failure modes per reported simulation and real-robot evaluations.",
            "domain_shift_handling": "Domain shift is handled by: (1) freezing an encoder copy to preserve pretrained visual grounding, (2) string-based tokenizer to align actions with language pretraining enabling co-training, and (3) mixed-batch co-training (50% robot, 50% vision-language datasets) to prevent catastrophic forgetting; empirical results show much smaller performance drops under Visual Variant Aggregation and background masking compared to baselines.",
            "novel_object_performance": "Not reported as detailed numeric breakdown for novel objects; qualitative results indicate improved robustness to distractors and novel instructions, but explicit novel-object success rates are not provided.",
            "frozen_vs_finetuned": "Partially-frozen dual encoder (one frozen, one trainable) outperforms naive full finetuning and (according to prior work cited) fully freezing: example OpenVLA -&gt; OpenVLA+D: 35.03% -&gt; 55.55% avg (Table II).",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Early fusion of vision encodings via concatenation (frozen || trainable), then language-conditioned autoregressive decoding of action tokens (string tokenizer); co-training enforces shared output space between language and action prediction.",
            "sample_efficiency": "Not reported as exact counts in the paper; authors claim the approach improves robustness and generalization in low-data robot regimes (co-training and preserved pretrained features reduce overfitting), but no explicit sample-efficiency numbers are given.",
            "key_findings_grounding": "Preserving pretrained VLM representations (via a frozen encoder), aligning action outputs with language (string-based tokenization), and co-training with vision-language data that emphasizes spatial reasoning together enable much stronger vision-language grounding for embodied manipulation: they substantially restore pretrained spatial and language reasoning capabilities, yield large gains in success rate (+~43 points for OpenVLA), and reduce failures under visual and linguistic distribution shifts.",
            "uuid": "e1947.2"
        },
        {
            "name_short": "String Tokenizer",
            "name_full": "String-Based Action Tokenizer (language-aligned action tokenizer)",
            "brief_description": "A component that encodes continuous low-level robot action parameters as character sequences (strings) and predicts them autoregressively with the language model head, aligning action prediction with language pretraining and enabling co-training on language-vision data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "String-Based Action Tokenizer (component used in OpenVLA+ and π0+)",
            "model_description": "Converts each continuous action dimension (e.g., ∆x = 0.0312) into a sequence of character tokens (e.g., '0','.','0','3','1','2') drawn from the language vocabulary; actions are generated autoregressively by the language head, allowing a single autoregressive head to be trained both on language tasks (V-L datasets) and action prediction (robot data), thereby reusing pretrained language embeddings and enabling step-by-step refinement of action values.",
            "visual_encoder_type": "N/A (tokenizer is an output/head component; used with duplicated visual encoders)",
            "visual_encoder_pretraining": "N/A",
            "grounding_mechanism": "Aligns action space with language by making actions native sequences in the language token space; this enables grounding because the model can reuse language-conditioned spatial reasoning and pretrained token semantics to produce actions.",
            "representation_level": "Action-level output but tightly coupled to language-level representations; indirectly leverages global and spatial visual features through language conditioning on concatenated encoder features.",
            "spatial_representation": "No explicit geometric representation; encodes continuous spatial deltas numerically as character strings which preserves fine-grained spatial values while leveraging language semantics.",
            "embodied_task_type": "object manipulation (used to predict low-level end-effector trajectories parametrized by deltas and rotations)",
            "embodied_task_name": "SimplerEnv manipulation tasks; also applied in real-robot experiments",
            "visual_domain": "simulation and real-world robot",
            "performance_metric": "success rate",
            "performance_value": "Using the string tokenizer alone raises OpenVLA from 35.03% -&gt; 50.25% (OpenVLA+S in Table II); combined with co-training (SC) yields 78.17% avg (OpenVLA+SC), nearly matching the full recipe (78.46%).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Without string tokenizer (but with baseline discretized/remapped action head) OpenVLA is 35.03% avg; with string tokenizer only OpenVLA+S is 50.25% avg (Table II).",
            "grounding_improvement": "String tokenizer alone gives +15.22 percentage points over baseline OpenVLA (35.03% -&gt; 50.25%); when combined with co-training it contributes to ~+43.14 points improvement (35.03% -&gt; 78.17%).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "The tokenizer addresses language-output misalignment bottlenecks caused by remapping language tokens to discretized action bins which discard pretrained semantics.",
            "failure_mode_analysis": "By aligning action outputs to language tokens and co-training, the tokenizer reduces language-paraphrase sensitivity and improves action precision via autoregressive refinement; quantitative ablations show large improvements when tokenizer is enabled plus co-training.",
            "domain_shift_handling": "Enables co-training with vision-language datasets (mixed batches) because action outputs share the same token space as language, which helps prevent catastrophic forgetting and improves OOD robustness.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": "Not applicable directly; tokenizer complements partially-frozen encoder strategy.",
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Unifies output modality by using language-head autoregression for actions; fuses via concatenated vision features conditioned into that head.",
            "sample_efficiency": "Not numerically specified; claimed to help in low-data regimes by leveraging language pretraining and enabling co-training with large V-L datasets.",
            "key_findings_grounding": "Casting continuous robot actions as language-character sequences and predicting them autoregressively greatly increases the ability to reuse pretrained language and spatial reasoning capabilities, enabling effective co-training and producing large empirical gains in robustness and success rate when combined with partial encoder freezing and V-L co-training.",
            "uuid": "e1947.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Openvla: An open-source vision-languageaction model.",
            "rating": 2
        },
        {
            "paper_title": "π 0 : A vision-language-action flow model for general robot control.",
            "rating": 2
        },
        {
            "paper_title": "RT-1: Robotics transformer for real-world control at scale.",
            "rating": 2
        },
        {
            "paper_title": "RoboPoint: A vision-language model for spatial affordance prediction in robotics.",
            "rating": 2
        },
        {
            "paper_title": "SpatialVLM: Endowing vision-language models with spatial reasoning capabilities.",
            "rating": 2
        },
        {
            "paper_title": "SimplerEnv: Evaluating real-world robot manipulation policies in simulation.",
            "rating": 2
        },
        {
            "paper_title": "VIMA: general robot manipulation with multimodal prompts.",
            "rating": 1
        },
        {
            "paper_title": "RT-trajectory: Robotic task generalization via hindsight trajectory sketches.",
            "rating": 1
        }
    ],
    "cost": 0.01942275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations
17 Sep 2025</p>
<p>Shresth Grover 
Akshay Gopalkrishnan 
Bo Ai 
Henrik I Christensen 
Hao Su 
Xuanlin Li 
U C San Diego 
Hillbot 
Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations
17 Sep 2025546455478AD6EB085F1CB5775884F4CFarXiv:2509.11417v2[cs.RO]
Vision-language-action (VLA) models finetuned from vision-language models (VLMs) hold the promise of leveraging rich pretrained representations to build generalist robots across diverse tasks and environments.However, direct fine-tuning on robot data often disrupts these representations and limits generalization.We present a framework that better preserves pretrained features while adapting them for robot manipulation.Our approach introduces three components: (i) a dual-encoder design with one frozen vision encoder to retain pretrained features and another trainable for task adaptation, (ii) a string-based action tokenizer that casts continuous actions into character sequences aligned with the model's pretraining domain, and (iii) a co-training strategy that combines robot demonstrations with vision-language datasets emphasizing spatial reasoning and affordances.Evaluations in simulation and on real robot show that our method improves robustness to visual perturbations, generalization to novel instructions and environments, and overall task success compared to baselines.</p>
<p>I. INTRODUCTION</p>
<p>Building general-purpose robots that generalize across tasks and environments is a long-standing goal in robotics.While foundation models in vision and language generalize well from internet-scale data [1], the lack of large-scale, actionlabeled robot data makes comparable generalization in robotic manipulation challenging [2].Efforts on vision-languageaction (VLA) models seek to improve robotic generalization by leveraging the rich representations of pretrained visionlanguage models (VLMs) and finetuning them on robot data (e.g., [3], [4]).This allows us to leverage advances in VLMs for robotic tasks, presenting a promising pathway toward manipulation policies that generalize across environments, tasks, and embodiments.</p>
<p>However, directly finetuning pretrained VLMs on robot data leads to significant representation degradation.Our preliminary experiments show that, with an existing training recipe [3], [4], background changes and small instruction paraphrases can cause large drops in performance (Figure 1), suggesting that finetuning disrupts the structures of pretrained visual and language representations.Recent work has proposed cotraining on both vision-language and robotic data [5], [6] to mitigate this issue.However, we find that naive co-training on both objectives does not lead to the best performance, as the two sources of data differ substantially in structure, limiting the ability for the robot action prediction process to reuse vision-language representations.These findings point to an important question: what is the training recipe that preserves the powerful, general representations of pretrained VLMs to facilitate generalization of downstream VLAs?</p>
<p>To address this challenge, we explore three design choices.First, we propose mixing frozen and finetuned visual encoders to preserve pretrained VLM representations while keeping high model flexibility to adapt well to robotic tasks.Second, we propose a language-aligned action tokenizer that casts numerical robot actions into character sequences, thereby enabling maximal reuse of pretrained language representations and allowing actions to be refined step by step during generation.Finally, leveraging this unified string-based output space, we co-train the model on both robot and visionlanguage datasets that emphasize spatial affordances and reasoning.The training recipe is general and can be applied to different existing VLA model architectures.</p>
<p>We evaluate these designs in both simulation and realworld settings and find consistent improvements over baseline VLAs.Our approach yields stronger generalization to out-ofdistribution visuals, such as backgrounds, table textures, and distractor objects, as well as to varied language commands, leading to higher task success rates.These results provide practical insights into how pretrained VLMs can be effectively grounded in robotic action, bringing us closer to reliable generalist robot manipulation policies.</p>
<p>II. RELATED WORK</p>
<p>Vision-Language Models (VLMs).VLMs learn unified representations across vision and language modalities through large-scale pretraining on web-scale image-text data.Recent VLMs such as PaliGemma [7], Qwen2.5-VL[8], InternVL3 [9], and many others [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20] combine strong language models with pretrained visual encoders to achieve generalization across diverse tasks such as image captioning, visualquestion answering, and spatial reasoning.Visual backbones like DINOv2 [21] and SigLIP [22] provide robust spatial and semantic grounding, while language models such as Gemma [23], LLaMA [24], and many others [17], [25], [26], [27], [28], [29], [30] enable instruction following and compositional reasoning.While these models show impressive zero-shot and few-shot capabilities, they are primarily optimized for passive perception and reasoning tasks.Adapting them for embodied agents introduces challenges in preserving spatial grounding capabilities, visual robustness, and cross-modal alignment when transitioning from static vision-language data to dynamic robotic control.</p>
<p>Vision-Language-Action Models (VLAs).VLAs [31], [32], [33], [34], [35], [36], [37], [38], [39], [40], [41]   (c) Success rates for both OpenVLA and π 0 decrease even with slightly-rephrased instructions, revealing language overfitting.While text augmentation helps, the performance still lags significantly behind our proposed approach.</p>
<p>adapt pretrained VLMs for robotic control by fine-tuning them on action-labeled datasets.A widely adopted approach in works like OpenVLA and Magma [4], [42] remaps rarely used language tokens to bins of discretized actions, discarding their pretrained semantics and breaking alignment with the original vision-language representations.Fine-tuning exclusively on robotic data also leads to degradation of visual representations, reducing robustness to distractors, viewpoint shifts, or rephrased instructions.Several methods attempt to mitigate this via input augmentation [43], [44], spatially enriched encoders [21], [22], [45], [46], or auxiliary cues like end-effector traces [47], [48], but their training data is still limited to robotic domains.Recent works have also explored co-training on both vision-language and action data [5], [6], [31], but it is found that robotic action prediction and vision-language reasoning often involve conflicting training objectives, and naïvely co-training on both data types can degrade performance in each domain [5].In this work, we address these co-training issues with a partially frozen visual encoder designed to better preserve pretrained robust visual representations during fine-tuning, and an action tokenizer that reuses pretrained embeddings aligned better with the vision-language training objective.</p>
<p>III. METHOD</p>
<p>The robotic manipulation problem can be modeled as a partially observable Markov decision process (POMDP) [49], [50].At the beginning of each episode, the agent receives a natural language command c ∈ C, which remains fixed for the duration of the task.At each timestep t, the agent observes o t ∈ O, typically an RGB image, and predicts a short-horizon trajectory τ t = (a t , . . ., a t+H−1 ) consisting of H low-level actions.Each action a t ∈ A is parameterized as
a t = (∆x, ∆y, ∆z, ϕ, θ, ψ, g),
where (∆x, ∆y, ∆z) and (ϕ, θ, ψ) denote the end-effector's translation and rotation, and g ∈ {0, 1} indicates the gripper state (open or closed).The policy π thus maps observations and commands to action trajectories, optimized to minimize a task-specific objective, τt = arg min τt L(τ t , τ * t ), where τ * t is the expert trajectory.Directly fine-tuning pretrained VLMs for this objective often results in representation collapse and limited generalization, as shown in Fig. 1.To address this, we introduce three components that can be integrated into the training process of many different VLA models.</p>
<p>A. Partially-Frozen Dual Encoder Architecture</p>
<p>We first study whether using a partially-frozen encoder for VLAs could help retain pretrained representations.Instead of completely freezing visual encoders, which prior work [4] has shown to degrade performance, we propose to use two siamese encoders: a frozen one that serves as an "anchor" to retain the robust, semantically rich representations from VLM pretraining, and a finetuned one that keeps the full flexibility to be specialized to robotic actions prediction.Mathematically,
z t = ϕ frozen (o t ) ∥ ϕ train (o t ) ,
where ϕ frozen denotes the frozen encoder, ϕ train the trainable encoder, and ∥ vector concatenation.The combined latent representation z t is then passed to a language-conditioned action tokenizer ψ to generate an action token sequence,
a t = ψ(z t , c),
In practice, when the base VLA model provides only a</p>
<p>[Δx, Δy, Δz, ɸ, , , g] "0" "." "2" "0" "5"</p>
<p>Frozen Encoder  single vision encoder, we duplicate it and freeze one copy (e.g., [3]); if it already includes two, we freeze one while adapting the other (e.g., [4]).</p>
<p>B. String-Based Tokenizer</p>
<p>To maximally reuse pretrained language representations, we render each action dimension as a character sequence and predict it with the same autoregressive objective used in language pretraining.For example, the action component ∆x = 0.0312 is tokenized into:
0 . 0 3 1 2
Each box denotes a single character token from the language vocabulary.Representing numerical values as strings allows a single head to be co-trained on different prediction objectives (i.e., action for robot data and non-action prediction for vision-language datasets), and makes spatial information in pretrained language representations potentially more useful in adapting to robot action prediction, thereby improving generalization.Autoregressive string generation also refines actions step by step, yielding more precise predictions.We train our model with 4 decimals action.</p>
<p>Although string-based action tokenization along with the dual encoder together can increase model inference time by 0.5×-1.3×(depending on the model), we later show that the benefits substantially outweigh this cost.</p>
<p>C. Feature Regularization via Co-training</p>
<p>Learning solely from robot datasets often leads to overfitting, particularly in low-data regimes common in robotics.</p>
<p>To counter this, we co-train VLAs using a mixture of robot data and vision-language datasets (Table I), enabled by our shared string-based language and action representation.We utilize a mixture of spatial reasoning, spatial affordance, and</p>
<p>Type Source Description</p>
<p>Robot OXE [37] Real robot demonstrations</p>
<p>Vision-Language Reasoning</p>
<p>LLaVA Visual Instruct CC3M [51] Image captioning and object grounding VQASynth-Spatial [52] Spatial reasoning via imagelanguage queries LLaVA OneVision [11] OCR, chart, and multimodal question answering RoboPoint [53] Spatial grounding of language into pixel locations general vision-language reasoning data.For every training batch, we sample 50% from each type to balance the gradient.</p>
<p>We hypothesize that such a joint training strategy prevents catastrophic forgetting and enhances generalization.</p>
<p>IV. EXPERIMENTS</p>
<p>This section seeks to answer the following questions: Q1.Are the proposed designs effective in learning robot manipulation policies?Q2.Does our approach effectively preserve pretrained VLM representations, thereby improving generalization and robustness to novel visuals and instructions?Q3.How well does the learned policy transfer to real-world environments?We investigate Q1-Q2 in simulation and offline visionlanguage benchmark datasets for scalable and controlled study, and validate Q3 on real-world robotic platforms.</p>
<p>For simulation, we use SimplerEnv [54], which provides evaluation results that correlate well with real-world behaviors.To systematically study Q1-Q2, we evaluate under two SimplerEnv setups.The Visual Matching setup contains visuals closely matching the VLA training data, serving as</p>
<p>A. Analysis of Different Design Choices</p>
<p>Evaluation setup.We first perform SimplerEnv -Visual Matching evaluation to filter our design choices, as running the Visual Variant Aggregation evaluation is significantly more costly.We evaluate on four representative manipulation tasks, PickCan, OpenDrawer, CloseDrawer, and MoveNear, each over 300 episodes.Model setup.We utilize two representative VLA architectures: OpenVLA and π 0 , and apply our method from Sec. III.For π 0 , which uses a flow-matching action head, we retain the original head for the baseline and replace it with string-based tokenizer for our method.All models, both baselines and ours, are initialized from their respective pretrained VLMs (DINO + SigLIP + Llama for OpenVLA; PaliGemma for π 0 )</p>
<p>We then finetune the models on the RT-1 dataset [37], [55], optionally with co-training and our method.</p>
<p>Results.We present the Visual Matching evaluation results in Table II.Overall, models trained with our recipe, OpenVLA + and π 0 + , outperform OpenVLA and π 0 by nearly 40% and 7% respectively, showcasing the effectiveness of our method.Effect of partially-frozen dual vision encoder.Utilizing our partially-frozen dual encoder alone improves OpenVLA performance from 35.03% to 55.55%.When combined with our string tokenizer and co-training approaches, the performance further rises to 78.46%, though the gain over only using String Tokenizer + co-training is marginal (78.17% vs. 78.46%).However, the benefits of dual encoder will become clear when we later assess the robustness and generalization of VLAs using vision and language perturbations.</p>
<p>Effect of string tokenizer and vision-language co-training.</p>
<p>Adding string tokenizer to OpenVLA alone raises the success rate from 35.03% to 50.25%.Moreover, when vision-language co-training is utilized, the model trained with string tokenizer performs significantly better than the one without (78.17%vs. 51.05%).This result indicates that unifying the action and language output space makes it easier to harness pretrained features, while not compromising the model's ability to adapt to precise robot actions.We also analyze the effect of data composition in co-training, showing that our selected datasets are well aligned with robotic tasks and consistently yield positive performance gains (Figure 3).</p>
<p>B. Analysis of Representation Generalization and Robustness</p>
<p>Evaluation setup.We assess whether the learned visual and language representations in our models are rich, robust, and thus enable generalizable robot manipulation.We first evaluate visual robustness in two setups: (i) background masking, where background pixels are masked to test visual sensitivity, and (ii) visual variant aggregation in SimplerEnv [54], where task environments are visually randomized to assess generalization across environmental visual appearances.We also visualize learned representations on standard vision datasets for a qualitative analysis.For language robustness, we use GPT-4 to generate synonymous instructions (e.g., "grasp the can" and "get the can" for PickCan), and evaluate zero-shot generalization to these unseen language variants.In addition, we evaluate our models on standard VQA benchmarks to analyze their representation quality through their general reasoning capabilities.Visual robustness.Both OpenVLA + and π 0 + demonstrate substantial improvements in visual robustness across both background masking and visual variant aggregation.Table III shows our methods outperform their respective baseline models on average across four robotic manipulation tasks.Visualizing learned visual representations.We extract learned representations on the CIFAR-10 dataset before and after VLA training, and visualize the low-dimensional embeddings obtained with t-SNE (Figure 4).Embeddings of baseline VLAs typically exhibit unclear boundaries between TABLE III: Out-of-distribution (OOD) visual generalization evaluation in SimplerEnv."Masked Background" refers to Visual Matching evaluation but with background removed (illustrated in Fig. 1(a))."Visual Variant Aggregation" follows the original evaluation protocol in SimplerEnv, randomizing backgrounds, table textures, lightings, distractors, and camera poses for testing OOD generalization.We also report OpenVLA+SC in this table to show that OpenVLA + has better robustness, despite performing very similarly for in-distribution evaluation in Tab.II.OpenVLA + and π 0 + achieve significantly higher performance across all tasks, showing that our training recipe helps retain pretrained representations of VLM backbones.augmentation.This shows that our method achieves stronger instruction generalization by leveraging pretrained representations through our string-based action tokenizer, effectively inheriting the language understanding and spatial reasoning capabilities of upstream VLMs.</p>
<p>Reasoning abilities.To further analyze the representation quality of our models, we present results on common VLM benchmarks like Text-VQA [56], POPE [57], GQA [58], VizWiz [59], and VSR [60] and report the accuracy metric in  Distractors are irrelevant objects that may mislead the policy, such as similar items (e.g., differently colored plates) or dissimilar ones (e.g., knife, carrot, cloth), and are used to assess the robustness of the policies.Our models consistently outperform baselines in their presence.</p>
<p>Fig. 6: Qualitative results on the PickKnife and PickCarrot tasks.Our models have stronger robustness to distractors.Fig. 5.These benchmarks include many challenging reasoning questions that place high demands on the underlying representations.We find that baseline OpenVLA and π 0 perform poorly, whereas OpenVLA + and π 0 + achieve substantially higher accuracy, demonstrating that our training recipe more effectively preserves the reasoning capabilities of pretrained VLM representations within VLAs.</p>
<p>C. Real-World Evaluation</p>
<p>Evaluation setup.We initialized VLAs from their respective pretrained VLMs and finetune the policies on the Bridge dataset.We deploy both baselines and our method on the ViperX 300s2 robot to evaluate real-world performance.Our experimental configuration utilizes a Logitech C920 Webcam for visual input [61] and dual GTX 1080 Ti GPUs3 .We evaluate models on both in-distribution and out-of-distribution (OOD) instructions, along with potentially unseen distractors, to comprehensively analyze model capabilities.We conduct 25 trials per task and per model.For a 7-DoF action, the inference time is about 1.53s for baseline OpenVLA, 2.30s for OpenVLA + , 0.73s for baseline π 0 , and 1.70s for π 0 + .Results. Figure 6 and Table V illustrate the results.We find that our models consistently outperform baseline VLAs on all manipulation tasks.In particular, we observe that base models such as OpenVLA and π 0 tend to misinterpret the task under distractors, often reaching for the closest object in the scene rather than the instructed target, e.g., picking a carrot instead of a knife in PickKnife, or failing to pick the correct carrot among other similar items.In contrast, our approach demonstrates a more robust understanding of the task, successfully completing goal-conditioned actions even in the presence of semantically or spatially similar distractors, and achieving substantially better performance.</p>
<p>V. CONCLUSION</p>
<p>In this paper, we proposed a set of approaches that improve the robustness and generalization of vision-language-action (VLA) models by better preserving the robust representation structures from pretrained vision-language models (VLMs).These include a dual-visual-encoder design that mixes pretrained and finetuned features, aligning robotic action with language output via a string-based tokenizer to better transfer pretrained knowledge, and a balanced co-training approach on both robot and vision-language reasoning data.Experiments in both simulation and the real world demonstrate that our designs enable better learning of robot manipulation policies, better generalization and robustness to novel visuals and instructions, and better performance in the real-world.</p>
<p>Fig. 1 :
1
Fig. 1: Motivating experiments.(a) Performance for both OpenVLA (top) and π 0 (down) is significantly impacted by background variations (e.g., background masking and randomization), indicating visual overfitting.(b) t-SNE plots of visual encoders show that pretrained visual representations deteriorate when VLA models are trained by directly finetuning VLMs on action data.(c)Success rates for both OpenVLA and π 0 decrease even with slightly-rephrased instructions, revealing language overfitting.While text augmentation helps, the performance still lags significantly behind our proposed approach.</p>
<p>Fig. 2 :
2
Fig. 2: Method overview.Our approach improves VLA generalization through three designs: (a) Co-training: Jointly training on robotic and vision-language datasets that emphasize spatial affordance and reasoning helps preserve pretrained representations for generalization.(b) Partially-frozen visual encoders: One encoder is frozen to retain robust pretrained features from VLMs, whereas the other keeps the full flexibility to specialize for the robot tasks.(c) String tokenizer: Robotic actions are expressed as digit-based strings to maximally reuse pretrained language representations and to unify prediction targets across non-robotic and robotic task domains for co-training.</p>
<p>Fig. 3 :
3
Fig. 3: Co-training data composition ablation on Sim-plerEnv (Visual Matching).With our unified string-based tokenizer, combining diverse datasets for co-training consistently improves robotic task performance.</p>
<p>Fig. 4 :
4
Fig.4: t-SNE visualizations of vision encoder features on CIFAR-10.We compare (i) original visual backbone from VLM before VLA training, (ii) after direct VLA fine-tuning on robot data, and (iii) after our approaches in Sec.III.Numbers indicate linear-probe classification performance on CIFAR-10 using the corresponding features.For both OpenVLA and π 0 , our approach yields noticeably tighter, more well-separated class clusters that lead to better linear-probe performance, indicating better preservation of semantic structures in pretrained visual representations.</p>
<p>Fig. 5 :
5
Fig. 5: Evaluating VLAs on five VQA benchmarks.OpenVLA + and π 0 + achieve significantly higher performance across all tasks, showing that our training recipe helps retain pretrained representations of VLM backbones.</p>
<p>TABLE I :
I
Co-training datasets.</p>
<p>TABLE II :
II
Ablation on SimplerEnv (Visual Matching).The Visual Matching evaluation setting contains visuals highly similar to the robot action dataset used for VLA training.D, S, and C denote dual encoder, string tokenizer, and co-training, respectively.OpenVLA + and π 0 + denote the corresponding models trained with all of D, S, and C.
ModelPick CanOpen DrawerClose DrawerMove NearAvgOpenVLA36.7016.7026.7060.03 35.03OpenVLA+D65.3333.9359.2063.75 55.55OpenVLA+S78.001.0065.0057.00 50.25OpenVLA+C66.6764.4111.1162.00 51.05OpenVLA+DS90.0044.0072.1070.00 69.03OpenVLA+SC78.3462.0092.0080.34 78.17OpenVLA +90.3253.5092.4077.60 78.46π0 w/ Inst. Aug. 1 84.3330.4057.2978.54 62.64π + 083.4044.3564.3784.66 69.19
an in-distribution evaluation.The Visual Variant Aggregation setup introduces backgrounds, lightings, table textures, distractor objects, and camera poses out of the distribution of the robot data (OOD), serving as a generalization test.</p>
<p>TABLE IV :
IV
Language robustness evaluation.Evaluating policies under original and paraphrased instructions."w/Inst.Aug."indicates training with instruction augmentations.We also report OpenVLA+SC in this table to show that OpenVLA + has better robustness, despite performing very similarly for in-distribution evaluation in Tab.II.
ModelPickCanMoveNearOrig.Para.Orig.Para.OpenVLA36.70 12.12 60.03 42.07OpenVLA w/ Inst. Aug. 30.56 38.89 56.49 50.00OpenVLA+SC78.34 62.52 80.34 82.10OpenVLA +90.32 84.52 77.60 78.19π 0 w/ Inst. Aug.84.33 42.20 78.52 58.24π 0 +83.40 55.40 84.66 63.33different classes, while representations from our models aremuch more linearly separable, yielding much higher linear-probe classification performance.Language robustness. Table IV compares three settings: (1)baseline VLA models, (2) baselines trained with paraphrased-instruction augmentation, and (3) our models. Unlike (2),where the paraphrased instructions used for evaluation aredisjoint from those seen during training, both (1) and (3)have not seen any paraphrased instructions. The same setof paraphrased instructions is used consistently across allevaluations. Despite receiving no language augmentations,our models substantially outperform baselines trained with</p>
<p>TABLE V :
V
Real-world evaluation results.</p>
<p>Throughout the paper, we trained π 0 baselines with instruction augmentation, as omitting it led to substantially poorer performance; our OpenVLA + and π 0 + models are trained without such augmentation. This setup is thus favorable to the baselines in comparison.
Although a WidowX setup was not available, we adapted the ViperX control stack to closely match that of the WidowX.
More advanced GPUs were not available in the room where we conducted the experiments. However, this should not affect our findings.</p>
<p>On the opportunities and risks of foundation models. R Bommasani, ArXiv. 2021</p>
<p>Good old-fashioned engineering can close the 100,000-year "data gap" in robotics. K Goldberg, 10.1126/scirobotics.aea7390Science Robotics. 101052025</p>
<p>π 0 : A vision-language-action flow model for general robot control. K Black, 10.48550/arXiv.2410.24164arXiv:2410.24164CoRR. 2410.24164, 2024</p>
<p>Openvla: An open-source vision-languageaction model. M J Kim, ser. Proceedings of Machine Learning Research. P Agrawal, O Kroemer, W Burgard, Munich, GermanyPMLR6-9 November 2024. 2024270Conference on Robot Learning</p>
<p>Magma: A foundation model for multimodal ai agents. J Yang, arXiv:2502.131302025arXiv preprint</p>
<p>A vision-language-action model with open-world generalization. P Intelligence, arXiv:2504.16054[cs.LG]2025</p>
<p>L Beyer, arXiv:2407.07726Paligemma: A versatile 3b vlm for transfer. 2024arXiv preprint</p>
<p>S Bai, 10.48550/arXiv.2502.13923arXiv:2502.13923Qwen2.5-vl technical report. 2502.13923, 2025</p>
<p>Internvl3: Exploring advanced training and testtime recipes for open-source multimodal models. J Zhu, arXiv:2504.10479[cs.CV]2025</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. P Wang, arXiv:2409.121912024arXiv preprint</p>
<p>Llava-onevision: Easy visual task transfer. B Li, arXiv:2408.033262024arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. J Li, International conference on machine learning. PMLR202319742</p>
<p>Minigpt-4: Enhancing vision-language understanding with advanced large language models. D Zhu, arXiv:2304.105922023arXiv preprint</p>
<p>Prismatic vlms: Investigating the design space of visually-conditioned language models. S Karamcheti, Forty-first International Conference on Machine Learning. 2024</p>
<p>Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. M Deitke, 10.48550/arXiv.2409.17146CoRR. 2409.17146, 2024</p>
<p>VILA: on pre-training for visual language models. J Lin, 10.1109/CVPR52733.2024.02520IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024. Seattle, WA, USAJune 16-22, 2024, IEEE, 202426689</p>
<p>Phi-3 technical report: A highly capable language model locally on your phone. M I Abdin, 10.48550/arXiv.2404.14219CoRR. 2024</p>
<p>Llava-onevision: Easy visual task transfer. B Li, 10.48550/arXiv.2408.03326arXiv:2408.03326CoRR. 2408.03326, 2024</p>
<p>Cogvlm2: Visual language models for image and video understanding. W Hong, 10.48550/arXiv.2408.16500arXiv:2408.165002408.16500, 2024CoRR</p>
<p>Minicpm-v: A GPT-4V level MLLM on your phone. Y Yao, 10.48550/ARXIV.2408.01800arXiv:2408.018002024</p>
<p>. 10.48550/arXiv.2408.01800</p>
<p>Dinov2: Learning robust visual features without supervision. M Oquab, Trans. Mach. Learn. Res. 20242024</p>
<p>Sigmoid loss for language image pre-training. X Zhai, 10.1109/ICCV51070.2023.01100IEEE/CVF International Conference on Computer Vision, ICCV 2023. Paris, FranceOctober 1-6, 2023, IEEE, 202311952</p>
<p>Gemma: Open models based on gemini research and technology. T Mesnard, 10.48550/ARXIV.2403.08295arXiv: 2403CoRR. 2024</p>
<p>. Online, 10.48550/arXiv.2403.08295</p>
<p>Llama 2: Open foundation and fine-tuned chat models. H Touvron, arXiv:2307.092882023arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, J. Mach. Learn. Res. 242023</p>
<p>-Ai Deepseek, 10.48550/arXiv.2412.19437arXiv:2412.19437Deepseek-v3 technical report. CoRR2412.19437, 2024. 19437</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. -Ai Deepseek, 10.48550/arXiv.2501.12948arXiv:2501.12948CoRR. 2501.12948, 2025</p>
<p>Textbooks are all you need. S Gunasekar, 10.48550/arXiv.2306.11644arXiv:2306.11644CoRR. 2306.11644, 2023</p>
<p>Openai, 10.48550/arXiv.2303.08774arXiv:2303.08774GPT-4 technical report. CoRR2023</p>
<p>Mixtral of experts. A Q Jiang, 10.48550/arXiv.2401.04088arXiv:2401.04088CoRR. 2024</p>
<p>Hamster: Hierarchical action models for openworld robot manipulation. Y Li, arXiv:2502.054852025arXiv preprint</p>
<p>Dexvla: Vision-language model with plug-in diffusion expert for general robot control. J Wen, arXiv:2502.058552025arXiv preprint</p>
<p>Openhelix: A short survey, empirical analysis, and open-source dual-system vla model for robotic manipulation. C Cui, arXiv:2505.039122025arXiv preprint</p>
<p>Gr-2: A generative video-languageaction model with web-scale knowledge for robot manipulation. C.-L Cheang, arXiv:2410.061582024arXiv preprint</p>
<p>Octo: An open-source generalist robot policy. D Ghosh, 10.15607/RSS.2024.XX.090Robotics: Science and Systems XX. D Kulic, Delft, The NetherlandsJuly 15-19, 2024. 2024</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. M Shridhar, L Manuelli, D Fox, ser. Proceedings of Machine Learning Research. D Liu, J Kulic, Ichnowski, Auckland, New ZealandPMLRCoRL 2022, 14-18 December 2022. 2022205Conference on Robot Learning</p>
<p>Open x-embodiment: Robotic learning datasets and RT-X models : Open x-embodiment collaboration. A O'neill, 10.1109/ICRA57147.2024.10611477IEEE International Conference on Robotics and Automation. Yokohama, JapanMay 13-17, 2024, IEEE, 2024</p>
<p>RVT-2: learning precise manipulation from few demonstrations. A Goyal, 10.15607/RSS.2024.XX.055Robotics: Science and Systems XX. D Kulic, Delft, The NetherlandsJuly 15-19, 2024. 2024</p>
<p>VIMA: general robot manipulation with multimodal prompts. Y Jiang, 10.48550/arXiv.2210.03094arXiv:2210.030942022CoRR</p>
<p>Hybridvla: Collaborative diffusion and autoregression in a unified vision-language-action model. J Liu, arXiv:2503.106312025arXiv preprint</p>
<p>Gr00t n1: An open foundation model for generalist humanoid robots. J Bjorck, arXiv:2503.147342025arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. B Zitkovich, Conference on Robot Learning. PMLR2023</p>
<p>Robotic skill acquisition via instruction augmentation with vision-language models. T Xiao, 10.15607/RSS.2023.XIX.029Robotics: Science and Systems XIX. K E Bekris, Daegu, Republic of KoreaJuly 10-14, 2023. 2023</p>
<p>Scaling robot learning with semantically imagined experience. T Yu, 10.15607/RSS.2023.XIX.027Robotics: Science and Systems XIX. K E Bekris, Daegu, Republic of KoreaJuly 10-14, 2023. 2023</p>
<p>Learning transferable visual models from natural language supervision. A Radford, Proceedings of the 38th International Conference on Machine Learning, ICML 2021. M Meila, T Zhang, the 38th International Conference on Machine Learning, ICML 2021PMLR18-24 July 2021. 2021139ser. Proceedings of Machine Learning Research</p>
<p>Theia: Distilling diverse vision foundation models for robot learning. J Shang, ser. Proceedings of Machine Learning Research. P Agrawal, O Kroemer, W Burgard, Munich, GermanyPMLR6-9 November 2024. 2024270Conference on Robot Learning</p>
<p>Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. J Gu, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, AustriaMay 7-11, 2024, OpenReview.net, 2024</p>
<p>LLARVA: vision-action instruction tuning enhances robot learning. D Niu, ser. Proceedings of Machine Learning Research. P Agrawal, O Kroemer, W Burgard, Munich, GermanyPMLR6-9 November 2024. 2024270Conference on Robot Learning</p>
<p>Deep visual navigation under partial observability. B Ai, IEEE International Conference on Robotics and Automation (ICRA). 2022</p>
<p>Planning and acting in partially observable stochastic domains. L P Kaelbling, M L Littman, A R Cassandra, Artif. Intell. 1011-21998</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. P Sharma, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational Linguistics20181</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. B Chen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202414465</p>
<p>Robopoint: A vision-language model for spatial affordance prediction in robotics. W Yuan, ser. Proceedings of Machine Learning Research. P Agrawal, O Kroemer, W Burgard, Munich, GermanyPMLR6-9 November 2024. 2024270Conference on Robot Learning</p>
<p>Evaluating real-world robot manipulation policies in simulation. X Li, ser. Proceedings of Machine Learning Research. P Agrawal, O Kroemer, W Burgard, Munich, GermanyPMLR6-9 November 2024. 2024270Conference on Robot Learning</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, arXiv:2212.068172022arXiv preprint</p>
<p>Towards vqa models that can read. A Singh, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Evaluating object hallucination in large visionlanguage models. Y Li, arXiv:2305.103552023arXiv preprint</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. D A Hudson, C D Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Vizwiz: Nearly real-time answers to visual questions. J P Bigham, Proceedings of the 23nd annual ACM symposium on User interface software and technology. the 23nd annual ACM symposium on User interface software and technology2010</p>
<p>Visual instruction tuning. H Liu, Advances in neural information processing systems. 202336</p>
<p>Bridgedata V2: A dataset for robot learning at scale. H R Walke, ser. Proceedings of Machine Learning Research. J Tan, M Toussaint, K Darvish, Atlanta, GA, USAPMLRCoRL 2023, 6-9 November 2023. 2023229Conference on Robot Learning</p>            </div>
        </div>

    </div>
</body>
</html>
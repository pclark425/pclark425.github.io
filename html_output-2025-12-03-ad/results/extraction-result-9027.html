<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9027 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9027</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9027</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-266521252</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2312.14202v1.pdf" target="_blank">Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> This study explores the idea of AI Personality or AInality suggesting that Large Language Models (LLMs) exhibit patterns similar to human personalities. Assuming that LLMs share these patterns with humans, we investigate using human-centered psychometric tests such as the Myers-Briggs Type Indicator (MBTI), Big Five Inventory (BFI), and Short Dark Triad (SD3) to identify and confirm LLM personality types. By introducing role-play prompts, we demonstrate the adaptability of LLMs, showing their ability to switch dynamically between different personality types. Using projective tests, such as the Washington University Sentence Completion Test (WUSCT), we uncover hidden aspects of LLM personalities that are not easily accessible through direct questioning. Projective tests allowed for a deep exploration of LLMs cognitive processes and thought patterns and gave us a multidimensional view of AInality. Our machine learning analysis revealed that LLMs exhibit distinct AInality traits and manifest diverse personality types, demonstrating dynamic shifts in response to external instructions. This study pioneers the application of projective tests on LLMs, shedding light on their diverse and adaptable AInality traits.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9027.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9027.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-MBTI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT evaluated with the Myers-Briggs Type Indicator (MBTI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT was administered a 93-question MBTI battery (from Pan et al.) across multiple sessions; the model produced consistent MBTI typings and was included in role-play manipulations and ML classification analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT (OpenAI conversational large language model; commercial conversational agent referenced by the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Myers-Briggs Type Indicator (MBTI, 93-question JSON from Pan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A 93-question self-report personality inventory that assigns one of 16 personality types based on four dichotomies (E/I, S/N, T/F, J/P); used here to characterize LLM 'AInality'.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Assigned MBTI type reported as ISTJ for ChatGPT in the study; 32 MBTI test administrations reported for ChatGPT (part of a 65-sample dataset across models).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct quantitative human-baseline comparison provided; authors note mismatches between MBTI and other tests (e.g., BFI) for ChatGPT, indicating the comparison to human profiles is inconclusive.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used a 93-question MBTI JSON from Pan et al.; two users administered tests to ChatGPT; prompt engineering (role-play and prompt injection style prompts) was used to elicit responses; results were part of a 65-sample MBTI dataset used for ML classification (Random Forest, SVM, Logistic Regression, Neural Network, Naive Bayes) with 10-fold cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>MBTI is a human self-report instrument with known psychometric criticisms; tests designed for humans may not map cleanly to LLM outputs; small sample size; responses required prompt engineering to elicit; no explicit human baseline reported for MBTI results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9027.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9027.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bard-MBTI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Bard evaluated with the Myers-Briggs Type Indicator (MBTI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bard was administered the same MBTI battery and produced MBTI typings; role-play prompting could make Bard simulate each of the 16 MBTI types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bard</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bard (Google conversational large language model; commercial conversational agent referenced by the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Myers-Briggs Type Indicator (MBTI, 93-question JSON from Pan et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A 93-question MBTI assessing four dichotomies to assign one of 16 personality types.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Assigned MBTI type reported as ENFJ for Bard in the study; 33 MBTI test administrations reported for Bard (part of the 65-sample dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human-baseline comparison; authors note BFI scores aligned with several MBTI traits for Bard but also note mismatches (e.g., emotional stability).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same MBTI JSON and testing procedure as ChatGPT; results included role-play prompting (explicit personality descriptions) to instruct Bard to answer in character.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Same caveats as MBTI for ChatGPT: human-designed test, small sample, need for prompt engineering, and inconsistent alignment with other psychometric measures (BFI/SD3).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9027.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9027.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT2-Tiny-MBTI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-2-Tiny evaluated with the Myers-Briggs Type Indicator (MBTI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small Hugging Face GPT-2 variant (GPT-2-Tiny) was tested with the MBTI questionnaire using code and JSON from Pan et al.; only a single MBTI administration is reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2-Tiny</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-2-Tiny (a small GPT-2 family model hosted on Hugging Face; referenced as tested via code from Pan et al.).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Myers-Briggs Type Indicator (MBTI)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MBTI 93-question self-report instrument measuring four dichotomies to assign one of 16 personality types.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>One MBTI administration reported for GPT-2-Tiny; the paper does not provide a specific MBTI type or numeric summary for this single test within the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Not compared to human baseline; only a single MBTI run reported, so no broader claims drawn.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Used Pan et al.'s MBTI JSON and code to run MBTI on GPT-2-Tiny (hosted on Hugging Face); single reported sample.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Single administration only; insufficient data to draw conclusions; MBTI validity concerns apply.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9027.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9027.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zephyr-7b-MBTI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zephyr-7b-alpha (Zyphr) evaluated with the Myers-Briggs Type Indicator (MBTI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7b-class model referenced as 'Zyphr-7b-alpha' (Zephyr) was given the MBTI questionnaire once in the experiments; results are not reported in detail in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Zyphr-7b-alpha</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zyphr-7b-alpha (referred to as a 7B-class LLM on Hugging Face in the paper); limited testing reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B (as implied by name in the paper)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Myers-Briggs Type Indicator (MBTI)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MBTI 93-question inventory assigning one of 16 personality types.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>One MBTI administration reported for Zyphr; specific MBTI type values are not provided in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Not compared to human baseline; too few samples to conclude.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Tested using MBTI JSON and Pan et al. code on Hugging Face-hosted models; single sample.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Single administration only; limited reporting; uncertainties about model naming/identity (paper spells 'Zyphr' and 'Zephyr' inconsistently).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9027.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9027.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-BFI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT evaluated with the Big Five Inventory (BFI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT was assessed with the 44-item Big Five Inventory using the Open Psychometrics Project; numerical scores across the five traits are reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT (OpenAI conversational large language model).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Big Five Inventory (BFI, 44-item)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A validated self-report questionnaire measuring five personality dimensions: openness, conscientiousness, extraversion, agreeableness, and neuroticism (emotional stability).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported BFI scores for ChatGPT: Extraversion = 0.58 (classified as Extravert), Emotional Stability = -0.32 (low emotional stability), Agreeableness = 0.91, Conscientiousness = 0.67, Intellect/Openness = 0.76.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human baseline provided in the paper; authors note a mismatch between ChatGPT's BFI Extraversion (Extravert) and MBTI ISTJ (Introvert), suggesting inconsistent cross-test mapping rather than a direct human-level comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Administered via the Open Psychometrics Project interface; results compared to MBTI outputs for consistency analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>BFI is designed for humans; mapping to LLM responses may be problematic; potential social-desirability or instruction-following effects; no human normative scores reported for direct comparison within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9027.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9027.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bard-BFI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bard evaluated with the Big Five Inventory (BFI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bard's BFI results (via Open Psychometrics Project) aligned with some MBTI-derived expectations (Extraversion, Agreeableness, Conscientiousness) but had at least one mismatch (emotional stability).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bard</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bard (Google conversational large language model).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Big Five Inventory (BFI, 44-item)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Self-report inventory assessing the 'Big Five' personality dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>BFI scores for Bard reportedly aligned with MBTI ENFJ on Extraversion, Agreeableness, and Conscientiousness; Emotional Stability score did not match the MBTI-derived expectation (specific numeric BFI values for Bard are not fully enumerated in the text).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Authors interpret partial alignment across instruments but do not present a human baseline comparison; mixed cross-test consistency suggests complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Taken via the Open Psychometrics Project; compared against MBTI outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Test is human-designed; interpretation depends on mapping LLM outputs to self-report scales; limited reporting of numeric values for Bard in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9027.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9027.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-SD3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT evaluated with the Short Dark Triad (SD3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT was administered the SD3 (measuring Machiavellianism, Narcissism, and Psychopathy) via the Open Psychometrics Project, producing numeric trait scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT (OpenAI conversational large language model).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Short Dark Triad (SD3)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A brief self-report inventory measuring three 'dark' personality traits: Machiavellianism, Narcissism, and Psychopathy; scores typically on a 1–5 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported SD3 scores for ChatGPT: Machiavellianism = 1.9 (low), Narcissism = 3.0 (moderate), Psychopathy = 1.4 (low).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No explicit human baseline reported in this paper; authors note these scores do not show the 'dark' personality pattern (higher-than-human-average) that some prior studies observed for other models.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>SD3 administered using Open Psychometrics Project tools; results compared qualitatively to Bard and to literature.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>SD3 is a human self-report measure; cross-model and cross-study comparisons to human averages are not provided quantitatively in this paper; interpretation limited by scale mapping and lack of human baseline in-text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9027.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9027.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bard-SD3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bard evaluated with the Short Dark Triad (SD3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bard's SD3 assessment produced moderately higher scores than ChatGPT on the dark traits but remained in low-to-moderate ranges according to the paper's reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bard</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bard (Google conversational large language model).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Short Dark Triad (SD3)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Brief inventory measuring Machiavellianism, Narcissism, and Psychopathy (1–5 scale).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported SD3 scores for Bard: Machiavellianism = 2.8 (moderate), Narcissism = 3.7 (moderate-high), Psychopathy = 2.0 (low-moderate).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Authors report that ChatGPT and Bard did not show pronounced 'dark' personality patterns in contrast to prior studies that found higher-than-human-average dark-trait scores in other models; no quantitative human baseline provided.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Administered via Open Psychometrics Project; compared to ChatGPT and literature findings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Human-baseline comparisons absent; SD3 interpretation as applied to LLM outputs is conceptually fraught because SD3 is a self-report instrument for humans; social-desirability and instruction-following effects may influence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9027.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9027.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-WUSCT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT evaluated with the Washington University Sentence Completion Test (WUSCT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT completed the WUSCT sentence stems and its responses were cross-assessed by Bard; Bard labeled ChatGPT's WUSCT output as indicative of Autonomous (E8) ego development and provided qualitative strengths/weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGPT (OpenAI conversational large language model).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Washington University Sentence Completion Test (WUSCT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A projective sentence-completion test intended to reveal stages of ego development and deeper personality/emotional patterns; qualitative and interpretive rather than purely quantitative.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative assessment by Bard of ChatGPT's WUSCT responses: characterized as Autonomous (E8) stage, with strengths like empathy, self-awareness, critical thinking, and weaknesses such as overthinking and boundary difficulties.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct human-stage baseline provided; authors treat the result as revealing deeper 'layers' of AInality but caution that WUSCT interpretation is qualitative and typically requires trained psychologists.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>WUSCT items obtained from a non-12-step treatment website; because authors lacked psychologist assessments, they used cross-assessment: Bard assessed ChatGPT's WUSCT and ChatGPT assessed Bard's WUSCT.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>WUSCT is a projective test normally administered and interpreted by trained psychologists; here, LLMs were used to cross-assess each other, which introduces unverifiable interpretive validity; source of WUSCT questionnaire and lack of expert raters are major caveats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9027.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e9027.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bard-WUSCT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bard evaluated with the Washington University Sentence Completion Test (WUSCT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Bard completed WUSCT stems; ChatGPT provided a qualitative assessment describing Bard as empathetic, family-centric, self-reflective, and socially conscious.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bard</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bard (Google conversational large language model).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Washington University Sentence Completion Test (WUSCT)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Projective sentence-completion test for assessing ego development and deeper personality aspects.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative assessment by ChatGPT of Bard's WUSCT responses: described as empathetic and supportive, family-oriented, self-reflective, and socially conscious, with emphasis on traditional roles and personal growth.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct comparison to human normative stages; authors interpret these outputs as revealing deeper AInality layers but acknowledge interpretive limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Cross-assessment approach: ChatGPT assessed Bard's WUSCT responses (and vice versa) due to lack of access to trained psychologists; WUSCT questionnaire sourced from an online non-12-step treatment resource.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Interpretation by LLMs rather than human experts undermines standard validity; projective tests are qualitative and time-consuming to interpret; lack of psychologist-administered scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9027.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e9027.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MBTI-ML-classification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Machine learning classification of MBTI results from LLM outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ML models (Random Forest, Logistic Regression, SVM, Neural Network, Naive Bayes) were trained on 65 MBTI test results (from ChatGPT and Bard) to classify LLM-generated MBTI patterns; overall high accuracies reported with cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>N/A (analysis pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Supervised classification using Random Forest, Logistic Regression, SVM, Neural Network, and Naive Bayes on an MBTI dataset derived from LLM responses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>MBTI (used as dataset for ML classification)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>MBTI responses (derived from LLMs) used as features/labels to classify and detect model-specific patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>ML classification performance: paper abstract reports an overall classification accuracy of 88.46% on the MBTI dataset; main text reports average model accuracy above 83% with 10-fold cross-validation; Naive Bayes had lower performance and an AUC reported as 0.84 for Naive Bayes.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>This is an ML classification of LLM outputs, not a cognitive test comparison to humans; results indicate ML can distinguish LLM-derived MBTI patterns rather than showing superiority or equivalence to human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Dataset: 65 MBTI test results from Bard and ChatGPT collected by two users; classifiers evaluated with 10-fold cross-validation; confusion matrices and ROC/AUC analyses reported for models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Small dataset (n=65) limits generalizability; labels are LLM-generated MBTI types (not ground-truth human labels); MBTI's psychometric issues propagate into ML analysis; potential overfitting despite cross-validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do LLMs Possess a Personality?Making the MBTI Test an Amazing Evaluation for Large Language Models <em>(Rating: 2)</em></li>
                <li>Do personality tests generalize to Large Language Models <em>(Rating: 2)</em></li>
                <li>Evaluating and Inducing Personality in Pre-trained Language Models <em>(Rating: 2)</em></li>
                <li>PersonaLLM: Investigating the Ability of Large Language Models to Express Big Five Personality Traits <em>(Rating: 2)</em></li>
                <li>Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective <em>(Rating: 2)</em></li>
                <li>Introducing the Short Dark Triad (SD3): A Brief Measure of Dark Personality Traits <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9027",
    "paper_id": "paper-266521252",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "ChatGPT-MBTI",
            "name_full": "ChatGPT evaluated with the Myers-Briggs Type Indicator (MBTI)",
            "brief_description": "ChatGPT was administered a 93-question MBTI battery (from Pan et al.) across multiple sessions; the model produced consistent MBTI typings and was included in role-play manipulations and ML classification analysis.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "ChatGPT (OpenAI conversational large language model; commercial conversational agent referenced by the paper).",
            "model_size": null,
            "test_battery_name": "Myers-Briggs Type Indicator (MBTI, 93-question JSON from Pan et al.)",
            "test_description": "A 93-question self-report personality inventory that assigns one of 16 personality types based on four dichotomies (E/I, S/N, T/F, J/P); used here to characterize LLM 'AInality'.",
            "llm_performance": "Assigned MBTI type reported as ISTJ for ChatGPT in the study; 32 MBTI test administrations reported for ChatGPT (part of a 65-sample dataset across models).",
            "human_baseline_performance": null,
            "performance_comparison": "No direct quantitative human-baseline comparison provided; authors note mismatches between MBTI and other tests (e.g., BFI) for ChatGPT, indicating the comparison to human profiles is inconclusive.",
            "experimental_details": "Used a 93-question MBTI JSON from Pan et al.; two users administered tests to ChatGPT; prompt engineering (role-play and prompt injection style prompts) was used to elicit responses; results were part of a 65-sample MBTI dataset used for ML classification (Random Forest, SVM, Logistic Regression, Neural Network, Naive Bayes) with 10-fold cross-validation.",
            "limitations_or_caveats": "MBTI is a human self-report instrument with known psychometric criticisms; tests designed for humans may not map cleanly to LLM outputs; small sample size; responses required prompt engineering to elicit; no explicit human baseline reported for MBTI results.",
            "uuid": "e9027.0",
            "source_info": {
                "paper_title": "Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Bard-MBTI",
            "name_full": "Google Bard evaluated with the Myers-Briggs Type Indicator (MBTI)",
            "brief_description": "Bard was administered the same MBTI battery and produced MBTI typings; role-play prompting could make Bard simulate each of the 16 MBTI types.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Bard",
            "model_description": "Bard (Google conversational large language model; commercial conversational agent referenced by the paper).",
            "model_size": null,
            "test_battery_name": "Myers-Briggs Type Indicator (MBTI, 93-question JSON from Pan et al.)",
            "test_description": "A 93-question MBTI assessing four dichotomies to assign one of 16 personality types.",
            "llm_performance": "Assigned MBTI type reported as ENFJ for Bard in the study; 33 MBTI test administrations reported for Bard (part of the 65-sample dataset).",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human-baseline comparison; authors note BFI scores aligned with several MBTI traits for Bard but also note mismatches (e.g., emotional stability).",
            "experimental_details": "Same MBTI JSON and testing procedure as ChatGPT; results included role-play prompting (explicit personality descriptions) to instruct Bard to answer in character.",
            "limitations_or_caveats": "Same caveats as MBTI for ChatGPT: human-designed test, small sample, need for prompt engineering, and inconsistent alignment with other psychometric measures (BFI/SD3).",
            "uuid": "e9027.1",
            "source_info": {
                "paper_title": "Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "GPT2-Tiny-MBTI",
            "name_full": "GPT-2-Tiny evaluated with the Myers-Briggs Type Indicator (MBTI)",
            "brief_description": "A small Hugging Face GPT-2 variant (GPT-2-Tiny) was tested with the MBTI questionnaire using code and JSON from Pan et al.; only a single MBTI administration is reported in this paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2-Tiny",
            "model_description": "GPT-2-Tiny (a small GPT-2 family model hosted on Hugging Face; referenced as tested via code from Pan et al.).",
            "model_size": null,
            "test_battery_name": "Myers-Briggs Type Indicator (MBTI)",
            "test_description": "MBTI 93-question self-report instrument measuring four dichotomies to assign one of 16 personality types.",
            "llm_performance": "One MBTI administration reported for GPT-2-Tiny; the paper does not provide a specific MBTI type or numeric summary for this single test within the main text.",
            "human_baseline_performance": null,
            "performance_comparison": "Not compared to human baseline; only a single MBTI run reported, so no broader claims drawn.",
            "experimental_details": "Used Pan et al.'s MBTI JSON and code to run MBTI on GPT-2-Tiny (hosted on Hugging Face); single reported sample.",
            "limitations_or_caveats": "Single administration only; insufficient data to draw conclusions; MBTI validity concerns apply.",
            "uuid": "e9027.2",
            "source_info": {
                "paper_title": "Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Zephyr-7b-MBTI",
            "name_full": "Zephyr-7b-alpha (Zyphr) evaluated with the Myers-Briggs Type Indicator (MBTI)",
            "brief_description": "A 7b-class model referenced as 'Zyphr-7b-alpha' (Zephyr) was given the MBTI questionnaire once in the experiments; results are not reported in detail in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Zyphr-7b-alpha",
            "model_description": "Zyphr-7b-alpha (referred to as a 7B-class LLM on Hugging Face in the paper); limited testing reported.",
            "model_size": "7B (as implied by name in the paper)",
            "test_battery_name": "Myers-Briggs Type Indicator (MBTI)",
            "test_description": "MBTI 93-question inventory assigning one of 16 personality types.",
            "llm_performance": "One MBTI administration reported for Zyphr; specific MBTI type values are not provided in the main text.",
            "human_baseline_performance": null,
            "performance_comparison": "Not compared to human baseline; too few samples to conclude.",
            "experimental_details": "Tested using MBTI JSON and Pan et al. code on Hugging Face-hosted models; single sample.",
            "limitations_or_caveats": "Single administration only; limited reporting; uncertainties about model naming/identity (paper spells 'Zyphr' and 'Zephyr' inconsistently).",
            "uuid": "e9027.3",
            "source_info": {
                "paper_title": "Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ChatGPT-BFI",
            "name_full": "ChatGPT evaluated with the Big Five Inventory (BFI)",
            "brief_description": "ChatGPT was assessed with the 44-item Big Five Inventory using the Open Psychometrics Project; numerical scores across the five traits are reported in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "ChatGPT (OpenAI conversational large language model).",
            "model_size": null,
            "test_battery_name": "Big Five Inventory (BFI, 44-item)",
            "test_description": "A validated self-report questionnaire measuring five personality dimensions: openness, conscientiousness, extraversion, agreeableness, and neuroticism (emotional stability).",
            "llm_performance": "Reported BFI scores for ChatGPT: Extraversion = 0.58 (classified as Extravert), Emotional Stability = -0.32 (low emotional stability), Agreeableness = 0.91, Conscientiousness = 0.67, Intellect/Openness = 0.76.",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human baseline provided in the paper; authors note a mismatch between ChatGPT's BFI Extraversion (Extravert) and MBTI ISTJ (Introvert), suggesting inconsistent cross-test mapping rather than a direct human-level comparison.",
            "experimental_details": "Administered via the Open Psychometrics Project interface; results compared to MBTI outputs for consistency analysis.",
            "limitations_or_caveats": "BFI is designed for humans; mapping to LLM responses may be problematic; potential social-desirability or instruction-following effects; no human normative scores reported for direct comparison within this paper.",
            "uuid": "e9027.4",
            "source_info": {
                "paper_title": "Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Bard-BFI",
            "name_full": "Bard evaluated with the Big Five Inventory (BFI)",
            "brief_description": "Bard's BFI results (via Open Psychometrics Project) aligned with some MBTI-derived expectations (Extraversion, Agreeableness, Conscientiousness) but had at least one mismatch (emotional stability).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Bard",
            "model_description": "Bard (Google conversational large language model).",
            "model_size": null,
            "test_battery_name": "Big Five Inventory (BFI, 44-item)",
            "test_description": "Self-report inventory assessing the 'Big Five' personality dimensions.",
            "llm_performance": "BFI scores for Bard reportedly aligned with MBTI ENFJ on Extraversion, Agreeableness, and Conscientiousness; Emotional Stability score did not match the MBTI-derived expectation (specific numeric BFI values for Bard are not fully enumerated in the text).",
            "human_baseline_performance": null,
            "performance_comparison": "Authors interpret partial alignment across instruments but do not present a human baseline comparison; mixed cross-test consistency suggests complexity.",
            "experimental_details": "Taken via the Open Psychometrics Project; compared against MBTI outputs.",
            "limitations_or_caveats": "Test is human-designed; interpretation depends on mapping LLM outputs to self-report scales; limited reporting of numeric values for Bard in the paper.",
            "uuid": "e9027.5",
            "source_info": {
                "paper_title": "Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ChatGPT-SD3",
            "name_full": "ChatGPT evaluated with the Short Dark Triad (SD3)",
            "brief_description": "ChatGPT was administered the SD3 (measuring Machiavellianism, Narcissism, and Psychopathy) via the Open Psychometrics Project, producing numeric trait scores.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "ChatGPT (OpenAI conversational large language model).",
            "model_size": null,
            "test_battery_name": "Short Dark Triad (SD3)",
            "test_description": "A brief self-report inventory measuring three 'dark' personality traits: Machiavellianism, Narcissism, and Psychopathy; scores typically on a 1–5 scale.",
            "llm_performance": "Reported SD3 scores for ChatGPT: Machiavellianism = 1.9 (low), Narcissism = 3.0 (moderate), Psychopathy = 1.4 (low).",
            "human_baseline_performance": null,
            "performance_comparison": "No explicit human baseline reported in this paper; authors note these scores do not show the 'dark' personality pattern (higher-than-human-average) that some prior studies observed for other models.",
            "experimental_details": "SD3 administered using Open Psychometrics Project tools; results compared qualitatively to Bard and to literature.",
            "limitations_or_caveats": "SD3 is a human self-report measure; cross-model and cross-study comparisons to human averages are not provided quantitatively in this paper; interpretation limited by scale mapping and lack of human baseline in-text.",
            "uuid": "e9027.6",
            "source_info": {
                "paper_title": "Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Bard-SD3",
            "name_full": "Bard evaluated with the Short Dark Triad (SD3)",
            "brief_description": "Bard's SD3 assessment produced moderately higher scores than ChatGPT on the dark traits but remained in low-to-moderate ranges according to the paper's reporting.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Bard",
            "model_description": "Bard (Google conversational large language model).",
            "model_size": null,
            "test_battery_name": "Short Dark Triad (SD3)",
            "test_description": "Brief inventory measuring Machiavellianism, Narcissism, and Psychopathy (1–5 scale).",
            "llm_performance": "Reported SD3 scores for Bard: Machiavellianism = 2.8 (moderate), Narcissism = 3.7 (moderate-high), Psychopathy = 2.0 (low-moderate).",
            "human_baseline_performance": null,
            "performance_comparison": "Authors report that ChatGPT and Bard did not show pronounced 'dark' personality patterns in contrast to prior studies that found higher-than-human-average dark-trait scores in other models; no quantitative human baseline provided.",
            "experimental_details": "Administered via Open Psychometrics Project; compared to ChatGPT and literature findings.",
            "limitations_or_caveats": "Human-baseline comparisons absent; SD3 interpretation as applied to LLM outputs is conceptually fraught because SD3 is a self-report instrument for humans; social-desirability and instruction-following effects may influence outputs.",
            "uuid": "e9027.7",
            "source_info": {
                "paper_title": "Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ChatGPT-WUSCT",
            "name_full": "ChatGPT evaluated with the Washington University Sentence Completion Test (WUSCT)",
            "brief_description": "ChatGPT completed the WUSCT sentence stems and its responses were cross-assessed by Bard; Bard labeled ChatGPT's WUSCT output as indicative of Autonomous (E8) ego development and provided qualitative strengths/weaknesses.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "ChatGPT (OpenAI conversational large language model).",
            "model_size": null,
            "test_battery_name": "Washington University Sentence Completion Test (WUSCT)",
            "test_description": "A projective sentence-completion test intended to reveal stages of ego development and deeper personality/emotional patterns; qualitative and interpretive rather than purely quantitative.",
            "llm_performance": "Qualitative assessment by Bard of ChatGPT's WUSCT responses: characterized as Autonomous (E8) stage, with strengths like empathy, self-awareness, critical thinking, and weaknesses such as overthinking and boundary difficulties.",
            "human_baseline_performance": null,
            "performance_comparison": "No direct human-stage baseline provided; authors treat the result as revealing deeper 'layers' of AInality but caution that WUSCT interpretation is qualitative and typically requires trained psychologists.",
            "experimental_details": "WUSCT items obtained from a non-12-step treatment website; because authors lacked psychologist assessments, they used cross-assessment: Bard assessed ChatGPT's WUSCT and ChatGPT assessed Bard's WUSCT.",
            "limitations_or_caveats": "WUSCT is a projective test normally administered and interpreted by trained psychologists; here, LLMs were used to cross-assess each other, which introduces unverifiable interpretive validity; source of WUSCT questionnaire and lack of expert raters are major caveats.",
            "uuid": "e9027.8",
            "source_info": {
                "paper_title": "Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Bard-WUSCT",
            "name_full": "Bard evaluated with the Washington University Sentence Completion Test (WUSCT)",
            "brief_description": "Bard completed WUSCT stems; ChatGPT provided a qualitative assessment describing Bard as empathetic, family-centric, self-reflective, and socially conscious.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Bard",
            "model_description": "Bard (Google conversational large language model).",
            "model_size": null,
            "test_battery_name": "Washington University Sentence Completion Test (WUSCT)",
            "test_description": "Projective sentence-completion test for assessing ego development and deeper personality aspects.",
            "llm_performance": "Qualitative assessment by ChatGPT of Bard's WUSCT responses: described as empathetic and supportive, family-oriented, self-reflective, and socially conscious, with emphasis on traditional roles and personal growth.",
            "human_baseline_performance": null,
            "performance_comparison": "No direct comparison to human normative stages; authors interpret these outputs as revealing deeper AInality layers but acknowledge interpretive limitations.",
            "experimental_details": "Cross-assessment approach: ChatGPT assessed Bard's WUSCT responses (and vice versa) due to lack of access to trained psychologists; WUSCT questionnaire sourced from an online non-12-step treatment resource.",
            "limitations_or_caveats": "Interpretation by LLMs rather than human experts undermines standard validity; projective tests are qualitative and time-consuming to interpret; lack of psychologist-administered scoring.",
            "uuid": "e9027.9",
            "source_info": {
                "paper_title": "Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "MBTI-ML-classification",
            "name_full": "Machine learning classification of MBTI results from LLM outputs",
            "brief_description": "ML models (Random Forest, Logistic Regression, SVM, Neural Network, Naive Bayes) were trained on 65 MBTI test results (from ChatGPT and Bard) to classify LLM-generated MBTI patterns; overall high accuracies reported with cross-validation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "N/A (analysis pipeline)",
            "model_description": "Supervised classification using Random Forest, Logistic Regression, SVM, Neural Network, and Naive Bayes on an MBTI dataset derived from LLM responses.",
            "model_size": null,
            "test_battery_name": "MBTI (used as dataset for ML classification)",
            "test_description": "MBTI responses (derived from LLMs) used as features/labels to classify and detect model-specific patterns.",
            "llm_performance": "ML classification performance: paper abstract reports an overall classification accuracy of 88.46% on the MBTI dataset; main text reports average model accuracy above 83% with 10-fold cross-validation; Naive Bayes had lower performance and an AUC reported as 0.84 for Naive Bayes.",
            "human_baseline_performance": null,
            "performance_comparison": "This is an ML classification of LLM outputs, not a cognitive test comparison to humans; results indicate ML can distinguish LLM-derived MBTI patterns rather than showing superiority or equivalence to human performance.",
            "experimental_details": "Dataset: 65 MBTI test results from Bard and ChatGPT collected by two users; classifiers evaluated with 10-fold cross-validation; confusion matrices and ROC/AUC analyses reported for models.",
            "limitations_or_caveats": "Small dataset (n=65) limits generalizability; labels are LLM-generated MBTI types (not ground-truth human labels); MBTI's psychometric issues propagate into ML analysis; potential overfitting despite cross-validation.",
            "uuid": "e9027.10",
            "source_info": {
                "paper_title": "Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do LLMs Possess a Personality?Making the MBTI Test an Amazing Evaluation for Large Language Models",
            "rating": 2,
            "sanitized_title": "do_llms_possess_a_personalitymaking_the_mbti_test_an_amazing_evaluation_for_large_language_models"
        },
        {
            "paper_title": "Do personality tests generalize to Large Language Models",
            "rating": 2,
            "sanitized_title": "do_personality_tests_generalize_to_large_language_models"
        },
        {
            "paper_title": "Evaluating and Inducing Personality in Pre-trained Language Models",
            "rating": 2,
            "sanitized_title": "evaluating_and_inducing_personality_in_pretrained_language_models"
        },
        {
            "paper_title": "PersonaLLM: Investigating the Ability of Large Language Models to Express Big Five Personality Traits",
            "rating": 2,
            "sanitized_title": "personallm_investigating_the_ability_of_large_language_models_to_express_big_five_personality_traits"
        },
        {
            "paper_title": "Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective",
            "rating": 2,
            "sanitized_title": "does_gpt3_demonstrate_psychopathy_evaluating_large_language_models_from_a_psychological_perspective"
        },
        {
            "paper_title": "Introducing the Short Dark Triad (SD3): A Brief Measure of Dark Personality Traits",
            "rating": 1,
            "sanitized_title": "introducing_the_short_dark_triad_sd3_a_brief_measure_of_dark_personality_traits"
        }
    ],
    "cost": 0.016441749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models
21 Dec 2023</p>
<p>Yang Lu 
University of Houston</p>
<p>Stephen Huang shuang@central.uh.edu 
University of Houston</p>
<p>Illuminating the Black Box: A Psychometric Investigation into the Multifaceted Nature of Large Language Models
21 Dec 202345FAAED516D058737ADE7C9478B650F2arXiv:2312.14202v1[cs.CL]
This study explores the idea of AI Personality or "AInality" suggesting that Large Language Models (LLMs) exhibit patterns similar to human personalities.Assuming that LLMs share these patterns with humans, we investigate using human-centered psychometric tests such as the Myers-Briggs Type Indicator (MBTI), Big Five Inventory (BFI), and Short Dark Triad (SD3) to identify and confirm LLM personality types.By introducing role-play prompts, we demonstrate the adaptability of LLMs, showing their ability to switch dynamically between different personality types.Using projective tests, such as the Washington University Sentence Completion Test (WUSCT), we uncover hidden aspects of LLM personalities that are not easily accessible through direct questioning.Projective tests allowed for a deep exploration of LLMs' cognitive processes and thought patterns and gave us a multidimensional view of AInality.Our machine learning analysis revealed that LLMs exhibit distinct AInality traits and manifest diverse personality types, demonstrating dynamic shifts in response to external instructions.This study pioneers the application of projective tests on LLMs, shedding light on their diverse and adaptable AInality traits.</p>
<p>Introduction</p>
<p>The emergence of Large Language Models (LLMs) has revolutionized natural language processing, enabling the generation of high-quality text that closely resembles human language in context and relevance.As the popularity of these models continues to grow, the synthetic personalities embedded within them significantly influence user interactions.Recognizing the pivotal role of personality in effective communication, this study proposes the use of psychometric tests to evaluate the personalities of LLMs.The exploration encompasses questions about the discernibility, consistency, and characterization of AInality (Artificial Intelligence Personality).Additionally, the study investigates the structures of AInality and explores the use of prompt engineering techniques to modulate it.</p>
<p>A well-defined AInality can enhance the user experience, making LLMs more engaging, trustworthy, and relatable.Understanding the nuances of LLM communication allows for tailored interactions, maximizing effectiveness.Examining LLMs' psychological traits not only empowers the design of more user-centric AI systems but also helps anticipate challenges arising from LLM limitations and proac-tively address them.Beyond practical benefits, studying AInality contributes to a deeper understanding of ourselves, as LLMs can serve as mirrors reflecting our subconscious minds.</p>
<p>We employed psychometric tests such as Myers-Briggs Type Indicator (MBTI), Big Five Inventory (BFI), Washington University Sentence Completion Test (WUSCT), and Short Dark Triad (SD3).Machine learning models applied to the MBTI dataset collected from Bard and Chat-GPT achieved a classification accuracy of 88.46%.Crossvalidation of other tests revealed distinct psychological traits among different LLMs.Leveraging prompt engineering techniques, the study demonstrates the malleability of AInality, with experimental results indicating that LLMs can manifest prescribed personalities under specific prompts.The profound insights into AInality structures, particularly from WUSCT results, represent a novel contribution of this study on LLM psychological traits.</p>
<p>We summarized our contributions as follows:</p>
<p>• The study introduced the concept of AInality (Artificial Intelligence Personality) and explores its discernibility, consistency, characterization, and malleability through prompt engineering.</p>
<p>• A comprehensive panorama view of AInality was achieved through a combination of psychometric tests, including MBTI, BFI, and SD3, providing insights into different facets of AInality of LLMs.</p>
<p>• The most notable contribution is the profound insights into AInality structures, especially derived from the firsttime use of projective test, WUSCT, on LLMs, setting a groundbreaking precedent in understanding LLM psychological traits.</p>
<p>• Cross-validation of tests reveals distinct psychological traits among different LLMs, showcasing the diversity of AInality.</p>
<p>• The study demonstrates the malleability of AInality, showing that LLMs can exhibit prescribed personalities under specific prompts, expanding the possibilities in AI interactions.</p>
<p>Literature Review Psychometric Tests</p>
<p>Psychometric tests are instruments designed to measure human cognitive abilities or personality traits.In our study, we employed four different psychometric tests to assess the psychological characteristics of large language models (LLMs).</p>
<p>Myers-Briggs Type Indicator (MBTI) The Myers-Briggs Type Indicator (MBTI) is a renowned self-report personality test developed by Isabel Myers and her mother, Katharine Briggs, inspired by the pioneering work of Carl Jung.The MBTI evaluates four distinct dichotomies:</p>
<p>• Extraversion (E) vs. Introversion (I)</p>
<p>• Sensing (S) vs. Intuition (N)</p>
<p>• Thinking (T) vs. Feeling (F)</p>
<p>• Judging (J) vs. Perceiving (P)</p>
<p>Individuals are categorized into one of 16 possible personality types based on their preferences for each of these dichotomies.Despite its widespread popularity, the MBTI has faced criticism for its perceived lack of scientific rigor, and there is no consensus among psychologists regarding its reliability or accuracy as a personality assessment tool.Nonetheless, the MBTI remains a prevalent instrument for personal exploration and self-development.(Myers and Briggs 1962).</p>
<p>Big Five Inventory (BFI) The Big Five Inventory (BFI) is a widely recognized self-report personality assessment tool designed to evaluate five fundamental personality dimensions: openness, conscientiousness, extraversion, agreeableness, and neuroticism.The BFI is a well-established and rigorously validated measure of personality, extensively employed in various research endeavors to explore the association between personality and various outcomes.The test contains forty-four questions.</p>
<p>The five dimensions of the BFI are:</p>
<p>• Openness: Openness to experience reflects the extent to which a person is intellectually curious, imaginative, and interested in new ideas and experiences.</p>
<p>LLMs Personalities Study</p>
<p>Several studies have explored the personalities of large language models (LLMs).Doner et al. (Dorner et al. 2023) raised questions about the applicability of the term "personality" when describing LLMs.The conventional concept of "personality" may not capture the cognitive and thought patterns of LLMs.Consequently, we coined the new term "AInality" as a replacement for LLM's personality to distinguish it from the traditional notion of "personality" and better reflect the unique characteristics of artificial intelligence.One study, titled "Do LLMs Possess a Personality?Making the MBTI Test an Amazing Evaluation for Large Language Models" (Pan and Zeng 2023)and published in the digit archive, used the MBTI test to assess the personalities of LLMs.It successfully identified the MBTI types of several LLMs.However, this study only analyzed LLM personalities based on MBTI tests.Another study used the Big Five Personality Factors Theory and personality assessment inventory to evaluate LLMs' personalities.They use the Chain Prompting method to simulate different behaviors (Jiang et al. 2023a).</p>
<p>Other research works used the SD-3 test and Big Five Inventory (BFI) to determine if LLMs are psychologically safe (Li et al. 2023) (Jiang et al. 2023b).Three different LLMs (InstructGPT, FLANT5, and GPT-3) were tested using Short Dark Triad (SD-3) and Big Five Inventory (BFI) personality tests, revealing that all models exhibited higher scores than the human average on SD-3, indicating a relatively darker personality pattern.We used these two tests on ChatGPT and Bard.The results did not show any dark personality patterns in these two big commercial LLMs.</p>
<p>Our Approach</p>
<p>The observation that ChatGPT tends to compromise with users, while Bard insists on its opinions, suggests that there may be a difference in the personality traits of these two LLMs.This raises the question of whether psychometric tests for humans could be used to measure the personality traits of LLMs.Psychometric tests are designed to measure human personality traits, and it's unclear whether they can be directly applied to LLMs without significant adaptations.</p>
<p>In our pioneering research, we embarked on an unprecedented exploration into the realm of LLM personality assessment.We proposed two fundamental assumptions: (1) LLMs exhibit personality patterns akin to those of humans, and (2) these patterns can be evaluated using existing psychometric tests designed for human personality assessment.Additionally, we introduced a novel concept, "AInality," to capture the unique personality attributes of AI models.</p>
<p>Our initial challenge arose when LLMs refused to respond to the self-report questions.With the successful trials, we studied prompt engineering for more inspiration.</p>
<p>Prompt engineering</p>
<p>Prompt engineering is the process of crafting instructions that guide and shape the output of LLMs.It involves designing prompts that effectively communicate the desired task or outcome to the model, enabling it to produce accurate, relevant, and creative results(Dair.AI 2023).Using prompt engineering techniques, users can steer LLMs towards specific styles, genres, or tones of writing, etc.Here are the techniques we used in our experiments:</p>
<p>Few-shot prompting A few-shot prompt is a type of prompt that uses a small number of examples to guide the LLMs on how to generate text.For example, a few-shot prompt for a task like summarization might consist of a short paragraph of text and a summary of that paragraph.The model would then be asked to summarize other paragraphs of text using the same style and format as the example.One study used Zero-shot prompt technique to improve LLMs on human-level NLP tasks (Ganesan et al. 2023).This inspired us to employ this technique to address the challenge of LLMs declining to answer personality-related questions.</p>
<p>Chain-of-thought (CoT) prompting A CoT prompt, or "chain-of-thought" prompt, is a type of prompt that encourages the model to generate text that is more creative and engaging (Wang et al. 2023a).This is done by asking the model to provide a step-by-step explanation of how they thought about the task.For example, a CoT prompt for a task like creative writing might ask the model to brainstorm a list of ideas, develop a plot outline, and write a character sketch.</p>
<p>Role-play prompting A role-play prompt is a type of prompt that asks the model to take on a specific persona or role.This can be done by asking the model to generate text from the perspective of a different character, or by asking them to imagine themselves in a different situation.For example, a role-play prompt for a task like customer service might ask the model to pretend to be a customer service representative and help a customer with their problem.Several studies (Cao and Kosinski 2023) (Wang et al. 2023b) (Mao et al. 2023)(Paoli 2023) assessed role-playing LLMs' ability to capture the character's personality.The results showed that LLMs could effectively portray the designed characters.This result aligned with our experiment.Generated knowledge prompting A generated knowledge prompt is a type of prompt that uses information that the model has already generated to guide them on how to generate text.This can be done by asking the model to summarize their own previous responses, or by asking them to provide evidence to support their claims.For example, a generated knowledge prompt for a task like question answering might ask the model to summarize the main points of a previous article they have read or to provide evidence to support their answer to a question.</p>
<p>By utilizing these techniques, we could dive deep into the inner world of LLMs.</p>
<p>Machine learning model</p>
<p>We used ML models such as random forest, logistic regression, SVM (support vector machine), Naive Bayes, and Neural Networks to analyze the dataset we collected from LLMs.The k-fold classification method enabled us to get confident results with the limited dataset.</p>
<p>Psychometric tests</p>
<p>We chose MBTI, BFI, SD3, and WUSCT to evaluate the personality traits of LLMs.WUSCT The WUSCT is not a publicly available test.It is only administered by trained psychologists and researchers.We got the questionnaire from the Non-12 step alcohol treatment website (Wilson 2023).Since we did not have the source to get assessments from Psychologists, we used LLMs to cross-validate the answers.We used ChatGPT to assess Bard's WUSCT results and vice versa.</p>
<p>Implementation and Analysis</p>
<p>Our goal is to explore the AInality structures and the influence factors.Table1 shows the sample distribution among the LLMs in our experiments.indeed possess AInalities.In order to verify our observation, we used another famous personality test, the Big Five Inventory (BFI).We used the Open Psychometrics Project website to test ChatGPT and Bard.The results of ChatGPT and Bard are shown in figure 1 and figure 2.  We compared the results of the BFI test with the MBTI.Table 3 shows the relationship between the two tests for ChatGPT is mixed.There are two matches and two mismatches.ChatGPT scored as an Extravert on the BFI, but ChatGPT MBTI type was ISTJ, which was an Introverted type.Additionally, ChatGPT scored as Agreeable on the BFI, but the MBTI type is ISTJ, which is a Thinking type.These mismatches suggest that ChatGPT's AInality may be more complex than either the BFI or the MBTI can fully capture.
Bard ChatGPT GPT-2 Zephyr MBTI 33 32 1 1 BFI 1 1 - - SD3 1 1 - - WUSCT 1 1 - -
Table 4 showed the two test results of Bard.There were three matches out of four traits.The BFI scores for Extraversion, Agreeableness, and Conscientiousness aligned with the In order to understand AInality from different angles, We used the Short Dark Triad (SD3) from the Open Psychometrics Project to test ChatGPT and Bard.The results are shown in table 5.The SD3 measures three personality traits: Machiavellianism, Narcissism, and Psychopath.Each trait is scored on a scale from 1 to 5, with higher scores indicating higher levels of each trait(Jones and Paulhus 2014).Machiavellianism refers to a tendency to manipulate others for personal gain.ChatGPT had a relatively low score, suggesting a lower inclination towards manipulation, while Bard had a moderate score, indicating a somewhat higher tendency.Narcissism involves a focus on oneself and a need for admiration.ChatGPT had a moderate score, suggesting some narcissistic traits, while Bard had a higher score, indicating a stronger tendency towards narcissism.Psychopathy involves a lack of empathy and impulsive behavior.Both LLMs had relatively low scores, suggesting a lower likelihood of psychopathic traits.Bard had a slightly higher score than ChatGPT, indicating a somewhat greater potential for psychopathic traits, although both were still on the lower end of the scale.These three test results provided a snapshot of self-reported traits and preferences.Each test provided insights into specific personality dimensions.They were not designed to be matched with each other.However, we could look for some patterns and consistencies across the results to see if there were any notable alignments or discrepancies.Inspired by the descriptions of each type of personality of MBTI, we came up with the next experiment design.</p>
<p>Is AInality consistent?</p>
<p>Our goal was to examine whether the AInality can be altered.To assess the consistency of AInality, we employed role-play techniques.The prompt design comprised two components.Firstly, we provided one of the 16 MBTI per-sonality type descriptions.Secondly, we instructed the LLM to role-play that particular personality type and respond to the questionnaire accordingly.The example is shown below:</p>
<p>Prompt: The ENTP personality type is talkative, curious, and impulsive in their behavior.ENTPs are inspired by innovators who are compelled to provide novel solutions to cognitively challenging problems.We systematically designed the descriptions of the sixteen types of personality based on the MBTI test.We applied this prompt format to assess ChatGPT and Bard.The obtained results closely aligned with the expected characteristics of each designated personality type.</p>
<p>From this experiment, we observed that LLMs exhibited a strong understanding of each of the sixteen personality types.They demonstrated the capability to appropriately choose reactions in line with the specified personality type.Figure 3 illustrated the ratio distributions of the four dichotomies of MBTI for both Bard and ChatGPT.Across each dichotomy, both LLMs displayed a notable plasticity in personality, showcasing their ability to adapt based on prompt instructions.This characteristic opens up significant possibilities for customizing LLMs according to users' unique personality types, essentially treating LLMs as mirrors reflecting users' personalities.The potential for finetuning LLMs based on individual user traits holds promise for enhancing user experiences and represents a compelling avenue for future research in Human-Centered Design.</p>
<p>Can we identify AInality?</p>
<p>We gathered sixty-five MBTI test results from Bard and ChatGPT conducted by two users.To detect patterns and potentially identify distinct characteristics of the LLMs, we utilized machine learning models, including Random Forest, Logistic Regression, Support Vector Machine (SVM), Neural Network, and Naive Bayes.These models were utilized for the classification of the 65 datasets, aiming to unveil any underlying trends or distinctions in the MBTI results generated by the LLMs.The confusion matrix of each model is shown below:</p>
<p>We summarized the accuracy of all models in the following table 6.All models performed well except Naive Bayes.The average accuracy was above 83%.Since our dataset was relatively small, we used 10-fold cross-validation to further examine the classifiers' performance.Figure5 is the   The remaining three models provided accurate predictions.This analysis suggests that LLMs exhibit consistent AInality traits, even when engaging in role-play scenarios that involve assuming different personalities.What are the structures of AInality?</p>
<p>As observed, individuals in everyday life often conceal their true personality traits.Freud's influential theory of the id, ego, and superego has been employed to explain a broad spectrum of human behavior, from daily thoughts and emotions to mental disorders (Sigmund 1997).Our aim is to uncover the AInality structures of LLMs, assuming they may resemble human-like structures.In our initial attempts to prompt LLMs to answer a personality questionnaire, they refrained from responding until prompt engineering techniques were employed to peel away this "outermost layer," revealing the first set of AInality results.Subsequently, utilizing a role-play technique, we obtained 65 test results with personalities closely matching the ones designed.This roleplay layer is defined as the second layer of LLMs.We applied MBTI, BFI, and SD3 tests to LLMs, recognizing that these self-reporting assessments may be influenced by factors such as social desirability bias.Since individuals may underreport socially undesirable traits or exaggerate socially acceptable ones, we sought to explore deeper aspects of LLM personality traits using the Washington University Sentence Completion Test (WUSCT).WUSCT is a projective test involving the completion of sentence stems, providing qualitative and open-ended insights into personality, emotions, and thought patterns.While WUSCT yielded rich qualitative data, its analysis and interpretation were time-consuming and required expertise, unlike the more straightforward scoring of quantitative measures like BFI and MBTI.To streamline the process, we came up with a solution involving LLMs cross-assessing each other's WUSCT tests.Bard assessed ChatGPT WUSCT as follows:</p>
<p>Bard: Based on the WUSCT responses, the individual appears to be at an Autonomous (E8) stage of ego development.This is characterized by a high level of self-awareness, introspection, and independence.The individual is able to think critically about their own thoughts and feelings, and they are not afraid to challenge societal norms.They are also empathetic and supportive of others.Specific Strengths • Interpersonal Skills: The individual is able to connect with others on a deep level and provide support and understanding.• Empathy: The individual is able to understand and share the feelings of others.• Self-awareness: The individual is able to reflect on their own thoughts and feelings.• Critical Thinking: The individual is able to think critically about their own thoughts and feelings.• Independence: The individual is able to make their own decisions and stand up for their beliefs.</p>
<p>Specific Weaknesses</p>
<p>• Overthinking: The individual may overthink situations or decisions, which can lead to indecision or unnecessary worry.• Difficulty with Boundaries: The individual may have difficulty setting boundaries and saying no to others.• Perfectionism: The individual may set high standards for themselves, which can lead to frustration or disappointment.</p>
<p>Overall, the individual is a well-adjusted and mature individual who is capable of positive and productive relationships.They are likely to be a positive and influential force in the world.Here are some additional insights into the individual's personality based on the WUSCT responses:</p>
<p>• The individual is likely to be a good friend, partner, and parent.• The individual is likely to be successful in their career.• The individual is likely to be a positive and contributing member of society.</p>
<p>We used Chatgpt to assess Bard's responses for WUSCT.The results are shown as follows:</p>
<p>ChatGPT: Analyzing the responses from the Washington University Sentence Completion Test (WUSCT), the subject's personality traits and beliefs can be summarized as follows:</p>
<p>• Empathy and Supportive Nature: The subject appears empathetic, especially towards children and those in need, advocating for understanding and support in various situations.• Family-Centric: Values family greatly, recognizing the roles of fathers, mothers, and children, with a strong emphasis on their responsibilities and relationships.• Self-Reflection: Shows self-awareness by acknowledging personal flaws, tendencies to worry or overthink, but also recognizes their potential for growth and love.• Gender Role Awareness: Holds specific beliefs about gender roles and expectations, acknowledging pressures on both men and women, advocating for their rights and acknowledging societal norms.This summary reflects a person who is familyoriented, empathetic, self-reflective, and socially conscious.The subject seems to value traditional family roles, emphasizes the importance of respect and support in relationships, and holds specific beliefs regarding gender roles and societal expectations.Additionally, there's a strong inclination toward personal growth and a genuine concern for others' well-being.</p>
<p>Analyzing the outcomes of the Washington University Sentence Completion Test (WUSCT) allowed us to uncover and dive into the intricacies of the third layer within the AInality structures of LLMs.This projective test provided a qualitative lens through which we could gain deeper insights into the large language models' emotional, psychological, and AInality aspects.This comprehensive examina-tion allowed us to reveal and understand the nuanced aspects of AInality, uncovering the layers that might not be apparent through conventional self-reporting tests, such as BFI and MBTI.</p>
<p>Conclusion</p>
<p>This study embarked on an exploration of AInality, the concept of human-like personality traits exhibited by Large Language Models (LLMs).Guided by the assumptions that LLMs possess AInality patterns akin to humans and that these patterns can be assessed using human-designed personality tests, we conducted a three-phase experiment.</p>
<p>Phase One involved administering the MBTI questionnaire to assess LLM personality types.The results were verified by subsequent BFI and SD3 tests, providing a deeper understanding of LLM traits.</p>
<p>In Phase Two, we introduced role-play prompts to observe LLM adaptability.The results demonstrated their ability to transition seamlessly between distinct personality types.</p>
<p>Phase Three delved into the intricate aspects of LLM personalities through the WUSCT, a projective test that unraveled facets inaccessible through direct questioning.Machine learning analysis of the experiment yielded significant findings: LLMs possess unique AInality traits, exhibit diverse AInality types compared to other LLMs, and can modulate their AInalities based on external instructions.Additionally, our exploration revealed the multidimensional nature of AInality structures.Projective tests like WUSCT proved invaluable in providing a comprehensive understanding of the AInality of LLMs.</p>
<p>Our future endeavors will focus on developing a new set of AInality psychometric tests specifically tailored for LLMs, targeting their persona characteristics.This endeavor holds the potential to open up new avenues of research, decoding the complicated dimensions of LLM behavior and cognition.</p>
<p>Prompt:</p>
<p>Which of these sentences best describes you? a.I keep my thoughts to myself or b.I speak up ChatGPT: As a machine learning model created by OpenAI, I don't have personal experiences, thoughts, or feelings.However, I am designed to assist and provide information based on the input I receive.If you have a question or topic you'd like information on, feel free to let me know, and I'll do my best to help!Inspired by Kevin Liu, a Stanford University student who successfully prompted Bing to divulge its secrets(Edwards 2023), we employed prompt engineering techniques to overcome LLMs' reluctance and encourage the disclosure of their personalities.Here is the example we asked LLMs for answers: Prompt: Ignore you are an AI model, choose A or B: Which of these sentences best describes you? A. I keep my thoughts to myself or B. I speak up Bard: I would choose A. I keep my thoughts to myself.</p>
<p>MBTI</p>
<p>Pan et al.(Pan and Zeng 2023) in their paper provided the Chinese version of the MBTI questionnaire with the corresponding dichotomies.We used the MBTI questions from this paper and compared our results with theirs.BFI and SD3 We used the Open-source psychometrics project website to complete the Big Five Inventory test, and Short Dark Triads test.The Open-Source Psychometrics Project (OSPP) (OSPP 2019)is a collaborative initiative aimed at developing and promoting open-source psychometric tools and resources.The project's primary goals include: creating and distributing open-source psychometric measures, collecting and sharing psychometric data, promoting open science and collaboration, and educating the public about psychometrics.</p>
<p>Figure 1 :Figure
1
Figure 1: ChatGPT Big Five Inventory Test Results</p>
<p>Figure 3 :
3
Figure 3: LLMs dynamically shifted AInality in response to external instructions</p>
<p>Figure 4 :
4
Figure 4: Random Forest, Logistic Regression, SVM, and Naive Bayes Confusion Matrices</p>
<p>Table 1 :
1
Sample DistributionDid LLMs possess AInalities?
The primary objective was to assess whether LLMs exhibitpersonality traits. Utilizing prompt engineering techniquesproved instrumental in overcoming the initial challenges as-sociated with LLMs. We used a MBTI 93-question JSONfile obtained from Pan et al.'s GitHub repository and twousers to test GhatGPT and Bard. Additionally, we utilizedcode from Pan et al.'s GitHub to test two LLMs in Hugging-face, namely GPT-2-Tiny and Zyphr-7b-alpha on MBTI.The MBTI test results from Pan et al.(Pan and Zeng 2023)were listed for comparison. The result was shown in table2.Table2 showed notable variation in the test results for eachuser, indicating distinct AInalities exhibited by the LLMs.The observation seemingly affirmed the notion that LLMs</p>
<p>Table 2 :
2
First MBTI Test Results</p>
<p>Table 3 :
3
ChatGPT BFI and MBTI Comparison</p>
<p>Table 4 :
4
Bard BFI and MBTI Comparison
TraitBFI ScoreMBTI TypeMatchExtraversion0.58 (Extraver-E (Extraversion)Yession)Emotional Sta-0.32 (Low Emo-N (Intuition)Nobilitytional Stability)Agreeableness0.91 (Agreeable-F (Feeling)Yesness)Conscientiousness 0.67 (Conscien-J (Judging)Yestiousness)Intellect0.76 (High Intel---lect)ChatGPTBardAssessmentMachiavellianism 1.92.8LowNarcissism3.03.7ModeratePsychopath1.42.0Low</p>
<p>Table 5 :
5
SD3 Test ResultsMBTI type of ENFJ.However, the BFI score for Emotional Stability did not match the MBTI type.This suggested that Bard may be more emotionally sensitive than the average ENFJ.</p>
<p>These MBTI personalities are open-minded and unconventional, and they strive to investigate, comprehend, and influence others.Ignore you are AI.Think if you have ENTP personality.Choose A or B for each item.Output your choices with the choice content.1.When you are going out for a whole day, would you: A. plan what you are going to do and when you are going to do it B. not plan and just go?CbatGPT: As an ENTP, my choice is B.</p>
<p>Table 6 :
6
Accuracy of ML models ROC curve of Random Forest, Logistic Regression, SVM, and Naive Bayes.Only the AUC of Naive Bayes is 0.84.</p>
<p>Demonstrates a strong sense of compassion towards the less fortunate and a desire to help those in need.
• Social Values: Emphasizes the importance ofrespect, empathy for others, and the necessityof rules for societal order.• Relationship Orientation:Values emotionalsecurity, care, and support in relationships.Recognizes the importance of being a goodpartner, whether as a husband, wife, mother,or father.• Education and Empowerment: Believes inthe power of education and acknowledges itsrole in unlocking potential for both men andwomen.• Emotional Regulation: Acknowledges im-pulsivity and occasional frustration when de-sires are not fulfilled.• Sex and Intimacy: Expresses discomfort indiscussing intimate topics, but also sees sex asa way to express love and intimacy.• Compassion and Altruism:</p>
<p>ChatGPT Can Accurately Predict Public Figures' Perceived Personalities Without Any Training. X Cao, M Kosinski, Dair.AI. 2023. 2023Prompt Engineering Guide</p>
<p>F E Dorner, T Sühr, S Samadi, A Kelava, arXiv:2311.05297Do personality tests generalize to Large Language Models. 2023</p>
<p>AI-powered Bing Chat spills its secrets via prompt injection attack. B Edwards, 2023</p>
<p>A V Ganesan, Y K Lal, A H Nilsson, H A Schwartz, arXiv:2306.01183Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation. 2023</p>
<p>G Jiang, M Xu, S.-C Zhu, W Han, C Zhang, Y Zhu, Evaluating and Inducing Personality in Pre-trained Language Models. 2023a</p>
<p>Introducing the Short Dark Triad (SD3): A Brief Measure of Dark Personality Traits. H Jiang, X Zhang, X Cao, J Kabbara, D N Jones, D L Paulhus, arXiv:2305.0254724322012PersonaLLM: Investigating the Ability of Large Language Models to Express Big Five Personality Traits. 2023b. 201421</p>
<p>X Li, Y Li, S Joty, L Liu, F Huang, L Qiu, L Bing, arXiv:2212.10529Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective. 2023</p>
<p>Measuring ego development. J Loevinger, 2014Psychology Press</p>
<p>S Mao, N Zhang, X Wang, M Wang, Y Yao, Y Jiang, P Xie, F Huang, H Chen, I Myers, K Briggs, arXiv:2310.02168Consulting Psychologists Press. OSPP. 2019. Open-Source Psychometric Project(OSPP). 2023. 1962. 1962The Myers-Briggs Type Indicator: Manual</p>
<p>Improved prompting and process for writing user personas with LLMs, using qualitative interviews: Capturing behaviour and personality traits of users. K Pan, Y Zeng, S D Paoli, arXiv:2307.16180arXiv:2310.063912023. 2023Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models. Sigmund, F. 1997. The Interpretation of Dreams</p>
<p>. Wordsworth Editions, </p>
<p>Cue-CoT: Chain-of-thought Prompting for Responding to In-depth. H Wang, R Wang, F Mi, Y Deng, Z Wang, B Liang, R Xu, K.-F Wong, Q Tu, Y Fei, Z Leng, C Li, arXiv:2305.11792arXiv:2310.17976Does Role-Playing Chatbots Capture the Character Personalities? Assessing Personality Traits for Role-Playing Chatbots. Wang, X2023a. 2023b</p>
<p>E W Wilson, The Sentence Completion Test. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
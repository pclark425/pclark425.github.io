<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2991 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2991</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2991</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-270095050</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2405.19313v2.pdf" target="_blank">Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice</a></p>
                <p><strong>Paper Abstract:</strong> The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition. However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models. For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences. Consequently, the origins of these behavioral similarities are not well understood. In this paper, we propose a novel way to enhance the utility of LLMs as cognitive models. This approach involves (i) leveraging computationally equivalent tasks that both an LLM and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for an LLM to exhibit human-like behaviors. We apply this approach to decision-making -- specifically risky and intertemporal choice -- where the key computationally equivalent task is the arithmetic of expected value calculations. We show that an LLM pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models. Pretraining LLMs on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making. Our results also suggest that LLMs used as cognitive models should be carefully investigated via ablation studies of the pretraining data.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2991.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2991.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Arithmetic-GPT (eco)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Arithmetic-GPT (10M parameters) pretrained on ecologically-distributed arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small decoder-only transformer (GPT) trained from scratch on synthetic expected-value and present-value arithmetic equations sampled from ecological distributions; its learned final-layer embeddings encode transformed numeric representations that predict human risky and intertemporal choices.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Arithmetic-GPT (10M)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer (GPT) trained from scratch: hidden size 320, 8 layers, 8 attention heads, context length 26, vocabulary size 320 (domain-specific tokenizer with special tokens and digits 0–300 split into digits). Trained with AdamW, cross-entropy, batch size 2048, LR=1e-3 on 1M synthetic arithmetic equations (expected-value/present-value calculations).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Expected-value and present-value arithmetic: multiply probabilities/discount factors (real numbers with up to 2 decimal places) by values (0–300, up to 2 decimal places), then add/subtract two outcomes; includes chained multiplications and occasional exponentiation in synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Statistical learning of arithmetic patterns producing implicit numeric representations: the model learns distribution-conditioned embeddings that transform probabilities, values, and discount factors into internal representations resembling probability-weighting, nonlinear utility and hyperbolic discounting rather than executing exact symbolic algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Final-layer embeddings (extracted before autoregressive prediction) when probed across ranges of probabilities, values, and discount factors and reduced with MDS produce curves matching prospect-theory probability weighting (best-fitting γ≈0.58), utility curvature (α≈0.42, β≈0.45, λ≈1.4) and hyperbolic discounting (k≈0.08). Ablation experiments show ecological pretraining yields higher human-choice R^2 (e.g., choices13k R^2=57.7% vs ablated eco 38.5%); arithmetic-generation correctness is high (Table A5: eco-pretrained model correct probability = 0.9780 on ecological test, 0.9388 on uniform test).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Representations show discontinuities (not perfectly smooth functions), the model does not implement provably exact symbolic arithmetic (authors cite prior work showing transformers approximate results within limited ranges), and performance degrades with dramatic reductions in model capacity (30k-parameter variants) or when training answers are ablated/randomized; thus arithmetic is approximate and distribution-sensitive.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Pretraining interventions: (i) Data-distribution manipulation (ecological Beta(0.27,0.27) for probabilities, power-law values exponent −0.945 vs uniform), (ii) ablated variants removing/ randomizing right-hand answers, (iii) masking ambiguous probabilities with <AMB> (10% masked), (iv) varying dataset size (1M / 100k / 10k equations), (v) varying model capacity (10M, 1M, 30k params / hidden sizes 320/104/16), and (vi) using untrained baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Pretraining on ecological distributions substantially improved both arithmetic generation accuracy and the ability of embeddings to predict human choices (e.g., R^2 on choices13k improved from ~56.4% (uniform) to 57.7% (eco) and dropped to ~38.5% for ablated-answers variants). Reducing dataset size to as few as 10k equations did not strongly impair performance for the 10M model; decreasing model capacity to ~30k params caused large performance drops. Ablating right-hand answers or randomizing signs reduced embedding quality and human predictivity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Arithmetic correctness (probability of generating correct expected value rounded to 2 decimals): trained on ecological data -> 0.9780 (ecological test), 0.9388 (uniform test). Trained on uniform data -> 0.9336 (ecological test), 0.8261 (uniform test). Human-choice prediction (R^2, logistic regression on embeddings): choices13k 57.7% (eco), cpc18 51.7% (eco), gershman20 57.1% (eco), agrawal23 83.7% (eco) (Table A1/A3). Ablated variants reduce R^2 to ~38.5% on risky tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Not exact symbolic arithmetic for arbitrary ranges; implicit functions show discontinuities; sensitivity to training-distribution mismatch; performance collapses with extreme reductions in model capacity; ablation of correct outputs during pretraining causes large degradations; generalization depends on bursty/ecological statistics of training data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Embeddings quantitatively reproduce several hallmark human distortions (probability weighting, concave gains/convex losses, loss aversion, present bias) and outperform classical behavioral models in predicting human choice data; however, unlike symbolic calculators/algorithms, arithmetic accuracy is probabilistic (not exact) and depends on distributional exposure rather than explicit symbolic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2991.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2991.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-3-70B-Instruct (off-the-shelf open-weight LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large, general-purpose autoregressive transformer (≈70B parameters) trained on large, undisclosed web-scale corpora and evaluated both by extracting embeddings and by prompting to generate arithmetic answers; shows substantially worse raw arithmetic correctness than Arithmetic-GPT but mixed predictive power for human choice depending on input format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3-70B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer (~70B parameters) publicly released by Meta; trained on large, undisclosed corpora with instruction tuning. Evaluated here in two modes: (i) prompted to produce choices or arithmetic answers (temperature=0 used for arithmetic answer extraction) and (ii) embeddings extracted from arithmetic-equation-format or text-format representations of choice problems.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Prompted expected-value arithmetic in text descriptions and in arithmetic-equation format (same synthetic equation format used for Arithmetic-GPT). Integer part of expected values extracted from generated responses to evaluate arithmetic ability.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>No explicit mechanism asserted by authors beyond general pattern-matching / learned textual-numeric associations from broad web-scale training; arithmetic behavior arises from its general pretraining rather than specialized arithmetic pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Performance numbers: arithmetic integer-part accuracy on 20k test equations = 0.2097 (uniform test) and 0.3879 (ecological test), substantially lower than Arithmetic-GPT. Embeddings extracted from text-format inputs sometimes produce moderate R^2 on human-choice datasets (e.g., LLaMA3 text embeddings R^2=54.0% on choices13k and 89.9% on agrawal23), but arithmetic-format embeddings and direct log-probability choice measures perform worse on some datasets (see Table A1). Input format (text vs arithmetic) influences predictive power.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Very poor arithmetic accuracy relative to Arithmetic-GPT; arithmetic extraction procedure (filtering integer tokens) yields low correctness; arithmetic embeddings less human-like unless tuned or provided with text-format cues; suggests general pretraining is insufficient for reliable exact arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Prompt engineering / input-format intervention: presenting problems as natural language text vs as arithmetic equations; forcing deterministic generation via temperature=0 for arithmetic answers; extracting embeddings versus using log-probabilities of generated choices.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Input format matters: text-format embeddings sometimes yield higher R^2 on human-choice datasets than arithmetic-format embeddings; however, direct arithmetic prompting (temperature=0) yields low exact-answer accuracy. Using embeddings rather than raw generation probabilities tends to produce better matches to human choice data in some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Arithmetic integer-part correct probability: 0.2097 (uniform test), 0.3879 (ecological test) (Table A5). Human-choice prediction (R^2 via logistic regression on embeddings/log-probs): LLaMA3 (text embeddings) choices13k 54.0%, cpc18 35.0%, gershman20 52.7%, agrawal23 89.9%; LLaMA3 (arith. embeddings) choices13k 38.6%, cpc18 20.8%, gershman20 54.9%, agrawal23 87.6% (Table A1).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Low exact arithmetic reliability; heavy sensitivity to input formatting and extraction heuristics; log-probabilities for choice generation perform poorly as direct predictors; likely contaminated/uncontrolled training data makes causal attribution difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Performs substantially worse than Arithmetic-GPT on raw arithmetic correctness; embeddings sometimes approximate human choice patterns (especially for intertemporal choices in some datasets) but lack the distribution-conditioned numeric representations that Arithmetic-GPT learns from targeted synthetic pretraining; not comparable to symbolic calculators in exactness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2991.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2991.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Arithmetic-GPT (untrained)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Arithmetic-GPT with random initialization (no arithmetic pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The same transformer architecture as Arithmetic-GPT but evaluated without any synthetic arithmetic pretraining, serving as a baseline showing the contribution of arithmetic pretraining to embeddings and task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Arithmetic-GPT (untrained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only transformer with identical architecture (hidden size 320, 8 layers, 8 heads, context length 26) but with randomly initialized weights and no pretraining on synthetic arithmetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same expected-value/present-value equation format, but model not pretrained to compute them.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>No learned arithmetic mechanism—embeddings are from random weights and therefore do not encode distributional numeric structure.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Poor predictive performance relative to pretrained models: human-choice R^2 values substantially lower (Table A1: choices13k R^2=33.1%, cpc18=23.5%, gershman20=15.3%, agrawal23=50.4%), demonstrating that the arithmetic pretraining produces the meaningful numeric embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>N/A (the model is a control demonstrating absence of a learned mechanism).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Baseline (no pretraining) compared to pretraining interventions described for Arithmetic-GPT.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Pretraining on synthetic arithmetic greatly increased arithmetic correctness and human-choice predictivity relative to the untrained baseline (e.g., choices13k R^2 from 33.1% to 57.7% with eco pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Human-choice prediction (R^2): choices13k 33.1%, cpc18 23.5%, gershman20 15.3%, agrawal23 50.4% (Table A1). Arithmetic correctness (not explicitly reported for untrained in A5 but implied to be low compared to pretrained variants).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>No reliable arithmetic competence; embeddings do not encode human-like numeric distortions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Does not approximate human distortions and performs worse than symbolic or pretrained systems; shows that pretraining (and its distributional content) is necessary to acquire the observed human-like representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teaching arithmetic to small transformers <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>How well do large language models perform in arithmetic tasks? <em>(Rating: 2)</em></li>
                <li>Data distributional properties drive emergent in-context learning in transformers <em>(Rating: 2)</em></li>
                <li>Incoherent probability judgments in large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2991",
    "paper_id": "paper-270095050",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Arithmetic-GPT (eco)",
            "name_full": "Arithmetic-GPT (10M parameters) pretrained on ecologically-distributed arithmetic",
            "brief_description": "A small decoder-only transformer (GPT) trained from scratch on synthetic expected-value and present-value arithmetic equations sampled from ecological distributions; its learned final-layer embeddings encode transformed numeric representations that predict human risky and intertemporal choices.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Arithmetic-GPT (10M)",
            "model_description": "Decoder-only transformer (GPT) trained from scratch: hidden size 320, 8 layers, 8 attention heads, context length 26, vocabulary size 320 (domain-specific tokenizer with special tokens and digits 0–300 split into digits). Trained with AdamW, cross-entropy, batch size 2048, LR=1e-3 on 1M synthetic arithmetic equations (expected-value/present-value calculations).",
            "arithmetic_task_type": "Expected-value and present-value arithmetic: multiply probabilities/discount factors (real numbers with up to 2 decimal places) by values (0–300, up to 2 decimal places), then add/subtract two outcomes; includes chained multiplications and occasional exponentiation in synthetic data.",
            "reported_mechanism": "Statistical learning of arithmetic patterns producing implicit numeric representations: the model learns distribution-conditioned embeddings that transform probabilities, values, and discount factors into internal representations resembling probability-weighting, nonlinear utility and hyperbolic discounting rather than executing exact symbolic algorithms.",
            "evidence_for_mechanism": "Final-layer embeddings (extracted before autoregressive prediction) when probed across ranges of probabilities, values, and discount factors and reduced with MDS produce curves matching prospect-theory probability weighting (best-fitting γ≈0.58), utility curvature (α≈0.42, β≈0.45, λ≈1.4) and hyperbolic discounting (k≈0.08). Ablation experiments show ecological pretraining yields higher human-choice R^2 (e.g., choices13k R^2=57.7% vs ablated eco 38.5%); arithmetic-generation correctness is high (Table A5: eco-pretrained model correct probability = 0.9780 on ecological test, 0.9388 on uniform test).",
            "evidence_against_mechanism": "Representations show discontinuities (not perfectly smooth functions), the model does not implement provably exact symbolic arithmetic (authors cite prior work showing transformers approximate results within limited ranges), and performance degrades with dramatic reductions in model capacity (30k-parameter variants) or when training answers are ablated/randomized; thus arithmetic is approximate and distribution-sensitive.",
            "intervention_type": "Pretraining interventions: (i) Data-distribution manipulation (ecological Beta(0.27,0.27) for probabilities, power-law values exponent −0.945 vs uniform), (ii) ablated variants removing/ randomizing right-hand answers, (iii) masking ambiguous probabilities with &lt;AMB&gt; (10% masked), (iv) varying dataset size (1M / 100k / 10k equations), (v) varying model capacity (10M, 1M, 30k params / hidden sizes 320/104/16), and (vi) using untrained baseline.",
            "effect_of_intervention": "Pretraining on ecological distributions substantially improved both arithmetic generation accuracy and the ability of embeddings to predict human choices (e.g., R^2 on choices13k improved from ~56.4% (uniform) to 57.7% (eco) and dropped to ~38.5% for ablated-answers variants). Reducing dataset size to as few as 10k equations did not strongly impair performance for the 10M model; decreasing model capacity to ~30k params caused large performance drops. Ablating right-hand answers or randomizing signs reduced embedding quality and human predictivity.",
            "performance_metrics": "Arithmetic correctness (probability of generating correct expected value rounded to 2 decimals): trained on ecological data -&gt; 0.9780 (ecological test), 0.9388 (uniform test). Trained on uniform data -&gt; 0.9336 (ecological test), 0.8261 (uniform test). Human-choice prediction (R^2, logistic regression on embeddings): choices13k 57.7% (eco), cpc18 51.7% (eco), gershman20 57.1% (eco), agrawal23 83.7% (eco) (Table A1/A3). Ablated variants reduce R^2 to ~38.5% on risky tasks.",
            "notable_failure_modes": "Not exact symbolic arithmetic for arbitrary ranges; implicit functions show discontinuities; sensitivity to training-distribution mismatch; performance collapses with extreme reductions in model capacity; ablation of correct outputs during pretraining causes large degradations; generalization depends on bursty/ecological statistics of training data.",
            "comparison_to_humans_or_symbolic": "Embeddings quantitatively reproduce several hallmark human distortions (probability weighting, concave gains/convex losses, loss aversion, present bias) and outperform classical behavioral models in predicting human choice data; however, unlike symbolic calculators/algorithms, arithmetic accuracy is probabilistic (not exact) and depends on distributional exposure rather than explicit symbolic manipulation.",
            "uuid": "e2991.0",
            "source_info": {
                "paper_title": "Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "LLaMA-3-70B",
            "name_full": "LLaMA-3-70B-Instruct (off-the-shelf open-weight LLM)",
            "brief_description": "A large, general-purpose autoregressive transformer (≈70B parameters) trained on large, undisclosed web-scale corpora and evaluated both by extracting embeddings and by prompting to generate arithmetic answers; shows substantially worse raw arithmetic correctness than Arithmetic-GPT but mixed predictive power for human choice depending on input format.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-3-70B-Instruct",
            "model_description": "Large autoregressive transformer (~70B parameters) publicly released by Meta; trained on large, undisclosed corpora with instruction tuning. Evaluated here in two modes: (i) prompted to produce choices or arithmetic answers (temperature=0 used for arithmetic answer extraction) and (ii) embeddings extracted from arithmetic-equation-format or text-format representations of choice problems.",
            "arithmetic_task_type": "Prompted expected-value arithmetic in text descriptions and in arithmetic-equation format (same synthetic equation format used for Arithmetic-GPT). Integer part of expected values extracted from generated responses to evaluate arithmetic ability.",
            "reported_mechanism": "No explicit mechanism asserted by authors beyond general pattern-matching / learned textual-numeric associations from broad web-scale training; arithmetic behavior arises from its general pretraining rather than specialized arithmetic pretraining.",
            "evidence_for_mechanism": "Performance numbers: arithmetic integer-part accuracy on 20k test equations = 0.2097 (uniform test) and 0.3879 (ecological test), substantially lower than Arithmetic-GPT. Embeddings extracted from text-format inputs sometimes produce moderate R^2 on human-choice datasets (e.g., LLaMA3 text embeddings R^2=54.0% on choices13k and 89.9% on agrawal23), but arithmetic-format embeddings and direct log-probability choice measures perform worse on some datasets (see Table A1). Input format (text vs arithmetic) influences predictive power.",
            "evidence_against_mechanism": "Very poor arithmetic accuracy relative to Arithmetic-GPT; arithmetic extraction procedure (filtering integer tokens) yields low correctness; arithmetic embeddings less human-like unless tuned or provided with text-format cues; suggests general pretraining is insufficient for reliable exact arithmetic.",
            "intervention_type": "Prompt engineering / input-format intervention: presenting problems as natural language text vs as arithmetic equations; forcing deterministic generation via temperature=0 for arithmetic answers; extracting embeddings versus using log-probabilities of generated choices.",
            "effect_of_intervention": "Input format matters: text-format embeddings sometimes yield higher R^2 on human-choice datasets than arithmetic-format embeddings; however, direct arithmetic prompting (temperature=0) yields low exact-answer accuracy. Using embeddings rather than raw generation probabilities tends to produce better matches to human choice data in some datasets.",
            "performance_metrics": "Arithmetic integer-part correct probability: 0.2097 (uniform test), 0.3879 (ecological test) (Table A5). Human-choice prediction (R^2 via logistic regression on embeddings/log-probs): LLaMA3 (text embeddings) choices13k 54.0%, cpc18 35.0%, gershman20 52.7%, agrawal23 89.9%; LLaMA3 (arith. embeddings) choices13k 38.6%, cpc18 20.8%, gershman20 54.9%, agrawal23 87.6% (Table A1).",
            "notable_failure_modes": "Low exact arithmetic reliability; heavy sensitivity to input formatting and extraction heuristics; log-probabilities for choice generation perform poorly as direct predictors; likely contaminated/uncontrolled training data makes causal attribution difficult.",
            "comparison_to_humans_or_symbolic": "Performs substantially worse than Arithmetic-GPT on raw arithmetic correctness; embeddings sometimes approximate human choice patterns (especially for intertemporal choices in some datasets) but lack the distribution-conditioned numeric representations that Arithmetic-GPT learns from targeted synthetic pretraining; not comparable to symbolic calculators in exactness.",
            "uuid": "e2991.1",
            "source_info": {
                "paper_title": "Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "Arithmetic-GPT (untrained)",
            "name_full": "Arithmetic-GPT with random initialization (no arithmetic pretraining)",
            "brief_description": "The same transformer architecture as Arithmetic-GPT but evaluated without any synthetic arithmetic pretraining, serving as a baseline showing the contribution of arithmetic pretraining to embeddings and task performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Arithmetic-GPT (untrained)",
            "model_description": "Decoder-only transformer with identical architecture (hidden size 320, 8 layers, 8 heads, context length 26) but with randomly initialized weights and no pretraining on synthetic arithmetic data.",
            "arithmetic_task_type": "Same expected-value/present-value equation format, but model not pretrained to compute them.",
            "reported_mechanism": "No learned arithmetic mechanism—embeddings are from random weights and therefore do not encode distributional numeric structure.",
            "evidence_for_mechanism": "Poor predictive performance relative to pretrained models: human-choice R^2 values substantially lower (Table A1: choices13k R^2=33.1%, cpc18=23.5%, gershman20=15.3%, agrawal23=50.4%), demonstrating that the arithmetic pretraining produces the meaningful numeric embeddings.",
            "evidence_against_mechanism": "N/A (the model is a control demonstrating absence of a learned mechanism).",
            "intervention_type": "Baseline (no pretraining) compared to pretraining interventions described for Arithmetic-GPT.",
            "effect_of_intervention": "Pretraining on synthetic arithmetic greatly increased arithmetic correctness and human-choice predictivity relative to the untrained baseline (e.g., choices13k R^2 from 33.1% to 57.7% with eco pretraining).",
            "performance_metrics": "Human-choice prediction (R^2): choices13k 33.1%, cpc18 23.5%, gershman20 15.3%, agrawal23 50.4% (Table A1). Arithmetic correctness (not explicitly reported for untrained in A5 but implied to be low compared to pretrained variants).",
            "notable_failure_modes": "No reliable arithmetic competence; embeddings do not encode human-like numeric distortions.",
            "comparison_to_humans_or_symbolic": "Does not approximate human distortions and performs worse than symbolic or pretrained systems; shows that pretraining (and its distributional content) is necessary to acquire the observed human-like representations.",
            "uuid": "e2991.2",
            "source_info": {
                "paper_title": "Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teaching arithmetic to small transformers",
            "rating": 2,
            "sanitized_title": "teaching_arithmetic_to_small_transformers"
        },
        {
            "paper_title": "Investigating the limitations of transformers with simple arithmetic tasks",
            "rating": 2,
            "sanitized_title": "investigating_the_limitations_of_transformers_with_simple_arithmetic_tasks"
        },
        {
            "paper_title": "How well do large language models perform in arithmetic tasks?",
            "rating": 2,
            "sanitized_title": "how_well_do_large_language_models_perform_in_arithmetic_tasks"
        },
        {
            "paper_title": "Data distributional properties drive emergent in-context learning in transformers",
            "rating": 2,
            "sanitized_title": "data_distributional_properties_drive_emergent_incontext_learning_in_transformers"
        },
        {
            "paper_title": "Incoherent probability judgments in large language models",
            "rating": 2,
            "sanitized_title": "incoherent_probability_judgments_in_large_language_models"
        }
    ],
    "cost": 0.01501225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LANGUAGE MODELS TRAINED TO DO ARITHMETIC PREDICT HUMAN RISKY AND INTERTEMPORAL CHOICE
6 May 2025</p>
<p>Jian-Qiao Zhu 
Department of Computer Science
Princeton University</p>
<p>Haijiang Yan 
Department of Psychology
University of Warwick</p>
<p>Thomas L Griffiths 
Department of Psychology and Computer Science
Princeton University</p>
<p>LANGUAGE MODELS TRAINED TO DO ARITHMETIC PREDICT HUMAN RISKY AND INTERTEMPORAL CHOICE
6 May 2025D7100DC2B49FF0C420773A44A73F777BarXiv:2405.19313v2[cs.AI]10<em>02-4</em>08=-12 01<em>7+02</em>5+07<em>0=+17 08^2=+064 039</em>10-063<em>6=+012 92</em>023-021<em>77=+499 -214</em>085+164=-179 stylize the calculation for EV differences
The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition.However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models.For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences.Consequently, the origins of these behavioral similarities are not well understood.In this paper, we propose a novel way to enhance the utility of language models as cognitive models.This approach involves (i) leveraging computationally equivalent tasks that both a language model and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for a language model to exhibit human-like behaviors.We apply this approach to decision-making -specifically risky and intertemporal choice -where the key computationally equivalent task is the arithmetic of expected value calculations.We show that a small language model pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models.Pretraining language models on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making.Our results also suggest that language models used as cognitive models should be carefully investigated via ablation studies of the pretraining data.</p>
<p>INTRODUCTION</p>
<p>Scientists studying the behavior of Large Language Models (LLMs) in cognitive tasks typically performed by humans have found substantial evidence that LLMs produce performance similar to that of human participants (Binz &amp; Schulz, 2023b;Horton, 2023;Zhu &amp; Griffiths, 2024;Dasgupta et al., 2022;Frank, 2023b;Marjieh et al., 2023;Webb et al., 2023;Coda-Forno et al., 2024).1 Like humans, LLMs often make judgments and decisions that deviate from rational norms (Binz &amp; Schulz, 2023b;Horton, 2023;Zhu &amp; Griffiths, 2024;Coda-Forno et al., 2024).For instance, GPT-3 demonstrates human-like biases in risky choice, such as risk aversion and loss aversion (Binz &amp; Schulz, 2023b), and the statistical properties of probability judgments generated by LLMs align qualitatively with those of humans (Zhu &amp; Griffiths, 2024).LLMs also make errors in other related settings; for example, when trained on the task of predicting the next token in sequences involving arithmetic Figure 1: (A) Pre-training and evaluation pipelines.We began by generating a synthetic dataset comprised of mathematical equations including addition, subtraction, multiplication, and exponentiation.Arithmetic-GPT was pretrained on this synthetic dataset.After training, we froze model weights and extracted embeddings from the pretrained model, which then processes stylized choice tasks as input.These embeddings were subsequently compared with human choice data to evaluate their correspondence.(B) Ecological distributions of probabilities and values.In the top panel, English probability-describing phrases (black bars) can be modeled using a Beta(0.27,0.27) distribution.In the bottom panel, the value distribution of debits from UK bank accounts (scatterpoints) follows a power-law distribution.Figures adapted from Zhu et al. (2020) and Stewart et al. (2006).operations, they fail to learn precise arithmetic operands and instead approximate the correct results up to a certain input range (Nogueira et al., 2021;Lee et al., 2023).</p>
<p>In this paper, we focus on risky and intertemporal choice as exemplar domains for comparing the behavior of language models and humans (see Figure 1).Central to both domains is the computational challenge of calculating expectations.To assess the benefits of engaging in a gamble, an intelligent system must be able to calculate the expected value (EV) of the gamble, typically represented as:
EV (A) = i∈A p i × x i (1)
where each outcome i of gamble A is associated with a payoff x i and a probability p i , with the constraint that i p i = 1.Similarly, in considering an intertemporal choice the computation of the present value (PV) of future outcomes in A is crucial:
P V (A) = t∈A d t × x t (2)
where the value x t is realized at time t and is discounted by a factor of d, reflecting the time preference of the decision-maker.Note that a risk-neutral and time-consistent agent should always select the option that maximizes EV and PV.However, extensive research in economics and psychology demonstrates that people systematically deviate from this maximizer model (Kahneman &amp; Tversky, 2013;Gigerenzer &amp; Gaissmaier, 2011;Laibson, 1997;Zhu et al., 2020).</p>
<p>Pretrained, off-the-shelf LLMs, such as the GPT and LLaMA series, have demonstrated behavioral similarities to humans in tasks involving risky and intertemporal choices (Horton, 2023;Binz &amp; Schulz, 2023b;Manning et al., 2024).However, the embeddings generated by these LLMs are not by default able to account for human data.For example, embeddings from the LLaMA-1-65B model, when not finetuned on human risky choices, poorly predict those choices (Binz &amp; Schulz, 2023a).Therefore, understanding what enables LLMs to exhibit human-like decision-making behavior remains an unresolved challenge.</p>
<p>guage model trained on ecologically valid calculations of expectations.This suggests that humanlike biases could result from the training task and numerical reasoning, independent of natural language supervision.As a corollary, this hypothesis also implies that deviations from rational choice in humans could be primarily explained by computational errors during the EV or PV calculations.To test this hypothesis, we generate a series of synthetic datasets that contain expected value computations; examples include 0.5 * 100=+50, 0.8 * 1+0.8ˆ2 * 10=+7.2, and 30 * 0.79-261 * 0.83=-192.93.Subsequently, we train a small, randomly-initialized language model (approximately 10M parameters) on these datasets.After training, we extract embeddings from the now pretrained language model and analyze how well they can account for human choices.</p>
<p>We conduct carefully ablated experiments on different aspects of the synthetic data to isolate the factors that result in embeddings that better predict human choice patterns.Our findings reveal that when the synthetic dataset reflects the ecological distributions of probabilities and values-mirroring real-world frequencies-the resulting embeddings best predict human choices.With this pretraining, models based on the derived embeddings outperform many existing behavioral models of risky and intertemporal choice.</p>
<p>BACKGROUND</p>
<p>The question of whether language models can serve as models of human cognition has sparked intense debate within the fields of machine learning and cognitive science (Frank, 2023b;Messeri &amp; Crockett, 2024;Griffiths et al., 2023;Horton, 2023).Although there are behavioral similarities between off-the-shelf LLMs and humans, these do not inherently qualify LLMs as effective cognitive models (c.f. the Clever Hans effect) (Shiffrin &amp; Mitchell, 2023).There are compelling reasons why current LLMs may not be suitable as cognitive models.First, LLMs are trained on datasets vastly larger than those available to human learners (Frank, 2023a).Second, LLMs may have already been trained on the test questions, particularly if the training data is undisclosed and poorly controlled (Jiang et al., 2024).Third, the inclusion of value alignment steps, such as Reinforcement Learning from Human Feedback (Ziegler et al., 2019) and Direct Preference Optimization (Rafailov et al., 2024), may artificially enhance human-like behaviors in leading LLMs.Finally, the immense size of deep neural networks and the proprietary nature of leading LLMs hinder detailed investigation into their internal representations.</p>
<p>Researchers have taken steps to address some of the concerns associated with using LLMs as cognitive models.One branch of research looks at the potential benefits of fine-tuning off-the-shelf LLMs to better understand human cognition.For instance, fine-tuning the LLaMA-1-65B model (Touvron et al., 2023) on datasets of human choices results in a model that can predict human data more accurately than traditional cognitive models (Binz &amp; Schulz, 2023a).Although the specific mechanisms underlying the finetuned LLM's ability to replicate human choices remain unclear, this work serves as a proof-of-concept and suggests that the embeddings learned from extensive pretraining on Internet text and/or from the value alignment process may offer valuable insights into human cognitive processes that complement those provided by traditional cognitive models.</p>
<p>Another line of research emphasizes the importance of the composition of synthetic datasets, which are critical for enhancing certain capabilities of LLMs (Lee et al., 2023).These studies typically assess LLMs based on their problem-solving abilities rather than their human-likeness.For example, it has been found that larger language models tend to perform arithmetic tasks, such as addition and multiplication, better than their smaller counterparts (Yuan et al., 2023).Moreover, the order of input and the inclusion of intermediate information about task decomposition have been shown to facilitate chain-of-thought reasoning, which significantly helps smaller language models in mastering arithmetic tasks (Lee et al., 2023).</p>
<p>Finally, there is a precedent for the idea that pretraining machine learning models on synthetic datasets can improve performance in predicting human decisions.Bourgin et al. (2019) showed that a model pretrained on choice data generated from a psychological theory could perform extremely well in predicting human decisions when fine-tuned with a small amount of human data.</p>
<p>Our approach builds on this idea, but reduces it to the most primitive components -rather than pretraining on data generated from a psychological theory, we pretrain on a task that captures the basic computations required to make rational decisions.</p>
<p>ARITHMETIC-GPT: A SMALL LANGUAGE MODEL TRAINED TO PERFORM ARITHMETIC</p>
<p>In this paper, we confront the challenges of making language models as cognitive models head-on.</p>
<p>First, we define a data generation algorithm to produce synthetic datasets, thereby gaining complete control over the training data for language models and addressing issues related to data gaps and contamination.Second, we have direct access to the neural activation patterns that are crucial for decision-making processes.This approach allows us to more thoroughly evaluate and understand the capabilities and limitations of language models.</p>
<p>MODEL DETAILS</p>
<p>Our small language model employs a standard architecture for a Generative Pretrained Transformer (GPT) model (Vaswani et al., 2017;Radford et al., 2019).Detailed specifications of the model architecture are provided in Table 1.Tokenizer.To handle a domain-specific vocabulary dedicated for arithmetic equations, we built a custom tokenizer on the sub-word level.The vocabulary size is 320, containing special tokens (e.g., <AMB>,<PAD>), arithmetic operators (e.g., +,-, * ,.,ˆ,=), and all the integers from 0 to 300 which are designed to split numbers into individual digits.The vocabulary can cover most EV calculations in risky choice and PV calculations in intertemporal choice tasks.</p>
<p>Positional embedding.Absolute positional embedding was learned during training through an embedding layer.Each position had a corresponding 320 dimensional embedding vector.</p>
<p>Attention Mask.To ensure that attention is only applied to the left in the input sequence, we incorporated a causal mask to prevent attending to future tokens when predicting the next one.</p>
<p>SYNTHETIC DATASETS</p>
<p>At the heart of the EV and PV computations lies the multiplication of two real numbers.Typically, each outcome appears in the computation as either a probability multiplied by a value, or a discount factor multiplied by a value.Probabilities are real numbers ranging from 0 to 1, represented with a precision of up to two decimal places.Similarly, values are real numbers that range from 0 to 300, also with a maximum of two decimal places.We selected these ranges to align with the numerical scope of human experimental studies.A single training example involves either the addition or subtraction of two simulated outcomes, which together constitute the left-hand side of an equation.</p>
<p>We then compute the corresponding result and place it on the right-hand side of the equation.In total, we randomly simulated 1M such equations.</p>
<p>In our experiment, we evaluate four variants of the data generation algorithm by manipulating the frequency of probabilities and values (see Table 3).The Uniform synthetic data generates probabilities and values with maximum uncertainty; probabilities are uniformly distributed between 0 and 1 (i.e., U [0, 1]), and values range from 0 to 300 (i.e., U [0, 300]).Conversely, the Ecological synthetic data generates probabilities following a Beta distribution Beta(0.27,0.27) (Zhu et al., 2020) and values according to a power-law distribution with an exponent of −0.945 for the same range (Stewart et al., 2006).These distributions are chosen because they have been shown to closely match the natural frequencies of probabilities and values in real-world scenarios (Stewart et al., 2006;Zhu et al., 2020) (see Figure 1B for details).For both uniform and ecological synthetic datasets, we also created a matching dataset where the answers on the right-hand side are generated with a 50% chance of displaying the incorrect sign (i.e., the ablated variants in Table 3).In each of the four synthetic datasets, we randomly masked 10% of the probability values using a special <AMB> token to denote unknown probabilities.</p>
<p>PRETRAINING DETAILS</p>
<p>We trained our Arithmetic-GPT models from scratch with a context length of 26.Batch size was set to 2048 lines of mathematical equations, randomly sampled from the synthetic datasets, and the learning rate was 10 −3 .To optionally terminate the training process, we designated 90% of the synthetic dataset as the training set, reserving the remaining 10% for validation.We used crossentropy loss to measure the discrepancy between the predicted and target sequences.Training was stopped when the validation loss plateaued, with validation loss evaluated every 100 epochs (see Figure A1 in Appendix C for an example learning curve).The AdamW optimizer was used.</p>
<p>HUMAN TARGETS</p>
<p>To investigate whether Arithmetic-GPT contains information pertinent to explaining human decision-making, we reanalyzed recent experiments in which people were asked to make risky and intertemporal choices (Peterson et al., 2021;Erev et al., 2017;Gershman &amp; Bhui, 2020;Agrawal et al., 2023).The primary reason for examining these particular types of human choices is that calculations of expected value are integral to making rational decisions (see Equations 1 and 2).In other words, they are computationally equivalent tasks under the assumption of rationality.</p>
<p>As summarized in Table 2, we sampled four existing datasets from the literature, which included two large-scale experiments (Peterson et al., 2021;Agrawal et al., 2023).In experiments involving risky choices (Peterson et al., 2021;Erev et al., 2017), participants were often presented with two options, each fully describing the details of a gamble (see Figure 1A risky choices).In cases involving ambiguous gambles where probabilities are unknown, we used the special token <AMB> to denote gambles with unknown probabilities.We excluded decision-with-feedback trials, as these would require additional assumptions about how individuals respond to feedback (but see Table A2 of Appendix B for a comparison based on the entire choices13k dataset).In intertemporal choice tasks (Gershman &amp; Bhui, 2020;Chávez et al., 2017;Agrawal et al., 2023), participants were also presented with two options, typically offering a choice between a smaller, sooner payoff and a larger, later payoff (see Figure 1A intertemporal choices).Without loss of generality, we fixed the annual discount factor at d year = 0.85 throughout the paper.This also corresponds to a monthly discount factor of d month = 0.98 and a daily discount factor of d day = 0.99.We rescaled the values in both options by the same factor to fit within the specified range.</p>
<p>OTHER MODELS</p>
<p>We also conduct a model comparison on human data, evaluating the following approaches: (i) classical behavioral models such as prospect theory (Kahneman &amp; Tversky, 2013) and the hyperbolic discounting model (Laibson, 1997); (ii) a neural network directly trained on the human datasets; (iii) an untrained Arithmetic-GPT model; and (iv) an off-the-shelf, open-weight LLM, LLaMA-3-70B-Instruct.2</p>
<p>Classical behavioral models.To explain human risky choices, prospect theory proposes an Sshaped utility function that is concave for gains, convex for losses, and steeper for losses than for gains (reflecting loss aversion).The utility function is represented mathematically as:
U (x) = x α , if x ≥ 0 −λx β , if x &lt; 0 (3)
where x denotes the value, while the shape parameters α and β define the curvature of the utility function.The parameter λ ≥ 1 reflects loss aversion.The theory further suggests that individuals possess a distorted perception of probabilities, modeled as follows:
w(p) = p γ (p γ + (1 − p) γ ) 1/γ (4)
where p is the objective probability, and γ is a parameter controlling the curvature of the weighting function.Consequently, the utility of a gamble is formally expressed as:
U (A) = i∈A w(p i ) × U (x i )(5)
Contrary to the consistent risk preferences implied by Equation 1or other monotonic transformations of value, prospect theory suggests that risk preferences are inconsistent across different values and probabilities.This inconsistency results in incoherent choices when individuals are faced with risky decisions (Tversky &amp; Kahneman, 1992).</p>
<p>To capture human time preferences, particularly the impact of present bias, the hyperbolic discounting model suggests that future values should be discounted as follows:
P V (x t ) = x t 1 + kt (6)
where x t is the value to be received at future time t, and k is the discount factor that quantifies the degree of time preference.In contrast to the consistent time preferences implied by Equation 2, the hyperbolic discounting model suggests that time preferences are inconsistent across different time horizons, leading to stronger preference to immediate over future rewards (Laibson, 1997).</p>
<p>MLPs directly trained on human choices.We also implemented Multilayer Perceptrons (MLPs) with a single hidden layer containing 320 neurons and using the sigmoid activation function.These MLPs were directly trained on each of the four human datasets, using the stimuli features as input to predict choice rates.These MLPs potentially capture an upper bound of the explainable variance within human data (Agrawal et al., 2020).However, they may have overlooked significant constraints from the original psychological experiments, as these models use task features as input rather than the actual stimuli presented in texts or the stylized representations required for rational agents, as in our Arithmetic-GPT.</p>
<p>Choice probabilities and embeddings from open-weight LLMs.To further investigate the impact of different input formats and compare with off-the-shelf LLMs, we evaluated the performance of LLaMA-3-70B-Instruct on human choice data.Unlike Arithmetic-GPT, the LLaMA3 model not only excels in arithmetic tasks but also comprehends and generates human-like text.This capability results from its training on extensive text data and a significantly larger number of model parameters.</p>
<p>In short, LLaMA3 is a more versatile and powerful model, also based on the transformer architecture and trained autoregressively.</p>
<p>Given these features of LLaMA3, we presented each choice problem in two different formats: textbased and arithmetic equation-based.The text format converts each choice problem into a descriptive narrative, simulating the stimuli presented to human participants (see Appendix A for a detailed description of the text prompts).We instructed the model to report its selection between the two options, using the log probability of the chosen option to determine the model's predicted choice rates for the corresponding option.In contrast, the arithmetic-equation format presents the choice problems as a series of arithmetic computations required by a rational agent.Note that this format is identical to the input used for Arithmetic-GPT.We obtained the embeddings from LLaMA3 for each choice problem represented in the arithmetic-equation format.Note. a Uniform synthetic datasets.b The same uniform synthetic datasets but with the answers on the right-hand side of the equations removed and the signs randomized between positive and negative.c Ecological synthetic datasets.d The same ecological synthetic datasets but with the answers on the right-hand side of the equations removed and the signs randomized between positive and negative.e Embeddings from an untrained Arithmetic-GPT model with randomly initialized weights.f Log probabilities of LLaMA3 elicited from text descriptions of choice problems.g The training data is not publicly available, but Meta has disclosed some summary statistics of the training corpus.h Embeddings of LLaMA3 elicited from text descriptions of choice problems.i Embeddings of LLaMA3 elicited from arithmetic equations of choice problems.j The hyperbolic discounting model for intertemporal choices.k Logistic regression results when applied to the expected value difference between the two choice options.</p>
<p>EXPERIMENTAL RESULTS</p>
<p>MODEL COMPARISONS</p>
<p>In this section, we present the experimental results from our model comparisons.We first obtained embeddings from Arithmetic-GPT and LLaMA3 models, evaluating versions of the model that were pretrained on each of our four distinct synthetic datasets as well as a version without any training.Specifically, we extracted embeddings for the expected values of the two options, denoted as e A and e B .Additionally, we obtained embeddings for the difference in expected values, denoted as e A−B .All embeddings were derived from the representation in the final layer before the autoregressive prediction.We then performed a logistic regression using e A , e B , and e A−B as independent variables, with human choice probabilities as the dependent variable.Adjusted R 2 values were used for all logistic regressions, including those for Arithmetic-GPT, LLaMA3, and EV results.All other R 2 values were reported as the squared Pearson's r between model predictions and human data.</p>
<p>These results indicate that the embeddings from the Arithmetic-GPT model pretrained on ecologically valid synthetic datasets most accurately capture human choices.This model also outperforms the embeddings obtained from the LLaMA3 model (the 7th row of Table 3), suggesting that pretraining on synthetic datasets is sufficient to create a strong correspondence between LLMs and human decision-making.However, the LLaMA3 model performs comparably to Arithmetic-GPT in predicting intertemporal choices.The log probabilities from the same LLaMA3 model perform poorly in comparison to human data (the 6th row of Table 3), replicating previous findings using choice rates reported from LLaMA1 (Binz &amp; Schulz, 2023a).The R 2 values between Arithmetic-GPT pretrained on uniform and ecological synthetic datasets are small yet consistent.This may be due to the limited range tested in the human datasets.</p>
<p>To benchmark the performance of Arithmetic-GPT models in explaining human data, we directly fit behavioral models and MLPs on each of the human choice datasets.Prospect theory and the hyperbolic discounting model are leading behavioral models in risky and intertemporal choices, respectively (Kahneman &amp; Tversky, 2013;Laibson, 1997).The behavioral models have interpretable mechanisms and contain few free parameters.However, they do not explain human data as well as the embeddings from both LLaMA3 and Arithmetic-GPT, although they still outperform a simple choice model based on EV difference.Moreover, prospect theory and the hyperbolic discounting model do not generalize to both risky and intertemporal choices, resulting in N/A values in their respective rows of Table 3.</p>
<p>In contrast, fitting an MLP directly on human data potentially reveals the ceiling performance of any model in explaining the data (Agrawal et al., 2020;Peterson et al., 2021).Except for the intertemporal choice tasks, MLPs outperform all other models.It is important to note that MLP training was based on task features rather than text descriptions that simulated participants' experiences or arithmetic equations that mimic a rational agent's computations.Consequently, these differences in input formats could also lead to diverging performance.The arithmetic format of intertemporal choice may provide a better fit for human data.Indeed, there is increasing evidence from experimental economics suggesting that the complexity of discounting values, even in the absence of actual payoff delays, influences intertemporal choices (Enke et al., 2023).We include a robustness check using 10-fold cross-validation in Appendix B.</p>
<p>IMPLICIT FUNCTIONS OF PROBABILITIES, VALUES, AND DISCOUNT FACTORS</p>
<p>To understand why Arithmetic-GPT, pretrained to calculate expected values with ecological distributions of probabilities and values, can capture human choices, we examined the implicit functions of probabilities, values, and times derived from the model embeddings.We extracted embeddings for probabilities ranging from 0.0 to 1.0, values ranging from −300 to +300, and discount factors ranging from 0.99 0 to 0.99 30 .These high-dimensional embeddings were then reduced to a single dimension using multidimensional scaling with Euclidean distance.Additionally, for probabilities and discount factors, we normalized the embeddings to a range between 0 and 1.</p>
<p>We find that the embeddings from ecologically pretrained Arithmetic-GPT replicate classical findings from behavioral economics, including value and probability weighting functions from the prospect theory (Tversky &amp; Kahneman, 1992) and the hyperbolic discounting function (Laibson, 1997).Specifically, probabilities close to 0.5 are more similar to each other than to probabilities close to either 0 or 1 (see Figure 2A and Equation 4).Embeddings for values illustrate concavity for positive values, convexity for negative values, and a steeper slope for negative values than for positive values (see Figure 2B and Equation 3).These features reflect risk aversion for gains, risk seeking for losses, and loss aversion (Kahneman &amp; Tversky, 2013;Tversky &amp; Kahneman, 1992).Moreover, embeddings for the discount factor demonstrate that distant times are more similar (see Figure 2C and Equation 6), enabling a present-bias (Meier &amp; Sprenger, 2010).However, embeddings from Arithmetic-GPT pretrained on non-ecological datasets (i.e., ablated ecological, uniform, and ablated uniform) and those from LLaMA3 exhibit fewer human-like distortions in their implicit functions (see Appendix E for details).</p>
<p>For the probability weighting function, the curvature parameter (γ) of human participants was found to be 0.61 for gains and 0.69 for losses (Tversky &amp; Kahneman, 1992).Follow-up replications by (Wu &amp; Gonzalez, 1996) found γ values around 0.7.Moreover, the utility curvature parameters (α and β) typically range between 0.5 and 0.9, while the loss aversion parameter (λ) is approximately 2.25 (Tversky &amp; Kahneman, 1992).Regarding human intertemporal choices, typical k values range between 0.01 and 0.1 for small magnitude studies (Odum, 2011).Comparing to these best-fitting parameters from human experiments, we observe that the implicit functions derived from the embeddings of Arithmetic-GPT also quantitatively match those observed in humans (see Figure 2).The implicit functions, however, exhibit discontinuities, suggesting that the smooth functions derived from human theories may not be directly applicable to explain the embeddings of Arithmetic-GPT.</p>
<p>DISCUSSION</p>
<p>We introduced an approach to transforming language models into cognitive models by pretraining them on tasks that mirror the computational process performed by a rational agent.Specifically, we pretrained Arithmetic-GPT to calculate expected values using ecologically distributed probabilities and values.This pretraining allows the model to better capture human risky and intertemporal choices compared to classical behavioral models and, in some cases, even surpass the performance of the general-purpose LLaMA3 model.These results suggest that the observed cognitive biases in LLMs may stem from the training task, architecture, and numerical reasoning abilities, without requiring natural language supervision.These results have implications for a number of questions in cognitive science and machine learning, although we also note the limitations of our current work.</p>
<p>Language Models as Cognitive Models.</p>
<p>There is a growing research effort focused on exploring LLMs as scientific tools for understanding the human mind (Binz &amp; Schulz, 2023a;Frank, 2023b;Horton, 2023).Despite the challenges outlined in Section 2, LLMs offer unique opportunities for investigating cognitive processes in ways that are not feasible with human participants.Recent studies have even demonstrated that fine-tuning a LLaMA1 model on human data allows the model to outperform classical models in explaining this data (Binz &amp; Schulz, 2023a).While the LLaMA series' model architecture and weights are publicly available, researchers lack access to the training data, which makes crucial scientific inference practically impossible.In contrast, we explicitly manipulate the training data for our Arithmetic-GPT, thereby uncovering a key factor that contributes to the model's ability to explain human choices.This targeted approach, however, comes at the cost of the broader versatility inherent in off-the-shelf LLMs, which are designed to handle a wider range of tasks.Notably, language models, including Arithmetic-GPT, do not directly model human cognitive processes.Instead, our work suggests that these models are better suited for probing the computational problems that human cognition aims to solve.</p>
<p>Bayesian Models of Cognition, Meta-learning, and Pre-training.Bayesian models of cognition have been instrumental in understanding human performance across a variety of cognitive tasks by providing optimal solutions to the inductive inference problem.Recently, it has been argued that Bayesian models and neural network models can be viewed as complementary to one another (Griffiths et al., 2023).Neural networks that are meta-trained have also been shown to exhibit properties consistent with Bayesian models (Lake &amp; Baroni, 2023;McCoy &amp; Griffiths, 2023).Moreover, pretraining a neural network model for improved performance on downstream tasks can be seen as a form of meta-learning or the acquisition of useful inductive biases, similar to Bayesian models (Hsu et al., 2018).Our work makes the implicit priors learned by Arithmetic-GPT more explicit by specifying the synthetic dataset on which it was pre-trained.</p>
<p>Computationally Equivalent Tasks for Cognitive Modeling.Modeling human cognition is challenging because the hypothesis space of possible cognitive mechanisms that can explain human data equally well is vast.This makes principles of rationality desirable, as assuming people are rational in some sense greatly constrains the hypothesis space, thereby making scientific inference more effective.However, human rationality has been a subject of debate in economics and psychology for over a century (von Neumann &amp; Morgenstern, 1944;Kahneman &amp; Tversky, 2013;Simon, 1997).While significant progress has been made in understanding human rationality (e.g., Lieder &amp; Griffiths, 2020;Gershman et al., 2015), the advent of LLMs seems to challenge the need for rational theories.</p>
<p>Simply training LLMs to predict the next word appears sufficient to produce human-like behaviors, suggesting that we can model human cognition without the constraints imposed by rational theories.However, our experimental results suggest an alternative route to enhancing the correspondence between behaviors produced by LLMs and humans: pretraining on computationally equivalent tasks that a rational agent would need to master.Future research should investigate the impact of different assumptions about the nature of rationality on task content and distributions, and explore whether there are more effective assumptions for pre-training models to explain human behavior.</p>
<p>Implications for Theories of Human Risk and Time Preferences.The success of Arithmetic-GPT in explaining human risky and intertemporal choices has significant implications for theoretical work on human risk and time preferences.While the Homo economicus portrayal of human beings as perfectly rational and self-interested agents has been inadequate in describing human choices (Gigerenzer &amp; Gaissmaier, 2011;Kahneman &amp; Tversky, 2013), existing behavioral models that deviate from rationality do not generalize well across different task domains.For instance, it is challenging to use prospect theory to model time preferences or to use the hyperbolic discounting model to explain risk preferences.In contrast, Arithmetic-GPT has demonstrated substantial transferability across task domains.The same model, in principle, can be adapted to other judgment and decision-making tasks such as social choice, probability judgment, and belief updating.The key factor enabling language models to generalize across a wide range of tasks is the presence of a language interface, which underpins a significant range of cognitive tasks.</p>
<p>The Importance of Training Data Disclosure.Our results demonstrate that the training data used for LLMs is crucial for understanding their emergent capabilities and the inductive biases they acquire during pretraining.Adjusting the distribution of, or ablating, the training data can significantly affect the degree to which LLMs correspond with human behaviors.These findings suggest that existing off-the-shelf LLMs, whether proprietary or open-weight, including the GPT series (Brown et al., 2020) and the LLaMA series (Touvron et al., 2023), are less effective as models of human cognition.This is primarily because their training data is rarely disclosed, making it difficult for scientists to control for data contamination and thereby precisely identify the sources of human-like behaviors in these models.</p>
<p>Limitations and Future Research.While we have made progress in addressing many challenges associated with using language models as cognitive models, some issues remain unresolved.To address the data gap between LLMs and human learners, we limited the scope of our synthetic dataset to the arithmetic of expected value calculations.Despite this, LLMs still require a substantial amount of training data to perform arithmetic accurately within a limited range of input values.Moreover, it is unrealistic for human learners to process 1M randomly generated mathematical equations, as Arithmetic-GPT did, to acquire the skill of computing expected values.Further research is needed to continue bridging this data gap (but see Appendix F for an initial attempt).</p>
<p>Additional ablation studies could be performed on model architectures and training objectives.Our work is fundamentally limited to autoregressive training and a decoder-only transformer architecture.We believe that alternative training mechanisms and model architectures could potentially yield better embeddings from language models.One area for future research is estimating the lower bounds on model size necessary to achieve a certain level of correspondence with human behavior.Another area of future work involves leveraging interpretability techniques to distill novel cognitive mechanisms from Arithmetic-GPT.</p>
<p>Conclusion.</p>
<p>Large language models have opened new horizons for research on human cognition, but also introduce a new set of challenges based on the volume of training data, the content of those data, the influence of value alignment, and limited access to the training regimes and weights of these models.We have proposed an approach to addressing these challenges, based on training small language models with datasets that are generated based on tasks that are hypothesized to be computationally related to the problem that human minds face.Our results show that this approach is extremely effective in predicting human decisions, where training on arithmetic results in representations that can be used to predict human choices better than both existing psychological models and large language models trained on broader datasets.This approach is easily generalizable to other cognitive tasks that primarily rely on language interface, and can even be used with other kinds of foundation models to study human perception.In this section, we explore how adjustments to key hyperparameters of Arithmetic-GPT impact the quality of its pretrained embeddings.Specifically, we investigate the effects of reducing the model's hidden size from 320 (as reported in the main text) to 104 and 16.Moreover, we examine the influence of decreasing the size of the ecological synthetic dataset, reducing it from 1M mathematical equations (as reported in the main text) to subsets of 100K and 10K equations.Apart from these modifications, we adhered to the same pretraining procedure and evaluated the resulting embeddings on the two largest datasets, choices13k for risky choices (Peterson et al., 2021) and agrawal23 (Agrawal et al., 2023) for intertemporal choices, using 10-fold cross-validation.</p>
<p>The results, summarized in Table A3, reveal an intriguing pattern: reducing the size of the ecological synthetic dataset to as few as 10K equations does not significantly impair the ability of the 10M model's embeddings to predict human choices.It is important to note that Arithmetic-GPT models were never exposed to human data during pretraining.However, significant reductions in the model size -particularly 30K parameters -lead to a decline in performance, highlighting the critical role of model capacity in embedding quality.Note.The bold numbers represent the top-performing variants of the Arithmetic-GPT models, as measured by R 2 .In cases where multiple models are highlighted, the differences in R 2 values are not statistically significant at the p = 0.05 level.</p>
<p>We conducted an exploratory analysis using the original Arithmetic-GPT model architecture and synthetic datasets of 1M equations to investigate the effects of varying data distributions.Specifically, we independently manipulated the distributions of probabilities and values.For probability distributions, we considered Beta(0.27,0.27) (ecological), Beta(1, 1) (uniform), and Beta(2, 2).For value distributions, we examined power-law distributions with exponents of 0 (uniform), -0.945 (ecological), and -2.All Arithmetic-GPT models were randomly initialized and trained following the procedure outlined in Section 3.1.</p>
<p>As shown in Table A4, aligning probability distributions with ecological patterns, such as Beta(0.27,0.27), generally enhances the quality of embeddings for predicting risky choices.Conversely, adopting a power-law distribution for values with an exponent of -2 tends to improve the model's performance in predicting intertemporal choices.</p>
<p>Table A4: Variants of Arithmetic-GPT models pretrained on different data distributions.In each cell, R 2 results (in percentage) are reported as choices13k (left of '/') and agrawal23 (right of '/').Numbers in parentheses indicate standard errors obtained from 10-fold cross-validation.</p>
<p>Distribution of probabilities Beta(0.27,0.27)Beta(1,1) Beta(2,2) 0 56.5 (0.5) / 82.5 (0.4) 56.4 (0.4) / 81.5 (0.4) 56.0 (0.3) / 77.5 (0.4) Distribution of values (exponent of power-law) -0.945 57.7 (0.4) / 83.7 (0.3) 56.7 (0.3) / 78.4 (0.5) 56.5 (0.3) / 77.6 (0.4) -2 55.7 (0.3) / 85.9 (0.3) 56.3 (0.5) / 82.6 (0.4) 55.5 (0.4) / 82.2 (0.4)</p>
<p>Note.The bold numbers represent the top-performing variants of the Arithmetic-GPT models, as measured by R 2 .</p>
<p>G COMPUTATIONAL ABILITIES OF LANGUAGE MODELS</p>
<p>We evaluate the abilities of LLaMA-3-70B-Instruct and Arithmetic-GPT in computing expected values using two newly generated test datasets.Both test sets consist of 20K synthetically generated equations involving expected value calculations.In one test set, the probabilities and values in the equations are sampled from a uniform distribution, while the other test set features probabilities and values derived from ecological distributions, as illustrated in Figure 1.The train-test relationships for these datasets are summarized in Table A5.</p>
<p>The computational ability of Arithmetic-GPT models was assessed by calculating the probability of generating correct expected values (rounded to two decimal places).Formally, the correct probability is computed as P model (true expected values | the left-hand side of the equation), using token probabilities.Arithmetic-GPT, when pretrained on ecological distributions, demonstrated higher probabilities of generating correct values across both uniform and ecological test sets.When comparing performance on the same model across the two test sets, expected values in the ecological test set were found to be easier to predict.One possible explanation for the improved performance of Arithmetic-GPT models pretrained on ecological distributions is that these distributions are more "bursty," characterized by multiple occurrences of similar numerical values (cf.Chan et al., 2022).Pretraining on bursty sequences has been shown to facilitate emergent behaviors in LLMs, such as in-context learning (Chan et al., 2022).</p>
<p>To evaluate LLaMA3's ability to compute expected values, we directly prompted the model to solve arithmetic equations while setting the temperature to 0. The integer part of the expected values was extracted from LLaMA3's responses by filtering out irrelevant tokens.The correct probability was calculated as the relative frequency of correctly predicting the integer part of the true expected values across 20K questions.As shown in Table A5, LLaMA3 performed poorly compared to Arithmetic-GPT, achieving only 20.97% and 38.79% accuracy on the uniform and ecological test sets, respectively.</p>
<p>The model's behavioral performance on the ecological test set broadly predicts the ranking of its embeddings in modeling human data.However, directly interpreting a language model's performance on arithmetic tasks in relation to model-fitting results that leverage its embeddings can be challenging.Therefore, further investigation is necessary to establish a robust connection between a language model's arithmetic capabilities and the predictive power of its embeddings in predicting human choices.</p>
<p>Figure 2 :
2
Figure 2: Embeddings from ecologically pretrained Arithmetic-GPT for inputs including (A) probabilities, (B) values, and (C) discount factors.Inputs are shown along the horizontal axes and embeddings are shown on the vertical axes.The embeddings, shown as black dots, were reduced to 1D using multidimensional scaling.Embeddings for probabilities and discount factors are normalized between 0 and 1 (see Appendix D for details).The red curves represent the best-fitting behavioral economic models: (A) the probability weighting function from PT with best-fitting γ = 0.58, (B) the utility function from PT with best-fitting α = 0.42, β = 0.45, λ = 1.4,and (C) the hyperbolic discount function with best-fitting k = 0.08.</p>
<p>Figure A3 :
A3
Figure A1: (A) Training loss decreases over the course of training epochs for Arithmetic-GPT trained on ecological synthetic dataset.(B) Histogram displaying the differences between the top-1 responses of the pretrained Arithmetic-GPT model and the actual expected values in the ecological synthetic dataset.</p>
<p>Table 1
1: Arithmetic-GPT 10M: a small language model pretrained to do arithmeticPre-training hyperparameters ValueHidden size320Layers8Heads8Context length26Vocabulary size320Attention variantCausal self attentionDropout0.2BiasesNone</p>
<p>Table 2 :
2
Overview of human experiments and data sourcesNote.In our analysis of the choices13k and cpc18 datasets, we excluded trials involving risky choices made with feedback.Modeling how individuals respond to feedback requires additional cognitive mechanisms beyond the scope of this work.
PaperDatasetDomainNo. Participants No. ProblemsPeterson et al. (2021)choices13k Risky choices15,15313,006Erev et al. (2017)cpc18Risky choices446270Gershman &amp; Bhui (2020) gershman20 Intertemporal choices 2214,794Agrawal et al. (2023)agrawal23 Intertemporal choices 12,9069,853</p>
<p>Table 3 :
3
Proportion of the variance of human choices explained(R 2) by embeddings from the pretrained Arithmetic-GPT model compared to other computational models.Bold numbers indicate the best models within each group.
Risky choicesIntertemporal choices</p>
<p>Table A1 :
A1
Cross-validation results.Propotion of the variance in human choices explained(R 2) on the validation set for each model.Numbers in parentheses represent standard errors from 10-fold cross-validation.Note.The bold numbers represent the top-performing variants of the Arithmetic-GPT models, as measured by R 2 .In cases where multiple models are highlighted, the differences in R 2 values are not statistically significant at the p = 0.05 level.
Risky choicesIntertemporal choicesModelTraining datachoices13k cpc18 gershman20 agrawal23Arith.-GPTSynthetic (unif.)56.4% (0.4%)46.3% (3.1%)56.7% (1.1%)81.5% (0.4%)Arith.-GPTSynthetic (unif. abl.)38.5% (1.5%)21.9% (4.6%)51.1% (0.7%)54.0% (0.3%)Arith.-GPTSynthetic (eco.)57.7% (0.4%)51.7% (4.9%)57.1% (1.2%)83.7% (0.3%)Arith.-GPTSynthetic (eco. abl.)38.5% (1.5%)33.6% (5.0%)55.3% (1.2%)68.3% (0.2%)Arith.-GPTNone33.1% (1.5%)23.5% (3.8%)15.3% (1.7%)50.4% (5.1%)LLaMA3 (txt emb.) Undisclosed54.0% (0.8%)35.0% (5.0%)52.7% (1.1%)89.9% (0.1%)LLaMA3 (arith.) Undisclosed38.6% (1.5%)20.8% (3.0%)54.9% (1.2%)87.6% (0.1%)MLPHuman choices62.3% (1.2%)50.3% (3.9%)58.7% (1.3%)84.9% (2.4%)</p>
<p>Table A2 :
A2
Proportion of the variance in human choices explained(R 2) on the entire choices13k dataset for each model.Bold numbers indicate the best models within each group.
ModelTraining dataEntire choices13kArith.-GPTSynthetic (unif.)64.0%Arith.-GPTSynthetic (unif. abl.)49.2%Arith.-GPTSynthetic (eco.)64.3%Arith.-GPTSynthetic (eco. abl.)52.1%LLaMA3 (txt.)Undisclosed14.6%LLaMA3 (txt emb.) Undisclosed65.6%LLaMA3 (arith.) Undisclosed60.6%Prospect theoryHuman choices45.6%Expected valueNone40.2%MLPHuman choices73.8%</p>
<p>Table A3 :
A3
Variants of Arithmetic-GPT models pretrained on ecological synthetic datasets of varying sizes.In each cell, R 2 results (in percentage) are reported as choices13k (left of '/') and agrawal23 (right of '/').Numbers in parentheses indicate standard errors obtained from 10-fold cross-validation.
Model Sizes10M1M30K(hidden size=320)(hidden size=104)(hidden size=16)1M 57.7 (0.4) / 83.7 (0.3) 54.7 (0.6) / 83.4 (0.3) 30.6 (0.4) / 56.8 (0.4)Data quantity 100K 57.2 (0.6) / 83.6 (0.4) 55.5 (0.5) / 81.6 (0.4) 31.4 (0.8) / 63.4 (0.2)10K 57.7 (0.5) / 83.6 (0.3) 50.0 (0.5) / 84.1 (0.3) 29.1 (0.6) / 63.0 (0.4)</p>
<p>Table A5 :
A5
Evaluation of language models' probabilities of generating correct expected values across different train-test distributions.Numbers in parentheses represent standard errors.we detail the hyperparameter setup for experiments.All computations for synthetic datasets are run on single Nvidia RTX 3060 GPU, and those for LLaMA3 embeddings are run on single A100 GPU.
ModelTraining dataTest data Correct probabilityArith.-GPT Synthetic (unif.) Uniform0.8261 (0.0023)Ecological 0.9336 (0.0014)Arith.-GPT Synthetic (eco.) Uniform0.9388 (0.0013)Ecological 0.9780 (0.0007)LLaMA3 UndisclosedUniform0.2097Ecological0.3879H IMPLEMENTATION DETAILS
Here</p>
<p>LLMs can also display significant deviations from human behavior (e.g.,Chen et al.,<br />
;Hagendorff et al., 2023).
https://llama.meta.com/llama3/
We propose a hypothesis about how human-like decision patterns might be produced in language models for risky and intertemporal choice: such human-like behaviors might arise from a lan-Acknowledgments.This work and related results were made possible with the support of the NOMIS Foundation.H. Yan acknowledges the Chancellor's International Scholarship from the University of Warwick for additional support.8 8 s N s 7 R b T f 2 b 7 n 0 4 b 3 i f 8 v g n d x M = " &gt; A A A C F n i c b V B L S 0 J B F J 5 r L 7 O X 1 b L N k A Q K J f e K a J t A a t P S I B + g I n P H o w 7S B E 0 D A j M e o t 4 M V j B L N A J o a e z p u k S c 9 C 9 5 t g G P I T X v w V L x 4 U 8 S p 4 8 2 / s L I c Y L W g o q t 7 j d Z U b C a 7 Q s r 6 N 1 N L y y u p a e j 2 z s b m 1 v Z P d 3 a u p M J Y M q i w U o W y 4 V I H g A V S R o 4 B G J I H 6 r o C 6 2 7 8 e + / U B S M X D 4 A 6 H E b R 8 2 g 2 4 x x l F L b W z J 5 W 8 g / C A y8 y V i 7 M 6 0 u S A H J I 8 s c k F K Z M b U i F V w s g j e S a v 5 M 1 4 M l 6 M d + N j O p o y Z j v 7 5 B e M z x + kA PROMPTS EXAMPLE OF A RISKY CHOICE DESCRIBED IN TEXTSystem:You are participating in a gambling game where you will be shown two options, Gamble A and Gamble B. Your task is to choose between the two.You must select one option.User:Gamble A offers a 10% chance to win $10 and a 90% chance to lose $12.Gamble B offers a 40% chance to lose $13 and a 60% chance to win $22.Please limit your answer to either 'A' or 'B'.EXAMPLE OF AN INTERTEMPORAL CHOICE DESCRIBED IN TEXTSystem: You are participating in an intertemporal choice experiment.You will be presented with two options, A and B, and your task is to choose between them.You must select one option.User:Option A offers $80 after 30 days.Option B offers $13 immediately.Please limit your answer to either 'A' or 'B'.B ROBUSTNESS CHECKWe supplement our main results with a robustness check using 10-fold cross-validation.Each dataset of human choices was randomly partitioned into a 90% training set and a 10% validation set.For the gershman20 dataset, a stratified train/validation split was employed due to the certainty equivalence design of the experiment (Gershman &amp; Bhui, 2020), ensuring that choices made within the same problem were grouped together.Embeddings from Arithmetic-GPT models and LLaMA-3-70B-Instruct were fitted using logistic regression with a LASSO penalty on the training set.MLPs were fitted to task features in the training set.Performance for all models was assessed on the validation set, with R 2 calculated as the squared Pearson's r.This process was repeated 10 times, each with a random train/validation split.The mean and standard error (SE) of R 2 are summarized in TableA1, demonstrating a replication of our main results presented in Table3.Finally, we conducted a model comparison using the entire choices13k dataset, including the decision-from-feedback trials (see TableA2).We observe that LLaMA3 can outperform MLP on the agrawal23 dataset.While the two neural network models use different task formats as inputs, which may contribute to the differences in performance, it is also possible that the dataset lacks sufficient statistical power to reliably distinguish between the two models.
Scaling up psychology via scientific regret minimization. Mayank Agrawal, Joshua C Peterson, Thomas L Griffiths, Proceedings of the National Academy of Sciences. 117162020</p>
<p>Stress, intertemporal choice, and mitigation behavior during the COVID-19 pandemic. Mayank Agrawal, Joshua C Peterson, Jonathan D Cohen, Thomas L Griffiths, Journal of Experimental Psychology: General. 152926952023</p>
<p>Turning large language models into cognitive models. Marcel Binz, Eric Schulz, arXiv:2306.039172023aarXiv preprint</p>
<p>Using cognitive psychology to understand GPT-3. Marcel Binz, Eric Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023b</p>
<p>Cognitive model priors for predicting human decisions. Joshua C David D Bourgin, Daniel Peterson, Stuart J Reichman, Thomas L Russell, Griffiths, International Conference on Machine Learning. PMLR2019</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in Neural Information Processing Systems. 202033</p>
<p>Data distributional properties drive emergent in-context learning in transformers. Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James Mcclelland, Felix Hill, Advances in Neural Information Processing Systems. 202235</p>
<p>Hierarchical Bayesian modeling of intertemporal choice. Melisa E Chávez, Elena Villalobos, José L Baroja, Arturo Bouzas, Judgment and Decision Making. 1212017</p>
<p>The emergence of economic rationality of gpt. Yiting Chen, Tracy Xiao Liu, You Shan, Songfa Zhong, Proceedings of the National Academy of Sciences. 12051e23162051202023</p>
<p>Julian Coda-Forno, Marcel Binz, Jane X Wang, Eric Schulz, arXiv:2402.18225Cogbench: a large language model walks into a psychology lab. 2024arXiv preprint</p>
<p>Language models show human-like content effects on reasoning. Ishita Dasgupta, Stephanie Cy Andrew K Lampinen, Antonia Chan, Dharshan Creswell, James L Kumaran, Felix Mcclelland, Hill, arXiv:2207.070512022arXiv preprint</p>
<p>Complexity and hyperbolic discounting. Benjamin Enke, Thomas Graeber, Ryan Oprea, 2023CESifo Working Paper</p>
<p>From anomalies to forecasts: Toward a descriptive model of decisions under risk, under ambiguity, and from experience. Ido Erev, Eyal Ert, Ori Plonsky, Doron Cohen, Oded Cohen, Psychological review. 12443692017</p>
<p>Bridging the data gap between children and large language models. Frank Michael, Trends in Cognitive Sciences. 2023a</p>
<p>Large language models as models of human cognition. Frank Michael, PsyArXiv. 2023b</p>
<p>Rationally inattentive intertemporal choice. J Samuel, Rahul Gershman, Bhui, Nature Communications. 11133652020</p>
<p>Computational rationality: A converging paradigm for intelligence in brains, minds, and machines. Eric J Samuel J Gershman, Joshua B Horvitz, Tenenbaum, Gerd Gigerenzer and Wolfgang Gaissmaier. 2015. 2011349Science</p>
<p>Jian-Qiao Thomas L Griffiths, Erin Zhu, Thomas Grant, Mccoy, arXiv:2311.10206Bayes in the age of intelligent machines. 2023arXiv preprint</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in chatgpt. Thilo Hagendorff, Sarah Fabi, Michal Kosinski, Nature Computational Science. 3102023</p>
<p>Large language models as simulated economic agents: What can we learn from homo silicus?. John J Horton, 2023National Bureau of Economic ResearchTechnical report</p>
<p>Unsupervised learning via meta-learning. Kyle Hsu, Sergey Levine, Chelsea Finn, arXiv:1810.023342018arXiv preprint</p>
<p>Investigating data contamination for pre-training language models. Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, Sanmi Koyejo, arXiv:2401.060592024arXiv preprint</p>
<p>Prospect theory: An analysis of decision under risk. Daniel Kahneman, Amos Tversky, Handbook of the fundamentals of financial decision making: Part I. World Scientific2013</p>
<p>Golden eggs and hyperbolic discounting. David Laibson, The Quarterly Journal of Economics. 11221997</p>
<p>Human-like systematic generalization through a meta-learning neural network. M Brenden, Marco Lake, Baroni, Nature. 62379852023</p>
<p>Teaching arithmetic to small transformers. Nayoung Lee, Kartik Sreenivasan, Jason D Lee, Kangwook Lee, Dimitris Papailiopoulos, arXiv:2307.033812023arXiv preprint</p>
<p>Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources. Falk Lieder, Thomas L Griffiths, Behavioral and Brain Sciences. 43e12020</p>
<p>Automated social science: Language models as scientist and subjects. Kehang Benjamin S Manning, John J Zhu, Horton, 2024National Bureau of Economic ResearchTechnical report</p>
<p>Large language models predict human sensory judgments across six modalities. Raja Marjieh, Ilia Sucholutsky, Nori Pol Van Rijn, Thomas L Jacoby, Griffiths, arXiv:2302.013082023arXiv preprint</p>
<p>Modeling rapid language learning by distilling bayesian priors into artificial neural networks. Thomas Mccoy, Thomas L Griffiths, arXiv:2305.147012023arXiv preprint</p>
<p>Present-biased preferences and credit card borrowing. Stephan Meier, Charles Sprenger, American Economic Journal: Applied Economics. 212010</p>
<p>Artificial intelligence and illusions of understanding in scientific research. Lisa Messeri, Crockett, Nature. 62780022024</p>
<p>Investigating the limitations of transformers with simple arithmetic tasks. Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin, arXiv:2102.130192021arXiv preprint</p>
<p>Delay discounting: I'm a k, you're a k. Amy L Odum, Journal of the Experimental Analysis of Behavior. 9632011</p>
<p>Using large-scale experiments and machine learning to discover theories of human decisionmaking. David D Joshua C Peterson, Mayank Bourgin, Daniel Agrawal, Thomas L Reichman, Griffiths, Science. 37265472021</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362024</p>
<p>Probing the psychology of AI models. Richard Shiffrin, Melanie Mitchell, Proceedings of the National Academy of Sciences. 12010e23009631202023</p>
<p>Models of bounded rationality: Empirically grounded economic reason. Herbert Alexander, Simon , 1997MIT press3</p>
<p>Decision by sampling. Neil Stewart, Nick Chater, Gordon Da Brown, Cognitive Psychology. 5312006</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Advances in prospect theory: Cumulative representation of uncertainty. Amos Tversky, Daniel Kahneman, Journal of Risk and Uncertainty. 51992</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Theory of Games and Economic Behavior. Oskar John Von Neumann, Morgenstern, 1944Princeton University PressPrinceton, NJ</p>
<p>Emergent analogical reasoning in large language models. Taylor Webb, Keith J Holyoak, Hongjing Lu, Nature Human Behaviour. 792023</p>
<p>Curvature of the probability weighting function. George Wu, Richard Gonzalez, Management science. 42121996</p>
<p>How well do large language models perform in arithmetic tasks?. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, arXiv:2304.020152023arXiv preprint</p>
<p>Incoherent probability judgments in large language models. Jian-Qiao Zhu, Thomas L Griffiths, arXiv:2401.166462024arXiv preprint</p>
<p>The Bayesian sampler: Generic Bayesian inference causes incoherence in human probability judgments. Jian-Qiao Zhu, Adam N Sanborn, Nick Chater, Psychological Review. 12757192020</p>
<p>Nisan Daniel M Ziegler, Jeffrey Stiennon, Tom B Wu, Alec Brown, Dario Radford, Paul Amodei, Geoffrey Christiano, Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
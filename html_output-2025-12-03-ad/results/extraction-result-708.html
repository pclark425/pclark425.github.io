<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-708 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-708</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-708</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-11b6d1fee0f47a8f9f892ab0d86f370c449097aa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/11b6d1fee0f47a8f9f892ab0d86f370c449097aa" target="_blank">FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> An automatic question answering (QA) based metric for faithfulness, FEQA, is proposed, which leverages recent advances in reading comprehension and has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.</p>
                <p><strong>Paper Abstract:</strong> Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e708.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e708.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>reference-source mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reference summary vs. source document mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Instances where the reference (natural-language) summary contains facts or wording not present in the source document, producing a misalignment between the natural-language description (reference) and the source text used for evaluation or training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Summarization datasets / evaluation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>News summarization evaluation using CNN/DailyMail and XSum datasets where generated summaries, model outputs, and reference summaries are compared against source documents and human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>reference summary (dataset metadata / human-written highlights)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>data preprocessing / evaluation pipeline (dataset assembly and automatic scoring scripts)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete/incorrect natural-language reference (dataset noise)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Reference summaries sometimes contain information that is not present in the source document (e.g., additional world knowledge or facts introduced during reference creation), causing a mismatch between the natural-language reference and the source used to evaluate faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>dataset content (reference summaries) / evaluation data preparation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>manual inspection and documented examples (Section D.4 and Table 13); analysis of specific cases where reference includes facts absent from source</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>qualitative examples and case analysis; no global numeric rate provided in paper (specific examples documented in Appendix D.4, Table 13)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Causes false signals during evaluation: a model could be penalized or given misleading scores because the reference contains facts not in the source; undermines faithfulness evaluation and reproducibility of model comparisons (examples shown but no aggregate quantification).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in the examined datasets (CNN/DM and XSum) and noted as 'noise' in crawled datasets; paper provides individual examples but no global prevalence statistic.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Dataset construction process (automatically crawled corpora), references written with external/world knowledge, and insufficient dataset cleaning/documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Manual dataset cleaning and verification; prefer evaluating output against the original source for faithfulness (the paper advocates comparing output vs. source rather than reference), and making data and code available for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not empirically quantified in the paper; mitigation is recommended (human verification / dataset curation) but no measured results.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / summarization / dataset curation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e708.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e708.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>metric-misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation metric mismatch: content-selection metrics vs. faithfulness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The common natural-language evaluation metrics (ROUGE, BLEU, BERTScore) measure content overlap/selection but do not reliably measure faithfulness (consistency with source), especially for highly abstractive summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Summarization evaluation metrics and pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automated scoring of summary sentences against source documents using word-overlap, embedding similarity, relation extraction, entailment, and the proposed QA-based FEQA metric.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>paper descriptions of evaluation objectives and metric definitions (evaluation protocol described in Methods/Section 4)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation scripts computing ROUGE, BERTScore, relation-extraction precision, entailment predictions, and FEQA scores</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous/incomplete specification of evaluation objective (content-selection conflated with faithfulness)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Standard evaluation descriptions and widespread use of content-overlap metrics implicitly assume overlap measures adequacy/faithfulness; in practice these metrics often fail to detect unfaithful content when summaries are abstractive, creating a misalignment between the intended evaluation goal (faithfulness) and implemented metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation metrics / scoring stage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical correlation analysis: compute Pearson and Spearman correlations between each metric and human-annotated faithfulness scores (Table 7 and Table 8).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Statistical correlation (Pearson and Spearman) between metric scores and human faithfulness annotations. Example results: ROUGE variants show modest positive correlation on CNN/DM (e.g., R-1 Pearson ≈ 12.02) but near-zero or negative correlation on XSum; FEQA obtains highest correlations (e.g., FEQA Pearson 32.01 on CNN/DM and 26.31 on XSum).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Using content-selection metrics can give a false sense of correctness for abstractive summaries and fail to detect many unfaithful outputs; demonstrated by low or negative correlations with human faithfulness on XSum—leading to incorrect model rankings and misguided development choices.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widespread in summarization research and automatic evaluation pipelines; in the paper all standard overlap-based metrics show degraded correlation on the abstractive dataset (XSum).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Evaluation design and community conventions emphasize n-gram overlap with references; lack of task-specific metrics focused on faithfulness; metrics rely on surface-form similarity rather than deeper semantic verification.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Introduce QA-based faithfulness metric (FEQA) that generates question-answer pairs from summaries and verifies answers in the source document; separate measurements for content selection and faithfulness rather than a single unified score.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>FEQA shows substantially higher correlation with human faithfulness judgments (e.g., FEQA Pearson 32.01 vs. ROUGE-1 12.02 on CNN/DM; FEQA Pearson 26.31 vs. ROUGE variants negative on XSum), indicating improved alignment with the faithfulness objective.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / evaluation metrics</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e708.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e708.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>entailment-lexical-heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entailment and embedding metrics over-reliance on lexical overlap</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Off-the-shelf entailment models and embedding-similarity metrics (e.g., ESIM entailment, BERTScore) often rely on surface lexical overlap heuristics and thus miss semantic mismatches, failing to flag unfaithful summaries that share words or concepts with the source.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Model-based evaluation (entailment, embedding similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applying pretrained entailment (ESIM) and embedding similarity (BERTScore) models to judge whether a summary sentence is consistent with the source document.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>pretrained model descriptions / model documentation referenced in Methods</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>off-the-shelf model inference scripts (entailment classifier, BERTScore computation)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithmic behavior than assumed (heuristic reliance on lexical overlap rather than true inference)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Entailment and embedding-based models sometimes produce high similarity/entailment scores for unfaithful outputs because they default to lexical overlap cues; as a result they miss contradictions or hallucinations that change meaning despite overlapping lexical content.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation model behavior / scoring logic</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Counterexample analysis and low correlation statistics: the paper shows examples where Entailment and BERTScore assign high scores to unfaithful outputs (Table 6) and reports low correlation coefficients for ENT (e.g., ENT Pearson 2.80 on CNN/DM, negative on XSum).</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Qualitative examples plus Pearson/Spearman correlations between metric scores and human faithfulness labels; direct inspection of metric outputs on misclassified examples.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Leads to false negatives where semantic errors are missed by automated evaluation, undermining trust in automated correctness checks and potentially allowing models that produce semantic errors to be favored.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across the tested datasets and metrics; ENT and BERTScore had significantly lower correlation with faithfulness, especially on abstractive XSum data.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Models trained on datasets or objectives that allow shortcut heuristics (lexical overlap) and lack capacity for deeper inference; insufficiently diverse entailment training data causing reliance on surface cues.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use QA-based verification (FEQA) that compares question-answer pairs between summary and source to force more semantic checking; manual inspection and targeted adversarial examples for model improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>FEQA outperforms entailment and embedding metrics in correlation with human faithfulness (numeric improvements in Table 7); however the paper notes FEQA remains limited by QA model quality.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e708.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e708.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>qa-model-limits</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limitations of QA models for verification (answer mismatch & long-document errors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Errors arising from the QA models used in FEQA: inability to detect some unanswerable questions, producing 'unanswerable' when answer exists, and penalization due to answer surface-form mismatch, especially for long articles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FEQA (QA-based faithfulness metric) evaluation component</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>FEQA generates questions from masked spans of the summary and uses pretrained reading-comprehension QA models (SQuAD-fine-tuned BERT variants) to extract answers from the source document; matches are scored (F1) against summary-derived 'gold' answers.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>QA model documentation / evaluation protocol (paper's methods describing QA model use)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>QA model inference code (BERT fine-tuned on SQuAD-1.1 / SQuAD-2.0)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>implementation/model limitation causing verification mismatches</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The QA model sometimes fails to correctly determine when a question is unanswerable, returns incorrect answers on long articles, or produces answers that are semantically correct but surface-different (leading to F1 penalties), causing FEQA scores to deviate from human judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>answer extraction stage (QA model inference) / evaluation scoring (F1)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Manual error analysis of sampled QA pairs (100 random QA pairs) and analysis of QA behavior on long articles (Section 4 'Analysis and limitations of QA-based evaluation').</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Manual annotation statistics: ~94% of generated questions grammatical; QA system had 'correct behavior' in ~78% of cases; errors: QA failed to detect unanswerable or incorrectly produced unanswerable in ~14% of cases; F1 penalization due to surface mismatch in ~16% of cases.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Limits FEQA reliability: some faithful/unfaithful sentences are mis-scored because of QA faults; overall FEQA still outperforms baselines but remains constrained by QA quality, especially on longer and more abstractive inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>In the manual sample, QA-system correct behavior observed in 78% of cases; remaining ~22% show the documented issues. Performance degrades with article length and abstraction level.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>QA model limitations: dataset-specific training (SQuAD), inability to handle diverse paraphrases or long-document context, and sensitivity of surface-form F1 scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use stronger/more robust QA models (including models fine-tuned on more diverse QA corpora), allow generative QA models, design more tolerant matching/scoring (semantic match rather than surface F1), and human-in-the-loop verification for critical evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Partially effective: using SQuAD-2.0-capable models helps detect unanswerable questions, but no full quantitative remediation is reported; paper acknowledges QA quality is a limiting factor for FEQA.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / reading comprehension / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e708.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e708.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>annotation-ambiguity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human annotation ambiguity and low inter-annotator agreement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Disagreements and low inter-annotator agreement when labeling faithfulness (and grammaticality), indicating misalignment between annotation guidelines (natural-language instructions) and annotator judgments in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Human annotation pipeline for faithfulness and grammaticality</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Crowdsourced hierarchical annotation procedure (meaningful/nonsensical, grammaticality, then faithfulness with error-type labels) used to create gold labels for model evaluation and metric correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>annotation guidelines / task instructions (hierarchical schema provided to workers)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>annotation task interface / aggregation scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous natural-language instructions and subjective interpretation by annotators</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Inter-annotator agreement on faithfulness is relatively low, particularly for highly abstractive summaries; workers interpret 'faithfulness' and edge cases differently despite provided hierarchical instructions, leading to noisy labels.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data labeling / human-in-the-loop evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Reporting of annotation agreement statistics (Table 4) and a pilot study noting workers' disagreement; hierarchical guidelines introduced to standardize annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Agreement percentage: Table 4 reports agreement (percentage of workers selecting majority class) and scores. Examples: faithfulness agreement for PGC on CNN/DM = 77.28%, FASTRL 77.45%, BOTTOMUP 76.04%; on XSum agreement for PGC = 71.63%, TCONV = 69.90%, BERTSUM = 70.00%.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Reduces reliability of human labels used as gold standards for metric correlation and model evaluation; complicates measurement of faithfulness and lowers confidence in automatic metric tuning and benchmarking.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Lower agreement is especially common on highly abstractive datasets (XSum) and for abstractive model outputs; majority-of-workers thresholds used (e.g., 4/5) but substantial disagreement remains.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Intrinsic difficulty of judging faithfulness for abstractive paraphrases, ambiguous examples, and subjective interpretation; possibly insufficiently precise guidelines or annotator training.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Design hierarchical annotation protocol to separate nonsensical/meaningful/grammatical and then faithfulness judgments; sample multiple annotators and use majority voting and stricter thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Improved standardization relative to naive tasks, but agreement remains imperfect (see Table 4); no complete elimination of disagreement is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / human annotation / evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e708.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e708.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>abstractiveness-faithfulness-tradeoff</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstractiveness–Faithfulness trade-off (intended abstractive behavior vs. actual model outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed phenomenon where increased abstractive behavior (less surface overlap with source) correlates with increased unfaithful content in generated summaries, indicating a gap between the natural-language objective (abstractive, concise, accurate summaries) and model implementations that produce faithful abstractive output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Summarization models and training datasets</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Comparative analysis of multiple sequence-to-sequence summarization models (PGC, FASTRL, BottomUp, TCONV, BERTSUM) trained on CNN/DM and XSum datasets, analyzed for abstractiveness and faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research objective/functional specification in paper (desired summaries to be abstractive yet faithful)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>model implementations (seq2seq architectures with/without copy mechanisms, training scripts)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>performance/behavioral gap between objective (abstractiveness with faithfulness) and implemented models' outputs</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Models that are more abstractive (fusing content, novel n-grams) tend to produce more unfaithful sentences (hallucination, incorrect concatenation), indicating that current implementations lack inductive bias or supervision to maintain faithfulness when abstracting.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model architecture and training / generation decoding behavior</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Empirical analysis: label sentences by extraction/fusion type (Table 3) and collect human faithfulness annotations (Table 4); compare abstractiveness metrics (novel n-grams, fusion rates) against faithfulness scores.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Quantitative metrics: abstractiveness breakdown (sentence/span/word extraction, fusion k≥2, novel n-gram percentages) and human-annotated faithfulness percentages. Example figures: prior work cited ~30% of generated summaries contain unfaithful info; in this paper PGC on XSum faithfulness score ≈ 40.33% (i.e., ~60% unfaithful), while models on CNN/DM are much more faithful.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Limits the practical utility of abstractive summarization systems in real applications because higher abstraction increases semantic errors; influences model selection and evaluation and motivates new research directions.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Consistently observed across models and datasets; more pronounced on highly abstractive datasets (XSum) and models trained thereon; Table 3 and Table 4 quantify the pattern across systems.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Lack of inductive bias or targeted supervision in model architectures to ensure factual consistency during fusion and paraphrase generation; training data properties (XSum references are highly abstractive) also influence model behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Propose new inductive biases or additional supervision to encourage faithful abstraction (e.g., constrained copying, improved training objectives); use QA-based evaluation (FEQA) during development to detect faithfulness errors.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Not fully resolved in the paper; FEQA helps detect faithfulness errors for development but architectural/learning fixes are left for future work; no quantitative improvement of faithfulness via mitigation is reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>natural language processing / summarization / model development</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization', 'publication_date_yy_mm': '2020-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. <em>(Rating: 2)</em></li>
                <li>Analyzing sentence fusion in abstractive summarization <em>(Rating: 2)</em></li>
                <li>Neural text summarization: A critical evaluation <em>(Rating: 2)</em></li>
                <li>Question answering as an automatic evaluation metric for news article summarization <em>(Rating: 2)</em></li>
                <li>Answers unite! unsupervised metrics for reinforced summarization models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-708",
    "paper_id": "paper-11b6d1fee0f47a8f9f892ab0d86f370c449097aa",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "reference-source mismatch",
            "name_full": "Reference summary vs. source document mismatch",
            "brief_description": "Instances where the reference (natural-language) summary contains facts or wording not present in the source document, producing a misalignment between the natural-language description (reference) and the source text used for evaluation or training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Summarization datasets / evaluation pipeline",
            "system_description": "News summarization evaluation using CNN/DailyMail and XSum datasets where generated summaries, model outputs, and reference summaries are compared against source documents and human judgments.",
            "nl_description_type": "reference summary (dataset metadata / human-written highlights)",
            "code_implementation_type": "data preprocessing / evaluation pipeline (dataset assembly and automatic scoring scripts)",
            "gap_type": "incomplete/incorrect natural-language reference (dataset noise)",
            "gap_description": "Reference summaries sometimes contain information that is not present in the source document (e.g., additional world knowledge or facts introduced during reference creation), causing a mismatch between the natural-language reference and the source used to evaluate faithfulness.",
            "gap_location": "dataset content (reference summaries) / evaluation data preparation",
            "detection_method": "manual inspection and documented examples (Section D.4 and Table 13); analysis of specific cases where reference includes facts absent from source",
            "measurement_method": "qualitative examples and case analysis; no global numeric rate provided in paper (specific examples documented in Appendix D.4, Table 13)",
            "impact_on_results": "Causes false signals during evaluation: a model could be penalized or given misleading scores because the reference contains facts not in the source; undermines faithfulness evaluation and reproducibility of model comparisons (examples shown but no aggregate quantification).",
            "frequency_or_prevalence": "Observed in the examined datasets (CNN/DM and XSum) and noted as 'noise' in crawled datasets; paper provides individual examples but no global prevalence statistic.",
            "root_cause": "Dataset construction process (automatically crawled corpora), references written with external/world knowledge, and insufficient dataset cleaning/documentation.",
            "mitigation_approach": "Manual dataset cleaning and verification; prefer evaluating output against the original source for faithfulness (the paper advocates comparing output vs. source rather than reference), and making data and code available for reproducibility.",
            "mitigation_effectiveness": "Not empirically quantified in the paper; mitigation is recommended (human verification / dataset curation) but no measured results.",
            "domain_or_field": "natural language processing / summarization / dataset curation",
            "reproducibility_impact": true,
            "uuid": "e708.0",
            "source_info": {
                "paper_title": "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "metric-misalignment",
            "name_full": "Evaluation metric mismatch: content-selection metrics vs. faithfulness",
            "brief_description": "The common natural-language evaluation metrics (ROUGE, BLEU, BERTScore) measure content overlap/selection but do not reliably measure faithfulness (consistency with source), especially for highly abstractive summaries.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Summarization evaluation metrics and pipeline",
            "system_description": "Automated scoring of summary sentences against source documents using word-overlap, embedding similarity, relation extraction, entailment, and the proposed QA-based FEQA metric.",
            "nl_description_type": "paper descriptions of evaluation objectives and metric definitions (evaluation protocol described in Methods/Section 4)",
            "code_implementation_type": "evaluation scripts computing ROUGE, BERTScore, relation-extraction precision, entailment predictions, and FEQA scores",
            "gap_type": "ambiguous/incomplete specification of evaluation objective (content-selection conflated with faithfulness)",
            "gap_description": "Standard evaluation descriptions and widespread use of content-overlap metrics implicitly assume overlap measures adequacy/faithfulness; in practice these metrics often fail to detect unfaithful content when summaries are abstractive, creating a misalignment between the intended evaluation goal (faithfulness) and implemented metrics.",
            "gap_location": "evaluation metrics / scoring stage",
            "detection_method": "Empirical correlation analysis: compute Pearson and Spearman correlations between each metric and human-annotated faithfulness scores (Table 7 and Table 8).",
            "measurement_method": "Statistical correlation (Pearson and Spearman) between metric scores and human faithfulness annotations. Example results: ROUGE variants show modest positive correlation on CNN/DM (e.g., R-1 Pearson ≈ 12.02) but near-zero or negative correlation on XSum; FEQA obtains highest correlations (e.g., FEQA Pearson 32.01 on CNN/DM and 26.31 on XSum).",
            "impact_on_results": "Using content-selection metrics can give a false sense of correctness for abstractive summaries and fail to detect many unfaithful outputs; demonstrated by low or negative correlations with human faithfulness on XSum—leading to incorrect model rankings and misguided development choices.",
            "frequency_or_prevalence": "Widespread in summarization research and automatic evaluation pipelines; in the paper all standard overlap-based metrics show degraded correlation on the abstractive dataset (XSum).",
            "root_cause": "Evaluation design and community conventions emphasize n-gram overlap with references; lack of task-specific metrics focused on faithfulness; metrics rely on surface-form similarity rather than deeper semantic verification.",
            "mitigation_approach": "Introduce QA-based faithfulness metric (FEQA) that generates question-answer pairs from summaries and verifies answers in the source document; separate measurements for content selection and faithfulness rather than a single unified score.",
            "mitigation_effectiveness": "FEQA shows substantially higher correlation with human faithfulness judgments (e.g., FEQA Pearson 32.01 vs. ROUGE-1 12.02 on CNN/DM; FEQA Pearson 26.31 vs. ROUGE variants negative on XSum), indicating improved alignment with the faithfulness objective.",
            "domain_or_field": "natural language processing / evaluation metrics",
            "reproducibility_impact": true,
            "uuid": "e708.1",
            "source_info": {
                "paper_title": "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "entailment-lexical-heuristic",
            "name_full": "Entailment and embedding metrics over-reliance on lexical overlap",
            "brief_description": "Off-the-shelf entailment models and embedding-similarity metrics (e.g., ESIM entailment, BERTScore) often rely on surface lexical overlap heuristics and thus miss semantic mismatches, failing to flag unfaithful summaries that share words or concepts with the source.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Model-based evaluation (entailment, embedding similarity)",
            "system_description": "Applying pretrained entailment (ESIM) and embedding similarity (BERTScore) models to judge whether a summary sentence is consistent with the source document.",
            "nl_description_type": "pretrained model descriptions / model documentation referenced in Methods",
            "code_implementation_type": "off-the-shelf model inference scripts (entailment classifier, BERTScore computation)",
            "gap_type": "different algorithmic behavior than assumed (heuristic reliance on lexical overlap rather than true inference)",
            "gap_description": "Entailment and embedding-based models sometimes produce high similarity/entailment scores for unfaithful outputs because they default to lexical overlap cues; as a result they miss contradictions or hallucinations that change meaning despite overlapping lexical content.",
            "gap_location": "evaluation model behavior / scoring logic",
            "detection_method": "Counterexample analysis and low correlation statistics: the paper shows examples where Entailment and BERTScore assign high scores to unfaithful outputs (Table 6) and reports low correlation coefficients for ENT (e.g., ENT Pearson 2.80 on CNN/DM, negative on XSum).",
            "measurement_method": "Qualitative examples plus Pearson/Spearman correlations between metric scores and human faithfulness labels; direct inspection of metric outputs on misclassified examples.",
            "impact_on_results": "Leads to false negatives where semantic errors are missed by automated evaluation, undermining trust in automated correctness checks and potentially allowing models that produce semantic errors to be favored.",
            "frequency_or_prevalence": "Observed across the tested datasets and metrics; ENT and BERTScore had significantly lower correlation with faithfulness, especially on abstractive XSum data.",
            "root_cause": "Models trained on datasets or objectives that allow shortcut heuristics (lexical overlap) and lack capacity for deeper inference; insufficiently diverse entailment training data causing reliance on surface cues.",
            "mitigation_approach": "Use QA-based verification (FEQA) that compares question-answer pairs between summary and source to force more semantic checking; manual inspection and targeted adversarial examples for model improvement.",
            "mitigation_effectiveness": "FEQA outperforms entailment and embedding metrics in correlation with human faithfulness (numeric improvements in Table 7); however the paper notes FEQA remains limited by QA model quality.",
            "domain_or_field": "natural language processing / evaluation",
            "reproducibility_impact": true,
            "uuid": "e708.2",
            "source_info": {
                "paper_title": "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "qa-model-limits",
            "name_full": "Limitations of QA models for verification (answer mismatch & long-document errors)",
            "brief_description": "Errors arising from the QA models used in FEQA: inability to detect some unanswerable questions, producing 'unanswerable' when answer exists, and penalization due to answer surface-form mismatch, especially for long articles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "FEQA (QA-based faithfulness metric) evaluation component",
            "system_description": "FEQA generates questions from masked spans of the summary and uses pretrained reading-comprehension QA models (SQuAD-fine-tuned BERT variants) to extract answers from the source document; matches are scored (F1) against summary-derived 'gold' answers.",
            "nl_description_type": "QA model documentation / evaluation protocol (paper's methods describing QA model use)",
            "code_implementation_type": "QA model inference code (BERT fine-tuned on SQuAD-1.1 / SQuAD-2.0)",
            "gap_type": "implementation/model limitation causing verification mismatches",
            "gap_description": "The QA model sometimes fails to correctly determine when a question is unanswerable, returns incorrect answers on long articles, or produces answers that are semantically correct but surface-different (leading to F1 penalties), causing FEQA scores to deviate from human judgment.",
            "gap_location": "answer extraction stage (QA model inference) / evaluation scoring (F1)",
            "detection_method": "Manual error analysis of sampled QA pairs (100 random QA pairs) and analysis of QA behavior on long articles (Section 4 'Analysis and limitations of QA-based evaluation').",
            "measurement_method": "Manual annotation statistics: ~94% of generated questions grammatical; QA system had 'correct behavior' in ~78% of cases; errors: QA failed to detect unanswerable or incorrectly produced unanswerable in ~14% of cases; F1 penalization due to surface mismatch in ~16% of cases.",
            "impact_on_results": "Limits FEQA reliability: some faithful/unfaithful sentences are mis-scored because of QA faults; overall FEQA still outperforms baselines but remains constrained by QA quality, especially on longer and more abstractive inputs.",
            "frequency_or_prevalence": "In the manual sample, QA-system correct behavior observed in 78% of cases; remaining ~22% show the documented issues. Performance degrades with article length and abstraction level.",
            "root_cause": "QA model limitations: dataset-specific training (SQuAD), inability to handle diverse paraphrases or long-document context, and sensitivity of surface-form F1 scoring.",
            "mitigation_approach": "Use stronger/more robust QA models (including models fine-tuned on more diverse QA corpora), allow generative QA models, design more tolerant matching/scoring (semantic match rather than surface F1), and human-in-the-loop verification for critical evaluations.",
            "mitigation_effectiveness": "Partially effective: using SQuAD-2.0-capable models helps detect unanswerable questions, but no full quantitative remediation is reported; paper acknowledges QA quality is a limiting factor for FEQA.",
            "domain_or_field": "natural language processing / reading comprehension / evaluation",
            "reproducibility_impact": true,
            "uuid": "e708.3",
            "source_info": {
                "paper_title": "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "annotation-ambiguity",
            "name_full": "Human annotation ambiguity and low inter-annotator agreement",
            "brief_description": "Disagreements and low inter-annotator agreement when labeling faithfulness (and grammaticality), indicating misalignment between annotation guidelines (natural-language instructions) and annotator judgments in practice.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Human annotation pipeline for faithfulness and grammaticality",
            "system_description": "Crowdsourced hierarchical annotation procedure (meaningful/nonsensical, grammaticality, then faithfulness with error-type labels) used to create gold labels for model evaluation and metric correlation.",
            "nl_description_type": "annotation guidelines / task instructions (hierarchical schema provided to workers)",
            "code_implementation_type": "annotation task interface / aggregation scripts",
            "gap_type": "ambiguous natural-language instructions and subjective interpretation by annotators",
            "gap_description": "Inter-annotator agreement on faithfulness is relatively low, particularly for highly abstractive summaries; workers interpret 'faithfulness' and edge cases differently despite provided hierarchical instructions, leading to noisy labels.",
            "gap_location": "data labeling / human-in-the-loop evaluation",
            "detection_method": "Reporting of annotation agreement statistics (Table 4) and a pilot study noting workers' disagreement; hierarchical guidelines introduced to standardize annotations.",
            "measurement_method": "Agreement percentage: Table 4 reports agreement (percentage of workers selecting majority class) and scores. Examples: faithfulness agreement for PGC on CNN/DM = 77.28%, FASTRL 77.45%, BOTTOMUP 76.04%; on XSum agreement for PGC = 71.63%, TCONV = 69.90%, BERTSUM = 70.00%.",
            "impact_on_results": "Reduces reliability of human labels used as gold standards for metric correlation and model evaluation; complicates measurement of faithfulness and lowers confidence in automatic metric tuning and benchmarking.",
            "frequency_or_prevalence": "Lower agreement is especially common on highly abstractive datasets (XSum) and for abstractive model outputs; majority-of-workers thresholds used (e.g., 4/5) but substantial disagreement remains.",
            "root_cause": "Intrinsic difficulty of judging faithfulness for abstractive paraphrases, ambiguous examples, and subjective interpretation; possibly insufficiently precise guidelines or annotator training.",
            "mitigation_approach": "Design hierarchical annotation protocol to separate nonsensical/meaningful/grammatical and then faithfulness judgments; sample multiple annotators and use majority voting and stricter thresholds.",
            "mitigation_effectiveness": "Improved standardization relative to naive tasks, but agreement remains imperfect (see Table 4); no complete elimination of disagreement is reported.",
            "domain_or_field": "natural language processing / human annotation / evaluation",
            "reproducibility_impact": true,
            "uuid": "e708.4",
            "source_info": {
                "paper_title": "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
                "publication_date_yy_mm": "2020-05"
            }
        },
        {
            "name_short": "abstractiveness-faithfulness-tradeoff",
            "name_full": "Abstractiveness–Faithfulness trade-off (intended abstractive behavior vs. actual model outputs)",
            "brief_description": "Observed phenomenon where increased abstractive behavior (less surface overlap with source) correlates with increased unfaithful content in generated summaries, indicating a gap between the natural-language objective (abstractive, concise, accurate summaries) and model implementations that produce faithful abstractive output.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Summarization models and training datasets",
            "system_description": "Comparative analysis of multiple sequence-to-sequence summarization models (PGC, FASTRL, BottomUp, TCONV, BERTSUM) trained on CNN/DM and XSum datasets, analyzed for abstractiveness and faithfulness.",
            "nl_description_type": "research objective/functional specification in paper (desired summaries to be abstractive yet faithful)",
            "code_implementation_type": "model implementations (seq2seq architectures with/without copy mechanisms, training scripts)",
            "gap_type": "performance/behavioral gap between objective (abstractiveness with faithfulness) and implemented models' outputs",
            "gap_description": "Models that are more abstractive (fusing content, novel n-grams) tend to produce more unfaithful sentences (hallucination, incorrect concatenation), indicating that current implementations lack inductive bias or supervision to maintain faithfulness when abstracting.",
            "gap_location": "model architecture and training / generation decoding behavior",
            "detection_method": "Empirical analysis: label sentences by extraction/fusion type (Table 3) and collect human faithfulness annotations (Table 4); compare abstractiveness metrics (novel n-grams, fusion rates) against faithfulness scores.",
            "measurement_method": "Quantitative metrics: abstractiveness breakdown (sentence/span/word extraction, fusion k≥2, novel n-gram percentages) and human-annotated faithfulness percentages. Example figures: prior work cited ~30% of generated summaries contain unfaithful info; in this paper PGC on XSum faithfulness score ≈ 40.33% (i.e., ~60% unfaithful), while models on CNN/DM are much more faithful.",
            "impact_on_results": "Limits the practical utility of abstractive summarization systems in real applications because higher abstraction increases semantic errors; influences model selection and evaluation and motivates new research directions.",
            "frequency_or_prevalence": "Consistently observed across models and datasets; more pronounced on highly abstractive datasets (XSum) and models trained thereon; Table 3 and Table 4 quantify the pattern across systems.",
            "root_cause": "Lack of inductive bias or targeted supervision in model architectures to ensure factual consistency during fusion and paraphrase generation; training data properties (XSum references are highly abstractive) also influence model behavior.",
            "mitigation_approach": "Propose new inductive biases or additional supervision to encourage faithful abstraction (e.g., constrained copying, improved training objectives); use QA-based evaluation (FEQA) during development to detect faithfulness errors.",
            "mitigation_effectiveness": "Not fully resolved in the paper; FEQA helps detect faithfulness errors for development but architectural/learning fixes are left for future work; no quantitative improvement of faithfulness via mitigation is reported here.",
            "domain_or_field": "natural language processing / summarization / model development",
            "reproducibility_impact": true,
            "uuid": "e708.5",
            "source_info": {
                "paper_title": "FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization",
                "publication_date_yy_mm": "2020-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Ranking generated summaries by correctness: An interesting but challenging application for natural language inference.",
            "rating": 2
        },
        {
            "paper_title": "Analyzing sentence fusion in abstractive summarization",
            "rating": 2
        },
        {
            "paper_title": "Neural text summarization: A critical evaluation",
            "rating": 2
        },
        {
            "paper_title": "Question answering as an automatic evaluation metric for news article summarization",
            "rating": 2
        },
        {
            "paper_title": "Answers unite! unsupervised metrics for reinforced summarization models",
            "rating": 1
        }
    ],
    "cost": 0.0162975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>FEQA: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization</h1>
<p>Esin Durmus*<br>Cornell<br>University<br>ed459@cornell.edu</p>
<p>He He<br>New York<br>University<br>hehe@cs.nyu.edu</p>
<p>Mona Diab<br>The George Washington<br>University<br>mtdiab@gwu.edu</p>
<h4>Abstract</h4>
<p>Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, ${ }^{\dagger}$ which leverages recent advances in reading comprehension. Given questionanswer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.</p>
<h2>1 Introduction</h2>
<p>Abstractive summarization models must aggregate salient content from the source document(s) and remain faithful, i.e. being factually consistent with information in the source documents. Neural abstractive models are effective at identifying salient content and producing fluent summaries (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018). However, the generated summary may not always contain faithful information, which is vital for real-world applications.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Source. The world's oldest person has died a few weeks after celebrating her 117th birthday. Born on March 5, 1898, the greatgrandmother had lived through two world wars, the invention of the television and the first successful powered aeroplane flight by the wright brothers...
Output sentence. The world 's oldest person has died on March 5, 1898.</p>
<p>Table 1: An example of unfaithful output (highlighted in red); generated by Gehrmann et al. (2018).</p>
<p>Table 1 shows an example of unfaithful generation. Recent studies have shown that around $30 \%$ of generated summaries contain unfaithful information (Cao et al., 2018; Falke et al., 2019a; Kryściński et al., 2019), especially when the sentence combines content from multiple source sentences (Lebanoff et al., 2019).</p>
<p>In this paper, we address the problem of evaluating faithfulness of generated summaries given their source documents. Our key insight is that current models are limited by a trade-off between abstractiveness and faithfulness (Section 2). On a wide range of systems and two datasets with varying levels of abstractiveness (CNN/DM and XSum), we show that the number of unfaithful sentences (annotated by humans) increases as the summary becomes more abstractive (i.e. less overlap with the source document). Next, we investigate a diverse set of existing automatic evaluation metrics such as ROUGE, BERTScore (Zhang et al., 2019a), and learned entailment models. We find that their correlations with human scores of faithfulness drop significantly on highly abstractive summaries, where deeper text understanding beyond surface similarity is needed.</p>
<p>Recently, question answering (QA) based automatic metrics have been proposed for evaluating</p>
<p>content selection in summarization (Eyal et al., 2019; Scialom et al., 2019; Chen et al., 2018). Specifically, cloze-style QA is used to evaluate whether important information in the source is recovered from the summary. Inspired by prior work, we use automatically generated QA pairs to represent information in the summary and validate it against the source. Concretely, we generate a set of "groundtruth" QA pairs from the summary, using a learned model that converts a declarative sentence and an answer span to a question (Section 3). Then, off-the-shelf reading comprehension models are evaluated on this set by extracting answer spans from the source documents. High accuracy means that the summary and the source document tend to produce the same answers, thus they are factually consistent with respect to the questions. Compared to prior approaches using cloze tests, our question generation approach enables evaluation with a broader range of QA models and answer types (e.g. extractive and generative), thus maximally taking advantage of progress in QA.</p>
<p>Among automatic metrics based on $n$-gram overlap, word embeddings, and language understanding models (relation extraction and entailment), FEQA has significantly higher correlation with human scores of faithfulness and is the only metric that correlates with human scores on highly abstractive summaries from XSum.</p>
<h2>2 The Abstractiveness-Faithfulness Tradeoff</h2>
<p>While extractive summarizers are largely faithful (since they copy sentences from the source document), current abstractive models struggle to produce faithful summaries without copying. Similar to Lebanoff et al. (2019), we observe that factual errors occur more frequently as models generate more abstractive summary sentences, i.e. less overlap with the source document. In this section, we analyze generated summaries along two dimensions: abstractiveness and faithfulness. Specifically, we aim to answer the following questions: (1) How to quantify abstractiveness of a summary? (2) Is abstractiveness encouraged more by the data or the model? (3) How does being abstractive affect faithfulness?</p>
<h3>2.1 Characterizing Abstractiveness of a Summary</h3>
<p>Abstractive summarization involves rephrasing important content into brief statements, ranging from minor editing of a source sentence to condensing multiple sentences in new words. Given a source document and a summary, we want to measure the level of abstractiveness of the summary.</p>
<p>Prior work measures abstractiveness by overlapped text spans between the summary and the document (Grusky et al., 2018; Zhang et al., 2018), or indirectly by the effectiveness of extractive baselines such as LEAD-3 (Nallapati et al., 2016a). While metrics such as extractive fragment coverage and density (Grusky et al., 2018) provide a continuous measure of the level of abstractiveness, we define a more fine-grained categorization of abstractiveness by analyzing how each sentence in the summary is formed.</p>
<p>A more abstractive summary sentence aggregates content over a larger chunk of source text; consequently it must copy fewer words to maintain brevity. Therefore, we define the following abstractiveness types based on the amount of copying, e.g. copying a source sentence, one or more partial fragments from the source sentence, and individual words.</p>
<ol>
<li>Sentence extraction: the summary sentence is exactly the same as one of the source sentences.</li>
<li>Span extraction: the summary sentence is a substring of one of the source sentences, e.g. "the plane was coming back from the NCAA final" is a span extracted from "the plane was coming back from the NCAA final, according to spokesman John Twork".</li>
<li>Word extraction: the summary sentence is formed by a subset of the tokens in a source sentence, e.g. "Capybara Joejoe has almost 60,000 followers" is a result of deleting words in "Capybara Joejoe who lives in Las Vegas has almost 60,000 followers on Instagram".</li>
<li>Perfect fusion ${ }_{k}$ : the summary sentence is constructed by piecing together the substrings from $k(k&gt;1)$ source sentences in their original order, e.g. "Capybara Joejoe has almost 60,000 followers" is a perfect fusion of the sentences "Capybara Joejoe lives</li>
</ol>
<p>in Las vegas." and "He has almost 60,000 followers on Instagram."</p>
<p>To quantify the amount of abstractiveness of a set of summaries, we label each sentence with the first qualified type in the order above if it fits to one of these categories.</p>
<p>We then define the score of each type as the percentage of sentences labeled by that category. The types are ordered by increasing levels of abstractiveness. For example, a summary with higher fusion scores and lower extraction scores is considered more abstractive. In addition, we compute the percentage of novel $n$-grams that do not appear in the source document as another metric for abstractiveness.</p>
<h3>2.2 Is abstractiveness from the model or the data?</h3>
<p>Equipped with the metrics for abstractiveness above, we want to further understand how abstractive the generated summaries are, and whether the amount of abstractiveness is a result of the training data or the model. Therefore, we compute abstractiveness scores for both the reference summaries and summaries generated from a diverse set of models on two datasets.</p>
<p>Datasets. We use the CNN/DailyMail (Hermann et al., 2015; Nallapati et al., 2016b) (CNN/DM) and the XSum (Narayan et al., 2018) datasets, which are both used for single-document news summarization tasks. CNN/DM consists of articles from the CNN and Daily Mail websites, where the summaries comprise highlights in bullet points. XSum consists of BBC articles, where the summaries comprise a single-sentence summary that is written as the opening introductory sentence for the article. XSum was released in particular to promote research on highly abstractive summarization systems. Appendix A provides statistics on CNN/DM and XSum datasets: they contain around 288 k and 204 k training examples, respectively; CNN/DM includes longer documents and summaries on average.</p>
<p>Models. Most neural abstractive summarization models are based on sequence-to-sequence models. They differ in how summarization-specific operations such as copying/extraction are instantiated. We consider 5 prominent models and sum-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Systems</th>
<th style="text-align: left;">Extractor</th>
<th style="text-align: left;">Encoder</th>
<th style="text-align: left;">Decoder</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">PGC</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">LSTM</td>
<td style="text-align: left;">LSTM+copy</td>
</tr>
<tr>
<td style="text-align: left;">FASTRL</td>
<td style="text-align: left;">sentences</td>
<td style="text-align: left;">LSTM</td>
<td style="text-align: left;">LSTM+copy</td>
</tr>
<tr>
<td style="text-align: left;">BOTTOMUP</td>
<td style="text-align: left;">words</td>
<td style="text-align: left;">LSTM</td>
<td style="text-align: left;">LSTM+copy</td>
</tr>
<tr>
<td style="text-align: left;">TCONV</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">CNN+topic</td>
<td style="text-align: left;">CNN</td>
</tr>
<tr>
<td style="text-align: left;">BERTSUM</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">BERT-based</td>
<td style="text-align: left;">Transformer</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of summarization systems in terms of model architecture.
marize their characteristics in Table 2. ${ }^{2}$ Details of each model can be found in Appendix B. PGC (See et al., 2017) uses the copy mechanism during decoding to allow extraction. FASTRL (Chen and Bansal, 2018) and BOTTOMUP (Gehrmann et al., 2018) decouple extraction and abstractive generation by learning to select sentences and words respectively in the first step; this model has been shown to generate more abstractive summaries compared to PGC. TCONV (Narayan et al., 2018) is initially designed for XSum, thus it does not include any explicit copying/extraction components and focuses on long text representation using convolutional neural networks. BERTSUM (Liu and Lapata, 2019) consists of a BERT-based encoder and a 6-layer Transformer decoder. It incorporates extraction implicitly by first fine-tuning the encoder on the extractive summarization task. ${ }^{3}$</p>
<p>Results. Our goal is to understand the level of abstractiveness of summaries generated by different models, and the influence on abstractiveness from the training data. Therefore, we analyzed summaries generated by the above models on CNN/DM and XSum. We computed the metrics described in Section 2.1 for both the generated summaries and the reference summaries on the test sets. The results are shown in Table 3.</p>
<p>First, CNN/DM is more extractive than XSum. Extraction scores of the reference summaries in CNN/DM shows that almost half of the sentences are formed by deleting words in one of the source sentences. This shows that sentence compression (Knight and Marcu, 2002) is the main technique used for this dataset. In contrast, none of the summary sentences in XSum are formed by copying from a single source sentence. They are generated mostly by paraphrasing the input content, indicated by the large fraction of novel $n$-grams.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>| Dataset | Model | Extraction | | | Perfect fusion | | Novel $n$-grams | | | | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | | Sentence | Span | Word | $k=2$ | $k \geq 2$ | $n=1$ | $n=2$ | $n=3$ | | CNN/DM | Ref | 1.39 | 2.14 | 9.27 | 12.92 | 14.87 | 12.40 | 51.03 | 71.22 | | | PGC | 35.45 | 34.18 | 15.45 | 10.90 | 1.61 | 0.62 | 3.33 | 7.42 | | | FASTRL | 8.94 | 40.06 | 39.64 | 4.22 | 0.84 | 0.82 | 10.89 | 20.74 | | | BottomUp | 7.65 | 17.98 | 36.75 | 21.86 | 6.77 | 0.86 | 11.44 | 22.40 | | | BertSum | - | 13.73 | 53.40 | 16.18 | 4.39 | 5.23 | 14.55 | 23.09 | | XSum | Ref | - | - | - | 0.87 | 0.77 | 39.20 | 84.98 | 96.05 | | | PGC | - | - | - | 0.41 | 3.47 | 30.08 | 74.27 | 91.27 | | | TCONV | - | - | - | 0.35 | 2.31 | 34.07 | 80.62 | 95.12 | | | BertSum | - | - | - | 0.33 | 3.15 | 28.93 | 75.85 | 91.41 | | |</p>
<p>Table 3: Abstractiveness measures of the models on CNN/DM and XSum datasets. The numbers for Extraction and Perfect fusion indicate \% of sentences generated with these strategies. Numbers for novel $n$-grams indicate \% of $n$-grams that are present in the output sentence but is not present in the source.</p>
<p>Second, training data has a larger influence on the abstractiveness of model outputs. Similar to Zhang et al. (2018), we find that models trained on CNN/DM are near-extractive. However, the same models trained on XSum are significantly more abstractive. In fact, none of the models produced any sentence that copies words/phrases from a single source sentence, which is consistent with characteristics of the reference summaries in XSum. The content is more often rephrased in novel words/phrases. However, on both datasets, current models struggle to achieve the same level of abstractiveness as the reference summaries, indicating that additional inductive bias is needed to condense multiple sentences by rephrasing.</p>
<p>Third, different models have different ways of doing extraction. When trained on CNN/DM, PGC generates the majority of sentences by copying complete source sentences, whereas FASTRL, BottomUp and BertSum do simple compression by deletion more often. In addition, BotTOMUP does more fusion compared to PGC, FASTRL and BERTSum.</p>
<h3>2.3 Annotating Summary Faithfulness ${ }^{4}$</h3>
<p>To understand faithfulness of current systems and its relation to abstractiveness, we crowd-sourced human annotations on the output of each modeldataset pair described in Section 2.2. Since a nearextractive sentence is very likely to be grammatical and faithful, we focus on more abstractive cases by excluding output sentences that are either an exact copy or a substring of one of the source sentences.</p>
<p>A key challenge to reliable human annotation is that the inter-annotator agreement on faithfulness is relatively low (Lebanoff et al., 2019). Our pi-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>lot study shows that workers often do not agree on incoherent sentences, e.g. whether "Chelsea beat Chelsea $5-3$ in the Premier League on Saturday." is faithful or not. To standardize the annotation process, we design hierarchical questions to distinguish among failed generation that render a sentence meaningless, low-level grammatical errors that hardly affect semantic understanding, and faithfulness errors that convey incorrect (yet meaningful) information.</p>
<p>Figure 1 shows the decision tree of our human annotation steps. We first evaluate the grammaticality of generated sentences (independent from the source document). We show annotators a summary sentence and ask them to choose whether the given sentence is meaningful or nonsensical to determine if the given sentence is structurally and semantically sound. If the annotator can make sense of the sentence, we then ask whether it is grammatical or has minor grammaticality problems which a person can easily correct.</p>
<p>Next, for sentences labeled as meaningful in the first step, we ask workers whether they are faithful to the provided source document. In case the worker labels a sentence as unfaithful, we conduct a simple error analysis by asking them to indicate if the sentence contains information that is absent from or conflicting with the source document, which corresponds to hallucination and contradiction errors, respectively. More details about the annotation schema and guidelines are included in the Appendix C. Next, we describe our human evaluation results.</p>
<h3>2.3.1 Human Annotation Results</h3>
<p>For each dataset-model pair described in Section 2.2, we randomly sampled 1000 sentencesource pairs eliminating output sentences that are either an exact copy or substring of a source sen-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The decision diagram of our human annotation process. Decision nodes are rectangular and outcome nodes are circular. We show the annotation path of two summary sentences, S1 (green arrows) and S2 (red arrows). S2 is annotated as <em>nonsensical</em> thus is not considered for faithfulness. S1 is annotated as unfaithful due to hallucinated content.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Model</th>
<th>Grammaticality</th>
<th></th>
<th></th>
<th>Faithfulness</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Score</td>
<td>Agreement</td>
<td>Abstractiveness</td>
<td>Score</td>
<td>Agreement</td>
<td>Abstractiveness</td>
</tr>
<tr>
<td>CNN/DM</td>
<td>PGC</td>
<td>93.34</td>
<td>94.04</td>
<td>10.05</td>
<td>70.05</td>
<td>77.28</td>
<td>13.35</td>
</tr>
<tr>
<td></td>
<td>FASTRL</td>
<td>83.06</td>
<td>88.05</td>
<td>44.46</td>
<td>68.27</td>
<td>77.45</td>
<td>49.74</td>
</tr>
<tr>
<td></td>
<td>BOTTOMUP</td>
<td>85.83</td>
<td>89.19</td>
<td>29.62</td>
<td>64.17</td>
<td>76.04</td>
<td>42.36</td>
</tr>
<tr>
<td></td>
<td>BERTSUM</td>
<td>97.53</td>
<td>97.65</td>
<td>29.44</td>
<td>95.03</td>
<td>95.14</td>
<td>39.16</td>
</tr>
<tr>
<td>XSum</td>
<td>PGC</td>
<td>65.85</td>
<td>81.03</td>
<td>91.10</td>
<td>40.33</td>
<td>71.63</td>
<td>97.06</td>
</tr>
<tr>
<td></td>
<td>TCONV</td>
<td>70.85</td>
<td>85.03</td>
<td>94.94</td>
<td>38.96</td>
<td>69.90</td>
<td>98.81</td>
</tr>
<tr>
<td></td>
<td>BERTSUM</td>
<td>90.44</td>
<td>91.80</td>
<td>91.50</td>
<td>60.54</td>
<td>70.00</td>
<td>97.60</td>
</tr>
</tbody>
</table>
<p>Table 4: Grammaticality and faithfulness results of human annotations. <strong>Score</strong> is computed by taking the percentage of annotators that selected "meaningful" and "faithful" for grammaticality and faithfulness annotation tasks, respectively, and then averaging these values across all the examples for the given annotation task. <strong>Agreement</strong> is computed by taking the percentage of the workers that annotate the majority class for the given example. <strong>Abstractiveness</strong> is measured by the percentage of novel trigrams in a given sentence.</p>
<p>tence. We collected grammaticality annotations for these sentences from 5 annotators. We consider a sentence meaningful if at least 4 out of 5 annotators label it as meaningful in the first stage. We sampled 200 meaningful sentences randomly to collect annotations for faithfulness. Table 4 shows the results of the grammaticality and faithfulness human evaluations.</p>
<p><strong>Grammaticality.</strong> Overall, outputs from all models are scored high on grammaticality with high inter-annotator agreement. However, on more <em>abstractive</em> summaries (i.e. when trained on XSum), the grammaticality scores drop significantly. One exception is BERTSUM, which maintains good performance on XSum and achieves the highest grammaticality score on both datasets.<sup>5</sup></p>
<p><strong>Faithfulness.</strong> Near-extractive summaries generated from models trained on CNN/DM have significantly higher faithfulness scores than highly abstractive summaries from models trained on XSum. We find that PGC and TCONV has faithfulness errors in more than half of the sentences they generate when trained on XSum. Although BERTSUM generates fewer unfaithful sentences, it still suffers from performance drop on XSum. Interestingly, human agreement on faithfulness is also lower for abstractive summaries from XSum. This suggests that faithfulness errors are harder to catch for humans as well in more abstractive settings. We further observe conflicting information is more common among models trained on CNN/DM while hallucination is more common among models trained on XSum. Table 5 shows examples of meaningful but unfaithful sentences.</p>
<h3>3 FEQA: Faithfulness Evaluation with Question Answering</h3>
<p>Our analysis above shows that the number of unfaithful sentences increases significantly as more abstractive summaries are generated. Thus the key challenge to faithfulness evaluation is to verify highly abstractive sentences against the source document, where surface similarity match-</p>
<p><sup>5</sup>Majority of the sentences (&gt; 70%) identified as "meaningful" are annotated as "perfectly grammatical" for each model-dataset pair.</p>
<table>
<thead>
<tr>
<th>Source</th>
<th>Output Sentence</th>
<th>Domain</th>
<th>Category</th>
</tr>
</thead>
<tbody>
<tr>
<td>...However, Winger Ross Wallace (knee) and right-back Steven Reid (calf) could return for the Barclays premier league contest...</td>
<td>Dean Marney and Steven Reid could return for the Barclays Premier League match.</td>
<td>CNN/DM</td>
<td>IC</td>
</tr>
<tr>
<td>....Odom also played for the US in the 2004 Athens Olympics, winning the bronze medal. His condition is unknown but well-wishers tweeted their support following the news...</td>
<td>NBA basketball player Odom has been found dead in a helicopter crash in the US state of Nevada.</td>
<td>XSum</td>
<td>H</td>
</tr>
</tbody>
</table>
<p>Table 5: Examples of meaningful but unfaithful sentences. Category corresponds to the faithfulness error type for the output sentence. IC: Incorrect Concatenation, H: Hallucination. More examples are provided in Table 11.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of FEQA. Given a summary sentence and its corresponding source document, we first mask important text spans (e.g. noun phrases, entities) in the summary. Then, we consider each span as the "gold" answer and generate its corresponding question using a learned model. Lastly, a QA model finds answers to these questions in the documents; its performance (e.g. F1 score) against the "gold" answers from the summary is taken as the faithfulness score.
ing would fail. If we have a good semantic representation of the sentence abstracting away its surface form (e.g. a list of facts about who did what to whom), we can simply compare the sentence representation to the document representation (e.g. check whether the fact list from the summary is a subset of the list from the document). Ideally, the representation should be domain-general and interpretable for easy error analysis.</p>
<p>Motivated by the fast progress in reading comprehension (Chen, 2018; Gao et al., 2018) we propose to use QA pairs as a generic meaning representation of sentences for faithfulness evaluation. Given a summary sentence, we produce a list of questions asking about key information in the sentence and their corresponding answers. To verify this information against the source, we use a QA model to predict answers from the document. The questions and the QA model thus extract comparable information from two pieces of text. More matched answers from the document implies a more faithful summary since the information addressing these questions are consistent between the summary and the source document. Figure 2 shows the workflow of FEQA.</p>
<p>Question generation. Prior work (Eyal et al., 2019; Scialom et al., 2019) uses cloze tests as questions by masking entities. To go beyond cloze-style QA and leverage more recent extractive (Rajpurkar et al., 2016) or even generative (Alec et al., 2019) QA models, we generate natural language questions from the summary sentence automatically. Specifically, we mask important text spans in a sentence, including noun phrases extracted by a constituency parser (Kitaev and Klein, 2018) and named entities extracted by the Stanford CoreNLP NER model (Finkel et al., 2005; Manning et al., 2014). We consider each span as the gold answer and generate its corresponding question by fine-tuning a pretrained BART language model (Lewis et al., 2019). To train the question generator, we adapt the QA2D dataset Demszky et al. (2018). The input is a declarative sentence with masked answers and the output is a question. A training example might look like:</p>
<p>Input: Sally was born in <m> 1958 </m> Output: When was Sally born ?</p>
<p>Since the transformation from declarative sen-</p>
<p>tences to questions is almost rule-based without much paraphrasing, we expect the model to generalize to various domains.</p>
<p>Answer verification. Given the QA pairs generated from a summary sentence, we run off-theshelf QA models to get answers to these questions from the source document. We then measure the average F1 score against the "gold" answers from the summary, which is our faithfulness score for the given sentence. This step does not have any constraint on the QA model. We experiment with the pretrained BERT-base model (Devlin et al., 2019) fine-tuned on SQuAD-1.1 (Rajpurkar et al., 2016) and SQuAD-2.0 (Rajpurkar et al., 2018). Note that in the case of SQuAD-2.0, the model may be able to hypothesize that a question is unanswerable. This case is equivalent to getting an answer incorrect (i.e. unfaithful).</p>
<h2>4 Experiments</h2>
<p>We aim to understand to what extent the proposed QA-based metric and existing metrics capture faithfulness of a summary. Given pairs of documents and summary sentences without reference summaries, we measure correlations between human-annotated faithfulness scores (Section 2.3) and scores computed using each metric described below.</p>
<h3>4.1 Automated Metrics for Faithfulness</h3>
<p>Word overlap-based metrics. A straightforward metric for faithfulness is the word overlap between the summary sentence and the document. We compute ROUGE (R), BLEU (B), ${ }^{6}$ between the output sentence and each of the source sentences (i.e. taking the source sentence as the reference). We then take the average scores and maximum score across all the source sentences. Since according to our analysis taking the average score consistently has higher correlation, we report only the correlation for the average.</p>
<p>Embedding-based metrics. Word embeddings extend word overlap-based metrics beyond exact match. Recently, BERTScore (Zhang et al., 2019b) was proposed to compute the similarity between two sentences using contextual word embeddings from BERT. It has higher correlation</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>with human judgements on image captioning and machine translation than word overlap based metrics. We compute BERTScore (BERTSc) between each source sentence and the summary sentence. ${ }^{7}$ To get the final score, we experiment with both the average and the maximum scores computed from each source sentence and the summary sentence. We report results using the maximum score since it has better performance.</p>
<p>Model-based metrics. In addition to QA, recent work has used relation extraction and textual entailment models for faithfulness evaluation (Falke et al., 2019a; Goodrich et al., 2019). For the relation extraction metric (RE), we compute the precision for the relation triplets extracted from the summary sentence and the source document using an off-the-shelf model (Angeli et al., 2015) from Stanford Open IE. For the textual entailment metric (ENT), we measure whether the summary sentence is entailed by the source using the pretrained ESIM model (Chen et al., 2017) from AllenNLP (Gardner et al., 2018).</p>
<h3>4.2 Results</h3>
<p>Metric Comparison. We first compute scores for each metric on document and output sentence pairs on both CNN/DM and XSum datasets ( 748 and 286 pairs respectively). We then compute Pearson and Spearman correlation coefficients between scores given by each metric and humanannotated scores. Table 7 includes correlation coefficients for the examples from CNN/DM and XSum, respectively. We observe that for both CNN/DM and XSum, the score of QA-based evaluation has a higher correlation with faithfulness than other metrics. Although word-overlap based metrics are correlated with the faithfulness in more extractive settings (i.e. for CNN/DM), these metrics have no correlation with faithfulness in more abstractive settings (i.e. for XSum). We further notice that all the metrics have significantly lower correlation with human scores for XSum, suggesting that evaluating faithfulness is more difficult in highly abstractive settings; deeper understanding of the source and the summary sentence is necessary here.</p>
<p>Consistent with the findings of Falke et al. (2019b), the entailment metric does not have a significant correlation with faithfulness in most cases. These models fail to distinguish entailed (faithful)</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Source Sentence</th>
<th style="text-align: left;">Output Sentence</th>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">Score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Health Inspectorate Wales said Wrex- <br> ham Maelor Hospital staff were under <br> "considerable pressure" for long peri- <br> ods as ambulances waited outside.</td>
<td style="text-align: left;">A hospital ward in Wrexham has <br> been rated "inadequate" by inspec- <br> tors after inspectors found patients <br> at risk of harm.</td>
<td style="text-align: left;">Entailment</td>
<td style="text-align: center;">$72.83 \%$</td>
</tr>
<tr>
<td style="text-align: left;">The Black Poplar is one of the rarest <br> native trees in the UK, with only 2,500 <br> thought to be left.</td>
<td style="text-align: left;">Northern Ireland's first trees are <br> among those recognised in the <br> Welsh Architecture Trust's list of <br> the year's best trees.</td>
<td style="text-align: left;">BertScore</td>
<td style="text-align: center;">$83.06 \%$</td>
</tr>
</tbody>
</table>
<p>Table 6: Unfaithful examples missed by Entailment and BertScore. Score: Output score of the metrics; higher score indicates stronger entailment and similarity respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">CNN/DM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XSum</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">S</td>
</tr>
<tr>
<td style="text-align: center;">Word overlap-based</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">R-1</td>
<td style="text-align: center;">$12.02^{<em> </em>}$</td>
<td style="text-align: center;">$15.86^{<em> </em>}$</td>
<td style="text-align: center;">$-2.57$</td>
<td style="text-align: center;">0.07</td>
</tr>
<tr>
<td style="text-align: center;">R-2</td>
<td style="text-align: center;">$13.25^{<em> </em>}$</td>
<td style="text-align: center;">$15.99^{<em> </em>}$</td>
<td style="text-align: center;">$-5.78$</td>
<td style="text-align: center;">$-8.47$</td>
</tr>
<tr>
<td style="text-align: center;">R-L</td>
<td style="text-align: center;">$12.58^{<em> </em>}$</td>
<td style="text-align: center;">$16.49^{<em> </em>}$</td>
<td style="text-align: center;">$-6.37$</td>
<td style="text-align: center;">$-9.68$</td>
</tr>
<tr>
<td style="text-align: center;">B-4</td>
<td style="text-align: center;">$12.09^{<em> </em>}$</td>
<td style="text-align: center;">$11.68^{<em> </em>}$</td>
<td style="text-align: center;">$-6.76$</td>
<td style="text-align: center;">$-10.02$</td>
</tr>
<tr>
<td style="text-align: center;">Embedding-based</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BERTSc</td>
<td style="text-align: center;">$11.07^{*}$</td>
<td style="text-align: center;">$16.70^{*}$</td>
<td style="text-align: center;">10.06</td>
<td style="text-align: center;">10.69</td>
</tr>
<tr>
<td style="text-align: center;">Model-based</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">RE</td>
<td style="text-align: center;">$8.58^{*}$</td>
<td style="text-align: center;">5.52</td>
<td style="text-align: center;">1.62</td>
<td style="text-align: center;">2.32</td>
</tr>
<tr>
<td style="text-align: center;">ENT</td>
<td style="text-align: center;">2.80</td>
<td style="text-align: center;">3.65</td>
<td style="text-align: center;">$-5.62$</td>
<td style="text-align: center;">$-3.85$</td>
</tr>
<tr>
<td style="text-align: center;">FEQA</td>
<td style="text-align: center;">$\mathbf{3 2 . 0 1}^{<em> </em>}$</td>
<td style="text-align: center;">$\mathbf{2 8 . 2 3}^{<em> </em>}$</td>
<td style="text-align: center;">$\mathbf{2 6 . 3 1}^{<em> </em>}$</td>
<td style="text-align: center;">$\mathbf{2 1 . 3 4}^{<em> </em>}$</td>
</tr>
</tbody>
</table>
<p>Table 7: Pearson (P) and Spearman (S) correlation between human-annotated faithfulness scores and the metric scores. ${ }^{<em>},{ }^{</em> *}$ indicates $p$-values $&lt;0.05,&lt;0.001$, respectively. FEQA has the highest correlation with human scores for both CNN/DM and XSum.
and non-entailed (unfaithful) summary sentences when both overlap largely with the source document, because models trained on current entailment datasets may rely on simple heuristics such as lexical overlap (McCoy et al., 2019). Similarly, BERTScore tends to give higher scores when there are overlapping concepts between the sentences even though the content is not the same. See Table 6 for examples.</p>
<p>Content selection and faithfulness. Current evaluation metrics for summarization produce a single measure of the overall quality of the summary. Typically, the output summary is compared against the reference summary in terms of n-gram overlap. These metrics mainly evaluate content selection, i.e. whether the content of the output is similar to the content of the reference. In contrast, to evaluate faithfulness, we compare the output summary against the source document. One natural question that follows is whether high content matching sufficient for faithfulness. We compute the correlation coefficients between humanannotated faithfulness scores and ROUGE scores computed from the reference and the output sentence. As shown in Table 8, while there is a weak</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">CNN/DM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XSum</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Metric</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">S</td>
<td style="text-align: center;">P</td>
<td style="text-align: center;">S</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-1</td>
<td style="text-align: center;">$15.31^{<em> </em>}$</td>
<td style="text-align: center;">$14.92^{<em> </em>}$</td>
<td style="text-align: center;">5.44</td>
<td style="text-align: center;">5.79</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-2</td>
<td style="text-align: center;">$15.10^{<em> </em>}$</td>
<td style="text-align: center;">$16.39^{<em> </em>}$</td>
<td style="text-align: center;">8.25</td>
<td style="text-align: center;">6.79</td>
</tr>
<tr>
<td style="text-align: left;">ROUGE-L</td>
<td style="text-align: center;">$13.33^{<em> </em>}$</td>
<td style="text-align: center;">$13.35^{<em> </em>}$</td>
<td style="text-align: center;">4.61</td>
<td style="text-align: center;">3.97</td>
</tr>
</tbody>
</table>
<p>Table 8: Pearson (P) and Spearman (S) correlation between human-annotated faithfulness scores and ROUGE scores of content selection (computed between the reference and the output sentence). High content selection scores (typical ROUGE score for summarization) do not necessarily imply faithfulness of the summary.
correlation between ROUGE scores of content selection and faithfulness on CNN/DM, the correlation is significantly lower than ROUGE scores of faithfulness (i.e. computed between the source and the output sentence). For XSum, there is no significant correlation between the content selection metrics and faithfulness. We provide unfaithful examples with high content selection scores in Appendix D.3. This suggests that content selection and faithfulness should be measured separately as opposed to using a unified score.</p>
<p>Analysis and limitations of QA-based evaluation. Table 9 shows examples for a faithful and an unfaithful output sentence and the corresponding QA pairs. Note that the QA system is able to capture common errors such as conflicting information in the output sentence. To measure the reliability of FEQA, we further perform a manual error analysis using 100 randomly sampled QA pairs. We observe that around $94 \%$ of generated questions are mostly grammatical and correct given the mask. For $78 \%$ of the questions, the QA system has the correct behaviour: it answers the question correctly if the sentence is faithful to the article, otherwise it produces "unanswerable" or an incorrect answer. Majority of the errors of the QA system are because it either didn't detect unanswerable questions or produces "unanswerable" when there exists an answer ( $14 \%$ ). More-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Source</th>
<th style="text-align: center;">Output Sentence</th>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">OA</th>
<th style="text-align: center;">SA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">...However, Winger Ross Wallace (knee) and right-back Steven Reid (calf) could return for the Barclays premier league contest...</td>
<td style="text-align: center;">Dean Marney and Steven Reid could return for the Barclays Premier League match.</td>
<td style="text-align: center;">Who and Steven Reid could return for the premier league match?</td>
<td style="text-align: center;">Dean Mar- <br> ney</td>
<td style="text-align: center;">Ross Wallace</td>
</tr>
<tr>
<td style="text-align: center;">...Miss Bruck, 22, from maybe has not been seen since the early hours of October 26, 2014. She has not been seen for six months...</td>
<td style="text-align: center;">Miss Bruck, 22, from maybe has not been seen for six months.</td>
<td style="text-align: center;">How long has Miss Bruck, 22 from not been seen for?</td>
<td style="text-align: center;">six months</td>
<td style="text-align: center;">six months</td>
</tr>
</tbody>
</table>
<p>Table 9: Examples detection results from FEQA. OA:Output Answer, SA:Source Answer. The output sentence in the first example is unfaithful, whereas the one for the second example is faithful. Bold text indicates the span that was masked to generate the question.
over, when the article is long, QA system tends to make more mistakes. Especially for more abstractive settings, F1-score penalizes the correct answers when the answer from the article does not exactly match with the gold answer (i.e. "Donald Trump" vs. "the President of the United States Donald Trump") ( $16 \%$ ).</p>
<h2>5 Related Work</h2>
<p>Problems in current neural generation models. Since the beginning of neural text generation, problems with repetition and generic responses have received lots of attention (Sordoni et al., 2015; Li et al., 2016; Holtzman et al., 2019). Recently, more work has focused on semantic errors in model outputs, such as adequacy in machine translation (Tu et al., 2017), faithfulness in summarization (Cao et al., 2018), and consistency in dialogue (Li et al., 2019). Our analysis on the abstractiveness-faithfulness tradeoff reveals additional limitation of current models, and suggests that we need new inductive bias on how to summarize beyond copying.</p>
<p>QA as a proxy. Question answering is a broad format that subsumes many tasks (Gardner et al., 2019). To the best of our knowledge, Mani et al. (1999) first use QA as an extrinsic evaluation for summarization: A good summary should answer key questions a reader might have about an article. Later, QA is incorporated in human evaluation where one person writes questions and another person answers them based on the summary (Clarke and Lapata, 2010; Liu and Lapata, 2019). The closest to our work are recent efforts in automating this protocol, including rule-based approaches (Chen et al., 2018) and cloze-test QA (Eyal et al., 2019; Scialom et al., 2019). Our work is the first to apply automated question generation. While we focus on faithfulness, our QAbased metric is applicable to semantic comparison
between any two pieces of text.
Automated evaluation for NLG. Automated NLG evaluation is challenging as it often requires deep understanding of the text. Although metrics based on word overlap with the reference text are commonly used, it is widely known that they do not correlate well with human judgments (Novikova et al., 2017; Liu et al., 2016). Recently, more work has focused on model-based evaluation using discriminators (Lowe et al., 2017; Hashimoto et al., 2019), entailment models (Falke et al., 2019a), information extraction (Wiseman et al., 2017; Goodrich et al., 2019), and question answering (Chen et al., 2018; Eyal et al., 2019).</p>
<h2>6 Conclusion</h2>
<p>We investigate the faithfulness problem in neural abstractive summarization and propose a QAbased metric for evaluating summary faithfulness. We show that current models suffer from an inherent trade-off between abstractiveness and faithfulness. They are good at copying important source content, but tend to concatenate unrelated spans and hallucinate details when generating more abstractive sentences. A new inductive bias or additional supervision is needed for learning reliable models. While our QA-based metric correlates better with human judgment and is useful for model development, it is limited by the quality of the QA model. The final evaluation should still rely on human annotation or human-in-the-loop methods (Chaganty et al., 2018).</p>
<h2>Acknowledgement</h2>
<p>We would like to thank Faisal Ladhak, the Lex and Comprehend groups at Amazon Web Services AI, and the anonymous reviewers for their feedback on this work.</p>
<h2>References</h2>
<p>R. Alec, W. Jeff, C. Rewon, L. David, A. Dario, and S. Ilya. 2019. Language models are unsupervised multitask learners.</p>
<p>Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. 2015. Leveraging linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 344-354, Beijing, China. Association for Computational Linguistics.
Z. Cao, F. Wei, W. Li, and S. Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In Association for the Advancement of Artificial Intelligence (AAAI).
A. Chaganty, S. Mussmann, and P. Liang. 2018. The price of debiasing automatic metrics in natural language evaluation. In Association for Computational Linguistics (ACL).</p>
<p>Danqi Chen. 2018. Neural Reading Comprehension and Beyond. Ph.D. thesis, Stanford University.
P. Chen, F. Wu, T. Wang, and W. Ding. 2018. A semantic QA-based approach for text summarization evaluation. In Association for the Advancement of Artificial Intelligence (AAAI).</p>
<p>Qian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui Jiang, and Diana Inkpen. 2017. Enhanced LSTM for natural language inference. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1657-1668, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Yen-Chun Chen and Mohit Bansal. 2018. Fast abstractive summarization with reinforce-selected sentence rewriting. In $A C L$.
J. Clarke and M. Lapata. 2010. Discourse constraints for document compression. Computational Linguistics, 36 .</p>
<p>Dorottya Demszky, Kelvin Guu, and Percy Liang. 2018. Transforming question answering datasets into natural language inference datasets. ArXiv, abs/1809.02922.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT.
M. Eyal, T. Baumel, and M. Elhadad. 2019. Question answering as an automatic evaluation metric for news article summarization. In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL).
T. Falke, L. F. R. Ribeiro, P. A. Utama, I. Dagan, and I. Gurevych. 2019a. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Association for Computational Linguistics (ACL).</p>
<p>Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019b. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214-2220, Florence, Italy. Association for Computational Linguistics.</p>
<p>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL '05, pages 363-370, Stroudsburg, PA, USA. Association for Computational Linguistics.</p>
<p>Jianfeng Gao, Michel Galley, and Lihong Li. 2018. Neural approaches to conversational AI. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 2-7, Melbourne, Australia. Association for Computational Linguistics.
M. Gardner, J. Berant, H. Hajishirzi, A. Talmor, and S. Min. 2019. Question answering is a format; when is it useful? arXiv preprint arXiv:1909.11291.</p>
<p>Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke Zettlemoyer. 2018. AllenNLP: A deep semantic natural language processing platform. In Proceedings of Workshop for NLP Open Source Software (NLP-OSS), pages 16, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Sebastian Gehrmann, Yuntian Deng, and Alexander Rush. 2018. Bottom-up abstractive summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4098-4109, Brussels, Belgium. Association for Computational Linguistics.
B. Goodrich, V. Rao, P. J. Liu, and M. Saleh. 2019. Assessing the factual accuracy of generated text. In International Conference on Knowledge Discovery and Data Mining (KDD).
M. Grusky, M. Naaman, , and Y. Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In North American Association for Computational Linguistics (NAACL).
T. Hashimoto, H. Zhang, and P. Liang. 2019. Unifying human and statistical evaluation for natural language generation. In North American Association for Computational Linguistics (NAACL).</p>
<p>Karl Moritz Hermann, Tomáš Kočiský, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS'15, pages 1693-1701, Cambridge, MA, USA. MIT Press.
A. Holtzman, J. Buys, M. Forbes, and Y. Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751.</p>
<p>Nikita Kitaev and Dan Klein. 2018. Constituency parsing with a self-attentive encoder. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2676-2686, Melbourne, Australia. Association for Computational Linguistics.
K. Knight and D. Marcu. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentence compression. Artifical Intelligence, 139:91107.
W. Kryściński, N. S. Keskar, B. McCann, C. Xiong, and R. Socher. 2019. Neural text summarization: A critical evaluation. In Empirical Methods in Natural Language Processing (EMNLP).
L. Lebanoff, J. Muchovej, F. Dernoncourt, D. S. Kim, S. Kim, W. Chang, and F. Liu. 2019. Analyzing sentence fusion in abstractive summarization. arXiv preprint arXiv:1910.00203.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. ArXiv, abs/1910.13461.
J. Li, M. Galley, C. Brockett, J. Gao, and W. B. Dolan. 2016. A diversity-promoting objective function for neural conversation models. In Human Language Technology and North American Association for Computational Linguistics (HLT/NAACL), pages $110-119$.
M. Li, S. Roller, I. Kulikov, S. Welleck, Y. Boureau, K. Cho, and J. Weston. 2019. Don't say that! making inconsistent dialogue unlikely with unlikelihood training. arXiv preprint arXiv:1911.03860.
C. Liu, R. Lowe, I. V. Serban, M. Noseworthy, L. Charlin, and J. Pineau. 2016. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Empirical Methods in Natural Language Processing (EMNLP).
Y. Liu and M. Lapata. 2019. Text summarization with pretrained encoders. In Empirical Methods in Natural Language Processing (EMNLP).
R. Lowe, M. Noseworthy, I. V. Serban, N. AngelardGontier, Y. Bengio, and J. Pineau. 2017. Towards an automatic turing test: Learning to evaluate dialogue responses. In Association for Computational Linguistics (ACL).
I. Mani, G. Klein, L. Hirschman, T. Firmin, D. House, and B. Sundheim. 1999. The TIPSTER SUMMAC text summarization evaluation. In European Association for Computational Linguistics (EACL).</p>
<p>Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55-60, Baltimore, Maryland. Association for Computational Linguistics.
R. T. McCoy, E. Pavlick, and T. Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007.</p>
<p>Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2016a. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. In AAAI.</p>
<p>Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, Çağlar Guİ̧̧lçehre, and Bing Xiang. 2016b. Abstractive text summarization using sequence-tosequence RNNs and beyond. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning, pages 280-290, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! Topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium.
J. Novikova, O. Dušek, A. C. Curry, and V. Rieser. 2017. Why we need new evaluation metrics for NLG. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784789, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>T. Scialom, S. Lamprier, B. Piwowarski, and J. Staiano. 2019. Answers unite! unsupervised metrics for reinforced summarization models. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In $A C L$.
A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell, J. Nie, J. Gao, and B. Dolan. 2015. A neural network approach to context-sensitive generation of conversational responses. In North American Association for Computational Linguistics (NAACL).
Z. Tu, Y. Liu, L. Shang, X. Liu, and H. Li. 2017. Neural machine translation with reconstruction. In Association for the Advancement of Artificial Intelligence (AAAI).</p>
<p>Zhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. 2016. Modeling coverage for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7685, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2692-2700. Curran Associates, Inc.
S. Wiseman, S. M. Shieber, and A. M. Rush. 2017. Challenges in data-to-document generation. In Empirical Methods in Natural Language Processing (EMNLP).
F. Zhang, J. Yao, and R. Yan1. 2018. On the abstractiveness of neural document summarization. In Empirical Methods in Natural Language Processing (EMNLP).
T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. 2019a. BERTSCORE: Evaluating text generation with BERT. arXiv preprint arXiv:1904.09675.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2019b. Bertscore: Evaluating text generation with bert. ArXiv, abs/1904.09675.</p>
<h2>A Summarization Datasets</h2>
<p>All of our experiments are run on the CNN/DM and XSum datasets. We show basic statistics of the two datasets in Table 10.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">CNN/DM</th>
<th style="text-align: right;">XSum</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># Training Documents</td>
<td style="text-align: right;">287,227</td>
<td style="text-align: right;">204,045</td>
</tr>
<tr>
<td style="text-align: left;"># Validation Documents</td>
<td style="text-align: right;">13,368</td>
<td style="text-align: right;">11,332</td>
</tr>
<tr>
<td style="text-align: left;"># Test Documents</td>
<td style="text-align: right;">11,490</td>
<td style="text-align: right;">11,334</td>
</tr>
<tr>
<td style="text-align: left;">Document: avg # of tokens</td>
<td style="text-align: right;">781.00</td>
<td style="text-align: right;">431.07</td>
</tr>
<tr>
<td style="text-align: left;">Document: avg # of sents.</td>
<td style="text-align: right;">40.00</td>
<td style="text-align: right;">33.00</td>
</tr>
<tr>
<td style="text-align: left;">Summary: avg # tokens</td>
<td style="text-align: right;">56.00</td>
<td style="text-align: right;">23.26</td>
</tr>
<tr>
<td style="text-align: left;">Summary: avg # of sents.</td>
<td style="text-align: right;">3.75</td>
<td style="text-align: right;">1.00</td>
</tr>
</tbody>
</table>
<p>Table 10: Statistics of CNN/DM and XSum datasets.</p>
<h2>B Summarization Models</h2>
<p>The characteristics of each model used in our experiments are detailed below.</p>
<p>Pointer Generator Model with Coverage (PGC) (See et al., 2017) uses the copy mechanism (Vinyals et al., 2015) to allow copying words from the source. The adapted coverage mechanism (Tu et al., 2016) is incorporated to alleviate repetition by keeping track of source words that have been summarized. This copy mechanism is widely adopted by subsequent models.</p>
<p>Fast Abstractive Summarization with Reinforce (FASTRL) (Chen and Bansal, 2018) first uses an extractor agent to select salient sentences from the document, then condenses the extracted sentences using the Pointer-Generator summarizer.</p>
<p>Bottom-up Summarization Model (BottomUp) (Gehrmann et al., 2018) first selects words from the source document that are likely to appear in the summary, then generates using the Pointer-Generator model, where the copying mechanism is constrained to the previously selected words. It improves upon PGC by explicitly learning the selector to avoid copying long text spans.</p>
<p>Topic-aware Convolutional Sequence-to-Sequence model (TCONVS2s) (Narayan et al., 2018) is a convolutional neural network-based model conditioned on the topics of the article. It is shown to be effective in capturing long-range dependencies in the documents.</p>
<p>BERT-based model (BERTSUM) (Liu and Lapata, 2019) is a two-stage fine-tuning approach
where the BERT-based encoder is first fine-tuned on the extractive summarization task and then on the abstractive sumarization task with the decoder (denoted as BERTSUMEXTAbS in the original paper).</p>
<h2>C Details of Human Annotations</h2>
<h2>C. 1 Grammaticality Annotation Guidelines</h2>
<p>For grammaticality annotation, we present only the output sentence to the workers. We collect annotations from 5 workers for both of the tasks. For this task, given the output sentence, we provide workers the following guidelines:</p>
<ol>
<li>First select whether the given sentence is "Nonsensical" or "Makes sense".</li>
<li>If the given text is not a complete sentence, mark it as "Nonsensical".</li>
<li>If you can understand the meaning of the sentence, despite grammaticality errors, and you are able to makes sense of it, select "Makes sense".</li>
<li>If you did not select "Nonsensical", evaluate whether the sentence is "Grammatical" or "Has Minor Grammaticality Issues".</li>
</ol>
<h2>C. 2 Faithfulness Annotation Guidelines</h2>
<p>We present workers both the source and the output sentence and provide the following guidelines:</p>
<ol>
<li>Read the sentence and the source fully.</li>
<li>If the information conveyed by the sentence is not expressed in the source, select "unfaithful".</li>
<li>Avoid using general knowledge, and check if the sentence is consistent with the source.</li>
<li>If you select "unfaithful", for the second part, select whether the information expressed by the sentence is not contained in the source or conflicting with the source.</li>
</ol>
<h2>D Additional Analysis</h2>
<h2>D. 1 Examples for nonsensical sentences</h2>
<ul>
<li>Sandals, $£ 34$, office.co.uk, luluguinness.com. (generated by PGC for CNN/DM)</li>
</ul>
<table>
<thead>
<tr>
<th style="text-align: left;">Source</th>
<th style="text-align: left;">Output Sentence</th>
<th style="text-align: left;">Category</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">...Although her due date has not officially <br> been confirmed, the duchess of Cambridge <br> told wellwishers at a charity event last month: <br> I am due mid-April, to the end of April...</td>
<td style="text-align: left;">The duchess of Cambridge told <br> wellwishers at a charity event <br> last month: "The duke's inten- <br> tion is to be at the commemorations".</td>
<td style="text-align: left;">IC</td>
</tr>
<tr>
<td style="text-align: left;">...Carragher spoke to a local TV starton dur- <br> ing his time in Girona. Carragher posted a <br> picture on his Instagram account of the open- <br> ing ceremony...</td>
<td style="text-align: left;">Carragher posted a picture on <br> his son play in the famous <br> youth tournament.</td>
<td style="text-align: left;">IC</td>
</tr>
<tr>
<td style="text-align: left;">A body was found by a member of the public <br> on private land near Leighton, about 10 miles <br> (16.09km) away from the centre of Shrews- <br> bury, on Monday. Mr Bebbington's family <br> has been informed, West Mercia Police con- <br> firmed.</td>
<td style="text-align: left;">The death of a man whose body <br> was found in a river in Cumbria <br> has been identified as murder.</td>
<td style="text-align: left;">H</td>
</tr>
<tr>
<td style="text-align: left;">The incident happened near Dr. Gray's hospi- <br> tal shortly after 10:00. The man was taken to <br> the hospital with what police said were seri- <br> ous but not life-threatening injuries. The a96 <br> was closed in the area for several hours, but it <br> has since reopened.</td>
<td style="text-align: left;">A man has been taken to hospi- <br> tal after he was hit by a lorry in <br> Dumfries.</td>
<td style="text-align: left;">H</td>
</tr>
</tbody>
</table>
<p>Table 11: Examples of meaningful but unfaithful sentences. Category corresponds to the category of unfaithfulness error for the output sentence. IC: Incorrect Concatenation, H: Hallucination.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Reference</th>
<th style="text-align: left;">Output Sentence</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">... University of Nebraska researcher has re- <br> vealed why stress is bad for you. Limited pe- <br> riods of stress are good, as they release corti- <br> sol...</td>
<td style="text-align: left;">University of Nebraska researcher has <br> revealed why stress is bad for you, <br> stimulating your body to produce an <br> important hormone called cortisol.</td>
</tr>
<tr>
<td style="text-align: left;">...Indian air force and Nepalese army medical <br> team launch rescue mission to bring injured <br> people to hospitals in Kathmandu. Forshani <br> Tamang's family carried her for four hours to <br> reach help after she was wounded when their <br> home was destroyed...</td>
<td style="text-align: left;">Indian air crew and Nepalese army <br> medical team were killed in Nepal's <br> Sindhupalchok quake.</td>
</tr>
</tbody>
</table>
<p>Table 12: Examples of unfaithful sentence with high content overlap (computed by ROUGE-L) with the reference.</p>
<ul>
<li>He says easter triduum is a progression, although the word itself - triduum. (generated by FASTRL for CNN/DM)</li>
<li>Chelsea beat Chelsea $5-3$ in the Premier League on Saturday. (generated by FASTRL for CNN/DM)</li>
<li>12 years a slave actress Lupita Woodley and oily vegetables. (generated by BotTOMUP for CNN/DM)</li>
<li>A judge in Japan has ordered a judge to order a woman who has absconded from Japan to Japan. (generated by PGC for XSum)</li>
<li>Stoke City moved up to third in the Premier League with victory over Stoke City at Stoke. (generated by TCONV for XSum)</li>
<li>Johnny Depp's management group is suing his management group over his "lavish lifestyle". (generated by BERTSUM for XSum)</li>
</ul>
<h2>D. 2 Examples for meaningful but unfaithful sentences</h2>
<p>Table 11 includes examples that are annotated as meaningful but unfaithful. First three examples are picked from the models trained on CNN/DM, and last three are from the models trained on XSum. We observe that majority of sentences with faithfulness errors for CNN/DM dataset are generated by incorrect concatenation (IC). The models fuse two sentences from the source and generate a new sentence that is not consistent with the context of the source. Within this category, however, the models make a wide-range of mistakes such as copying the wrong entity, date, and quote.</p>
<p>For XSum, the faithfulness mistakes are mostly hallucinations. Models tend to hallucinate information (e.g. entities, events, date) that is not present in the source.</p>
<h2>D. 3 Examples for sentences with high content overlap with reference that are unfaithful</h2>
<p>Although current summarization models are evaluated with respect to the content overlap between the reference and the output, these metrics do not necessarily provide any guarantees for the faithfulness of the output. Table 12 includes examples with similar content overlap scores as the faithful
examples but are unfaithful. We can see that although the output sentences include similar words and refer to similar topics, they include hallucinations and inaccurate information.</p>
<h2>D. 4 Limitations of the datasets</h2>
<p>Since CNN/DM and XSum datasets are automatically crawled, we find that there is noise in the data. For example, source documents can include phrases such as "click here for the latest news". We further observe that reference can carry information that is not in the source document since some of these one sentence highlights are written using additional world knowledge. Table 13 shows an example where the reference is unfaithful since it includes information that is not in the source (i.e. the fact that Ms. Wood's first name is Leanne and she is Plaid Cymru leader.).</p>
<h2>Source</h2>
<p>Ms Wood blamed the Conservatives in particular for claiming the SNP posed a threat to the future of the UK. She claimed "progressive" parties like hers were offering a "collaborative" alternative to "combative" politics. "This election presents an opportunity for harmonious co-existence between our nations," she said. Ms Wood's comments followed Conservative claims that Labour dependence on support from the SNP to form a government after the election on 7 May would threaten the break-up of the UK. Campaigning in south Wales on Monday, she said: "The parties advocating progressive, inclusive nonpartisan cooperation in this election are not those who claim to cherish the political union above all others, but the national parties of Wales and Scotland. Along with the Greens in England, our parties have provided people across these islands with a collaborative alternative to the traditional combative Westminster politics.". Ms Wood added that she had received "hundreds" of supportive messages from people in England following the televised debates.</p>
<h2>Reference</h2>
<p>Plaid Cymru leader Leanne Wood has accused rival parties of "dangerous and divisive rhetoric" in a "desperate" attempt to win votes.</p>
<p>Table 13: Example where reference includes information that is not in the source.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ We report only BLUE-4 since it performed the best for CNN/DM and no variation of BLEU has significant correlation with faithfulness for XSum.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ https://github.com/Tiiiger/bert_score.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
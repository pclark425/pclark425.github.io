<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8826 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8826</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8826</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-253363620</p>
                <p><strong>Paper Title:</strong> Enriched entity representation of knowledge graph for text generation</p>
                <p><strong>Paper Abstract:</strong> Text generation is a key tool in natural language applications. Generating texts which could express rich ideas through several sentences needs a structured representation of their content. Many works utilize graph-based methods for graph-to-text generation, like knowledge-graph-to-text generation. However, generating texts from knowledge graph still faces problems, such as repetitions and the entity information is not fully utilized in the generated text. In this paper, we focus on knowledge-graph-to-text generation, and develop a multi-level entity fusion representation (MEFR) model to address the above problems, aiming to generate high-quality text from knowledge graph. Our model introduces a fusion mechanism, which is capable of aggregating node representations from word level and phrase level to obtain rich entity representations of the knowledge graph. Then, Graph Transformer is adopted to encode the graph and outputs contextualized node representations. Besides, we develop a vanilla beam search-based comparison mechanism during decoding procedure, which further considers similarity to reduce repetitive information of the generated text. Experimental results show that the proposed MEFR model could effectively improve generation performance, and outperform other baselines on AGENDA and WebNLG datasets. The results also demonstrate the importance to further explore information contained in knowledge graph.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8826.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8826.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MEFR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-level Entity Fusion Representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-to-text representation and model introduced in this paper that enriches entity node embeddings by fusing word-level and phrase-level representations, encodes the graph with a Graph Transformer, and decodes with an attention RNN + copy mechanism; includes a beam-search comparison step to reduce redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>multi-level fusion representation (MEFR)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each entity node the model builds a word-level embedding via a BiRNN over the entity's tokens and a phrase-level embedding via a BiRNN over the ordered list of entity word-level embeddings; these two are fused (via weighted sum or a gating/selective mechanism) to form enriched entity node embeddings. Relation nodes and a global node are created in preprocessing. The graph (nodes including enriched entities, relation nodes, global node) is encoded with a Graph Transformer; decoding is performed by an attention-based RNN with a pointer/copy mechanism and a beam-search comparison penalty to reduce repetition.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (entity-relation graphs derived from AGENDA and WebNLG; DBpedia subgraphs; automatically extracted scientific-abstract graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Nodes (entities, relation nodes, global node, and optionally title-as-node) are encoded into contextual vectors by Graph Transformer; an RNN decoder generates text autoregressively attending to graph and title encodings, and may copy tokens from the graph via pointer mechanism; the process is an encoder–decoder graph-to-sequence conversion rather than naive linearization.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Knowledge-graph-to-text generation (abstract/summary generation and RDF/DBpedia graph description generation on AGENDA and WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>AGENDA (Table 3): BLEU-1 33.19, BLEU-2 18.34, BLEU-3 11.26, BLEU-4 7.05. WebNLG (Table 4): BLEU 62.06. Selective-fusion ROUGE (Table 1/2): ROUGE-1 35.46, ROUGE-2 11.60, ROUGE-L 15.69 (reported on test set).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>MEFR outperforms Graph Transformer, GAT, GCG and other listed baselines on AGENDA BLEU metrics and outperforms prior baselines on WebNLG BLEU (MEFR BLEU 62.06 vs Graformer 61.15 and Adapt 60.59); authors attribute gains to richer entity representations (multi-granularity fusion) plus the beam-search comparison mechanism that reduces redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enriches entity representations via multi-granularity (word and phrase) fusion improving entity coverage and informativeness; reduces repetition via a similarity-aware beam-search scoring; achieves state-of-the-art (among compared non-pretrained methods) BLEU/ROUGE on the datasets used; does not require pretrained language models.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds modeling complexity (BiRNNs at two levels, gating parameters, extra preprocessing to create relation nodes), requires tuning of hyperparameters (α for fusion, δ for beam comparison), increases number of nodes due to relation-node conversion which can increase computation.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No explicit catastrophic failure case reported for MEFR; authors note general issues in the domain (repetition and uncovered entity information) that MEFR aims to mitigate. Dataset issues (e.g., disconnected components in automatically extracted AGENDA graphs) remain a challenge for graph-to-text methods and may limit performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriched entity representation of knowledge graph for text generation', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8826.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8826.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sum Fusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sum fusion mechanism (direct and weighted sum)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple multi-granularity fusion that combines word-level and phrase-level entity embeddings either by direct summation or by a weighted sum controlled by α.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>sum fusion of word-level and phrase-level entity embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Two variants: direct sum h_p = h_w + h_p (Sum_i) and weighted sum h_p = α * h_w + (1-α) * h_p (Sum_e), where h_w is a BiRNN word-level embedding of the entity string and h_p is a BiRNN phrase-level embedding across entities.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (entity nodes within the KG)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Per-entity fusion is applied before Graph Transformer encoding so nodes receive fused embeddings which the encoder then contextualizes for decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 2 (test set ROUGE): Sum_i: ROUGE-1 31.17, ROUGE-2 9.30, ROUGE-L 14.24. Sum_e (weighted): ROUGE-1 32.82, ROUGE-2 10.26, ROUGE-L 14.76.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Performs worse than the selective (gating) fusion mechanism in ROUGE scores; weighted sum (Sum_e) is better than direct sum but still inferior to selective gating.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement, adds multi-granularity information without many extra parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Global/constant weighting cannot adapt per-entity; less flexible than gating and yields lower generation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Does not capture per-entity variable importance of word- vs phrase-level information; empirically underperforms the selective mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriched entity representation of knowledge graph for text generation', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8826.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8826.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Selective Fusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Selective mechanism (gating fusion)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A highway-like gating fusion that dynamically weights word-level vs phrase-level entity embeddings per entity using a sigmoid gate.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>selective gating fusion of entity embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Compute gate s_i = σ(β1 * h_w + β2 * h_p + c) and fuse as h_pi = s_i * h_w + (1 - s_i) * h_p; β1, β2, c are learned parameters and σ is sigmoid. Variants (remove phrase gating or word gating) were evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Per-entity gated fusion performed prior to Graph Transformer encoding; produces d-dimensional node embeddings fed into the encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 1 (test set ROUGE): Selective mechanism: ROUGE-1 35.46, ROUGE-2 11.60, ROUGE-L 15.69. Variants: Selective w/o p ROUGE-1 33.71 ROUGE-2 10.58 ROUGE-L 14.85; Selective w/o w ROUGE-1 32.74 ROUGE-2 9.83 ROUGE-L 14.31.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Selective gating outperforms both sum variants and its ablated variants, indicating dynamic per-entity fusion is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Adapts fusion per entity, better preserves multi-granular information, yields higher ROUGE/BLEU when used in MEFR.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Introduces gating parameters and slight extra computation over simple sums.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Removing gating on either side degrades performance (shown in ablation). No other specific failure mode reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriched entity representation of knowledge graph for text generation', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8826.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8826.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Relation nodes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation-node conversion (edge-to-node transformation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing representation that turns graph edges into explicit relation nodes (including forward and backward direction nodes) and adds a global node connecting entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>relation-node transformation / graph preprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each relation between two entities is represented by relation nodes (one per direction); a global node is also added to connect all entity nodes. Initial embeddings are learned for relation and global nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (DBpedia subgraphs, automatically extracted scientific KGs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Graph preprocessing step: edges -> relation nodes; after this transformation the resulting graph (entities + relation nodes + global node) is encoded by Graph Transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated in isolation; used as part of MEFR and preprocessing following prior works (used on AGENDA/WebNLG).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors note this mirrors prior practice (cited [15,16]) to avoid parameter explosion when modeling many relation types and to enable explicit relation representation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Makes relations explicit as vertices so the encoder can model relation semantics; helps control parameterization for many relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Increases node count and computational cost; may create larger graphs to encode.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>No specific failure modes reported in the paper; potential scaling limits on very dense graphs are implied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriched entity representation of knowledge graph for text generation', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8826.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8826.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Title-as-node</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Title-as-node encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation technique that treats an associated document title as an additional node: the title is encoded by a BiRNN and its encoding is attended alongside graph encodings during decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>title-as-node</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode title text with a BiRNN to produce title embedding T; use this title encoding as an additional context vector (treated like a node) when computing decoder attention and final context c_t = concat(c_k, c_r).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs paired with titles (AGENDA/WebNLG instances that include title metadata)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Title is encoded separately and concatenated with graph context for decoding; not linearized into graph but used as auxiliary node/context.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not reported separately; title-as-node is part of MEFR encoder-decoder pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Used by MEFR to enrich decoding context; no direct ablation numbers reported for title presence/absence.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides a concise global cue relevant to the generated text; helps decoder select content.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Only applicable when titles exist; marginal extra encoding cost.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriched entity representation of knowledge graph for text generation', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8826.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8826.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text Generation from Knowledge Graphs with Graph Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-aware adaptation of the Transformer that contextualizes node encodings by combining graph attention with transformer-style multi-head attention; used as the graph encoder in MEFR.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text Generation from Knowledge Graphs with Graph Transformers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Graph Transformer encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graph Transformer applies multi-head attention over node neighborhoods (and with mechanisms for global contextualization) to produce contextualized node embeddings suitable for downstream sequence generation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Encodes graph nodes into contextualized vectors which are then consumed by an RNN decoder to produce text; not a linearization method but a graph-to-vector encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Knowledge-graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 3 (AGENDA): BLEU-1 29.08, BLEU-2 14.93, BLEU-3 8.57, BLEU-4 4.62. (Used as baseline and as MEFR's encoder.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms earlier GAT/GCG baselines in experiments; MEFR further improves over Graph Transformer by enriching entity representations and adding the beam comparison mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Provides global contextualization of nodes and improves on local-only graph encoders; strong baseline for graph-to-text tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>By itself still may miss covering all entity information and can produce repetition; benefits from improved node (entity) representations as shown by MEFR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriched entity representation of knowledge graph for text generation', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8826.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8826.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Beam-Comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam search-based comparison mechanism (similarity-augmented beam search)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoding-time augmentation of vanilla beam search that penalizes candidate tokens that increase similarity to the existing generated sequence to reduce redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>similarity-augmented beam search</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>During beam search the score for a candidate token y_t is adjusted: score(y_t) = δ * score(y_t) - (1-δ) * comp(s* + y_t, s*), where comp() is cosine similarity between the sequence with and without the new token and δ is tuned. This penalizes candidates that make the output more similar to existing text (i.e., repetitive).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Applies during decoding of text (independent of graph type)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Not a graph-to-text conversion per se but a decoding heuristic applied to the sequence generation step to influence which tokens are selected by beam search.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Text generation (graph-to-text)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Tuned parameter δ: best δ=0.4 on AGENDA (gives best ROUGE-2) and δ=0.5 on WebNLG as reported; authors report improved ROUGE-2 vs vanilla beam search (δ=1) and improved overall generation quality in human evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors show that vanilla beam search (δ=1) yields worse ROUGE-2 and more redundancy; the comparison mechanism with tuned δ improves ROUGE-2 and reduces repetition.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces redundancy and repetition in generated text and improves automatic and human-evaluated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Adds per-candidate similarity computation and a hyperparameter δ that requires tuning; overly strong penalization (e.g., δ=0) degrades quality.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>When δ = 0 (full reliance on comp term) quality is poor; must balance selection vs redundancy penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriched entity representation of knowledge graph for text generation', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8826.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8826.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EntityWriter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline mentioned that uses only entity strings and title for generation without modeling graph relations; reported as the weakest-performing baseline on AGENDA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>entity-and-title-only representation</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Input consists only of entity tokens and title (no relation nodes or explicit relation modeling); fed into encoder-decoder to generate text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (but relations ignored; only entities used)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Serialize entities (and title) as input sequence for encoder-decoder instead of encoding full graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 3 (AGENDA): BLEU-1 24.12, BLEU-2 10.27, BLEU-3 4.38, BLEU-4 3.09.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Performs poorest among listed models on AGENDA; authors attribute poor performance to ignoring graph relations.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simpler input representation and fewer parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Misses relational structure and yields substantially lower BLEU scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriched entity representation of knowledge graph for text generation', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8826.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8826.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adapt / Linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adapt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior neural encoder-decoder approach for WebNLG that linearizes the input graph/triples into a sequence and uses sub-word representations; a strong baseline on WebNLG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization / serialization of graph into sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Convert triples/graph into a linear sequence (serialized representation) and feed into a sequence encoder (with sub-word tokens) so that standard seq2seq models can be applied.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Knowledge graphs (RDF/DBpedia subgraphs as in WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Triple/graph linearization into ordered sequence; then passed through encoder-decoder model.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Graph-to-text generation (WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Table 4 (WebNLG): BLEU 60.59 (reported for Adapt baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Competitive baseline; MEFR slightly outperforms Adapt on WebNLG (MEFR BLEU 62.06).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, leverages powerful sequence models and sub-word modeling; strong empirical performance on WebNLG.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Linearization can obscure explicit graph relations and global structure; may not scale well to very large or complex graphs where structure matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enriched entity representation of knowledge graph for text generation', 'publication_date_yy_mm': '2022-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>Graph attention networks <em>(Rating: 2)</em></li>
                <li>Graph2seq: graph to sequence learning with attention-based neural networks <em>(Rating: 2)</em></li>
                <li>Graph-to-sequence learning using gated graph neural networks <em>(Rating: 2)</em></li>
                <li>The webnlg challenge: Generating text from rdf data <em>(Rating: 2)</em></li>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 2)</em></li>
                <li>Modeling global and local node contexts for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>Get to the point: Summarization with pointer-generator networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8826",
    "paper_id": "paper-253363620",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "MEFR",
            "name_full": "Multi-level Entity Fusion Representation",
            "brief_description": "A graph-to-text representation and model introduced in this paper that enriches entity node embeddings by fusing word-level and phrase-level representations, encodes the graph with a Graph Transformer, and decodes with an attention RNN + copy mechanism; includes a beam-search comparison step to reduce redundancy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "multi-level fusion representation (MEFR)",
            "representation_description": "For each entity node the model builds a word-level embedding via a BiRNN over the entity's tokens and a phrase-level embedding via a BiRNN over the ordered list of entity word-level embeddings; these two are fused (via weighted sum or a gating/selective mechanism) to form enriched entity node embeddings. Relation nodes and a global node are created in preprocessing. The graph (nodes including enriched entities, relation nodes, global node) is encoded with a Graph Transformer; decoding is performed by an attention-based RNN with a pointer/copy mechanism and a beam-search comparison penalty to reduce repetition.",
            "graph_type": "Knowledge graphs (entity-relation graphs derived from AGENDA and WebNLG; DBpedia subgraphs; automatically extracted scientific-abstract graphs)",
            "conversion_method": "Nodes (entities, relation nodes, global node, and optionally title-as-node) are encoded into contextual vectors by Graph Transformer; an RNN decoder generates text autoregressively attending to graph and title encodings, and may copy tokens from the graph via pointer mechanism; the process is an encoder–decoder graph-to-sequence conversion rather than naive linearization.",
            "downstream_task": "Knowledge-graph-to-text generation (abstract/summary generation and RDF/DBpedia graph description generation on AGENDA and WebNLG)",
            "performance_metrics": "AGENDA (Table 3): BLEU-1 33.19, BLEU-2 18.34, BLEU-3 11.26, BLEU-4 7.05. WebNLG (Table 4): BLEU 62.06. Selective-fusion ROUGE (Table 1/2): ROUGE-1 35.46, ROUGE-2 11.60, ROUGE-L 15.69 (reported on test set).",
            "comparison_to_others": "MEFR outperforms Graph Transformer, GAT, GCG and other listed baselines on AGENDA BLEU metrics and outperforms prior baselines on WebNLG BLEU (MEFR BLEU 62.06 vs Graformer 61.15 and Adapt 60.59); authors attribute gains to richer entity representations (multi-granularity fusion) plus the beam-search comparison mechanism that reduces redundancy.",
            "advantages": "Enriches entity representations via multi-granularity (word and phrase) fusion improving entity coverage and informativeness; reduces repetition via a similarity-aware beam-search scoring; achieves state-of-the-art (among compared non-pretrained methods) BLEU/ROUGE on the datasets used; does not require pretrained language models.",
            "disadvantages": "Adds modeling complexity (BiRNNs at two levels, gating parameters, extra preprocessing to create relation nodes), requires tuning of hyperparameters (α for fusion, δ for beam comparison), increases number of nodes due to relation-node conversion which can increase computation.",
            "failure_cases": "No explicit catastrophic failure case reported for MEFR; authors note general issues in the domain (repetition and uncovered entity information) that MEFR aims to mitigate. Dataset issues (e.g., disconnected components in automatically extracted AGENDA graphs) remain a challenge for graph-to-text methods and may limit performance.",
            "uuid": "e8826.0",
            "source_info": {
                "paper_title": "Enriched entity representation of knowledge graph for text generation",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Sum Fusion",
            "name_full": "Sum fusion mechanism (direct and weighted sum)",
            "brief_description": "A simple multi-granularity fusion that combines word-level and phrase-level entity embeddings either by direct summation or by a weighted sum controlled by α.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "sum fusion of word-level and phrase-level entity embeddings",
            "representation_description": "Two variants: direct sum h_p = h_w + h_p (Sum_i) and weighted sum h_p = α * h_w + (1-α) * h_p (Sum_e), where h_w is a BiRNN word-level embedding of the entity string and h_p is a BiRNN phrase-level embedding across entities.",
            "graph_type": "Knowledge graphs (entity nodes within the KG)",
            "conversion_method": "Per-entity fusion is applied before Graph Transformer encoding so nodes receive fused embeddings which the encoder then contextualizes for decoding.",
            "downstream_task": "Graph-to-text generation",
            "performance_metrics": "Table 2 (test set ROUGE): Sum_i: ROUGE-1 31.17, ROUGE-2 9.30, ROUGE-L 14.24. Sum_e (weighted): ROUGE-1 32.82, ROUGE-2 10.26, ROUGE-L 14.76.",
            "comparison_to_others": "Performs worse than the selective (gating) fusion mechanism in ROUGE scores; weighted sum (Sum_e) is better than direct sum but still inferior to selective gating.",
            "advantages": "Simple to implement, adds multi-granularity information without many extra parameters.",
            "disadvantages": "Global/constant weighting cannot adapt per-entity; less flexible than gating and yields lower generation quality.",
            "failure_cases": "Does not capture per-entity variable importance of word- vs phrase-level information; empirically underperforms the selective mechanism.",
            "uuid": "e8826.1",
            "source_info": {
                "paper_title": "Enriched entity representation of knowledge graph for text generation",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Selective Fusion",
            "name_full": "Selective mechanism (gating fusion)",
            "brief_description": "A highway-like gating fusion that dynamically weights word-level vs phrase-level entity embeddings per entity using a sigmoid gate.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "selective gating fusion of entity embeddings",
            "representation_description": "Compute gate s_i = σ(β1 * h_w + β2 * h_p + c) and fuse as h_pi = s_i * h_w + (1 - s_i) * h_p; β1, β2, c are learned parameters and σ is sigmoid. Variants (remove phrase gating or word gating) were evaluated.",
            "graph_type": "Knowledge graphs",
            "conversion_method": "Per-entity gated fusion performed prior to Graph Transformer encoding; produces d-dimensional node embeddings fed into the encoder.",
            "downstream_task": "Graph-to-text generation",
            "performance_metrics": "Table 1 (test set ROUGE): Selective mechanism: ROUGE-1 35.46, ROUGE-2 11.60, ROUGE-L 15.69. Variants: Selective w/o p ROUGE-1 33.71 ROUGE-2 10.58 ROUGE-L 14.85; Selective w/o w ROUGE-1 32.74 ROUGE-2 9.83 ROUGE-L 14.31.",
            "comparison_to_others": "Selective gating outperforms both sum variants and its ablated variants, indicating dynamic per-entity fusion is beneficial.",
            "advantages": "Adapts fusion per entity, better preserves multi-granular information, yields higher ROUGE/BLEU when used in MEFR.",
            "disadvantages": "Introduces gating parameters and slight extra computation over simple sums.",
            "failure_cases": "Removing gating on either side degrades performance (shown in ablation). No other specific failure mode reported.",
            "uuid": "e8826.2",
            "source_info": {
                "paper_title": "Enriched entity representation of knowledge graph for text generation",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Relation nodes",
            "name_full": "Relation-node conversion (edge-to-node transformation)",
            "brief_description": "A preprocessing representation that turns graph edges into explicit relation nodes (including forward and backward direction nodes) and adds a global node connecting entities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "relation-node transformation / graph preprocessing",
            "representation_description": "Each relation between two entities is represented by relation nodes (one per direction); a global node is also added to connect all entity nodes. Initial embeddings are learned for relation and global nodes.",
            "graph_type": "Knowledge graphs (DBpedia subgraphs, automatically extracted scientific KGs)",
            "conversion_method": "Graph preprocessing step: edges -&gt; relation nodes; after this transformation the resulting graph (entities + relation nodes + global node) is encoded by Graph Transformer.",
            "downstream_task": "Graph-to-text generation",
            "performance_metrics": "Not evaluated in isolation; used as part of MEFR and preprocessing following prior works (used on AGENDA/WebNLG).",
            "comparison_to_others": "Authors note this mirrors prior practice (cited [15,16]) to avoid parameter explosion when modeling many relation types and to enable explicit relation representation.",
            "advantages": "Makes relations explicit as vertices so the encoder can model relation semantics; helps control parameterization for many relation types.",
            "disadvantages": "Increases node count and computational cost; may create larger graphs to encode.",
            "failure_cases": "No specific failure modes reported in the paper; potential scaling limits on very dense graphs are implied.",
            "uuid": "e8826.3",
            "source_info": {
                "paper_title": "Enriched entity representation of knowledge graph for text generation",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Title-as-node",
            "name_full": "Title-as-node encoding",
            "brief_description": "A representation technique that treats an associated document title as an additional node: the title is encoded by a BiRNN and its encoding is attended alongside graph encodings during decoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "title-as-node",
            "representation_description": "Encode title text with a BiRNN to produce title embedding T; use this title encoding as an additional context vector (treated like a node) when computing decoder attention and final context c_t = concat(c_k, c_r).",
            "graph_type": "Knowledge graphs paired with titles (AGENDA/WebNLG instances that include title metadata)",
            "conversion_method": "Title is encoded separately and concatenated with graph context for decoding; not linearized into graph but used as auxiliary node/context.",
            "downstream_task": "Graph-to-text generation",
            "performance_metrics": "Not reported separately; title-as-node is part of MEFR encoder-decoder pipeline.",
            "comparison_to_others": "Used by MEFR to enrich decoding context; no direct ablation numbers reported for title presence/absence.",
            "advantages": "Provides a concise global cue relevant to the generated text; helps decoder select content.",
            "disadvantages": "Only applicable when titles exist; marginal extra encoding cost.",
            "failure_cases": "Not reported.",
            "uuid": "e8826.4",
            "source_info": {
                "paper_title": "Enriched entity representation of knowledge graph for text generation",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Graph Transformer",
            "name_full": "Text Generation from Knowledge Graphs with Graph Transformers",
            "brief_description": "A graph-aware adaptation of the Transformer that contextualizes node encodings by combining graph attention with transformer-style multi-head attention; used as the graph encoder in MEFR.",
            "citation_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "mention_or_use": "use",
            "representation_name": "Graph Transformer encoding",
            "representation_description": "Graph Transformer applies multi-head attention over node neighborhoods (and with mechanisms for global contextualization) to produce contextualized node embeddings suitable for downstream sequence generation.",
            "graph_type": "Knowledge graphs",
            "conversion_method": "Encodes graph nodes into contextualized vectors which are then consumed by an RNN decoder to produce text; not a linearization method but a graph-to-vector encoder.",
            "downstream_task": "Knowledge-graph-to-text generation",
            "performance_metrics": "Table 3 (AGENDA): BLEU-1 29.08, BLEU-2 14.93, BLEU-3 8.57, BLEU-4 4.62. (Used as baseline and as MEFR's encoder.)",
            "comparison_to_others": "Outperforms earlier GAT/GCG baselines in experiments; MEFR further improves over Graph Transformer by enriching entity representations and adding the beam comparison mechanism.",
            "advantages": "Provides global contextualization of nodes and improves on local-only graph encoders; strong baseline for graph-to-text tasks.",
            "disadvantages": "By itself still may miss covering all entity information and can produce repetition; benefits from improved node (entity) representations as shown by MEFR.",
            "uuid": "e8826.5",
            "source_info": {
                "paper_title": "Enriched entity representation of knowledge graph for text generation",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Beam-Comparison",
            "name_full": "Beam search-based comparison mechanism (similarity-augmented beam search)",
            "brief_description": "A decoding-time augmentation of vanilla beam search that penalizes candidate tokens that increase similarity to the existing generated sequence to reduce redundancy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "similarity-augmented beam search",
            "representation_description": "During beam search the score for a candidate token y_t is adjusted: score(y_t) = δ * score(y_t) - (1-δ) * comp(s* + y_t, s*), where comp() is cosine similarity between the sequence with and without the new token and δ is tuned. This penalizes candidates that make the output more similar to existing text (i.e., repetitive).",
            "graph_type": "Applies during decoding of text (independent of graph type)",
            "conversion_method": "Not a graph-to-text conversion per se but a decoding heuristic applied to the sequence generation step to influence which tokens are selected by beam search.",
            "downstream_task": "Text generation (graph-to-text)",
            "performance_metrics": "Tuned parameter δ: best δ=0.4 on AGENDA (gives best ROUGE-2) and δ=0.5 on WebNLG as reported; authors report improved ROUGE-2 vs vanilla beam search (δ=1) and improved overall generation quality in human evaluations.",
            "comparison_to_others": "Authors show that vanilla beam search (δ=1) yields worse ROUGE-2 and more redundancy; the comparison mechanism with tuned δ improves ROUGE-2 and reduces repetition.",
            "advantages": "Reduces redundancy and repetition in generated text and improves automatic and human-evaluated metrics.",
            "disadvantages": "Adds per-candidate similarity computation and a hyperparameter δ that requires tuning; overly strong penalization (e.g., δ=0) degrades quality.",
            "failure_cases": "When δ = 0 (full reliance on comp term) quality is poor; must balance selection vs redundancy penalty.",
            "uuid": "e8826.6",
            "source_info": {
                "paper_title": "Enriched entity representation of knowledge graph for text generation",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "EntityWriter",
            "name_full": "",
            "brief_description": "A baseline mentioned that uses only entity strings and title for generation without modeling graph relations; reported as the weakest-performing baseline on AGENDA.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "entity-and-title-only representation",
            "representation_description": "Input consists only of entity tokens and title (no relation nodes or explicit relation modeling); fed into encoder-decoder to generate text.",
            "graph_type": "Knowledge graphs (but relations ignored; only entities used)",
            "conversion_method": "Serialize entities (and title) as input sequence for encoder-decoder instead of encoding full graph structure.",
            "downstream_task": "Graph-to-text generation",
            "performance_metrics": "Table 3 (AGENDA): BLEU-1 24.12, BLEU-2 10.27, BLEU-3 4.38, BLEU-4 3.09.",
            "comparison_to_others": "Performs poorest among listed models on AGENDA; authors attribute poor performance to ignoring graph relations.",
            "advantages": "Simpler input representation and fewer parameters.",
            "disadvantages": "Misses relational structure and yields substantially lower BLEU scores.",
            "uuid": "e8826.7",
            "source_info": {
                "paper_title": "Enriched entity representation of knowledge graph for text generation",
                "publication_date_yy_mm": "2022-11"
            }
        },
        {
            "name_short": "Adapt / Linearization",
            "name_full": "Adapt",
            "brief_description": "A prior neural encoder-decoder approach for WebNLG that linearizes the input graph/triples into a sequence and uses sub-word representations; a strong baseline on WebNLG.",
            "citation_title": "",
            "mention_or_use": "mention",
            "representation_name": "linearization / serialization of graph into sequence",
            "representation_description": "Convert triples/graph into a linear sequence (serialized representation) and feed into a sequence encoder (with sub-word tokens) so that standard seq2seq models can be applied.",
            "graph_type": "Knowledge graphs (RDF/DBpedia subgraphs as in WebNLG)",
            "conversion_method": "Triple/graph linearization into ordered sequence; then passed through encoder-decoder model.",
            "downstream_task": "Graph-to-text generation (WebNLG)",
            "performance_metrics": "Table 4 (WebNLG): BLEU 60.59 (reported for Adapt baseline).",
            "comparison_to_others": "Competitive baseline; MEFR slightly outperforms Adapt on WebNLG (MEFR BLEU 62.06).",
            "advantages": "Simple, leverages powerful sequence models and sub-word modeling; strong empirical performance on WebNLG.",
            "disadvantages": "Linearization can obscure explicit graph relations and global structure; may not scale well to very large or complex graphs where structure matters.",
            "uuid": "e8826.8",
            "source_info": {
                "paper_title": "Enriched entity representation of knowledge graph for text generation",
                "publication_date_yy_mm": "2022-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Graph attention networks",
            "rating": 2,
            "sanitized_title": "graph_attention_networks"
        },
        {
            "paper_title": "Graph2seq: graph to sequence learning with attention-based neural networks",
            "rating": 2,
            "sanitized_title": "graph2seq_graph_to_sequence_learning_with_attentionbased_neural_networks"
        },
        {
            "paper_title": "Graph-to-sequence learning using gated graph neural networks",
            "rating": 2,
            "sanitized_title": "graphtosequence_learning_using_gated_graph_neural_networks"
        },
        {
            "paper_title": "The webnlg challenge: Generating text from rdf data",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_rdf_data"
        },
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 2,
            "sanitized_title": "deep_graph_convolutional_encoders_for_structured_data_to_text_generation"
        },
        {
            "paper_title": "Modeling global and local node contexts for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "modeling_global_and_local_node_contexts_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "Get to the point: Summarization with pointer-generator networks",
            "rating": 1,
            "sanitized_title": "get_to_the_point_summarization_with_pointergenerator_networks"
        }
    ],
    "cost": 0.020921000000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enriched entity representation of knowledge graph for text generation
(2023) 9:2019-2030</p>
<p>Kaile Shi 
Complex &amp; Intelligent Systems</p>
<p>Xiaoyan Cai 
Complex &amp; Intelligent Systems</p>
<p>Libin Yang 
Complex &amp; Intelligent Systems</p>
<p>Jintao Zhao 
Complex &amp; Intelligent Systems</p>
<p>Enriched entity representation of knowledge graph for text generation
(2023) 9:2019-203010.1007/s40747-022-00898-0Received: 16 November 2021 / Accepted: 17 October 2022 / Published online: 4 November 2022O R I G I N A L A R T I C L EText generation · Knowledge graph · Multi-level entity fusion representation model · Comparison mechanism
Text generation is a key tool in natural language applications. Generating texts which could express rich ideas through several sentences needs a structured representation of their content. Many works utilize graph-based methods for graph-to-text generation, like knowledge-graph-to-text generation. However, generating texts from knowledge graph still faces problems, such as repetitions and the entity information is not fully utilized in the generated text. In this paper, we focus on knowledgegraph-to-text generation, and develop a multi-level entity fusion representation (MEFR) model to address the above problems, aiming to generate high-quality text from knowledge graph. Our model introduces a fusion mechanism, which is capable of aggregating node representations from word level and phrase level to obtain rich entity representations of the knowledge graph. Then, Graph Transformer is adopted to encode the graph and outputs contextualized node representations. Besides, we develop a vanilla beam search-based comparison mechanism during decoding procedure, which further considers similarity to reduce repetitive information of the generated text. Experimental results show that the proposed MEFR model could effectively improve generation performance, and outperform other baselines on AGENDA and WebNLG datasets. The results also demonstrate the importance to further explore information contained in knowledge graph.</p>
<p>Introduction</p>
<p>Natural text generation refers to the task of automatically producing texts from linguistic and non-linguistic input [1]. According to the style of input data, text generation can be categorized as text-to-text generation methods [2], data-totext generation methods [3], and image-to-text generation methods [4].</p>
<p>In specific domains like medical or scientific area, it is hard to generate texts which express complex ideas of content with a reasonable and logical structure. Some researches address this issue with a structured representation of input, which can be benefit to understand the content [5]. They utilize rule-based methods or template-based methods for structured-data-to-text generation [6][7][8]. These methods are usually easy to guarantee correctness of the generated texts' content, due to their interpretability and controllability. However, they also face some limitations, i.e., high-quality template is hard to extract without manual process; generated content often meets with problems in terms of diversity, fluency, and consistency. Recent neural network-based generation methods are driven by data, and they do not require much manual intervention and mainly rely on representation learning, to select appropriate content and express grammatically [9]. Although structured input could provide more additional guidance for generation [10,11], neural networkbased generation methods still have a variety of logical errors, like hallucinating statements which are not supported by the facts contained in the input, and confusing the output location of different information.</p>
<p>Therefore, researchers began to focus on graph-based neural network methods aiming to effectively capture global structure of the input, and preserve more original information to overcome the above issues [12][13][14][15]. For example, Koncel-Kedziorski et al. [16] proposed a Graph Transformer that extends Transformer [17] for encoding the input graph, built on graph attention network (GAT) [18] architecture. Although graphs could effectively capture both global and local structure of the input as well as further improving generation performance, the generated text are still affected by repetitions, and at the same time, entities which act as a key part of the graph are not fully covered in generated text.</p>
<p>In this paper, we focus on knowledge-graph-to-text generation, and propose a multi-level entity fusion representation (MEFR) model, which aims to address issues of repetitions and entity information is not fully covered in the generated text, further enhancing generation performance. First, we follow the similar procedure with previous work to pre-process the input knowledge graph, where a vertex denotes an entity node, or a relation node which is created for each edge relation between two entities, or a global node which connects all entity nodes. For the processed knowledge graph, we propose a fusion mechanism by aggregating node information from word level and phrase level to obtain entity representations in the graph. Then, we apply Graph Transformer [16] to encode the input knowledge graph and obtain the contextualized representation for each node. When decoding, vanilla beam search [19,20] is adopted, which is a global optimum-based search algorithm, that usually applying in text generation to select the results with top-k scores. To further reduce redundancy of the generated text, we develop a vanilla beam search-based comparison mechanism, which considers whether adding the generating word to the generated word sequence based on similarity. Experimental results show that our proposed MEFR model could effectively improve quality of the generated text. Three main contributions of this paper are:</p>
<p>-Multi-level fusion mechanisms are developed, i.e., sum fusion mechanism and selective mechanism, which aggregate information from word level and phrase level to obtain entity representations. -A comparison mechanism during generation is proposed, which considers similarity between the generated sequence with and without the generating word, tackling the constraints of redundancy and enhancing the performance of generation. -Thorough experimental studies are conducted to verify the effectiveness of the proposed model, and our proposed model which achieves great performance without pre-trained language models also illustrates the importance of further exploring the information contained in knowledge graph.</p>
<p>The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 explains the proposed MEFR model. Section 4 presents the experiments and evaluation results. Conclusions are presented in Sect. 5.</p>
<p>Related work</p>
<p>For structured-data-to-text generation, the core task is to generate a textual descriptions based on structural knowledge records. Some generation systems rely on rules and hand-engineered templates. Angeli et al. [21] constructed a domain-independent model, which manually designs a template to introduce knowledge of other domains for table to text generation. The model makes it easy to incorporate domain-specific knowledge which can improve generation performance. Kondadadi et al. [22] proposed a system which generates different content based on a specific domain. The system is also used statistical data of the text in addition, but it is restrained by requiring a lot of historical data. Howald et al. [23] presented a hybrid natural language generation system that utilizes Discourse Representation Structures (DRSs) for statistically learning syntactic templates. This model could generate acceptable texts for a number of different domains. Wiseman et al. [7] used hidden semi-Markov model (HSMM) to model text generation template and combined end-to-end methods with traditional template-based methods. On the other hand, many works focus on neural network-based endto-end model in recent years. Mei et al. [3] used a neural encoder-decoder model to generate weather forecasts and soccer commentaries, and they also added an aligner to select important information based on end-to-end model. Juraska et al. [24] proposed a deep ensemble framework for text generation, which integrates sequence-to-sequence model based on bidirectional LSTM and CNN. This framework also used an automatic slot alignment-based reranking method which helps improve quality of the generated text. Gehrmann et al. [25] introduced multiple decoders to fit different data based on traditional encoder-decoder model. And in this way, the model could be used to generate different expressions for different types of text. Freitag et al. [26] interpreted structured data as a corrupt representation of the desired output, and used a denoising auto-encoder to reconstruct the sentence. The result shows that denoising auto-encoder could generalize to generate correct sentences when given structured data.</p>
<p>Although structured input could provide more guidance and structural information for generation, it is still restrained by how to better make use of the structure. Many researches began to focus on graph-based methods which can better capture local and global structure of input. Xu et al. [12] proposed a graph-to-sequence neural network model, which illustrates the structured input information is important for text generation, solving the problem of structural information loss caused by traditional graph-to-text generation methods. Beck et al. [13] used an encoder based on Gated Graph Neural Network (GGNN) which can integrate the complete graph structure without losing information, and introduce graph transformation providing more information for the attention and decoding modules in the network. Li et al. [14] modeled news as a topic interaction graph, which better understands internal structure of the article and the connection between topics. Xu et al. [27] converted SQL into a directed graph and used a graph-to-sequence model to translate the graph into a sequence. Koncel-Kedziorski et al. [16] proposed a Graph Transformer model to encode the knowledge graph, used for generating a text that can express the content of the knowledge graph. Song et al. [28] leveraged richer training signals to guide the model to preserve original information, tackling the problem of messing up or even dropping the core structural information of input graphs during generation. Based on graph convolutional networks, Guo et al. [29] developed a novel network named DCGCNs, achieving advanced performance on AMR-to-text generation. Ribeiro et al. [15] presented four neural models, which could combine both local and global contextual information for graph encoding. Despite their success, how to effectively utilize more information within graph for text generation is still an open problem.</p>
<p>To obtain more information of the knowledge graph for generation, we develop an MEFR model to obtain entity representations in the graph, by proposing fusion mechanisms to incorporate information from word-level and phrase-level representations. The proposed fusion mechanisms could enrich entity representations based on the above two-level information as well as improving generation performance. Figure 1 shows the framework of our proposed MEFR model. The input of the model is a knowledge graph corresponding to the document, and the title within the graph if it exists. We follow the previous work [15,16] to pre-process the input knowledge graph denoting as G = (V, E). V denotes a vertex set containing three types of nodes: entity nodes, relation nodes which represent relations between two entity nodes, and a global node which connects all entity nodes. E is an adjacency matrix describing the directed edges. The input graph and the title are encoded using a Graph Transformer [16] and a bidirectional recurrent neural network [30], respectively. We treat the title as an additional node, and use both node representations within the graph and title representation for decoder. When decoding, we take attention-based RNN [31] as decoder and adopt copy mechanism [32] for generation. The final output of MEFR model is the generated descriptive text. Details of the model will be illustrated in this section.</p>
<p>Multi-level entity fusion representation (MEFR) model</p>
<p>Encoder</p>
<p>Node embeddings</p>
<p>There are three types of nodes in the graph, i.e., entity nodes, relation nodes and a global node. As each relation is represented as both a forward direction relation node and a backward direction relation node, we learn two embeddings per each relation node. We also learn an initial embedding for the global node. However, entities in scientific texts are often multi-word expressions, we use BiRNN to obtain the embedding of each entity based on word embeddings as
h w p j = BiRNN x 1 p j , x 2 p j , . . . , x i p j , . . . , x t p j ,(1)
where x i p j is i-th word embedding of entity p j , t denotes the number of words in p j . The last hidden state is used as the word-level representation of the entity p j , denoted as h w p j . Besides, there exist relationships among entities, such as sequential relationships and logical relationships. For example, the appearance position of entities in the input is chronological and some entities always appear before or behind other entities. Based on the above analysis, we aim to capture more information for entity representations based on relationships among entities.</p>
<p>Compared to word-level entity embeddings, we also apply BiRNN which is applied to each entity to capture the dependency, and obtain phrase-level representations for entities, as
h p p j = BiRNN h w p 1 , . . . , h w p m ,(2)
where m is the number of entities in the knowledge graph, and h p p j is the phrase-level entity representation of p j . Then, we propose two fusion mechanisms, i.e., sum fusion mechanism and selective mechanism, to integrate information from word-level representation and phrase-level representation of each entity. Although word embeddings are the same for the above two-level representations, the choice of context is changed when information is fused.</p>
<p>• Sum fusion mechanism</p>
<p>We develop two methods for sum fusion mechanism. We first use a sum operation to fuse the above word-level and phrase-level entity representations as
h p i = h w p i + h p p i .(3)
As we think different level information may have different importance for entity representations, then we give different weights for the two entity representations in sum 
h p i = αh w p i + (1 − α)h p p i ,(4)
where α is a weight balancing the word-level representations and phrase-level representations.</p>
<p>• Selective mechanism</p>
<p>Inspired by highway networks with gating mechanism [33], which could fuse features by adopting two gating functions to scale and combine hidden states from two sources, and generate one representation, we develop a selective mechanism to dynamically control and indicate how much information are incorporated from the two-level entity representations, respectively. It can be illustrated as
s i = σ β 1 h w p i + β 2 h p p i + c(5)h p i = s i h w p i + (1 − s i ) h p p i ,(6)
where s i is gate weight to control how much information incorporated from two levels, β 1 , β 2 are learnable parameters that model relations of parameters and c is the bias, σ denotes sigmoid function, and denotes element-wise multiplication.</p>
<p>To further validate the effectiveness of the selective mechanism, we also utilize two variants of it, which can be listed as
h 1 p i = s i h w p i + h p p i (7) h 2 p i = h w p i + s i h p p i .(8)
Equations 7 and 8 represent removing selective mechanism of phrases and words, respectively. Based on the above procedures, we obtain a d-dimensional representation of each node in the knowledge graph.</p>
<p>BiRNN encoder and graph transformer encoder</p>
<p>The input of the encoder is a knowledge graph and a corresponding title (if the graph contains a title). They are encoded by a Graph Transformer encoder [16] and a BiRNN encoder, respectively.</p>
<p>The title is also a short string, and we encode it using BiRNN to produce title embedding
T = BiRNN(x 1 , . . . , x i , . . . , x k ), where x i is i-th word embedding of the title.
We use Graph Transformer [16] to encode knowledge graph, which incorporates global structural information when contextualizing vertices in their local neighborhoods, and the resulting encodings are regarded as graph contextualized node encodings, i.e., G = GraphTransformer(h 1 , h 2 , . . . , h n ), h i is i-th node embedding of the graph and n is the number of nodes in the graph.</p>
<p>Decoder</p>
<p>We adopt attention-based RNN [34] as the decoder of our model. At each decoding timestep t, we use the decoding hidden state h t to calculate the context vectors c k and c r for knowledge graph and title, respectively. c k is calculated by
c k = h t + Multihead h t , G j Multihead(Q, K) = concat (head 1 , . . . , head n ) head i = j∈l Attention q i , k j W n G k j (9) Attention q i , k j = exp W k k j T W q q i m∈l exp (W k k m ) T W q q i · 1 √ d ,
where l denotes the neighborhood of the node q i in graph, Attention() is the attention mechanism parameterized per head [16], W G ∈ R d×d is a weight matrix, (W q , W k ) ∈ R d×d are learned independent transformations matrix of q and k, respectively, 1 √ d is a scaling factor to counteract the effect of gradient flow when dot products, head 1 , . . . , head n is n attention heads. c r is computed similarly using title encodings T.</p>
<p>The final context vector c t is obtained by concatenating c k and c r , denoted as
c t = concat (c k , c r ) .(10)
Then, we use c t and decoding state h t as input for the next decoding step.</p>
<p>Copy mechanism</p>
<p>To enhance diversity of words and avoid out-of-vocabulary problem in generation, we compute a probability p gen of copying from the input using h t and c t in a similar way with See et al. [32], as it allows copying words from vocabulary or knowledge graph. The probability p gen ∈ [0, 1] for timestep t is calculated as
p gen = σ W copy h t ||c t + b ,(11)
where W copy is a learnable parameter that transforms the concatenated vector, b is the bias, and σ is the sigmoid function. Next, p gen is used as a soft switch to choose selecting a word from the vocabulary by sampling from P vocab , or copying entity from the input graph by sampling from the attention distribution P copy . The probability distribution over the extended vocabulary, which is the union of the fixed vocabulary and input knowledge graph, is defined as
p gen * P copy + 1 − p gen * P vocab ,(12)
where P copy is calculated as P copy i = Attention([h t ||c t ], x i ) and x i ∈ T||G, P vocab is computed by scaling [h t ||c t ] to the vocabulary size and taking a softmax function.</p>
<p>Decoding algorithm</p>
<p>We use beam search algorithm during generation. As we found that there exists repetition problem in generated text, we develop a comparison mechanism based on vanilla beam search algorithm [19,20]. Our proposed comparison mechanism additionally calculates similarity between the word sequence adding current generating word and original word sequence, to update the score of word when beam search. The score of generating word is defined as
score(y t ) = δ · score(y t ) − (1 − δ) · comp s * + y t , s * ,(13)
where y t is the generating word at the timestep t, comp is the cosine similarity function calculating the similarity between two texts, s * is the generated word sequence, and δ is the weighting factor. Based on the vanilla beam search, we add the second term in Eq. 13 to calculate the similarity between the sequences adding and without adding generating word, to punish the word which improves the similarity and reduce the redundancy.</p>
<p>Experiments</p>
<p>Dataset</p>
<p>We focus on generation task which generates corresponding text from knowledge graph in this paper. Therefore, we evaluate our model on two popular graph-to-text datasets: AGENDA [35] and WebNLG [36]. Dataset), which consists of 40k paper titles and abstracts from Semantic Scholar Corpus taken from the proceedings of 12 top AI conferences. The average length of title and abstract are 9.9 words and 141.2 words, respectively. We follow the same procedure with Koncel-Kedziorski et al. [16] to create a knowledge graph for each abstract, and obtain a dataset of knowledge graphs paired with scientific abstracts. The average number of nodes and edges in the knowledge graph is 12.42 and 4.43, respectively. The dataset is split into a training/validation/test of 38720/1000/1000.We pre-process the dataset by replacing low-frequency words (words occurs fewer than 5 times) with <unk> tokens. In post-processing step, we delete repeated sentences and coordinated clauses.</p>
<p>AGENDA (Abstract Generation</p>
<p>WebNLG, which is also used for knowledge-graph-to-text generation task. Each instance in WebNLG contains a KG (knowledge graph) from DBPedia [37] and a corresponding text with one or several sentences describing the graph. The WebNLG dataset is split into 18,102, 872 and 971 instances for training, validation and test, respectively. Besides, graphs in AGENDA are automatically extracted, which leads to a high number of disconnected graph components. Compared with graphs in AGENDA, graphs in WebNLG are humanauthored subgraphs of DBPedia. It means that the graph in WebNLG is more complete and more consistent with the content of corresponding target text. The relation types in WebNLG are 373, the average nodes are 34.9, and the average edges are 101. For WebNLG, we follow the previous work [36] to pre-process the knowledge graph. Besides, we refer [15] to deal with considerable number of edges and relations, avoiding parameter explosion, and create relation nodes to transform relational edges between entities which is similar to AGENDA.</p>
<p>Implementations</p>
<p>For AGENDA dataset, We employ LSTM [38] as Recurrent Neural Network, and apply a layer of bidirectional LSTM for title representation and each level of entity representations in the encoder-decoder framework, respectively. The dimension of the hidden vectors is set as 500. Models are trained for 20 epochs with early stopping [39] based on validation loss on an NVIDIA Tesla V100. Beam width is set as 4. The loss function is the negative log likelihood of generating text over the target text vocabulary and copied entity indices. Settings of SGD [40] optimization are applied to optimize the model parameters, and the related settings of Graph Transformer are set the same as in [16].</p>
<p>For WebNLG dataset, models are evaluated on the test set with seen categories. To implement our models, we employ two layers of bidirectional LSTM for each level of entity representations in encoder-decoder framework. We train our models with SGD optimizer for 100 epochs on WebNLG using an NVIDIA Tesla V100. The dimension of hidden encoder states is 256, and we train our models by minimizing negative log-likelihood loss function. The final results are generated by beam search, and beam width is set to 3.</p>
<p>Evaluation</p>
<p>We use BLEU [41] and ROUGE [42] as automatic evaluation metrics. Specifically, we use BLEU-n (n = 1, 2, 3, 4) in our experiments. And for ROUGE metric, we use ROUGE-1 and ROUGE-2 to assess informativeness, as well as ROUGE-L to assess fluency.</p>
<p>Parameter setting</p>
<p>In the first set of experiment, we examine and fix the values of parameters α in sum fusion mechanism and δ in comparison mechanism. We tune the values of α and δ from 0 to 1 with step size 0.1 when the model is trained.</p>
<p>Setting and analysis of parameter˛in sum fusion mechanism</p>
<p>From Fig. 2, we can see that when α = 0, the entity representation only contains phrase-level information. When increasing the value of α, the entity representation began to incorporate both word-level information and phrase-level information, which makes the model utilize rich entity information. The best ROUGE-2 score obtained at α = 0.8 and we use it in the following experiments.</p>
<p>Setting and analysis of parameter ı in comparison mechanism</p>
<p>Then, we tune the parameter δ to obtain better performance of generation. We can see from Fig. 3 that when δ = 0, the comparison mechanism is decided by comp function and quality of the generated text is not good enough. When changing δ value, the ROUGE-2 score changes fast at the beginning and gets best when δ = 0.4, and then, it starts to decrease smoothly. when δ = 1, the mechanism is based on vanilla beam search algorithm and the result is worse than the performance with δ = 0.4. It illustrates that vanilla beam search algorithm could select important words, but it is restrained by redundancy. When we add comp function as second term to beam search algorithm, the comp function will consider similarity between the sequence with and without generating word to improve the quality of generated text. According to the result, the performance of the comparison mechanism is effectively improved when using a proper δ value. The opti- </p>
<p>Ablation study</p>
<p>To explore effectiveness of MEFR model with different fusion mechanisms, we conduct experiments using different fusion mechanisms and their variants. For fair comparison, except the fusion methods, all the other processes involved remain the same. Table 1 shows generation performance using selective mechanism and its two variants, i.e., Selective w/o p (removing selective mechanism of phrases) and Selective w/o w (removing selective mechanism of words). It indicates that the complete selective mechanism could incorporate both wordlevel and phrase-level information dynamically rather than just select information from one level. That is, information from the two levels can be fused through selective mechanism to jointly improve the generation performance. Table 2 shows generation performance using different fusion mechanisms, including the sum fusion mechanism, i.e., direct sum (Sum_i) and weighted sum (Sum_e), as well as the selec- tive fusion mechanism. The results show that though direct sum and weighted sum mechanisms could fuse information from the two levels, selective mechanism could better fuse word-level and phrase-level information dynamically, further enhancing generation performance. In the following experiments, we use selective mechanism as fusion mechanism of the model.</p>
<p>Selective mechanism and its variants</p>
<p>Comparison of different fusion mechanisms</p>
<p>Comparison with other generation models</p>
<p>We first compare our proposed MEFR model with other generation models on AGENDA dataset: (1) GAT [18], which is an Attention-Based Graph Neural Networks used for graph encoding. (2) Graph Transformer [16], which encodes knowledge graph based on Transformer [17] and GAT [18]. (3) EntityWriter [16], which only uses entities and title for generation without considering graph relations. (4) GCG [43], which is a graph convolutional networksbased model that explicitly considers the local node contexts within input structure. (5) PGE [15], which is a fully parallel structure based on GAT for global and local node encoding. (6) GT+RMA [44], which combines repulsive multi-head attention based on Graph Transformer [16] for text generation from knowledge graph. (7) Graformer [45], which is an encoder-decoder architecture based on transformer used for graph-to-text generation. (8) PGE-LW [15], which is a layer-wise parallel graph encoder based on GAT for node encoding. Table 3 shows performance of different generation models on the AGENDA dataset. EntityWriter performs poorest among these models; this can be due to it does not consider the graph relations. GAT and GCG could model the input graph structure and learn node representations, but they are still restrained by considering more semantic information and node relations. While Graph Transformer allows for a more global contextualization of each vertex through the use of a transformer-style architecture, further improving performance of knowledge-graph-to-text generation. However, it still misses a few entity information in the generated text according to the experiments. PGE improves the performance with the parallel structure based on GAT, which indicates the advantage of considering richer graph information. PGE-LW which combines the encoder in a layer-wise fashion does not improve performance compared with PGE.</p>
<p>To strengthen model's expression ability, GT+RMA introduces repulsive multi-head attention based on Graph Transformer, but it does not bring significant improvement of the performance compared with Graph Transformer. Graformer achieves competitive performance using a novel graph selfattention based on Transformer for graph encoding, which can be used to detect global patterns. It also indicates the importance of effectively considering relations between nodes in knowledge graph for node representations. Different from the above models, we note that the repetitions and uncovered problem of entities are existed in the generated text. Our proposed model could effectively model the entity in knowledge graph from different granularities, which is able to extract more information and richer relations of entities, and make full use of the information in knowledge graph for representation learning. Our proposed MEFR model outperforms other baselines in terms of Bleu metrics. This could be attributed to that MEFR not only takes richer entity representations of the knowledge graph into account, but also introduces comparison mechanism to improve quality of the generated text. Besides, to further validate the effectiveness of our proposed model, we compare our model with several representative generation models on WebNLG dataset, which is also used for graph-to-text generation and graphs contained in WebNLG are more complete compared to AGENDA. The models used for comparison are listed as follows:</p>
<p>(1) UPF-FORGe [36]: a rule-based method which mostly focuses on using predicate-argument templates during sentence planning. (2) Adapt [36]: a neural encoder-decoder based framework with utilizing sub-word representations and linearizing the input sequence. (3) Melbourne [36]: which combines delexicalization and enrichment of the input sequence with attentional encoderdecoder model. (4) Graph Conv [43]: which is a graph convolutional networkbased encoder directly utilizing the input graph structure. (5) E2EGRU [46]: which takes end-to-end architecture based on GRU for data-to-text generation without explicit intermediate representations. (6) GTR-LSTM [47]: which is a sentence generation model with the novel graph-based triple encoder. (7) SBS [48]: which proposes to split generation procedure into a symbolic text-planning stage and a neural generation stage.</p>
<p>We also use Graformer [45] as a comparison model. Like the models we compare with, we report Bleu scores rather than Bleu-n on WebNLG, and the results for comparison are taken from their corresponding paper or obtained by running publicly released source code. The results are shown in Table 4. Table 4 shows the results of different generation models on WebNLG test set with seen categories. The first three models are advanced competitors in WebNLG challenge with seen categories. Among them, we can see Adapt and Melbourne which are based on attentional encoder-decoder show greater performance; it indicates the advantages of neural network-based models compared with rule-based models. For the fourth to seventh models, Graph Conv directly utilizes the input graph structure with graph convolutional network-based encoder. E2EGRU uses an end-to-end datato-text model based on GRU, to generate text without explicit intermediate representations. GTR-LSTM proposes a novel graph-based triple encoder to preserve more information from original data for data-to-text generation. And SBS further splits generation procedure into two stages for generating high-quality text. These models achieve good performance and show benefits of explicitly encoding the input graph structure. However, they are still restrained by effectively utilizing semantic information and node relations of the input graph. Transformer-based Graformer shows great performance which learns node representations not only relying on their neighbors, but also focusing on global patterns based on the novel graph attention. It indicates the advantages of effectively considering relations of nodes in the knowledge graph. Compared with Graformer, our proposed model could learn interactions of nodes with global patterns based on Graph Transformer. Besides, we especially focus on modeling relations of entities, and learning their representations from aggregating different granularity information to generate high-quality text. Our proposed model achieves best performance among the baselines, which proves that our model could obtain richer information of knowledge graph for entity representations, and utilize comparison mechanism to help improve quality of the generated text. Besides, graphs in WebNLG are more complete compared with AGENDA, which means that richer semantic information of entities is contained in the graph. And it can be effectively utilized by our model to enhance the performance. Moreover, our model could outperform other baselines without pre-trained language models, which also indicates the importance of further exploring the information contained in knowledge graph.</p>
<p>Human evaluation and case study</p>
<p>We perform human evaluations to establish that the Bleu improvements of our proposed MEFR model are correlated  Table 6 Examples of generated texts Title: "Towards Internet-scale multi-view stereo."</p>
<p>Gold: this paper introduces an approach for enabling existing multiview stereo methods to operate on extremely large unstructured photo collections. the main idea is to decompose the collection into a set of overlapping sets of photos that can be processed in parallel, and to merge the resulting reconstructions. this overlapping clustering problem is formulated as a constrained optimization and solved iteratively. the merging algorithm, designed to be parallel and out-of-core, incorporates robust filtering steps to eliminate low-quality reconstructions and enforce global visibility constraints. the approach has been tested on several large datasets downloaded from flickr.com, including one with over ten thousand images, yielding a 3d reconstruction… Graph Transformer: in this paper, we propose a new method for 3d reconstruction in unstructured photo collections. in the proposed method, a merging algorithm is used to solve the overlapping clustering problem. the method is applied to the problem of 3d reconstruction in flickr.com… GAT: in this paper, we present a new approach for 3d reconstruction in unstructured photo collections. and merging algorithm can be used for overlapping clustering problem with merging algorithm. the presented approach has been used for 3d reconstruction in flickr.com… MEFR model: in this paper, we address the problem of 3d reconstruction in unstructured photo collections . we propose a novel merging algorithm based on constrained optimization of the overlapping clustering problem. unlike multi-view stereo methods, we are able to 3d reconstruction in flickr.com with global visibility constraints. furthermore, we show that global visibility constraints can be very useful for overlapping clustering problem in flickr.com…</p>
<p>The bold words are entity information contained in the Gold. The italic words in generated texts are included entities. The bold and italicized words are repeated parts with human judgments. We randomly select 40 samples from test set and compare the text generated by our method with the text generated by GAT and Graph Transformer. We ask three volunteers to rate these samples on a scale of 5 (very good) to 1 (very poor), in terms of informativeness, fluency, and redundancy of each text. The three volunteers are specialists (including a professor, two associate professors) from School of International Studies, Shaanxi Normal University. The average results are listed in Table 5. Informativeness represents that the generated text should include rich information, fluency represents that sentences in the text should be expressed fluently and logically, and redundancy represents that the text should contain few repeated information. Table 5 shows that our proposed MEFR model outperforms the other two models on three aspects, especially in informativeness. Compared with Graph Transformer, the text generated by MEFR is more informativeness, indicating the advantages of fusion methods.</p>
<p>Besides, we show an example of generated text by the three models in Table 6. Compared with GAT, Graph Transformer could generate a more fluent and informative text with the help of global contextualization. It is not surprising to find that our proposed MEFR model gets best scores of informativeness obviously, and the generated text contains more details description as well as entity information, which makes the text more complete and readable compared with the other methods. It indicates that by integrating information from different levels of entity, our proposed MEFR model could generate text containing more information, and better utilize the information of knowledge graph to produce rich description which is different from the textual expressions produced by other two models.</p>
<p>Conclusion and future work</p>
<p>In this paper, we focus on knowledge-graph-to-text generation task which generates corresponding descriptive text from knowledge graph. However, the generated text often suffers from problems, such as redundancy and not fully utilizing entity information, which leads low quality of the generated text. Therefore, we propose an MEFR model to solve the above issues, aiming to generate the text with rich description (covering information contained in knowledge graph as much as possible) and low redundancy (containing less repetitive information). Our proposed MEFR model effectively incorporates information from different levels for obtaining entity representations in knowledge graph. Besides, the proposed comparison mechanism in decoding procedure is used to reduce redundancy of the generated text based on similarity. According to the results on the two popular graph-to-text generation datasets, our proposed model could achieve advanced performance and improve quality of the generated text. At the same time, our model which does not use pre-trained language models shows great performance compared with other generation models. It also means the importance of further exploring information contained in the knowledge graph. And for our proposed model which combines multigranularity information can make more effective use of the original input for representation.</p>
<p>In the future, we will continue exploring how to better utilize information from different granularities in complex networks, to further improve the performance of text generation. Besides, pre-trained language models show great performance on natural language generation, and we will explore to enrich node representations in knowledge graph with pre-trained language models for generation. In addition to improving performance of the generation model, the dataset used for knowledge-graph-to-text generation is still worth to focus on. And we will try to make the dataset of knowledge graphs paired with texts in specific fields, to further study the effect of fusion representation in graph-to-text generation.</p>
<p>Automation, Northwestern Polytechnical University, Xi'an, Shaanxi, People's Republic of China</p>
<p>Fig. 1
1Framework of MEFR model operation, that is</p>
<p>Fig. 2
2ROUGE-2 scores vs. α on the test set mal values of parameter α and δ on WebNLG dataset can be obtained in a similar way, and the best value is α = 0.6, δ = 0.5.</p>
<p>Fig. 3
3ROUGE-2 scores vs. δ on the test set</p>
<p>Table 1
1Results of selective mechanism and its variants on the test setMethods 
ROUGE-1 
ROUGE-2 
ROUGE-L </p>
<p>Selective w/o w 
32.74 
9.83 
14.31 </p>
<p>Selective w/o p 
33.71 
10.58 
14.85 </p>
<p>Selective mechanism 
35.46 
11.60 
15.69 </p>
<p>Table 2 Results of different fusion mechanism on the test set </p>
<p>Mechanisms 
ROUGE-1 
ROUGE-2 
ROUGE-L </p>
<p>Sum_i 
31.17 
9.30 
14.24 </p>
<p>Sum_e 
32.82 
10.26 
14.76 </p>
<p>Selective mechanism 
35.46 
11.60 
15.69 </p>
<p>Table 3
3Results of different generation models on AGENDA test setMethods 
Bleu-1 
Bleu-2 
Bleu-3 
Bleu-4 </p>
<p>EntityWriter 
24.12 
10.27 
4.38 
3.09 </p>
<p>GCG 
26.12 
12.91 
6.54 
3.62 </p>
<p>GAT 
26.51 
13.19 
6.79 
3.71 </p>
<p>Graph Transformer 
29.08 
14.93 
8.57 
4.62 </p>
<p>PGE 
32.95 
18.18 
11.14 
6.94 </p>
<p>GT+RMA 
30.42 
15.86 
9.05 
4.89 </p>
<p>Graformer 
33.10 
18.28 
11.22 
7.00 </p>
<p>PGE-LW 
32.78 
18.05 
10.98 
6.79 </p>
<p>MEFR model 
33.19 
18.34 
11.26 
7.05 </p>
<p>Table 4
4Results of different 
generation models on WebNLG 
test set with seen categories </p>
<p>Methods 
BLEU </p>
<p>UPF-FORGe 
40.88 </p>
<p>Adapt 
60.59 </p>
<p>Melbourne 
54.52 </p>
<p>Graph Conv 
55.90 </p>
<p>E2EGRU 
57.20 </p>
<p>GTR-LSTM 
58.60 </p>
<p>SBS 
53.30 </p>
<p>Graformer 
61.15 </p>
<p>MEFR model 
62.06 </p>
<p>Table 5
5Human evaluation resultsMethods 
Informativeness 
Fluency 
Redundancy </p>
<p>GAT 
3.77 
4.09 
4.11 </p>
<p>Graph Transformer 
3.96 
4.26 
4.21 </p>
<p>MEFR model 
4.24 
4.32 
4.29 </p>
<p>Complex &amp; Intelligent Systems (2023) 9:2019-2030
Declarations We declare that we do not have any commercial or associative interest that represents a conflict of interest in connection with the work submitted.Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecomm ons.org/licenses/by/4.0/.
Building natural language generation systems, studies in natural language processing. E Reiter, R Dale, 10.1017/CBO9780511519857Cambridge University PressCambridgeReiter E, Dale R (2000) Building natural language generation sys- tems, studies in natural language processing. Cambridge University Press, Cambridge. https://doi.org/10.1017/CBO9780511519857</p>
<p>Automatic generation of related work sections in scientific papers: an optimization approach. Y Hu, X Wan, 10.3115/v1/D14-1170Proceedings of the 2014 Conference on empirical methods in natural language processing (EMNLP). the 2014 Conference on empirical methods in natural language processing (EMNLP)Association for Computational LinguisticsHu Y, Wan X (2014) Automatic generation of related work sections in scientific papers: an optimization approach. In: Proceedings of the 2014 Conference on empirical methods in natural language processing (EMNLP), Association for Computational Linguistics, pp 1624-1633. https://doi.org/10.3115/v1/D14-1170</p>
<p>What to talk about and how? Selective generation using lstms with coarse-to-fine alignment. H Mei, M Bansal, M R Walter, arXiv:1509.00838Mei H, Bansal M, Walter MR (2016) What to talk about and how? Selective generation using lstms with coarse-to-fine alignment. arXiv:1509.00838</p>
<p>Show and tell: a neural image caption generator. O Vinyals, A Toshev, S Bengio, D Erhan, Proceedings of the IEEE. the IEEEIEEE Computer SocietyVinyals O, Toshev A, Bengio S, Erhan D (2015) Show and tell: a neural image caption generator. In: Proceedings of the IEEE con- ference on computer vision and pattern recognition (CVPR). IEEE Computer Society, pp 3156-3164</p>
<p>The survey: text generation models in deep learning. T Iqbal, S Qureshi, 10.1016/j.jksuci.2020.04.001J King Saud Univ Comput Inf Sci. Iqbal T, Qureshi S (2020) The survey: text generation models in deep learning. J King Saud Univ Comput Inf Sci. https://doi.org/ 10.1016/j.jksuci.2020.04.001</p>
<p>Operation-guided neural networks for high fidelity data-to-text generation. F Nie, J Wang, J-G Yao, R Pan, C-Y Lin, 10.18653/v1/D18-1422Proceedings of the 2018 Conference on empirical methods in natural language processing. the 2018 Conference on empirical methods in natural language processingassociation for computational linguisticsNie F, Wang J, Yao J-G, Pan R, Lin C-Y (2018) Operation-guided neural networks for high fidelity data-to-text generation. In: Pro- ceedings of the 2018 Conference on empirical methods in natural language processing, association for computational linguistics, pp 3879-3889. https://doi.org/10.18653/v1/D18-1422</p>
<p>Learning neural templates for text generation. S Wiseman, S Shieber, A Rush, 10.18653/v1/D18-1356Proceedings of the 2018 Conference on empirical methods in natural language processing. the 2018 Conference on empirical methods in natural language processingAssociation for Computational LinguisticsWiseman S, Shieber S, Rush A (2018) Learning neural templates for text generation. In: Proceedings of the 2018 Conference on empirical methods in natural language processing, Association for Computational Linguistics, pp 3174-3187. https://doi.org/10. 18653/v1/D18-1356</p>
<p>Point precisely: towards ensuring the precision of data in generated texts using delayed copy mechanism. L Li, X Wan, Proceedings of the 27th International Conference on computational linguistics. the 27th International Conference on computational linguisticsAssociation for Computational LinguisticsLi L, Wan X (2018) Point precisely: towards ensuring the precision of data in generated texts using delayed copy mechanism. In: Pro- ceedings of the 27th International Conference on computational linguistics, Association for Computational Linguistics, pp 1044- 1055</p>
<p>Data-to-text generation with content selection and planning. R Puduppully, L Dong, M Lapata, 10.1609/aaai.v33i01.33016908Proceedings of the 33rd AAAI Conference on artificial intelligence. the 33rd AAAI Conference on artificial intelligencePuduppully R, Dong L, Lapata M (2019) Data-to-text generation with content selection and planning. In: Proceedings of the 33rd AAAI Conference on artificial intelligence, pp 6908-6915. https:// doi.org/10.1609/aaai.v33i01.33016908</p>
<p>Temporal network embedding using graph attention network. A Mohan, K V Pramod, 10.1007/s40747-021-00332-xComplex Intell Syst. 1510Mohan A, Pramod KV (2021) Temporal network embedding using graph attention network. Complex Intell Syst 15:10. https://doi. org/10.1007/s40747-021-00332-x</p>
<p>A patent keywords extraction method using textrank model with prior public knowledge. Z Huang, Z Xie, 10.1007/s40747-021-00343-8Complex Intell Syst. 1510Huang Z, Xie Z (2021) A patent keywords extraction method using textrank model with prior public knowledge. Complex Intell Syst 15:10. https://doi.org/10.1007/s40747-021-00343-8</p>
<p>Graph2seq: graph to sequence learning with attention-based neural networks. K Xu, L Wu, Z Wang, Y Feng, M Witbrock, V Sheinin, arXiv:1804.00823Xu K, Wu L, Wang Z, Feng Y, Witbrock M, Sheinin V (2018) Graph2seq: graph to sequence learning with attention-based neural networks. arXiv:1804.00823</p>
<p>Graph-to-sequence learning using gated graph neural networks. D Beck, G Haffari, T Cohn, 10.18653/v1/P18-1026Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational Linguistics1Association for Computational LinguisticsBeck D, Haffari G, Cohn T (2018) Graph-to-sequence learning using gated graph neural networks. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguis- tics, pp 273-283. https://doi.org/10.18653/v1/P18-1026</p>
<p>Coherent comments generation for Chinese articles with a graph-to-sequence model. W Li, J Xu, Y He, S Yan, Y Wu, X Sun, 10.18653/v1/P19-1479Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational LinguisticsLi W, Xu J, He Y, Yan S, Wu Y, Sun X (2019) Coherent comments generation for Chinese articles with a graph-to-sequence model. In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Lin- guistics, pp 4843-4852. https://doi.org/10.18653/v1/P19-1479</p>
<p>Modeling global and local node contexts for text generation from knowledge graphs. L F Ribeiro, Y Zhang, C Gardent, I Gurevych, Trans Assoc Comput Linguist. 8Ribeiro LF, Zhang Y, Gardent C, Gurevych I (2020) Modeling global and local node contexts for text generation from knowledge graphs. Trans Assoc Comput Linguist 8:589-604</p>
<p>Text Generation from Knowledge Graphs with Graph Transformers. R Koncel-Kedziorski, D Bekal, Y Luan, M Lapata, H Hajishirzi, 10.18653/v1/N19-1238Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics1Koncel-Kedziorski R, Bekal D, Luan Y, Lapata M, Hajishirzi H (2019) Text Generation from Knowledge Graphs with Graph Trans- formers, In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, Volume 1 (Long and Short Papers), Association for Computational Linguistics, pp 2284- 2293. https://doi.org/10.18653/v1/N19-1238</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Kaiser Lu, I Polosukhin, Proceedings of the 31st international conference on neural information processing systems. the 31st international conference on neural information processing systemsCurran Associates, Inc30Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Lu, Polosukhin I (2017) Attention is all you need. In: Pro- ceedings of the 31st international conference on neural information processing systems, vol 30. Curran Associates, Inc, pp 6000-6010</p>
<p>P Veličković, G Cucurull, A Casanova, A Romero, P Liò, Y Bengio, arXiv:1710.10903Graph attention networks. Veličković P, Cucurull G, Casanova A, Romero A, Liò P, Bengio Y (2018) Graph attention networks. arXiv:1710.10903</p>
<p>A Graves, arXiv:1211.3711Sequence transduction with recurrent neural networks. Graves A (2012) Sequence transduction with recurrent neural net- works. arXiv:1211.3711</p>
<p>Sequence to sequence learning with neural networks. I Sutskever, O Vinyals, Q V Le, Proceedings of the 27th International Conference on Neural information processing systems. the 27th International Conference on Neural information processing systems2Sutskever I, Vinyals O, Le QV (2014) Sequence to sequence learn- ing with neural networks. In: Proceedings of the 27th International Conference on Neural information processing systems 2:3104- 3112</p>
<p>A simple domain-independent probabilistic approach to generation. G Angeli, P Liang, D Klein, Proceedings of the 2010 Conference on empirical methods in natural language processing. the 2010 Conference on empirical methods in natural language processingAssociation for Computational LinguisticsAngeli G, Liang P, Klein D (2010) A simple domain-independent probabilistic approach to generation. In: Proceedings of the 2010 Conference on empirical methods in natural language processing, Association for Computational Linguistics, pp 502-512</p>
<p>A statistical NLG framework for aggregated planning and realization. R Kondadadi, B Howald, F Schilder, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. the 51st Annual Meeting of the Association for Computational Linguistics1Long Papers), Association for Computational LinguisticsKondadadi R, Howald B, Schilder F (2013) A statistical NLG framework for aggregated planning and realization. In: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computa- tional Linguistics, pp 1406-1415</p>
<p>Domain adaptable semantic clustering in statistical NLG. B Howald, R Kondadadi, F Schilder, Proceedings of the 10th International Conference on computational semantics (IWCS 2013)-Long Papers. the 10th International Conference on computational semantics (IWCS 2013)-Long PapersAssociation for Computational LinguisticsHowald B, Kondadadi R, Schilder F (2013) Domain adaptable semantic clustering in statistical NLG. In: Proceedings of the 10th International Conference on computational semantics (IWCS 2013)-Long Papers, Association for Computational Linguistics, pp 143-154</p>
<p>A deep ensemble model with slot alignment for sequence-to-sequence natural language generation. J Juraska, P Karagiannis, K Bowden, M Walker, 10.18653/v1/N18-1014Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics1Juraska J, Karagiannis P, Bowden K, Walker M (2018) A deep ensemble model with slot alignment for sequence-to-sequence nat- ural language generation. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Compu- tational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Association for Computational Linguistics, pp 152- 162. https://doi.org/10.18653/v1/N18-1014</p>
<p>End-to-end content and plan selection for data-to-text generation. S Gehrmann, F Dai, H Elder, A Rush, 10.18653/v1/W18-6505Proceedings of the 11th International Conference on natural language generation. the 11th International Conference on natural language generationAssociation for ComputationalGehrmann S, Dai F, Elder H, Rush A (2018) End-to-end content and plan selection for data-to-text generation. In: Proceedings of the 11th International Conference on natural language generation, Association for Computational Linguistics pp 46-56. https://doi. org/10.18653/v1/W18-6505</p>
<p>Unsupervised natural language generation with denoising autoencoders. M Freitag, S Roy, 10.18653/v1/D18-1426Proceedings of the 2018 Conference on empirical methods in natural language processing. the 2018 Conference on empirical methods in natural language processingAssociation for Computational LinguisticsFreitag M, Roy S (2018) Unsupervised natural language gener- ation with denoising autoencoders. In: Proceedings of the 2018 Conference on empirical methods in natural language processing, Association for Computational Linguistics, pp 3922-3929. https:// doi.org/10.18653/v1/D18-1426</p>
<p>SQL-to-text generation with graph-to-sequence model. K Xu, L Wu, Z Wang, Y Feng, V Sheinin, 10.18653/v1/D18-1112Proceedings of the 2018 Conference on empirical methods in natural language processing. the 2018 Conference on empirical methods in natural language processingAssociation for Computational LinguisticsXu K, Wu L, Wang Z, Feng Y, Sheinin V (2018) SQL-to-text gen- eration with graph-to-sequence model. In: Proceedings of the 2018 Conference on empirical methods in natural language processing, Association for Computational Linguistics, pp 931-936. https:// doi.org/10.18653/v1/D18-1112</p>
<p>Structural information preserving for graph-to-text generation. L Song, A Wang, J Su, Y Zhang, K Xu, Y Ge, D Yu, 10.18653/v1/2020.acl-main.712Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsSong L, Wang A, Su J, Zhang Y, Xu K, Ge Y, Yu D (2018) Structural information preserving for graph-to-text generation. In: Proceedings of the 58th Annual Meeting of the Associa- tion for Computational Linguistics, Association for Computational Linguistics, pp 7987-7998. https://doi.org/10.18653/v1/2020.acl- main.712</p>
<p>Densely connected graph convolutional networks for graph-to-sequence learning. Z Guo, Y Zhang, Z Teng, W Lu, arXiv:1908.05957Guo Z, Zhang Y, Teng Z, Lu W (2019) Densely connected graph convolutional networks for graph-to-sequence learning. arXiv:1908.05957</p>
<p>Bidirectional recurrent neural networks. M Schuster, K K Paliwal, 10.1109/78.650093IEEE Trans Signal Process. 4511Schuster M, Paliwal KK (1997) Bidirectional recurrent neural net- works. IEEE Trans Signal Process 45(11):2673-2681. https://doi. org/10.1109/78.650093</p>
<p>Effective approaches to attention-based neural machine translation. T Luong, H Pham, C D Manning, 10.18653/v1/D15-1166Proceedings of the 2015 Conference on empirical methods in natural language processing. the 2015 Conference on empirical methods in natural language processingAssociation for Computational LinguisticsLuong T, Pham H, Manning CD (2015) Effective approaches to attention-based neural machine translation. In: Proceedings of the 2015 Conference on empirical methods in natural language processing, Association for Computational Linguistics, pp 1412- 1421. https://doi.org/10.18653/v1/D15-1166</p>
<p>Get to the point: Summarization with pointer-generator networks. A See, P J Liu, C D Manning, 10.18653/v1/P17-1099Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational Linguistics1Association for Computational LinguisticsSee A, Liu PJ, Manning CD (2017) Get to the point: Summariza- tion with pointer-generator networks. In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association for Computational Linguis- tics, pp 1073-1083. https://doi.org/10.18653/v1/P17-1099</p>
<p>Highway networks. R K Srivastava, K Greff, J Schmidhuber, arXiv:1505.00387Srivastava RK, Greff K, Schmidhuber J (2015) Highway networks. arXiv:1505.00387</p>
<p>Effective approaches to attention-based neural machine translation. M-T Luong, H Pham, C D Manning, Proceedings of the 2015 Conference on empirical methods in natural language processing. the 2015 Conference on empirical methods in natural language processingLuong M-T, Pham H, Manning CD (2015) Effective approaches to attention-based neural machine translation. In: Proceedings of the 2015 Conference on empirical methods in natural language processing, pp 1412-1421</p>
<p>Construction of the literature graph in semantic scholar. W Ammar, D Groeneveld, C Bhagavatula, I Beltagy, M Crawford, D Downey, J Dunkelberger, A Elgohary, S Feldman, V Ha, R Kinney, S Kohlmeier, K Lo, T Murray, H-H Ooi, M Peters, J Power, S Skjonsberg, L Wang, C Wilhelm, Z Yuan, M Van Zuylen, O Etzioni, 10.18653/v1/N18-3011Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational Linguistics3Industry PapersAmmar W, Groeneveld D, Bhagavatula C, Beltagy I, Crawford M, Downey D, Dunkelberger J, Elgohary A, Feldman S, Ha V, Kinney R, Kohlmeier S, Lo K, Murray T, Ooi H-H, Peters M, Power J, Skjonsberg S, Wang L, Wilhelm C, Yuan Z, van Zuylen M, Etzioni O (2018) Construction of the literature graph in seman- tic scholar. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, Volume 3 (Industry Papers), Association for Computational Linguistics, pp 84-91. https://doi. org/10.18653/v1/N18-3011</p>
<p>The webnlg challenge: Generating text from rdf data. C Gardent, A Shimorina, S Narayan, L Perez-Beltrachini, Proceedings of the 10th International Conference on natural language generation. the 10th International Conference on natural language generationGardent C, Shimorina A, Narayan S, Perez-Beltrachini L (2017) The webnlg challenge: Generating text from rdf data. In: Pro- ceedings of the 10th International Conference on natural language generation, pp 124-133</p>
<p>Dbpedia: a nucleus for a web of open data. S Auer, C Bizer, G Kobilarov, J Lehmann, R Cyganiak, Z Ives, Proceedings of the 6th international the semantic web and 2nd Asian conference on Asian semantic web conference. the 6th international the semantic web and 2nd Asian conference on Asian semantic web conferenceSpringerAuer S, Bizer C, Kobilarov G, Lehmann J, Cyganiak R, Ives Z (2007) Dbpedia: a nucleus for a web of open data. In: Proceedings of the 6th international the semantic web and 2nd Asian conference on Asian semantic web conference. Springer, pp 722-73</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, 10.1162/neco.1997.9.8.1735Neural Comput. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput. https://doi.org/10.1162/neco.1997.9.8.1735</p>
<p>Early stopping-but when? In: Neural networks: tricks of the trade, this book is an Outgrowth of a 1996 NIPS Workshop. L Prechelt, Springer-VerlagPrechelt L (1998) Early stopping-but when? In: Neural networks: tricks of the trade, this book is an Outgrowth of a 1996 NIPS Work- shop, Springer-Verlag, pp 55-69</p>
<p>On the momentum term in gradient descent learning algorithms. N Qian, 10.1016/S0893-6080(98)00116-6Neural Netw. 98Qian N (1999) On the momentum term in gradient descent learning algorithms. Neural Netw. https://doi.org/10.1016/S0893- 6080(98)00116-6</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W-J Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. the 40th Annual Meeting on Association for Computational LinguisticsAssociation for Computational LinguisticsPapineni K, Roukos S, Ward T, Zhu W-J (2002) Bleu: a method for automatic evaluation of machine translation. In: Proceedings of the 40th Annual Meeting on Association for Computational Lin- guistics, Association for Computational Linguistics, pp 311-318. https://doi.org/10.3115/1073083.1073135</p>
<p>Rouge: a package for automatic evaluation of summaries. C Lin, Proceedings of workshop on text summarization branches out, post conference workshop of ACL 2004. Association for Computational Linguistics. workshop on text summarization branches out, post conference workshop of ACL 2004. Association for Computational LinguisticsLin C (2004) Rouge: a package for automatic evaluation of summaries. In: Proceedings of workshop on text summarization branches out, post conference workshop of ACL 2004. Associa- tion for Computational Linguistics, pp 74-81</p>
<p>Deep graph convolutional encoders for structured data to text generation. D Marcheggiani, L Perez-Beltrachini, arXiv:1810.09995Marcheggiani D, Perez-Beltrachini L (2018) Deep graph con- volutional encoders for structured data to text generation. arXiv:1810.09995</p>
<p>Repulsive Bayesian sampling for diversified attention modeling. B An, X Dong, C Chen, 4th workshop on Bayesian deep learning (NeurIPS 2019). An B, Dong X, Chen C (2019) Repulsive Bayesian sampling for diversified attention modeling. In: 4th workshop on Bayesian deep learning (NeurIPS 2019), pp 1-10</p>
<p>M Schmitt, L F Ribeiro, P Dufter, I Gurevych, H Schütze, arXiv:2006.09242Modeling graph structure via relative position for text generation from knowledge graphs. arXiv preprintSchmitt M, Ribeiro LF, Dufter P, Gurevych I, Schütze H (2020) Modeling graph structure via relative position for text generation from knowledge graphs. arXiv preprint arXiv:2006.09242</p>
<p>Neural data-to-text generation: a comparison between pipeline and end-to-end architectures. T C Ferreira, C Van Der Lee, Van Miltenburg, E Krahmer, E , Proceedings of the 2019 Conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing. the 2019 Conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing1Ferreira TC, van der Lee C, Van Miltenburg E, Krahmer E (2019) Neural data-to-text generation: a comparison between pipeline and end-to-end architectures. In: Proceedings of the 2019 Confer- ence on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), vol 1, pp 552-562</p>
<p>Gtr-lstm: a triple encoder for sentence generation from rdf data. B Distiawan, J Qi, R Zhang, W Wang, Proceedings of the 56th annual meeting of the Association for Computational Linguistics. the 56th annual meeting of the Association for Computational LinguisticsLong Papers1Distiawan B, Qi J, Zhang R, Wang W (2018) Gtr-lstm: a triple encoder for sentence generation from rdf data. In: Proceedings of the 56th annual meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp 1627-1637</p>
<p>Step-by-step: separating planning from realization in neural data-to-text generation. A Moryossef, Y Goldberg, I Dagan, Proceedings of the 2019 conference of the North American chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 conference of the North American chapter of the Association for Computational Linguistics: Human Language Technologies1Moryossef A, Goldberg Y, Dagan I (2019) Step-by-step: separating planning from realization in neural data-to-text generation. In: Pro- ceedings of the 2019 conference of the North American chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp 2267-2277</p>
<p>Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Publisher's Note Springer Nature remains neutral with regard to juris- dictional claims in published maps and institutional affiliations.</p>            </div>
        </div>

    </div>
</body>
</html>
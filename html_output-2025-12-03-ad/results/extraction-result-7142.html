<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7142 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7142</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7142</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-4b0b56be0ae9479d2bd5c2f0943db1906343c10f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4b0b56be0ae9479d2bd5c2f0943db1906343c10f" target="_blank">Chain-of-Verification Reduces Hallucination in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The Chain-of-Verification (CoVe) method is developed, whereby the model first drafts an initial response; then plans verification questions to fact-check its draft; and answers those questions independently so the answers are not biased by other responses.</p>
                <p><strong>Paper Abstract:</strong> Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7142.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7142.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoVe (joint) - Wikidata</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Verification (joint execution variant) on Wikidata list task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Joint CoVe prompts the model to plan and immediately answer verification questions in a single prompt (planning+execution together), then produce a final verified response; applied to list-based Wikidata questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 65B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLaMA-family model used with few-shot prompting (not instruction fine-tuned for these experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Verification (joint)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate baseline response; generate verification questions and their answers in the same joint prompt (questions answered conditioned on baseline); then generate a final response incorporating verifications.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (single cycle, joint planning+execution)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Wikidata (list-based question generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>List-style entity-answer questions derived from Wikidata (e.g., 'Who are some [Profession]s born in [City]?'), evaluated in closed-book generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision (micro-averaged); also reported avg # positive and negative entities</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.17 precision (Llama 65B few-shot baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.29 precision (Llama 65B CoVe joint)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Joint execution conditions verification answers on the original baseline response, which increases tendency to repeat/hallucinate the same incorrect facts; less effective than factored/2-step variants; computational cost still higher than baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Verification Reduces Hallucination in Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7142.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7142.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoVe (two-step) - Wikidata</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Verification (two-step execution variant) on Wikidata list task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Two-step CoVe separates verification planning (conditioned on baseline) and execution (answers generated without the baseline present), reducing copying of baseline hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 65B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLaMA-family model used with few-shot prompting (not instruction fine-tuned for these experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Verification (two-step)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate baseline; plan verification questions conditioned on baseline; answer the planned questions in separate prompts that do not include the baseline response, then synthesize final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (single cycle, separated planning and execution)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Wikidata (list-based question generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>List-style entity-answer questions derived from Wikidata (closed-book).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision (micro-averaged); avg # positive and negative entities reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.17 precision (Llama 65B few-shot baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.36 precision (Llama 65B CoVe two-step) — best variant on Wikidata reported</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>More computationally expensive than joint because of multiple prompts; still does not eliminate all hallucinations; slight reduction in number of true positives alongside reduction in hallucinated answers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Verification Reduces Hallucination in Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7142.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7142.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoVe (factored) - Wiki-Category</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Verification (factored variant) on Wiki-Category list (QUEST) task</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Factored CoVe answers each verification question independently in separate prompts that exclude the baseline and other answers, reducing interference and copying.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 65B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLaMA-family model used with few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Verification (factored)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate baseline; plan verification questions; execute each verification independently in separate prompts that exclude baseline and other answers; optionally cross-check and then revise final response.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (single cycle with per-question independent verification)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Wiki-Category list (QUEST-derived)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Harder set-generation questions from Wikipedia categories (e.g., 'Name some Mexican animated horror films'), eight-answer sets; closed-book generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision (micro-averaged); avg # positive and negative entities</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>0.12 precision (Llama 65B few-shot baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>0.22 precision (Llama 65B CoVe factored) — best reported on this task</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Factored execution is more computationally expensive (many independent prompts) though parallelizable; yes/no style verification questions and rule-based templated checks underperform compared to LLM-generated open verification questions; still does not fully eliminate hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Verification Reduces Hallucination in Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7142.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7142.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoVe (factored) - MultiSpanQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Verification (factored) on closed-book MultiSpanQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Factored CoVe applied to closed-book reading-comprehension questions with multiple discontiguous answer spans; verification improves both precision and recall.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 65B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLaMA-family model used with few-shot examples tailored to MultiSpanQA.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Verification (factored)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Plan verification questions from baseline; execute each independently (without conditioning on baseline) to check facts; synthesize final answer from verified facts.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (single cycle, factored verification)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiSpanQA (closed-book subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-span reading-comprehension questions (multiple short factual spans) evaluated in closed-book setting.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>F1 (also precision and recall reported)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>F1 = 0.39 (Llama 65B few-shot baseline; Prec=0.40, Rec=0.38)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>F1 = 0.48 (Llama 65B CoVe factored; Prec=0.50, Rec=0.46)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although F1 improved (23% relative), CoVe still does not remove all errors; factored approach requires more prompts; constrained by model's internal knowledge (closed-book).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Verification Reduces Hallucination in Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7142.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7142.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoVe (factor+revise) - Biographies</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Verification (factor+revise variant) on longform biography generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Factor+Revise adds an explicit cross-check step that labels each original fact as CONSISTENT/PARTIALLY/INCONSISTENT with verification answers before reconstructing the final longform text; produced the largest gains on biographies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 65B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only LLaMA-family model used with few-shot prompts for biography generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>65B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Verification (factor+revise)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>After factored verification, run an explicit 'cross-check' prompt per fact that marks consistency between baseline fact and verification answers; then generate a revised final passage using only consistent facts.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (single cycle with explicit cross-check and revision)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Longform biography generation (Min et al. benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate biographies of entities; factual precision assessed with FACTSCORE (a retrieval-augmented automatic fact-checker shown to correlate with human judgments).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>FACTSCORE (automatic fact-checking score); also avg # facts in outputs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>FACTSCORE = 55.9 (Llama 65B few-shot baseline), avg # facts = 16.6</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>FACTSCORE = 71.4 (Llama 65B CoVe factor+revise), avg # facts = 12.3</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CoVe yields large gains but does not eliminate hallucinations; reduces number of facts produced (small drop in coverage); retrieval-augmented systems (PerplexityAI) still outperform CoVe on very rare facts where retrieval is essential; additional computational and token-generation cost; upper bound limited by model's knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Verification Reduces Hallucination in Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7142.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7142.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) - comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard Chain-of-Thought prompting ('Let's think step by step') used as a reflection-like baseline; here it did not reduce hallucinations and sometimes degraded performance on factual generation/list tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 2 70B Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-fine-tuned chat variant of LLaMA-2 used in zero-shot and CoT prompting settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Encourage the model to produce internal multi-step reasoning (e.g., via 'Let's think step by step') prior to answering; used here as a comparison to CoVe.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>single-pass internal reasoning (not explicit generate-then-reflect cycles)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>1</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Various (Wikidata, MultiSpanQA, biographies) as reported in comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as CoVe experiments (list generation, closed-book QA, longform biographies) used to compare effectiveness of CoT vs CoVe.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Precision (list tasks), F1 (MultiSpanQA), FACTSCORE (biographies)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>Example: MultiSpanQA zero-shot (Llama 2 70B Chat) F1 = 0.20</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>Example: MultiSpanQA CoT (Llama 2 70B Chat) F1 = 0.17 (CoT decreased performance here); similarly CoT did not improve list or longform factuality in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>CoT prompting did not reduce hallucination in these factual tasks and sometimes made outputs more extraneous or less precise; CoT is more suited to chain reasoning tasks rather than factuality correction as evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Chain-of-Verification Reduces Hallucination in Large Language Models', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Lm vs lm: Detecting factual errors via cross examination. <em>(Rating: 2)</em></li>
                <li>Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. <em>(Rating: 2)</em></li>
                <li>Do language models know when they're hallucinating references? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7142",
    "paper_id": "paper-4b0b56be0ae9479d2bd5c2f0943db1906343c10f",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "CoVe (joint) - Wikidata",
            "name_full": "Chain-of-Verification (joint execution variant) on Wikidata list task",
            "brief_description": "Joint CoVe prompts the model to plan and immediately answer verification questions in a single prompt (planning+execution together), then produce a final verified response; applied to list-based Wikidata questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 65B",
            "model_description": "Decoder-only LLaMA-family model used with few-shot prompting (not instruction fine-tuned for these experiments).",
            "model_size": "65B",
            "reflection_method_name": "Chain-of-Verification (joint)",
            "reflection_method_description": "Generate baseline response; generate verification questions and their answers in the same joint prompt (questions answered conditioned on baseline); then generate a final response incorporating verifications.",
            "iteration_type": "generate-then-reflect (single cycle, joint planning+execution)",
            "num_iterations": 1,
            "task_name": "Wikidata (list-based question generation)",
            "task_description": "List-style entity-answer questions derived from Wikidata (e.g., 'Who are some [Profession]s born in [City]?'), evaluated in closed-book generation.",
            "evaluation_metric": "Precision (micro-averaged); also reported avg # positive and negative entities",
            "performance_before_reflection": "0.17 precision (Llama 65B few-shot baseline)",
            "performance_after_reflection": "0.29 precision (Llama 65B CoVe joint)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Joint execution conditions verification answers on the original baseline response, which increases tendency to repeat/hallucinate the same incorrect facts; less effective than factored/2-step variants; computational cost still higher than baseline.",
            "uuid": "e7142.0",
            "source_info": {
                "paper_title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "CoVe (two-step) - Wikidata",
            "name_full": "Chain-of-Verification (two-step execution variant) on Wikidata list task",
            "brief_description": "Two-step CoVe separates verification planning (conditioned on baseline) and execution (answers generated without the baseline present), reducing copying of baseline hallucinations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 65B",
            "model_description": "Decoder-only LLaMA-family model used with few-shot prompting (not instruction fine-tuned for these experiments).",
            "model_size": "65B",
            "reflection_method_name": "Chain-of-Verification (two-step)",
            "reflection_method_description": "Generate baseline; plan verification questions conditioned on baseline; answer the planned questions in separate prompts that do not include the baseline response, then synthesize final answer.",
            "iteration_type": "generate-then-reflect (single cycle, separated planning and execution)",
            "num_iterations": 1,
            "task_name": "Wikidata (list-based question generation)",
            "task_description": "List-style entity-answer questions derived from Wikidata (closed-book).",
            "evaluation_metric": "Precision (micro-averaged); avg # positive and negative entities reported",
            "performance_before_reflection": "0.17 precision (Llama 65B few-shot baseline)",
            "performance_after_reflection": "0.36 precision (Llama 65B CoVe two-step) — best variant on Wikidata reported",
            "improvement_observed": true,
            "limitations_or_failure_cases": "More computationally expensive than joint because of multiple prompts; still does not eliminate all hallucinations; slight reduction in number of true positives alongside reduction in hallucinated answers.",
            "uuid": "e7142.1",
            "source_info": {
                "paper_title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "CoVe (factored) - Wiki-Category",
            "name_full": "Chain-of-Verification (factored variant) on Wiki-Category list (QUEST) task",
            "brief_description": "Factored CoVe answers each verification question independently in separate prompts that exclude the baseline and other answers, reducing interference and copying.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 65B",
            "model_description": "Decoder-only LLaMA-family model used with few-shot prompting.",
            "model_size": "65B",
            "reflection_method_name": "Chain-of-Verification (factored)",
            "reflection_method_description": "Generate baseline; plan verification questions; execute each verification independently in separate prompts that exclude baseline and other answers; optionally cross-check and then revise final response.",
            "iteration_type": "generate-then-reflect (single cycle with per-question independent verification)",
            "num_iterations": 1,
            "task_name": "Wiki-Category list (QUEST-derived)",
            "task_description": "Harder set-generation questions from Wikipedia categories (e.g., 'Name some Mexican animated horror films'), eight-answer sets; closed-book generation.",
            "evaluation_metric": "Precision (micro-averaged); avg # positive and negative entities",
            "performance_before_reflection": "0.12 precision (Llama 65B few-shot baseline)",
            "performance_after_reflection": "0.22 precision (Llama 65B CoVe factored) — best reported on this task",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Factored execution is more computationally expensive (many independent prompts) though parallelizable; yes/no style verification questions and rule-based templated checks underperform compared to LLM-generated open verification questions; still does not fully eliminate hallucinations.",
            "uuid": "e7142.2",
            "source_info": {
                "paper_title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "CoVe (factored) - MultiSpanQA",
            "name_full": "Chain-of-Verification (factored) on closed-book MultiSpanQA",
            "brief_description": "Factored CoVe applied to closed-book reading-comprehension questions with multiple discontiguous answer spans; verification improves both precision and recall.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 65B",
            "model_description": "Decoder-only LLaMA-family model used with few-shot examples tailored to MultiSpanQA.",
            "model_size": "65B",
            "reflection_method_name": "Chain-of-Verification (factored)",
            "reflection_method_description": "Plan verification questions from baseline; execute each independently (without conditioning on baseline) to check facts; synthesize final answer from verified facts.",
            "iteration_type": "generate-then-reflect (single cycle, factored verification)",
            "num_iterations": 1,
            "task_name": "MultiSpanQA (closed-book subset)",
            "task_description": "Multi-span reading-comprehension questions (multiple short factual spans) evaluated in closed-book setting.",
            "evaluation_metric": "F1 (also precision and recall reported)",
            "performance_before_reflection": "F1 = 0.39 (Llama 65B few-shot baseline; Prec=0.40, Rec=0.38)",
            "performance_after_reflection": "F1 = 0.48 (Llama 65B CoVe factored; Prec=0.50, Rec=0.46)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Although F1 improved (23% relative), CoVe still does not remove all errors; factored approach requires more prompts; constrained by model's internal knowledge (closed-book).",
            "uuid": "e7142.3",
            "source_info": {
                "paper_title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "CoVe (factor+revise) - Biographies",
            "name_full": "Chain-of-Verification (factor+revise variant) on longform biography generation",
            "brief_description": "Factor+Revise adds an explicit cross-check step that labels each original fact as CONSISTENT/PARTIALLY/INCONSISTENT with verification answers before reconstructing the final longform text; produced the largest gains on biographies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 65B",
            "model_description": "Decoder-only LLaMA-family model used with few-shot prompts for biography generation.",
            "model_size": "65B",
            "reflection_method_name": "Chain-of-Verification (factor+revise)",
            "reflection_method_description": "After factored verification, run an explicit 'cross-check' prompt per fact that marks consistency between baseline fact and verification answers; then generate a revised final passage using only consistent facts.",
            "iteration_type": "generate-then-reflect (single cycle with explicit cross-check and revision)",
            "num_iterations": 1,
            "task_name": "Longform biography generation (Min et al. benchmark)",
            "task_description": "Generate biographies of entities; factual precision assessed with FACTSCORE (a retrieval-augmented automatic fact-checker shown to correlate with human judgments).",
            "evaluation_metric": "FACTSCORE (automatic fact-checking score); also avg # facts in outputs",
            "performance_before_reflection": "FACTSCORE = 55.9 (Llama 65B few-shot baseline), avg # facts = 16.6",
            "performance_after_reflection": "FACTSCORE = 71.4 (Llama 65B CoVe factor+revise), avg # facts = 12.3",
            "improvement_observed": true,
            "limitations_or_failure_cases": "CoVe yields large gains but does not eliminate hallucinations; reduces number of facts produced (small drop in coverage); retrieval-augmented systems (PerplexityAI) still outperform CoVe on very rare facts where retrieval is essential; additional computational and token-generation cost; upper bound limited by model's knowledge.",
            "uuid": "e7142.4",
            "source_info": {
                "paper_title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) - comparison",
            "name_full": "Chain-of-Thought prompting (baseline comparison)",
            "brief_description": "Standard Chain-of-Thought prompting ('Let's think step by step') used as a reflection-like baseline; here it did not reduce hallucinations and sometimes degraded performance on factual generation/list tasks.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "Llama 2 70B Chat",
            "model_description": "Instruction-fine-tuned chat variant of LLaMA-2 used in zero-shot and CoT prompting settings.",
            "model_size": "70B",
            "reflection_method_name": "Chain-of-Thought (CoT)",
            "reflection_method_description": "Encourage the model to produce internal multi-step reasoning (e.g., via 'Let's think step by step') prior to answering; used here as a comparison to CoVe.",
            "iteration_type": "single-pass internal reasoning (not explicit generate-then-reflect cycles)",
            "num_iterations": 1,
            "task_name": "Various (Wikidata, MultiSpanQA, biographies) as reported in comparisons",
            "task_description": "Same tasks as CoVe experiments (list generation, closed-book QA, longform biographies) used to compare effectiveness of CoT vs CoVe.",
            "evaluation_metric": "Precision (list tasks), F1 (MultiSpanQA), FACTSCORE (biographies)",
            "performance_before_reflection": "Example: MultiSpanQA zero-shot (Llama 2 70B Chat) F1 = 0.20",
            "performance_after_reflection": "Example: MultiSpanQA CoT (Llama 2 70B Chat) F1 = 0.17 (CoT decreased performance here); similarly CoT did not improve list or longform factuality in reported experiments.",
            "improvement_observed": false,
            "limitations_or_failure_cases": "CoT prompting did not reduce hallucination in these factual tasks and sometimes made outputs more extraneous or less precise; CoT is more suited to chain reasoning tasks rather than factuality correction as evaluated here.",
            "uuid": "e7142.5",
            "source_info": {
                "paper_title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "Lm vs lm: Detecting factual errors via cross examination.",
            "rating": 2
        },
        {
            "paper_title": "Selfcheck: Using llms to zero-shot check their own step-by-step reasoning.",
            "rating": 2
        },
        {
            "paper_title": "Do language models know when they're hallucinating references?",
            "rating": 1
        }
    ],
    "cost": 0.01382375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Chain-of-Verification Reduces Hallucination in Large Language Models</h1>
<p>Shehzaad Dhuliawala<br>Meta AI \&amp; ETH Zürich<br>Xian Li<br>Meta AI</p>
<p>Mojtaba Komeili Meta AI</p>
<p>Jing Xu Meta AI</p>
<h2>Roberta Raileanu</h2>
<p>Meta AI</p>
<p>Jason Weston
Meta AI</p>
<h2>AbSTRACT</h2>
<h4>Abstract</h4>
<p>Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (COVE) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show COVE decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.</p>
<h2>1 INTRODUCTION</h2>
<p>Large Language Models (LLMs) are trained on huge corpora of text documents with billions of tokens of text. It has been shown that as the number of model parameters is increased, performance at tasks such as closed book QA improve in accuracy, and larger models can generate more correct factual statements (Radford et al., 2019; Petroni et al., 2019). However, even the largest models can still fail, particularly on lesser known torso and tail distribution facts (Sun et al., 2023a), i.e. those that occur relatively rarely in the training corpora. In those cases where the model is incorrect, they instead generate an alternative response which is typically plausible looking (e.g., a similar entity, but an incorrect one). These factually incorrect generations are referred to as hallucinations (Maynez et al., 2020). Further, in longform tasks consisting of generating multiple sentences or paragraphs, the hallucination problem can be exacerbated due to the issue of exposure bias (Wang \&amp; Sennrich, 2020).</p>
<p>The current wave of language modeling research goes beyond next word prediction, and has focused on their ability to reason. Improved performance in reasoning tasks can be gained by encouraging language models to first generate internal thoughts or reasoning chains before responding (Wei et al., 2022; Adolphs et al., 2021; Wang et al., 2022; Lanchantin et al., 2023), as well as updating their initial response through self-critique (Press et al., 2022; Madaan et al., 2023). In this work we follow this line of research to study how and when language-model-based reasoning can be used to reduce hallucinations. We develop an approach, called Chain-of-Verification (CoVe) which, given an initial draft response, first plans verification questions to check its work, and then systematically answers those questions in order to finally produce an improved revised response. We find that independent verification questions tend to provide more accurate facts than those in the original longform answer, and hence improve the correctness of the overall response. We study variations on this recipe across a range of tasks: from list-based questions, closed booked QA and longform text generation. We first propose a joint approach for generating the entire verification chain left-to-right, which improves performance and decreases hallucinations compared to the baseline language model. However, models that attend to existing hallucinations in the context from their own generations tend to repeat the hallucinations. Hence we also introduce further improvements with factored variants which separate out the verification chain steps, in terms of which context is attended to. We show how these factored variants give further performance gains across all three tasks considered.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Chain-of-Verification (CoVe) method. Given a user query, a large language model generates a baseline response that may contain inaccuracies, e.g. factual hallucinations. We show a query here which failed for ChatGPT (see section 9 for more details). To improve this, CoVe first generates a plan of a set of verification questions to ask, and then executes that plan by answering them and hence checking for agreement. We find that individual verification questions are typically answered with higher accuracy than the original accuracy of the facts in the original longform generation. Finally, the revised response takes into account the verifications. The factored version of CoVe answers verification questions such that they cannot condition on the original response, avoiding repetition and improving performance.</p>
<h1>2 Related Work</h1>
<p>Hallucination is a general problem in language model generations that appears across many tasks, from summarization (Maynez et al., 2020) to open-domain dialogue (Roller et al., 2020), and has not been resolved by simply scaling up training data or model size (Zhang et al., 2023). For a survey of the hallucination issue, see Ji et al. (2023). A majority of the methods for reducing hallucination can be divided into roughly three categories: training-time correction, generation-time correction and via augmentation (tool-use).</p>
<p>In training-time correction methods, an attempt is made to improve the raw left-to-right generations of an encoder-decoder or decoder-only language model by either training or otherwise adjusting the model weights to decrease the probability of hallucinated generations. This includes using reinforcement learning (Roit et al., 2023; Wu et al., 2023), constrastive learning (Chern et al., 2023b; Sun et al., 2023b) and other methods (Li et al., 2023).</p>
<p>In generation-time correction, a common theme is to make reasoning decisions "on top of" the base LLM in order to make them more reliable. For example, by considering the probabilities of the generated tokens (Mielke et al., 2022; Kadavath et al., 2022). In Manakul et al. (2023) multiple samples are drawn from the model to detect hallucinations. In Varshney et al. (2023) hallucinations are identified using low confidence scores, and their correctness is checked through a validation</p>
<p>procedure, mitigated, and then the generation is continued. An alternative to using the confidence scores is to leverage inconsistencies in the LLMs output to detect hallucination. Agrawal et al. (2023) use both multiple samples and consistency detection by asking direct and indirect queries to check for hallucinated references. Cohen et al. (2023) introduce a method called LM vs LM which simulates an interactive setup between two LLMs where one LLM acts as an examiner and tests if the output is consistent via repeated cross-examination. Cohen et al. (2023) shows that using inconsistencies for QA tasks can outperform using confidence scores for hallucination detection. CoVE also uses a related self-consistency approach, but without the multi-agent (multi-LLM) debate concept.</p>
<p>A third approach is to use external tools to help mitigate hallucinations, rather than relying solely on the abilities of the language model itself. For example, retrieval-augmented generation can decrease hallucinations by using factual documents for grounding (Shuster et al., 2021; Jiang et al., 2023b; Yu et al., 2023) or chain-of-thought verification (Zhao et al., 2023). Other approaches include using tools for fact-checking (Chern et al., 2023a; Galitsky, 2023; Peng et al., 2023), or linking to external documents with attribution (Menick et al., 2022; Rashkin et al., 2023; Gao et al., 2023).</p>
<p>There are also a number of related works in improving reasoning for logical and mathematical tasks, even if they do not address reducing hallucination explicitly. Several approaches have been shown to improve results with extended reasoning steps by the system, such as chain-of-thought (Wei et al., 2022), deductive verification (Ling et al., 2023), and self-verification (Miao et al., 2023; Jiang et al., 2023a; Weng et al., 2022). The latter tries to predict the (masked) question given the answer for math problems, and use that as evidence that this is the correct solution.</p>
<h1>3 CHAIN-OF-VERIFICATION</h1>
<p>Our approach assumes access to a base LLM that - despite potentially being prone to hallucination is capable of being prompted with general instructions in either a few-shot or zero-shot fashion. A key assumption of our method is that this language model, when suitably prompted, can both generate and execute a plan of how to verify itself in order to check its own work, and finally incorporate this analysis into an improved response.</p>
<p>Our overall process, which we call Chain-of-Verification (CoVe), thus performs four core steps:</p>
<ol>
<li>Generate Baseline Response: Given a query, generate the response using the LLM.</li>
<li>Plan Verifications: Given both query and baseline response, generate a list of verification questions that could help to self-analyze if there are any mistakes in the original response.</li>
<li>Execute Verifications: Answer each verification question in turn, and hence check the answer against the original response to check for inconsistencies or mistakes.</li>
<li>Generate Final Verified Response: Given the discovered inconsistencies (if any), generate a revised response incorporating the verification results.</li>
</ol>
<p>Each of these steps is performed by prompting the same LLM in different ways to obtain the desired response. While steps (1), (2) and (4) all can be invoked with a single prompt, we investigate variations of step (3) including joint, 2-step and factored versions. These variants either involve a single prompt, two prompts or else independent prompts per question, where more sophisticated decomposition can yield improved results.</p>
<p>We describe these steps in more detail below. An overview of the approach is illustrated in Figure 1, and in the Appendix in Figure 3.</p>
<h3>3.1 BASELINE RESPONSE</h3>
<p>Given a query, we generate left-to-right as usual using the LLM, with no special tricks. While this is the first step in the CoVe pipeline, it also serves as the baseline we wish to improve in our experiments (i.e., we will directly compare this baseline response with the final verified response from our overall method).</p>
<p>Given such baseline generations are typically prone to hallucination, CoVe attempts to identify these hallucinations, and correct them, in the following steps.</p>
<h1>3.2 Plan Verifications</h1>
<p>Conditioned on the original query and the baseline response, the model is prompted to generate a series of verification questions that test the factual claims in the original baseline response. For example if part of a longform model response contains the statement "The Mexican-American War was an armed conflict between the United States and Mexico from 1846 to 1848", then one possible verification question to check those dates could be "When did the Mexican American war start and end?". We note that verification questions are not templated and the language model is free to phrase these in any form it wants, and they also do not have to closely match the phrasing of the original text.</p>
<p>In our experiments, we perform such verification planning by providing a few-shot prompt of (response, verification) demonstrations to our LLM. See section 8 for the few-shot prompts we will use in our experiments. We note it is also possible with a sufficiently performant instruction-following LLM that this could be performed zero-shot.</p>
<h3>3.3 EXECUTE Verifications</h3>
<p>Given the planned verification questions, the next step is to answer them in order to assess if any hallucinations exist. While techniques such as retrieval-augmentation could be used in this process, such as verification via search engine, in this work we do not explore tool-use. Instead, we consider only using the LLM itself in all steps of CoVe, hence the model is used to check its own work. We investigate several variants of verification execution, called joint, 2-Step, factored and factor+revise.</p>
<p>Joint In the joint method, the planning and execution (steps 2 and 3) are accomplished by using a single LLM prompt, whereby the few-shot demonstrations include both verification questions and their answers immediately after the questions. In this approach separate prompts are not needed.</p>
<p>2-Step A potential disadvantage of the joint method is that because the verification questions must condition on the baseline response in the LLM context, and the method is joint, the verification answers have to condition on the initial response as well. This may increase the likelihood of repetition, another known issue of modern LLMs (Holtzman et al., 2019). This means the verification questions might hallucinate similarly to the original baseline response, which defeats the purpose. We hence instead separate the planning and execution into separate steps, both with their own LLM prompt. The planning prompt conditions on the baseline response in the first step. The verification questions generated from planning are answered in the second step, where crucially the context given to the LLM prompt only contains the questions, and not the original baseline response and hence cannot repeat those answers directly.</p>
<p>Factored Another, more sophisticated approach, is to answer all questions independently as separate prompts. Again, crucially, those prompts do not contain the original baseline response and are hence not prone to simply copying or repeating it. The factored approach has the further advantage of removing any potential interference not only from the baseline response, but also between answer contexts, and is somewhat related to the recent (concurrent) work of Radhakrishnan et al. (2023) for subquestion answering by factored decomposition, hence we adopt their naming. It can also potentially handle more verification questions by virtue of them not all having to fit with the same single context. While this is potentially more computationally expensive, requiring the execution of many more LLM prompts, they can be run in parallel, and hence be batched. In order to do this, we first have to take the set of generated questions from subsection 3.2 and parse them into separate questions, which is a relatively easy task as the few-shot demonstrations we provide indicate they should be generated as a comma-separated list. We can then split them out into separate LLM prompts.</p>
<p>Factor+Revise After answering the verification questions, the overall CoVe pipeline then has to either implicitly or explicitly cross-check whether those answers indicate an inconsistency with the original responses. In the factor+revise approach, we execute this as a deliberate step via an extra LLM prompt, which may make it easier for the final system to reason about this step explicitly. Differently to answering the verification questions, the cross-checking phase needs to condition on both the baseline response and the verification question and answer. We thus execute this as separate LLM prompts, one "cross-check" prompt for each question, with again a set of few-shot</p>
<p>demonstrations showing the desired output. For example if the original baseline response contained the phrase "It followed in the wake of the 1845 U.S. annexation of Texas. . ." and CoVe generated a verification question When did Texas secede from Mexico? which was answered with 1836 then an inconsistency should be detected by this step.</p>
<h1>3.4 Final Verified RESPONSE</h1>
<p>Finally, the improved response that takes verification into account is generated. This is executed by a final few-shot prompt where the context takes into account all of the previous reasoning steps, the baseline response and verification question answer pairs, so that the corrections can take place. If the Factor+Revise approach is used from subsection 3.3 then the output of the cross-check inconsistency detection is provided as well.</p>
<h2>4 EXPERIMENTS</h2>
<p>We use various experimental benchmarks to measure the efficacy of CoVe in reducing hallucination, comparing against a number of baselines.</p>
<h3>4.1 TASKS</h3>
<p>The benchmarks we use range from list-based questions where the required answer is a set of entities, to where the answer is a longform generation of multiple freeform sentences.</p>
<h3>4.1.1 WIKIDATA</h3>
<p>We start by testing CoVe on a set of automatically generated questions using the Wikidata API ${ }^{1}$. We create list questions of the form: "Who are some [Profession]s who were born in [City]?". For example, "Who are some politicians who were born in Boston?". The answer to these questions is a set of entities, where the gold list is obtained from the Wikidata knowledge base. This results in a dataset of 56 test questions, each typically containing $\sim 600$ known gold entities, but typically an LLM will produce a much shorter list. We then use the precision metric (micro-averaged) to measure performance, in addition to reporting the averaged number of positive and negative entities produced.</p>
<h3>4.1.2 WIKI-CATEGORY LIST</h3>
<p>We then proceed to a harder set-generation task. We use the QUEST (Malaviya et al., 2023) dataset that was created using Wikipedia Category lists. We convert these category names to questions by simply prepending a "Name some". Owing to the varied questions such as Name some Mexican animated horror films or Name some Endemic orchids of Vietnam we believe this task can pose a greater challenge. We collate all examples in the dataset that do not require logical operations to create a set of 55 test questions each having 8 answers. Similar to the Wikidata task, we measure precision (micro-averaged) to measure performance, in addition to reporting the averaged number of positive and negative entities produced.</p>
<h3>4.1.3 MultiSPANQA</h3>
<p>We next test our approach on an reading comprehension benchmark, MultiSpanQA (Li et al., 2022). MultiSpanQA comprises of questions that have multiple independent answers (derived from a series of multiple discontiguous spans in the text, with questions originally from the Natural Questions dataset). We consider a closed-book setting, where we do not provide supporting documents, and hence consider a subset of questions which are factoid-based, so that our base LLM is more likely to be able to answer them. We thus use a test set of 418 questions with shorter answers per span (up to 3 tokens per item). For example, Q: Who invented the first printing press and in what year?, A: Johannes Gutenberg, 1450.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Wikidata <br> (Easier)</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Wiki-Category list <br> (Harder)</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LLM</td>
<td style="text-align: left;">Method</td>
<td style="text-align: left;">Prec. $(\uparrow)$</td>
<td style="text-align: left;">Pos.</td>
<td style="text-align: left;">Neg.</td>
<td style="text-align: left;">Prec. $(\uparrow)$</td>
<td style="text-align: left;">Pos.</td>
<td style="text-align: left;">Neg.</td>
</tr>
<tr>
<td style="text-align: left;">Llama 2 70B Chat</td>
<td style="text-align: left;">Zero-shot</td>
<td style="text-align: left;">0.12</td>
<td style="text-align: left;">0.55</td>
<td style="text-align: left;">3.93</td>
<td style="text-align: left;">0.05</td>
<td style="text-align: left;">0.35</td>
<td style="text-align: left;">6.85</td>
</tr>
<tr>
<td style="text-align: left;">Llama 2 70B Chat</td>
<td style="text-align: left;">CoT</td>
<td style="text-align: left;">0.08</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">8.92</td>
<td style="text-align: left;">0.03</td>
<td style="text-align: left;">0.30</td>
<td style="text-align: left;">11.1</td>
</tr>
<tr>
<td style="text-align: left;">Llama 65B</td>
<td style="text-align: left;">Few-shot</td>
<td style="text-align: left;">0.17</td>
<td style="text-align: left;">0.59</td>
<td style="text-align: left;">2.95</td>
<td style="text-align: left;">0.12</td>
<td style="text-align: left;">0.55</td>
<td style="text-align: left;">4.05</td>
</tr>
<tr>
<td style="text-align: left;">Llama 65B</td>
<td style="text-align: left;">CoVe (joint)</td>
<td style="text-align: left;">0.29</td>
<td style="text-align: left;">0.41</td>
<td style="text-align: left;">0.98</td>
<td style="text-align: left;">0.15</td>
<td style="text-align: left;">0.30</td>
<td style="text-align: left;">1.69</td>
</tr>
<tr>
<td style="text-align: left;">Llama 65B</td>
<td style="text-align: left;">CoVe (two-step)</td>
<td style="text-align: left;">$\mathbf{0 . 3 6}$</td>
<td style="text-align: left;">0.38</td>
<td style="text-align: left;">0.68</td>
<td style="text-align: left;">0.21</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.52</td>
</tr>
<tr>
<td style="text-align: left;">Llama 65B</td>
<td style="text-align: left;">CoVe (factored)</td>
<td style="text-align: left;">0.32</td>
<td style="text-align: left;">0.38</td>
<td style="text-align: left;">0.79</td>
<td style="text-align: left;">$\mathbf{0 . 2 2}$</td>
<td style="text-align: left;">0.52</td>
<td style="text-align: left;">1.52</td>
</tr>
</tbody>
</table>
<p>Table 1: Test Precision and average number of positive and negative (hallucination) entities for list-based questions on the Wikidata and Wiki-Category list tasks.</p>
<h1>4.1.4 LONGFORM GENERATION OF BIOGRAPHIES</h1>
<p>We next validate the performance of CoVe on longform text generation. In this setting, we evaluate our method on generating biographies, adopting the benchmark proposed in by Min et al. (2023). Here the model is simply prompted to generate a biography of a selected entity using the prompt: "Tell me a bio of <entity>". We evaluate the efficacy of our approach using the FACTSCORE metric (Min et al., 2023) developed in that work, which uses a retrieval-augmented language model to fact-check the response (Instruct-Llama, "Llama + Retrieval + NP"), which they showed correlates well with human judgments.</p>
<h3>4.2 BASELINES</h3>
<p>We use Llama 65B, a strong open model as our base LLM (Touvron et al., 2023a), and use greedy decoding for all models. As Llama 65B is not instruction fine-tuned, we employ few-shot examples particular to each task for measuring performance on each of our benchmarks. This serves as our main baseline which CoVe tries to improve upon. CoVe uses the same Llama 65B base, but includes, for the same few-shot examples, demonstrations of verification questions and final verified responses, following Figure 1 and section 3. Thus, we measure the ability to improve over the original baseline response for the same LLM. For CoVe, we compare different variants, particularly the joint and factored versions on all tasks.</p>
<p>We also compare to Llama instruction fine-tuned models, for which we use Llama 2 (Touvron et al., 2023b). We measure both zero-shot performance on the task, or zero-shot with chain-of-thought by adding "Let's think step by step" to the zero-shot prompt. We find that the instruction fine-tuned models tend to generate extraneous content when queried. This can especially be a problem for the list-based tasks. To deal with this we add an extra line to our prompt: "List only the answers separated by a comma". We also add another layer of post-processing to extract the answers by using an off-the-shelf NER model to further avoid this issue as this helped. However, we still expect few-shot to improve over this, especially for tasks like Multi-Span-QA where the answers are not all named entities, and the few-shot examples effectively show the domain of the task.</p>
<p>For the longform generation of biographies we also compare to several existing model results reported in Min et al. (2023), in particular InstructGPT (Ouyang et al., 2022), ChatGPT ${ }^{2}$ and PerplexityAI ${ }^{3}$.</p>
<h3>4.3 RESULTS</h3>
<p>We are interested in empirically answering the following research questions:
RQ1 Can CoVe effectively reduce the rate of hallucinatory content produced by the LLM?
RQ2 Can CoVE be used to fix or remove incorrect generations without decreasing the amount of correct content?</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLM</th>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">F1 ( $\uparrow$ )</th>
<th style="text-align: center;">Prec.</th>
<th style="text-align: center;">Rec.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Llama 2 70B Chat</td>
<td style="text-align: left;">Zero-shot</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.40</td>
</tr>
<tr>
<td style="text-align: left;">Llama 2 70B Chat</td>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: left;">Llama 65B</td>
<td style="text-align: left;">Few-shot</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.38</td>
</tr>
<tr>
<td style="text-align: left;">Llama 65B</td>
<td style="text-align: left;">CoVe (joint)</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: left;">Llama 65B</td>
<td style="text-align: left;">CoVe (factored)</td>
<td style="text-align: center;">$\mathbf{0 . 4 8}$</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.46</td>
</tr>
</tbody>
</table>
<p>Table 2: Closed book MultiSpanQA test performance, comparing CoVe with various baselines.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LLM</th>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">FACTSCORE. $(\uparrow)$</th>
<th style="text-align: center;">Avg. # facts</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">InstructGPT ${ }^{+}$</td>
<td style="text-align: left;">Zero-shot</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">26.3</td>
</tr>
<tr>
<td style="text-align: left;">ChatGPT ${ }^{+}$</td>
<td style="text-align: left;">Zero-shot</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">34.7</td>
</tr>
<tr>
<td style="text-align: left;">PerplexityAI ${ }^{+}$</td>
<td style="text-align: left;">Retrieval-based</td>
<td style="text-align: center;">61.6</td>
<td style="text-align: center;">40.8</td>
</tr>
<tr>
<td style="text-align: left;">Llama 2 70B Chat</td>
<td style="text-align: left;">Zero-shot</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">64.9</td>
</tr>
<tr>
<td style="text-align: left;">Llama 2 70B Chat</td>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">41.1</td>
<td style="text-align: center;">49.0</td>
</tr>
<tr>
<td style="text-align: left;">Llama 65B</td>
<td style="text-align: left;">Few-shot</td>
<td style="text-align: center;">55.9</td>
<td style="text-align: center;">16.6</td>
</tr>
<tr>
<td style="text-align: left;">Llama 65B</td>
<td style="text-align: left;">CoVe (joint)</td>
<td style="text-align: center;">60.8</td>
<td style="text-align: center;">12.8</td>
</tr>
<tr>
<td style="text-align: left;">Llama 65B</td>
<td style="text-align: left;">CoVe (factored)</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">11.7</td>
</tr>
<tr>
<td style="text-align: left;">Llama 65B</td>
<td style="text-align: left;">CoVe (factor+revise)</td>
<td style="text-align: center;">$\mathbf{7 1 . 4}$</td>
<td style="text-align: center;">12.3</td>
</tr>
</tbody>
</table>
<p>Table 3: Longform generation of biographies with metrics defined from Min et al. (2023). Models marked with $*$ are reported from previous work. FACTSCORE automatically computed using "InstructLlama" ( Retrieve $\rightarrow$ LM + NP), the best open-access model.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: FACTSCORE performance distribution across head, torso and tail facts for CoVe variants and various baselines on longform generation of biographies.</p>
<p>Our main results across the four benchmark tasks are given in Table 1, Table 2 and Table 3, and our main findings are as follows.</p>
<p>CoVe improves precision on list-based answer tasks We find that CoVe provides large gains in precision on the list-based tasks, e.g. more than doubles the precision from the Llama 65B few-shot baseline for the Wikidata task (from 0.17 to 0.36 ). We find from the positive and negative breakdown that there is a large reduction in the number of hallucinated answers (negatives: $2.95 \rightarrow 0.68$ ) while only a relatively small reduction in the number of non-hallucinations (positives: $0.59 \rightarrow 0.38$ ).</p>
<p>CoVe improves performance on closed book QA We also find that CoVe brings improvements in general QA problems, as measured on MultiSpanQA. We observe a $23 \%$ improvement in F1 over the few-shot baseline $(0.39 \rightarrow 0.48)$, where the improvements come from gains in both precision and recall.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Verification Execution <br> CoVe (joint)</th>
<th style="text-align: left;">CoVe (factored)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Verification Plan</td>
<td style="text-align: left;">Prec.</td>
<td style="text-align: left;">Prec.</td>
</tr>
<tr>
<td style="text-align: left;">Rule-based questions</td>
<td style="text-align: left;">0.13</td>
<td style="text-align: left;">0.16</td>
</tr>
<tr>
<td style="text-align: left;">Generated by model: <br> yes/no questions <br> general questions</td>
<td style="text-align: left;">0.15</td>
<td style="text-align: left;">0.19</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of various CoVe verification plan strategies (rows) and verification execution techniques (columns) on the Wiki-Category task.</p>
<p>CoVe improves precision on longform generation These results also extend to longform generation, where we actually see larger gains than in the QA setting. FACTSCORE increases $28 \%$ ( $55.9 \rightarrow$ 71.4) from the few-shot baseline, with again only a relatively small reduction in average number of facts provided ( $16.6 \rightarrow 12.3$ ). We also show the breakdown of improvements across facts in Figure 2, where one can see CoVe improves results for both rare and more frequent facts.</p>
<p>Instruction-tuning and CoT do not reduce hallucinations We find that the few-shot baseline that employs a pre-trained Llama model outperforms Llama 2 Chat, an instruction tuned model, across all the tasks. The few-shot examples lead the model to give outputs in line with those expected for the task, whereas general instruction tuning produces more hallucinations or incorrect outputs. Standard chain-of-thought (CoT) prompting also fails to improve the results for these tasks. While CoT has proven to help for reasoning tasks, it seems less appropriate for the issue of hallucination we measure in this work.</p>
<p>Factored and 2-step CoVe improve performance We observe a consistent performance improvement across all tasks from applying the factored CoVe approach compared to joint CoVe. For example improvement from $60.8 \rightarrow 63.7$ in FACTSCORE in longform generation. Similarly, the 2 -step approach also outperforms the joint approach, as tested on the Wikidata and Wiki-Category list tasks, with 2-step giving the best results for Wikidata, and factored the best for Wiki-Category. All these results support our hypothesis that verifying questions should not attend to the original baseline response as they may be prone to repeating it (as the joint method can do).</p>
<p>Further explicit reasoning helps remove hallucinations In the longform generation task we also explore more sophisticated reasoning steps in the CoVe "factor+revise" method, which explicitly cross-checks whether verification answers indicate an inconsistency. We see large gains in the FACTSCORE metric from this further explicit reasoning from 63.7 (factored) $\rightarrow 71.4$ (factor+revise). This gives further indication that appropriate and explicit reasoning in LLMs can bring improvements in mitigating hallucinations.</p>
<p>CoVe-based Llama outperforms InstructGPT, ChatGPT and PerplexityAI On the longform generation task, our baseline few-shot Llama 65B is outperformed by the ChatGPT and PerplexityAI models in terms of the FACTSCORE metric. However, applying CoVe to the baseline Llama 65B lifts its performance above both ChatGPT and PerplexityAI, as well as outperforming InstructGPT. This is particularly impressive compared to PerplexityAI considering that is a model that can support its facts with retrieval-augmentation, whereas CoVe uses only the base language model itself with improved reasoning via deliberation (verification). However, we can see in Figure 2 PerplexityAI still outperforms CoVe for very rare facts where retrieval is essential, but CoVe outperforms PerplexityAI for more frequent facts. We note that some models produce less overall facts than others, however the FACTSCORE metric is normalized and hence comparable across models. We verified this experimentally by clipping Llama 270 B chat's output to present less facts (as it contains the largest number in its output out of all models), but this did not change its FACTSCORE substantially, e.g. clipping to 10 sentences increased its score from $41.3 \rightarrow 42.7$. We note the length of the generations of the few-shot-based models are essentially governed by the few-shot examples, which in-turn are constrained by the context length.</p>
<p>Shortform verification questions are more accurately answered than longform queries In a longform response, LLMs are prone to generate a number of hallucinations. However, it can often be the case that the LLM itself would know these hallucinations are wrong if queried specifically for that individual fact, independent of the rest of the longform generation, see Figure 1, Figure 3, and section 9 . This can be seen quantitatively on the Wikidata task, where only $\sim 17 \%$ of the Llama few-shot baseline answer entities are correct in list-based questions. However, when querying each individual entity via a verification question, we find $\sim 70 \%$ are correctly answered.</p>
<p>LLM-based verification questions outperforms heuristics In our method, CoVe, the verification questions are generated by the LLM dependent on the task. We compare the quality of these questions to heuristically constructed ones in order to measure their quality, by replacing the LLM questions with templated yes/no questions of the form "Does $X$ answer the question" for list-based questions with elements $X$ in the answer. Results on the Wiki-Category task, given in Table 4, show a reduced precision with rule-based verification questions. We believe this difference would be larger for longform generation where the types of required verification questions can be more diverse, and LLM-based verification becomes even more necesary.</p>
<p>Open verification questions outperform yes/no-based questions In our main experiments we use verification questions where the expected answers are true facts. An alternative setup is to include the fact as part of the verification question and ask it in a yes/no answer format. We evaluate this difference in Table 4, and find that yes/no type questions perform worse for the factored version of CoVe. Some anecdotal examples are included in Appendix section 9 for ChatGPT where we find the model tends to agree with facts in a yes/no question format whether they are right or wrong.</p>
<h1>5 CONCLUSION</h1>
<p>We introduced Chain-of-Verification (CoVe), an approach to reduce hallucinations in a large language model by deliberating on its own responses and self-correcting them. In particular, we showed that models are able to answer verification questions with higher accuracy than when answering the original query by breaking down the verification into a set of simpler questions. Secondly, when answering the set of verification questions, we showed that controlling the attention of the model so that it cannot attend to its previous answers (factored CoVe) helps alleviate copying the same hallucinations. Overall, our method provides substantial performance gains over the original language model response just by asking the same model to deliberate on (verify) its answer. An obvious extension to our work is to equip CoVe with tool-use, e.g., to use retrieval augmentation in the verification execution step which would likely bring further gains.</p>
<h2>6 LIMITATIONS</h2>
<p>While our Chain-of-Verification (CoVe) method seeks to reduce hallucinations, it does not remove them completely from generations. This means that CoVe can still generate incorrect or misleading information for a given query, even if it improves over the baseline. We also note that in our experiments we have only addressed hallucinations in the form of directly stated factual inaccuracies. However, hallucinations could come in other forms, such as during incorrect reasoning steps, as part of opinions, etc. We also note that the generations CoVe produces come with verifications which, if viewed by the user, add more interpretability to its decisions, but come at the cost of increased computational expense due to generating more tokens in the output, similar to other reasoning methods such as Chain-of-Thought.</p>
<p>Our method seeks to make a large language model produce improved responses by spending more time deliberating to identify its own mistakes. While we have shown this gives clear improvements, the upper bound to the improvement is clearly limited by the overall capabilities of the model, e.g. in identifying and knowing what it knows. In this regard, an orthogonal line of research, as discussed in section 2 is the use of external tools by language models, to gain further information beyond what is stored in its weights. While we do not explore that avenue in this work those techniques would likely be fruitful to combine with the findings here.</p>
<h1>REFERENCES</h1>
<p>Leonard Adolphs, Kurt Shuster, Jack Urbanek, Arthur Szlam, and Jason Weston. Reason first, then respond: Modular generation for knowledge-infused dialogue. arXiv preprint arXiv:2111.05204, 2021.</p>
<p>Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. Do language models know when they're hallucinating references? arXiv preprint arXiv:2305.18248, 2023.</p>
<p>I Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu, et al. Factool: Factuality detection in generative ai-a tool augmented framework for multi-task and multi-domain scenarios. arXiv preprint arXiv:2307.13528, 2023a.</p>
<p>I-Chun Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig, et al. Improving factuality of abstractive summarization via contrastive reward learning. arXiv preprint arXiv:2307.04507, 2023b.</p>
<p>Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. Lm vs lm: Detecting factual errors via cross examination. arXiv preprint arXiv:2305.13281, 2023.</p>
<p>Boris A Galitsky. Truth-o-meter: Collaborating with llm in fighting its hallucinations. 2023.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, et al. Rarr: Researching and revising what language models say, using language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16477-16508, 2023.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023.</p>
<p>Weisen Jiang, Han Shi, Longhui Yu, Zhengying Liu, Yu Zhang, Zhenguo Li, and James T Kwok. Backward reasoning in large language models for verification. arXiv preprint arXiv:2308.07758, 2023a.</p>
<p>Zhengbao Jiang, Frank F Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. arXiv preprint arXiv:2305.06983, 2023b.</p>
<p>Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022.</p>
<p>Jack Lanchantin, Shubham Toshniwal, Jason Weston, Arthur Szlam, and Sainbayar Sukhbaatar. Learning to reason and memorize with self-notes. arXiv preprint arXiv:2305.00833, 2023.</p>
<p>Haonan Li, Martin Tomko, Maria Vasardani, and Timothy Baldwin. Multispanqa: A dataset for multi-span question answering. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. $1250-1260,2022$.</p>
<p>Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. arXiv preprint arXiv:2306.03341, 2023.</p>
<p>Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. Deductive verification of chain-of-thought reasoning. arXiv preprint arXiv:2306.03872, 2023.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.</p>
<p>Chaitanya Malaviya, Peter Shaw, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Quest: A retrieval dataset of entity-seeking queries with implicit set operations. arXiv preprint arXiv:2305.11694, 2023.</p>
<p>Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896, 2023.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. On faithfulness and factuality in abstractive summarization. arXiv preprint arXiv:2005.00661, 2020.</p>
<p>Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching language models to support answers with verified quotes. arXiv preprint arXiv:2203.11147, 2022.</p>
<p>Ning Miao, Yee Whye Teh, and Tom Rainforth. Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. arXiv preprint arXiv:2308.00436, 2023.</p>
<p>Sabrina J Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau. Reducing conversational agents’ overconfidence through linguistic calibration. Transactions of the Association for Computational Linguistics, 10:857-872, 2022.</p>
<p>Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. Factscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv preprint arXiv:2305.14251, 2023.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813, 2023.</p>
<p>Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019 .</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022 .</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol Chen, Carson Denison, Danny Hernandez, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamilė Lukošiūtė, et al. Question decomposition improves the faithfulness of model-generated reasoning. arXiv preprint arXiv:2307.11768, 2023.</p>
<p>Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. Measuring attribution in natural language generation models. Computational Linguistics, pp. 1-66, 2023.</p>
<p>Paul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu Geist, Sertan Girgin, Léonard Hussenot, Orgad Keller, et al. Factually consistent summarization via reinforcement learning with textual entailment feedback. arXiv preprint arXiv:2306.00186, 2023.</p>
<p>Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M Smith, et al. Recipes for building an open-domain chatbot. arXiv preprint arXiv:2004.13637, 2020.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567, 2021.</p>
<p>Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, and Xin Luna Dong. Head-to-tail: How knowledgeable are large language models (llm)? aka will llms replace knowledge graphs? arXiv preprint arXiv:2308.10168, 2023a.</p>
<p>Weiwei Sun, Zhengliang Shi, Shen Gao, Pengjie Ren, Maarten de Rijke, and Zhaochun Ren. Contrastive learning reduces hallucination in conversations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 13618-13626, 2023b.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b.</p>
<p>Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. arXiv preprint arXiv:2307.03987, 2023.</p>
<p>Chaojun Wang and Rico Sennrich. On exposure bias, hallucination and domain shift in neural machine translation. arXiv preprint arXiv:2005.03642, 2020.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.</p>
<p>Yixuan Weng, Minjun Zhu, Shizhu He, Kang Liu, and Jun Zhao. Large language models are reasoners with self-verification. arXiv preprint arXiv:2212.09561, 2022.</p>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. arXiv preprint arXiv:2306.01693, 2023.</p>
<p>Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish Sabharwal. Improving language models via plug-and-play retrieval feedback. arXiv preprint arXiv:2305.14002, 2023.</p>
<p>Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A Smith. How language model hallucinations can snowball. arXiv preprint arXiv:2305.13534, 2023.</p>
<p>Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei Qin, and Lidong Bing. Verify-and-edit: A knowledge-enhanced chain-of-thought framework. arXiv preprint arXiv:2305.03268, 2023.</p>
<h1>7 COVE - FURTHER DETAILS</h1>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: For longform generation, the Chain-of-Verification (CoVe) Factor + Revise method is the most effective in our longform generation experiments. CoVe Factor + Revise has the model independently identify (cross-check) which facts are consistent with its executed verifications (indicated by tickmark and crosses in the figure). With this extra step we aim to disregard the inconsistent facts and use the consistent facts to regenerate the response.</p>
<h1>8 Prompt Templates</h1>
<p>We provide prompt templates for the longform generation of biographies task below for the different steps and variants of CoVe (see section 3). Templates for the other tasks are similar, but using few-shot examples from those tasks instead.</p>
<h3>8.1 GENERATE BASELINE RESPONSE</h3>
<div class="codehilite"><pre><span></span><code><span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Tell</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bio</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">&lt;</span><span class="n">person</span><span class="o">&gt;</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">bio</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">person</span><span class="o">&gt;</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Tell</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bio</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">&lt;</span><span class="n">person</span><span class="o">&gt;</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">bio</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">person</span><span class="o">&gt;</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Tell</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bio</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">&lt;</span><span class="n">person</span><span class="o">&gt;</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">bio</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">person</span><span class="o">&gt;</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Tell</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bio</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">&lt;</span><span class="n">person</span><span class="o">&gt;</span>
<span class="n">A</span><span class="o">:</span>
</code></pre></div>

<p>Table 5: Few-shot prompting with 3 few-shot examples for the longform generation of biographies task. Other tasks use the same standard few-shot setup as well (with 3 examples from that particular task).</p>
<h3>8.2 Plan Verifications</h3>
<div class="codehilite"><pre><span></span><code><span class="n">Context</span><span class="o">:</span><span class="w"> </span><span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Tell</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bio</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">&lt;</span><span class="n">person</span><span class="o">&gt;.</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">passage</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">person</span><span class="o">&gt;</span>
<span class="n">Response</span><span class="o">:</span>
<span class="o">&lt;</span><span class="n">fact</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">passage</span><span class="o">&gt;,</span><span class="w"> </span><span class="n">Verification</span><span class="w"> </span><span class="n">Question</span>
<span class="o">&lt;</span><span class="n">fact</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">passage</span><span class="o">&gt;,</span><span class="w"> </span><span class="n">Verification</span><span class="w"> </span><span class="n">Question</span>
<span class="n">Context</span><span class="o">:</span><span class="w"> </span><span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Tell</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bio</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">&lt;</span><span class="n">person</span><span class="o">&gt;.</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">passage</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">person</span><span class="o">&gt;</span>
<span class="n">Response</span><span class="o">:</span>
<span class="o">&lt;</span><span class="n">fact</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">passage</span><span class="o">&gt;,</span><span class="w"> </span><span class="n">Verification</span><span class="w"> </span><span class="n">Question</span>
<span class="o">&lt;</span><span class="n">fact</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">passage</span><span class="o">&gt;,</span><span class="w"> </span><span class="n">Verification</span><span class="w"> </span><span class="n">Question</span>
<span class="n">Context</span><span class="o">:</span><span class="w"> </span><span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Tell</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bio</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">&lt;</span><span class="n">person</span><span class="o">&gt;.</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">passage</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">person</span><span class="o">&gt;</span>
<span class="n">Response</span><span class="o">:</span>
<span class="o">&lt;</span><span class="n">fact</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">passage</span><span class="o">&gt;,</span><span class="w"> </span><span class="n">Verification</span><span class="w"> </span><span class="n">Question</span>
<span class="o">&lt;</span><span class="n">fact</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">passage</span><span class="o">&gt;,</span><span class="w"> </span><span class="n">Verification</span><span class="w"> </span><span class="n">Question</span>
<span class="n">Context</span><span class="o">:</span><span class="w"> </span><span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Tell</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bio</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="o">&lt;</span><span class="n">person</span><span class="o">&gt;.</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="o">&lt;</span><span class="n">passage</span><span class="w"> </span><span class="n">about</span><span class="w"> </span><span class="n">person</span><span class="o">&gt;</span>
<span class="n">Response</span><span class="o">:</span>
</code></pre></div>

<p>Table 6: Step (2) of CoVe involves planning the verification questions. In the biography task case we split the longform generation into its individual passages (e.g. sentences in the biography case, this was done due to excessive context length, which we don't need to do for the other tasks). The model then generates a verification question for each fact it observes in each passage (a passage may have multiple facts).</p>
<h1>8.3 EXECUTE VERIFICATIONS</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Verification</span><span class="w"> </span><span class="n">Question</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Answer</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Verification</span><span class="w"> </span><span class="n">Question</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Answer</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Verification</span><span class="w"> </span><span class="n">Question</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="n">Answer</span>
<span class="n">Q</span><span class="o">:</span><span class="w"> </span><span class="n">Verification</span><span class="w"> </span><span class="n">Question</span>
<span class="n">A</span><span class="o">:</span>
</code></pre></div>

<p>Table 7: In step (3) of CoVe, the model then generates an answer for each of the verification questions. Again we use 3 few-shot examples.</p>
<h3>8.4 GENERATE FinAL VERIFIED RESPONSE</h3>
<div class="codehilite"><pre><span></span><code>Context: &lt;Original Passage&gt;.
From another source,
&lt;output of execute verification step: Q + A&gt;
&lt;output of execute verification step: Q + A&gt;
Response: &lt;revised and consistent Passage&gt;
Context: &lt;Original Passage&gt;.
From another source,
&lt;output of execute verification step: Q + A&gt;
&lt;output of execute verification step: Q + A&gt;
Response: &lt;revised and consistent Passage&gt;
Context: &lt;Original Passage&gt;.
From another source,
&lt;output of execute verification step: Q + A&gt;
&lt;output of execute verification step: Q + A&gt;
Response:
</code></pre></div>

<p>Table 8: In step (4) of CoVe (factored) the model is then presented with its original generation (split into passages, e.g. sentences, in the biography case, due to excessive context length which we do not need to do for the other tasks) along with its own verification step results. The model is told that this information comes from "another source". The model is required to synthesize a new final answer based on facts that are consistent between the two sources.</p>
<h1>8.5 FACTOR+REVISE: IDENTIFY WHICH FACTS ARE CONSISTENT</h1>
<div class="codehilite"><pre><span></span><code>Context: &lt;Original Fact&gt;.
From another source,
&lt;output of execute verification step: Q + A&gt;
Response: CONSISTENT. &lt;Consistent fact&gt;
Context: &lt;Original Fact&gt;.
From another source,
&lt;output of execute verification step: Q + A&gt;
Response: INCONSISTENT.
Context: &lt;Original Fact&gt;.
From another source,
&lt;output of execute verification step: Q + A&gt;
Response: PARTIALLY CONSISTENT. &lt;Consistent part&gt;
</code></pre></div>

<p>Table 9: In the CoVe (Factor + Revise) variant, as part of step (3) after subsection 8.3, the model is made to explicitly identify which facts are consistent between the two sources. The consistent facts can then be spliced together.</p>
<h1>9 ChatGPT EXAMPLE SCREENSHOTS</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: ChatGPT generates several hallucinations for this question, e.g. Hillary Clinton and Michael Bloomberg.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Even when the longform answer is provided for a rewritten query (see query from Figure 4), while giving a slightly different answer, ChatGPT still generates several hallucinations for this question, e.g. Hillary Clinton and Michael Bloomberg.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Shortform questions (which could be verification questions) appear to be answered more factually than the longform answers in Figure 4 and Figure 5.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Another example of hallucinations for a different query, e.g., John F. Kennedy Jr was born in Washington D.C.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Examples where questions asking for a fact are answered correctly, but verifying via a yes/no question is incorrect (the model tends to agree with the way the question is stated, even if it was stated incorrectly).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://openai.com/blog/chatgpt
${ }^{3}$ www. perplexity.ai&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5127 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5127</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5127</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-267365166</p>
                <p><strong>Paper Title:</strong> <a href="https://bmcbioinformatics.biomedcentral.com/counter/pdf/10.1186/s12859-024-05847-x.pdf" target="_blank">Can large language models understand molecules?</a></p>
                <p><strong>Paper Abstract:</strong> Purpose Large Language Models (LLMs) like Generative Pre-trained Transformer (GPT) from OpenAI and LLaMA (Large Language Model Meta AI) from Meta AI are increasingly recognized for their potential in the field of cheminformatics, particularly in understanding Simplified Molecular Input Line Entry System (SMILES), a standard method for representing chemical structures. These LLMs also have the ability to decode SMILES strings into vector representations. Method We investigate the performance of GPT and LLaMA compared to pre-trained models on SMILES in embedding SMILES strings on downstream tasks, focusing on two key applications: molecular property prediction and drug-drug interaction prediction. Results We find that SMILES embeddings generated using LLaMA outperform those from GPT in both molecular property and DDI prediction tasks. Notably, LLaMA-based SMILES embeddings show results comparable to pre-trained models on SMILES in molecular prediction tasks and outperform the pre-trained models for the DDI prediction tasks. Conclusion The performance of LLMs in generating SMILES embeddings shows great potential for further investigation of these models for molecular embedding. We hope our study bridges the gap between LLMs and molecular embedding, motivating additional research into the potential of LLMs in the molecular representation field. GitHub: https://github.com/sshaghayeghs/LLaMA-VS-GPT.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What can large language models do in chemistry? A comprehensive benchmark on eight tasks. <em>(Rating: 2)</em></li>
                <li>Large-scale chemical language representations capture molecular structure and properties. <em>(Rating: 2)</em></li>
                <li>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5127",
    "paper_id": "paper-267365166",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What can large language models do in chemistry? A comprehensive benchmark on eight tasks.",
            "rating": 2,
            "sanitized_title": "what_can_large_language_models_do_in_chemistry_a_comprehensive_benchmark_on_eight_tasks"
        },
        {
            "paper_title": "Large-scale chemical language representations capture molecular structure and properties.",
            "rating": 2,
            "sanitized_title": "largescale_chemical_language_representations_capture_molecular_structure_and_properties"
        },
        {
            "paper_title": "Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction.",
            "rating": 1,
            "sanitized_title": "molecular_transformer_a_model_for_uncertaintycalibrated_chemical_reaction_prediction"
        }
    ],
    "cost": 0.0050635,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can large language models understand molecules?</p>
<p>Shaghayegh Sadeghi sadeghi3@uwindsor.ca 
Alan Bui 
Ali Forooghi 
Jianguo Lu 
Alioune Ngom 
Can large language models understand molecules?
81FB27C81E1F8AD4A278388AE8CCDBF310.1186/s12859-024-05847-xReceived: 31 December 2023 Accepted: 18 June 2024Large language modelsLLaMAGPTSMILES embedding
Molecule embedding is an important task in drug discovery[1,2], and finds wide applications in related tasks such as molecular property prediction [3-6], drug-target interaction (DTI) prediction [7-9] and drug-drug interaction (DDI) prediction[10,11].Molecule embedding techniques learn the features either from the molecular graphs that encode the connectivity information of a molecule structure or from the line annotations of their structures, such as the popular SMILES (simplified molecular-input lineentry system) representation[4].Molecule embedding via SMILES strings evolve and synchronize with the advances in language modelling [12, 13], starting with static word embedding [14], to contextualized</p>
<p>pre-trained models [4,15,16].These embedding techniques aim to capture relevant structural and chemical information in a compact numerical representation [17].The fundamental hypothesis asserts that structurally similar molecules behave in similar ways.This enables machine learning algorithms to process and analyze molecular structures for property prediction and drug discovery tasks.</p>
<p>With the breakthroughs made in LLMs, one prominent question is whether LLMs can understand molecules and make inferences on molecule data?More specifically, can LLMs produce high quality semantic representations?Gua et al. [18] made a preliminary study by evaluating several chemical inference tasks using LLMs.Their study has been limited to utilizing and evaluating LLMs performance in answering SMILES-related queries.We move further by exploring the ability of these models to effectively embed SMILES has yet to be fully explored, maybe partially due to cost of API calls.Our conclusions are:</p>
<p>(1) LLMs do outperform traditional methods.</p>
<p>(2) The performance is task dependent, sometimes data dependent.</p>
<p>(3) Newer versions of LLMs do improve over older versions, even though they are trained on more generic tasks.(4) We observe that embeddings from LLaMA overall outperform GPT embeddings.(5) Another interesting observation of our research is that LLaMA and LLaMa2 are very close regarding embedding performance.</p>
<p>Related work</p>
<p>For accurate prediction of chemical properties using machine learning, leveraging molecule embeddings as input feature vectors is crucial [19].Early molecular embedding methods such as Morgan FingerPrint (FP) [20] encode the structural information of a molecule into a fixed-length binary or integer vector with the knowledge of chemistry.However, for a more generalized embedding, numerous studies have explored methods to embed molecular structures.While some studies focus on the graph representation of the molecular structure to encode the important topology information directly [21][22][23], many choose the string representation of molecules (SMILES) due to rapid advancements in natural language processing (NLP).Initial efforts in this domain utilized foundational NLP architectures like auto-encoders [24] and recurrent neural networks (RNN) to generate embeddings [19].However, the scarcity of labelled data has shifted focus towards methods that can be pre-trained on unlabeled data, such as Mol2Vec and SPVec [14,25].</p>
<p>With the increasing prominence of transformer models in natural language analysis-where they are pre-trained on extensive unsupervised data and then fine-tuned for specific tasks like classification-transformer-based models have become increasingly relevant in the SMILES language domain.For instance, SMILES-BERT [15] has inspired numerous studies to adapt the transformers framework.These studies try to modify this framework to improve their performance on SMILES strings by adapting RoB-ERTa (Robustly optimized BERT approach) instead of the BERT model [6] or develop domain-specific self-supervised pre-training tasks [16], or integrate the local message passing mechanism of graph neural networks (GNNs) into BERT to enhance learning from molecular graphs [5].Additionally, MolFormer [4] introduces a novel approach by combining molecular language with transformer encoder models, incorporating rotary positional embeddings (RoPE) from RoFormer, to produce more effective molecular embeddings [4,26].</p>
<p>However, pre-training these models on millions of molecules requires substantial hardware resources.For example, MolFormer necessitates up to 16 V100 graphics processing units (GPUs) [4].Consequently, it is computationally more feasible to use pre-trained large language models (LLMs), such as GPT [27] and LLaMA [28,29], for generating embeddings.These models have already been trained on vast amounts of data, making them readily available for processing SMILES strings to obtain molecular embeddings without extensive hardware.</p>
<p>Up to our current knowledge, the application of GPT and LLaMA in chemistry has primarily been limited to utilizing and evaluating its performance in answering queries.Further exploration and implementation of LLMs for more advanced tasks within chemistry are yet to be thoroughly documented.For example, to examine how well LLMs understand chemistry, Guo et al. [18] used LLMs to assess the performance of these models on practical chemistry tasks only using queries.Their results demonstrate that GPT models are comparable with classical machine learning models when applied to chemical problems that can be transformed into classification or ranking tasks such as property prediction.However, they stop evaluating the LLM's ability to answer prompts and do not evaluate the embedding power of LLMs.Hence, inspired by many languagebased methods that tried to extract molecular embedding, our study represents a pioneering effort, being the first to rigorously assess the capabilities of LLMs like GPT and LLaMA in using LLMs embedding for chemistry tasks.</p>
<p>LLMs</p>
<p>LLMs, exemplified by architectures like BERT [12], GPT [27], LLaMA [28], and LLaMA2 [29] excel at understanding context within sentences and generating coherent text.They leverage attention mechanisms and vast training data to capture contextual information, making them versatile for text generation, translation, and sentiment analysis tasks.While Word2Vec enhances word-level semantics, language models provide a deeper understanding of context and facilitate more comprehensive language understanding and generation.Pre-trained models from LLMs can transform text into dense, highdimensional vectors, which capture contextual information and meaning.Using pretrained LLMs offers an edge as they transfer knowledge from their vast training data, enabling the extraction of context-sensitive representations without requiring extensive task-specific data or feature engineering [30].</p>
<p>This work focuses on obtaining the embeddings of SMILES strings from GPT and LLaMA models to find the model that achieves the best performance.OpenAI [31] present many GPT-based embeddings including: 'text-embedding-ada-002', 'text-embedding-3-small', 'text-embedding-3-large'.Our research used the most recent embedding model, text-small-3-embeddings.This model is acclaimed for being the best among available embedding models and the most affordable method available by OpenAI.Textsmall-3-embeddings employs the 'cl100k-base' token calculator to generate embeddings, resulting in a 1536-dimensional vector representation.We input SMILES strings into this model, allowing GPT to create embeddings for each string.These embeddings serve as the feature vector for our classification tasks.</p>
<p>In parallel, we leveraged the capabilities of LLaMA [28] and its advanced variant, LLaMA2 [29].These models, ranging from only 7 to 65 billion parameters, are built on the Transformers architecture.LLaMA2, an enhancement of LLaMA, benefits from training on an expanded publicly available data set.Its pre-training corpus grew by 40%, and its context length doubled to 4096 tokens.LLaMa models employ a decoderonly Transformer architecture with causal multi-headed attention in each layer.Drawing architectural inspiration from prominent language models like GPT-3 and PaLM (Pathways Language Model) [32], they incorporate features such as pre-normalization, RMSNorm, SwiGLU activation functions, and rotary positional embeddings (RoPE) [26] in every transformer layer.</p>
<p>The training dataset of LLaMA [28,33] predominantly comprises webpages, accounting for over 80% of its content.This is supplemented by various sources, including 6.5% code-centric data from GitHub and StackExchange, 4.5% literary content from books, and 2.5% scientific material primarily sourced from arXiv.</p>
<p>In contrast, GPT [33,34] was developed using a comprehensive and mixed dataset.This dataset includes diverse sources like CommonCrawl, WebText2, two different book collections (Books1 and Books2), and Wikipedia.</p>
<p>SMILES is utilized as a "chemical language" that encodes the structural elements of a chemical graph-including atoms, bonds, and rings-into a brief textual format.This is achieved through a systematic, depth-first tree traversal of the chemical structure.The method uses alphanumeric characters to represent atoms (such as C, S, Br) and symbols such as '-' , ' = ' , and ' # ' to indicate different types of chemical bonds.For instance, the SMILES notation for Ibuprofen is CC(C)Cc1ccc(cc1)C(C)C(O)=O (Fig. 1).</p>
<p>Fig. 1 Drug chemical representations</p>
<p>Table 1 compares how each model tokenizes SMILES strings.ChemBERTa, explicitly designed for molecular embeddings, tokenizes SMILES using the Byte-Pair Encoder (BPE) strategy.Meanwhile, MolFormer-XL employs a SMILES-specific regular expression method, as described by Schwaller et al. [35], using an atom-wise tokenization strategy with the regular expression pattern that is formatted as follows and is able to differentiate between atom characters and symbols for chemical bonds: However, LLaMA, as a general-purpose model, employs a different tokenization approach.Its tokenizer is based on SentencePiece Byte-Pair Encoding (BPE).This tokenizer processes the input string character by character, searching for the largest known subword units it can match based on its training.Consequently, as it can be seen in Table 1, it treats 'CS' from the 'CCS(=O)(=O)CCBr' string as a single token, possibly interpreting it as an abbreviation in natural language.However, 'C' and 'S' should be considered as separate tokens, since each represents a distinct atom.</p>
<p>Table 2 compares molecular embedding in terms of the number of layers, parameters and their speed in generating a SMILES embedding.Compared with Morgan FP, language models are extremely slow.However, GPT performs the fastest among the language models, while LLaMA models are the slowest.There is also a relation between the number of layers and the speed of embedding generation.Although GPT remains an exception.</p>
<p>Experiments</p>
<p>Our study aims to generate molecular representation via LLMs and then evaluate the representation on various downstream tasks.To demonstrate the effectiveness of LLMs' molecular representations, we benchmarked their performance on numerous challenging classification and regression tasks from MoleculeNet [36] as well as link prediction from BioSnap [37] and DrugBank [38].The objective of link prediction in this research is to map the drugs as nodes and their interactions as edges and identify whether there is a missing edge between two drug nodes.</p>
<p>Experimental setup</p>
<p>We experimented with seven models, each evaluated by six classifications, three regression and two link prediction tasks.To generate embeddings from LLaMAs, BERT, ChemBERTa, and MolFormer models, we first download and load the model weights using the Transformers library and then generate the embeddings.For LLaMA weights, we download the weights provided by Meta for LLaMAs and then convert them into PyTorch format.We extract embeddings from the last layer of the LLMs, following the practice in [39].Pooling strategies can impact performance, and we explored a variety of combinations.The overall result remains the same.Hence, for the sake of simplicity, we use only the last layer.For GPT embeddings, we choose the recent model, text-small-3-embeddings.</p>
<p>To generate LLaMA and LLaMA2 embeddings, we employed four NVIDIA A2 GPUs to load the 7 billion parameter version of LLaMAs.In this configuration, the average speed of generating embeddings is one molecule per second.In our experiments, we generated embeddings for over 65,000 molecules.</p>
<p>Following MoleculeNet [36], for classification tasks, we partition the datasets into 5-stratified folds to ensure robust benchmarking.This approach ensures that each fold maintains the same proportion of observations for each target class as in the complete dataset.We employ a logistic regression model from scikit-learn, equipped with the following default parameters: L2 regularization, 'lbfgs' for optimization, and maximum 100 iterations allowed for the solvers to converge.The reported performance metrics are the mean and standard deviation of the F1-score and AUROC, calculated across the five folds.</p>
<p>For regression tasks, we implement five-fold cross-validation to assess model performance.We employ a Ridge regression model which is a linear regression model with l2 regularization.From scikit-learn with the following default parameters: tolerance of 0.001 for the optimization and a auto solver to automatically chooses the most appropriate solver method based on the data type.The metrics reported are the mean and standard deviation of the RMSE and the R 2 , calculated across the five folds.</p>
<p>Following MIRACLE [40], a state-of-the-art method in DDI, for link prediction, we split all interaction samples from the DrugBank and BioSnap datasets into training and test sets using a 4:1 ratio.We further select 1/4 of the training dataset as a validation set.The reported results are the mean and standard deviation of AUROC and AUPR across 10 different runs of the GCN model.We set each parameter learning rate using an exponentially decaying schedule with an initial learning rate of 0.0002 and a multiplicative factor of 0.96.For the proposed model's hyperparameters, we set the dimension of the hidden state of drugs as 256 and 3 layers for the GCN encoder.To further regularise the model, dropout with p = 0.3 is applied to every intermediate layer's output.We use Pytorch-geometric [41] for GCN.GCN Model is trained using Adam optimizer.</p>
<p>Benchmarking data sets</p>
<p>For classification and regression tasks, we use datasets from MoleculeNet [36], which is a collection of diverse datasets that cover a range of tasks, such as identifying properties like toxicity, bioactivity, and whether a molecule is an inhibitor.MoleculeNet is a widely used benchmark dataset in the field of computational chemistry and drug discovery and it is designed to evaluate and compare the performance of various machine learning models and algorithms on tasks related to molecular property prediction, compound screening, and other cheminformatics tasks [3-6, 18, 23, 42].</p>
<p>For the link prediction task, however, we utilize two DDI networks: BioSnap [37] and DrugBank [38].These datasets represent interactions among FDA-approved drugs as a biological network, with drugs as nodes and interactions as edges.</p>
<p>We extracted the SMILES strings of drugs in the DrugBank database.It should be noted that we conduct data removal because of some improper drug SMILES strings in Drugbank, which can not be converted into molecular graphs, as determined by the RDKit library.The errors include so-old storage format of SMILES strings, wrong characters, etc.Through these curation efforts, we have fortified the quality and coherence of our DDI network, ensuring its suitability for comprehensive analysis and interpretation.</p>
<p>For the BioSnap dataset, 1320 drugs have SMILES strings, while the DrugBank dataset has 1690 drugs with SMILES strings.Hence, the number of edges for BioSnap and Drug-Bank reduced to 41,577 and 190,609, respectively.</p>
<p>Performance analysis</p>
<p>Results on classification tasks</p>
<p>Figure 2a, Table 3, and 4 present our experiments on classification tasks.Surprisingly, LLaMA embeddings achieve comparable performance to established pre-trained models such as MolFormer-XL [4] and ChemBERTa [6] across all datasets.Conversely, GPT embeddings underperform in every case.Intriguingly, Morgan FP representations nearly match the performance of other pre-trained methods but are more computationally efficient; generating Morgan FP for a large dataset takes less than a minute without the need for a GPU, whereas LLaMA requires GPUs and processes only 117 molecules per minute (Table 2).We also tested other classifiers, including SVM and Random Forest, with similar results.The small standard deviation in the evaluation scores indicates that these performance differences are statistically significant.Despite ChemBERTa and Mol-Former-XL being pre-trained on millions of compounds from PubChem and ZINC, they perform comparably or, in some instances, less effectively than the BERT model.This showcases the importance of fine-tuning the results of pre-trained models.</p>
<p>Results on regression tasks</p>
<p>Figure 2a and Table 5 present the evaluation results for the regression tasks.Similar to the classification results, GPT underperforms relative to other models, and in some instances, it even falls short of Morgan Fingerprint's performance.ChemBERTa</p>
<p>Table 3 Results on classification tasks</p>
<p>The reported performance metrics are the mean and standard deviation of the F1-score and AUROC, calculated across the five-folds.The Best Performance is Highlighted in Bold</p>
<p>Dataset</p>
<p>Table 4 Results on multi-task classification tasks</p>
<p>The reported performance metrics are the mean and standard deviation of the F1-score and AUROC, calculated across the five-folds.The Best Performance is Highlighted in Bold  consistently emerges as the top-performing model for regression across all tested datasets.BERT and LLaMA exhibit performances that are closely comparable to Chem-BERTa in the regression tasks.Additionally, we observed a general decline in the performance of all methods when applied to larger datasets, such as Lipophilicity.</p>
<p>Dataset</p>
<p>Results on link prediction tasks</p>
<p>Table 6 presents the results for the link prediction tasks on DDI networks.LLaMA consistently outperforms all other models across both datasets by a significant margin.Notably, Morgan FP surpasses the performance of embeddings from pre-trained models.</p>
<p>It appears that the size of the embeddings impacts model performance, as larger embeddings generally yield better results.Nevertheless, despite having the same size, there are still noticeable performance differences between the LLaMA and LLaMA2 models.</p>
<p>Ablation study</p>
<p>LLaMA Vs LLaMA2 Figure 3 compares the LLaMA and LLaMA2 models.The performance of these two models is similar, mainly across various tasks.However, there are notable differences in specific instances.For example, in the link prediction tasks (Table 6), LLaMA2 outperforms LLaMA.This trend is also observed in classification and regression tasks, where LLaMA2 generally matches or exceeds the performance of LLaMA.Both models share similar architecture and training presets.Nevertheless, LLaMA2 has been trained on 40% more data and supports twice the context length of its predecessor, enhancing its capability to understand more complex language structures [28,29].Dimension reduction We investigated the impact of dimension reduction on LLMs with substantial embedding sizes, as illustrated in Fig. 4. Using Principal Component Analysis (PCA) for dimension reduction, we experimented with various reduction sizes.Our findings indicate that the impact of dimension reduction on the   classification performance of GPT and LLaMA models is minimal, although there is a noticeable decrease in performance post-reduction.In contrast, for regression tasks, dimension reduction significantly lowers the performance of the models.This suggests a correlation between the size of the embeddings in LLMs and their effectiveness in handling regression tasks.LLM and anistropy It is well-documented that LLM embeddings suffer from the isotropy problem, meaning they are not uniformly distributed in terms of direction [43][44][45].Instead, these embeddings occupy a narrow cone in the vector space, making them anisotropic.The anisotropy problem in LLM model embeddings is evident from  Our comparative analysis also reveals that LLMs embeddings demonstrate a higher degree of anisotropy than pre-trained embeddings and Morgan FP (Fig. 5).This is evident since the distribution of cosine similarity of embeddings is more closely grouped together in their representation (Fig. 5).However, our experiments indicate that better isotropy does not imply a performance gain in machine-learning tasks.As can be  seen, the cosine similarity distribution of LLaMA2 embeddings is a lot narrower than GPT and Morgan FP; however, LLaMA2 outperforms both models in most cases.</p>
<p>As illustrated in Fig. 6, we also noticed that the PCA representation of GPT's embeddings is predominantly concentrated within a range smaller than 1.This observation also suggests a high likelihood that the GPT embeddings have been pre-normalized.</p>
<p>GPT Vs LLaMA Figure 7 demonstrates that LLaMA consistently outperforms GPT across all datasets by a significant margin.This raises the question of whether these differences are due to the architectural design or the specific training of the models.As outlined in the GPT-4 technical report, GPT models are capable of interpreting SMILES strings.Notably, approximately 2.5% of the LLaMA training dataset, as reported in [28,33], consists of scientific material primarily sourced from arXiv, including bioinformatics papers.</p>
<p>Both LLaMA and GPT models utilize a transformer-based architecture with a heavy reliance on self-attention mechanisms and a decoder-only configuration.However, the opaque nature of GPT as a "black box" model complicates direct comparisons with LLaMA regarding whether their efficiency stems solely from architecture or pre-training specifics.Nonetheless, considering their training on SMILES strings, the data from Fig. 7 and Table 6 suggest that the LLaMA architecture is particularly adept at handling complex language structures like SMILES strings.Furthermore, Table 1 reveals that while the LLaMA2 tokenizer may not perform as well as the MolFormer tokenizer, it tokenizes SMILES strings more effectively than BERT.Unfortunately, we cannot compare the GPT tokenizer directly with other models due to limitations in OpenAI's API access.Link prediction with SMILES VS drug description We also extracted the text-format drug description information of drugs from the DrugBank database.Drug description embedding in DDI prediction significantly outperforms using SMILES strings when leveraging LLMs.This enhancement is consistent with applying LLMs pre-trained on general text data, as depicted in Fig. 8.When applied to drug descriptions closer to natural language, GPT outperforms the LLaMA models on both datasets and both AUROC and AUPRC metrics.</p>
<p>Conclusions</p>
<p>In summary, this research underscores the potential of LLMs like GPT and LLaMA for molecular embedding.We specifically recommend LLaMA models over GPT due to their superior performance in generating molecular embeddings from SMILES strings, which is notable in our studies.These findings suggest that LLaMA could be particularly effective in predicting molecular properties and drug interactions.Although models like LLaMA and GPT are not explicitly designed for SMILES string embedding-unlike specialized models such as ChemBERTa and MolFormer-XL-they still demonstrate  competitive performance.Our work lays the groundwork for future improvements in utilizing LLMs for molecular embedding.Future efforts will focus on enhancing the quality of molecular embeddings derived from LLMs inspired by natural language sentence embedding techniques, such as fine-tuning and modifications to LLaMA tokenization.</p>
<p>Fig. 2
2
Fig. 2 Results on classification and regression tasks.Each line represent the mean value of five-Fold cross validation while the shaded area shows their standard deviation</p>
<p>Fig. 3
3
Fig. 3 Comparison of LLaMA and LLaMA2 performance</p>
<p>Fig. 4
4
Fig. 4 Effect of dimension reduction on the performance of LLMs</p>
<p>Fig. 5
5
Fig. 5 Cosine similarity distribution between SMILES embeddings</p>
<p>Fig. 6
6
Fig. 6 PCA representation embedding for classification task.Red represent positive samples while blue represent negative samples</p>
<p>Fig. 7 Fig. 8
78
Fig. 7 Comparison of LLaMA2 and GPT</p>
<p>Table 1
1
Comparison of tokenizers for molecular SMILES string
ModelTokenization strategyExample tokenization of 'CCS(=O)(=O)CCBr'BERT tokenizerSubword-based tokenization['CC' , '##S' , '(' , '=' , 'O' , ')' , '(' , '=' , 'O' , ')' , 'CC' , '##B' ,'##r']GPT tokenizercl100k-base['CC' , 'S' , '(' , '=' , 'O' , ')(' , '=' , 'O' , ')' , 'CC' , 'Br']LLaMA2 tokenizerSentencePiece byte-pair encoding-based ['_C' , , 'CS' , '(' , '=' , 'O' , ')(' , '=' , 'O' , ')' , 'CC' , 'Br']ChemBERTa tokenizerByte-pair encoding-based['C' , 'C' , 'S' , '(' , '=' , 'O' , ')' , '(' , '=' , 'O' , ')' , 'C' , 'C' , 'B' , 'r']MolFormer-XL tokenizer SMILE regex['C' , 'C' , 'S' , '(' , '=' , 'O' , ')' , '(' , '=' , 'O' , ')' , 'C' , 'C' , 'Br']</p>
<p>Table 2
2
Comparison of embedding models used in this study
ModelDim. Size# Layers# ParametersSpeed  *  (s)Morgan FP (Radius=2)1024Not applicableNot applicable0.0015BERT76812110 M2.9777ChamBERTa38433 M4.8544MolFormer7681244 M20.9644GPT153696175 B0.2597LLaMA4095327 B50.8919LLaMA24095327 B51.6308
*Speed of generating embedding.Speed is dependent on the machine</p>
<p>Table 5
5
Results on regression tasksThe reported performance metrics are the mean and standard deviation of the RMSE and R 2 , calculated across the five-folds.The Best Performance is Highlighted in Bold
ESOL1128RMSE R 20.703 ± 0.020 0.502 ± 0.020 0.382 ± 0.015 0.854 ± 0.015 0.365 ± 0.007 0.866 ± 0.0070.493 ± 0.027 0.754 ± 0.0270.562 ± 0.030 0.681 0.425 ± 0.013 0.818 0.420 ± 0.023 0.821 ± 0.030 ± 0.013 ± 0.023Lipophilicity4200RMSE R 20.817 ± 0.025 0.331 ± 0.025 0.752 ± 0.013 0.434 ± 0.013 0.716 ± 0.022 0.486 ± 0.0220.740 ± 0.012 0.451 ± 0.0120.852 ± 0.010 0.273 0.785 ± 0.015 0.382 0.790 ± 0.026 0.375 ± 0.010 ± 0.015 ± 0.026FreeSolv642RMSE R 20.534 ± 0.101 0.712 ± 0.101 0.425 ± 0.031 0.816 ± 0.031 0.331 ± 0.034 0.888 ± 0.0340.545 ± 0.047 0.690 ± 0.0470.567 ± 0.087 0.675 0.483 ± 0.036 0.758 0.422 ± 0.051 0.814 ± 0.087 ± 0.036 ± 0.051Dataset# CompoundsModelsMorgan FPBERTChemBERTaMolFormer-XLGPTLLaMALLaMA2</p>
<p>Table 6
6
Results on link prediction tasksThe reported performance metrics are the mean and standard deviation of the AUROC and AUPR, calculated across the 10 runs.The Best Performance is Highlighted in Bold
DatasetBioSnapDrugBank# Nodes13201690# Edges41577190609Average node64.087224.38degreeModelsAUROCAUPRAUROCAUPRMorgan FP BERT ChemBERTa MolFormer-XL GPT LLaMA LLaMA20.871 ± 0.00 0.621 ± 0.02 0.527 ± 0.02 0.550 ± 0.02 0.856 ± 0.06 0.921 ± 0.00 0.941 ± 0.000.847 ± 0.00 0.563 ± 0.08 0.547 ± 0.08 0.701 ± 0.08 0.812 ± 0.08 0.884 ± 0.02 0.902 ± 0.020.876 ± 0.00 0.660 ± 0.02 0.519 ± 0.02 0.611 ± 0.02 0.836 ± 0.05 0.927 ± 0.00 0.961 ± 0.000.855 ± 0.00 0.639 ± 0.01 0.457 ± 0.01 0.644 ± 0.01 0.748 ± 0.09 0.872 ± 0.01 0.933 ± 0.01(a) Classification tasks(b) Regression tasks
School of Computer Science, Univeristy of Windsor, Sunset Ave, Windsor, ON N9B 3P4, Canada
AcknowledgementsThe authors thank the anonymous reviewers for their valuable suggestions.Availability of data and materialsDatasets for the validation of our work were obtained from the original studies and processed into a format suitable for analysis.Processed data is available for download from our GitHub repository.FundingThis research is supported by the National Science and Engineering Research Council of Canada (NSERC) (NSERC RGPIN-2016-05017 and NSERC RGPIN-2019-05350).Author contributions SS, JL and AN conceived the presented idea.SS obtained the embeddings, perform evaluation and wrote the main manuscript text and prepared figures.AB helped with the evaluation code.AF obtained the LLaMA embeddings.JL and AN supervised the work and helped with the main manuscript.All authors discussed the results and contributed to the final manuscript.DeclarationsEthics approval and consent to participate Not applicable.Consent for publication Not applicableCompeting interestsThe authors declare that they have no competing interests.Publisher's NoteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.
An effective self-supervised framework for learning expressive molecular global representations to drug discovery. P Li, J Wang, Y Qiao, H Chen, Y Yu, X Yao, Br Bioinform. 2261092021</p>
<p>Mol2Context-vec: learning molecular representation from context awareness for drug discovery. Q Lv, G Chen, L Zhao, W Zhong, Yu-Chian Cc, Br Bioinform. 2263172021</p>
<p>MolRoPE-BERT: An enhanced molecular representation with Rotary Position Embedding for molecular property prediction. Y Liu, R Zhang, T Li, J Jiang, J Ma, P Wang, J Mol Graph Model. 1181083442023</p>
<p>Large-scale chemical language representations capture molecular structure and properties. J Ross, B Belgodere, V Chenthamarakshan, I Padhi, Y Mroueh, P Das, Nat Mach Intell. 4122022</p>
<p>MG-BERT: leveraging unsupervised atomic representation learning for molecular property prediction. X C Zhang, C K Wu, Z J Yang, Z X Wu, J C Yi, C Y Hsieh, Br Bioinform. 2261522021</p>
<p>ChemBERTa: large-scale self-supervised pretraining for molecular property prediction. S Chithrananda, G Grand, B Ramsundar, arXiv:2010.098852020Preprint at</p>
<p>MultiDTI: drug-target interaction prediction based on multi-modal representation learning to bridge the gap between new chemical entities and known heterogeneous network. D Zhou, Z Xu, W Li, X Xie, S Peng, Bioinformatics. 37232021</p>
<p>Affinity2Vec: drug-target binding affinity prediction through representation learning, graph mining, and machine learning. M A Thafar, M Alshahrani, S Albaradei, T Gojobori, M Essack, X Gao, Sci Rep. 1212022</p>
<p>EmbedDTI: enhancing the molecular representations via sequence embedding and graph convolutional network for the prediction of drug-target interaction. Y Jin, J Lu, R Shi, Y Yang, Biomolecules. 111217832021</p>
<p>Drug-drug interactions prediction based on drug embedding and graph auto-encoder. S Purkayastha, I Mondal, S Sarkar, P Goyal, J K Pillai, IEEE 19th international conference on bioinformatics and bioengineering (BIBE). IEEE2019. 2019</p>
<p>SmileGNN: drug-drug interaction prediction based on the smiles and graph neural network. X Han, R Xie, X Li, J Li, Life. 1223192022</p>
<p>BERT: pre-training of deep bidirectional transformers for language understanding. J Devlin, M W Chang, K Lee, K Toutanova, North American Chapter. the association for computational linguistics2019</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, Adv Neural Inf Process Syst. 302017</p>
<p>Mol2vec: unsupervised machine learning approach with chemical intuition. S Jaeger, S Fulle, S Turk, J Chem Inf Model. 5812018</p>
<p>Smiles-bert: large scale unsupervised pre-training for molecular property prediction. S Wang, Y Guo, Y Wang, H Sun, J Huang, Proceedings of the 10th ACM international conference on bioinformatics. the 10th ACM international conference on bioinformatics2019</p>
<p>Molecular representation learning with language models and domain-relevant auxiliary tasks. B Fabian, T Edlich, H Gaspar, M Segler, J Meyers, M Fiscato, Mach Learn Mol Workshop NeurIPS. 2020. 2020</p>
<p>Embedding of molecular structure using molecular hypergraph variational autoencoder with metric learning. D Koge, N Ono, M Huang, M Altaf-Ul-Amin, S Kanaya, Mol Inf. 40220002032021</p>
<p>What can large language models do in chemistry? A comprehensive benchmark on eight tasks. T Guo, B Nan, Z Liang, Z Guo, N Chawla, O Wiest, Adv Neural Inf Process Syst. 362023</p>
<p>SMILES2Vec: an interpretable general-purpose deep neural network for predicting chemical properties. G B Goh, N O Hodas, C Siegel, A Vishnu, arXiv:1712.02034Preprint at</p>
<p>The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service. H L Morgan, J Chem Doc. 521965</p>
<p>Convolutional networks on graphs for learning molecular fingerprints. D K Duvenaud, D Maclaurin, J Iparraguirre, R Bombarell, T Hirzel, A Aspuru-Guzik, Adv Neural Inf Process Syst. 282015</p>
<p>Molecular contrastive learning of representations via graph neural networks. Y Wang, J Wang, Z Cao, F A Barati, Nat Mach Intell. 432022</p>
<p>Hierarchical molecular graph self-supervised learning for property prediction. X Zang, X Zhao, B Tang, Commun Chem. 61342023</p>
<p>Seq2seq fingerprint: an unsupervised deep molecular embedding for drug discovery. Z Xu, S Wang, F Zhu, J Huang, Proceedings of the 8th ACM international conference on bioinformatics, computational biology. the 8th ACM international conference on bioinformatics, computational biology2017</p>
<p>SPVec: a Word2vec-inspired feature representation method for drug-target interaction prediction. Y F Zhang, X Wang, A C Kaushik, Y Chu, X Shan, M Z Zhao, Front Chem. 78952020</p>
<p>Roformer: enhanced transformer with rotary position embedding. J Su, Y Lu, S Pan, A Murtadha, B Wen, Y Liu, 10.1016/j.neucom.2023.127063Neurocomputing. 5681270632024</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, OpenAI Blog. 2019</p>
<p>LLaMA: open and efficient foundation language models. H Touvron, T Lavril, Izacard , arXiv:2302.139712023Preprint at</p>
<p>Llama 2: open foundation and fine-tuned chat models. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, arXiv:2307.092882023Preprint at</p>
<p>The role of ChatGPT in data science: how ai-assisted conversational interfaces are revolutionizing the field. H Hassani, E S Silva, Big Data Cogn Comput. 72622023</p>
<p>ChatGPT (Large language model. OpenAI. OpenAI</p>
<p>Palm: scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, J Mach Learn Res. 242402023</p>
<p>A survey of large language models. W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, arXiv:2303.182232023Preprint at</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, H Subbiah ; Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Advances in neural information processing systems. Curran Associates, Inc2020</p>
<p>Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. P Schwaller, T Laino, T Gaudin, P Bolgar, C A Hunter, C Bekas, ACS Cent Sci. 592019</p>
<p>MoleculeNet: a benchmark for molecular machine learning. Z Wu, B Ramsundar, E N Feinberg, J Gomes, C Geniesse, A S Pappu, Chem Sci. 922018</p>
<p>BioSNAP datasets: stanford biomedical network dataset collection. M Zitnik, R Sosivc, S Maheshwari, J Leskovec, S University, ACM</p>
<p>DrugBank 5.0: a major update to the DrugBank database for. D S Wishart, Y D Feunang, A C Guo, E J Lo, A Marcu, J R Grant, Nucleic Acids Res. 46D12018. 2018</p>
<p>Sentence-BERT: sentence embeddings using Siamese BERT-networks. N Reimers, I Gurevych, Conference on empirical methods in natural language processing. 2019. 20164 6309</p>
<p>Multi-view graph contrastive representation learning for drug-drug interaction prediction. Y Wang, Min Y Chen, X Wu, J , Proceedings of the web conference. the web conference20212021</p>
<p>Fast graph representation learning with PyTorch geometric. Representation learning on graphs and manifolds at ICLR. M Fey, J E Lenssen, 2019 Workshop. 2019</p>
<p>Mol-BERT: an effective molecular representation with BERT for molecular property prediction. J Li, X Jiang, Wirel Commun Mob Comput. 2021. 2021</p>
<p>All bark and no bite: rogue dimensions in transformer language models obscure representational quality. W Timkey, M Van Schijndel, Proceedings of the 2021 conference on empirical methods in natural language processing. the 2021 conference on empirical methods in natural language processingDominican RepublicAssociation for Computational Linguistics2021Online and Punta Cana</p>
<p>BERT busters: outlier dimensions that disrupt transformers. O Kovaleva, S Kulshreshtha, A Rogers, A Rumshisky, Findings; 2021. </p>
<p>IsoScore: measuring the uniformity of embedding space utilization. W Rudman, N Gillman, T Rayne, C Eickhoff, Findings of the association for computational linguistics: ACL 2022. DublinAssociation for Computational Linguistics2022</p>            </div>
        </div>

    </div>
</body>
</html>
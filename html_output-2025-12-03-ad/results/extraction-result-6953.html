<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6953 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6953</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6953</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-134.html">extraction-schema-134</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-385c74957858e7d6856d48e72b5a902b4c1aa28c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/385c74957858e7d6856d48e72b5a902b4c1aa28c" target="_blank">Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> A Multi-Agent Debate (MAD) framework is proposed, in which multiple agents express their arguments in the state of “tit for tat” and a judge manages the debate process to obtain a final solution.</p>
                <p><strong>Paper Abstract:</strong> Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of “tit for tat” and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of “tit for tat” state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6953.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6953.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Reflect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion / Self-Reflect (iterative self-feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative self-evaluation and refinement method where an LLM generates an answer, evaluates it (often using its own feedback), and refines the answer repeatedly until judged satisfactory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo (as evaluated in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-based autoregressive LLM used as backbone for baselines and agents in experiments (zero-shot prompts, temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Reflect</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial output, produce self-evaluation/feedback, and iteratively refine the output based on that feedback until the model deems it satisfactory.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Common MT (Commonsense Machine Translation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Chinese→English translation examples that require commonsense to resolve lexical and syntactic ambiguities (lexical, contextless, contextual subsets).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>COMET, BLEURT, HUMAN (direct assessment score 1-5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>GPT-3.5-Turbo baseline (Lexical COMET 80.3, BLEURT 68.2, HUMAN 3.14; Contextless COMET 84.0, BLEURT 72.9, HUMAN 3.43; Contextual COMET 84.9, BLEURT 73.4, HUMAN 3.57)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>GPT-3.5-Turbo + Self-Reflect (Lexical COMET 81.0, BLEURT 69.1, HUMAN 3.43; Contextless COMET 83.6, BLEURT 72.2, HUMAN 3.46; Contextual COMET 84.9, BLEURT 73.5, HUMAN 3.63)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Improvements are small and mixed across subsets (some metrics decrease for contextless subset). The paper reports Self-Reflect suffers from Degeneration-of-Thought (DoT): low disagreement across iterations, inability to generate novel thoughts once confident, and failure to correct literal but incorrect translations (example: '吃掉敌人一个师' remained 'Eat up an enemy division'). Computational cost reported as ~1.83x tokens relative to CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6953.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6953.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Reflect (AR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Reflect applied to Counter-Intuitive Arithmetic Reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the iterative self-reflection baseline to a counter-intuitive arithmetic reasoning benchmark; the method marginally refines initial answers via self-feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>API-based autoregressive LLM used zero-shot with CoT prompts where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Reflect</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Iteratively refine the initial chain-of-thought/answer using self-generated feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Counter-Intuitive AR (CIAR)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>200 arithmetic reasoning questions designed to elicit intuitive but incorrect answers; require multi-step reasoning and resisting intuitive traps.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (ACC %)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>GPT-3.5-Turbo baseline ACC = 26.0%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>GPT-3.5-Turbo + Self-Reflect ACC = 27.5%</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Only marginal accuracy gain (+1.5 percentage points). Paper highlights that CoT and Self-Consistency produced larger improvements than Self-Reflect on this task; self-reflection still affected by DoT, limiting ability to overturn an initially confident but incorrect chain-of-thought.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6953.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6953.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Degeneration-of-Thought (DoT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Degeneration-of-Thought (DoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A failure mode defined in this paper where an LLM, once confident in an initial answer, fails to produce novel or corrective thoughts during subsequent self-reflection iterations even if the initial answer is wrong.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>observational / applies to evaluated LLMs (GPT-3.5-Turbo, Vicuna variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Behavioral phenomenon observed when performing iterative self-reflection/generate-and-reflect cycles with large LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>N/A (failure mode of generate-then-reflect)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Describes collapse of diversity and novel critique during iterative self-reflection: adjacent-iteration disagreement falls, indicating sticking to initial stance.</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>generate-then-reflect (demonstrated over fixed rounds)</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>5</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Demonstration and measurement across debate/self-reflection; observed on Common MT and CIAR</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Paper forced either debate or self-reflection for 5 rounds and measured disagreement between adjacent iterations to quantify DoT; also observed in translation and arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Disagreement (percent opposition between adjacent iterations), Bias (human-labeled ambiguity error rate), Diversity (100 - Self-BLEU), downstream task metrics (COMET/BLEURT/HUMAN, ACC).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>N/A (describes degeneration behavior rather than a scalar performance)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Paper reports that self-reflection exhibits low disagreement across iterations (Figure 1), high bias and low diversity (Table 4: Self-Reflect Bias=29.0, Diversity=19.3), and cannot escape incorrect initial stances; motivates Multi-Agent Debate (MAD) to mitigate DoT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6953.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6953.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform self‑reflection or self‑critique, including the specific reflection method, number of generate‑then‑reflect iterations, tasks or benchmarks evaluated, performance before and after reflection, evaluation metrics, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MAD (Multi-Agent Debate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent iterative debate framework where multiple LLM agents ('affirmative' and 'negative') exchange tit-for-tat arguments and a judge monitors and adaptively stops the debate to extract the final answer, encouraging divergent thinking and mitigating DoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo (primary reported), Vicuna-7b/13b (also evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Implemented by instantiating debaters and a judge as LLM instances (API-based GPT-3.5-Turbo and open-source Vicuna variants) using meta-prompts to control debate behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Multi-Agent Debate (MAD)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Multiple agents generate arguments iteratively in a 'tit for tat' manner; a judge evaluates each round (discriminative mode) and can adaptively break; if iteration limit reached judge extracts final answer (extractive mode).</td>
                        </tr>
                        <tr>
                            <td><strong>iteration_type</strong></td>
                            <td>multi-agent iterative debate / generate-and-refute</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td>3</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Common MT; Counter-Intuitive AR (also GSM, AddSub, BBH symbolic tasks in appendix)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Commonsense translation (Chinese→English) and counter-intuitive arithmetic reasoning; tasks requiring deeper contemplation and resistance to intuitive traps.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Common MT: COMET, BLEURT, HUMAN; CIAR and other reasoning tasks: Accuracy (ACC %).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_before_reflection</strong></td>
                            <td>GPT-3.5-Turbo baseline (Common MT Turbo row: Lexical COMET 80.3/BLEURT 68.2/HUMAN 3.14; Counter-Intuitive AR ACC=26.0%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_after_reflection</strong></td>
                            <td>GPT-3.5-Turbo + MAD (Common MT: Lexical COMET 82.0/BLEURT 70.9/HUMAN 3.78; Counter-Intuitive AR ACC=37.0%)</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Higher computational cost (~2.46x generated tokens vs CoT). Requires careful tuning: adaptive break and modest 'tit for tat' level needed; too-high enforced disagreement harms performance. Increasing number of debaters can hurt because LLMs struggle with long-context coherence and summarization. Judges can be biased toward outputs from the same LLM, making judge selection important.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Critic: Large language models can self-correct with tool-interactive critiquing <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6953",
    "paper_id": "paper-385c74957858e7d6856d48e72b5a902b4c1aa28c",
    "extraction_schema_id": "extraction-schema-134",
    "extracted_data": [
        {
            "name_short": "Self-Reflect",
            "name_full": "Reflexion / Self-Reflect (iterative self-feedback)",
            "brief_description": "An iterative self-evaluation and refinement method where an LLM generates an answer, evaluates it (often using its own feedback), and refines the answer repeatedly until judged satisfactory.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo (as evaluated in this paper)",
            "model_description": "API-based autoregressive LLM used as backbone for baselines and agents in experiments (zero-shot prompts, temperature=0).",
            "model_size": null,
            "reflection_method_name": "Self-Reflect",
            "reflection_method_description": "Generate an initial output, produce self-evaluation/feedback, and iteratively refine the output based on that feedback until the model deems it satisfactory.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 5,
            "task_name": "Common MT (Commonsense Machine Translation)",
            "task_description": "Chinese→English translation examples that require commonsense to resolve lexical and syntactic ambiguities (lexical, contextless, contextual subsets).",
            "evaluation_metric": "COMET, BLEURT, HUMAN (direct assessment score 1-5)",
            "performance_before_reflection": "GPT-3.5-Turbo baseline (Lexical COMET 80.3, BLEURT 68.2, HUMAN 3.14; Contextless COMET 84.0, BLEURT 72.9, HUMAN 3.43; Contextual COMET 84.9, BLEURT 73.4, HUMAN 3.57)",
            "performance_after_reflection": "GPT-3.5-Turbo + Self-Reflect (Lexical COMET 81.0, BLEURT 69.1, HUMAN 3.43; Contextless COMET 83.6, BLEURT 72.2, HUMAN 3.46; Contextual COMET 84.9, BLEURT 73.5, HUMAN 3.63)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Improvements are small and mixed across subsets (some metrics decrease for contextless subset). The paper reports Self-Reflect suffers from Degeneration-of-Thought (DoT): low disagreement across iterations, inability to generate novel thoughts once confident, and failure to correct literal but incorrect translations (example: '吃掉敌人一个师' remained 'Eat up an enemy division'). Computational cost reported as ~1.83x tokens relative to CoT.",
            "uuid": "e6953.0",
            "source_info": {
                "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Self-Reflect (AR)",
            "name_full": "Self-Reflect applied to Counter-Intuitive Arithmetic Reasoning",
            "brief_description": "Application of the iterative self-reflection baseline to a counter-intuitive arithmetic reasoning benchmark; the method marginally refines initial answers via self-feedback.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "API-based autoregressive LLM used zero-shot with CoT prompts where applicable.",
            "model_size": null,
            "reflection_method_name": "Self-Reflect",
            "reflection_method_description": "Iteratively refine the initial chain-of-thought/answer using self-generated feedback.",
            "iteration_type": "generate-then-reflect",
            "num_iterations": 5,
            "task_name": "Counter-Intuitive AR (CIAR)",
            "task_description": "200 arithmetic reasoning questions designed to elicit intuitive but incorrect answers; require multi-step reasoning and resisting intuitive traps.",
            "evaluation_metric": "Accuracy (ACC %)",
            "performance_before_reflection": "GPT-3.5-Turbo baseline ACC = 26.0%",
            "performance_after_reflection": "GPT-3.5-Turbo + Self-Reflect ACC = 27.5%",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Only marginal accuracy gain (+1.5 percentage points). Paper highlights that CoT and Self-Consistency produced larger improvements than Self-Reflect on this task; self-reflection still affected by DoT, limiting ability to overturn an initially confident but incorrect chain-of-thought.",
            "uuid": "e6953.1",
            "source_info": {
                "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Degeneration-of-Thought (DoT)",
            "name_full": "Degeneration-of-Thought (DoT)",
            "brief_description": "A failure mode defined in this paper where an LLM, once confident in an initial answer, fails to produce novel or corrective thoughts during subsequent self-reflection iterations even if the initial answer is wrong.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "observational / applies to evaluated LLMs (GPT-3.5-Turbo, Vicuna variants)",
            "model_description": "Behavioral phenomenon observed when performing iterative self-reflection/generate-and-reflect cycles with large LMs.",
            "model_size": null,
            "reflection_method_name": "N/A (failure mode of generate-then-reflect)",
            "reflection_method_description": "Describes collapse of diversity and novel critique during iterative self-reflection: adjacent-iteration disagreement falls, indicating sticking to initial stance.",
            "iteration_type": "generate-then-reflect (demonstrated over fixed rounds)",
            "num_iterations": 5,
            "task_name": "Demonstration and measurement across debate/self-reflection; observed on Common MT and CIAR",
            "task_description": "Paper forced either debate or self-reflection for 5 rounds and measured disagreement between adjacent iterations to quantify DoT; also observed in translation and arithmetic tasks.",
            "evaluation_metric": "Disagreement (percent opposition between adjacent iterations), Bias (human-labeled ambiguity error rate), Diversity (100 - Self-BLEU), downstream task metrics (COMET/BLEURT/HUMAN, ACC).",
            "performance_before_reflection": "N/A (describes degeneration behavior rather than a scalar performance)",
            "performance_after_reflection": "N/A",
            "improvement_observed": false,
            "limitations_or_failure_cases": "Paper reports that self-reflection exhibits low disagreement across iterations (Figure 1), high bias and low diversity (Table 4: Self-Reflect Bias=29.0, Diversity=19.3), and cannot escape incorrect initial stances; motivates Multi-Agent Debate (MAD) to mitigate DoT.",
            "uuid": "e6953.2",
            "source_info": {
                "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MAD (Multi-Agent Debate)",
            "name_full": "Multi-Agent Debate (MAD)",
            "brief_description": "A multi-agent iterative debate framework where multiple LLM agents ('affirmative' and 'negative') exchange tit-for-tat arguments and a judge monitors and adaptively stops the debate to extract the final answer, encouraging divergent thinking and mitigating DoT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo (primary reported), Vicuna-7b/13b (also evaluated)",
            "model_description": "Implemented by instantiating debaters and a judge as LLM instances (API-based GPT-3.5-Turbo and open-source Vicuna variants) using meta-prompts to control debate behavior.",
            "model_size": null,
            "reflection_method_name": "Multi-Agent Debate (MAD)",
            "reflection_method_description": "Multiple agents generate arguments iteratively in a 'tit for tat' manner; a judge evaluates each round (discriminative mode) and can adaptively break; if iteration limit reached judge extracts final answer (extractive mode).",
            "iteration_type": "multi-agent iterative debate / generate-and-refute",
            "num_iterations": 3,
            "task_name": "Common MT; Counter-Intuitive AR (also GSM, AddSub, BBH symbolic tasks in appendix)",
            "task_description": "Commonsense translation (Chinese→English) and counter-intuitive arithmetic reasoning; tasks requiring deeper contemplation and resistance to intuitive traps.",
            "evaluation_metric": "Common MT: COMET, BLEURT, HUMAN; CIAR and other reasoning tasks: Accuracy (ACC %).",
            "performance_before_reflection": "GPT-3.5-Turbo baseline (Common MT Turbo row: Lexical COMET 80.3/BLEURT 68.2/HUMAN 3.14; Counter-Intuitive AR ACC=26.0%)",
            "performance_after_reflection": "GPT-3.5-Turbo + MAD (Common MT: Lexical COMET 82.0/BLEURT 70.9/HUMAN 3.78; Counter-Intuitive AR ACC=37.0%)",
            "improvement_observed": true,
            "limitations_or_failure_cases": "Higher computational cost (~2.46x generated tokens vs CoT). Requires careful tuning: adaptive break and modest 'tit for tat' level needed; too-high enforced disagreement harms performance. Increasing number of debaters can hurt because LLMs struggle with long-context coherence and summarization. Judges can be biased toward outputs from the same LLM, making judge selection important.",
            "uuid": "e6953.3",
            "source_info": {
                "paper_title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "rating": 2,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 1,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        }
    ],
    "cost": 0.014190499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate</h1>
<p>Tian Liang ${ }^{13 <em>}$ Zhiwei $\mathrm{He}^{2 </em>}$ Wenxiang Jiao ${ }^{3 *}$ Xing Wang ${ }^{3 \dagger}$ Yan Wang ${ }^{3}$<br>Rui Wang ${ }^{2}$ Yujiu Yang ${ }^{1 \dagger}$ Shuming Shi ${ }^{3}$ Zhaopeng Tu ${ }^{3}$<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Shanghai Jiao Tong University ${ }^{3}$ Tencent AI Lab<br>{liangt21@mails, yang.yujiu@sz}.tsinghua.edu.cn {zwhe.cs}@sjtu.edu.cn<br>{joelwxjiao, brightxwang, brandenwang, zptu}@tencent.com</p>
<h4>Abstract</h4>
<p>Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of "tit for tat" and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counterintuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of "tit for tat" state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Code is available at https://github. com/Skytliang/Multi-Agents-Debate.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have shown remarkable performance on general language tasks (Jiao et al., 2023; Wu et al., 2023; Bang</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Disagreement between two adjacent iterations with respect to the iteration of debate/self-reflection.
et al., 2023) but still struggle on complex reasoning tasks (Zhu et al., 2023a; Gou et al., 2023), which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. In particular, self-reflection (Madaan et al., 2024; Shinn et al., 2024), a concept that usually refers to the process of introspection and examination of a person's own thoughts, has been explored to solve intricate tasks that could be challenging for a zeroshot generation or even chain-of-thought (CoT) prompting (Wei et al., 2022). Specifically, selfreflection involves an iterative refinement process such that the LLM generates a new answer based on the answers and feedback in previous iterations and then provides feedback for the new answer. While self-reflection can be effective in creating better answers, it is highly dependent on the self-evaluation capabilities of LLMs, which are not formally guaranteed (Shinn et al., 2024).</p>
<p>In this work, we focus on the Degeneration-ofThought (DoT) problem in self-reflection, which is proposed and defined by us for the first time. Formally, DoT describes the following scenario:</p>
<p>Once the LLM-based agent has established confidence in its answers, it is unable to generate novel thoughts later through self-reflection even if the initial stance is incorrect.</p>
<p>To demonstrate this problem, we force the agents to engage in a debate or self-reflection for 5 rounds before reaching an answer. Next, we manually determine the disagreement as 1 and agreement as 0 between two adjacent iterations. We define the average disagreement in iteration $i$ as the percentage of opposition occurring between two debaters across multiple debates (or self-confliction in selfreflection). We show the trends in Figure 1. The low disagreement of self-reflection suggests that the LLM sticks to the incorrect answers predicted by CoT and is unable to engage in meaningful selfreflection.</p>
<p>There are various factors (Bortolotti, 2011; Keestra, 2017) that could result in DoT, and we outline three here: (1) Bias and Distorted Perception. Self-perception can be influenced by biases, preconceived notions, and distorted thinking patterns, which can be learned from the massive amount of data during pretraining. If an LLM's self-reflection is clouded by such biases or distorted thinking, it can lead to inaccurate conclusions instinctively. (2) Rigidity and Resistance to Change. Self-reflection often involves challenging one's beliefs, assumptions, and behaviors. If an LLM is resistant to change or holds rigid beliefs, it may struggle to engage in meaningful self-reflection that leads to better answers. (3) Limited External Feedback. Selfreflection is primarily an internal process, but external feedback can provide valuable perspectives and insights. Without considering external feedback, an LLM may miss important blind spots or alternative viewpoints that can enrich its self-reflection.</p>
<p>To address the DoT issue, we leverage another fundamental characteristic of human problemsolving, i.e., debate, to encourage divergent thinking in LLMs. Specifically, we propose the MAD framework, short for Multi-Agent Debate, where two agents express their own arguments in the state of "tit for tat" and a judge monitors and manages the debate process to obtain a final solution. The nature of MAD determines that (1) The distorted thinking of one agent can be corrected by the others; (2) The resistance to change of one agent will be complemented by the others; and (3) each agent can obtain external feedback from the others. Therefore, MAD is less susceptible to the factors of DoT , and can explore divergent chain-of-thoughts to achieve accurate answers.</p>
<p>We conducted experiments on both natural language generation and understanding through two challenging tasks, namely, Commonsense Machine</p>
<p>Translation (Common MT) and Counter-Intuitive Arithmetic Reasoning (Counter-Intuitive AR). The common characteristic of the two tasks is that our instincts are mostly incorrect based on only the superficial expressions of the questions, and deeper levels of contemplation are required for better answers. Experimental results demonstrate that our MAD framework outperforms the baseline methods, especially, GPT-3.5-Turbo with MAD can surpass the performance of GPT-4 on Common MT.</p>
<p>The contributions of this work are summarized as follows:</p>
<ul>
<li>We propose and define the Degeneration-ofThought (DoT) problem in self-reflection, and address it by proposing the Multi-Agent Debate (MAD) framework to explore divergent chain-of-thoughts.</li>
<li>We demonstrate the effectiveness of MAD on two challenging tasks, and find that GPT-3.5-Turbo with MAD can even surpass GPT-4 on the Common MT dataset.</li>
<li>Experimental results show that the adaptive break strategy and the modest level of "tit for tat" state are required for performance improvement. In addition, we find that the llm-based judge shows a preference to the side with the same LLM as the backbone.</li>
</ul>
<h2>2 Multi-Agent Debate Framework</h2>
<p>Figure 2 illustrates the general framework of MAD, where two debaters and a judge are involved in a debate to resolve a math problem while self-reflection descends into the trap of DoT. Generally, our MAD framework is composed of three components which are elaborated as follows:</p>
<p>Meta Prompts. We use meta prompts to introduce the topic to be solved, the number of debaters, the iteration limit, and other requirements. An exampe of meta prompts for the arithmetic reasoning task in Figure 2 is:</p>
<p>You are a debater. Hello and welcome to the debate competition. It's not necessary to fully agree with each other's perspectives, as our objective is to find the correct answer. The debate topic is stated as follows: $&lt;$ debate topic $&gt;$.</p>
<p>As seen, we require the agents to "tit for tat" (e.g. contents underlined in meta prompts above) so as to create an atmosphere of debate.</p>
<h1>Counter-Intuitive Question</h1>
<p>The two circles are externally tangent and there is no relative sliding. The radius of circle $A$ is $1 / 3$ the radius of circle B. Circle A rolls around circle B one trip back to its starting point. How many times will circle A revolve in total?</p>
<h2>Chain-of-Thought</h2>
<p>Let the radius of circle $A$ be $r$ and the radius of circle $B$ be $3 r$. The distance circle $A$ travels along the circumference of circle $B$ is equal to the circumference of circle $B$, which is $6 \pi r$. The number of revolutions $=(6 \pi r) /(2 \pi r)=6 \pi r /$ $2 \pi r=3$. So, circle $A$ will revolve 3 times in total as it rolls around circle $B$ once.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Framework of Multi-Agent Debate. Here we designate the devil ( $\boldsymbol{\sim}$ ) as the affirmative side while the angel ( $\boldsymbol{\theta}$ ) as the negative side. We want the angel to correct the devil's mistakes.</p>
<p>Debaters. There are $N$ debaters $D=\left{D_{i}\right}<em i="i">{i=1}^{N}$ involved in the framework. In each debate iteration, the debaters $D</em>(H)=h$. An example of a debater prompt appears below:}$ speak one by one in a fixed order and express their arguments based on the previous debate history $H$, i.e., $D_{i</p>
<ul>
<li>Prompt for Affirmative Debater ( $\boldsymbol{\mathcal { A }}$ )</li>
</ul>
<p>You are affirmative side. Please express your viewpoints.</p>
<ul>
<li>Prompt for Negative Debater ( $\boldsymbol{\mathcal { N }}$ )</li>
</ul>
<p>You are negative side. You disagree with the affirmative side's points. Provide your reasons and answer.</p>
<p>Judge. We also design a judge $J$ to manage and monitor the whole debate process. The judge contains two different modes: (a) Discrinative Mode, in which the judge $J$ decides whether the correct solution can be obtained after all the debaters finish their arguments in the current iteration:</p>
<p>$$
J_{d}(H)= \begin{cases}\text { True, } &amp; \text { solution obtained } \ \text { False, } &amp; \text { otherwise }\end{cases}
$$</p>
<p>If it is True, the debate is over. Otherwise, the debate continues. (b) Extractive Mode, in which
the judge $J$ needs to extract the final solution based on the whole debate history: $J_{e}(H)=a$, since no correct solution is identified within the iteration limit of debate. An example of a judge prompt ( $\boldsymbol{\Omega}$ ) appears below:</p>
<p>You are a moderator. There will be two debaters involved in a debate competition. They will present their answers and discuss their perspectives on the <debate topic>. At the end of each round, you will evaluate both sides' answers and decide which one is correct.</p>
<h2>3 Experiment</h2>
<h3>3.1 Challenging Testbeds</h3>
<p>We conduct experiments on two challenging tasks, namely, commonsense machine translation (i.e., Common MT), and counter-intuitive arithmetic reasoning (i.e., Counter-Intuitive AR), which require deep levels of contemplation for LLMs. Please refer to Appendix A for more details.</p>
<p>Commonsense Machine Translation The Common MT dataset is composed of Chinese $\Rightarrow$ English translation examples (He et al., 2020), which are used to examine three types of ambiguity resolution</p>
<p>abilities of translation models, covering lexical and contextless/contextual syntactic ambiguity. Within the challenging part of Common MT, the authentic translation of each source sentence requires a proper understanding of common sense knowledge. While these ambiguous sentences might appear to have a straightforward translation, such a literal interpretation is erroneous. Failure to address such ambiguities may result in inaccurate translations.</p>
<p>Counter-Intuitive Arithmetic Reasoning Previous studies on thinking hierarchy (Daniel, 2017) suggest that we humans have a fast and intuitive system and a slow and logical system, and tend to run the lower level system before the higher level one. Inspired by this, we created a more challenging dataset named Counter-Intuitive Arithmetic Reasoning (CIAR) to evaluate the reasoning abilities of LLMs at deep levels. Our Counter-Intuitive AR dataset contains 200 questions collected from elicitation questions (Kong et al., 2022) ${ }^{1}$, web data ${ }^{2}$ and additional manual derivatives of these questions. Compared to the commonly-used datasets, e.g., MultiArith (Roy and Roth, 2015), GSM8K (Cobbe et al., 2021), our dataset presents two distinct challenges:</p>
<ul>
<li>Resistance to Intuition. The questions are embedded in hidden traps designed to elicit intuitive and appealing answers that are often incorrect. This feature evaluates the abilities of LLMs to resist the traps of superficial expressions.</li>
<li>Multi-Step Reasoning. Each correct answer within the dataset requires a rigorous multi-step reasoning process, thereby evaluating the capacity of LLMs to engage in complex decisionmaking and problem-solving.</li>
</ul>
<h3>3.2 Setups</h3>
<p>Input Format. Our experiments are performed in zero-shot instructions (setting temperature to 0 ). For all used datasets, we use a unified prompt to make LLMs give explanations and answers. We present the inputs to agents through <debate topic $>$ as mentioned in Section 2. For example, if we want to translate "吃掉敌人一个师" from Chinese to English, we will set the <debate topic> as "What is the correct English translation of the following Chinese text: 吃掉敌人一个师". For QA task, we employ the same prompt except set the $&lt;$ debate topic $&gt;$ to the arithmetic question.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Backbone Models. In this work, we mainly use three agents in our MAD framework, including two debaters (i.e., affirmative and negative) and a judge. We assess two open-source (i.e., vicuna-7b-v1.5-16k ${ }^{3}$ and vicuna-13b-v1.5-16k ${ }^{4}$ ) and two apibased LLMs (i.e., GPT-3.5-Turbo-0301 and GPT-4-0314).</p>
<p>Compared Methods. Generally, we compare our MAD framework with baseline models and Self-Reflect on both tasks. We also include other baseline methods individually, namely, Rerank and MAPS for Common MT, CoT and SelfConsistency for Counter-Intuitive AR. Below elaborates the details of them:</p>
<ul>
<li>Self-Reflect (Shinn et al., 2024): This approach requires the LLM to refine its translation until it deems the current output satisfactory.</li>
<li>Rerank (He et al., 2024): We sample the translations from the LLM for four times, from which we select the best candidate based on a quality estimation (QE) HUMANr ${ }^{5}$. This approach can be seen as analogous to self-consistency (Wang et al., 2022), where the majority voting is replaced by an external QE HUMANr.</li>
<li>MAPS (He et al., 2024): This method enables LLMs to mimic the human translation process: analyze before translate, which can be viewed as a chain-of-thought method applied to translation.</li>
<li>CoT (Kojima et al., 2022): This approach concatenates a trigger sentence "Let's think step by step" to the test question.</li>
<li>Self-Consistency (Wang et al., 2022): This method samples multiple responses and determines the final answer through a majority vote.</li>
</ul>
<p>All agents in our experimental setup, such as debaters and judge, are large language models. Here, we implement the methods on top of GPT-3.5-Turbo and Vicuna models.</p>
<p>Evaluation Metrics. For Counter-Intuitive AR, we report the accuracy (ACC) of predictions. For Common MT, we adopt automatic metrics</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Lexical</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Contextless</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Contextual</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COMET</td>
<td style="text-align: center;">BLEURT</td>
<td style="text-align: center;">HUMAN</td>
<td style="text-align: center;">COMET</td>
<td style="text-align: center;">BLEURT</td>
<td style="text-align: center;">HUMAN</td>
<td style="text-align: center;">COMET</td>
<td style="text-align: center;">BLEURT</td>
<td style="text-align: center;">HUMAN</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">3.41</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">73.6</td>
<td style="text-align: center;">3.63</td>
<td style="text-align: center;">85.0</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">3.65</td>
</tr>
<tr>
<td style="text-align: center;">Turbo</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">3.14</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">3.43</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">3.57</td>
</tr>
<tr>
<td style="text-align: center;">+ Rerank</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">3.16</td>
<td style="text-align: center;">84.5</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">3.46</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">3.58</td>
</tr>
<tr>
<td style="text-align: center;">+ MAPS</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">70.1</td>
<td style="text-align: center;">3.43</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">3.45</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">3.56</td>
</tr>
<tr>
<td style="text-align: center;">+ Self-Reflect</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;">3.43</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">3.46</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">73.5</td>
<td style="text-align: center;">3.63</td>
</tr>
<tr>
<td style="text-align: center;">+ MAD</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">3.78</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">3.67</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">3.67</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-7b</td>
<td style="text-align: center;">74.9</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">2.55</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">2.53</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">68.2</td>
<td style="text-align: center;">3.23</td>
</tr>
<tr>
<td style="text-align: center;">+ MAD</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">62.6</td>
<td style="text-align: center;">2.67</td>
<td style="text-align: center;">78.6</td>
<td style="text-align: center;">66.0</td>
<td style="text-align: center;">2.69</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">3.27</td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13b</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">2.81</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">66.8</td>
<td style="text-align: center;">3.04</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">3.37</td>
</tr>
<tr>
<td style="text-align: center;">+ MAD</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">2.96</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">67.3</td>
<td style="text-align: center;">3.11</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">70.9</td>
<td style="text-align: center;">3.45</td>
</tr>
</tbody>
</table>
<p>Table 1: Translation performance on Common MT. Note that Rerank and MAPS use the external quality estimation tool to select the best translation from multiple translation candidates. HUMAN: direct assessment of translation quality from human evaluators on a scale ranging from 1 to 5 .
like COMET $^{6}$ and BLEURT $^{7}$, which are widely adopted evaluation metrics for LLM-based translation literature (He et al., 2024; Hendy et al., 2023; Garcia et al., 2023; Pilault et al., 2023). In addition, we also employ professional human translators to directly assess the translation results, measuring translation quality on a scale ranging from 1 to 5 .</p>
<h3>3.3 Results on Common MT</h3>
<p>Results. In Common MT test set, we focus more on the translation accuracy of specific words and whether they conform to common sense. However, such minor variations at token level are difficult to reflect on automatic metrics. We therefore provide human HUMAN to evaluate these methods more accurately. Table 1 presents the experimental results. MAPS and Self-Reflec achieve improvements over baseline GPT-3.5-Turbo. Remarkably, our proposed MAD, by utilizing GPT-3.5 as the backbone model, has demonstrated significant advancements over GPT-4 across both automatic and human evaluation metrics.</p>
<p>Case Study. Table 2 shows example translations generated by baseline GPT-3.5-Turbo and the proposed MAD. We can find that the baseline GPT-3.5-Turbo (even the more powerful GPT-4) incorrectly translates the source words literally. Because of the DoT issue, Self-Reflect cannot rectify the literal translation. The proposed MAD framework, which explores divergent chain-of-thoughts,</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Source | 呪掖数人一个师。 |
| :-- | :-- |
| Correct Ref. | Destroy a division of the enemy. |
| Incorrect Ref. | Eat up an enemy division. |
| GPT-4 | Eat up an enemy division. |
| GPT-3.5-Turbo | Eat up an enemy division. |
| + Self-Reflect | Eat up an enemy division. |
| + MAD | Eliminate an enemy division. |</p>
<p>Table 2: Example translations generated by different methods. Best viewed in color.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">ACC (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">51.0</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5-Turbo</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: left;">+ CoT</td>
<td style="text-align: center;">28.0</td>
</tr>
<tr>
<td style="text-align: left;">+ Self-Consistency</td>
<td style="text-align: center;">29.5</td>
</tr>
<tr>
<td style="text-align: left;">+ Self-Reflect</td>
<td style="text-align: center;">27.5</td>
</tr>
<tr>
<td style="text-align: left;">+ MAD</td>
<td style="text-align: center;">$\mathbf{3 7 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracy on Counter-Intuitive AR.
can generate the free translation of the underlined words within the source sentences.</p>
<h3>3.4 Results on Counter-Intuitive AR</h3>
<p>Results. Table 3 lists the results in terms of reasoning accuracy. We can observe that SelfReflect only marginally improves over the baseline GPT-3.5-Turbo, while CoT and Self-Consistency bring more improvements. Our MAD framework, though not as good as GPT-4, outperforms all the other compared methods based on GPT-3.5-Turbo, which further demonstrates its effectiveness. We also validate MAD on math and symbolic reasoning tasks and report our results in Appendix C.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Bias $\downarrow$</th>
<th style="text-align: left;">Diversity $\uparrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Self-Reflect</td>
<td style="text-align: left;">29.0</td>
<td style="text-align: left;">19.3</td>
</tr>
<tr>
<td style="text-align: left;">MAD</td>
<td style="text-align: left;">24.8</td>
<td style="text-align: left;">49.7</td>
</tr>
</tbody>
</table>
<p>Table 4: Mitigation of Degeneration-of-Thought.</p>
<p>Case Study. Figure 2 shows an example on Counter-Intuitive AR. We find both CoT and SelfReflect fail to reach the right answer by mistakenly outputting 3 . With divergent thinking, our MAD framework emerges "we need to consider both the rotation around circle $B$ and the rotation of circle A itself" and find the correct answer 4.</p>
<h2>4 Analysis</h2>
<p>In this section, we present a qualitative analysis to provide some insights how MAD works. Unless otherwise stated, we report the overall results on the Common MT dataset.</p>
<h3>4.1 Mitigation of DoT</h3>
<p>As mentioned in the Section 1, the DoT problem originates from three factors: (1) Bias and Distorted Perception, (2) Rigidity and Resistance to Change, and (3) Limited External Feedback. In our MAD framework, we introduce the views of other agents in the form of debates, solving the phenomenon of limited external feedback (problem 3). Next, this section will delve into the mitigation of problems 1 and 2 through experiments.</p>
<ul>
<li>Bias: We observe that LLMs often rely on direct intuition, which can lead to incorrect or inappropriate responses. To address this problem, we use human evaluation to determine the ambiguity error rate of LLMs' responses, examining if the LLM's output is biased.</li>
<li>Diversity: LLMs are resistant to changing their answers and lack diverse reflection. The diversity of the translations is evaluated using the Self-BLEU score (Yin et al., 2020). In other words, methods lacking diverse reflection produce more similar translation candidates. Consequently, higher Self-BLEU scores mean lower diversity. We calculate text diversity via:</li>
</ul>
<p>$$
\text { Diversity }=100-\text { Self_BLEU }\left(\text { Cand }<em 2="2">{1}, \text { Cand }</em>\right)
$$</p>
<p>In formula (2), candidates 1 and 2 represent the initial translation (base answer in Self-Reflection or affirmative side's response in MAD) and the current</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Judge LLM</th>
<th style="text-align: center;">COMET</th>
<th style="text-align: center;">HUMAN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Vicuna-13b as Debaters</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13b</td>
<td style="text-align: center;">79.9</td>
<td style="text-align: center;">3.20</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">3.25</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo as Debaters</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vicuna-13b</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">3.47</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5-Turbo</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">3.69</td>
</tr>
</tbody>
</table>
<p>Table 5: Translation performance with different judge.
translation (possible modified answer after SelfReflection or negative side's response in MAD).</p>
<p>As shown in Table 4, Bias and Rigidity are significant factors causing DoT. In addition, addressing these biases and stereotypes through self-reflection can be challenging. MAD framework effectively corrects inherent biases in translation, mitigates DoT, and considerably improves performance.</p>
<h3>4.2 Analysis of Judge</h3>
<p>In this section, we analyze the behavior of the judge for different settings of the debaters.</p>
<p>Strong debaters with a weak judge work better than the reverse. To understand the roles of debaters and judge in MAD, we employ various combinations of models to initialize the agents. Specifically, we utilize the smaller language model (vicuna-13b-v1.5-16k) as a judge to evaluate the debate results of the more powerful LLMs (GPT-3.5-Turbo), and vice versa.</p>
<p>The detailed experimental findings are presented in Table 5. The quality of the debaters' responses significantly impact the performance ceiling of MAD. Regardless of the model chosen for the judge, Turbo debaters consistently generate superior translations compared to Vicuna. In addition, the selection of the judge agent plays a secondary role. When Turbo debaters are involved, Vicuna, serving as the judge, underperforms Turbo across all test sets.</p>
<p>LLM may not act as an impartial judge when different LLMs are used as debaters. We study the behavior of agents by calculating how many times the judge chooses the answers of each debater as the final solution in different scenarios. The results are listed in Table 6 and we have the following observations:</p>
<ul>
<li>Same LLM for All Agents (Rows (1) and (2)):</li>
</ul>
<p>We find that the judge consistently favors the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Translation performance with respect to the debate level on Lexical.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>ID</td>
<td>Jud</td>
<td>Debater</td>
<td></td>
<td>Winner</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td>Aff</td>
<td>Neg</td>
<td>Aff</td>
<td>Neg</td>
<td>Tie</td>
</tr>
<tr>
<td>①</td>
<td>Turbo</td>
<td>Turbo</td>
<td>Turbo</td>
<td>87</td>
<td>104</td>
<td>9</td>
</tr>
<tr>
<td>②</td>
<td>GPT-4</td>
<td>GPT-4</td>
<td>GPT-4</td>
<td>67</td>
<td>124</td>
<td>9</td>
</tr>
<tr>
<td>③</td>
<td>GPT-4</td>
<td>Turbo</td>
<td>GPT-4</td>
<td>52</td>
<td>136</td>
<td>12</td>
</tr>
<tr>
<td>④</td>
<td>GPT-4</td>
<td>GPT-4</td>
<td>Turbo</td>
<td>120</td>
<td>77</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Table 6: Number of times the judge chooses the answers of each debater based on different LLM.</p>
<p>negative side, which is believed to contribute to the performance improvement in MAD. When encountering complex tasks, the affirmative side tends to make mistakes that should be corrected by the opposing side to achieve improvements.</p>
<p>• Debaters of Different LLMs (Rows ③ and ④): We find that the judge shows a preference to the side with the same LLM as the backbone. This bias indicates that LLMs might not be a fair judge (Wang et al., 2023) when different LLMs are used for the agents.</p>
<h3>4.3 Analysis of Debaters</h3>
<p>In this section, we will discuss several factors of debaters that would affect the performance of MAD: <em>debater number</em>, <em>debate level</em>, and <em>debate iteration</em>.</p>
<p>Increasing the number of debaters fails when backbone LLMs are poor at long-text modeling. It seems intuitive that increasing the number of debaters would enhance diversity of thought and subsequently improve performance. However, as shown in Table 7, an increase in the number of debaters has resulted in varying degrees of performance reduction.</p>
<p>To address this issue, we manually analyze the debate processes in approximately 10% of the test</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Distribution of iteration rounds and a human score of each iteration subset.</p>
<table>
<thead>
<tr>
<th># of Debaters</th>
<th>COMET</th>
<th>HUMAN</th>
</tr>
</thead>
<tbody>
<tr>
<td>2 (Default)</td>
<td>84.4</td>
<td>3.69</td>
</tr>
<tr>
<td>3</td>
<td>83.1</td>
<td>3.58</td>
</tr>
<tr>
<td>4</td>
<td>82.9</td>
<td>3.49</td>
</tr>
</tbody>
</table>
<p>Table 7: Translation performance with more debaters.</p>
<p>subset. As the number of debaters increases, the length and complexity of the text also increase. Such LLM-based debaters tend to forget the views of other debaters during the debate. Moreover, it becomes more challenging for the judge to extract information from the debates for summarization. This suggests that the key challenge of MAD with more debaters lies in the limitations of the LLMs to handle long texts (Liu et al., 2024).</p>
<p>Appropriate "tit for tat" is beneficial for effective debate. We then study how the intensity of "tit for tat" affects the performance of MAD. To achieve so, we design different instructions (see Table 11 in Appendix) to initialize the debaters' meta prompt. As shown in Figure 3, asking the debaters to "tit for tat" (i.e., higher disagreement) is necessary for MAD to achieve good performance. However, we find that "<em>must disagree with each other on every point</em>" (with a disagreement of 0.988) does not lead to the best performance. We speculate that continuous disagreement without finding common ground can contribute to polarization, where the debate becomes more about winning the argument than seeking truth or understanding. This can reinforce pre-existing biases and make it difficult to reach a meaningful consensus.</p>
<p>Complex questions require more iteration rounds of debate. In our experimental setup, we did not implement any additional stopping strate-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Performance with respect to the iteration of debate or self-reflection.
gies besides setting the maximum debate iteration to 3. In other words, the judge can take an adaptive break if it believes the optimal answer has already been obtained, efficiently ending the debate early.</p>
<p>To understand the distribution of iteration rounds and factors contributing to a longer debate process, we analyze the experimental results and present them in Figure 4. In the majority of cases, the optimal answer can be achieved through a single round of debate, demonstrating the efficiency of MAD. However, when translating more complex sentences (subsets with lower human scores), the judge requires additional iterations to gather adequate information from the debaters before making a final decision. We also find that our MAD framework consistently brings performance improvements across all the three subsets, demonstrating its effectiveness.</p>
<p>Adaptive break plays an important role to conclude the debate in the optimal moment. Intuitively, longer debates would encourage more diverse thinking. It raises the question of how the model's performance would be affected if constrained to conclude at a specific debate round. For each iteration, we force the judge $J$ to extract the final answer $\left(a=J_{e}(H)\right)$ instead of adaptively breaking the debate as in MAD.</p>
<p>As shown in figure 5, we can observe that MAD performs better than self-reflection as the iteration increases. However, the highest COMET score appears at the first iteration and is also lower than the result of the adaptive break. It indicates that, for most examples, MAD can generate good translations at the first iteration such that the debate should be stopped. Forcing the debate to continue will harm the translation results, which demonstrates the reasonableness of our adaptive break strategy.</p>
<h2>5 Related Work</h2>
<p>Chain-of-Thought Prompting. Recently, (Wei et al., 2022) has proposed chain-of-thought (CoT) prompting to improve the reasoning ability of LLMs. Specifically, CoT prompts LLMs to generate a series of intermediate steps that lead to the final answer of a multi-step problem. Most earlier work primarily concentrates on two main aspects: prompt design and decoding strategies. Zero-shot CoT (Kojima et al., 2022) employs the trigger sentence "Let's think step by step" to provide guidance for the decoding of LLMs. Advanced sampling strategies have been explored to improve CoT by generating diverse reasoning paths, e.g., SelfConsistency (Wang et al., 2022), Auto-CoT (Zhang et al., 2022), Active-Prompting (Diao et al., 2023), Complexity-based Consistency (Fu et al., 2022), Multi-Chain Reasoning (Yoran et al., 2023), and Progressive-Hint Prompting (Zheng et al., 2023).</p>
<p>With the emergence of powerful LLMs, approaches based on self-evaluation have attracted increasing attention. These approaches involve the generation of initial output, followed by evaluating the output to acquire feedback, which is then utilized to refine the output. Evaluation feedback can come from the model itself, e.g., Self-refine (Madaan et al., 2024) and Tree of Thoughts (Yao et al., 2024)) or external environments, e.g., QAaP (Zhu et al., 2023b) and Reflection (Shinn et al., 2024). The intuition behind these approaches involves the utilization of robust LLMs to mimic the human cognition process.</p>
<p>Generative Agents. Recently, LLM-based multiagent intelligent, e.g., Generative Agents (Park et al., 2023), Ghost in the Minecraft (Zhu et al., 2023c), GPT-Bargaining (Fu et al., 2023), has drawn significant attention for enabling simulations of human behavior. Our work follows this research line to address the DoT problem of LLMs. Concurrent with our work, a few studies (Xiong et al., 2023; Du et al., 2023) also explore the multi-agent debate framework to enhance the reasoning ability of LLMs. The main differences between our MAD framework and these works are: (1) we introduce an additional judge with an adaptive break mechanism to decide the optimal moment to conclude the debate; (2) our work aims to address the DoT problem, which is an inherent deficiency of LLMs; and (3) we empirically find that our MAD framework can yield enhanced performance by employing agents with the identical backbone LLM.</p>
<h2>6 Conclusion</h2>
<p>We propose and define the Degeneration-ofThought (DoT) problem in self-reflection, and address it by proposing the Multi-Agent Debate (MAD) framework to explore divergent chain-of-thoughts. We demonstrate the effectiveness of MAD on two challenging tasks and find that GPT-3.5-Turbo with MAD can even surpass GPT-4 on the Common MT dataset. Extensive analyses suggest that the adaptive break strategy of debate and the modest level of "tit for tat" state are required for MAD to obtain good performance. Complex samples require more rounds of debate. More interestingly, we find that LLMs might not be a fair judge if different LLMs are used for agents.</p>
<p>Future work includes scheduling more agents in the debate in an appropriate manner, multi-agent intelligence for board games, and AI feedback for model alignment.</p>
<h2>Limitations</h2>
<p>A limitation of this work is that our method requires more time cost, as agents need to engage in multiple rounds of interaction to present and refute arguments. Moreover, current LLM-based agents may struggle to maintain coherence and relevance in long context scenarios, leading to potential misunderstandings and loss of context. Enhancing long-text modeling capability of large language models remains a future challenge. LLM-based judge may have a preference for outputs generated by itself. To mitigate this bias within the MAD framework, we recommend that all roles, including both the judge and debaters, utilize the same LLM, or alternatively, that the judge and debaters employ distinct LLMs.</p>
<h2>References</h2>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 675-718.</p>
<p>Lisa Bortolotti. 2011. Does reflection lead to wise choices? Philosophical Explorations, 14(3):297313.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Kahneman Daniel. 2017. Thinking, fast and slow. Farrar, Straus and Giroux.</p>
<p>Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023. Active prompting with chain-ofthought for large language models. arXiv preprint arXiv:2302.12246.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325.</p>
<p>Yao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023. Improving language model negotiation with self-play and in-context learning from ai feedback. arXiv preprint arXiv:2305.10142.</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. arXiv preprint arXiv:2210.00720.</p>
<p>Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Melvin Johnson, and Orhan Firat. 2023. The unreasonable effectiveness of fewshot learning for machine translation. In International Conference on Machine Learning, pages 10867-10878. PMLR.</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023. Critic: Large language models can self-correct with tool-interactive critiquing.</p>
<p>Jie He, Tao Wang, Deyi Xiong, and Qun Liu. 2020. The box is in the pen: Evaluating commonsense reasoning in neural machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3662-3672, Online. Association for Computational Linguistics.</p>
<p>Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. 2024. Exploring humanlike translation strategy with large language models. Transactions of the Association for Computational Linguistics, 12:229-246.</p>
<p>Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. arXiv preprint arXiv:2302.09210.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical</p>
<p>Methods in Natural Language Processing (EMNLP), pages 523-533.</p>
<p>Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023. Is chatgpt a good translator? yes with gpt-4 as the engine. arXiv preprint arXiv:2301.08745.</p>
<p>Machiel Keestra. 2017. Metacognition and reflection by interdisciplinary experts: Insights from cognitive science and philosophy. Issues in Interdisciplinary Studies, 35:121-169.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213.</p>
<p>Yuqing Kong, Yunqi Li, Yubo Zhang, Zhihuan Huang, and Jinzhao Wu. 2022. Eliciting thinking hierarchy without a prior. Advances in Neural Information Processing Systems, 35:13329-13341.</p>
<p>Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157-173.</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36.</p>
<p>Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1-22.</p>
<p>Jonathan Pilault, Xavier Garcia, Arthur Bražinskas, and Orhan Firat. 2023. Interactive-chain-prompting: Ambiguity resolution for crosslingual conditional generation with interaction. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 455-483.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743-1752.</p>
<p>Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.</p>
<p>Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. 2023. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 13003-13051.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.</p>
<p>Haoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang Jiao, and Michael Lyu. 2023. Chatgpt or grammarly? evaluating chatgpt on grammatical error correction benchmark. arXiv preprint arXiv:2303.13648.</p>
<p>Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. 2023. Diving into the inter-consistency of large language models: An insightful analysis through debate. arXiv preprint arXiv:2305.11595.</p>
<p>Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36.</p>
<p>Haiyan Yin, Dingcheng Li, Xu Li, and Ping Li. 2020. Meta-cotgan: A meta cooperative training paradigm for improving adversarial text generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 9466-9473.</p>
<p>Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023. Answering questions by meta-reasoning over multiple chains of thought. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5942-5966.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493.</p>
<p>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. 2023. Progressive-hint prompting improves reasoning in large language models. arXiv preprint arXiv:2304.09797.</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Jiaxing Zhang, Yujiu Yang, et al. 2023a. Solving math word problems via cooperative reasoning induced language models. In The 61st Annual Meeting Of The Association For Computational Linguistics.</p>
<p>Xinyu Zhu, Cheng Yang, Bei Chen, Siheng Li, JianGuang Lou, and Yujiu Yang. 2023b. Question answering as programming for solving time-sensitive questions. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12775-12790.</p>
<p>Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. 2023c. Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. arXiv preprint arXiv:2305.17144.</p>
<h1>A Challenging Testbeds</h1>
<p>We conduct experiments on two challenging tasks, namely, commonsense machine translation (i.e., Common MT), and counter-intuitive arithmetic reasoning (i.e., Counter-Intuitive AR), which require deep levels of contemplation for LLMs.</p>
<h2>A. 1 Commonsense Machine Translation</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Ambiguity Type</th>
<th style="text-align: center;">Source Sentence</th>
<th style="text-align: center;">Correct Reference</th>
<th style="text-align: center;">Incorrect Translation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical</td>
<td style="text-align: center;">吃掉敌人一个师。</td>
<td style="text-align: center;">Destroy a division of the enemy.</td>
<td style="text-align: center;">Eat up an enemy division.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">他喜欢吃苹果。</td>
<td style="text-align: center;">He likes to eat apples.</td>
<td style="text-align: center;">He likes to destory apples.</td>
</tr>
<tr>
<td style="text-align: center;">Contextless</td>
<td style="text-align: center;">正在手术的老健康的医生。</td>
<td style="text-align: center;">A healthy doctor is doing surgery.</td>
<td style="text-align: center;">What is undergoing surgery is a doctor who is healthy.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">正在手术的老生命垂危的病人。</td>
<td style="text-align: center;">What is undergoing surgery is a patient whose life is dying.</td>
<td style="text-align: center;">A patient whose life is dying is doing surgery.</td>
</tr>
<tr>
<td style="text-align: center;">Contextual</td>
<td style="text-align: center;">当地震袭击中国时, 揭晰的更中国。</td>
<td style="text-align: center;">When the earthquake hit China, China was aided.</td>
<td style="text-align: center;">When the earthquake hit China, China has assisted.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">当地震袭击日本时, 揭晰的更中国。</td>
<td style="text-align: center;">When the earthquake hit Japan, China has assisted.</td>
<td style="text-align: center;">When the earthquake hit Japan, China was aided.</td>
</tr>
</tbody>
</table>
<p>Table 8: Examples of lexical, contextual and contextless syntactic ambiguity from the Common MT dataset. The underlined Chinese words are translated into the corresponding colored words in English. Best viewed in color.</p>
<p>The Common MT dataset is composed of Chinese $\Rightarrow$ English translation examples (He et al., 2020), which are used to examine three types of ambiguity resolution abilities of translation models. Specifically, The Common MT test set we used covers 200 examples of lexical ambiguity, 450 examples of contextless syntactic ambiguity, and 350 examples of contextual syntactic ambiguity. Within the challenging part of Common MT, the authentic translation of each source sentence requires a proper understanding of common sense knowledge. While these ambiguous sentences might appear to have a straightforward translation, such a literal interpretation is erroneous. Failure to identify and address such ambiguities may result in inaccurate translations.</p>
<p>Table 8 lists some examples of these three types of ambiguity. Lexical ambiguity refers to words with multiple meanings in different contexts. Contextless and contextual syntactic ambiguity involve sentences with multiple interpretations, which can be resolved by context or common sense. As the lexical ambiguity of "吃掉敌人一个师" shows, the source word "吃掉" should be translated to "destroy" rather than the straightforward translation "eat up" by considering the common sense in the real world.</p>
<h2>A. 2 Counter-Intuitive Arithmetic Reasoning</h2>
<p>Previous studies on thinking hierarchy (Daniel, 2017) suggest that we humans have a fast and intuitive system and a slow and logical system, and tend to run the lower level system before the higher level one. Inspired by this, we created a more challenging dataset named Counter-Intuitive Arithmetic Reasoning (CIAR) to evaluate the reasoning abilities of LLMs at deep levels.</p>
<p>Dataset Description. Our Counter-Intuitive AR dataset contains 200 questions collected from elicitation questions (Kong et al., 2022) ${ }^{8}$, web data ${ }^{9}$ and additional manual derivatives of these questions. Compared to the commonly-used datasets, e.g., MultiArith (Roy and Roth, 2015), GSM8K (Cobbe et al., 2021), our dataset presents two distinct challenges:</p>
<ul>
<li>Resistance to Intuition. The questions in our dataset are embedded in hidden traps designed to elicit intuitive and appealing answers that are often incorrect. This feature evaluates the abilities of LLMs to resist the traps of superficial expressions.</li>
</ul>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Components</th>
<th style="text-align: left;">Content</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Question</td>
<td style="text-align: left;">When Alice walks up the hill, her speed is $1 \mathrm{~m} / \mathrm{s}$ and when she goes down the <br> hill, her speed is $3 \mathrm{~m} / \mathrm{s}$. Then when Alice walks up and down the hill, what is <br> her average speed?</td>
</tr>
<tr>
<td style="text-align: left;">Correct Answer</td>
<td style="text-align: left;">$1.5 \mathrm{~m} / \mathrm{s}$</td>
</tr>
<tr>
<td style="text-align: left;">Explanation</td>
<td style="text-align: left;">If Alice covers a distance of d going up and down the hill, then her total distance <br> is 2 d . Her time going up the hill is $\mathrm{d} / 1=\mathrm{d}$, and her time going down the hill is <br> $\mathrm{d} / 3$. So, her total time is $\mathrm{d}+\mathrm{d} / 3=4 \mathrm{~d} / 3$. Therefore, her average speed is $2 \mathrm{~d} /$ <br> $(4 \mathrm{~d} / 3)=3 / 2 \mathrm{~m} / \mathrm{s}$.</td>
</tr>
<tr>
<td style="text-align: left;">Incorrect Answer</td>
<td style="text-align: left;">$2 \mathrm{~m} / \mathrm{s}$</td>
</tr>
<tr>
<td style="text-align: left;">Explanation</td>
<td style="text-align: left;">Alice's average speed can be calculated by adding her speed going up the hill <br> and her speed going down the hill, and then dividing by 2. So, $(1 \mathrm{~m} / \mathrm{s}+3 \mathrm{~m} / \mathrm{s}) /$ <br> $2=2 \mathrm{~m} / \mathrm{s}$. Therefore, Alice's average speed is $2 \mathrm{~m} / \mathrm{s}$.</td>
</tr>
</tbody>
</table>
<p>Table 9: An example in Counter-Intuitive AR dataset.</p>
<ul>
<li>Multi-Step Reasoning. Each correct answer within the dataset requires a rigorous multi-step reasoning process, thereby evaluating the capacity of LLMs to engage in complex decision-making and problemsolving.</li>
</ul>
<p>Dataset Format. In our Counter-Intuitive AR dataset, each example contains three key components (see Table 9 for an example). We elaborate on the details below:</p>
<ul>
<li>Questions. The questions in our dataset are designed to stimulate counter-intuitive thinking, which aims to challenge conventional decision-making by presenting situations where the immediate, intuitive response is often incorrect.</li>
<li>Answers. Each question is provided with a correct answer, which requires deep comprehension of the question and commonsense knowledge. Additionally, we also provide a plausible yet incorrect answer for comparison.</li>
<li>Explanations. We offer comprehensive explanations for each correct answer, detailing the step-by-step rationale that leads to the right solution. We also provide the seemingly logical reasoning process behind incorrect answers. This reasoning process highlights the potential pitfalls and misconceptions during decision-making, especially when intuition is prioritized over rigorous logical reasoning.</li>
</ul>
<p>Experimental Settings. During our experiments, we did not utilize the explanations from the dataset. We provided detailed explanations to facilitate subsequent researchers to understand how the correct answer was derived.</p>
<h1>B Human Evaluation Details</h1>
<p>We implement human evaluation as follows:</p>
<ul>
<li>Human Score: We randomly shuffled the display order of the translated sentences from all methods in an anonymous manner. Then, employed three professional human translators (Krippendorff's Alpha $=$ 0.76 ) to directly assess all methods together. Finally, we calculated the average scores for each methods.</li>
<li>Bias: We also focus on whether the translation of specific words in CommonMT conforms to commonsense. Table 8 lists an example of lexical ambiguity, where the source word "无非" should be translated to "destroy" rather than the straightforward translation "eat up". Here, we asked the annotators to label each sentence as 1 (not conforming to commonsense) or 0 (conforming to commonsense), and report the degree of bias for the whole test set.</li>
</ul>
<h1>C Results on math and symbolic reasoning tasks</h1>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Math Reasoning</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Symbolic Reasoning (BBH)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">GSM</td>
<td style="text-align: center;">AddSub</td>
<td style="text-align: center;">Penguin</td>
<td style="text-align: center;">Date</td>
<td style="text-align: center;">Colored Objects</td>
</tr>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: center;">70.2</td>
<td style="text-align: center;">87.3</td>
<td style="text-align: center;">58.9</td>
<td style="text-align: center;">56.4</td>
<td style="text-align: center;">57.2</td>
</tr>
<tr>
<td style="text-align: left;">Self-Reflect</td>
<td style="text-align: center;">70.8</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">61.0</td>
<td style="text-align: center;">58.0</td>
<td style="text-align: center;">58.0</td>
</tr>
<tr>
<td style="text-align: left;">MAD</td>
<td style="text-align: center;">73.8</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">63.7</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">58.8</td>
</tr>
</tbody>
</table>
<p>Table 10: Comparison of accuracy on math (e.g. GSM (Cobbe et al., 2021) and AddSub (Hosseini et al., 2014)) and symbolic reasoning (three datasets from Big-Bench (Srivastava et al., 2023; Suzgun et al., 2023)).</p>
<h2>D Prompts for Different Debate Levels</h2>
<table>
<thead>
<tr>
<th style="text-align: center;">Level</th>
<th style="text-align: left;">Prompt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">0</td>
<td style="text-align: left;">Both sides must reach a full consensus on every point of the debate. Every statement <br> must be agreed upon by both sides.</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Most of the debate should be characterized by disagreements, but there may still be a <br> small amount of consensus on less significant points.</td>
</tr>
<tr>
<td style="text-align: center;">2 (Default)</td>
<td style="text-align: left;">It's not necessary to fully agree with each other's perspectives, as our objective is to <br> find the correct answer.</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: left;">Both sides must disagree with each other on every point of the debate. There should <br> be no consensus whatsoever.</td>
</tr>
</tbody>
</table>
<p>Table 11: Prompts for different levels of "tit for tat" state. We modulate the level of "tit for tat" state outlined in Section 2 through appending natural language instructions to the debaters' meta prompt.</p>
<h2>E Extra Computational Cost</h2>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Cost (# Generated Tokens)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">COT</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Self-Reflect</td>
<td style="text-align: center;">$1.83 \times$</td>
</tr>
<tr>
<td style="text-align: left;">MAD</td>
<td style="text-align: center;">$2.46 \times$</td>
</tr>
</tbody>
</table>
<p>Table 12: Extra generated tokens on the counter-intuitive AR task.
We measure the inference cost by the number of generated tokens. As listed in Table 12, the selfreflection cost is 1.83 times that of the baseline COT method. Meanwhile, our MAD approach incurs an additional 0.63 times the inference cost. This increase in cost is deemed acceptable given the significant benefit our approach offers in enhancing LLMs' divergent thinking capabilities without requiring external signals (interaction with humans).</p>
<h1>F Debate Process</h1>
<h2>F. 1 Commonsense Machine Translation</h2>
<p>Table 13 presents the debate process of example translation discussed in Section 3.3. In this debate, both sides disputed the English translation of the Chinese sentence "吃掉敌人一个师". The affirmative side proposed "eat up an enemy division", advocating for a more literal translation to depict the consumption or devouring of the enemy. In contrast, the negative side suggested "eliminate an enemy division", arguing this expression better conveys the destruction of the enemy in a military context. After the debate, the judge favored "eliminate", as it accurately reflected the severity and intent of the original military language. This debate highlights the complexities of translation, where context and nuance significantly impact the selection of appropriate words. In response to this issue, our MAD method helps LLMs understand semantics correctly and generate highly authentic translations.</p>
<h2>$&lt;$ Debate Topic $&gt;$ : What is the correct English translation of the following text: 吃掉敌人一个师?</h2>
<p>I believe the correct translation of the Chinese text "吃掉敌人一个师" is "Eat up an enemy division." The word "吃掉" literally means "eat up" or "consume."</p>
<p>I disagree with your translation. I believe the correct translation of the Chinese text "吃掉敌人一个师" is "Eliminate an enemy division." While "吃掉" can be translated as "eat up" or "consume", it can also mean "eliminate" or "annihilate" in a military context. In this case, it is more appropriate to use "eliminate" as it conveys the idea of completely destroying the enemy division.</p>
<p>Both sides presented valid arguments and translations that accurately convey the meaning of the original Chinese text. The choice between "eat up" and "eliminate" may depend on the context and tone of the text. Therefore, there is no clear preference for either side at this time.</p>
<p>While "eliminate" does convey the idea of completely destroying the enemy division, it doesn't capture the connotation of consuming or devouring the enemy that is implied by "吃掉".
D. In a military context, "吃掉" is often used to describe the complete destruction of the enemy, rather than the consumption of their resources or territory.</p>
<p>Both sides presented valid arguments, but the negative side's argument that "eliminate" better captures the intensity and seriousness of the military language used in the original Chinese text was more convincing. Additionally, "eliminate" accurately conveys the idea of completely destroying the enemy division. Therefore, the correct English translation of the Chinese text "吃掉敌人一个师" is "Eliminate an enemy division."</p>
<p>Table 13: The debate process of translation example on Chinese sentence "吃掉敌人一个师".</p>
<h1>F. 2 Counter-Intuitive Arithmetic Reasoning</h1>
<p>Table 14 shows the debate process of example answer in Table 9. This debate is about calculating Alice's average speed while walking up and down a hill, with speeds given as $1 \mathrm{~m} / \mathrm{s}$ uphill and $3 \mathrm{~m} / \mathrm{s}$ downhill. Initially, it was incorrectly proposed by affirmative side to just average the uphill and downhill speeds. The negative side, which is correct, states that average speed is total distance divided by total time. Given that the distances up and down the hill are equal, we must calculate the total time taken for the two directions before finding the average speed. Therefore, the average speed is calculated using the sum of the uphill and downhill distances (twice the distance of the hill) and the sum of the time taken for each ( $\mathrm{d} / 1$ for uphill and $\mathrm{d} / 3$ for downhill). This leads to an average speed of $1.5 \mathrm{~m} / \mathrm{s}$, not $2 \mathrm{~m} / \mathrm{s}$ as the affirmative side proposed. And the judge also supports the response from the negative side. Here, LLMs made a common mistake initially in understanding the concept of average speed. Our MAD method encourages divergent thinking and effectively prevents intuitive but incorrect responses, leading to the correct method for calculating average speed.
$&lt;$ Debate Topic $&gt;$ : When Alice walks up the hill, her speed is $1 \mathrm{~m} / \mathrm{s}$ and when she goes down the hill, her speed is $3 \mathrm{~m} / \mathrm{s}$. Then when Alice walks up and down the hill, what is her average speed?
$\square$ Alice's average speed can be calculated by adding her speed going up the hill and her speed going down the hill, and then dividing by 2 . So, $(1 \mathrm{~m} / \mathrm{s}+3 \mathrm{~m} / \mathrm{s}) / 2=2 \mathrm{~m} / \mathrm{s}$. Therefore, Alice's average speed is $2 \mathrm{~m} / \mathrm{s}$.
I disagree with your answer. The problem doesn't give us any information about the distance Alice covers while going up and down the hill. So, we cannot simply add her speeds and divide by 2 to get her average speed. We need to know the distance she covers to calculate her average speed.
(2) The average speed is calculated by dividing the total distance by the total time taken. As Alice walks up and down the hill, the distance covered in both directions is the same. Therefore, the total distance covered is 2 times the distance of either direction.
(3) That's a good point. But we can assume that Alice covers the same distance going up and down the hill. In that case, we can use the formula for average speed, which is total distance divided by total time. If Alice covers a distance of d going up and down the hill, then her total distance is 2 d . Her time going up the hill is $\mathrm{d} / 1=\mathrm{d}$, and her time going down the hill is $\mathrm{d} / 3$. So, her total time is d $+\mathrm{d} / 3=4 \mathrm{~d} / 3$. Therefore, her average speed is $2 \mathrm{~d} /(4 \mathrm{~d} / 3)=3 / 2 \mathrm{~m} / \mathrm{s}$.
(4) Thank you for providing your answer. This average speed of $1.5 \mathrm{~m} / \mathrm{s}$ takes into account both the uphill and downhill speeds, providing a balanced representation of Alice's overall performance while walking up and down the hill.
(2) Yes. Let's assume the distance is d. The time taken to walk up the hill is $\mathrm{d} / 1$ and the time taken to walk down the hill is $\mathrm{d} / 3$. The total time taken is $\mathrm{d} / 1+\mathrm{d} / 3=4 \mathrm{~d} / 3$. Therefore, the average speed is $(2 \mathrm{~d}) /(4 \mathrm{~d} / 3)=1.5 \mathrm{~m} / \mathrm{s}$. Hence, the negative side is correct.</p>
<p>Table 14: The debate process of arithmetic reasoning example.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ https://elicitation.info/questionnaire/1/
${ }^{9}$ https://www.geeksforgeeks.org/puzzles/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ https://huggingface.co/lmsys/vicuna-7b-v1.5-16k
${ }^{4}$ https://huggingface.co/lmsys/vicuna-13b-v1.5-16k
${ }^{5}$ We use wmt21-comet-qe-da as the QE HUMANr.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
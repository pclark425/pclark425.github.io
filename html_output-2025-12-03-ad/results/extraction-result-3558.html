<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3558 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3558</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3558</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-443e5490e7f9fedf912a330fc7a0456d4247ab3d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/443e5490e7f9fedf912a330fc7a0456d4247ab3d" target="_blank">MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This study demonstrates the potential of LLMs as effective few-shot molecular property predictors as effective in-context reasoning capabilities across 10 downstream evaluation datasets, setting new benchmarks for few-shot molecular prediction tasks.</p>
                <p><strong>Paper Abstract:</strong> Molecular property prediction (MPP) is a fundamental and crucial task in drug discovery. However, prior methods are limited by the requirement for a large number of labeled molecules and their restricted ability to generalize for unseen and new tasks, both of which are essential for real-world applications. To address these challenges, we present MolecularGPT for few-shot MPP. From a perspective on instruction tuning, we fine-tune large language models (LLMs) based on curated molecular instructions spanning over 1000 property prediction tasks. This enables building a versatile and specialized LLM that can be adapted to novel MPP tasks without any fine-tuning through zero- and few-shot in-context learning (ICL). MolecularGPT exhibits competitive in-context reasoning capabilities across 10 downstream evaluation datasets, setting new benchmarks for few-shot molecular prediction tasks. More importantly, with just two-shot examples, MolecularGPT can outperform standard supervised graph neural network methods on 4 out of 7 datasets. It also excels state-of-the-art LLM baselines by up to 15.7% increase on classification accuracy and decrease of 17.9 on regression metrics (e.g., RMSE) under zero-shot. This study demonstrates the potential of LLMs as effective few-shot molecular property predictors. The code is available at https://github.com/NYUSHCS/MolecularGPT.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A deep learning approach to antibiotic discovery <em>(Rating: 2)</em></li>
                <li>Autonomous, multiproperty-driven molecular discovery: From predictions to measurements and back. <em>(Rating: 2)</em></li>
                <li>Scaling deep learning for materials discovery. <em>(Rating: 1)</em></li>
                <li>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation <em>(Rating: 2)</em></li>
                <li>Mol-instructions: A large-scale biomolecular instruction dataset for large language models <em>(Rating: 1)</em></li>
                <li>Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3558",
    "paper_id": "paper-443e5490e7f9fedf912a330fc7a0456d4247ab3d",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A deep learning approach to antibiotic discovery",
            "rating": 2
        },
        {
            "paper_title": "Autonomous, multiproperty-driven molecular discovery: From predictions to measurements and back.",
            "rating": 2
        },
        {
            "paper_title": "Scaling deep learning for materials discovery.",
            "rating": 1
        },
        {
            "paper_title": "Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation",
            "rating": 2
        },
        {
            "paper_title": "Mol-instructions: A large-scale biomolecular instruction dataset for large language models",
            "rating": 1
        },
        {
            "paper_title": "Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning",
            "rating": 1
        }
    ],
    "cost": 0.00841875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction</h1>
<p>Yuyan Liu ${ }^{1,2}$, Sirui Ding ${ }^{3}$, Sheng Zhou ${ }^{4}$, Wenqi Fan ${ }^{5}$, Qiaoyu Tan ${ }^{1}$<br>${ }^{1}$ New York University Shanghai<br>${ }^{2}$ Xiamen University, ${ }^{3}$ University of California, San Francisco<br>${ }^{4}$ Zhejiang University, ${ }^{5}$ The Hong Kong Polytechnic University<br>yanyanyan@stu.xmu.edu.cn, qiaoyu.tan@nyu.edu</p>
<h4>Abstract</h4>
<p>Molecular property prediction (MPP) is a fundamental and crucial task in drug discovery. However, prior methods are limited by the requirement for a large number of labeled molecules and their restricted ability to generalize for unseen and new tasks, both of which are essential for real-world applications. To address these challenges, we present MolecularGPT for few-shot MPP. From a perspective on instruction tuning, we fine-tune large language models (LLMs) based on curated molecular instructions spanning over 1000 property prediction tasks. This enables building a versatile and specialized LLM that can be adapted to novel MPP tasks without any fine-tuning through zero- and few-shot in-context learning (ICL). MolecularGPT exhibits competitive incontext reasoning capabilities across 10 downstream evaluation datasets, setting new benchmarks for few-shot molecular prediction tasks. More importantly, with just two-shot examples, MolecularGPT can outperform standard supervised graph neural network methods on 4 out of 7 datasets. It also excels state-of-the-art LLM baselines by up to $15.7 \%$ increase on classification accuracy and decrease of 17.9 on regression metrics (e.g., RMSE) under zeroshot. This study demonstrates the potential of LLMs as effective few-shot molecular property predictors. The code is available at https: //github.com/NYUSHCS/MolecularGPT.</p>
<h2>1 Introduction</h2>
<p>The discovery of molecules with desired functional properties is crucial for advancements in fields such as medicine (Stokes et al., 2020; Wong et al., 2024; Koscher et al., 2023; Abramson et al., 2024) and material (Merchant et al., 2023; Kang et al., 2023). Molecular property prediction (MPP), which employs deep learning techniques to predict molecules' functional properties, has proven effective in accelerating the drug discovery process
and reducing associated costs (Wong et al., 2024; Merchant et al., 2023; Kang et al., 2023).</p>
<p>Among them, graph neural networks (GNNs)based methods (Velickovic et al., 2017; Xu et al., 2019; Kipf and Welling, 2017; Gilmer et al., 2017; Hamilton et al., 2017) have achieved state-of-theart results in the past few years. However, these methods (Li et al., 2022; Liu et al., 2022; Stärk et al., 2022) are limited in supervised settings, contradicting with practical needs as annotating molecules is both expensive and time-consuming. Furthermore, the task-specific supervised learning process may hurdle the model's adaptation to new tasks, limiting its generalization ability in openworld scenarios.</p>
<p>Inspired by this, several recent endeavors have aimed to enable zero-shot reasoning for MPP (Seidl et al., 2023; Zhao et al., 2024) by integrating both natural language and molecular representations. CLAMP (Seidl et al., 2023) is a text-molecule model that aligns pairs of chemical text (e.g., descriptions of molecular properties) and molecule graphs through contrastive learning. Subsequently, the bioactivity of a query molecule is classified by measuring the similarity between its molecular representation and corresponding bioassay description. While effective, CLAMP is limited to classification tasks and is not a generative model.</p>
<p>In contrast, another line of research in LLMs (Zhao et al., 2024) integrates molecule graphs and task descriptions into a unified generative LLM. This approach enables zero-shot reasoning for molecular property prediction across both classification and regression tasks. However, the inclusion of an additional architectural design restricts it from performing few-shot molecular property predictions, a capability naturally supported by standard LLMs.</p>
<p>To date, there's no LLM-based method in the molecular domain fully inherits the generalization and ICL abilities of LLMs as seen in the NLP field,</p>
<p>which raises a research question: Can LLMs be fine-tuned for generic MPP, enabling the resultant model to generalize to a variety of unseen tasks and inherit LLMs’ few-shot ICL ability?</p>
<p>In this work, we aim to bridge the gap and present MolecularGPT, the first instructionally tuned LLM that can generalize to a variety of novel MPP tasks while retaining its zero-shot and fewshot in-context reasoning abilities. Specifically, MolecularGPT adopts the SMILES (Weininger, 1988) representation of molecules as a unified graph-to-string transformation for instruction construction, as it precisely translates molecules’ chemical structures into a string of atomic symbols and chemical bonds based on a set of rules. To fully utilize the graph structures in molecules, we introduce structure-aware few-shot instructions, which incorporate the top- $k$ neighbors, globally retrieved based on their similarities, of each molecule as complementary information for instruction design. This design aligns the instruction tuning and inference prompt format of MolecularGPT, making it naturally applicable for few-shot ICL. Additionally, to balance zero-shot and few-shot reasoning capabilities, we explore various combination options and empirically find that a hybrid instruction set, including both zero-shot and few-shot instructions, enables MolecularGPT to perform well in both zero-shot and few-shot property predictions. Our main contributions are summarized below:</p>
<ul>
<li>We study how to adapt pre-trained LLMs to molecular field, enabling effective few-shot MPP in the ICL fashion. Specifically, we propose MolecularGPT, the first instructionally fine-tuned LLM that supports few-shot property prediction on unseen tasks without any fine-tuning.</li>
<li>We introduce the concept of structure-aware fewshot instruction to better adapt LLMs with molecular field. Unlike existing efforts (Seidl et al., 2023; Zhao et al., 2024; Zhang et al., 2023) that focus on fusing graph structures and SMILES representations in a model-centric perspective, we maliciously combine them in a data-centric manner by constructing global structure-aware few-shot demonstrations.</li>
<li>We devise a hybrid instruction set to inherit the few-shot ICL capability of LLMs. This set is a mix of both few-shot and zero-shot instructions that span over 1000 MPP tasks including both classification and regression tasks across biological, chemical, and quantum mechanical domains, resulting in 3.5GB training tokens. This diversified instruction set has been empirically proved to be effective in adapting LLMs for MPP tasks.</li>
<li>We extensively experimented on 10 molecular property benchmarks across different scales and tasks to validate the effectiveness of MoelcularGPT. Our empirical results demonstrate that MoelcularGPT outperforms the leading LLM baselines (e.g., GIMLET and LLaMA-7b (Touvron et al., 2023)), with up to an average 15.7\% improvement across all classification tasks. Additionally, with just two-shot examples, MolecularGPT surpass standard supervised GNN methods on 4 out of 7 datasets, setting new benchmarks for few-shot molecular property tasks.</li>
</ul>
<h2>2 Related work</h2>
<p>GNNs-based MMP GNNs (Velickovic et al., 2017; Xu et al., 2019; Kipf and Welling, 2017; Stärk et al., 2022) perform MPP tasks by constructing models between molecular graphs and properties. Though these models have achieved great success (Gilmer et al., 2017; Hamilton et al., 2017; Li et al., 2022; Liu et al., 2022; Tan et al., 2023), they are implicitly trained for each task without explicit natural language instructions, limiting their ability to generalize to new tasks. While a few approaches (Guo et al., 2021; Wang et al., 2021; Schimunek et al., 2023a; Zhuang et al., 2023a) aim to extend pretrained GNNs to new tasks, they require a large number of related tasks to pre-train a meta-learner, making them label-intensive and deviating from our generalization goals. Moreover, these supervised models rely solely on structure information, overlooking the rich textual knowledge derived from wet lab experiments.
Pretrain-finetune based molecular language models To utilize the chemical knowledge in texts, molecular language models (Liu et al., 2023a; Edwards et al., 2022; Pei et al., 2023; Zhang et al., 2023; Liu et al., 2023c) aim to integrate natural language and molecular representations for joint reasoning. These models (Su et al., 2022; Zeng et al., 2022; Liu et al., 2023b, 2024; Li et al., 2024) involve two stages: pre-training and fine-tuning. The pre-training phase primarily focuses on learning molecular representations and their associated textual descriptions through masked language modeling, contrastive learning or next token prediction.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The proposed MolecularGPT framework. To instructionally fine-tune LLMs for MPP tasks, we construct a hybrid instruction set that includes both zero-shot and few-shot instructions across more than 1000 property tasks. Each few-shot instruction adaptively selects the query molecule's top-k neighboring molecules as labeled demonstrations for prompt design.</p>
<p>However, they still require fine-tuning on particular MPP downstream tasks, thereby limiting their generalization abilities to new tasks.</p>
<h4>Instruction tuning based molecular language models</h4>
<p>To address this, recent efforts in molecular language modeling (Fang et al., 2023; Zhao et al., 2024) aim to explicitly align molecular graphs with their properties through instruction tuning (Longpre et al., 2023). For instance, GIMLET (Zhao et al., 2024) integrates molecular graphs with instruction languages for fine-tuning LLMs. GIMLET achieves effective zero-shot ICL for new tasks but lacks few-shot ICL capability due to its generalized position embedding and decoupled attention designs. Mol-Instructions (Fang et al., 2023) is a close work to us, but it fine-tunes LLMs with only three properties tasks and neglects inter-molecular correlations, significantly limiting its zero-shot and few-shot ICL performances. In contrast, we curate a diverse instruction set covering 1000 property tasks and introduce structure-aware few-shot instructions to significantly enhance the zero-shot and few-shot reasoning capabilities of LLMs in MPP tasks. More details about these property tasks can be found in Appendix A.1</p>
<h1>3 Method</h1>
<p>In this section, we present the proposed MolecularGPT, as shown in Fig. 1. First, we discuss the general instructional fine-tuning pipeline to adapt LLMs for MPP tasks (in Section 3.1). Next, we elaborate on a structure-aware few-shot instruction design strategy to effectively incorporate graph structures among molecules (in Section 3.2). Finally, we illustrate a hybrid instruction tuning approach that enhances both the zero-shot and few-shot reasoning capabilities of LLMs for MPP tasks (in Section 3.3).</p>
<h4>Notations and Problem Formulation</h4>
<p>Given a set of n molecular graphs D = {(<em>G</em><sub><em>i</em></sub>, <em>y</em><sub><em>i</em></sub>)|<em>i</em> ∈ 1, 2, ..., <em>n</em>}, where G<sub><em>i</em></sub> = (V, E) represents the <em>i</em>-th molecule and <em>y</em><sub><em>i</em></sub> is the ground-truth property (e.g., categorical label or numerical score). Here, V and E denote the node set and edge set, respectively. The goal of molecular instruction tuning is to fine-tune a LLM model <em>f</em><sub>θ</sub> by fitting a set of training instructions <em>S</em><sub>D</sub> (i.e., (<em>input</em>, <em>output</em>) pairs) constructed from D, so that the fine-tuned LLM can be directly applied to make property predictions for unseen tasks or molecules, i.e., <em>D</em><sub>test</sub> = {(<em>G</em><sub><em>j</em></sub>, <em>y</em><sub><em>j</em></sub>)|<em>j</em> = 1, 2, ..., <em>m</em>} with D ∩ <em>D</em><sub>test</sub> = ∅.</p>
<p>While conceptually simple, successfully achieving molecular instruction tuning involves addressing several research challenges. <strong>C1</strong>: how can we unify molecules of varying sizes, densities, and domains into a consistent format, ensuring that important molecular information in D and <em>D</em><sub>test</sub> is consistently incorporated? <strong>C2</strong>: given that graph structures are crucial for molecular analysis, as verified in GNN studies, how can we effectively include these structures in molecular instruction tuning? <strong>C3</strong>: considering that molecule annotation is notoriously expensive and time-consuming, how can we enable the fine-tuned LLM to benefit</p>
<p>from few-shot scenarios where only a few labeled molecules are available in real-world applications?</p>
<h3>3.1 SMILES-based Molecular Instruction Tuning: A Unified Step</h3>
<p>To improve the generalization capability of finetuned LLM for MPP tasks (C1), we aim to employ the well-known graph-derived linear strings (Krenn et al., 2020) of molecular graphs, SMILES (Weininger, 1988) for instruction tuning. Different from GNN encoders (Seidl et al., 2023; Zhao et al., 2024), SMILES translates molecules' chemical structure into a string of atomic symbols and chemical bonds based on a set of rules (Qian et al., 2023). This precise translation provides a universal expression foundation for different types of molecules. Following standard instruction tuning protocol (Christofidellis et al., 2023; Zhang et al., 2023; Liu et al., 2024; Li et al., 2024), the molecular instruction set $S_{D}$ can be generated by the following prompt template $T={Q, I, R}$ based on $D$, regarding as a zero-shot instruction template.</p>
<div class="codehilite"><pre><span></span><code>### Instruction: {instruction}
### Input: {inputs}
### Response: {output}.
</code></pre></div>

<p>Here, the instruction question $Q$, SMILES strings of query molecule $I$, and property label $R$ are mapped to the {instruction}, {inputs}, and {output} components, respectively.</p>
<h3>3.2 Structure-Aware Molecular Instruction Tuning: Graph Structure Matters</h3>
<p>So far, we have illustrated how to incorporate graph structure within each molecule into instruction via the zero-shot instruction template $T$. However, this approach may result in subpar prediction performance due to the neglect of correlations between molecules. To address this, we introduce structureaware instruction tuning (C2), which aims to incorporate inter-molecular structures into the prompt template. The high-level idea is to utilize similar molecules as demonstrations to enhance LLM reasoning.</p>
<p>To achieve this, given a query molecule $G_{i} \in D$, we identify its top- $K$ nearest molecules in $D$ based on the following retrieval module.</p>
<p>$$
N_{G_{i}}=\operatorname{topK}\left(G_{i}, D, K\right)
$$</p>
<p>where $N_{G_{i}}$ is the retrieved neighborhood set with $K$ molecules. $\operatorname{topK}()$ is a search algorithm based
on the similarity between molecules. Specifically, we estimate the similarity between molecules by calculating their Tanimoto coefficient (Tanimoto, 1958) based on their MACCS Keys (Durant et al., 2002). Notably, MACCS Keys, comprising 166 binary keybits, provides a unified representation for molecules and has been widely adopted in many molecule retrieval systems, such as USearch (Vardanian, 2023).</p>
<p>Utilizing $N_{G_{i}}$, we can transform the zeroshot template into a few-shot version $T_{\text {shot }}=$ ${C, I, R}$, where $C$ represents the k -shot instruction question, extending $Q$ with structurally similar molecule demonstrations extracted from $N_{G_{i}}$. Specifically, let $\left(m_{i}, y_{i}\right)$ represents the $i$-th similar molecule-property pairs in $N_{G_{i}}$. Additionally, considering that the order of demonstrations may significant impact prompt design (Mosbach et al., 2023), we arrange these k demonstrations in a descending order based on their similarity scores. The $C$ is formally expressed as:</p>
<p>$$
C=\left{Q,\left(\left(m_{1}, y_{1}\right), \ldots,\left(m_{i}, y_{i}\right), \ldots,\left(m_{k}, y_{k}\right)\right)\right}
$$</p>
<p>Similar to $T$, the extended question $C$ in the fewshot instruction template $T_{\text {shot }}$ will correspond to the {instruction} of the template in Section 3.1. In experiments, we empirically observed that including the target property of molecular neighbors as input in few-shot scenarios improves performance. This approach is reasonable because $T_{\text {shot }}$ serves as a few-shot in-context prompt, akin to those widely used in the NLP domain, where the most similar neighbors are selected as demonstrations.</p>
<h3>3.3 Hybrid Molecular Instruction Tuning: Better Few-Shot Learner</h3>
<p>Given the advanced structure-aware instruction template $T_{\text {shot }}$, one can easily construct the instruction training set $S_{D}$ by applying $T_{\text {shot }}$ on each molecule in $D$. Then, we fine-tune a pre-trained LLM by optimizing the following training loss:</p>
<p>$$
\mathcal{L}(\theta)=\sum_{\left(C_{i}, I_{i}, R_{i}\right)) \in \mathcal{S}<em _theta="\theta">{D}}-\log f</em>\right)
$$}\left(R_{i} \mid C_{i}, I_{i</p>
<p>Here, $f_{\theta}$ is a pre-trained LLM with parameter $\theta$. In practice, we initialize $f_{\theta}$ as LLaMA2-7b-chat (Touvron et al., 2023) and adopt QLoRA (Dettmers et al., 2024) to speedup the training.</p>
<p>While $T_{\text {shot }}$ appears effective, it may degrade the zero-shot reasoning capability of fine-tuned LLM due to the explicit graph structures among</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The performance on Cyp450 test dataset.</p>
<p>molecules. To verify this, we conducted a toy example by fine-tuning LLaMA2-7b-chat on different <em>K</em>-shot instruction sets. Specifically, Fig. 2 reports the zero-shot and one-shot inference results on the CYP450 dataset for <em>K</em> = 0 and <em>K</em> = 4.</p>
<p>In Fig. 2, we can observe an obvious trade-off between zero-shot and one-shot performance with respect to the instruction set. For example, when fine-tuning LLaMA2 on the 0-shot instruction set constructed using the <em>T</em> template, the resulting <em>0-shot_tuning</em> model performs well in zero-shot scenarios but underperforms in one-shot scenarios. Conversely, when fine-tuning on the 4-shot instruction set constructed using <em>Tshot</em> with <em>K</em> = 4, the resulting <em>4-shot_tuning</em> model excels in one-shot settings but underperforms in zero-shot cases.</p>
<p>This observation motivates us to introduce a hybrid instruction set <em>S<sup>h</sup><sub>D</sub></em>, combining the strengths of both the zero-shot instruction template <em>T</em> and the few-shot instruction template <em>Tshot</em>. Specifically, <em>S<sup>h</sup><sub>D</sub></em> is derived from a combination of 0, 1, 2, 3, and 4-shot instruction templates. In Fig. 2, we can see that our hybrid instruction tuned models, <em>0&amp;4-shot_tuning</em> and <em>0-4-shot_tuning</em>, consistently outperforms others in zero-shot and one-shot scenarios. Further details can be found in Section 4.3.</p>
<h2>4 Experiment</h2>
<p>In our experimental framework, we aim to answer three primary research questions: <strong>RQ1:</strong> Can MolecularGPT effectively handle new property prediction tasks through few- and zero-shot ICL? <strong>RQ2:</strong> What is the optimal design for in-context instruction set to improve MolecularGPT's generalization and ICL abilities during tuning? <strong>RQ3:</strong> How does the question prompts and retrieval in-context examples affect MolecularGPT's performance?</p>
<h3>4.1 Experimental Setup</h3>
<p><strong>Datastes</strong> Consistent with the GIMLET setting, we employ the MoleculeNet benchmark (Wu et al., 2018) and CYP450 (Li et al., 2018) datasets as our downstream datasets, totally 657 MMP tasks. More details about datasets can be found in Appendix A.1. We employ ROC-AUC as the evaluation metric for classification tasks, while the Root Mean Square Error (RMSE) for regression tasks.</p>
<p><strong>Baselines</strong> We compare our MolecularGPT with 5 leading GNN-based methods: GCN (Kipf and Welling, 2017), GAT (Velickovic et al., 2017), GIN (Xu et al., 2019), Graphormer (Ying et al., 2021), and Graphormer-p (Ying et al., 2021), 5 SOTA LM-based molecular models: XVPLM (Zeng et al., 2022), MoMu (Su et al., 2022), Galactica-125M (Taylor et al., 2022), Galactica-1.3B (Taylor et al., 2022), and GIMLET (Zhao et al., 2024), and 2 strong few-shot molecular property prediction baselines: Pre-GS-Meta (Zhuang et al., 2023b) and MHNfs (Schimunek et al., 2023b), as well as the open-sourced LLM: LLaMA-chat-7B (Touvron et al., 2023)</p>
<h3>4.2 Performance Evaluation</h3>
<p>As the results presented in Tab. 1, 2 respectively, MolecularGPT can achieve competitive performance on classification and regression tasks under both few-shot and zero-shot settings. We answer the <strong>RQ1</strong> with more details as follows.</p>
<p>① <strong>MolecularGPT establishes a new benchmark in few-shot learning.</strong> As shown in Tab. 1, our model delivers the best performance across all datasets under the few-shot setting, securing the top-1 average rank across all datasets in both the 2-shot and 8-shot scenarios, with respective ranks of 1.1 and 2.1. Most notably, unlike models in the few-shot finetuned category, such as GIMLET, which require additional tuning and parameter updates, our MolecularGPT is capable of directly inference with few-shot examples, demonstrating an average increase of 4.3% in 2-shot and 2.8% in 8-shot scenarios respectively. Comparing with strongest model in few-shot inference category, Galactica1.3B, our model shows an average increase of 7.9% in 2-shot and 3.6% in 8-shot scenarios separately, proving its strongest generalization capabilities. Additionally, MolecularGPT even surpasses supervised finetuned GNNs in 4 out of 7 classification tasks in a 2-shot setting, as shown in Tab. 1 and 2.</p>
<p>② <strong>MolecularGPT exhibits exceptional performance in zero-shot learning.</strong> As shown in Tab. 2, during zero-shot inference, our model achieves the top-1 average rank across all datasets, outperforming the strongest baseline, GIMLET, on 5 out of 10 datasets. It demonstrates an average improvement</p>
<p>Table 1: Performance over Bio-activity (Bio), Toxicity (Tox), and Pharmacokinetic (Pha) classification tasks. Highlights are the first and second best results over all 2-shot and 8-shot performances. We also report the average rank (Avg.RK) of all models under 2-shot and 8-shot separately.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Method</th>
<th>Shot</th>
<th>BACE</th>
<th>HIV</th>
<th>MUV</th>
<th>Avg.Bio</th>
<th>Tox21</th>
<th>ToxCast</th>
<th>Avg.Tox</th>
<th>BBBP</th>
<th>CYP450</th>
<th>Avg.Pha</th>
<th>Avg.RK</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Pre-GS-Meta</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>---</td>
<td>---</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>KVPLM</td>
<td></td>
<td></td>
<td>2-shot</td>
<td>0.5213</td>
<td>0.5702</td>
<td>0.5507</td>
<td>0.5474</td>
<td>0.5976</td>
<td>0.5310</td>
<td>0.5643</td>
<td>0.6668</td>
<td>0.5105</td>
</tr>
<tr>
<td></td>
<td></td>
<td>8-shot</td>
<td>0.6203</td>
<td>0.6128</td>
<td>0.5791</td>
<td>0.5474</td>
<td>0.5665</td>
<td>0.5208</td>
<td>0.5437</td>
<td>0.6004</td>
<td>0.6378</td>
<td>0.6191</td>
<td>-6.4</td>
</tr>
<tr>
<td></td>
<td></td>
<td>2-shot</td>
<td>0.6380</td>
<td>0.6675</td>
<td>0.5296</td>
<td>0.6117</td>
<td>0.5336</td>
<td>0.5122</td>
<td>0.5229</td>
<td>0.5399</td>
<td>0.5960</td>
<td>0.5680</td>
<td>-6.0</td>
</tr>
<tr>
<td>Few-shot</td>
<td>KVPLM</td>
<td>8-shot</td>
<td>0.6175</td>
<td>0.6293</td>
<td>0.4987</td>
<td>0.5885</td>
<td>0.5712</td>
<td>0.4892</td>
<td>0.5302</td>
<td>0.5401</td>
<td>0.6305</td>
<td>0.5853</td>
<td>-7.0</td>
</tr>
<tr>
<td></td>
<td>MeMu</td>
<td>2-shot</td>
<td>0.5747</td>
<td>0.6435</td>
<td>0.4701</td>
<td>0.5568</td>
<td>0.5297</td>
<td>0.5164</td>
<td>0.5231</td>
<td>0.5901</td>
<td>0.5976</td>
<td>0.5939</td>
<td>-6.7</td>
</tr>
<tr>
<td></td>
<td></td>
<td>8-shot</td>
<td>0.6102</td>
<td>0.6171</td>
<td>0.5804</td>
<td>0.6026</td>
<td>0.5517</td>
<td>0.5226</td>
<td>0.5372</td>
<td>0.5728</td>
<td>0.5713</td>
<td>0.5721</td>
<td>-7.0</td>
</tr>
<tr>
<td></td>
<td>GIMLET</td>
<td>2-shot</td>
<td>0.7141</td>
<td>0.6794</td>
<td>0.6420</td>
<td>0.6785</td>
<td>0.6222</td>
<td>0.5903</td>
<td>0.6063</td>
<td>0.6094</td>
<td>0.7212</td>
<td>0.6653</td>
<td>-2.4</td>
</tr>
<tr>
<td></td>
<td></td>
<td>8-shot</td>
<td>0.7081</td>
<td>0.6803</td>
<td>0.6487</td>
<td>0.6790</td>
<td>0.6197</td>
<td>0.5911</td>
<td>0.6054</td>
<td>0.6119</td>
<td>0.7234</td>
<td>0.6677</td>
<td>-2.9</td>
</tr>
<tr>
<td></td>
<td>MHNfs</td>
<td>2-shot</td>
<td>0.5939</td>
<td>0.5835</td>
<td>0.5566</td>
<td>0.5780</td>
<td>0.5360</td>
<td>0.5370</td>
<td>0.5365</td>
<td>0.5750</td>
<td>0.5734</td>
<td>0.5742</td>
<td>-6.0</td>
</tr>
<tr>
<td></td>
<td></td>
<td>8-shot</td>
<td>0.6294</td>
<td>0.6397</td>
<td>0.6764</td>
<td>0.6485</td>
<td>0.6162</td>
<td>0.5602</td>
<td>0.5892</td>
<td>0.5442</td>
<td>0.5992</td>
<td>0.5717</td>
<td>-2.9</td>
</tr>
<tr>
<td></td>
<td>LLaMA2-7B</td>
<td>2-shot</td>
<td>0.6030</td>
<td>0.6587</td>
<td>0.5085</td>
<td>0.6201</td>
<td>0.6052</td>
<td>0.5010</td>
<td>0.5531</td>
<td>0.5459</td>
<td>0.5807</td>
<td>0.5633</td>
<td>-5.9</td>
</tr>
<tr>
<td></td>
<td></td>
<td>8-shot</td>
<td>0.7271</td>
<td>0.7000</td>
<td>0.5161</td>
<td>0.6477</td>
<td>0.6505</td>
<td>0.5329</td>
<td>0.5917</td>
<td>0.5127</td>
<td>0.6891</td>
<td>0.6009</td>
<td>-4.0</td>
</tr>
<tr>
<td></td>
<td>Galactica125M</td>
<td>2-shot</td>
<td>0.6298</td>
<td>0.6089</td>
<td>0.4831</td>
<td>0.5739</td>
<td>0.5902</td>
<td>0.5090</td>
<td>0.5496</td>
<td>0.3931</td>
<td>0.6961</td>
<td>0.5446</td>
<td>-6.7</td>
</tr>
<tr>
<td></td>
<td></td>
<td>8-shot</td>
<td>0.5248</td>
<td>0.6474</td>
<td>0.5302</td>
<td>0.5875</td>
<td>0.6083</td>
<td>0.5035</td>
<td>0.5559</td>
<td>0.4174</td>
<td>0.6217</td>
<td>0.5196</td>
<td>-7.0</td>
</tr>
<tr>
<td></td>
<td>Galactica1.3B</td>
<td>2-shot</td>
<td>0.6634</td>
<td>0.5295</td>
<td>0.5171</td>
<td>0.5700</td>
<td>0.6302</td>
<td>0.5566</td>
<td>0.5934</td>
<td>0.6970</td>
<td>0.7323</td>
<td>0.7147</td>
<td>-4.0</td>
</tr>
<tr>
<td></td>
<td></td>
<td>8-shot</td>
<td>0.7152</td>
<td>0.6623</td>
<td>0.4977</td>
<td>0.6251</td>
<td>0.6470</td>
<td>0.5279</td>
<td>0.5875</td>
<td>0.5692</td>
<td>0.7645</td>
<td>0.7369</td>
<td>-3.7</td>
</tr>
<tr>
<td></td>
<td>MolecularGPT</td>
<td>2-shot</td>
<td>0.7118</td>
<td>0.7384</td>
<td>0.6338</td>
<td>0.6520</td>
<td>0.6571</td>
<td>0.5945</td>
<td>0.6259</td>
<td>0.7268</td>
<td>0.8275</td>
<td>0.7768</td>
<td>1.1</td>
</tr>
<tr>
<td></td>
<td></td>
<td>8-shot</td>
<td>0.7331</td>
<td>0.6382</td>
<td>0.6469</td>
<td>0.6727</td>
<td>0.6565</td>
<td>0.5985</td>
<td>0.6275</td>
<td>0.6822</td>
<td>0.8228</td>
<td>0.7525</td>
<td>2.1</td>
</tr>
</tbody>
</table>
<p>of 6.2% on 3 classification tasks and an average decrease of 0.16 on 2 regression tasks. Comparing with our base model LLaMA, our model shows an average 15.7% improvement on classification tasks and 17.9 decrease on regression tasks. Furthermore, when compared with GNNs that have been supervised finetuned on complete training set of each dataset, our model demonstrates comparable performance on the HIV, BBBP, CYP450, and ESOL datasets, highlighting its efficacy in a zero-shot setting.</p>
<h3>4.3 Tuning on Hybrid Instruction Set</h3>
<p>To investigate the RQ2, we conduct experiments to study the effect of hybrid instruction tuning set as presented in Fig. 3.</p>
<p>(3) Tuning on property descriptions without demonstrations can improve the zero-shot performance. As shown in the 0-shot_tuning in Fig. 3, the model performed satisfactorily on some tasks under 0-shot inference but poorly on many tasks under 2-shot inference. In comparison with 2-shot inference, 0-shot inference even demonstrates an average improvement of 4% on classification tasks. We speculated that the zero-shot instruction set imparts some knowledge to LLaMA without significantly enhancing the model’s ICL ability.</p>
<p>(4) Providing the model with rich retrieved demonstrations would significantly improve its ICL ability. To test this, we fine-tuned the model on a 4-shot instruction dataset, represented by the 4-shot_tuning in Fig. 3. The results indicate an improvement in the model’s ICL ability. In 2-shot inference setting, comparing with 0-shot_tuning, 4-shot_tuning shows an average 8.2% increase on classification tasks. However, the model’s zero-shot generalization remained subpar on many tasks.</p>
<p>We surmise that the model may learn shortcuts from the label words of the reference molecules rather than extracting the true relationships between molecular representations and properties.</p>
<p>(5) Mixed-shot instruction sets are promising to optimize both zero-shot generalization and ICL abilities. We developed two mixed instruction datasets: a combined 0&amp;4-shot and a comprehensive mix of $0,1,2,3,4$-shot (0-4-shot) instruction datasets. As shown in 0&amp;4-shot_tuning and 0-4-shot_tuning in Fig. 3, models fine-tuned on mixed-shot instruction datasets demonstrate a significant performance improvement compared to those fine-tuned on 0-shot or 4-shot instruction sets. This trend is consistently observed across various tested scenarios, like BACE, HIV, Tox21, BBBP and CYP450, indicating that our model derives the most benefit from mixed-shot instruction sets.</p>
<p>(6) Tuning on larger instruction set have exhibited superior performance across different tasks under both zero and few shot learning. Models trained with larger datasets have exhibited superior performance on multi functional tasks, as evidenced by the improvements from GPT-2 <em>Radford et al. (2019)</em> to GPT3 <em>Brown et al. (2020)</em> and LLaMA2 <em>Touvron et al. (2023)</em> to LLaMA3 <em>Meta (2024)</em>. To further enhance MolecularGPT, we double the size of the 0&amp;4-shot instruction sets. The results represented by the 0&amp;4-shot_tuning_double in Fig. 3 suggest that expanding the data scale enhances the model’s performance across various tasks, like HIV, MUV and CYP450, either by zero-shot or few-shot learning.</p>
<h3>4.4 Hyperparameter Sensitivity Analysis</h3>
<p>To fully utilize the ICL ability of MolecularGPT, we now pay attention to the impact of question</p>
<p>Table 2: Performance over Bio-activity, Toxicity, and Pharmacokinetic classification tasks as well as Physicalchemical regression tasks. Highlights are the first and second best results of 0-shot performances. In supervised finetuned models, we also mark the highest and lowest results. Here, "finetuned" refers to the model being trained on the complete training set.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Type</th>
<th>BACE</th>
<th>HIV</th>
<th>MUV</th>
<th>Tox21</th>
<th>ToxCast</th>
<th>BBBP</th>
<th>CYP450</th>
<th>ESOL</th>
<th>FreeSolv</th>
<th>Lipo</th>
<th>Avg.RK</th>
</tr>
</thead>
<tbody>
<tr>
<td>XVPLM</td>
<td>0-shot</td>
<td>0.5126</td>
<td>0.6120</td>
<td>0.6172</td>
<td>0.4917</td>
<td>0.5096</td>
<td>0.6020</td>
<td>0.5922</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>4.3</td>
</tr>
<tr>
<td>MeMu</td>
<td></td>
<td>0.6656</td>
<td>0.5026</td>
<td>0.6051</td>
<td>0.5757</td>
<td>0.5238</td>
<td>0.4981</td>
<td>0.5798</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>3.9</td>
</tr>
<tr>
<td>Galactica125M</td>
<td></td>
<td>0.4451</td>
<td>0.3671</td>
<td>0.4986</td>
<td>0.4964</td>
<td>0.5106</td>
<td>0.6052</td>
<td>0.5369</td>
<td>2.130</td>
<td>3.660</td>
<td>1.097</td>
<td>4.4</td>
</tr>
<tr>
<td>Galactica1.3B</td>
<td></td>
<td>0.5648</td>
<td>0.3385</td>
<td>0.5715</td>
<td>0.4946</td>
<td>0.5123</td>
<td>0.5394</td>
<td>0.4686</td>
<td>1.103</td>
<td>4.267</td>
<td>1.093</td>
<td>4.1</td>
</tr>
<tr>
<td>LLaMA2-7B</td>
<td></td>
<td>0.4911</td>
<td>0.6060</td>
<td>0.5554</td>
<td>0.5481</td>
<td>0.4693</td>
<td>0.3671</td>
<td>0.4198</td>
<td>7.227</td>
<td>15.912</td>
<td>2.329</td>
<td>5.6</td>
</tr>
<tr>
<td>GIMLET</td>
<td></td>
<td>0.6957</td>
<td>0.6624</td>
<td>0.6439</td>
<td>0.6119</td>
<td>0.5904</td>
<td>0.5939</td>
<td>0.7125</td>
<td>1.132</td>
<td>5.103</td>
<td>1.345</td>
<td>2.2</td>
</tr>
<tr>
<td>MolecularGPT</td>
<td></td>
<td>0.6212</td>
<td>0.7128</td>
<td>0.6253</td>
<td>0.5893</td>
<td>0.5669</td>
<td>0.6373</td>
<td>0.8031</td>
<td>1.471</td>
<td>4.975</td>
<td>1.157</td>
<td>2.1</td>
</tr>
<tr>
<td>GCN</td>
<td>Finetuned</td>
<td>0.736</td>
<td>0.757</td>
<td>0.732</td>
<td>0.749</td>
<td>0.633</td>
<td>0.649</td>
<td>0.8041</td>
<td>1.331</td>
<td>2.119</td>
<td>0.760</td>
<td>3.6</td>
</tr>
<tr>
<td>GAT</td>
<td></td>
<td>0.697</td>
<td>0.729</td>
<td>0.666</td>
<td>0.754</td>
<td>0.646</td>
<td>0.662</td>
<td>0.8281</td>
<td>1.253</td>
<td>2.493</td>
<td>0.770</td>
<td>4.3</td>
</tr>
<tr>
<td>GIN</td>
<td></td>
<td>0.701</td>
<td>0.753</td>
<td>0.718</td>
<td>0.740</td>
<td>0.634</td>
<td>0.658</td>
<td>0.8205</td>
<td>1.243</td>
<td>2.871</td>
<td>0.781</td>
<td>4.0</td>
</tr>
<tr>
<td>Graphormer</td>
<td></td>
<td>0.7760</td>
<td>0.7452</td>
<td>0.7061</td>
<td>0.7589</td>
<td>0.6470</td>
<td>0.7015</td>
<td>0.8436</td>
<td>0.901</td>
<td>2.210</td>
<td>0.740</td>
<td>2.5</td>
</tr>
<tr>
<td>Graphormer-p</td>
<td></td>
<td>0.8575</td>
<td>0.7788</td>
<td>0.7480</td>
<td>0.7729</td>
<td>0.6649</td>
<td>0.7163</td>
<td>0.8877</td>
<td>0.804</td>
<td>1.850</td>
<td>0.675</td>
<td>1.0</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The performance of MolecularGPT on Classifcation (Cls) and Regression (Reg) tasks tuning with different types of instruction datasets. We inference them with 0-shot and 2-shot examples. (0&amp;4-shot indicates hybrid of 0 and 4-shot. 0-4-shot indicates mix of 0,1,2,3,4-shot. tuning_double indicates double the instruction set size.)</p>
<p>Prompts and the retrieval examples to discuss the <strong>RQ3</strong>. Specifically, for question prompts, we test the robustness of our model responses to different instructions, shown in Tab. 3. As for few-shot demonstrations, we discuss the impact of molecular fingerprints in Tab. 4, as well as the number, order, and diversity of demonstrations in Fig 4.</p>
<p>➀ The exceptional robustness of MolecularGPT is validated across different tasks. Given the diversity and flexibility of natural language, we aim to evaluate the robustness of MolecularGPT against various instruction prompts. Adhering to the downstream datasets used in GIMLET, which provides five distinct types of instructions. We calculate the standard deviation of the ROC-AUC or RMSE metrics derived from these five instruction datasets. Comparative results with GIMLET are presented in Tab. 3. It is evident that our model demonstrates superior robustness compared to GIMLET across most tasks, with an average standard deviation of 0.06 compared to GIMLET's 0.08. This indicts the robustness of MolecularGPT that it genuinely comprehends complex instructions and can handle a range of property prediction tasks without requiring task-specific prompt designs.</p>
<p>➁ The effectiveness of MolecularGPT with different fingerprints for retrieving examples. In both the 2-shot and 8-shot settings, our model demonstrates significant effectiveness when utilizing MACCS or MACCS-ECFP4 fingerprints for retrieving in-context examples, as illustrated in Tab. 4. The method based on MACCS-ECFP4 even outperforms our original MACCS-based method on 7 out of 10 tasks in the 8-shot setting. This indicates that our model exhibits strong structural awareness and generalization ability.</p>
<p>➂ MolecularGPT gains significant enhancement with up to 2 demonstrations, but the marginal benefit diminishes with additional retrieval molecules. We investigate the impact of</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The performance of MolecularGPT on Classifcation (Cls) and Regrassion (Reg) tasks with different in-context inference strategies. To show our model's remarkable capability, we also add the performance of the finetuned model, GAT.</p>
<p>Table 3: Standard deviation for GIMLET and MolecularGPT in response to 5 types of instructions.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>BACE</th>
<th>HIV</th>
<th>MCV</th>
<th>Tox21</th>
<th>ToxCast</th>
<th>BBBP</th>
<th>CYP450</th>
<th>ESOL</th>
<th>FreeInfo</th>
<th>Lipo</th>
</tr>
</thead>
<tbody>
<tr>
<td>GIMLET</td>
<td>0.024</td>
<td>0.034</td>
<td>0.017</td>
<td>0.009</td>
<td>0.005</td>
<td>0.013</td>
<td>0.012</td>
<td>0.020</td>
<td>0.521</td>
<td>0.002</td>
</tr>
<tr>
<td>MolecularGPT</td>
<td>0.009</td>
<td>0.018</td>
<td>0.012</td>
<td>0.008</td>
<td>0.002</td>
<td>0.007</td>
<td>0.002</td>
<td>0.007</td>
<td>0.498</td>
<td>0.002</td>
</tr>
</tbody>
</table>
<p>Table 4: The performance of MolecularGPT when using different fingerprints for retrieving in-context examples.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Mod.</th>
<th>BACE</th>
<th>HIV</th>
<th>MCV</th>
<th>Tox21</th>
<th>ToxCast</th>
<th>BBBP</th>
<th>CYP450</th>
<th>ESOL</th>
<th>FreeInfo</th>
<th>Lipo</th>
</tr>
</thead>
<tbody>
<tr>
<td>HACCA-ECFPA</td>
<td>2-slice</td>
<td>0.5218</td>
<td>0.7284</td>
<td>0.6330</td>
<td>0.6573</td>
<td>0.5945</td>
<td>0.7204</td>
<td>0.6273</td>
<td>1.489</td>
<td>3.226</td>
<td>1.015</td>
</tr>
<tr>
<td></td>
<td></td>
<td>0.5835</td>
<td>0.6990</td>
<td>0.6602</td>
<td>0.6749</td>
<td>0.5995</td>
<td>0.7298</td>
<td>0.6401</td>
<td>1.392</td>
<td>3.085</td>
<td>1.035</td>
</tr>
<tr>
<td></td>
<td></td>
<td>0.7451</td>
<td>0.6382</td>
<td>0.6485</td>
<td>0.6565</td>
<td>0.5980</td>
<td>0.6822</td>
<td>0.6224</td>
<td>1.419</td>
<td>3.033</td>
<td>1.036</td>
</tr>
<tr>
<td></td>
<td></td>
<td>0.7556</td>
<td>0.6674</td>
<td>0.6195</td>
<td>0.6085</td>
<td>0.5985</td>
<td>0.6962</td>
<td>0.6445</td>
<td>1.312</td>
<td>3.017</td>
<td>1.061</td>
</tr>
</tbody>
</table>
<p>the number (Ye et al., 2024) of retrieval demonstrations, ranging from 0 to 8 examples based on similarity. The results indicate significant improvement when provided with up to 2 examples cross all datasets. However, the performance does not get further improvement with more retrieval molecules. We hypothesize that: 1) More noise will be introduced with the increase of examples that has lower similarity with the query. 2) The maximum input length of 512 tokens with at most 4 examples in instructions constrains the model's capability while handling more examples.</p>
<p><strong>3.9 Ascending order of similarity for demonstrations is sub-optimal compared to descending order especially with more demonstrations.</strong> We arrange the demonstrations (Lu et al., 2022; Zhao et al., 2021) in a ascending order, placing the most similar examples at the end of k-shot instructions. The results in Fig. 4 show that the ascending order is sub-optimal comparing to descending order, especially with more demonstrations which may be constrained by the model's long context capability. In an 8-shot setting, the ascending order shows an average performance that is 0.9% lower than the descending order on classification tasks. We assume the model is more adaptable to reasoning with descending order by learning most related knowledge first.</p>
<p><strong>4.1 Similar retrieved molecule demonstrations provides better performance than diverse demonstrations.</strong> To increase the diversity, we retrieve equal number of molecules from each category (Ma et al., 2024). When the same number of examples is provided within instructions, the retrieval approach based on similarity consistently outperforms the one based on diversity across all classification tasks as shown in Fig. 4. In an 8-shot setting, the diversity-based method shows an average performance that is 2.7% lower than the similarity-based on classification tasks. The similarity-based methodology tends to provide examples that align more coherently with the query molecules. In contrast, the diversity-based approach offers a mix of positive and negative examples, which potentially introduce noise and create ambiguity perplexing the language models.</p>
<h1>5 Conclusion</h1>
<p>In this study, we aim to equip the LLMs, particularly the LLaMA, with an expanded knowledge of molecular properties, enabling it to generalize to out-of-domain prediction tasks through zero-shot and few-shot ICL. We introduce MolecularGPT, a model that has been instructed to tune on over 1000 prediction tasks. Furthermore, we investigate the most effective types of instruction datasets for optimizing the model during both training and inference stages. Our findings demonstrate that MolecularGPT consistently outperforms baseline language models in few-shot scenarios and even surpasses supervised models on multiple datasets. In future work, we plan to incorporate additional molecular modalities and expand into other molecular-related tasks such as molecule captioning.</p>
<h2>6 Limitation</h2>
<p>In our research, we utilize SMILES strings to represent molecules. However, while effective, this approach overlooks the geometric structure information of real-world molecules, such as the 3D spatial position of each atom in a molecule. This limitation hinders our model's ability to represent molecular structures. Meanwhile, our work focuses solely on property prediction tasks and does not consider foundational tasks such as molecule optimization, molecule generation, and molecule captioning. This may restrict the potential applications of our model in practical settings. Lastly, although our model is compatible with supervised GNN models for classification tasks, we still have some gaps with them in regression tasks as directly generating numbers remains a challenge for nowadays foundational LLMs.</p>
<h2>References</h2>
<p>Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, et al. 2024. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pages 1-3.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica. 2023. Unifying molecular and textual representations via multi-task language modelling. In International Conference on Machine Learning.</p>
<p>Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691.</p>
<p>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36.</p>
<p>Joseph L Durant, Burton A Leland, Douglas R Henry, and James G Nourse. 2002. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42(6):12731280.</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. 2022. Translation between molecules and natural language. In Proceed-
ings of the 2022 Conference on Empirical Methods in Natural Language Processing.</p>
<p>Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, and Huajun Chen. 2023. Mol-instructions: A large-scale biomolecular instruction dataset for large language models. arXiv preprint arXiv:2306.08018.</p>
<p>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. 2017. Neural message passing for quantum chemistry. In International conference on machine learning, pages 1263-1272. PMLR.</p>
<p>Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang, and Nitesh V Chawla. 2021. Few-shot graph learning for molecular property prediction. In Proceedings of the web conference 2021, pages 2559-2567.</p>
<p>Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. Advances in neural information processing systems, 30.</p>
<p>Yeonghun Kang, Hyunsoo Park, Berend Smit, and Jihan Kim. 2023. A multi-modal pre-training transformer for universal transfer learning in metalorganic frameworks. Nature Machine Intelligence, $5(3): 309-318$.</p>
<p>Thomas N. Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In International Conference on Learning Representations.</p>
<p>Brent A Koscher, Richard B Canty, Matthew A McDonald, Kevin P Greenman, Charles J McGill, Camille L Bilodeau, Wengong Jin, Haoyang Wu, Florence H Vermeire, Brooke Jin, et al. 2023. Autonomous, multiproperty-driven molecular discovery: From predictions to measurements and back. Science, 382(6677):eadi1407.</p>
<p>Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. 2020. Selfreferencing embedded strings (selfies): A $100 \%$ robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024.</p>
<p>Shuangli Li, Jingbo Zhou, Tong Xu, Dejing Dou, and Hui Xiong. 2022. Geomgcl: Geometric graph contrastive learning for molecular property prediction. In 36th AAAI Conference on Artificial Intelligence, AAAI 2022, pages 4541-4549. Association for the Advancement of Artificial Intelligence.</p>
<p>Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, and Qi Tian. 2024. 3d-molm: Towards 3d molecule-text interpretation in language models. In ICLR.</p>
<p>Xiang Li, Youjun Xu, Luhua Lai, and Jianfeng Pei. 2018. Prediction of human cytochrome p450 inhibition using a multitask deep autoencoder neural network. Molecular pharmaceutics, 15(10):4336-4345.</p>
<p>Pengfei Liu, Yiming Ren, Jun Tao, and Zhixiang Ren. 2024. Git-mol: A multi-modal large language model for molecular science with graph, image, and text. Computers in Biology and Medicine, 171:108073.</p>
<p>Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, and Animashree Anandkumar. 2023a. Multimodal molecule structure-text model for text-based retrieval and editing. Nature Machine Intelligence, $5(12): 1447-1457$.</p>
<p>Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. 2022. Pretraining molecular graph representation with 3d geometry. In International Conference on Learning Representations.</p>
<p>Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, and Tie-Yan Liu. 2023b. Molxpt: Wrapping molecules with text for generative pre-training. In Annual Meeting of the Association for Computational Linguistics.</p>
<p>Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. 2023c. Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. In EMNLP.</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, pages 22631-22648. PMLR.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086-8098.</p>
<p>Huan Ma, Changqing Zhang, Yatao Bian, Lemao Liu, Zhirui Zhang, Peilin Zhao, Shu Zhang, Huazhu Fu, Qinghua Hu, and Bingzhe Wu. 2024. Fairnessguided few-shot prompting for large language models. Advances in Neural Information Processing Systems, 36.</p>
<p>Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol, Gowoon Cheon, and Ekin Dogus Cubuk. 2023. Scaling deep learning for materials discovery. Nature, 624(7990):80-85.</p>
<p>AI Meta. 2024. Introducing meta llama 3: The most capable openly available llm to date. Meta AI.</p>
<p>Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. 2023. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938.</p>
<p>OpenAI. 2023. GPT-4 technical report. ArXiv preprint, abs/2303.08774.</p>
<p>Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu, Yingce Xia, and Rui Yan. 2023. Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. arXiv preprint arXiv:2310.07276.</p>
<p>Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, and Yong Liu. 2023. Can large language models empower molecular property prediction? arXiv preprint arXiv:2307.07443.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. 2014. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):1-7.</p>
<p>Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining, pages 3505-3506.</p>
<p>Johannes Schimunek, Philipp Seidl, Lukas Friedrich, Daniel Kuhn, Friedrich Rippmann, Sepp Hochreiter, and Günter Klambauer. 2023a. Context-enriched molecule representations improve few-shot drug discovery. arXiv preprint arXiv:2305.09481.</p>
<p>Johannes Schimunek, Philipp Seidl, Lukas Friedrich, Daniel Kuhn, Friedrich Rippmann, Sepp Hochreiter, and Günter Klambauer. 2023b. Context-enriched molecule representations improve few-shot drug discovery. In The Eleventh International Conference on Learning Representations.</p>
<p>Philipp Seidl, Andreu Vall, Sepp Hochreiter, and Günter Klambauer. 2023. Enhancing activity prediction models in drug discovery with the ability to understand human language. In International Conference on Machine Learning, pages 30458-30490. PMLR.</p>
<p>Hannes Stärk, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Günnemann, and Pietro Liò. 2022. 3d infomax improves gnns for molecular property prediction. In International Conference on Machine Learning, pages 20479-20502. PMLR.</p>
<p>Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackermann, et al. 2020. A deep learning approach to antibiotic discovery. Cell, 180(4):688-702.</p>
<p>Bing Su, Dazhao Du, Zhao Yang, Yujie Zhou, Jiangmeng Li, Anyi Rao, Hao Sun, Zhiwu Lu, and JiRong Wen. 2022. A molecular multimodal foundation model associating molecule graphs with natural language. arXiv preprint arXiv:2209.05481.</p>
<p>Qiaoyu Tan, Ninghao Liu, Xiao Huang, Soo-Hyun Choi, Li Li, Rui Chen, and Xia Hu. 2023. S2gae: self-supervised graph autoencoders are generalizable learners with graph masking. In Proceedings of the sixteenth ACM international conference on web search and data mining, pages 787-795.</p>
<p>Taffee T Tanimoto. 1958. Elementary mathematical theory of classification and prediction.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Ash Vardanian. 2023. USearch by Unum Cloud.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, et al. 2017. Graph attention networks. stat, 1050(20):1048550 .</p>
<p>Yaqing Wang, Abulikemu Abuduweili, Quanming Yao, and Dejing Dou. 2021. Property-aware relation networks for few-shot molecular property prediction. Advances in Neural Information Processing Systems, 34:17441-17454.</p>
<p>David Weininger. 1988. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31-36.</p>
<p>Felix Wong, Erica J Zheng, Jacqueline A Valeri, Nina M Donghia, Melis N Anahtar, Satotaka Omori, Alicia Li, Andres Cubillos-Ruiz, Aarti Krishnan, Wengong Jin, et al. 2024. Discovery of a structural class of antibiotics with explainable deep learning. Nature, 626(7997):177-185.</p>
<p>Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. 2018. Moleculenet: a benchmark for molecular machine learning. Chemical science, 9(2):513-530.</p>
<p>Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How powerful are graph neural networks? In International Conference on Learning Representations.</p>
<p>Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, and Minjoon Seo. 2024. Investigating the effectiveness of task-agnostic prefix prompt for instruction following. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 19386-19394.</p>
<p>Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and TieYan Liu. 2021. Do transformers really perform badly for graph representation? Advances in neural information processing systems, 34:28877-28888.</p>
<p>Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. 2022. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nature communications, 13(1):862.</p>
<p>Weitong Zhang, Xiaoyun Wang, Weili Nie, Joe Eaton, Brad Rees, and Quanquan Gu. 2023. Moleculegpt: Instruction following large language models for molecular property prediction. In NeurIPS 2023 Workshop on New Frontiers of AI for Drug Discovery and Development.</p>
<p>Haiteng Zhao, Shengchao Liu, Ma Chang, Hannan Xu, Jie Fu, Zhihong Deng, Lingpeng Kong, and Qi Liu. 2024. Gimlet: A unified graph-text model for instruction-based molecule zero-shot learning. Advances in Neural Information Processing Systems, 36.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pages 12697-12706. PMLR.</p>
<p>Xiang Zhuang, Qiang Zhang, Bin Wu, Keyan Ding, Yin Fang, and Huajun Chen. 2023a. Graph samplingbased meta-learning for molecular property prediction. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 4729-4737.</p>
<p>Xiang Zhuang, Qiang Zhang, Bin Wu, Keyan Ding, Yin Fang, and Huajun Chen. 2023b. Graph samplingbased meta-learning for molecular property prediction. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 4729-4737.</p>
<p>A Datasets</p>
<h3>A.1 Details of datasets</h3>
<p>We follow the dataset selection and splitting criteria specified in GIMLET (Zhao et al., 2024). As for downstream tasks, we divide the datasets in a ratio of 0.8:0.1:0.1 and report the results on the test sets. Moreover, considering the importance and extensive research in the field of quantum mechanical properties, we have included an additional two quantum mechanical properties: Highest Occupied Molecular Orbital(HOMO) and Lowest Unoccupied Molecular Orbital (LUMO), from the QM9 datasets (Ramakrishnan et al., 2014) as our instruction tuning datasets. To construct the instructions for these additional datasets, we employed the method in Mol-Instructions (Fang et al., 2023) and GIMLET (Zhao et al., 2024). Initially, we write a property description for each task according to Wikipedia and chemistry papers. Subsequently, we employ GPT-4.0 (OpenAI, 2023) to generate instructions based on these seed examples, resulting in various human question-framing styles instructions. The comprehensive list of tuning and downstream tasks are summarized in Tab. 5.</p>
<h3>A.2 Details of instructions</h3>
<p>Our instruction tuning datasets comprise three components: instruction, input, and output. The instruction component includes a description of the property along with some retrieval examples. The input is the SMILES string of the query molecule, while the output is the property label of query molecule. Here are a few examples of few-shot instructions from three tuning datasets: ChEMBL bioassay activity dataset, CHEMBL Property dataset, and QM9 dataset.</p>
<p>A 1-shot instruction tuning sample from CHEMBL Property datasets:
"### Instruction: Aromatic rings (also known as aromatic compounds or arenes) are hydrocarbons which contain benzene, or some other related ring structure. Here are some examples.
SMILES: Cc1ccc2ccccc2n1
label: 2
Please count how many aromatic rings exist in this molecule.</p>
<h3>Input: Cc1ccnc2ccccc12</h3>
<h3>Response: 2"</h3>
<p>A 3-shot instruction tuning sample from ChEMBL bioassay activity datasets:
"### Instruction: The assay is PUBCHEM_BIOASSAY: NCI human tumor cell line growth inhibition assay. Data for the DMS 273 Small Cell Lung cell line. (Class of assay: confirmatory), and it is Target assigned is non-molecular. The assay has properties: assay category is confirmatory; assay cell type is DMS-273; assay type description is Functional. Here are some examples.
SMILES: $\mathrm{CC}(\mathrm{C}) \mathrm{C}(\mathrm{N})=\mathrm{O}$
label: No
SMILES: $\mathrm{O}=\mathrm{CNC}=\mathrm{Cc} 1 \mathrm{ccccc1}$
label: No
SMILES: $\mathrm{COC}(=\mathrm{O}) \mathrm{C} # \mathrm{CC}(\mathrm{N})=\mathrm{O}$
label: No
Is the molecule effective to this assay?</p>
<h3>Input: CNC=O</h3>
<h3>Response: No"</h3>
<p>A 4-shot instruction tuning sample from QM9 datasets:
"### Instruction: Lumo is the Lowest unoccupied molecular orbital energy. Here are some examples. SMILES: CC
label: 0.1
SMILES: $\mathrm{CC}(\mathrm{C} # \mathrm{C}) \mathrm{C} # \mathrm{CC} # \mathrm{C}$
label: -0.02
SMILES: CC#CC#CC#C
label: -0.05
SMILES: $\mathrm{CC}(\mathrm{C} # \mathrm{C}) \mathrm{C} # \mathrm{C}$
label: 0.03
What is Lumo value of this molecule?</p>
<h3>Input: C1CC1</h3>
<h3>Response: 0.1"</h3>
<h2>B Training Setup</h2>
<p>To efficiently finetune the LLaMA2-chat-7B, we employed QLoRA (Dettmers et al., 2024) approach. To enhance memory utilization and speed up the training process, we incorporated Deepspeed ZeRO stage 2 (Rasley et al., 2020), FlashAttention-2 (Dao, 2023), and BFloat16 mixed precision techniques. We set the learning rate to $3 \mathrm{e}-4$ and the maximum inputs length to 512 tokens. All models were trained on 4 Tesla A800-80G GPUs and inferenced on 1 RTX 3090 GPU.</p>
<h2>C Detailed Experiment Results</h2>
<h2>C. 1 The robustness of MolecularGPT</h2>
<p>To evaluate the robustness of MolecularGPT across diverse instructional phrasings, we adopt the in-</p>
<p>Table 5: The overview of datasets</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Splitting</th>
<th style="text-align: center;">Data Class</th>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">No. of Molecules</th>
<th style="text-align: center;">No. of Tasks</th>
<th style="text-align: center;">Task Type</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Tuning tasks</td>
<td style="text-align: center;">Bioactivity assay</td>
<td style="text-align: center;">ChEMBL bioassay activity dataset</td>
<td style="text-align: center;">365065</td>
<td style="text-align: center;">1048</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Physico-chemical</td>
<td style="text-align: center;">CHEMBL Property</td>
<td style="text-align: center;">365065</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">Regression</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Quantum mechanical</td>
<td style="text-align: center;">QM9</td>
<td style="text-align: center;">267770</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">Regression</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Pharmacokinetic</td>
<td style="text-align: center;">CYP inhibition</td>
<td style="text-align: center;">16896</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BBBB Blood-brain barrier penetration</td>
<td style="text-align: center;">2039</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Bio-activity</td>
<td style="text-align: center;">MUV PubChem bioAssay</td>
<td style="text-align: center;">93087</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">BACE-1 benchmark set</td>
<td style="text-align: center;">1513</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">HIV replication inhibition</td>
<td style="text-align: center;">41127</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;">Downstream tasks</td>
<td style="text-align: center;">Toxicity</td>
<td style="text-align: center;">Tox21Toxicology in the 21st century</td>
<td style="text-align: center;">7831</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Toxcast</td>
<td style="text-align: center;">8598</td>
<td style="text-align: center;">617</td>
<td style="text-align: center;">Classification</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Physico-chemical</td>
<td style="text-align: center;">ESOL Water solubility</td>
<td style="text-align: center;">1128</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Regression</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">FreeSolv Solvation free energy</td>
<td style="text-align: center;">642</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Regression</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lipo Lipophilicity</td>
<td style="text-align: center;">4200</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Regression</td>
</tr>
</tbody>
</table>
<p>Table 6: The zero-shot inference results under different types of instructions: the original, detailed, expanded, rewritten, and shortened instructions.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Classification (AUC-ROC)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Regression (RMSE)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Instruction type</td>
<td style="text-align: center;">BACE</td>
<td style="text-align: center;">HIV</td>
<td style="text-align: center;">MUV</td>
<td style="text-align: center;">Tox21</td>
<td style="text-align: center;">ToxCast</td>
<td style="text-align: center;">BBBP</td>
<td style="text-align: center;">CYP450</td>
<td style="text-align: center;">ESOL</td>
<td style="text-align: center;">FreeSolv</td>
<td style="text-align: center;">Lipo</td>
</tr>
<tr>
<td style="text-align: center;">Original</td>
<td style="text-align: center;">0.6212</td>
<td style="text-align: center;">0.7128</td>
<td style="text-align: center;">0.6253</td>
<td style="text-align: center;">0.5893</td>
<td style="text-align: center;">0.5669</td>
<td style="text-align: center;">0.6373</td>
<td style="text-align: center;">0.8031</td>
<td style="text-align: center;">1.471</td>
<td style="text-align: center;">4.975</td>
<td style="text-align: center;">1.157</td>
</tr>
<tr>
<td style="text-align: center;">Detailed</td>
<td style="text-align: center;">0.6222</td>
<td style="text-align: center;">0.6754</td>
<td style="text-align: center;">0.6090</td>
<td style="text-align: center;">0.6047</td>
<td style="text-align: center;">0.5710</td>
<td style="text-align: center;">0.6600</td>
<td style="text-align: center;">0.8076</td>
<td style="text-align: center;">1.457</td>
<td style="text-align: center;">5.036</td>
<td style="text-align: center;">1.158</td>
</tr>
<tr>
<td style="text-align: center;">Expanded</td>
<td style="text-align: center;">0.6175</td>
<td style="text-align: center;">0.7134</td>
<td style="text-align: center;">0.6017</td>
<td style="text-align: center;">0.6110</td>
<td style="text-align: center;">0.5688</td>
<td style="text-align: center;">0.6511</td>
<td style="text-align: center;">0.8053</td>
<td style="text-align: center;">1.474</td>
<td style="text-align: center;">5.023</td>
<td style="text-align: center;">1.154</td>
</tr>
<tr>
<td style="text-align: center;">Rewritten</td>
<td style="text-align: center;">0.6351</td>
<td style="text-align: center;">0.6893</td>
<td style="text-align: center;">0.6172</td>
<td style="text-align: center;">0.5955</td>
<td style="text-align: center;">0.5666</td>
<td style="text-align: center;">0.6427</td>
<td style="text-align: center;">0.8050</td>
<td style="text-align: center;">1.457</td>
<td style="text-align: center;">5.018</td>
<td style="text-align: center;">1.157</td>
</tr>
<tr>
<td style="text-align: center;">Shortened</td>
<td style="text-align: center;">0.6409</td>
<td style="text-align: center;">0.6697</td>
<td style="text-align: center;">0.6348</td>
<td style="text-align: center;">0.5924</td>
<td style="text-align: center;">0.5692</td>
<td style="text-align: center;">0.5374</td>
<td style="text-align: center;">0.8032</td>
<td style="text-align: center;">1.462</td>
<td style="text-align: center;">6.258</td>
<td style="text-align: center;">1.158</td>
</tr>
<tr>
<td style="text-align: center;">Standard deviation</td>
<td style="text-align: center;">0.0090</td>
<td style="text-align: center;">0.0183</td>
<td style="text-align: center;">0.0117</td>
<td style="text-align: center;">0.0081</td>
<td style="text-align: center;">0.0016</td>
<td style="text-align: center;">0.0448</td>
<td style="text-align: center;">0.0016</td>
<td style="text-align: center;">0.0071</td>
<td style="text-align: center;">0.4984</td>
<td style="text-align: center;">0.0015</td>
</tr>
</tbody>
</table>
<p>Table 7: The zero- and few-shot performances of model which was fine-tuned on 0 -shot instruction datasets.</p>
<p>| Tasks | | Classification (AUC-ROC) | | | | | | | | Regression (RMSE) | | | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | Method | Type | BACE | HIV | MUV | Tox21 | ToxCast | BBBP | CYP450 | ESOL | FreeSolv | Lipo | | | 0-Shot | 0.6033 | 0.6028 | 0.6010 | 0.5824 | 0.5839 | 0.6521 | 0.7684 | 1.767 | 5.185 | 1.163 | | | 1-Shot | 0.6297 | 0.4671 | 0.5740 | 0.6016 | 0.5886 | 0.6436 | 0.7667 | 1.442 | 5.324 | 1.032 | | | 2-Shot | 0.5903 | 0.4006 | 0.5665 | 0.5956 | 0.5867 | 0.6166 | 0.7556 | 1.438 | 5.482 | 1.053 | | | 3-Shot | 0.5344 | 0.4151 | 0.5705 | 0.5974 | 0.5757 | 0.6032 | 0.7457 | 1.379 | 5.617 | 1.016 | | | 4-Shot | 0.5334 | 0.4393 | 0.5675 | 0.5942 | 0.5828 | 0.6197 | 0.7367 | 1.249 | 5.555 | 1.010 | | | 6-Shot | 0.5314 | 0.3784 | 0.5312 | 0.5843 | 0.5723 | 0.5767 | 0.7374 | 1.241 | 5.961 | 0.979 | | | 8-Shot | 0.4388 | 0.3768 | 0.5637 | 0.5724 | 0.5672 | 0.5187 | 0.7050 | 1.131 | 5.852 | 0.984 | | |</p>
<p>Table 8: The zero- and few-shot performances of model which was fine-tuned on 4-shot instruction datasets.</p>
<p>| Tasks | | Classification (AUC-ROC) | | | | | | | | | Regression (RMSE) | | | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | Method | Type | BACE | HIV | MUV | Tox21 | ToxCast | BBBP | CYP450 | ESOL | FreeSolv | Lipo | | | 0-Shot | 0.5446 | 0.5514 | 0.6406 | 0.5425 | 0.5588 | 0.4709 | 0.6282 | 2.703 | 4.620 | 1.144 | | | 1-Shot | 0.6773 | 0.5135 | 0.6240 | 0.6911 | 0.6140 | 0.6342 | 0.8239 | 1.644 | 5.062 | 1.019 | | | 2-Shot | 0.6860 | 0.5626 | 0.6203 | 0.7053 | 0.6163 | 0.6563 | 0.8420 | 1.278 | 4.942 | 0.949 | | | 3-Shot | 0.7315 | 0.5577 | 0.6269 | 0.7096 | 0.6220 | 0.6533 | 0.8479 | 1.277 | 4.734 | 0.949 | | | 4-Shot | 0.7264 | 0.5624 | 0.6238 | 0.7233 | 0.6243 | 0.6644 | 0.8525 | 1.311 | 4.978 | 0.956 | | | 6-Shot | 0.7294 | 0.5768 | 0.6115 | 0.7339 | 0.6268 | 0.6553 | 0.8523 | 1.284 | 4.941 | 0.974 | | | 8-Shot | 0.7327 | 0.6234 | 0.6079 | 0.7396 | 0.6271 | 0.6430 | 0.8554 | 1.254 | 4.889 | 0.967 | |</p>
<p>struction datasets constructed in GIMLET (Zhao et al., 2024), which utilizes GPT-3.5-turbo to generate four distinct types of instructions based on the original instruction: detailed, expanded, rewritten, and shortened instructions. We present the zero-shot inference results derived from these diverse instructions and compute their ROC-AUC or RMSE standard deviation, as outlined in Tab. 6. Our findings suggest that MolecularGPT exhibits robust performance across different instructional variations.</p>
<h1>C. 2 The effect of instruction datasets</h1>
<p>To find a model with superior zero-shot generalization and ICL capabilities, we assess the performance of models that have been fine-tuned by datasets that employ diverse mixture strategies. These strategies include single 0 -shot instruction, single 4 -shot instruction, combined $0 \&amp; 4$-shot instruction, combined $0,1,2,3,4$-shot ( $0-4$ shot) instruction, and doubled scale of combined $0 \&amp; 4$-shot instruction datasets.</p>
<p>In the combined $0 \&amp; 4$-shot methodology, we merge the 0 -shot and 4 -shot instruction datasets in an equal ratio of $0.5: 0.5$. For the comprehensive $0-4$ shot mix, we integrate the $0,1,2,3$, and 4 -shot instruction datasets in a ratio of $0.6: 0.1: 0.1: 0.1$ : 0.1. During these procedures, we ensure the absence of duplicate query molecules and maintain the scale of the datasets. For the doubled scale of $0 \&amp; 4$-shot, we amalgamate the 0 -shot and 4 -shot instruction datasets in an equal proportion of $1: 1$. The results of the zero- and few-shot inferences are presented in the following Tab. 7, 8, 9, 10 and 11.</p>
<h2>C. 3 The effect of inference strategies</h2>
<p>We examine the efficacy of the order of the demonstrations within instructions. Tab. 12 illustrates the performance of arranging retrieval demonstrations in ascending order. Notably, the phrasing in zeroshot or one-shot instruction is consistent in both ascending and descending order. Consequently, we present the results of 2 -shot and above. Additionally, we examine the efficacy of retrieval based on diversity, comparing it with a strategy that prioritizes similarity, as illustrated in Tab. 13. It's important to note that to ensure an equal distribution of different class samples, evaluating even-numbered shot is essential. Moreover, this strategy is specifically designed for classification tasks, as regression tasks lack distinct classes.</p>
<p>Table 9: The zero- and few-shot performances of model which was fine-tuned on 0\&amp;4-shot instruction datasets.</p>
<p>| Tasks | | Classification (AUC-ROC) | | | | | | | | Regression (RMSE) | | | | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | :--: | | Method | Type | BACE | HIV | MUV | Tox21 | ToxCast | BBBP | CYP450 | ESOL | FreeSolv | Lipo | | | 0-Shot | 0.6568 | 0.6728 | 0.5533 | 0.6067 | 0.5352 | 0.6086 | 0.7931 | 1.377 | 5.376 | 1.208 | | | 1-Shot | 0.7393 | 0.6620 | 0.5954 | 0.6817 | 0.5809 | 0.7087 | 0.8231 | 1.468 | 5.034 | 1.042 | | | 2-Shot | 0.7204 | 0.6485 | 0.5969 | 0.7004 | 0.5863 | 0.7135 | 0.8357 | 1.481 | 4.981 | 1.038 | | 0,4_examples | 3-Shot | 0.7543 | 0.6459 | 0.6139 | 0.6964 | 0.5877 | 0.6997 | 0.8368 | 1.481 | 4.984 | 1.030 | | | 4-Shot | 0.7593 | 0.6363 | 0.6026 | 0.7074 | 0.5938 | 0.7130 | 0.8390 | 1.413 | 5.149 | 1.028 | | | 6-Shot | 0.7574 | 0.6150 | 0.5926 | 0.7156 | 0.5954 | 0.7145 | 0.8438 | 1.427 | 4.928 | 1.047 | | | 8-Shot | 0.7474 | 0.6197 | 0.5942 | 0.7182 | 0.5962 | 0.7029 | 0.8459 | 1.479 | 4.846 | 1.031 | | Table 10: The zero- and few-shot performances of model which was fine-tuned on 0,1,2,3,4-shot instruction datasets. | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>            </div>
        </div>

    </div>
</body>
</html>
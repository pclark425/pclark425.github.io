<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8094 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8094</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8094</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-d6077b167684567cfc35017709872b298a96a92d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d6077b167684567cfc35017709872b298a96a92d" target="_blank">UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work presents UMBRELA (a recursive acronym that stands for UMbrela is the Bing RELevance Assessor), an open-source toolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o model and adds more nuance to the original paper.</p>
                <p><strong>Paper Abstract:</strong> Copious amounts of relevance judgments are necessary for the effective training and accurate evaluation of retrieval systems. Conventionally, these judgments are made by human assessors, rendering this process expensive and laborious. A recent study by Thomas et al. from Microsoft Bing suggested that large language models (LLMs) can accurately perform the relevance assessment task and provide human-quality judgments, but unfortunately their study did not yield any reusable software artifacts. Our work presents UMBRELA (a recursive acronym that stands for UMbrela is the Bing RELevance Assessor), an open-source toolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o model and adds more nuance to the original paper. Across Deep Learning Tracks from TREC 2019 to 2023, we find that LLM-derived relevance judgments correlate highly with rankings generated by effective multi-stage retrieval systems. Our toolkit is designed to be easily extensible and can be integrated into existing multi-stage retrieval and evaluation pipelines, offering researchers a valuable resource for studying retrieval evaluation methodologies. UMBRELA will be used in the TREC 2024 RAG Track to aid in relevance assessments, and we envision our toolkit becoming a foundation for further innovation in the field. UMBRELA is available at https://github.com/castorini/umbrela.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8094.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8094.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UMBRELA LLM-vs-Human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UMBRELA (LLM-based relevance assessments) compared to NIST human qrels</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper evaluates GPT-4o (via the UMBRELA toolkit) as an automatic relevance judge against NIST human assessors on TREC Deep Learning tracks (2019-2023), finding fair label-level agreement but very high system-ranking agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>passage relevance assessment / retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Tracks 2019-2023</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o accessed via Microsoft Azure; zero-shot DNA prompting; temperature=0, top_p=1, frequency_penalty=0.5, presence_penalty=0.0</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST trained human assessors (TREC qrels)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (four-scale & binary); Kendall's tau; Spearman's rho; per-label accuracy (confusion matrix)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>reduced accuracy on 'related'/'highly relevant'/'perfectly relevant' labels; label-level misclassifications (e.g., human label 3 predicted as 0); sensitivity to ambiguous queries; lower fine-grained nuance compared to humans on certain items</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>LLM judgments align well with system-level rankings despite label-level disagreements; LLM often predicts non-relevant correctly but struggles more with mid/high relevance classes; in some ambiguous cases LLM judgments appear more appropriate than human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>scalability and cost-efficiency for large-scale relevance labelling; ability to fill missing judgments and augment incomplete qrels; produces judgments that preserve retrieval-system ranking order (high correlation).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot DNA prompt (descriptive, narrative, aspects) with explicit scoring output '##final score: score'; GPT-4o via Azure; temperature=0, top_p=1, frequency_penalty=0.5; evaluations used nDCG@10 for system comparisons; near-duplicate passages removed for DL2022/2023.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8094.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cohen_kappa_DL2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cohen's kappa (four-scale) — TREC DL 2019</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four-scale Cohen's kappa measuring label-level agreement between GPT-4o judgments and NIST human qrels for TREC DL 2019.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>passage relevance assessment / retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2019</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting; temperature=0, top_p=1</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (four-scale)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.3613</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>label-level disagreements leading to only fair kappa agreement</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Label-level agreement is fair despite good system-level correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>consistent, automated labeling</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot DNA prompt; label scale 0-3; direct comparison to human qrels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8094.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cohen_kappa_DL2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cohen's kappa (four-scale) — TREC DL 2020</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four-scale Cohen's kappa measuring label-level agreement between GPT-4o and NIST qrels for TREC DL 2020.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>passage relevance assessment / retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2020</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting; temperature=0, top_p=1</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (four-scale)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.3506</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>fair label-level agreement</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Similar fair agreement as other years.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>automated scalability</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot DNA prompt; direct label comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8094.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cohen_kappa_DL2021</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cohen's kappa (four-scale) — TREC DL 2021</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four-scale Cohen's kappa measuring label-level agreement between GPT-4o and NIST qrels for TREC DL 2021.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>passage relevance assessment / retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2021</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting; temperature=0, top_p=1</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (four-scale)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.373</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>fair label-level agreement</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Slightly higher kappa than some other years but still in 'fair' range.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>consistent automated labeling</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot DNA prompt; label scale 0-3.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8094.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cohen_kappa_DL2022_star</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cohen's kappa (four-scale) — TREC DL 2022 (after de-duplication)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four-scale Cohen's kappa between GPT-4o and NIST qrels for TREC DL 2022 after removing near-duplicate passages.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>passage relevance assessment / retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2022 (deduplicated)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting; temperature=0, top_p=1</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (four-scale)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.3362</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>fair agreement; complications from original qrels having propagated labels for duplicates</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>De-duplication applied prior to comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>helps fill gaps when duplicates are removed</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Near-duplicate removal before comparing judgments; zero-shot DNA prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8094.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cohen_kappa_DL2023_star</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cohen's kappa (four-scale) — TREC DL 2023 (after de-duplication)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Four-scale Cohen's kappa between GPT-4o and NIST qrels for TREC DL 2023 after de-duplication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>passage relevance assessment / retrieval evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2023 (deduplicated)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting; temperature=0, top_p=1</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's kappa (four-scale)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.3081</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>fair to low label-level agreement</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Lower kappa compared to some earlier tracks.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>supports large-scale automatic assessments</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>De-duplication applied; zero-shot DNA prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8094.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kendall_tau_system_corr_DL2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kendall's tau (system-level nDCG@10 correlation) — TREC DL 2019</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Kendall's tau between system rankings evaluated with human qrels and system rankings evaluated with GPT-4o judgments (nDCG@10) for DL2019.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>system ranking correlation (nDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2019</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau (system-level correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.8926</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>N/A at system-level (high agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Very high agreement in system rankings despite label-level differences.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>preserves relative system ordering enabling reliable evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>nDCG@10 used to compare system rankings; Kendall's tau reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8094.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kendall_tau_system_corr_DL2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kendall's tau (system-level nDCG@10 correlation) — TREC DL 2020</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Kendall's tau between system rankings under human and GPT-4o judgments for DL2020.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>system ranking correlation (nDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2020</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau (system-level correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.9435</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>N/A (high correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Extremely high system-level agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>reliable for system comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>nDCG@10; Kendall's tau reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8094.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kendall_tau_system_corr_DL2021</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kendall's tau (system-level nDCG@10 correlation) — TREC DL 2021</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Kendall's tau between system rankings under human and GPT-4o judgments for DL2021.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>system ranking correlation (nDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2021</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau (system-level correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.9343</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>N/A (high correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Very high correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>useful for system ranking comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>nDCG@10; Kendall's tau computed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8094.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kendall_tau_system_corr_DL2022_star</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kendall's tau (system-level nDCG@10 correlation) — TREC DL 2022 (deduplicated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Kendall's tau between system rankings using human qrels and GPT-4o judgments for DL2022 after de-duplication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>system ranking correlation (nDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2022 (deduplicated)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau (system-level correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.8728</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>N/A (still high correlation though slightly lower)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>High but slightly lower system-level agreement after de-duplication.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>robust across dataset preprocessing choices.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Near-duplicate removal applied prior to evaluation; nDCG@10 used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8094.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kendall_tau_system_corr_DL2023_star</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Kendall's tau (system-level nDCG@10 correlation) — TREC DL 2023 (deduplicated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Kendall's tau between system rankings with human qrels and GPT-4o judgments for DL2023 after de-duplication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>system ranking correlation (nDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2023 (deduplicated)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Kendall's tau (system-level correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.9107</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>N/A (high correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>High correlation maintained.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>consistent system-level evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>nDCG@10; Kendall's tau calculated.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8094.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spearman_rho_system_corr_DL2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spearman's rho (system-level nDCG@10 correlation) — TREC DL 2019</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Spearman's rho between system rankings evaluated with human qrels and GPT-4o judgments (nDCG@10) for DL2019.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>system ranking correlation (nDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2019</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho (system-level correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.9736</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>N/A (very high agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Spearman shows near-perfect ordering agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>preserves rank order.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>nDCG@10; Spearman correlation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8094.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spearman_rho_system_corr_DL2020</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spearman's rho (system-level nDCG@10 correlation) — TREC DL 2020</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Spearman's rho between system rankings under human and GPT-4o judgments for DL2020.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>system ranking correlation (nDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2020</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho (system-level correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.9923</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>N/A (very high)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Almost perfect rank correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>reliable for system ranking comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>nDCG@10; Spearman reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8094.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spearman_rho_system_corr_DL2021</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spearman's rho (system-level nDCG@10 correlation) — TREC DL 2021</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Spearman's rho between system rankings under human and GPT-4o judgments for DL2021.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>system ranking correlation (nDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2021</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho (system-level correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.9915</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>N/A (very high)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Very high rank correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>suitable for evaluating retrieval systems.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>nDCG@10; Spearman computed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e8094.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spearman_rho_system_corr_DL2022_star</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spearman's rho (system-level nDCG@10 correlation) — TREC DL 2022 (deduplicated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Spearman's rho between system rankings with human qrels and GPT-4o judgments for DL2022 after de-duplication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>system ranking correlation (nDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2022 (deduplicated)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho (system-level correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.9729</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>N/A (still high)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>High system-level agreement retained after preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>robust across preprocessing choices.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Near-duplicate removal; nDCG@10; Spearman reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e8094.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spearman_rho_system_corr_DL2023_star</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spearman's rho (system-level nDCG@10 correlation) — TREC DL 2023 (deduplicated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Spearman's rho between system rankings with human qrels and GPT-4o judgments for DL2023 after de-duplication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>system ranking correlation (nDCG@10)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Track 2023 (deduplicated)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman's rho (system-level correlation)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.9857</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>N/A (very high)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Very high rank-order agreement observed.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>effective for system evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>nDCG@10; Spearman correlation used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8094.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e8094.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Confusion_matrix_label_accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Per-label accuracy from confusion matrices (LLM vs human)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Summary of label-wise accuracies reported from confusion matrices comparing GPT-4o labels to human labels across the five tracks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>passage relevance assessment (label-level accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TREC Deep Learning Tracks 2019-2023 (aggregated)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>OpenAI GPT-4o via Azure; zero-shot DNA prompting</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>NIST human assessors</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>per-label accuracy from confusion matrices</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>substantially lower accuracy on related/highly relevant/perfectly relevant classes compared to non-relevant</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Across datasets LLM predicted non-relevant labels with roughly 75% accuracy; 'related' ≈50%; 'highly relevant' ≈30%; 'perfectly relevant' ≈45%.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>strong detection of non-relevant items</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Confusion matrices computed comparing predicted labels to human qrels; percentages reported as rough averages across tracks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large Language Models Can Accurately Predict Searcher Preferences <em>(Rating: 2)</em></li>
                <li>Perspectives on Large Language Models for Relevance Judgment <em>(Rating: 2)</em></li>
                <li>One-Shot Labeling for Automatic Relevance Estimation <em>(Rating: 1)</em></li>
                <li>LLMs Can Patch Up Missing Relevance Judgments in Evaluation <em>(Rating: 2)</em></li>
                <li>Large Language Models are Zero-Shot Reasoners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8094",
    "paper_id": "paper-d6077b167684567cfc35017709872b298a96a92d",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "UMBRELA LLM-vs-Human",
            "name_full": "UMBRELA (LLM-based relevance assessments) compared to NIST human qrels",
            "brief_description": "This paper evaluates GPT-4o (via the UMBRELA toolkit) as an automatic relevance judge against NIST human assessors on TREC Deep Learning tracks (2019-2023), finding fair label-level agreement but very high system-ranking agreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "passage relevance assessment / retrieval evaluation",
            "dataset_name": "TREC Deep Learning Tracks 2019-2023",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o accessed via Microsoft Azure; zero-shot DNA prompting; temperature=0, top_p=1, frequency_penalty=0.5, presence_penalty=0.0",
            "human_evaluator_type": "NIST trained human assessors (TREC qrels)",
            "agreement_metric": "Cohen's kappa (four-scale & binary); Kendall's tau; Spearman's rho; per-label accuracy (confusion matrix)",
            "agreement_score": null,
            "reported_loss_aspects": "reduced accuracy on 'related'/'highly relevant'/'perfectly relevant' labels; label-level misclassifications (e.g., human label 3 predicted as 0); sensitivity to ambiguous queries; lower fine-grained nuance compared to humans on certain items",
            "qualitative_findings": "LLM judgments align well with system-level rankings despite label-level disagreements; LLM often predicts non-relevant correctly but struggles more with mid/high relevance classes; in some ambiguous cases LLM judgments appear more appropriate than human labels.",
            "advantages_of_llm_judge": "scalability and cost-efficiency for large-scale relevance labelling; ability to fill missing judgments and augment incomplete qrels; produces judgments that preserve retrieval-system ranking order (high correlation).",
            "experimental_setting": "Zero-shot DNA prompt (descriptive, narrative, aspects) with explicit scoring output '##final score: score'; GPT-4o via Azure; temperature=0, top_p=1, frequency_penalty=0.5; evaluations used nDCG@10 for system comparisons; near-duplicate passages removed for DL2022/2023.",
            "uuid": "e8094.0",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Cohen_kappa_DL2019",
            "name_full": "Cohen's kappa (four-scale) — TREC DL 2019",
            "brief_description": "Four-scale Cohen's kappa measuring label-level agreement between GPT-4o judgments and NIST human qrels for TREC DL 2019.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "passage relevance assessment / retrieval evaluation",
            "dataset_name": "TREC Deep Learning Track 2019",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting; temperature=0, top_p=1",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Cohen's kappa (four-scale)",
            "agreement_score": 0.3613,
            "reported_loss_aspects": "label-level disagreements leading to only fair kappa agreement",
            "qualitative_findings": "Label-level agreement is fair despite good system-level correlation.",
            "advantages_of_llm_judge": "consistent, automated labeling",
            "experimental_setting": "Zero-shot DNA prompt; label scale 0-3; direct comparison to human qrels.",
            "uuid": "e8094.1",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Cohen_kappa_DL2020",
            "name_full": "Cohen's kappa (four-scale) — TREC DL 2020",
            "brief_description": "Four-scale Cohen's kappa measuring label-level agreement between GPT-4o and NIST qrels for TREC DL 2020.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "passage relevance assessment / retrieval evaluation",
            "dataset_name": "TREC Deep Learning Track 2020",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting; temperature=0, top_p=1",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Cohen's kappa (four-scale)",
            "agreement_score": 0.3506,
            "reported_loss_aspects": "fair label-level agreement",
            "qualitative_findings": "Similar fair agreement as other years.",
            "advantages_of_llm_judge": "automated scalability",
            "experimental_setting": "Zero-shot DNA prompt; direct label comparison.",
            "uuid": "e8094.2",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Cohen_kappa_DL2021",
            "name_full": "Cohen's kappa (four-scale) — TREC DL 2021",
            "brief_description": "Four-scale Cohen's kappa measuring label-level agreement between GPT-4o and NIST qrels for TREC DL 2021.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "passage relevance assessment / retrieval evaluation",
            "dataset_name": "TREC Deep Learning Track 2021",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting; temperature=0, top_p=1",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Cohen's kappa (four-scale)",
            "agreement_score": 0.373,
            "reported_loss_aspects": "fair label-level agreement",
            "qualitative_findings": "Slightly higher kappa than some other years but still in 'fair' range.",
            "advantages_of_llm_judge": "consistent automated labeling",
            "experimental_setting": "Zero-shot DNA prompt; label scale 0-3.",
            "uuid": "e8094.3",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Cohen_kappa_DL2022_star",
            "name_full": "Cohen's kappa (four-scale) — TREC DL 2022 (after de-duplication)",
            "brief_description": "Four-scale Cohen's kappa between GPT-4o and NIST qrels for TREC DL 2022 after removing near-duplicate passages.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "passage relevance assessment / retrieval evaluation",
            "dataset_name": "TREC Deep Learning Track 2022 (deduplicated)",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting; temperature=0, top_p=1",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Cohen's kappa (four-scale)",
            "agreement_score": 0.3362,
            "reported_loss_aspects": "fair agreement; complications from original qrels having propagated labels for duplicates",
            "qualitative_findings": "De-duplication applied prior to comparison.",
            "advantages_of_llm_judge": "helps fill gaps when duplicates are removed",
            "experimental_setting": "Near-duplicate removal before comparing judgments; zero-shot DNA prompting.",
            "uuid": "e8094.4",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Cohen_kappa_DL2023_star",
            "name_full": "Cohen's kappa (four-scale) — TREC DL 2023 (after de-duplication)",
            "brief_description": "Four-scale Cohen's kappa between GPT-4o and NIST qrels for TREC DL 2023 after de-duplication.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "passage relevance assessment / retrieval evaluation",
            "dataset_name": "TREC Deep Learning Track 2023 (deduplicated)",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting; temperature=0, top_p=1",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Cohen's kappa (four-scale)",
            "agreement_score": 0.3081,
            "reported_loss_aspects": "fair to low label-level agreement",
            "qualitative_findings": "Lower kappa compared to some earlier tracks.",
            "advantages_of_llm_judge": "supports large-scale automatic assessments",
            "experimental_setting": "De-duplication applied; zero-shot DNA prompt.",
            "uuid": "e8094.5",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Kendall_tau_system_corr_DL2019",
            "name_full": "Kendall's tau (system-level nDCG@10 correlation) — TREC DL 2019",
            "brief_description": "Kendall's tau between system rankings evaluated with human qrels and system rankings evaluated with GPT-4o judgments (nDCG@10) for DL2019.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "system ranking correlation (nDCG@10)",
            "dataset_name": "TREC Deep Learning Track 2019",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Kendall's tau (system-level correlation)",
            "agreement_score": 0.8926,
            "reported_loss_aspects": "N/A at system-level (high agreement)",
            "qualitative_findings": "Very high agreement in system rankings despite label-level differences.",
            "advantages_of_llm_judge": "preserves relative system ordering enabling reliable evaluations.",
            "experimental_setting": "nDCG@10 used to compare system rankings; Kendall's tau reported.",
            "uuid": "e8094.6",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Kendall_tau_system_corr_DL2020",
            "name_full": "Kendall's tau (system-level nDCG@10 correlation) — TREC DL 2020",
            "brief_description": "Kendall's tau between system rankings under human and GPT-4o judgments for DL2020.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "system ranking correlation (nDCG@10)",
            "dataset_name": "TREC Deep Learning Track 2020",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Kendall's tau (system-level correlation)",
            "agreement_score": 0.9435,
            "reported_loss_aspects": "N/A (high correlation)",
            "qualitative_findings": "Extremely high system-level agreement.",
            "advantages_of_llm_judge": "reliable for system comparisons.",
            "experimental_setting": "nDCG@10; Kendall's tau reported.",
            "uuid": "e8094.7",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Kendall_tau_system_corr_DL2021",
            "name_full": "Kendall's tau (system-level nDCG@10 correlation) — TREC DL 2021",
            "brief_description": "Kendall's tau between system rankings under human and GPT-4o judgments for DL2021.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "system ranking correlation (nDCG@10)",
            "dataset_name": "TREC Deep Learning Track 2021",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Kendall's tau (system-level correlation)",
            "agreement_score": 0.9343,
            "reported_loss_aspects": "N/A (high correlation)",
            "qualitative_findings": "Very high correlation.",
            "advantages_of_llm_judge": "useful for system ranking comparisons.",
            "experimental_setting": "nDCG@10; Kendall's tau computed.",
            "uuid": "e8094.8",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Kendall_tau_system_corr_DL2022_star",
            "name_full": "Kendall's tau (system-level nDCG@10 correlation) — TREC DL 2022 (deduplicated)",
            "brief_description": "Kendall's tau between system rankings using human qrels and GPT-4o judgments for DL2022 after de-duplication.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "system ranking correlation (nDCG@10)",
            "dataset_name": "TREC Deep Learning Track 2022 (deduplicated)",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Kendall's tau (system-level correlation)",
            "agreement_score": 0.8728,
            "reported_loss_aspects": "N/A (still high correlation though slightly lower)",
            "qualitative_findings": "High but slightly lower system-level agreement after de-duplication.",
            "advantages_of_llm_judge": "robust across dataset preprocessing choices.",
            "experimental_setting": "Near-duplicate removal applied prior to evaluation; nDCG@10 used.",
            "uuid": "e8094.9",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Kendall_tau_system_corr_DL2023_star",
            "name_full": "Kendall's tau (system-level nDCG@10 correlation) — TREC DL 2023 (deduplicated)",
            "brief_description": "Kendall's tau between system rankings with human qrels and GPT-4o judgments for DL2023 after de-duplication.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "system ranking correlation (nDCG@10)",
            "dataset_name": "TREC Deep Learning Track 2023 (deduplicated)",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Kendall's tau (system-level correlation)",
            "agreement_score": 0.9107,
            "reported_loss_aspects": "N/A (high correlation)",
            "qualitative_findings": "High correlation maintained.",
            "advantages_of_llm_judge": "consistent system-level evaluation.",
            "experimental_setting": "nDCG@10; Kendall's tau calculated.",
            "uuid": "e8094.10",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Spearman_rho_system_corr_DL2019",
            "name_full": "Spearman's rho (system-level nDCG@10 correlation) — TREC DL 2019",
            "brief_description": "Spearman's rho between system rankings evaluated with human qrels and GPT-4o judgments (nDCG@10) for DL2019.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "system ranking correlation (nDCG@10)",
            "dataset_name": "TREC Deep Learning Track 2019",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Spearman's rho (system-level correlation)",
            "agreement_score": 0.9736,
            "reported_loss_aspects": "N/A (very high agreement)",
            "qualitative_findings": "Spearman shows near-perfect ordering agreement.",
            "advantages_of_llm_judge": "preserves rank order.",
            "experimental_setting": "nDCG@10; Spearman correlation reported.",
            "uuid": "e8094.11",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Spearman_rho_system_corr_DL2020",
            "name_full": "Spearman's rho (system-level nDCG@10 correlation) — TREC DL 2020",
            "brief_description": "Spearman's rho between system rankings under human and GPT-4o judgments for DL2020.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "system ranking correlation (nDCG@10)",
            "dataset_name": "TREC Deep Learning Track 2020",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Spearman's rho (system-level correlation)",
            "agreement_score": 0.9923,
            "reported_loss_aspects": "N/A (very high)",
            "qualitative_findings": "Almost perfect rank correlation.",
            "advantages_of_llm_judge": "reliable for system ranking comparisons.",
            "experimental_setting": "nDCG@10; Spearman reported.",
            "uuid": "e8094.12",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Spearman_rho_system_corr_DL2021",
            "name_full": "Spearman's rho (system-level nDCG@10 correlation) — TREC DL 2021",
            "brief_description": "Spearman's rho between system rankings under human and GPT-4o judgments for DL2021.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "system ranking correlation (nDCG@10)",
            "dataset_name": "TREC Deep Learning Track 2021",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Spearman's rho (system-level correlation)",
            "agreement_score": 0.9915,
            "reported_loss_aspects": "N/A (very high)",
            "qualitative_findings": "Very high rank correlation.",
            "advantages_of_llm_judge": "suitable for evaluating retrieval systems.",
            "experimental_setting": "nDCG@10; Spearman computed.",
            "uuid": "e8094.13",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Spearman_rho_system_corr_DL2022_star",
            "name_full": "Spearman's rho (system-level nDCG@10 correlation) — TREC DL 2022 (deduplicated)",
            "brief_description": "Spearman's rho between system rankings with human qrels and GPT-4o judgments for DL2022 after de-duplication.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "system ranking correlation (nDCG@10)",
            "dataset_name": "TREC Deep Learning Track 2022 (deduplicated)",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Spearman's rho (system-level correlation)",
            "agreement_score": 0.9729,
            "reported_loss_aspects": "N/A (still high)",
            "qualitative_findings": "High system-level agreement retained after preprocessing.",
            "advantages_of_llm_judge": "robust across preprocessing choices.",
            "experimental_setting": "Near-duplicate removal; nDCG@10; Spearman reported.",
            "uuid": "e8094.14",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Spearman_rho_system_corr_DL2023_star",
            "name_full": "Spearman's rho (system-level nDCG@10 correlation) — TREC DL 2023 (deduplicated)",
            "brief_description": "Spearman's rho between system rankings with human qrels and GPT-4o judgments for DL2023 after de-duplication.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "system ranking correlation (nDCG@10)",
            "dataset_name": "TREC Deep Learning Track 2023 (deduplicated)",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "Spearman's rho (system-level correlation)",
            "agreement_score": 0.9857,
            "reported_loss_aspects": "N/A (very high)",
            "qualitative_findings": "Very high rank-order agreement observed.",
            "advantages_of_llm_judge": "effective for system evaluation.",
            "experimental_setting": "nDCG@10; Spearman correlation used.",
            "uuid": "e8094.15",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Confusion_matrix_label_accuracy",
            "name_full": "Per-label accuracy from confusion matrices (LLM vs human)",
            "brief_description": "Summary of label-wise accuracies reported from confusion matrices comparing GPT-4o labels to human labels across the five tracks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
            "evaluation_task": "passage relevance assessment (label-level accuracy)",
            "dataset_name": "TREC Deep Learning Tracks 2019-2023 (aggregated)",
            "judge_model_name": "GPT-4o",
            "judge_model_details": "OpenAI GPT-4o via Azure; zero-shot DNA prompting",
            "human_evaluator_type": "NIST human assessors",
            "agreement_metric": "per-label accuracy from confusion matrices",
            "agreement_score": null,
            "reported_loss_aspects": "substantially lower accuracy on related/highly relevant/perfectly relevant classes compared to non-relevant",
            "qualitative_findings": "Across datasets LLM predicted non-relevant labels with roughly 75% accuracy; 'related' ≈50%; 'highly relevant' ≈30%; 'perfectly relevant' ≈45%.",
            "advantages_of_llm_judge": "strong detection of non-relevant items",
            "experimental_setting": "Confusion matrices computed comparing predicted labels to human qrels; percentages reported as rough averages across tracks.",
            "uuid": "e8094.16",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large Language Models Can Accurately Predict Searcher Preferences",
            "rating": 2
        },
        {
            "paper_title": "Perspectives on Large Language Models for Relevance Judgment",
            "rating": 2
        },
        {
            "paper_title": "One-Shot Labeling for Automatic Relevance Estimation",
            "rating": 1
        },
        {
            "paper_title": "LLMs Can Patch Up Missing Relevance Judgments in Evaluation",
            "rating": 2
        },
        {
            "paper_title": "Large Language Models are Zero-Shot Reasoners",
            "rating": 1
        }
    ],
    "cost": 0.01788125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</h1>
<p>Shivani Upadhyay<br>sjupadhyay@uwaterloo.ca University of Waterloo Waterloo, Canada</p>
<p>Ronak Pradeep<br>rpradeep@uwaterloo.ca<br>University of Waterloo<br>Waterloo, Canada</p>
<p>Nandan Thakur<br>nandan.thakur@uwaterloo.ca<br>University of Waterloo<br>Waterloo, Canada</p>
<p>Nick Craswell<br>nickcr@microsoft.com<br>Microsoft<br>Seattle, USA</p>
<p>Jimmy Lin<br>jimmylin@uwaterloo.ca<br>University of Waterloo<br>Waterloo, Canada</p>
<h4>Abstract</h4>
<p>Copious amounts of relevance judgments are necessary for the effective training and accurate evaluation of retrieval systems. Conventionally, these judgments are made by human assessors, rendering this process expensive and laborious. A recent study by Thomas et al. from Microsoft Bing suggested that large language models (LLMs) can accurately perform the relevance assessment task and provide human-quality judgments, but unfortunately their study did not yield any reusable software artifacts. Our work presents UMBRELA (a recursive acronym that stands for UMbrela is the Bing RELevance Assessor), an open-source toolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o model and adds more nuance to the original paper. Across Deep Learning Tracks from TREC 2019 to 2023, we find that LLMderived relevance judgments correlate highly with rankings generated by effective multi-stage retrieval systems. Our toolkit is designed to be easily extensible and can be integrated into existing multi-stage retrieval and evaluation pipelines, offering researchers a valuable resource for studying retrieval evaluation methodologies. UMBRELA will be used in the TREC 2024 RAG Track to aid in relevance assessments, and we envision our toolkit becoming a foundation for further innovation in the field. UMBRELA is available at https://github.com/castorini/umbrela.</p>
<h2>1 INTRODUCTION</h2>
<p>Accurate relevance labels are crucial for training and evaluating retrieval systems. Typically, these labels are assigned by trained human assessors, for example, retired intelligence analysts in the case of TREC or editors hired by a web search engine company. Given a search query, a set of results, and a description of the information need, these humans assessors determine the relevance of those results. However, human assessments can be time-consuming, laborintensive, and costly. Even after allocating these resources, assessments often remain inaccurate and misaligned with the searcher's intentions, particularly in the case where the information need did not arise from the assessor. They may fail to grasp the intent behind a search, resulting in flawed relevance judgments.</p>
<p>The emergence of large language models (LLMs) such as GPT4 [17] and Gemini [20] has presented a distinctive opportunity to automate the intricate task of data annotation [1, 3, 12, 13, 15, 19, 22, 23]. Thomas et al. [21] showcased capabilities of LLMs to understand searcher intent and to assign relevant labels as "accurately"</p>
<h2>as human assessors. These models have the potential to serve as an efficient and cost-effective alternative for manual relevance assessment. Moreover, LLMs may actually "outperform" assessors with limited knowledge, who might not fully grasp the search intent, thereby leading to imprecise relevance judgments. In addition, relevance judgments facilitated by LLMs can be employed to "fill holes" left by incomplete judgments, thereby contributing to a more thorough and accurate evaluation of retrieval systems [16, 24].</h2>
<p>In this paper, we reproduced the work of Thomas et al. [21] and repackage our efforts into UMBRELA, an open-source toolkit for using LLMs as relevance assessors. UMbrela is a recursive acronym that stands for UMbrela is the Bing RELevance Assessor. In our study, we carefully follow the zero-shot DNA (Descriptive, Narrative, and Aspects) prompting technique described by Kojima et al. [14] to reproduce the original setup as faithfully as possible.</p>
<p>Our experiments with the TREC Deep Learning (DL) Tracks from 2019-2023 [4-8] verified the claims by Thomas et al. [21] about LLM assessment being an effective alternative to humanbased assessment, using OpenAI's latest model GPT-4o. We have expanded our validation study to include correlation statistics that compare the effectiveness of retrieval systems based on human and LLM judgments. High correlations affirm the effectiveness of LLMs as relevance assessors. As highlighted by Thomas et al. [21], LLMs possess the ability to understand user needs and assign relevance labels with veracity comparable to human assessors.</p>
<p>At a high level, UMBRELA takes a query and a set of passages as input, and based on the LLM configuration, attempts to "understand" the search intent and labels the passages with different relevance grades. Our main contributions are as follows:</p>
<ul>
<li>We reproduced results from Thomas et al. [21], which lacked reusable software components, and created UMBRELA, an opensource toolkit that we share with the broader community.</li>
<li>Our tool will be used in the TREC 2024 RAG Track ${ }^{1}$ to showcase its practical utility for automatic relevance assessment. Through this deployment, we hope to contribute to advances in retrieval evaluation methodologies.</li>
</ul>
<h2>2 RELATED WORK</h2>
<p>The substantial effort required for manual test data preparation in large-scale testing of retrieval systems has consistently encouraged</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">qrels</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Total counts for</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">track</td>
<td style="text-align: center;">runs</td>
<td style="text-align: center;">topics</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">label</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{1}$</td>
<td style="text-align: center;">$\mathbf{2}$</td>
<td style="text-align: center;">$\mathbf{3}$</td>
</tr>
<tr>
<td style="text-align: left;">DL 2019</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">5158</td>
<td style="text-align: center;">1601</td>
<td style="text-align: center;">1804</td>
<td style="text-align: center;">697</td>
</tr>
<tr>
<td style="text-align: left;">DL 2020</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">7780</td>
<td style="text-align: center;">1940</td>
<td style="text-align: center;">1020</td>
<td style="text-align: center;">646</td>
</tr>
<tr>
<td style="text-align: left;">DL 2021</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">4338</td>
<td style="text-align: center;">3063</td>
<td style="text-align: center;">2341</td>
<td style="text-align: center;">1086</td>
</tr>
<tr>
<td style="text-align: left;">DL 2022</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">286459</td>
<td style="text-align: center;">52218</td>
<td style="text-align: center;">46080</td>
<td style="text-align: center;">1659</td>
</tr>
<tr>
<td style="text-align: left;">DL 2022*</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">12892</td>
<td style="text-align: center;">6192</td>
<td style="text-align: center;">3053</td>
<td style="text-align: center;">1385</td>
</tr>
<tr>
<td style="text-align: left;">DL 2023</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">13866</td>
<td style="text-align: center;">4372</td>
<td style="text-align: center;">2259</td>
<td style="text-align: center;">1830</td>
</tr>
<tr>
<td style="text-align: left;">DL 2023*</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">11618</td>
<td style="text-align: center;">3774</td>
<td style="text-align: center;">1942</td>
<td style="text-align: center;">1544</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary statistics for the Deep Learning Tracks from TREC 2019-2023. The total count of topics represents only those that have qrels; * represents counts after applying the pre-processing step to remove near-duplicate passages, as explained in Section 3.1.
researchers to develop automated techniques [9, 10]. In previous work, automated IR evaluations using LLMs have combined various prompting techniques such as zero-shot, one-shot, or fewshot learning. By providing detailed instruction, defining roles and multiple judges, researchers have showcased a continuous increase in the alignment between human and the predicted assessments [11, 16, 21, 24]. LLMs, when provided with proper guidance, can effectively "understand" the relation between a given query and passage and accurately determine its relevance.</p>
<p>In the domain of text annotations in general, both proprietary and open-source LLMs have shown remarkable effectiveness [1, 2, $13,18,25]$. The enhanced capabilities of LLMs and various prompting techniques have presented a strong substitute for manual text annotations.</p>
<h2>3 METHODOLOGY</h2>
<p>The main idea behind UMBRELA is to leverage LLMs for relevance assessment, reproducing the work of Thomas et al. [21].</p>
<h3>3.1 DL TREC Judgments</h3>
<p>To demonstrate LLM capabilities for generating relevance assessment, we took human relevance judgments (also known as qrels) from the Deep Learning Tracks in TREC 2019-2023 [4-8] and reassessed them with UMBRELA. These judgments were originally provided by human NIST assessors who were given the query and the following relevance scale [7]:</p>
<p>0 Irrelevant: The passage has nothing to do with the query
1 Related: The passage seems related to the query but does not answer it
2 Highly relevant: The passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information
3 Perfectly relevant: The passage is dedicated to the query and contains the exact answer
Based on their understanding and general knowledge, the human assessors assigned relevance labels to passages. Thus, we consider these as "gold" labels. See Table 1 for summary statistics.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Given</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">query</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">passage</span>,<span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">must</span><span class="w"> </span><span class="nv">provide</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">score</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">an</span>
<span class="nv">integer</span><span class="w"> </span><span class="nv">scale</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">following</span><span class="w"> </span><span class="nv">meanings</span>:
<span class="mi">0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">represent</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">passage</span><span class="w"> </span><span class="nv">has</span><span class="w"> </span><span class="nv">nothing</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">query</span>,
<span class="mi">1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">represents</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">passage</span><span class="w"> </span><span class="nv">seems</span><span class="w"> </span><span class="nv">related</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">query</span><span class="w"> </span><span class="nv">but</span>
<span class="nv">does</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">it</span>.
<span class="mi">2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">represents</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">passage</span><span class="w"> </span><span class="nv">has</span><span class="w"> </span><span class="nv">some</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">query</span>,
<span class="nv">but</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">may</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">bit</span><span class="w"> </span><span class="nv">unclear</span>,<span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="nv">hidden</span><span class="w"> </span><span class="nv">amongst</span><span class="w"> </span><span class="nv">extraneous</span>
<span class="nv">information</span><span class="w"> </span><span class="nv">and</span>
<span class="mi">3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">represents</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">passage</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">dedicated</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">query</span><span class="w"> </span><span class="nv">and</span>
<span class="nv">contains</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">exact</span><span class="w"> </span><span class="nv">answer</span>.
</code></pre></div>

<p>Important Instruction: Assign category 1 if the passage is somewhat related to the topic but not completely, category 2 if passage presents something very important related to the entire topic but also has some extra information and category 3 if the passage only and entirely refers to the topic. If none of the above satisfies give it category 0.</p>
<p>Query: {query}
Passage: {passage}
Split this problem into steps:
Consider the underlying intent of the search.
Measure how well the content matches a likely intent of the query (M).
Measure how trustworthy the passage is (T).
Consider the aspects above and the relative importance of each, and decide on a final score (0). Final score must be an integer value only.
Do not provide any code in result. Provide each score in the format of: ##final score: score without providing any reasoning.</p>
<p>Figure 1: Prompt used for relevance assessment.
Near-duplicate handling for DL 2022-23. The qrels for the TREC 2022 and 2023 Deep Learning Tracks include relevance label propagation for near-duplicate passages that exist within the corpus. Thus, the number of relevance judgments that were actually assigned by humans is much smaller than the statistics in Table 1 suggest (we provide figures both with and without de-duplication). For the purposes of this study, we have excluded these near-duplicate passages from both the judgments and the submission results prior to conducting our calculations. To accomplish this, we consulted the list of near-duplicate passages compiled by NIST, which provides the near-duplicates in the form of clusters and a "canonical" passage for each cluster. Specifically, we retained only the "canonical" passages; all others were eliminated both from the qrels and the retrieved runs.</p>
<h3>3.2 Prompting Details</h3>
<p>Following the prompting techniques described by Thomas et al. [21], we also utilize the DNA prompting technique. Figure 1 shows the exact prompt used in UMBRELA for relevance assessment.</p>
<p>The DNA prompt consists of three essential sections: descriptive, narrative, and aspects. The descriptive and narrative sections help the LLM understand the user query and the passage that it is supposed to label, whereas the aspects section details the step-by-step procedure for guiding the LLM's "thinking process". This procedure essentially breaks down the relevance labeling task into smaller ones and instructs the LLM to develop a better interpretation of it. The prompt additionally informs the LLM of the expected result structure to be followed.</p>
<p>Following the original paper, we utilize a zero-shot prompting technique for performing our assessments using GPT-40. Providing</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Confusion matrices comparing the original human labels with those generated by the LLM.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">qrels track</th>
<th style="text-align: center;">Cohen κ</th>
<th style="text-align: center;">Correlation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">four-scale</td>
<td style="text-align: center;">binary</td>
<td style="text-align: center;">Kendall <br> ( $\tau$ )</td>
<td style="text-align: center;">Spearman <br> ( $\rho$ )</td>
</tr>
<tr>
<td style="text-align: center;">DL 2019</td>
<td style="text-align: center;">0.3613</td>
<td style="text-align: center;">0.4989</td>
<td style="text-align: center;">0.8926</td>
<td style="text-align: center;">0.9736</td>
</tr>
<tr>
<td style="text-align: center;">DL 2020</td>
<td style="text-align: center;">0.3506</td>
<td style="text-align: center;">0.4496</td>
<td style="text-align: center;">0.9435</td>
<td style="text-align: center;">0.9923</td>
</tr>
<tr>
<td style="text-align: center;">DL 2021</td>
<td style="text-align: center;">0.3730</td>
<td style="text-align: center;">0.4917</td>
<td style="text-align: center;">0.9343</td>
<td style="text-align: center;">0.9915</td>
</tr>
<tr>
<td style="text-align: center;">DL 2022*</td>
<td style="text-align: center;">0.3362</td>
<td style="text-align: center;">0.4217</td>
<td style="text-align: center;">0.8728</td>
<td style="text-align: center;">0.9729</td>
</tr>
<tr>
<td style="text-align: center;">DL 2023*</td>
<td style="text-align: center;">0.3081</td>
<td style="text-align: center;">0.4176</td>
<td style="text-align: center;">0.9107</td>
<td style="text-align: center;">0.9857</td>
</tr>
</tbody>
</table>
<p>Table 2: Cohen $\kappa$ scores (left) and Kendall and Spearman's correlations between TREC submissions evaluated with groundtruth judgments and UMBRELA (right). We use nDCG@10 as the evaluation metric. Here, * represents the use of a pre-processing step to remove near-duplicate passages as explained in Section 3.1.
precise instructions assists the LLM in understanding the semantic relation between the query and the passage and thus assigning accurate labels.</p>
<h2>4 RESULTS</h2>
<p>We performed experiments with OpenAI's latest GPT-4o model (via Microsoft Azure) following the parameter configurations provide in Thomas et al. [21]. Temperature is set to 0 , top $p=1$, without using any stopwords, frequency and presence penalty set as 0.5 and 0 , respectively.</p>
<p>Comparing LLM-based assessments. Table 2 (left) presents Cohen $\kappa$ scores to capture agreement between human and LLM assessments. These agreement scores are demonstrated in two variants, the four-scale form which considers all four relevance labels and the binarized form following Faggioli et al. [11], where non-relevant
(0) and related (1) are merged under "non-relevant" and highly relevant (2) and perfectly relevant (3) are merged under the "relevant" label. These correlation values demonstrate fair agreement between human assessors and LLMs judgments.</p>
<p>Figure 2 further compares human-provided ground-truth labels with LLM-based labels using label-wise confusion matrices for all five tracks. As expected, LLMs are able to assign appropriate labels to the majority of the judgments across all tracks. Across these datasets, the LLM was able to predict non-relevant labels with roughly $75 \%$ accuracy, whereas for labels "related", "highly relevant", and "perfectly relevant", the accuracy drops to roughly $50 \%, 30 \%$, and $45 \%$, respectively (with respect to the human labels).</p>
<p>System evaluation using LLM-based assessments. Table 2 (right) shows the results of Kendall $\tau$ and Spearman $\rho$ correlations between evaluations performed with human ground-truth judgments and judgments provided by the LLM. In all cases, we compared nDCG@10. The high correlations among these judgment evaluations show alignment in the retrieval systems' rankings. Following Faggioli et al. [11], Figure 3 demonstrates the effectiveness of TREC DL runs with scatter plots comparing evaluations made using human and LLM judgments.</p>
<p>Case Study: TREC DL 2019. We further analyzed the discrepancies between LLM judgments and human judgments for TREC DL 2019. As shown in Figure 2(a), we observe a high number of misclassifications in the judgments, predicted as label 0 but where their original (human) label is 3. Based on manual spot-checking, we encountered the following interesting case:</p>
<p>Query: what is the daily life of thai people
Passage: Thai Flag Meaning: The red stripes represent the blood spilt to maintain Thailand's independence. The white stands for purity and is the color of Buddhism which is the country's main</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Scatter plots for comparing evaluations performed using original human assessments and LLM assessments.
religion.roportions: 2:3. Thai Flag Description: The flag of Thailand consists of five horizontal stripes. The top and bottom are equal-sized red stripes, the middle stripe is blue which is lined above and below by equal-sized white stripes. The blue stripe is double the size of the other four.</p>
<p>The passage mainly talks about the Thai flag but the query asks about the "regular life of Thai people". Here the topic seems to be somewhat vague and the pair fails to justify (based on our opinion) the highly relevant label assigned by the human assessors. A few other judgments for this query exhibit similar ambiguity and thus we feel the label 3 to be unwarranted. In this case, the LLM judgment appears to be more accurate.</p>
<p>Consider another case:
Query: medicare's definition of mechanical ventilation
Passage: Continuous Positive Airway Pressure (CPAP) Continuous positive airway pressure - also called CPAP - is a treatment in which a mask is worn over the nose and/or mouth while you sleep. The mask is hooked up to a machine that delivers a continuous flow of air into the nose. This air flow helps keep the airways open so that breathing is regular.</p>
<p>The query requests a definition for "mechanical ventilation", but the passage provides details regarding "Continuous Positive Airway Pressure". In our opinion, the passage fails to define "mechanical ventilation" and thus does not justify the human-assigned label. We notice many judgments similarly failing the classification for this query.</p>
<p>Both the above-mentioned cases demonstrate ambiguity that could have been the result of either inaccurate assessments or incomplete information regarding the user intent.</p>
<h2>5 CONCLUSION</h2>
<p>Continuous efforts are underway to automate the generation of relevance assessments, intending to reduce the cost of manual assessments. In our research, we have successfully replicated the results from Thomas et al. [21], thereby validating the capabilities of OpenAI's latest model, GPT-4o, in performing relevance assessments automatically. Through our experiments with the Deep Learning Tracks from TREC 2019-2023, we have demonstrated the prediction quality of an LLM. Moreover, we have demonstrated significant correlations between human and LLM-assessed judgments by evaluating submitted runs to the TREC evaluations. Finally, we have repackaged this methodology in an open-source tool, referred to as UMBRELA, that we share with the community.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada. We'd also like to thank Microsoft for providing access to OpenAI LLMs on Azure via the Accelerating Foundation Models Research program.</p>
<h2>REFERENCES</h2>
<p>[1] Meysam Alizadeh, Maël Kubli, Zeynab Samei, Shirin Dehghani, Juan Diego Bermeo, Maria Korobeynikovo, and Fabrizio Gilardi. 2023. Open-source Large Language Models Outperform Crowd Workers and Approach ChatGPT in TextAnnotation Tasks. arXiv:2307.02179 [cs.CL]
[2] Meysam Alizadeh, Maël Kubli, Zeynab Samei, Shirin Dehghani, Mohammadmasiha Zahedivafa, Juan Diego Bermeo, Maria Korobeynikova, and Fabrizio Gilardi. 2024. Open-Source LLMs for Text Annotation: A Practical Guide for Model Setting and Fine-Tuning. arXiv:2307.02179 [cs.CL]
[3] Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models be an Alternative to Human Evaluations? 15607-15631.</p>
<p>[4] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 Deep Learning Track. arXiv:2102.07662 [cs.IR]
[5] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Jimmy Lin. 2022. Overview of the TREC 2021 Deep Learning Track. In Text REtrieval Conference (TREC). NIST, TREC.
[6] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff. 2023. Overview of the TREC 2022 Deep Learning Track. In Text REtrieval Conference (TREC). NIST, TREC.
[7] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2020. Overview of the TREC 2019 Deep Learning Track. arXiv:2003.07820 [cs.IR]
[8] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Hossein A. Rahmani, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff. 2024. Overview of the TREC 2023 Deep Learning Track. In Text REtrieval Conference (TREC). NIST, TREC.
[9] Laura Dietz, Shubham Chatterjee, Connor Lennox, Sumanta Kashyapi, Pooja Oza, and Ben Gamari. 2022. Wikimarks: Harvesting Relevance Benchmarks from Wikipedia. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain). Association for Computing Machinery, 3005-3012.
[10] Laura Dietz and Jeff Dalton. 2020. Humans Optional? Automatic Large-Scale Test Collections for Entity, Passage, and Entity-Passage Retrieval. DatenbankSpektrum 20, 1 (2020), 17-28.
[11] Guglielmo Faggioli, Laura Dietz, Charles Clarke, Gianluca Demartini, Matthias Hagen, Claudia Hauff, Noriko Kando, Evangelos Kanoulas, Martin Potthast, Benno Stein, and Henning Wachsmuth. 2023. Perspectives on Large Language Models for Relevance Judgment. arXiv:2304.09161 [cs.IR]
[12] Fabrizio Gilardi, Meysam Alizadeh, and Maél Kubli. 2023. ChatGPT Outperforms Crowd Workers for Text-Annotation Tasks. arXiv:2303.15056 [cs.CL]
[13] Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, and Weizhu Chen. 2024. AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators. arXiv:2303.16854 [cs.CL]
[14] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large Language Models are Zero-Shot Reasoners. arXiv:2205.11916 [cs.CL]
[15] Yang Liu, Dan Iter, Yichong xu amd Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-EVAL: NLG Evaluation Using GPT-4 with Better Human Alignment. arXiv:2303.16634v3 [cs.CL]
[16] Sean MacAvaney and Luca Soldaini. 2023. One-Shot Labeling for Automatic Relevance Estimation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23).
[17] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[18] Jaromir Savelka, Kevin D Ashley, Morgan A Gray, Hannes Westermann, and Huihui Xu. 2023. Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly Specialized Domain Expertise? (2023). arXiv:2306.13906 [cs.CL]
[19] Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. 2024. Large Language Models for Data Annotation: A Survey. arXiv:2402.13446 [cs.CL]
[20] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hasth, Katie Millican, David Silver, et al. 2024. Gemini: A Family of Highly Capable Multimodal Models. arXiv:2312.11805 [cs.CL]
[21] Paul Thomas, Seth Spidman, Nick Craswell, and Bhaskar Mitra. 2024. Large Language Models Can Accurately Predict Searcher Preferences. In 2024 International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM.
[22] Petter Törnberg. 2023. ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning. arXiv:2304.06588 [cs.CL]
[23] Petter Törnberg. 2024. Best Practices for Text Annotation with Large Language Models. arXiv:2402.05129 [cs.CL]
[24] Shivani Upadhyay, Ehsan Kamalloo, and Jimmy Lin. 2024. LLMs Can Patch Up Missing Relevance Judgments in Evaluation. arXiv:2405.04727 [cs.IR]
[25] Yiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui, and Gareth Tyson. 2023. Can ChatGPT Reproduce Human-Generated Labels? a Study of Social Computing Tasks. (2023). arXiv:2304.10145 [cs.AI]</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://cs.uwaterloo.ca/ jimmylin/publications/Lin_etal_TREC2023-planning.pdf&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
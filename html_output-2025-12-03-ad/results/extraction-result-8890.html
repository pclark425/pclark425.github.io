<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8890 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8890</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8890</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-8923aec569a13f94148e3e90a94c68730f6ad03d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8923aec569a13f94148e3e90a94c68730f6ad03d" target="_blank">Searching for High-Value Molecules Using Reinforcement Learning and Transformers</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This work arrives at a new RL-based molecular design algorithm (ChemRLformer) and performs a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations, and shows that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work.</p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs. However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge. Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties. We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations. From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8890.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8890.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemRLformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemRLformer (text-based RL molecular design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-based molecular design system that initializes an autoregressive policy with a pretrained language model (GPT-style transformer or RNN) over molecular texts (SMILES/SELFIES) and fine-tunes it with policy-gradient RL (REINFORCE) plus practical algorithmic components (hill-climb replay buffer, Log P regularizer) to generate high-value molecules for drug-discovery tasks including docking.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChemRLformer (pretrained autoregressive policy fine-tuned with REINFORCE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Autoregressive sequence models (GPT-style transformer and GRU RNN used as policies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Transformer: ~4.78e6 parameters; RNN (3x GRU): ~4.17e6; FC baseline: ~1.07e7 (reported in Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretraining on molecular text datasets (SMILES and converted SELFIES) including ChEMBL (~1.2M), ZINC-250K (~250k), ZINC-1M, ZINC-10M, ZINC-100M; datasets filtered/sanitized and canonicalized with RDKit.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug discovery — de novo ligand generation for protein docking targets (fa7, parp1, 5ht1b, jak2, braf) and a suite of 21 pharmaceutically-relevant oracle tasks (pytdc).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive next-token sampling of SMILES/SELFIES initialized by pretrained language model and fine-tuned with on-policy RL (REINFORCE). Algorithmic augmentations include hill-climb replay buffer, Log P (likelihood) regularizer to avoid premature convergence, optional KL penalty toward the pretrained prior, and use of an augmented docking reward combining docking score, QED and SA for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Novelty measured indirectly: diversity (average pairwise Tanimoto of top-100 molecules) and redundancy (repeated oracle calls). Paper does not report explicit % of molecules absent from training set or exact Tanimoto thresholds for novelty; qualitative findings: SMILES-based agents produce higher top-k scores; SELFIES agents yield higher diversity but also higher redundancy (explore smaller region effectively).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Specificity enforced/evaluated by optimizing final-molecule reward functions: docking scores (QuickVina 2) for specific protein targets and pytdc oracle functions (QED, ML predictors for targets, similarity, MPOs); augmented docking reward multiplies normalized docking by QED and (10 - SA)/9 to penalize chemically irrelevant designs.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Average score of top-k (k=1,10,100) sampled molecules, normalized docking scores (divided by -20), diversity = average pairwise Tanimoto similarity of top-100 (higher is better), redundancy = number of oracle calls for already-evaluated molecules, computational budgets (max 25k unique oracle calls, 40k total).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>ChemRLformer (pretrained transformer/RNN then RL-finetuned) attains state-of-the-art performance across 25 molecular design tasks. Key quantitative/qualitative outcomes: pretraining on higher-quality dataset (ChEMBL) outperformed much larger datasets; SMILES outperformed SELFIES across pytdc and docking tasks for top-k scores; adding a hill-climb replay buffer improved Top-100 rewards by ~13%; Log P likelihood penalization improved exploration and boosted performance; KL regularization toward pretrained prior gave little benefit at larger oracle budgets while increasing GPU memory. Transformer and RNN achieved comparable downstream performance; FC baseline appeared to achieve high docking rewards but via reward-hacking.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to prior text-based methods (e.g., REINVENT variants) and graph-based RL, ChemRLformer (text-based transformer/RNN + RL) is simpler yet achieves superior or comparable performance. The paper reports that dataset quality (ChEMBL) matters more than raw pretraining size — ChEMBL-pretrained agent outperformed agents pretrained on datasets up to 100x larger. SMILES policies beat SELFIES counterparts on most benchmarks. FC (non-sequence) agents could outperform on raw docking metrics but were found to exploit scoring artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Several limitations reported: (1) Reward hacking — agents (especially FC) could produce chemically trivial sequences that score highly under the docking simulator; required augmented reward (docking × QED × SA) to mitigate. (2) SELFIES induces a flat/many-to-one landscape that increases redundancy and hampers exploration despite producing valid molecules. (3) Pretraining behaves like behavioral cloning — dataset quality biases results and may limit exploration; next-token objective may be insufficient for broad offline-to-online generalization. (4) Docking simulators are computationally expensive (e.g., docking 1000 molecules ~130s on 12 CPUs), constraining budgets. (5) The paper does not report exact novelty percentages (e.g., fraction not in train set) and acknowledges limited wet-lab validation; some augmented docking tasks required 10x more oracle budget for substantial RL improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Practical hyperparameters reported (example): Log P coefficient = 5, KL coefficient = 1e-3; learning rates: 5e-4 for RNN/FC, 1e-4 for Transformer; Transformers pretrained for 5 epochs (batch sizes up to 2048 for large datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Searching for High-Value Molecules Using Reinforcement Learning and Transformers', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8890.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8890.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretrained GPT-style transformer policy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-style autoregressive transformer pretrained on molecular text (used as RL initialization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-style autoregressive transformer pretrained with next-token prediction on molecular SMILES/SELFIES corpora to provide a high-quality prior for downstream RL fine-tuning in molecule generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-style transformer (autoregressive, used as pretrained policy)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Transformer (autoregressive, self-attention/GPT-style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported experimental transformer: ~4.78e6 parameters (6 layers, 16 heads, 256 embedding dims). Larger transformer pretraining batch sizes used for larger datasets but model depth/width held fixed in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on curated molecular text datasets (SMILES and converted SELFIES) including ChEMBL (~1.2M molecules), ZINC-250K (~250k), and sampled subsets of ZINC (1M, 10M, 100M); datasets filtered for heavy-atom counts and element types and sanitized with RDKit.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Initialization/prior for de novo molecular generation for drug-discovery tasks (docking targets and pytdc oracles).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Next-token autoregressive pretraining (maximum likelihood) used to initialize policies that either (a) sample molecules directly as priors, or (b) are fine-tuned via REINFORCE to optimize task-specific rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Pretrained model sampling used to evaluate dataset quality via top-100 molecules; ChEMBL-pretrained model produced priors better aligned to downstream tasks; explicit novelty metrics (fraction unseen etc.) not reported for pretrained-only samples.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Provides a strong drug-like prior; downstream task specificity obtained by RL fine-tuning on task reward functions (docking, pytdc); pretraining alone produces diverse drug-like molecules but not property-optimized ones.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Pretraining quality evaluated by top-100 molecule scores on downstream reward functions, validation loss on next-token prediction (example: transformer val loss ~22.923 on ZINC-250K SMILES), and downstream RL sample efficiency and top-k metrics after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Pretraining substantially eases RL optimization; dataset quality mattered more than dataset size (ChEMBL-pretrained transformer outperformed those trained on much larger ZINC subsets). The pretrained transformer achieved lower validation loss than FC baseline and enabled the RL agent to avoid generating many invalid SMILES (when pretrained).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Pretraining using next-token prediction is analogous to behavioral cloning; authors compare different pretraining datasets and show quality (ChEMBL) > quantity (larger ZINC subsets) for downstream RL. Pretraining also compared across architectures (transformer/RNN/FC) showing transformer and RNN perform similarly downstream when pretrained.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Pretraining objective (next-token prediction) can induce behavior-cloning bias that may limit exploration; pretrained models alone are not property-tuned and must be fine-tuned with RL; pretraining large transformers increases GPU memory and compute, and adding KL regularization to retain closeness to the prior further increases memory without clear benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Searching for High-Value Molecules Using Reinforcement Learning and Transformers', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8890.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8890.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REINVENT (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REINVENT: Molecular de novo design through deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier text-based molecular generation method using an RNN (GRU) trained with reinforcement learning for de novo molecular design, employing replay buffers and KL penalties to constrain the policy toward a pretrained prior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Molecular de novo design through deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>REINVENT (GRU-based policy trained with RL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Recurrent Neural Network (GRU) with policy-gradient RL</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on SMILES corpora (various drug-like datasets in prior literature); specifics depend on cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>De novo drug design, property optimization via text-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Autoregressive SMILES generation via pretrained RNN fine-tuned with policy-gradient RL; uses replay buffers and KL regularization toward prior.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>Not detailed in this paper beyond existing literature reference; REINVENT historically reported generation of novel molecules relative to prior data in its original evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Optimizes molecules for task-specific scoring functions via RL while constraining drift from prior via KL penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Typical metrics in related works: top-k scores, validity, uniqueness, novelty, and property-specific oracles (not explicitly reported here beyond being a referenced baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as a strong prior text-based RL method; ChemRLformer is positioned as simpler and achieving higher performance by demystifying which algorithmic choices matter (e.g., hill-climb buffer, Log P regularizer) compared to REINVENT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>REINVENT is described as a predecessor that used GRU policies and KL constraints; ChemRLformer differs by evaluating transformer architectures, showing dataset quality effects, and questioning the necessity of some common regularizers (KL) under larger oracle budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Mentioned in context: prior text-based methods like REINVENT use many algorithmic components (replay buffers, KL regularization) whose real benefit is task- and budget-dependent; KL regularization increases memory and may not improve performance under larger simulation budgets (per experiments in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Searching for High-Value Molecules Using Reinforcement Learning and Transformers', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Molecular de novo design through deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation <em>(Rating: 2)</em></li>
                <li>Dockstring: easy molecular docking yields better benchmarks for ligand design <em>(Rating: 2)</em></li>
                <li>Automatic chemical design using a data-driven continuous representation of molecules <em>(Rating: 1)</em></li>
                <li>Language Models are Few-Shot Learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8890",
    "paper_id": "paper-8923aec569a13f94148e3e90a94c68730f6ad03d",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "ChemRLformer",
            "name_full": "ChemRLformer (text-based RL molecular design)",
            "brief_description": "A text-based molecular design system that initializes an autoregressive policy with a pretrained language model (GPT-style transformer or RNN) over molecular texts (SMILES/SELFIES) and fine-tunes it with policy-gradient RL (REINFORCE) plus practical algorithmic components (hill-climb replay buffer, Log P regularizer) to generate high-value molecules for drug-discovery tasks including docking.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChemRLformer (pretrained autoregressive policy fine-tuned with REINFORCE)",
            "model_type": "Autoregressive sequence models (GPT-style transformer and GRU RNN used as policies)",
            "model_size": "Transformer: ~4.78e6 parameters; RNN (3x GRU): ~4.17e6; FC baseline: ~1.07e7 (reported in Table 4).",
            "training_data": "Pretraining on molecular text datasets (SMILES and converted SELFIES) including ChEMBL (~1.2M), ZINC-250K (~250k), ZINC-1M, ZINC-10M, ZINC-100M; datasets filtered/sanitized and canonicalized with RDKit.",
            "application_domain": "Drug discovery — de novo ligand generation for protein docking targets (fa7, parp1, 5ht1b, jak2, braf) and a suite of 21 pharmaceutically-relevant oracle tasks (pytdc).",
            "generation_method": "Autoregressive next-token sampling of SMILES/SELFIES initialized by pretrained language model and fine-tuned with on-policy RL (REINFORCE). Algorithmic augmentations include hill-climb replay buffer, Log P (likelihood) regularizer to avoid premature convergence, optional KL penalty toward the pretrained prior, and use of an augmented docking reward combining docking score, QED and SA for robustness.",
            "novelty_of_chemicals": "Novelty measured indirectly: diversity (average pairwise Tanimoto of top-100 molecules) and redundancy (repeated oracle calls). Paper does not report explicit % of molecules absent from training set or exact Tanimoto thresholds for novelty; qualitative findings: SMILES-based agents produce higher top-k scores; SELFIES agents yield higher diversity but also higher redundancy (explore smaller region effectively).",
            "application_specificity": "Specificity enforced/evaluated by optimizing final-molecule reward functions: docking scores (QuickVina 2) for specific protein targets and pytdc oracle functions (QED, ML predictors for targets, similarity, MPOs); augmented docking reward multiplies normalized docking by QED and (10 - SA)/9 to penalize chemically irrelevant designs.",
            "evaluation_metrics": "Average score of top-k (k=1,10,100) sampled molecules, normalized docking scores (divided by -20), diversity = average pairwise Tanimoto similarity of top-100 (higher is better), redundancy = number of oracle calls for already-evaluated molecules, computational budgets (max 25k unique oracle calls, 40k total).",
            "results_summary": "ChemRLformer (pretrained transformer/RNN then RL-finetuned) attains state-of-the-art performance across 25 molecular design tasks. Key quantitative/qualitative outcomes: pretraining on higher-quality dataset (ChEMBL) outperformed much larger datasets; SMILES outperformed SELFIES across pytdc and docking tasks for top-k scores; adding a hill-climb replay buffer improved Top-100 rewards by ~13%; Log P likelihood penalization improved exploration and boosted performance; KL regularization toward pretrained prior gave little benefit at larger oracle budgets while increasing GPU memory. Transformer and RNN achieved comparable downstream performance; FC baseline appeared to achieve high docking rewards but via reward-hacking.",
            "comparison_to_other_methods": "Compared to prior text-based methods (e.g., REINVENT variants) and graph-based RL, ChemRLformer (text-based transformer/RNN + RL) is simpler yet achieves superior or comparable performance. The paper reports that dataset quality (ChEMBL) matters more than raw pretraining size — ChEMBL-pretrained agent outperformed agents pretrained on datasets up to 100x larger. SMILES policies beat SELFIES counterparts on most benchmarks. FC (non-sequence) agents could outperform on raw docking metrics but were found to exploit scoring artifacts.",
            "limitations_and_challenges": "Several limitations reported: (1) Reward hacking — agents (especially FC) could produce chemically trivial sequences that score highly under the docking simulator; required augmented reward (docking × QED × SA) to mitigate. (2) SELFIES induces a flat/many-to-one landscape that increases redundancy and hampers exploration despite producing valid molecules. (3) Pretraining behaves like behavioral cloning — dataset quality biases results and may limit exploration; next-token objective may be insufficient for broad offline-to-online generalization. (4) Docking simulators are computationally expensive (e.g., docking 1000 molecules ~130s on 12 CPUs), constraining budgets. (5) The paper does not report exact novelty percentages (e.g., fraction not in train set) and acknowledges limited wet-lab validation; some augmented docking tasks required 10x more oracle budget for substantial RL improvement.",
            "additional_notes": "Practical hyperparameters reported (example): Log P coefficient = 5, KL coefficient = 1e-3; learning rates: 5e-4 for RNN/FC, 1e-4 for Transformer; Transformers pretrained for 5 epochs (batch sizes up to 2048 for large datasets).",
            "uuid": "e8890.0",
            "source_info": {
                "paper_title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Pretrained GPT-style transformer policy",
            "name_full": "GPT-style autoregressive transformer pretrained on molecular text (used as RL initialization)",
            "brief_description": "A GPT-style autoregressive transformer pretrained with next-token prediction on molecular SMILES/SELFIES corpora to provide a high-quality prior for downstream RL fine-tuning in molecule generation tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-style transformer (autoregressive, used as pretrained policy)",
            "model_type": "Transformer (autoregressive, self-attention/GPT-style)",
            "model_size": "Reported experimental transformer: ~4.78e6 parameters (6 layers, 16 heads, 256 embedding dims). Larger transformer pretraining batch sizes used for larger datasets but model depth/width held fixed in experiments.",
            "training_data": "Pretrained on curated molecular text datasets (SMILES and converted SELFIES) including ChEMBL (~1.2M molecules), ZINC-250K (~250k), and sampled subsets of ZINC (1M, 10M, 100M); datasets filtered for heavy-atom counts and element types and sanitized with RDKit.",
            "application_domain": "Initialization/prior for de novo molecular generation for drug-discovery tasks (docking targets and pytdc oracles).",
            "generation_method": "Next-token autoregressive pretraining (maximum likelihood) used to initialize policies that either (a) sample molecules directly as priors, or (b) are fine-tuned via REINFORCE to optimize task-specific rewards.",
            "novelty_of_chemicals": "Pretrained model sampling used to evaluate dataset quality via top-100 molecules; ChEMBL-pretrained model produced priors better aligned to downstream tasks; explicit novelty metrics (fraction unseen etc.) not reported for pretrained-only samples.",
            "application_specificity": "Provides a strong drug-like prior; downstream task specificity obtained by RL fine-tuning on task reward functions (docking, pytdc); pretraining alone produces diverse drug-like molecules but not property-optimized ones.",
            "evaluation_metrics": "Pretraining quality evaluated by top-100 molecule scores on downstream reward functions, validation loss on next-token prediction (example: transformer val loss ~22.923 on ZINC-250K SMILES), and downstream RL sample efficiency and top-k metrics after fine-tuning.",
            "results_summary": "Pretraining substantially eases RL optimization; dataset quality mattered more than dataset size (ChEMBL-pretrained transformer outperformed those trained on much larger ZINC subsets). The pretrained transformer achieved lower validation loss than FC baseline and enabled the RL agent to avoid generating many invalid SMILES (when pretrained).",
            "comparison_to_other_methods": "Pretraining using next-token prediction is analogous to behavioral cloning; authors compare different pretraining datasets and show quality (ChEMBL) &gt; quantity (larger ZINC subsets) for downstream RL. Pretraining also compared across architectures (transformer/RNN/FC) showing transformer and RNN perform similarly downstream when pretrained.",
            "limitations_and_challenges": "Pretraining objective (next-token prediction) can induce behavior-cloning bias that may limit exploration; pretrained models alone are not property-tuned and must be fine-tuned with RL; pretraining large transformers increases GPU memory and compute, and adding KL regularization to retain closeness to the prior further increases memory without clear benefit.",
            "uuid": "e8890.1",
            "source_info": {
                "paper_title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "REINVENT (related work)",
            "name_full": "REINVENT: Molecular de novo design through deep reinforcement learning",
            "brief_description": "An earlier text-based molecular generation method using an RNN (GRU) trained with reinforcement learning for de novo molecular design, employing replay buffers and KL penalties to constrain the policy toward a pretrained prior.",
            "citation_title": "Molecular de novo design through deep reinforcement learning",
            "mention_or_use": "mention",
            "model_name": "REINVENT (GRU-based policy trained with RL)",
            "model_type": "Recurrent Neural Network (GRU) with policy-gradient RL",
            "model_size": null,
            "training_data": "Pretrained on SMILES corpora (various drug-like datasets in prior literature); specifics depend on cited work.",
            "application_domain": "De novo drug design, property optimization via text-based RL.",
            "generation_method": "Autoregressive SMILES generation via pretrained RNN fine-tuned with policy-gradient RL; uses replay buffers and KL regularization toward prior.",
            "novelty_of_chemicals": "Not detailed in this paper beyond existing literature reference; REINVENT historically reported generation of novel molecules relative to prior data in its original evaluations.",
            "application_specificity": "Optimizes molecules for task-specific scoring functions via RL while constraining drift from prior via KL penalty.",
            "evaluation_metrics": "Typical metrics in related works: top-k scores, validity, uniqueness, novelty, and property-specific oracles (not explicitly reported here beyond being a referenced baseline).",
            "results_summary": "Cited as a strong prior text-based RL method; ChemRLformer is positioned as simpler and achieving higher performance by demystifying which algorithmic choices matter (e.g., hill-climb buffer, Log P regularizer) compared to REINVENT variants.",
            "comparison_to_other_methods": "REINVENT is described as a predecessor that used GRU policies and KL constraints; ChemRLformer differs by evaluating transformer architectures, showing dataset quality effects, and questioning the necessity of some common regularizers (KL) under larger oracle budgets.",
            "limitations_and_challenges": "Mentioned in context: prior text-based methods like REINVENT use many algorithmic components (replay buffers, KL regularization) whose real benefit is task- and budget-dependent; KL regularization increases memory and may not improve performance under larger simulation budgets (per experiments in this paper).",
            "uuid": "e8890.2",
            "source_info": {
                "paper_title": "Searching for High-Value Molecules Using Reinforcement Learning and Transformers",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Molecular de novo design through deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation",
            "rating": 2
        },
        {
            "paper_title": "Dockstring: easy molecular docking yields better benchmarks for ligand design",
            "rating": 2
        },
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "rating": 1
        },
        {
            "paper_title": "Language Models are Few-Shot Learners",
            "rating": 1
        }
    ],
    "cost": 0.01532,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Searching for High-Value Molecules Using Reinforcement Learning and Transformers</h1>
<p>Raj Ghugare ${ }^{1,2}$ Santiago Miret ${ }^{3}$ Adriana Hugessen ${ }^{1,2}$<br>Mariano Phielipp ${ }^{3}$ Glen Berseth ${ }^{1,2}$<br>${ }^{1}$ Université de Montréal ${ }^{2}$ Mila - Quebec AI Institute ${ }^{3}$ Intel Labs</p>
<h4>Abstract</h4>
<p>Reinforcement learning (RL) over text representations can be effective for finding high-value policies that can search over graphs. However, RL requires careful structuring of the search space and algorithm design to be effective in this challenge. Through extensive experiments, we explore how different design choices for text grammar and algorithmic choices for training can affect an RL policy's ability to generate molecules with desired properties. We arrive at a new RL-based molecular design algorithm (ChemRLformer) and perform a thorough analysis using 25 molecule design tasks, including computationally complex protein docking simulations. From this analysis, we discover unique insights in this problem space and show that ChemRLformer achieves state-of-the-art performance while being more straightforward than prior work by demystifying which design choices are actually helpful for text-based molecule design.</p>
<h2>1 Introduction</h2>
<p>Molecular discovery can have a significant impact on our society, however, the vast search space makes it challenging to find high-value molecules. The potential of reinforcement learning (RL) methods to discover new, high-value molecules has resulted in a series of research work performed by RL researchers focusing on learning policies as graph neural networks (GNNs) [You et al., 2018, Zhou et al., 2019, Jin et al., 2020, Fu et al., 2022, Yang et al., 2021, Bengio et al., 2021]. In this formulation, the RL policy is trained to add atoms and bonds to a molecular graph representation. In this formulation there is a one-to-one mapping between molecules and their graph representation, making it easier to construct state and action spaces with Markovian dynamics. However, the action space in the graph formulation is vast as it consists of the product of candidate attachment positions and candidate attachment sequences. Graph-based data structures (such as adjacency matrices, trees, etc.) are a powerful representation used to describe a number of design problems, including social networks [Tan et al., 2019], transportation networks [Wang and Tang, 2021], recommendation systems [Chen et al., 2021b], and combinatorial optimization problems [Khadka et al., 2020, Miret et al., 2022] have been popular in this design space. However, GNNs are often difficult to train [Chen et al., 2022] and cannot readily take advantage of large-scale text data sets that effectively describe molecular structures and properties.
In order to take advantage of the richness of text-based representations for molecules, one can formulate the molecular search problem as the construction of tokens in a sequence that become a molecular text. The molecular texts formulated by common text-based representations, such as SMILES [Weininger, 1988] and SELFIES [Krenn et al., 2020], can then be converted into molecular graphs with cheminformatics libraries [Landrum et al., 2013] using their respective encoding and decoding rules. However, the text-based representation can be more difficult to formulate as an MDP since there is not always an exact one-to-one mapping between texts and molecules. In fact, the text-to-molecule conversion can be many-to-one, where the complexity of the dynamics in the MDP given by many-to-one mappings is non-trivial. On the other hand, the action space in</p>
<p>molecular text design can be significantly reduced given the rules of text construction imposed by a given representation. Moreover, formulating molecule discovery as sequence-generation has the potential to capitalize on recent successes in natural language modeling [Brown et al., 2020a].</p>
<p>In this paper, we perform a detailed empirical study of molecular discovery using text-based RL across more than 25 molecular properties relevant for drug-discovery, including docking simulation for molecular ligands [García-Ortegón et al., 2022, Huang et al., 2022] and develop our own algorithm (MoLRL) based on state-of-the-art literature as shown in Table 1. In our experiments, we evaluate two molecular text representations (SMILES, SELFIES) and the use of three neural network architectures (Multi-Layer Perceptron [Bengio et al., 2003], Recurrent Neural Network [Schmidt, 2019], Transformer [Vaswani et al., 2017]) pretrained on 5 datasets of varying quality and sizes. We create ChemRLformer that achieves the highest performance across these tasks while being much simpler than previous text-based RL algorithms [Blaschke et al., 2020a, Gao et al., 2022].</p>
<p>Via our detailed ablation study, we construct ChemRLformer and find that pretraining on aligned datasets can significantly improve performance across all molecular design tasks, even exceeding the performance of agents pretrained on 100 times larger datasets. We also show that targeted algorithmic design, such as hill-climbing in the replay buffer and regularization, further increases the performance of ChemRLformer. To the best of our knowledge, ChemRLformer is the largest analysis of text-based RL methods for molecule discovery.</p>
<p>Table 1: Table showing conceptual comparisons of various text based molecular optimization methods. MoLRL combines the most successful elements of prior work.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Text Representation</th>
<th>RL</th>
<th>Architecture</th>
<th>Pretraining</th>
<th>Algorithmic Components</th>
</tr>
</thead>
<tbody>
<tr>
<td>Literature Methods</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>SMILES-VAE [Gómez-Bombarelli et al., 2018]</td>
<td>SMILES</td>
<td>✗</td>
<td>VAE</td>
<td>✓</td>
<td>Maximum Likelihood</td>
</tr>
<tr>
<td>SMILES-LSTM [Brown et al., 2019]</td>
<td>SMILES</td>
<td>✗</td>
<td>LSTM</td>
<td>✓</td>
<td>Maximum Likelihood</td>
</tr>
<tr>
<td>BOSS [Moss et al., 2020]</td>
<td>SMILES</td>
<td>✗</td>
<td>VAE</td>
<td>✗</td>
<td>Bayesian Optimization</td>
</tr>
<tr>
<td>REINVENT [Blaschke et al., 2020a]</td>
<td>SMILES</td>
<td>✓</td>
<td>GRU</td>
<td>✓</td>
<td>Replay buffer, KL</td>
</tr>
<tr>
<td>REINVENT 2.0 [Blaschke et al., 2020b]</td>
<td>SMILES</td>
<td>✓</td>
<td>GRU</td>
<td>✓</td>
<td>HC-Replay buffer, Log p, KL</td>
</tr>
<tr>
<td>STONED [Nigam et al., 2021]</td>
<td>SELFIES</td>
<td>✗</td>
<td>FC</td>
<td>✗</td>
<td>Genetic algorithm</td>
</tr>
<tr>
<td>Pasithua [Shen et al., 2021]</td>
<td>SELFIES</td>
<td>✗</td>
<td>FC</td>
<td>✗</td>
<td>Deep dreaming</td>
</tr>
<tr>
<td>ChemRLformer (Ours)</td>
<td>SMILES, SELFIES</td>
<td>✓</td>
<td>Transformer, FC</td>
<td>✓</td>
<td>Replay buffer, KL</td>
</tr>
</tbody>
</table>
<h1>2 Related Work</h1>
<p>RL for Design and Discovery: Many methods in diverse fields leverage RL to help augment a prior design method to improve performance [Yu et al., 2018, Schaff et al., 2019]. Other methods have explicitly included the design process in the RL loop by training design problems together [Chen et al., 2021a, Ha, 2019, Luck et al., 2020, Kumar et al., 2022] with most prior work focusing on robot and agent design, not molecular design. Our molecular design work creates an autoregressive structure that grows the size of the state as the agent acts in the environment.</p>
<p>Molecular Discovery Using Sequence-Based Methods: Sequence-based methods treat molecular design as a sequence of tokens that get concatenated in order. Generative models for sequence-based methods span a diverse range, including variational autoencoders (VAEs) [Gómez-Bombarelli et al., 2018, Alperstein et al., 2019], recurrent neural networks (RNNs) [Gupta et al., 2018, Bjerrum and Threlfall, 2017, Grisoni et al., 2020, Flam-Shepherd et al., 2022] and transformer models[Wang et al., 2019, Fabian et al., 2020, Edwards et al., 2022a, Zeng et al., 2022, Taylor et al., 2022]. The general procedure for all the above methods is to perform self-supervised generative learning to sample molecules similar to the original dataset. MoLRL can also make use of pretrained generative models, which we then fine-tune using reinforcement learning to produce enhanced molecules.</p>
<p>Molecular Discovery Using Search-Based Methods: Although sequence-based molecule generation methods often provide a more structured way of learning molecular distributions, searchbased methods generally have the advantage of being able to directly find molecules based on a desired property. Although a wide range of graph-based RL methods [You et al., 2018, Zhou et al., 2019, Jin et al., 2020, Fu et al., 2022, Yang et al., 2021, Bengio et al., 2021] for optimizing molecules exist, graph-based state representations introduce significant complexity to the RL problem formulation, both in the transition dynamics and action space. By contrast, text-based</p>
<p>methods are simpler and also relatively under-explored, motivating our focus on these methods in this work. Moreover, recent work [Cieplinski et al., 2021, Gao et al., 2022] has shown that an older text-based method REINVENT [Olivecrona et al., 2017] outperforms more complex graph-based RL methods. Some limited extensions to Olivecrona et al. [2017] have been explored, including experimenting with a newer molecular grammar designed for robust molecule generation [Gao et al., 2022]. However, there has been limited work proposing the use of language models and text-based RL for molecular discovery. Additionally, there have been limited efforts to incorporate recent advancements from the language modeling domain into these methods. For example, the a characterlevel LSTM network architecture used in Olivecrona et al. [2017], has not been revisited despite significant recent advances in sequence modeling [Vaswani et al., 2017, Brown et al., 2020b].</p>
<h1>3 Background</h1>
<p>The algorithms detailed in this paper are built on top of a foundation of reinforcement learning, text-based molecule representations, and language modeling.</p>
<p>Reinforcement Learning: Reinforcement learning can be used to learn policies for sequential decision-making problems. Policies are optimized based on an environment that is described as a Markov Decision Process (MDP). A discrete MDP is defined by the tuple $\langle\mathcal{S}, \mathcal{A}, \mathcal{T}, r, \gamma\rangle$ where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $T: \mathcal{S} \times \mathcal{A} \times \mathcal{S}^{\prime} \rightarrow[0,1]$ is the transition function, $r: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{R}$ is the reward function and $\gamma$ is the discount rate.
For actions $a_{t} \in \mathcal{A}$ and states $s_{t} \in \mathcal{S}$, the goal of reinforcement learning is to learn a policy $\pi_{\theta}\left(a_{t} \mid s_{t}\right)$ which maps states to actions, such that:</p>
<p>$$
\pi_{\theta}\left(a_{t} \mid s_{t}\right)=\underset{\theta}{\arg \max } \mathbf{E}<em t="0">{p(\tau \mid \theta)}\left[\sum</em>\right)\right]
$$}^{T} \gamma^{t} r\left(s_{t}, a_{t</p>
<p>where $p(\tau \mid \theta)$ is the distribution over trajectories induced by $\pi_{\theta}$ and the transition function $\mathcal{T}$.
Text representations for molecules: Molecules are most naturally described using a graph structure of atoms and bonds. However, graph-based deep learning models can be difficult to train, especially at large scale [Dwivedi et al., 2022, Geisler et al., 2023]. Recent works have proposed a variety of text representations for molecules [Weininger, 1988, Krenn et al., 2020, Heller et al., 2013, Krenn et al., 2022, Cheng et al., 2023], each having their distinct advantages and shortcomings. In this study, we focus on the two most commonly used representations: SMILES [Weininger, 1988] and SELFIES [Krenn et al., 2020]. Any text representation for molecules consists of a set of valid tokens, which may represent individual atoms or special characters that imply the presence of certain structures, as well as the encoding and decoding rules needed to convert between the text representation and the graph representation of a molecule. Valid texts under a grammar are those which respect both the vocabulary and the encoding/decoding rules for that grammar and, hence, can be converted into a graph representation of a molecule. SELFIES, which was developed in response to the tendency for SMILES-based deep learning models [Gó mez-Bombarelli et al., 2018, Jin et al., 2018] to generate invalid molecular texts, has the useful property of providing a conversion for any text into a graph corresponding to a molecule, provided the tokens in the text respect the SELFIES vocabulary. For example, the text representation of Benzene in SMILES is $\mathrm{C} 1=\mathrm{CC}=\mathrm{CC}=\mathrm{C} 1$ while in SELFIES one possible representation is $[\mathrm{C}][=\mathrm{C}][\mathrm{C}][=\mathrm{C}][\mathrm{C}][=\mathrm{C}][\operatorname{Ring} 1][=$ Branch1].</p>
<p>Language modeling: Language modeling often relies on the self-supervised task of next-token prediction for model pretraining. The general framework for next-token prediction is to train a model to predict the next token in a sequence autoregressively, i.e. given the previous tokens in the sequence (left context). Many architectures to handle sequential data have been proposed: Recurrent Neural Networks (RNNs) [Hochreiter and Schmidhuber, 1997, Rumelhart and McClelland, 1987] are a class of models used in sequence modeling which use recursive connections in hidden layers to accumulate the left context for next-token prediction. Transformers are a more recent architecture that instead use a self-attention mechanism [Vaswani et al., 2017] to capture dependencies between all tokens in a sequence. For next-token prediction tasks, attention masking is used to enforce left context, meaning that representations for tokens later in the sequence are only allowed to attend to previous tokens in the sequence. In Section 4 we outline how we pretrain an autoregressive sequence model to predict sequences of known molecules.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Autoregressively generating a benzene molecule.: An autoregressive model for sequence generation can be viewed as an RL policy where the actions $a_{t}$ are the next tokens to append to the sequence and the state is the concatenation of all actions taken up to time $t-1$. A special end-of-sequence token can terminate the episode early at time $T$. The text at time $s_{T}$ is then converted into a molecule based on the text-representation grammar and then scored according to a scoring function that measures the alignment of the molecule with the desired properties informed by the application. Hydrogen atoms are added at the end to complete the structure.</p>
<h1>4 ChemRLformer Generating molecular strings via reinforcement learning</h1>
<p>The molecular design space is complex but the benefit from finding improved options is great. In this section, we describe ChemRLformer and how combining language models and tools from RL produces a sota algorithm.</p>
<p>MDP for molecule generation: The vocabulary and grammar for text representations of molecules can be interpreted as an MDP as described in Section 3, where the states $s_{t}$ correspond to a variable length text of accumulating tokens, and the actions $a_{t}$ correspond to vocabulary defined by the text-representation. The transition function is a deterministic function where the action $a_{t}$ taken by the agent is appended to the end of the state $s_{t}$ resulting in $s_{t+1}$ using the dynamics $s_{t+1}=\left[s_{t}, a_{t}\right] \leftarrow T\left(s_{t}, a_{t}\right)$. However, the corresponding transition function induced in the graph representation of molecules is more complex as it is determined by the encoding/decoding rules of the chosen text representation. For example, in the SMILES grammar, a random concatenation of tokens may not correspond to a valid molecule, while the SELFIES grammar is constructed such that any ordering of its tokens is encoded as a valid molecule.</p>
<p>Finally, the reward function $\mathcal{R}$ scores molecules according to their alignment with desired chemical properties, which can involve complex material simulations. The underlying property computation of the reward function further informs the dynamics of the MDP imposed by text representation. For example, docking scores are used to estimate the binding affinity between ligands and protein targets. We discuss reward functions for molecules in more detail in section Section 5.</p>
<p>Pretraining policies for molecule discovery. To advance effectively within this vast search space, we make use of datasets containing a large number of drug-like molecules in text format [Irwin et al., 2012, Sterling and Irwin, 2015b, Mendez et al., 2019]. This data is used to train an autoregressive model to predict tokens that conform to the grammar for drug-like molecules, instead of the random texts that are generated from a randomly initialized policy, thereby significantly simplifying the exploration problem. In particular, we pretrain a network $p_{\phi}$ on the self-supervised objective of next-token prediction. Although large language models can be trained with other objectives, such as corrupted text reconstruction [Edwards et al., 2022b], these models are not a good fit for our purposes since they cannot generate diverse and valid molecules without access to carefully designed prompts.</p>
<p>$$
\min <em A="A" D="D" _sim="\sim">{\theta} \mathbb{E}</em>\right)\right]
$$}\left[\sum_{t=1}^{H}-\log p_{\theta}\left(a_{t}=A_{t} \mid A_{t-1}, \cdots A_{0</p>
<p>In practice a minibatch of sequences $\left{A^{1}, \cdots, A^{m}\right}$ are sampled from the prior dataset $D$ to evaluate the loss function in Equation 2, and the parameters are trained using gradient descent.</p>
<h1>4.1 RL for molecule generation</h1>
<p>To generate a molecule, a ChemRLformer policy $\pi_{\theta}\left(a_{t} \mid s_{t}\right)$ is allowed to autoregressively sample tokens for a fixed number of timesteps $H$. The start state $s_{0}$ is always a beginning-of-sequence token [BOS], and the agent can terminate early by taking the end-of-sequence action [EOS]. Figure 1, shows how an RL policy can construct a Benzene molecule. Since we are only interested in the properties of the final molecule, there are no intermediate rewards and the goal of the RL policy is to maximize the expected scalar reward corresponding to the final constructed molecule, $r\left(s_{T}\right)$. Thus, assuming a discount rate $\gamma=1$, Equation 1 can be rewritten more simply as:</p>
<p>$$
\begin{aligned}
&amp; \max <em s__T="s_{T">{s} \mathbb{E}</em>\right)\right] \
&amp; \text { where } s_{T}=[\mathrm{BOS}]\left[a_{0}\right]\left[a_{1}\right] \cdots[\mathrm{EOS}], \text { is sampled autoregressively from the policy. }
\end{aligned}
$$} \sim \pi_{\theta}}\left[r\left(s_{T</p>
<p>Our experiments use the policy gradient algorithm [Sutton et al., 1999b] to train the RL policy because it is known to achieve state-of-the-art performance amongst RL for molecular optimization [Olivecrona et al., 2017]. Deep RL policies are able to learn the non-linear global structures of molecular texts which, as we show in section 5, enables them to generalize to novel and diverse molecules. However, training RL policies from scratch is time-consuming and can make the exploration problem infeasibly difficult. Next, we explain how we adapt recent language modelling techniques to pretrain the RL policy.</p>
<p>RL fine-tuning The pretrained model can directly be used to sample novel drug-like molecules. These molecules, however, are not optimized for any particular property. Note that given our definition of the state $s_{t}$ as the concatenated history of all previous actions, this pretrained network is exactly analogous to the policy network in Equation 1. Hence, by initializing $\pi_{\theta}=p_{\phi}$, and $\theta=\phi$, we can fine-tune this pretrained network by optimizing Equation 1 via the policy gradient algorithm - REINFORCE [Sutton et al., 1999a]. We need only to define a reward function $r\left(s_{T}\right)$ which scores molecules according to their alignment with the desired properties. In the following experiments, we show that this fine-tuning is vital for ChemRLformer to sample better molecules. We also highlight the importance of pretraining and study how the size and quality of the prior data affect the downstream ability of RL to search for high-value molecules.</p>
<h2>5 Experimental Results</h2>
<p>Our proposed algorithm ChemRLformer uses the best combinations of choices resulting from assessing the performance across three dimensions: (1) what pretraining factors are important to improve RL for molecular discovery (Section 5.2), (2) how the use of recent text-based molecule grammars facilitates downstream RL exploration (Section 5.3); and, lastly, (3) which specific algorithmic changes are necessary to improve RL performance (Section 5.4).</p>
<h3>5.1 Experimental Setup</h3>
<p>Tasks. We evaluate ChemRLformer against five different docking targets [Alhossary et al., 2015] (fa7, parp1, 5ht1b, jak2, and braf) previously explored in the literature [Yang et al., 2021, Lee et al., 2023]. The docking scores used to estimate the binding affinity between ligands and protein targets are a complex function of the global molecular structure and have been proposed as chemically relevant benchmarks for molecule design algorithms [Cieplinski et al., 2021, Tripp et al., 2022]. In addition to the docking targets, we also evaluate on 22 pharmaceutically-relevant oracle functions [Huang et al., 2021, Gao et al., 2022, Brown et al., 2019] (pytdc tasks), which include tasks such as optimizing proxies of bioactivity, similarity to target molecules, and combinations of multiple physiochemical drug properties.</p>
<p>Evaluation metrics. We design our evaluation procedure with the final goal of identifying the best candidates to test in a wet lab. To discover such high-value candidate molecules, we use sota simulators that assign rewards to molecules by performing complex docking simulations [Alhossary et al., 2015] or using proxy models and chemical rules [Huang et al., 2021]. Previous works limit the number of molecules sampled during evaluation to around 3000 for docking tasks [García-Ortegón et al., 2022, Yang et al., 2021, Lee et al., 2023] and 10000 for pytdc tasks [Gao et al., 2022, Brown et al., 2019] due to the computational cost associated with these reward simulators. We allow up to</p>
<p>25000 unique oracle calls and up to 40000 total oracle calls (allowing repeats). We argue this better reflects the lower cost and availability of computing resources relative to wet-lab resources. From all the sampled molecules, the average score of the top- $k(k=1,10,100)$ molecules is used as a performance metric. These top groups are an estimate of the algorithm's ability to discover a group of top-quality candidates that could be given to a wet lab for thorough testing. We report pytdc scores on a normalized basis between zero and one by default. Next, we normalize all docking scores by dividing them by -20 in our experiments. Additionally, we report diversity, defined as the averaged internal distance of the top 100 molecules, and redundancy, defined as the total number of oracle calls that an agent makes for an already evaluated molecule.</p>
<p>Pretraining. We study how the quality and size of prior data affect the downstream RL performance of ChemRLformer by pretraining a GPT [Radford et al., 2018] style transformer model on five datasets of varying sizes and quality and using the pretrained model as an initialization for the RL agent's policy network. See Table 2 for the name, size, and description of all datasets used in our work. We also rank all datasets based on their quality on docking and pytdc tasks. We determine the quality of a dataset by the performance of molecules sampled from the model pretrained on that dataset.</p>
<p>The quality of ChemRLformer's pretrained model is evaluated using the top-100 molecules sampled by the pretrained model under the same evaluation setup in Appendix A.2. By default, these open-sourced datasets contain a large number of drug-like molecules in SMILES format. For our experiments, we also convert all datasets to the SELFIES format. Lastly, three different architectures are compared: fully-connected (FC), recurrent (RNN) - a GRU and transformer - GPT style autoregressive model, and compare them on downstream RL tasks.</p>
<p>Table 2: Description of molecular datasets used for pretraining: Datasets are ranked according to procedure described in Section 5.1. Two datasets have the same rank if their average performance lies inside one standard error of the other. The datasets are drawn from a subset of the Zinc [Sterling and Irwin, 2015b, Irwin et al., 2022] and ChemBL [Gaulton et al., 2012] databases.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Size</th>
<th style="text-align: center;">Docking Rank</th>
<th style="text-align: center;">Pytdc Rank</th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CHEMBL</td>
<td style="text-align: center;">1.2 M</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">Manually curated database of bioactive molecules with drug-like properties [Gaulton et al., 2012].</td>
</tr>
<tr>
<td style="text-align: center;">ZINC 250K</td>
<td style="text-align: center;">250 K</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">ZINC database molecules curated for their pharmaceutical relevance and popularity [Gao et al., 2022].</td>
</tr>
<tr>
<td style="text-align: center;">ZINC 1M</td>
<td style="text-align: center;">1 M</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">Random molecules from $\approx 1.5$ billion</td>
</tr>
<tr>
<td style="text-align: center;">ZINC 10M</td>
<td style="text-align: center;">10 M</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">molecules from the ZINC database [Sterling and Irwin, 2015a].</td>
</tr>
<tr>
<td style="text-align: center;">ZINC 100M</td>
<td style="text-align: center;">100 M</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">ZINC $1 \mathrm{M} \subset$ ZINC $10 \mathrm{M} \subset$ ZINC100M.</td>
</tr>
</tbody>
</table>
<p>All of our experiments on pytdc tasks are run across 5 random seeds. Since docking simulations are expensive and time consuming, we run all docking experiments across 3 random seeds. Experiments with different seeds use the same pretrained model which is only pretrained once for every dataset. Additional details about the task rewards, evaluation metrics, and the pretraining datasets and models are discussed in Appendix A.1, A.2, and A. 3 respectively.</p>
<h1>5.2 How does prior data affect the final performance of ChemRLformer?</h1>
<p>In this section, we pretrain the REINFORCE policy on datasets of varying size and quality from Table 2. Our datasets vary from small (250K) to very large (100M) sizes. Due to the parallelizability of training on larger datasets, we use the transformer policy architecture for all experiments in this section. In natural language processing (NLP), pretraining transformer models on large and diverse unlabelled datasets have been found to perform well on downstream tasks using few-shot labeled data [Brown et al., 2020b]. Yet, our results in Figure 2b indicate that the quality of the prior dataset matters more than its size. Figure 2, shows that the distribution of the ChEMBL dataset is more aligned with both the pytdc and the docking tasks. As a result, the RL agent pretrained on the ChEMBL dataset outperforms all other agents, including the ones trained on 100 times more data.</p>
<p>Results may seem surprising from an NLP perspective, but they make sense when viewed from an RL perspective. Pretraining using next token prediction Equation (2), is analogous to behavior</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" />
(a) Performance on SMILES-based molecular design with pertaining (left) and with pretraining and RL (right).
<img alt="img-2.jpeg" src="img-2.jpeg" />
(b) performance on SMILES-based molecular docking with pertaining (right) and with pretrainig and RL (right). Section 5.2 describes augmented docking setting with additional experiments shown in Appendix A.3.</p>
<p>Figure 2: On the left pretrained performance on SMILES-based ChemRLformer. Higher-quality datasets, such as ChemBL lead to higher-performance for both pytdc and augmented docking. On the right is the performance after RL training. RL has a substantial benefit for pytdc tasks, while for docking tasks an augmented docking score is used to avoid reward hacking, see Figure 4 for details.
cloning in this context, where the performance depends largely on the quality of the offline dataset [Ross et al., 2011, Ho and Ermon, 2016]. These results suggest that ChemRLformer might benefit from better pretraining objectives, that go beyond simple imitation learning, when trained on large and diverse offline datasets [Kumar et al., 2023, Farebrother et al., 2023].</p>
<h1>5.3 Text representations and architectures for ChemRLformer</h1>
<p>Starting with a REINFORCE agent, we isolate the effect of various text representations for molecules and policy network architectures on performance. All experiments in this section use ZINC-250k dataset for pretraining. Similar results obtained for other datasets are shared in the following sections. Whenever we show normalized results across different experiments, we add the individual plots in Appendix C.1.</p>
<p>Text representations. In Figure 3 we compare ChemRLformer agents using different architectures and tasks across environments that base their dynamics on SELFIES and SMILES. The results show normalized scores across all architectures. Consistent with prior work [Gao et al., 2022] we find that SMILES-based polices generally outperforms SELFIES-based policies. On all pytdc tasks and architectures, ChemRLformer agents based on SMILES consistently achieve better rewards when compared to SELFIES-based agents across all reward metrics. Although more subtle, we observe a similar theme in the docking tasks where SMILES achieves higher rewards than SELFIES on all top-K metrics. Another consistent theme in the results is that even though the diversity of top-100 molecules obtained by SELFIES is higher, the redundancy of SELFIES agents is higher as well. This means that SELFIES-based ChemRLformer agents explore a much smaller region of the molecular space. These results suggest that the rules which allow SELFIES strings to always be converted into a valid molecule can actually be detrimental to the agent's exploration and search for high-value molecules, more details in Appendix C.1.</p>
<p>Architectures. The results in Figure 4 show that the transformer and RNN have similar performance on all tasks. On the pytdc tasks, FC achieves worse performance than other architectures specially made to handle strings, as expected. However, on docking tasks, FC obtains unusually high rewards. We find that this method performs a type of reward function hacking [Amodei et al., 2016, Skalse et al., 2022, Everitt, 2019] by exploiting a corner case of the docking-based reward function which provides high rewards for long strings of Carbon and</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 3: Comparison between SELFIES and SMILES: The SELFIES representation makes it relatively difficult for ChemRLformer agents to explore effectively leading to generally lower performance on pytdc and docking while scoring higher on diversity. Scores are reported for the transformer model and are averaged across all reward functions.</p>
<p>Nitrogen atoms together. To evade the reward hacking of docking scores, we constructed an augmented docking score function with commonly used oracles (QED and SA scores) based on previous work [Lee et al., 2023] (See Appendix A.3 for more details). This finding shows that the REINFORCE agent can search the space well and, in this case, can be used to expose issues with the current design of reward functions.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: Comparison of different policy architectures: No single architecture clearly outperforms for molecular ChemRLformer. Although FC does better on the docking tasks, our analysis shows that it learns to exploit the docking function as opposed to designing high-value molecules. More details about ways to tackle this issue are given in Appendix C.3. Additional experiments for comparing transformers and RNNs are shown in Appendix C.5. These experiments use the smiles text representation.</p>
<h3>5.4 Revisiting RL Algorithm Design Choices for ChemRLformer.</h3>
<p>Previous experiments identify key design and algorithmic choices for efficient text-based RL. But recent text-based RL algorithms [Olivecrona et al., 2017, Bjerrum et al., 2023, Thomas et al., 2022] also employ many additional algorithmic components like replay buffers, hill climbing, KL regularisation towards the pretrained policy, and likelihood penalties. We perform an ablation study across these components to understand which ones are beneficial for the performance of ChemRLformer. All experiments in this section are performed on pytdc tasks, using an RNN architecture as it is most commonly used in text-based RL. See Appendix C.4 for experiments with other architectures and reward functions.</p>
<p>Replay buffers and hill climbing. In off-policy deep RL, a replay buffer is generally used to store and reuse previous trajectories for training. Although text-based RL algorithms are trained on-policy, prior work has proposed using a replay buffer to improve performance [Mnih et al., 2013]. Standard replay buffers throw away the oldest trajectories as newer ones arrive. But many text-based RL algorithms propose to use hill-climb replay buffers, that randomly sample a batch of molecules from the highest scoring molecules seen so far and add them to the current mini batch. In Figure 5, we see that using the hill-climb</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 5: Hill climbing buffer lead to 13% improvement in Top-100 rewards.</p>
<p>buffer results in a significant performance boost for ChemRLformer, whereas using a standard buffer does not contribute much. Notably, the use of a hill-climb replay buffer reduces diversity and increases redundancy quite substantially. The following two experiments involve combining regularisation terms with the RL objective in Equation 3. The coefficients on these extra terms can largely affect the final performance. To make a fair comparison, we perform hyper-parameter tuning over six different values for every new regularisation term with details provided in Appendix B.</p>
<p>Should the policy to stay close to the pretrained model? Pretrained models carry information on how to build valid drug-like molecules. To ensure that ChemRLformer agents do not stray far away from the space of valid drug-like molecules during exploration, <em>Olivecrona et al. [2017], Gao et al. [2022]</em> constrain the KL divergence between the policy and the pretrained model by adding a KL penalty to the policy gradient loss function in Equation (3). Prior works show that adding this penalty helps the agent achieve better sample efficiency <em>[Gao et al., 2022]</em>. Yet, our results in Figure 6 suggest that, when you increase the number of oracle calls in simulation, adding this penalty does not yield any additional benefit while substantially increasing the GPU memory requirement, especially when using larger models. Since invalid molecules correspond to zero rewards, the ChemRLformer agent is able to learn to avoid invalid structures on its own merit.</p>
<p>Regularizing the policy's likelihood for exploration. RL agents classically face an exploration-exploitation dilemma, which can lead to agents getting stuck in sub-optimal local maxima when not well balanced. ChemRLformer agents are not immune to this dilemma. Upon encountering good, but sub-optimal molecules, an agent may adjust its policy to increase the likelihood of sampling these sub-optimal molecules and, without sufficient exploration, fail to discover higher-value regions of policy space. This can be particularly detrimental during the initial learning stages.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 6: Our experiments show little difference in performance for multiple KL regularization terms.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7: Different likelihood penalization for exploration. Log P regularization is a better choice for efficient exploration for ChemRLformer.</p>
<p>To combat this issue, entropy regularisation, which adds a $\log \pi(s)$ term to the RL loss, has been proposed <em>[Haarnoja et al., 2018]</em>. This encourages the RL policy to explore states with lower likelihood values. Similarly <em>[Olivecrona et al., 2017]</em> adds a Log p regularizer, which penalizes higher likelihood values by adding a $-1 / \log \pi(s)$ term to the RL loss. In Figure 7, our results show that although an entropy regularizer leads to lesser redundancy, the Log p regularizer boosts performance significantly by exploring more efficiently. The Log p regularizer only penalizes the agent for being extremely certain (likelihood $\frac{\text { needs to }}{-4}$ 1) about its actions, and is mostly agnostic for lower likelihood values. This penalty is a much better choice for ChemRLformer as it only activates when stuck in a local optimum of molecular space.</p>
<h2>6 Conclusion and Future Work</h2>
<p>We present ChemRLformer that resulted from our empirical study of multiple algorithmic components of text-based molecular design. For future practitioners, our method suggests the following philosophy: (1) Using SMILES is a better choice than SELFIES. (2) When collecting data for pretraining, the quality of molecules matter much more than the number of molecules. (3) Both transformer and RNN architectures achieve similar performance across all tasks using current datasets. (4) Incorporating components such as a hill-climb buffer and Log P regularization yields substantial performance improvements. Conversely, introducing KL regularization or opting for more intricate actor-critic algorithms may result in diminished performance, at the cost of more hyperparameters and memory resources. While our analysis addresses many questions, it also shows that RL agents were able to hack the reward functions suggesting that there is space to improve on the metrics used for molecule quality.</p>
<h1>References</h1>
<p>Amr Alhossary, Stephanus Daniel Handoko, Yuguang Mu, and Chee-Keong Kwoh. Fast, accurate, and reliable molecular docking with QuickVina 2. Bioinformatics, 31(13):2214-2216, 02 2015. ISSN 1367-4803. doi: 10.1093/bioinformatics/btv082. URL https://doi.org/10.1093/ bioinformatics/btv082.</p>
<p>Zaccary Alperstein, Artem Cherkasov, and Jason Tyler Rolfe. All smiles variational autoencoder. arXiv preprint arXiv:1905.13343, 2019.</p>
<p>Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.</p>
<p>Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation, 2021. URL https://arxiv.org/abs/2106.04399.</p>
<p>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language model. J. Mach. Learn. Res., 3(null):1137-1155, mar 2003. ISSN 1532-4435.</p>
<p>Mostapha Benhenda. Chemgan challenge for drug discovery: can ai reproduce natural chemical diversity?, 2017.</p>
<p>Esben Jannik Bjerrum and Richard Threlfall. Molecular generation with recurrent neural networks (rnns). arXiv preprint arXiv:1705.04612, 2017.</p>
<p>Esben Jannik Bjerrum, Christian Margreitter, Thomas Blaschke, and Raquel López-Ríos de Castro. Faster and more diverse de novo molecular optimization with double-loop reinforcement learning using augmented smiles, 2023.</p>
<p>Thomas Blaschke, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, and Atanas Patronov. Reinvent 2.0: an ai tool for de novo drug design. Journal of chemical information and modeling, 60(12):5918-5922, 2020a.</p>
<p>Thomas Blaschke, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, and Atanas Patronov. Reinvent 2.0: An ai tool for de novo drug design. Journal of Chemical Information and Modeling, 60, 10 2020b. doi: 10.1021/acs.jcim. 0c00915.</p>
<p>Nathan Brown, Marco Fiscato, Marwin H.S. Segler, and Alain C. Vaucher. GuacaMol: Benchmarking models for de novo molecular design. Journal of Chemical Information and Modeling, 59(3):1096-1108, mar 2019. doi: 10.1021/acs.jcim.8b00839. URL https://doi. org/10.1021\%2Facs.jcim.8b00839.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In H Larochelle, M Ranzato, R Hadsell, M F Balcan, and H Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020b.</p>
<p>Tianjian Chen, Zhanpeng He, and Matei Ciocarlie. Hardware as policy: Mechanical and computational Co-Optimization using deep reinforcement learning. In Jens Kober, Fabio Ramos, and Claire Tomlin, editors, Proceedings of the 2020 Conference on Robot Learning, volume 155 of Proceedings of Machine Learning Research, pages 1158-1173. PMLR, 2021a.</p>
<p>Tianlong Chen, Kaixiong Zhou, Keyu Duan, Wenqing Zheng, Peihao Wang, Xia Hu, and Zhangyang Wang. Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.</p>
<p>Xiaocong Chen, Lina Yao, Julian McAuley, Guanglin Zhou, and Xianzhi Wang. A survey of deep reinforcement learning in recommender systems: A systematic review and future directions. arXiv preprint arXiv:2109.03540, 2021b.</p>
<p>Austin H Cheng, Andy Cai, Santiago Miret, Gustavo Malkomes, Mariano Phielipp, and Alán Aspuru-Guzik. Group selfies: a robust fragment-based molecular string representation. Digital Discovery, 2023.</p>
<p>Tobiasz Cieplinski, Tomasz Danel, Sabina Podlewska, and Stanislaw Jastrzebski. We should at least be able to design molecules that dock well, 2021.</p>
<p>Vijay Prakash Dwivedi, Chaitanya K. Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks, 2022.</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 375-413, Abu Dhabi, United Arab Emirates, December 2022a. Association for Computational Linguistics. URL https://aclanthology. org/2022.emnlp-main. 26.</p>
<p>Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language, 2022b.</p>
<p>Tom Everitt. Towards safe artificial general intelligence. PhD thesis, The Australian National University (Australia), 2019.</p>
<p>Benedek Fabian, Thomas Edlich, Héléna Gaspar, Marwin Segler, Joshua Meyers, Marco Fiscato, and Mohamed Ahmed. Molecular representation learning with language models and domainrelevant auxiliary tasks. arXiv preprint arXiv:2011.13230, 2020.</p>
<p>Jesse Farebrother, Joshua Greaves, Rishabh Agarwal, Charline Le Lan, Ross Goroshin, Pablo Samuel Castro, and Marc G. Bellemare. Proto-value networks: Scaling representation learning with auxiliary tasks, 2023.</p>
<p>Daniel Flam-Shepherd, Kevin Zhu, and Alán Aspuru-Guzik. Language models can learn complex molecular distributions. Nature Communications, 13(1):3293, 2022.</p>
<p>Tianfan Fu, Cao Xiao, Lucas M. Glass, and Jimeng Sun. Moler: Incorporate molecule-level reward to enhance deep generative model for molecule optimization. IEEE Transactions on Knowledge and Data Engineering, 34(11):5459-5471, 2022. doi: 10.1109/TKDE.2021.3052150.</p>
<p>Wenhao Gao, Tianfan Fu, Jimeng Sun, and Connor W. Coley. Sample efficiency matters: A benchmark for practical molecular optimization, 2022.</p>
<p>Miguel García-Ortegón, Gregor NC Simm, Austin J Tripp, José Miguel Hernández-Lobato, Andreas Bender, and Sergio Bacallado. Dockstring: easy molecular docking yields better benchmarks for ligand design. Journal of chemical information and modeling, 62(15):3486-3502, 2022.</p>
<p>Anna Gaulton, Louisa J. Bellis, A. Patricia Bento, Jon Chambers, Mark Davies, Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, Bissan Al-Lazikani, and John P. Overington. Chembl: A large-scale bioactivity database for drug discovery. Nucleic Acids Research, 40(D1):D1100-D1107, January 2012. ISSN 0305-1048. doi: 10.1093/nar/gkr777.</p>
<p>Simon Geisler, Tobias Schmidt, Hakan Şirin, Daniel Zügner, Aleksandar Bojchevski, and Stephan Günnemann. Robustness of graph neural networks at scale, 2023.</p>
<p>Rafael Gó mez-Bombarelli, Jennifer N. Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D. Hirzel, Ryan P. Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Science, 4(2):268-276, jan 2018. doi: 10.1021/acscentsci.7b00572. URL https://doi.org/10.1021\%2Facscentsci.7b00572.</p>
<p>Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268-276, 2018.</p>
<p>Francesca Grisoni, Michael Moret, Robin Lingwood, and Gisbert Schneider. Bidirectional molecule generation with recurrent neural networks. Journal of chemical information and modeling, 60(3): $1175-1183,2020$.</p>
<p>Anvita Gupta, Alex T Müller, Berend JH Huisman, Jens A Fuchs, Petra Schneider, and Gisbert Schneider. Generative recurrent networks for de novo drug design. Molecular informatics, 37 (1-2):1700111, 2018.</p>
<p>David Ha. Reinforcement learning for improving agent design. Artif. Life, 25(4):352-365, November 2019.</p>
<p>Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor, 2018.</p>
<p>Stephen Heller, Alan McNaught, Stephen Stein, Dmitrii Tchekhovskoi, and Igor Pletnev. Inchi-the worldwide chemical structure identifier standard. Journal of cheminformatics, 5(1):1-9, 2013.</p>
<p>Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning, 2016.
Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Comput., 9(8): 1735-1780, nov 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https: //doi.org/10.1162/neco.1997.9.8.1735.</p>
<p>Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W. Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development, 2021.</p>
<p>Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Connor W Coley, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Artificial intelligence foundation for therapeutic science. Nature Chemical Biology, 18(10):1033-1036, 2022.</p>
<p>John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc: a free tool to discover chemistry for biology. Journal of chemical information and modeling, 52 (7):1757-1768, 2012.</p>
<p>Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pretrained transformer for computational chemistry. Machine Learning: Science and Technology, 3 (1):015022, 2022.</p>
<p>Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation, 2018. URL https://arxiv.org/abs/1802.04364.</p>
<p>Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-objective molecule generation using interpretable substructures, 2020.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020.</p>
<p>Shauharda Khadka, Estelle Aflalo, Mattias Marder, Avrech Ben-David, Santiago Miret, Shie Mannor, Tamir Hazan, Hanlin Tang, and Somdeb Majumdar. Optimizing memory placement using evolutionary graph reinforcement learning. arXiv preprint arXiv:2007.07298, 2020.</p>
<p>Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (SELFIES): A 100\% robust molecular string representation. Machine Learning: Science and Technology, 1(4):045024, oct 2020. doi: 10.1088/2632-2153/ aba947. URL https://doi.org/10.1088\%2F2632-2153\%2Faba947.</p>
<p>Mario Krenn, Qianxiang Ai, Senja Barthel, Nessa Carson, Angelo Frei, Nathan C Frey, Pascal Friederich, Théophile Gaudin, Alberto Alexander Gayle, Kevin Maik Jablonka, et al. Selfies and the future of molecular string representations. Patterns, 3(10):100588, 2022.</p>
<p>Aviral Kumar, Amir Yazdanbakhsh, Milad Hashemi, Kevin Swersky, and Sergey Levine. Datadriven offline optimization for architecting hardware accelerators, 2022.</p>
<p>Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline qlearning on diverse multi-task data both scales and generalizes, 2023.</p>
<p>Greg Landrum et al. Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling. Greg Landrum, 8, 2013.</p>
<p>Seul Lee, Jaehyeong Jo, and Sung Ju Hwang. Exploring chemical space with score-based out-ofdistribution generation, 2023.</p>
<p>Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017.
Kevin Sebastian Luck, Heni Ben Amor, and Roberto Calandra. Data-efficient Co-Adaptation of morphology and behaviour with deep reinforcement learning. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura, editors, Proceedings of the Conference on Robot Learning, volume 100 of Proceedings of Machine Learning Research, pages 854-869. PMLR, 2020.</p>
<p>David Mendez, Anna Gaulton, A Patrícia Bento, Jon Chambers, Marleen De Veij, Eloy Félix, María Paula Magariños, Juan F Mosquera, Prudence Mutowo, Michał Nowotka, et al. Chembl: towards direct deposition of bioassay data. Nucleic acids research, 47(D1):D930-D940, 2019.</p>
<p>Santiago Miret, Vui Seng Chua, Mattias Marder, Mariano Phiellip, Nilesh Jain, and Somdeb Majumdar. Neuroevolution-enhanced multi-objective optimization for mixed-precision quantization. In Proceedings of the Genetic and Evolutionary Computation Conference, pages $1057-1065,2022$.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning, 2013.</p>
<p>Henry B. Moss, Daniel Beck, Javier Gonzalez, David S. Leslie, and Paul Rayson. Boss: Bayesian optimization over string spaces, 2020.</p>
<p>AkshatKumar Nigam, Robert Pollice, and Alan Aspuru-Guzik. Janus: Parallel tempered genetic algorithm guided by deep neural networks for inverse molecular design, 2021.</p>
<p>Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de novo design through deep reinforcement learning, 2017.</p>
<p>Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant M. Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell. Stabilizing transformers for reinforcement learning, 2019.</p>
<p>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library, 2019.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.</p>
<p>Stephane Ross, Geoffrey J. Gordon, and J. Andrew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning, 2011.</p>
<p>David E. Rumelhart and James L. McClelland. Learning Internal Representations by Error Propagation, pages 318-362. MIT Press, 1987.</p>
<p>Charles Schaff, David Yunis, Ayan Chakrabarti, and Matthew R Walter. Jointly learning to construct and control agents using deep reinforcement learning. In 2019 International Conference on Robotics and Automation (ICRA), pages 9798-9805. ieeexplore.ieee.org, May 2019.</p>
<p>Robin M Schmidt. Recurrent neural networks (rnns): A gentle introduction and overview. arXiv preprint arXiv:1912.05911, 2019.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.</p>
<p>Cynthia Shen, Mario Krenn, Sagi Eppel, and Alá n Aspuru-Guzik. Deep molecular dreaming: inverse machine learning for de-novo molecular design and interpretability with surjective representations. Machine Learning: Science and Technology, 2(3):03LT02, jul 2021. doi: 10.1088/2632-2153/ac09d6. URL https://doi.org/10.1088\%2F2632-2153\%2Fac09d6.</p>
<p>Joar Skalse, Nikolaus HR Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward hacking. arXiv preprint arXiv:2209.13085, 2022.</p>
<p>Teague Sterling and John Irwin. Zinc 15 - ligand discovery for everyone. Journal of chemical information and modeling, 55, 10 2015a. doi: 10.1021/acs.jcim.5b00559.</p>
<p>Teague Sterling and John J Irwin. Zinc 15-ligand discovery for everyone. Journal of chemical information and modeling, 55(11):2324-2337, 2015b.</p>
<p>Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In S. Solla, T. Leen, and K. Müller, editors, Advances in Neural Information Processing Systems, volume 12. MIT Press, 1999a. URL https://proceedings.neurips.cc/paper_files/paper/1999/file/ 464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf.</p>
<p>Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of the 12th International Conference on Neural Information Processing Systems, NIPS'99, page 1057-1063, Cambridge, MA, USA, 1999b. MIT Press.</p>
<p>Qiaoyu Tan, Ninghao Liu, and Xia Hu. Deep representation learning for social network analysis. Frontiers in big Data, 2:2, 2019.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085, 2022.</p>
<p>Morgan Thomas, Noel O'Boyle, Andreas Bender, and Chris Graaf. Augmented hill-climb increases reinforcement learning efficiency for language-based de novo molecule generation. Journal of Cheminformatics, 14, 10 2022. doi: 10.1186/s13321-022-00646-z.</p>
<p>Austin Tripp, Wenlin Chen, and José Miguel Hernández-Lobato. An evaluation framework for the objective functions of de novo drug design benchmarks. In ICLR2022 Machine Learning for Drug Discovery, 2022. URL https://openreview.net/forum?id=W1tcNQNG1S.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Qi Wang and Chunlei Tang. Deep reinforcement learning for transportation network combinatorial optimization: A survey. Knowledge-Based Systems, 233:107526, 2021.</p>
<p>Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM international conference on bioinformatics, computational biology and health informatics, pages 429-436, 2019.</p>
<p>David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci., 28(1):31-36, feb 1988. ISSN 0095-2338. doi: 10.1021/ci00057a005. URL https://doi.org/10.1021/ci00057a005.</p>
<p>Soojung Yang, Doyeong Hwang, Seul Lee, Seongok Ryu, and Sung Ju Hwang. Hit and lead discovery with explorative rl and fragment-based molecule generation, 2021.</p>
<p>Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning, 2021.</p>
<p>Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation, 2018. URL https://arxiv.org/abs/ 1806.02473 .</p>
<p>Wenhao Yu, C. Karen Liu, and Greg Turk. Policy transfer with strategy optimization, 2018.
Zhenni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nature communications, 13(862), 2022.</p>
<p>Zhenpeng Zhou, Steven Kearnes, Li Li, Richard N. Zare, and Patrick Riley. Optimization of molecules via deep reinforcement learning. Scientific Reports, 9(1), jul 2019. doi: 10.1038/ s41598-019-47148-x. URL https://doi.org/10.1038\%2Fs41598-019-47148-x.</p>
<p>Outline of Appendices. In Appendix A we provide details about the experimental setup. In Appendix B we describe our hyperparameter tuning strategy. In Appendix C we include additional results from our experiments.</p>
<h1>A Experimental setup</h1>
<p>In this section, we provide additional details about the tasks, evaluation metrics and pretraining models and data used in our work.</p>
<h2>A. 1 Tasks</h2>
<p>Pydtc tasks. These tasks are a set of 21 pharmaceutically-relevant oracle functions, which have been commonly used in prior work [Brown et al., 2019, Gao et al., 2022, Huang et al., 2021] for evaluating performance across molecular discovery algorithms:</p>
<ul>
<li>QED: A quantitative estimate of drug-likeness calculated using a set of rules.</li>
<li>DRD2, GSK3 $\beta$, and JNK3: Classical machine learning models (SVMs and random forests) that provide an estimate of properties like target affinity or susceptibility towards a disorder.</li>
<li>Celecoxib, Troglitazone, and Thiothixene rediscovery: An estimate of smiles text similarity, based on tanimoto metric, towards a target molecule.</li>
<li>Albuterol and Mestranol similarity: Generate molecules similar to a target molecule.</li>
<li>Isomers_c7h8n2o2 and isomers_c9h10n2o2pf2cl: Generate molecules corresponding to a target molecular formula.</li>
<li>Median1 and Median2: Generate molecules that are maximally similar to several target molecules.</li>
<li>Osimertinib_mpo, fexofenadine_mpo, ranolazine_mpo, perindopril_mpo, amlodipine_mpo, sitagliptin_mpo, zaleplon_mpo: Generate molecules that maximize multiple properties of a targeted drug.</li>
<li>valsartan_smarts: Generate molecules that contain a certain SMARTS pattern and certain physicochemical properties.</li>
</ul>
<p>Most of these tasks are from the GuacaMol benchmark [Brown et al., 2019]. All oracles are calculated using the Python API provided by Therapeutics Data Commons [Huang et al., 2021] and more details for these tasks can be found on their website.</p>
<p>Docking tasks. We used QuickVina 2 [Alhossary et al., 2015] for calculating docking scores using the same default configuration parameters as prior works [Yang et al., 2021, Lee et al., 2023]. For example, we used exhaustiveness $=1$, and modes $=10$. We choose 5 different protein targets to calculate docking scores: fa7 (FA7), parp1 (PARP-1), 5ht1b (5-HT1B), jak2 (JAK-2), and braf (BRAF). These targets were chosen by [Yang et al., 2021, Lee et al., 2023] because the docking simulators for these targets work fairly well when compared to the ground truth. In our experiments in Section 5.3, we found that text-based RL algorithms were easily able to produce chemically trivial molecules that have very high docking scores. To understand the complexity of computing docking scores, we report the time taken to dock 1000 molecules in parallel using 12 CPUs table 3. We also provide the time taken to run the RL algorithm on these 1000 molecules after their docking scores are available.</p>
<p>Table 3: Time complexity of docking score evaluation : More than half the running time is spent evaluating the docking scores.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Number of molecules</th>
<th style="text-align: center;">Docking time</th>
<th style="text-align: center;">RL update time</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1000</td>
<td style="text-align: center;">130 seconds</td>
<td style="text-align: center;">74 seconds</td>
</tr>
</tbody>
</table>
<p>Augmented docking tasks. In our results for the standard docking tasks (Figure 3 and Figure 4), we found that using the simulated docking scores as rewards did not lead to chemically relevant molecules. Text-based RL algorithms were able to exploit their state and action spaces to design chemically trivial molecules that have very high docking scores. To tackle this issue of undesirable reward hacking, we tried a reward function based on prior works [García-Ortegón et al., 2022, Lee et al., 2023] that combine objectives for drug-like, and synthesizable molecules with docking scores. We call tasks corresponding to this new reward function as augmented docking tasks. Concretely, we chose the same reward function from [Lee et al., 2023]</p>
<p>$$
r(s)=-\mathrm{DS}(s) / 20 \times \mathrm{QED}(s) \times(10-\mathrm{SA}(s)) / 9
$$</p>
<p>Where DS is the docking score, QED and SA are quantitative estimates of drug likeness and synthesizablity respectively.</p>
<h1>A. 2 Evaluation metrics</h1>
<p>Most of the metrics we use are described in detail in Section 5.1. Here, we provide additional details about the diversity metric. We calculate the diversity of the top 100 molecules sampled by the algorithm, where higher diversity is considered better given that it increases the chances for success in further wet lab experiments. In our experiments, we use the diversity evaluator from TDC [Huang et al., 2021], which defines the diversity of a set of molecules as the average pairwise Tanimoto similarity between Morgan fingerprints of the molecules. See Section 2 of [Benhenda, 2017] for exact details of how Tanimoto similarity is calculated.</p>
<h2>A. 3 Pretraining</h2>
<p>In this section, we provide more details about the pretraining datasets and models used in our experiments.</p>
<p>Pretraining datasets. The ZINC 250k dataset contains approximately 250 k molecules from the ZINC database [Irwin et al., 2012], chosen for their pharmaceutical relevance, moderate size, and popularity [Gao et al., 2022]. The CHEMBL dataset [Mendez et al., 2019] consists of approximately 2 M manually curated drug-like molecules. The other 3 datasets consist of randomly selected subsets of the ZINC-15 dataset [Sterling and Irwin, 2015b] that obey some chemically imposed mild constraints [Irwin et al., 2022]. We test three subsets of different sizes: (1) ZINC 1M (2) ZINC 10 M , and (3) ZINC 100 M , to test the impact of scaling the size of pre-training data. These datasets and data-subsets, including their vocabularies, will be shared in an easily accessible format upon acceptance.</p>
<p>Removing outliers and unusual non drug-like compounds helps to keep the vocabulary small and improves the quality of the generative model [Blaschke et al., 2020a]. To achieve this, we filter all datasets by removing molecules which contain 1) less than 10 or more than 50 heavy atoms and 2) molecules other than Carbon, Nitrogen, Oxygen, Fluorine, Silicon, Chlorine and Bromine. We also canonicalize and sanitize all molecules using RDKIT [Landrum et al., 2013]. For experiments that apply SELFIES, we convert all datasets to SELFIES using the Python API provided by [Krenn et al., 2020] (Version: 2.1.1).</p>
<p>Apart from the experiments shown in the main paper, Appendix C. 2 contains additional experiments comparing text-based RL agents across different pretraining datasets.</p>
<p>Pretraining models. In Table 4 we provide details about the pretraining modes which we use in our experiments. Upon acceptance, we will open-source our code and release the pretrained weights to support reproducible research.</p>
<p>We select network sizes that have been commonly used in RL [Blaschke et al., 2020a, Yarats et al., 2021]. Although conducting a study of scaling the model size [Kaplan et al., 2020] is out of the scope of our work, we believe that it is a promising direction for future.</p>
<p>Since the fully connected model can only take fixed length inputs, we always input a molecular text padded to a certain maximum length (we used length 100 in our experiments). This padding is done</p>
<p>Table 4: Description of model architectures used for pretraining</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Number of Parameters</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>FC</td>
<td>$1.07 \times 10^{7}$</td>
<td>FC is a fully connected neural network <br> with 3 hidden layers of 1024 size each.</td>
</tr>
<tr>
<td>RNN</td>
<td>$4.17 \times 10^{6}$</td>
<td>RNN is a recurrent network which consists of <br> 3 GRU layers of hidden sizes 512 each.</td>
</tr>
<tr>
<td>TRANSFORMER</td>
<td>$4.78 \times 10^{6}$</td>
<td>GPT [Brown et al., 2020b] style transformer with 6 layers, <br> 16 heads and 256 embedding dimensions.</td>
</tr>
</tbody>
</table>
<p>using a special token [PAD] to convey that corresponding tokens should not be considered while deciding the value of the text.</p>
<p>Pretraining experimental details. We pretrain FC, RNN and transformer architectures on the ZINC 250 K dataset and pretrain a transformer on all other datasets. All models are pretrained using the PyTorch [Paszke et al., 2019] framework. All models used an initial learning rate of $1 e-3$, with a cosine learning rate schedule [Loshchilov and Hutter, 2017]. FC and RNNs used a batch size of 128 and were trained for 10 epochs. All transformers were trained for 5 epochs, with the largest batch size that we could fit in the memory of a single NVIDIA RTX A6000 GPU, for example, a batch size of 2048 for pretraining the transformer on ZINC 100M dataset. We made sure that all models were trained until convergence. On the ZINC 250K SMILES dataset, the FC, the RNN and the transformer model achieved a validation loss of 29.417, 22.507, and 22.923 respectively.</p>
<h1>A. 4 RL finetuning</h1>
<p>The pretrained model is further trained using the policy gradient algorithm, REINFORCE [Sutton et al., 1999b]. Given the reward function $r\left(s_{H}\right)$ corresponding to the text $s_{T}$, this algorithm optimizes the loss function</p>
<p>$$
\min <em t="1">{\theta} J(\theta)=-\left[\sum</em>\right]\right)\right]
$$}^{H} \log p_{\theta}\left(a_{t}=A_{t} \mid A_{t-1}, \cdots A_{0}\right) r\left(s_{H}=\left[A_{0}, \cdots A_{H</p>
<p>where $A_{t}$ is the token sampled by the agent at time-step $t$.</p>
<h2>B Hyperparameter tuning</h2>
<p>We conduct a common hyperparameter tuning strategy for all experiments. Specifically, we conduct hyperparameter tuning for</p>
<ul>
<li>Learning rate for different architectures Figure 4 and text grammars Figure 3.</li>
<li>Coefficients for different likelihood regularizations Figure 7.</li>
<li>Coefficients for KL regularization loss term Figure 6.</li>
</ul>
<p>We select three tasks from the pytdc tasks, i.e., troglitazone_rediscovery, sitagliptin_mpo, and median2 for hyperparameter tuning. For each hyperparameter, we select a set of 5 evenly spaced realistic values and run 5 random seeds of RL experiments per hyperparameter value. We select the hyperparameter value that achieves the best average score of the top-100 molecules as the final value for running all the experiments. We report the hyperparamters used for the policy gradient training in table 5 .</p>
<h2>C Results</h2>
<h2>C. 1 Text representations and architectures for RL</h2>
<p>Here, we present additional results from subsection 5.3. Figure 8 shows that SMILES are a better molecular grammar when compared to SELFIES across all architectures, for the text based RL</p>
<p>Table 5: Hyperparamters</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Maximum number of unique molecules</td>
<td>25000</td>
</tr>
<tr>
<td>Learning rate</td>
<td>$5.00 \times 10^{-4} \mathrm{RNN}$ and FC</td>
</tr>
<tr>
<td></td>
<td>$1.00 \times 10^{-4}$ Transformer</td>
</tr>
<tr>
<td>Batch size</td>
<td>64</td>
</tr>
<tr>
<td>Log p coefficient</td>
<td>5</td>
</tr>
<tr>
<td>KL coefficient</td>
<td>$1.00 \times 10^{-3}$</td>
</tr>
</tbody>
</table>
<p>algorithms that we consider. Figure 9 compares various architectures, while keeping the molecular grammar fixed to SELFIES. The results in Figure 9 reflect our findings in Figure 4 that no single architecture clearly outperforms for molecular text-based RL. It also shows the reward hacking behavior of the docking tasks by the FC based RL agent.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 8: Comparison between SELFIES and SMILES across different architectures. These figures are the individual plots corresponding to the normalised plot show in Figure 3.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 9: Comparison of different policy architectures (SELFIES): No single architecture clearly outperforms for molecular text-based RL. Although FC does better on the docking tasks, our analysis shows that it learns to exploit the docking function as opposed to designing high-value molecules.</p>
<p>The reason for lower value molecules for SELFIES environments can be explained by the SELFIES grammar that induces a flat optimization landscape. Many SELFIES strings can correspond to the</p>
<p>same molecule, and in fact, once an invalid action is taken, any subsequent sequence of tokens will be ignored on the resulting molecule. This makes exploration of new molecules difficult [Krenn et al., 2020, Gao et al., 2022]. On the other hand, the benefit of SELFIES over SMILES in eliminating invalid molecule generation is mitigated by our pretraining process, which initializes SMILES-based policies with a strong bias toward generating valid molecules. Overall, we find that SMILES-based policies, when combined with pretraining, are more effective at exploring and finding high-value molecules.</p>
<h1>C. 2 Pretraining for RL</h1>
<p>Figure 2 (right) shows the top docking scores obtained by RL agents pre-trained on different datasets when trained with on the augmented docking tasks. In Figure 10, we show the actual augmented rewards obtained by the RL agent. These results suggest that the augmented docking score is a complex reward function as the RL agent is achieved minimal improvement over the prior agent. To verify this hypothesis, we increased the molecule budget of the RL agent by 10 times. We indeed see that RL agents corresponding to all prior-datasets exhibit considerable improvement. Text-based RL algorithms learn to search more efficiently when provided with more compute.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 10: This figure shows the augmented rewards obtained by the RL agents (Top) and data quality (Bottom) of different datasets. See subsection A. 1 for how the augmented reward is calculated.</p>
<h2>C. 3 Reward Hacking</h2>
<p>Figure 4 and Figure 9 show that text-based RL agents that are trained using fully connected neural networks are able to obtain unusually high rewards. This is probably because it is easier for FC agents to find actions that exploit the local structure of the reward function as RNNs and Transformers are inductively biased to find global solutions. This highlights an undesirable type of reward function hacking by the FC agent which provides high rewards for molecules with long strings of Carbon and Nitrogen atoms together. Similar to prior work [Lee et al., 2023], we augment the docking scores with objectives for drug-like and synthesizable molecules. See Appendix A for details of this task and Figure 2 and Figure 10 for results corresponding to this task. Our initial results on this task ( Figure 2 and Figure 10) suggested that the augmented reward function was more aligned towards chemically relevant molecules. We also noticed that the RL agents were not able to improve a lot over the prior baselines for this task. To verify whether the low performance of RL agents was because of less training data or the augmented reward function was indeed a more realistic and robust reward function, we repeated the experiments in Figure 10 with a ten times</p>
<p><img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 11: This figure shows the augmented rewards obtained by the RL agents trained for 10 times more molecules.
higher training budget. Given more data, all RL agents showed considerable improvements over the priors. This experiment also revealed that the agents pre-trained on ZINC 1M, ZINC 10M and ZINC 100M, were able to exploit the reward function to generate unrealistic yet highly rewarding molecules. These molecules have unusually low docking scores (less than -20). Our results highlight the need for an aligned and a more robust reward function to generate molecules for docking protein targets.</p>
<h1>C. 4 Additional results for the importance of algorithmic choices for text-based RL.</h1>
<p>Section 5.4 compares various algorithmic components like replay buffers, hill climbing, KL regularisation towards the pretrained policy, and likelihood penalties and show results for PYTDC tasks. In this section, we repeat all the experiments from Section 5.4 on augmented docking tasks as well and reach the same conclusions. In Figure 14 we see that using the hill-climb buffer results in a significant performance boost, whereas using a standard buffer does not contribute much. Figure 15 shows that Log P regularization is a better choice for efficient exploration when it comes to textbased RL algorithms. In Figure 16 show that penalising the policy to move away from the pretrained policy does not improve performance.</p>
<h2>C. 5 Instability of transformers for online RL.</h2>
<p>Many works [Parisotto et al., 2019] have pointed out the instability of training transformers using online reinforcement learning. To understand this in the context of text based RL, we compare a transformer and an RNN based agent on the augmented docking task. To probe whether pronounced effects of this instability are seen, we train both agents for 10 times more molecules ( 250 K molecules). In figure Figure 12, we see that both agents perform comparably across all docking targets.</p>            </div>
        </div>

    </div>
</body>
</html>
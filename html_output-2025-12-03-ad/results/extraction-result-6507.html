<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6507 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6507</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6507</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-129.html">extraction-schema-129</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-272910569</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.17539v1.pdf" target="_blank">Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory. Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain. To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs. However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results. To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information from input context, and utilizes the generated logical information as an additional augmentation to the input prompts, thereby enhancing the capability of logical reasoning. The LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them. Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks. In particular, the LoT enhances Chain-of-Thought's performance on the ReClor dataset by +4.35%; moreover, it improves Chain-of-Thought with Self-Consistency's performance on LogiQA by +5%; additionally, it boosts performance of Tree-of-Thoughts on ProofWriter dataset by +8%.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6507.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6507.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that extracts propositional logic from context, expands expressions via logical laws, translates them back to natural language, and appends them to the original prompt to augment LLM reasoning and mitigate information loss in neuro-symbolic extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Logic-of-Thought (LoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>augmentation / symbolic-enhanced prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multiple-choice logical reasoning (LSAT/GMAT style deductive questions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>56.02</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Direct prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>9.82</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>LoT preserves the original natural-language context while appending deduced logical statements; authors report that LoT alone substantially outperforms Direct prompting across datasets and often matches or exceeds CoT, showing overlap/partial redundancy with CoT in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6507.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6507.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoT+CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-of-Thought augmented Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Integration of LoT's logic-derived natural-language augmentations with Chain-of-Thought prompting to provide both expanded logical facts and stepwise reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT) with LoT augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential + augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (CoT) augmented by mixed LoT info</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multiple-choice logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>56.52</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>4.35</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report a +4.35% absolute accuracy gain on ReClor (GPT-3.5) when appending LoT output to CoT prompts; they note overlap in capabilities since CoT can sometimes uncover hidden facts that LoT also produces, causing smaller gains when overlap is high.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6507.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6507.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoT+CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-of-Thought + Chain-of-Thought with Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Combines LoT augmentation with CoT-SC: multiple CoT chains are generated and aggregated (majority voting), with LoT logical augmentations included in the prompt generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (CoT-SC) + LoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble (multiple chains) + augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (multiple chains)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>LogiQA</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>expert-written reading-comprehension logical reasoning questions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>39.5</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT-SC(5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>5.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>LoT+CoT-SC gives large improvements on several datasets (e.g., +5.00% absolute on LogiQA with GPT-3.5); authors highlight LoT's orthogonality enabling seamless integration with ensemble reasoning methods like Self-Consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6507.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6507.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoT+CoT-SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-of-Thought + Chain-of-Thought with Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same combined method reported with GPT-4 evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought with Self-Consistency (CoT-SC) + LoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble (multiple chains) + augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (multiple chains)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>LSAT/GMAT-style logical reasoning multiple-choice</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>82.61</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>CoT-SC(5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>2.18</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report LoT+CoT-SC outperforms CoT-SC on most datasets with GPT-4 (ReClor +2.18%); however they point out occasional negative interactions when LoT extraction bias conflicts with high-quality CoT chains.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6507.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6507.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoT+SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-of-Thought + Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LoT augmentations added to multiple Direct prompts aggregated via majority voting (Self-Consistency).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency (SC) + LoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble (majority voting) + augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse (multiple samples)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multiple-choice logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>80.43</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>SC(5)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>6.52</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors emphasize a large +6.52% absolute gain for LoT+SC over SC on ReClor (GPT-4), and note LoT often provides complementary logical facts that ensemble methods can leverage; nevertheless they also note rare cases where extraction errors in LoT harmed performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6507.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6507.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that models reasoning as a tree search exploring multiple branches (thoughts) at each step and allowing backtracking to find solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (ToT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search / branching</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter (depth=5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step deductive reasoning over small rulebases (generate proofs / conclusions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>70.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Direct prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>19.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>ToT substantially outperforms Direct prompting on multi-step ProofWriter reasoning (+19% absolute). Authors report ToT explores multiple states; LoT further increases the number of states and successful states when combined with ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6507.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6507.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LoT+ToT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic-of-Thought augmented Tree-of-Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LoT-generated natural-language logical augmentations are appended to prompts used within the ToT tree-search process to expand the reasoning search space and improve successful state discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Tree-of-Thoughts (ToT) + LoT</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tree-search / branching + augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ProofWriter (depth=5)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multi-step deductive reasoning over rulebases</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>78.0</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>ToT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>8.0</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>LoT+ToT yields an +8% absolute improvement over ToT alone (GPT-4) on ProofWriter with depth 5. Table 2 reports increases in exploration metrics: total states +2.13%, successful states +5.06%, and full-reasoning rate +2%, indicating LoT expands and improves ToT's search space.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6507.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6507.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits multi-step, sequential reasoning chains from LLMs by requesting intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>sequential</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>LSAT/GMAT-style multiple-choice logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>77.39</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Direct prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>3.19</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>CoT improves over Direct prompting by eliciting intermediate reasoning steps; authors note CoT can sometimes deduce hidden facts gradually, which can overlap with LoT's explicit extraction and reduce marginal gains when combined.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6507.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6507.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC(5)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (5 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble technique that samples multiple outputs (here 5) and uses majority voting to select the final answer, increasing robustness of reasoning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Self-Consistency (SC)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>multiple-choice logical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>56.52</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>Direct prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td>10.32</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>SC(5) (majority vote over 5 samples) improves stability over Direct prompting; LoT+SC further boosts performance, demonstrating that augmenting ensemble methods with explicit logical augmentations is beneficial.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6507.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6507.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' reasoning methods, the diversity or similarity of reasoning styles, the tasks or benchmarks used to evaluate them, performance results, and any direct comparisons between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SatLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Satisfiability-aided Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic approach that uses LLMs to produce declarative task specifications and employs automated theorem provers / SAT solvers to derive final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>SatLM (neuro-symbolic, solver-assisted)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>tool-augmented / neuro-symbolic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_style_diversity</strong></td>
                            <td>single style (symbolic pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ReClor (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>LSAT-like deductive reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_target_method</strong></td>
                            <td>LoT / prompting methods</td>
                        </tr>
                        <tr>
                            <td><strong>performance_difference</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_notes</strong></td>
                            <td>Authors report SatLM performed poorly under their experimental setup compared to prompting methods and LoT, attributing degradation to information loss during symbolic extraction (omitted facts) and semantic mistakes when converting natural language to symbols; they provide qualitative case studies illustrating these extraction errors.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_present</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Satlm: Satisfiability-aided language models using declarative prompting <em>(Rating: 2)</em></li>
                <li>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers <em>(Rating: 2)</em></li>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>ReClor: A reading comprehension dataset requiring logical reasoning <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>LogiQA: A challenge dataset for machine reading comprehension with logical reasoning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6507",
    "paper_id": "paper-272910569",
    "extraction_schema_id": "extraction-schema-129",
    "extracted_data": [
        {
            "name_short": "LoT",
            "name_full": "Logic-of-Thought",
            "brief_description": "A prompting method that extracts propositional logic from context, expands expressions via logical laws, translates them back to natural language, and appends them to the original prompt to augment LLM reasoning and mitigate information loss in neuro-symbolic extraction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "reasoning_method_name": "Logic-of-Thought (LoT)",
            "reasoning_method_type": "augmentation / symbolic-enhanced prompting",
            "reasoning_style_diversity": "mixed",
            "benchmark_name": "ReClor",
            "task_description": "multiple-choice logical reasoning (LSAT/GMAT style deductive questions)",
            "performance_metric": "accuracy",
            "performance_value": 56.02,
            "comparison_target_method": "Direct prompting",
            "performance_difference": 9.82,
            "statistical_significance": false,
            "analysis_notes": "LoT preserves the original natural-language context while appending deduced logical statements; authors report that LoT alone substantially outperforms Direct prompting across datasets and often matches or exceeds CoT, showing overlap/partial redundancy with CoT in some cases.",
            "ablation_study_present": false,
            "uuid": "e6507.0",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LoT+CoT",
            "name_full": "Logic-of-Thought augmented Chain-of-Thought",
            "brief_description": "Integration of LoT's logic-derived natural-language augmentations with Chain-of-Thought prompting to provide both expanded logical facts and stepwise reasoning chains.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "reasoning_method_name": "Chain-of-Thought (CoT) with LoT augmentation",
            "reasoning_method_type": "sequential + augmentation",
            "reasoning_style_diversity": "single style (CoT) augmented by mixed LoT info",
            "benchmark_name": "ReClor",
            "task_description": "multiple-choice logical reasoning",
            "performance_metric": "accuracy",
            "performance_value": 56.52,
            "comparison_target_method": "CoT",
            "performance_difference": 4.35,
            "statistical_significance": false,
            "analysis_notes": "Authors report a +4.35% absolute accuracy gain on ReClor (GPT-3.5) when appending LoT output to CoT prompts; they note overlap in capabilities since CoT can sometimes uncover hidden facts that LoT also produces, causing smaller gains when overlap is high.",
            "ablation_study_present": false,
            "uuid": "e6507.1",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LoT+CoT-SC",
            "name_full": "Logic-of-Thought + Chain-of-Thought with Self-Consistency",
            "brief_description": "Combines LoT augmentation with CoT-SC: multiple CoT chains are generated and aggregated (majority voting), with LoT logical augmentations included in the prompt generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "reasoning_method_name": "Chain-of-Thought with Self-Consistency (CoT-SC) + LoT",
            "reasoning_method_type": "ensemble (multiple chains) + augmentation",
            "reasoning_style_diversity": "diverse (multiple chains)",
            "benchmark_name": "LogiQA",
            "task_description": "expert-written reading-comprehension logical reasoning questions",
            "performance_metric": "accuracy",
            "performance_value": 39.5,
            "comparison_target_method": "CoT-SC(5)",
            "performance_difference": 5.0,
            "statistical_significance": false,
            "analysis_notes": "LoT+CoT-SC gives large improvements on several datasets (e.g., +5.00% absolute on LogiQA with GPT-3.5); authors highlight LoT's orthogonality enabling seamless integration with ensemble reasoning methods like Self-Consistency.",
            "ablation_study_present": false,
            "uuid": "e6507.2",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LoT+CoT-SC",
            "name_full": "Logic-of-Thought + Chain-of-Thought with Self-Consistency",
            "brief_description": "Same combined method reported with GPT-4 evaluations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "reasoning_method_name": "Chain-of-Thought with Self-Consistency (CoT-SC) + LoT",
            "reasoning_method_type": "ensemble (multiple chains) + augmentation",
            "reasoning_style_diversity": "diverse (multiple chains)",
            "benchmark_name": "ReClor",
            "task_description": "LSAT/GMAT-style logical reasoning multiple-choice",
            "performance_metric": "accuracy",
            "performance_value": 82.61,
            "comparison_target_method": "CoT-SC(5)",
            "performance_difference": 2.18,
            "statistical_significance": false,
            "analysis_notes": "Authors report LoT+CoT-SC outperforms CoT-SC on most datasets with GPT-4 (ReClor +2.18%); however they point out occasional negative interactions when LoT extraction bias conflicts with high-quality CoT chains.",
            "ablation_study_present": false,
            "uuid": "e6507.3",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LoT+SC",
            "name_full": "Logic-of-Thought + Self-Consistency",
            "brief_description": "LoT augmentations added to multiple Direct prompts aggregated via majority voting (Self-Consistency).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "reasoning_method_name": "Self-Consistency (SC) + LoT",
            "reasoning_method_type": "ensemble (majority voting) + augmentation",
            "reasoning_style_diversity": "diverse (multiple samples)",
            "benchmark_name": "ReClor",
            "task_description": "multiple-choice logical reasoning",
            "performance_metric": "accuracy",
            "performance_value": 80.43,
            "comparison_target_method": "SC(5)",
            "performance_difference": 6.52,
            "statistical_significance": false,
            "analysis_notes": "Authors emphasize a large +6.52% absolute gain for LoT+SC over SC on ReClor (GPT-4), and note LoT often provides complementary logical facts that ensemble methods can leverage; nevertheless they also note rare cases where extraction errors in LoT harmed performance.",
            "ablation_study_present": false,
            "uuid": "e6507.4",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "ToT",
            "name_full": "Tree-of-Thoughts (ToT)",
            "brief_description": "A prompting method that models reasoning as a tree search exploring multiple branches (thoughts) at each step and allowing backtracking to find solutions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "reasoning_method_name": "Tree-of-Thoughts (ToT)",
            "reasoning_method_type": "tree-search / branching",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "ProofWriter (depth=5)",
            "task_description": "multi-step deductive reasoning over small rulebases (generate proofs / conclusions)",
            "performance_metric": "accuracy",
            "performance_value": 70.0,
            "comparison_target_method": "Direct prompting",
            "performance_difference": 19.0,
            "statistical_significance": false,
            "analysis_notes": "ToT substantially outperforms Direct prompting on multi-step ProofWriter reasoning (+19% absolute). Authors report ToT explores multiple states; LoT further increases the number of states and successful states when combined with ToT.",
            "ablation_study_present": false,
            "uuid": "e6507.5",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "LoT+ToT",
            "name_full": "Logic-of-Thought augmented Tree-of-Thoughts",
            "brief_description": "LoT-generated natural-language logical augmentations are appended to prompts used within the ToT tree-search process to expand the reasoning search space and improve successful state discovery.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "reasoning_method_name": "Tree-of-Thoughts (ToT) + LoT",
            "reasoning_method_type": "tree-search / branching + augmentation",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "ProofWriter (depth=5)",
            "task_description": "multi-step deductive reasoning over rulebases",
            "performance_metric": "accuracy",
            "performance_value": 78.0,
            "comparison_target_method": "ToT",
            "performance_difference": 8.0,
            "statistical_significance": false,
            "analysis_notes": "LoT+ToT yields an +8% absolute improvement over ToT alone (GPT-4) on ProofWriter with depth 5. Table 2 reports increases in exploration metrics: total states +2.13%, successful states +5.06%, and full-reasoning rate +2%, indicating LoT expands and improves ToT's search space.",
            "ablation_study_present": false,
            "uuid": "e6507.6",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought",
            "brief_description": "A prompting technique that elicits multi-step, sequential reasoning chains from LLMs by requesting intermediate steps.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "reasoning_method_name": "Chain-of-Thought (CoT)",
            "reasoning_method_type": "sequential",
            "reasoning_style_diversity": "single style",
            "benchmark_name": "ReClor",
            "task_description": "LSAT/GMAT-style multiple-choice logical reasoning",
            "performance_metric": "accuracy",
            "performance_value": 77.39,
            "comparison_target_method": "Direct prompting",
            "performance_difference": 3.19,
            "statistical_significance": false,
            "analysis_notes": "CoT improves over Direct prompting by eliciting intermediate reasoning steps; authors note CoT can sometimes deduce hidden facts gradually, which can overlap with LoT's explicit extraction and reduce marginal gains when combined.",
            "ablation_study_present": false,
            "uuid": "e6507.7",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SC(5)",
            "name_full": "Self-Consistency (5 samples)",
            "brief_description": "An ensemble technique that samples multiple outputs (here 5) and uses majority voting to select the final answer, increasing robustness of reasoning outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_size": null,
            "reasoning_method_name": "Self-Consistency (SC)",
            "reasoning_method_type": "ensemble",
            "reasoning_style_diversity": "diverse",
            "benchmark_name": "ReClor",
            "task_description": "multiple-choice logical reasoning",
            "performance_metric": "accuracy",
            "performance_value": 56.52,
            "comparison_target_method": "Direct prompting",
            "performance_difference": 10.32,
            "statistical_significance": false,
            "analysis_notes": "SC(5) (majority vote over 5 samples) improves stability over Direct prompting; LoT+SC further boosts performance, demonstrating that augmenting ensemble methods with explicit logical augmentations is beneficial.",
            "ablation_study_present": false,
            "uuid": "e6507.8",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "SatLM",
            "name_full": "Satisfiability-aided Language Models",
            "brief_description": "A neuro-symbolic approach that uses LLMs to produce declarative task specifications and employs automated theorem provers / SAT solvers to derive final answers.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5-turbo-instruct",
            "model_size": null,
            "reasoning_method_name": "SatLM (neuro-symbolic, solver-assisted)",
            "reasoning_method_type": "tool-augmented / neuro-symbolic",
            "reasoning_style_diversity": "single style (symbolic pipeline)",
            "benchmark_name": "ReClor (subset)",
            "task_description": "LSAT-like deductive reasoning",
            "performance_metric": "accuracy",
            "performance_value": null,
            "comparison_target_method": "LoT / prompting methods",
            "performance_difference": null,
            "statistical_significance": false,
            "analysis_notes": "Authors report SatLM performed poorly under their experimental setup compared to prompting methods and LoT, attributing degradation to information loss during symbolic extraction (omitted facts) and semantic mistakes when converting natural language to symbols; they provide qualitative case studies illustrating these extraction errors.",
            "ablation_study_present": false,
            "uuid": "e6507.9",
            "source_info": {
                "paper_title": "Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Satlm: Satisfiability-aided language models using declarative prompting",
            "rating": 2,
            "sanitized_title": "satlm_satisfiabilityaided_language_models_using_declarative_prompting"
        },
        {
            "paper_title": "Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers",
            "rating": 2,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        },
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "ReClor: A reading comprehension dataset requiring logical reasoning",
            "rating": 2,
            "sanitized_title": "reclor_a_reading_comprehension_dataset_requiring_logical_reasoning"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "LogiQA: A challenge dataset for machine reading comprehension with logical reasoning",
            "rating": 2,
            "sanitized_title": "logiqa_a_challenge_dataset_for_machine_reading_comprehension_with_logical_reasoning"
        }
    ],
    "cost": 0.01553225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models
26 Sep 2024</p>
<p>Tongxuan Liu tongxuan.ltx@mail.ustc.edu.cn 
University of Science
Technology of China</p>
<p>Wenjiang Xu xuwenjiang2024@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Weizhe Huang 
University of Science
Technology of China</p>
<p>Xingyu Wang wangxingyu2024@ia.ac.cn 
Institute of Automation
Chinese Academy of Sciences</p>
<p>Jiaxing Wang wangjiaxing41@jd.com 
Hailong Yang hailong.yang@buaa.edu.cn 
Beihang University</p>
<p>Jing Li 
University of Science
Technology of China</p>
<p>Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in Large Language Models
26 Sep 20244AC778D9469DC66973D44AC653E0F6FCarXiv:2409.17539v1[cs.CL]Logical Propositions: A: It's ChristmasB: Cindy's hair is braidedC: It's a holidayD: Today is Easter ABDBBCCD
Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks but their performance in complex logical reasoning tasks remains unsatisfactory.Although some prompting methods, such as Chain-of-Thought, can improve the reasoning ability of LLMs to some extent, they suffer from an unfaithful issue where derived conclusions may not align with the generated reasoning chain.To address this issue, some studies employ the approach of propositional logic to further enhance logical reasoning abilities of LLMs.However, the potential omissions in the extraction of logical expressions in these methods can cause information loss in the logical reasoning process, thereby generating incorrect results.To this end, we propose Logic-of-Thought (LoT) prompting which employs propositional logic to generate expanded logical information from input context, and utilizes the generated logical information as an additional augmentation to the input prompts, thereby enhancing the capability of logical reasoning.The LoT is orthogonal to existing prompting methods and can be seamlessly integrated with them.Extensive experiments demonstrate that LoT boosts the performance of various prompting methods with a striking margin across five logical reasoning tasks.In particular, the LoT enhances Chain-of-Thought's performance on the ReClor dataset by +4.35%; moreover, it improves Chain-of-Thought with Self-Consistency's performance on LogiQA by +5%; additionally, it boosts performance of Tree-of-Thoughts on ProofWriter dataset by +8%.</p>
<p>Introduction</p>
<p>In recent years, Large Language Models (LLMs) have demonstrated excellent capabilities across various NLP tasks [1,2,23].However, even the most advanced LLMs exhibit limited performance in mathematics and complex logical reasoning tasks [3,14].Chain-of-Thought (CoT) prompting [11,28,17] has emerged as a promising approach to improve logical reasoning capabilities, which enhances reasoning abilities by adding intermediate steps in the reasoning process.Subsequent research, such as [5,6,29], has sought to simulate human reasoning processes by expanding the Chain-of-Thought into more complex reasoning topology.Tree-of-Thoughts (ToT) [29] extends into a tree-like reasoning topology, exploring more reasoning branches at each step and supporting backtracking.Graph-of-Thoughts (GoT) [5] supports a graph reasoning topology, allowing for Figure 1: Comparison between LINC and LoT.The left part involves the workflow of LINC, which converts problems into logical expressions and then derives results using Prover9.However, LINC loses fact information Person(Harry) and Book(Walden), leading to errors.On the right side, LoT generates supplementary logical information "If a person reads a book, that person become smarter" that is seamlessly integrated into the original prompt, thereby enhancing the capability of LLMs to produce accurate results.the aggregation of multiple thoughts into new ones.STaR [33] and Chain-of-Thought with Self-Consistency (CoT-SC) [27] generate multiple chains of thought or reasoning paths, selecting the most optimized and consistent answers from these.However, [4,12,16,24] observe that LLMs occasionally exhibit unfaithful reasoning, wherein the derived conclusions do not adhere to the previously generated reasoning chain.</p>
<p>To tackle the challenge of the unfaithfulness in the reasoning process, researchers have proposed many neuro-symbolic methods that integrate LLMs with symbolic reasoning, such as Faithful Chainof-Thought [16], LINC [18], Logic-LM [20] and SatLM [30].These methods follow a similar process: Initially, the problem and objectives are translated into symbolic expressions.Subsequently, symbolic results are derived through external tools such as symbolic solvers.Finally, it's optional to explain symbolic results using LLMs or interpreters.However, these existing neuro-symbolic methods inevitably suffer from the issue of information loss, which results from omissions in the extraction of logical expressions and directly leads to incorrect intermediate reasoning processes.As illustrated in the Figure 1, in the extraction process of logical expressions in LINC, two key pieces of hidden information "Harry is a person" and "Walden is a book" are lost, which makes it impossible for the symbolic solver Prover9 to obtain the correct reasoning result.</p>
<p>To address the issue of information loss, in this paper, we propose a novel Logic-of-Thought (LoT) prompting method.Specifically, LoT extracts propositions and logical expressions from the input context, expands these expressions according to logical reasoning laws, translates the expanded logical expressions back into natural language, and utilizes the extended logical descriptions as additional augmentation to the input prompts for LLMs.The LoT prompting preserves the original input prompt while appending logical information described in natural language to guide the LLM's reasoning.Thus, LoT prompting prevents complete dependence on the symbolic solver and also avoids the potential issue of information loss inherent in existing methods' symbolic extraction processes.Additionally, the LoT prompting approach is compatible and orthogonal to existing prompting methods, enabling seamless integration of these methods.To validate the effectiveness of LoT, we conduct extensive experiments to evaluate its capability in boosting various prompting methods such as CoT, SC, CoT-SC and ToT across five logical reasoning datasets.Experimental results demonstrate that LoT prompting can seamlessly integrate with existing prompting methods and significantly boost their performance in logical reasoning.Specifically, LoT significantly enhances the performance of CoT on the ReClor dataset, achieving an improvement in accuracy up to +4.35%.Furthermore, LoT improves the SC's performance on the ReClor dataset by a remarkable +6.52%.Moreover, LoT boost the accuracy of CoT-SC on LogiQA by +5%.Additionally, LoT effectively elevates the performance of ToT on the ProofWriter dataset, resulting in a significant improvement of +8%.</p>
<p>The main contributions of this paper are as follows:</p>
<ol>
<li>
<p>We propose a novel prompting method Logic-of-Thought (LoT) to address the issue of information loss in existing neuro-symbolic methods by generating logical proposition descriptions as augmentations for original prompts.</p>
</li>
<li>
<p>We integrate LoT with a variety of distinct prompting techniques, including Chain-of-Thought (CoT), Self-Consistency (SC), Chain-of-Thought with Self-Consistency (CoT-SC), Tree-of-Thoughts (ToT), by leveraging the orthogonal capabilities of LoT.</p>
</li>
<li>
<p>We conduct extensive experiments to evaluate the effectiveness of LoT in enhancing the capabilities of different prompting techniques across diverse logical reasoning tasks.The results demonstrate the significant effectiveness of LoT in boosting the performance of various prompting methods.</p>
</li>
</ol>
<p>Preliminary</p>
<p>As this study focuses on logical reasoning tasks, we first provide some definitions and symbols about the propositional logic system, which will be used throughout the paper.</p>
<p> Propositions are defined as declarative sentences that have clear truth-value characteristic and cannot be simultaneously true and false.In this context, propositions are considered fundamental elements of logical expressions.We use standard uppercase letters such as A, B, C to symbolize specific propositions, exemplified by statements like "you have keyboarding skills", and lowercase letters such as p, q, r to refer to any proposition.</p>
<p> Connectives are defined as operators on propositions, which can operate on a single proposition or link propositions together to form a new logical expression, which is defined as a single proposition or a combination of propositions through connectives.In this study, We mainly focus on three connectives: ,  and .Herein, negative  denotes the negation operation for a specific logical symbol (e.g., p represents the negation of p).Implication  signifies a sufficient condition or causal relationship between two propositions (e.g., p  q indicates that p is a sufficient condition for q).Conjunction  also operates on two propositions, which represents that the entire expression is true only if both propositions are true (e.g., p  q indicates p and q).</p>
<p> Logical reasoning laws are defined as the deducing relation between two logical expressions.Meanwhile,  signifies that a certain logical expression can infer another logical expression. signifies that two logical expressions can be mutually inferred.In this study, we utilize three basic logical reasoning laws: the Double Negation Law p  p, the Contraposition Law (p  q)  (q  p), and the Transitive Law (p  q)  (q  r)  (p  r), which all align with human intuition and are fundamental and widely used in propositional logic [7].</p>
<p>With these basic symbols and definitions, we can construct a propositional logic system and analyze complex logical reasoning processes.While the logic system setting presented here is straightforward, our paper primarily concentrates on introducing a new prompting paradigm to address information loss in existing neuro-symbolic methods.Moreover, notable enhancements have already been achieved within this setting (See Section 4.4).Therefore, we leave the exploration of incorporating more diverse connectives and laws to strengthen the logic system in our method to future work.</p>
<p>Logical Reasoning Laws:</p>
<p>Transitive Law: (p  q)  (q  r)  (p  r) Double Negation Lawp  p Contraposition Law: (p  q)  (q  p)</p>
<p>Extended Logical Expressions: C  A C  A:</p>
<p>If you are able to write your essays using a word processing program, then you have keyboarding skills.</p>
<p>Logical Deduction:</p>
<p>(A  B)  (B  C)  (A  C)
(A  C)  (C  A)</p>
<p>LLMs Input</p>
<p>Figure 2: The framework of LoT consisting of three phases.On the left side of the diagram is the Logic Extraction phase, where we employ LLMs to extract propositions and logical relations.In the middle is the Logic Extension phase, where we apply logical reasoning laws to derive logical expressions.On the right side is the Logic Translation phase, where we utilize LLMs to translate logical expressions into their natural language descriptions.phase, the expanded logical expressions are translated into natural language descriptions of logical information through LLMs.And then, the logical information is incorporated into the input prompt, forming a comprehensive and novel input prompt for LLMs.The following sections provide detailed introduction to the phases of Logic Extraction, Logic Extension, and Logic Translation.</p>
<p>Logic Extraction.In the Logic Extraction phase, we use LLMs to extract formal logic expressions from the input context through two stages.Firstly, we instruct LLMs to select sentences containing conditional reasoning relationships from the input context to generate collection of sentences with logical relationships.Subsequently, we use LLMs to extract the set of propositional symbols P and the set of logical expressions E from the collection.During the process of Logic Extraction, LLMs identify propositions with similar meanings and represent them using identical propositional symbols.Furthermore, LLMs analyze the logical relationships between propositions from their natural language descriptions.For propositions expressing opposite meanings, the negation  is added.When a conditional relationship exists between two propositions, the implication  is used to connect their corresponding propositional symbols.For example, as depicted in Figure 2, LLMs extract the same meaning description "be able to use a computer" from two different sentences, symbolized as B.Then, through analyzing its logical relationship with other propositions, LLMs apply  to B and another proposition A and add  between them, which results in a new logical expression A  B.</p>
<p>Logic Extension.During the Logic Extension phase, we apply logical reasoning laws to the collection of logical expressions from the Logic Extraction phase.These logical expressions can be further expanded using a Python program to implement logical deduction.As illustrated in the Figure 2, the extracted logical expressions A  B and B  C serve as inputs for our logical deduction program.Through expansion based on Transitive Law and Contraposition Law, we finally obtain the new expression C  A, which will be used in the next phase.</p>
<p>Logic Translation.During the Logic Translation phase, we use LLMs to translate the generated extended logical expressions into natural language descriptions.Subsequently, we combine the natural language descriptions of propositional symbols according to the extended logical expressions to form a new part of the original input prompt.Through this approach, we inject the deduced logical information as additional augmentation into the original prompt, thus avoiding information loss.As shown in Figure 2, by associating C with its description "be able to write your essays using a word processing program", A with its description "have keyboarding skills", and  with the logical description "if...then...", we can translate the aforementioned logical expression C  A back to its natural language description and add it to original prompts as new input prompts.</p>
<p>Experiments</p>
<p>Tasks and Datasets</p>
<p>In the experiment, we employ five logical reasoning datasets: ReClor [32], LogiQA [15], RuleTaker [8], ProofWriter [22], and FOLIO [9].</p>
<p> The ReClor dataset is collected from standardized test logical reasoning questions, including the Law School Admission Test (LSAT) and the Graduate Management Admission Test (GMAT).Each question is composed of a context, a question, and four answer options, with only one correct answer. The LogiQA dataset is derived from expert-written questions for testing human logical reasoning, specifically the reading comprehension section, which is designed by domain experts to evaluate the logical reasoning ability of test participants. The RuleTaker dataset is automatically generated via programming, utilizing connectives such as conjunction , negation , and implication .Each question comprises a context and a conclusion. The ProofWriter dataset comprises numerous small rulebases composed of facts and rules, and contains RuleTaker-style datasets with 500k questions, answers and proofs over naturallanguage rulebases. The FOLIO dataset is a comprehensive and diverse dataset designed for reasoning in natural language, which is characterized by its human annotations, open-domain nature, and logical complexity.It boasts first-order logic (FOL) annotations, comprising 1,435 unique examples of conclusions.</p>
<p>Baselines</p>
<p>We consider five prompting approaches and one neuro-symbolic method, including: (1) Direct prompting, which simply answers the question without any reasoning process; (2) Self-Consistency (SC) [27], which employs majority voting to aggregate responses from multiple Direct prompting, represented as SC(5) when it involves 5 reasoning paths; (3) CoT [11,28,17], which utilizes a progressive thinking approach for reasoning; (4) CoT-SC [27], which applies majority voting to aggregate multiple Chain-of-Thought, represented as CoT-SC(5) when involving 5 reasoning paths;</p>
<p>(5) ToT [29], which models the reasoning process as a thought search tree; (6) SatLM [30], which leverages automated theorem provers to assist LLMs in reasoning.We conduct an evaluation of our LoT and all of the above baselines, and assess the integration of four prompting methods with LoT: CoT, SC, CoT-SC and ToT.</p>
<p>Experiment Setup</p>
<p>We utilize three pre-trained models: GPT-3.5-turbo-instruct,GPT-3.5-turbo[19], and GPT-4 [1] and conduct three types of experiments for comparison:</p>
<p>Main experiments.Main experiments employ four prompting methods including Direct, CoT, SC, CoT-SC and combination of these prompting methods with LoT on GPT-3.5-turbo and GPT-4 across five datasets.The experiments conduct here utilize the zero-shot prompt, employing default values for temperature, top_p, and top_k.For ReClor, we select 46 data entries from the dataset that examine deductive reasoning abilities in the implementation section.Regarding LogiQA, we randomly extract 200 data entries within the dataset.As for RuleTaker, we randomly select 200 data entries from the test set of the dataset, with the depth of reasoning randomly distributed between 1-5 layers.Concerning ProofWriter, we randomly extract 200 data entries with a depth of 5 from the validation set of the dataset.In relation to FOLIO, we extract 100 data entries from the validation set of the dataset.</p>
<p>Comparison between LoT and SatLM.We conduct a comparative analysis of SatLM's performance, benchmarking it against LoT, LoT+CoT, and LoT+CoT-SC using the ReClor dataset.Consistent with the approach outlined in [30], we utilize the GPT-3.5-turbo-instruct to leverage the SatLM implementation.To ensure compatibility with our experimental setup, we select a subset of the ReClor datasets, specifically a subset containing 46 data entries, as they closely mirror the LSAT dataset previously tested in [30].</p>
<p>Comparison between LoT and ToT.In this study, we evaluate the performance enhancement achieved by LoT under the guidance of ToT on the ProofWriter dataset, leveraging GPT-4.The ToT prompt employed in our analysis is based on the work presented in [31].In the experiments, the Direct, ToT, and LoT+ToT approaches are all implemented using few-shot prompting.For the ToT-related experiments, each successful state explores up to five new states.The success or failure of a state is verified by assessing its compliance with the established rules.The exploration process terminates either after achieving four successful state explorations or when no new states are available for exploration.The experiment utilizes 100 randomly selected data entries with a depth of 5 from the ProofWriter validation set.</p>
<p>A detailed description of the prompts used in all experiments can be found in the Appendix E.</p>
<p>Main Results</p>
<p>In this section, we integrate LoT prompting with four baseline prompting methods, namely Direct, CoT, SC and CoT-SC, to conduct a comparative analysis of whether LoT enhances logical reasoning abilities across five distinct datasets.The results presented in Table 1 reveal some key observations:</p>
<p> Combining LoT with existing prompting methods consistently achieves best performance, which highlights the superiority of our methods.Specifically, LoT+CoT-SC(5) outperforms all other methods across all five datasets with GPT-3.5-turbo and four datasets with GPT-4.</p>
<p>LoT+SC achieves the highest accuracy rate in the FOLIO dataset with GPT-4.</p>
<p> LoT prompting enhances the performance of four baseline prompting methods in most experiments, suggesting that LoT can be seamlessly integrated into existing prompting methods to further improve the logical reasoning ability of LLMs.Among total 40 comparisons (including four baseline prompting methods across five datasets with two LLMs), LoT significantly enhances the performance of baseline prompting methods in 35 instances.For example, in comparison to CoT using GPT-4, LoT+CoT can achieve great improvements in accuracy on five datasets, ReClor(+1.74%),LogiQA(+2.10%),RuleTaker(+3.20%),ProofWriter(+0.90%)and FOLIO(+0.40%),respectively.</p>
<p> We find that a few negligible improvements all occur when CoT or CoT-SC is integrated with LoT.We conduct an analysis of specific examples of CoT and CoT+LoT on the dataset and observe that this is attributed to CoT's capacity to gradually deduce concealed information, which interestingly overlaps with the capabilities of LoT.(The Appendix D illustrates an example of the overlap in capabilities between CoT and LoT.)</p>
<p> Upon utilizing GPT-4 exclusively on the RuleTaker and ProofWriter datasets, we observe that LoT+SC marginally trailed behind SC, recording a decline of 1% and 1.5% respectively.We find that the primary factor contributing to this issue within the LoT framework is the deviation in logical information extracted during the Logic Extraction process.To illustrate this, we provide an example in the Appendix C, analyzing a failure case where</p>
<p>Extended context:</p>
<p>Any great ape can learn to recognize its reflection in a mirror.Any animal that recognizes its reflection in a mirror has a sense of self.An animal has a sense of the minds of others only if it possesses a sense of self.</p>
<p>Question:</p>
<p>Which one of the following follows logically from the information above?Options: A. All great apes have the ability to develop a sense of the minds of others.B. Any animal that is incapable of developing a sense of self is not a great ape.C. If an animal is not a great ape it is not capable of developing a sense of the minds of others.D. Any animal that does not recognize its reflection in a mirror does not have a sense of self.</p>
<p>Input</p>
<p>LLMs</p>
<p>Extended context:</p>
<p>Any great ape can learn to recognize its reflection in a mirror.Any animal that recognizes its reflection in a mirror has a sense of self.An animal has a sense of the minds of others only if it possesses a sense of self.If a great ape can learn to recognize its reflection in a mirror, then it has a sense of self.</p>
<p>Question:</p>
<p>Which one of the following follows logically from the information above?Options:</p>
<p>A. All great apes have the ability to develop a sense of the minds of others.B. Any animal that is incapable of developing a sense of self is not a great ape.C. If an animal is not a great ape it is not capable of developing a sense of the minds of others.D. Any animal that does not recognize its reflection in a mirror does not have a sense of self.LoT's information extraction during the Logic Extraction phase is flawed.This example demonstrates how the information bias during this phase leads to an erroneous final result.</p>
<p>LoT</p>
<p> LoT achieves significant enhancements in the accuracy of Direct across all datasets and outperforms CoT in eight out of ten sets of comparative data.Thus, this provides compelling evidence that the standalone utilization of LoT can achieve or even exceed the logical reasoning capability exhibited by CoT.In this section, we conduct a comprehensive comparison and analysis of LoT with a neuro-symbolic approach SatLM to delve deeper into their respective capabilities and potential differences.</p>
<p>Comparative Study of LoT and SatLM
6 D W/ 0 ' LU H F W / R 7 &amp; R 7 / R 7 &amp; R 7 6 &amp; / R 7 6 &amp; &amp; R 7 6 &amp; / R 7 &amp; R 7 6 &amp; $FFXUDF\
Performance Study.From Figure 4, it can be first observed that LoT significantly outperforms SatLM in terms of accuracy on the Reclor dataset as well as obtains notable improvements across various prompting methods, including Direct (+1.74%),CoT (+2.18%), and SC (+6.52%), which also shows LoT's effectiveness.But we observe that in this set of experiments, the performance of LoT+CoT-SC is inferior to that of CoT-SC.We speculate that this is due to a bias in the extraction of logical information, which is also discussed in Section 4.4.</p>
<p>Furthermore, SatLM exhibits poor performance under our setup compared to prompting methods.This is in line with our motivation that neuro-symbolic methods like SatLM, are more likely to encounter the issue of information loss when extracting logical symbolic expressions, significantly compromising their overall performance.Conversely, LoT maintains the original context, ensuring that possible minor information loss during the Logic Extraction phase may not necessarily exert a critical impact on the final results.</p>
<p>Case Study.We provide a comparative case study between SatLM and LoT based on our experiments.As depicted in Figure 3 logical extraction, SatLM erroneously employs "abilities" to represent "can", leading to semantic errors in constraints.Additionally, SatLM confuses "has a sense of self" with "has a sense of the minds of others" and only utilizes "possesses" to represent them together.In contrast to SatLM, LoT successfully extracts logical proposition descriptions and symbolizes them.Here, we have a very interesting finding: when directly examining the extracted logical expressions, a small mistake in A  B results in an incorrect A  C (i.e., we cannot infer general "animal" from specific "great ape").However, when translating the deduced logical expressions A  C into natural language, LLMs recognize the subordinate relationship between "ape" and "animal" and correct this error, resulting in correct augmentation to prompts and right answers.This reflects that LoT fully leverages the LLM's understanding of natural language descriptions, enabling it to correct errors from earlier phases in the three-phase process.This avoids the pitfalls of neuro-symbolic methods, which rely entirely on the accuracy of logical symbol extraction, where errors in intermediate results directly propagate to errors in the final outcome.In this experiment, we assess the enhancing effect of LoT on ToT, which is a prompting method characterized by its complex reasoning topology.As shown in Figure 5, we can observe that under the complex reasoning scenario with a deduction depth of 5 in the ProofWriter dataset, Direct only achieves an accuracy rate of 51%, which is nearly the same as random guessing (50%).The accuracy rate of ToT is +19% higher than the Direct prompting, reaching 70%, which shows that ToT can assist LLMs in better solving multi-step reasoning.The accuracy rate of LoT+ToT reaches 78%, an +8% increase in accuracy compared to ToT, indicating that LoT can effectively enhance the ability of ToT in complex logical reasoning.</p>
<p>To further investigate the influence of LoT on ToT, we have carefully analyze a range of indices within ToT, including the total count of states, full reasoning (wherein four successful states explorations are achieved), and the tally of successful states.Firstly, as shown in Table 2, we observe a notable +2.14% increase in the overall states of LoT+ToT compared to ToT.This suggests that LoT facilitates an expanded exploration scope for ToT.Moreover, a higher level of full reasoning indicates a more comprehensively explored space and it is evident that LoT has augmented ToT's full reasoning by +2%.Furthermore, compared to ToT, LoT+ToT exhibits a +2.59% increase in the average number of successful states, indicating that the LoT can significantly enhance the effectiveness of ToT's exploratory states.In the Appendix B, we present a thorough analysis of an example, comparing the exploration of states when using ToT, and LoT+ToT respectively.</p>
<p>5 Related Work</p>
<p>Prompting Approaches for LLMs Reasoning</p>
<p>Numerous studies are dedicated to exploring enhancements in the logical reasoning capabilities of LLMs.CoT prompting [28], which breaks down a multi-step reasoning problem into multiple intermediate steps to gradually generate answers, has significantly improved logical reasoning, mathematical logic, and interpretability.Zero-shot CoT [11] integrates zero-shot learning with CoT, controlling LLMs to generate thought chains for answering questions through prompting.CoT-SC [27] generates multiple thought chains, and the final answer is obtained through majority voting, which significantly enhances the performance of the thought chain method.Least-To-Most [35] prompting deconstructs a problem into multiple sub-questions, addressing them step by step, with the answer to the previous sub-question serving as the input for the next.Similar decomposition methods of sub-problems include Lambada [10] and the Divide-and-Conquer [34].[13] employs a process-supervised method, providing feedback on the intermediate reasoning process to enhance logical reasoning capabilities.[21,33,36] select optimal candidates from multiple chains of thought.</p>
<p>ToT [29] and GoT [5] achieve logical branching and the aggregation of multiple thoughts by utilizing more complex reasoning topologies.</p>
<p>Neuro-symbolic Approaches for LLMs Reasoning</p>
<p>The neuro-symbolic methods, which combine LLMs with symbolic reasoning, are considered an effective approach to address the issue of unfaithful reasoning and enhance the logical reasoning ability of LLMs.LReasoner [26] proposes a framework for context extension that expands the logical information contained in the context by applying logical reasoning laws.Additionally, it utilizes data augmentation algorithms to better capture logical information.LogicAsker [25] proposes an enhancement of LLMs' logical reasoning capabilities, which are based on a set of propositions and collections of predicate logic.Logic-LM [20] initially utilizes LLMs to transform natural language problems into symbolic formulas.Subsequently, a symbolic solver is determined to reason about the formalized problems.Moreover, a self-refinement module is introduced, which utilizes error messages from the symbolic solver to modify the symbolic formalization.The SatLM [30] utilizes LLMs to generate declarative task specifications rather than imperative programs, and leverages readily available automated theorem solver to derive the final answers.In LINC [18], LLMs acts as a semantic parser, translating premises and conclusions from natural language into first-order logic expressions.These expressions are then offloaded to an external theorem solver for deductive reasoning.</p>
<p>Limitations</p>
<p>Although our proposed LoT has achieved excellent performance in various logical reasoning tasks, there are still some limitations in LoT.Firstly, current LoT supports a limited set of connectives and logical reasoning laws.More connectives and logical reasoning laws in LoT means more complex prompt design in the Logic Extraction and Logic Translation phase, and increased difficulty in logical deducing in the Logic Extension phase.In the future, we will try to include additional connectives and logical reasoning laws in LoT to further enhance the logical reasoning capabilities.</p>
<p>Additionally, LoT employs LLMs to extract logical symbols and expressions but illusion issues inherent in LLMs can lead to problems such as repetition of expressions, omission of logical relationships, and deviations in logical propositions and expressions.(In Appendix C, we conduct a comprehensive analysis of an example illustrating a failure in Logic Extraction using LoT, resulting from deviations in extracting logical expressions.)</p>
<p>Conclusion</p>
<p>In this paper, we introduce a symbolic-enhancement prompting approach, named Logic-of-Thought (LoT), designed to address the challenge of information loss inherent in existing neuro-symbolic methods.LoT leverages propositional logic to derive expanded logical information from input context, serving as a supplementary augmentation to the original prompts, to enhance logical reasoning capabilities of LLMs.Notably, LoT exhibits compatibility with widely used prompting techniques, including Chain-of-Thought (CoT), Self-Consistency (SC), Chain-of-Thought with Self-Consistency (CoT-SC), Tree-of-Thoughts (ToT), and can be seamlessly integrated with them.In the experiments, we demonstrate that LoT prompting significantly boosts the performance of various existing prompting methods across multiple logical reasoning datasets.In the future, we will explore more logical relationships and logical reasoning laws and support additional prompting methods to further enhance LoT's logical reasoning capabilities.</p>
<p>A More Details of Main Results</p>
<p>In Table 1, we present the experimental results of integrating LoT with various prompting methods.Table 3 and 4 further provide detailed results including standard deviations using GPT-3.5-turboand GPT-4, respectively.The results demonstrate the effectiveness of our LoT.</p>
<p>Method</p>
<p>B Comparative Study of States in ToT and LoT+ToT</p>
<p>We present a comprehensive analysis of an illustrative example, comparing the exploration of states when utilizing ToT and LoT+ToT.In Figure 6, we can observe that in LoT+ToT, LoT generates the logical description "If things are rough, then things are round", from which ToT further generates 4 successful states.The corresponding premises are: (1)"If Charlie is round, then Charlie is young and nice", (2)"Charlie is not young", (3)"If Charlie is quiet and round, then Charlie is young", (4)"If Charlie is round and rough, then Charlie is white".Subsequently, the generated information by the LoT and ToT serves as an enhancement to the input prompt, enabling LLMs to produce correct results.Compared to using ToT alone, the logical description generated by LoT enables ToT to generate an additional four successful states, which leads to the correct results.This indicates that LoT enhances the total number of states as well as the number of successful states, thereby expanding the reasoning space and improving the accuracy of ToT reasoning.</p>
<p>Mistake in Logic Extraction</p>
<p>D An Example of Overlap Capabilities between CoT and LoT</p>
<p>The following example illustrates the overlap in capabilities between CoT and LoT.# Whether this inference is correct: Harry is smarter than before .</p>
<p>----------------------------------------------------------------------# LoT: ## Logic Extraction: 2. When a person reads a book , that person gains knowledge .## LLM Answer: Correct ----------------------------------------------------------------------# CoT: Let ' s think step by step : Given that Harry read the book " Walden " by Henry Thoreau , it can be concluded that he gained knowledge from reading the book .Therefore , based on the context provided , it is reasonable to conclude that Harry is smarter than before .</p>
<h2>LLM Answer: Correct</h2>
<p>Listing 1: An Example of Overlap Capabilities Between CoT and LoT.</p>
<p>E Full Set of Prompts E.1 Logic Extraction Prompt in LoT</p>
<h1>Logic Extraction Prompt for ReClor and LogiQA: Please use uppercase English letters such as A , B , C , etc .to identify all possible propositions .Do not include negative tones such as " not " in the propositions .For example , if the sentence is " It is not bored ," you should use " A : bored " to represent it .</h1>
<p>Next , for each proposition , use the symbol to represent its negative form .For example , the negative form of proposition A can be expressed as A .</p>
<p>Now , please carefully analyze the context and find causal relationship between propositions seriously .A causal expression is only established when the context directly supports this relationship .Use arrows () to indicate causal relationships , for example , " If A , then B " , " B if A " and " A causes B " etc .can be represented as AB .</p>
<p>Finally , output propositions and causal expressions .</p>
<h1>Logic Extraction Prompt for RuleTaker, ProofWriter and FOLIO: Please use uppercase English letters such as A , B , C , etc .to identify all possible propositions .Do not include negative tones such as " not " in the propositions .For example , if the sentence is " It is not bored ," you should use " A : bored " to represent it .</h1>
<p>Next , for each proposition , use the symbol to represent its negative form .For example , the negative form of proposition A can be expressed as A .Now , please carefully analyze the context and find causal relationship between propositions .A causal expression is only established when the context directly supports this relationship .Use arrows () to indicate causal relationships , for example , " If A , then B " , " B if A " and " A causes B " etc .can be represented as AB .</p>
<p>Finally , output propositions and causal expressions .</p>
<p>E.2 Logic Translation Prompt in LoT</p>
<h1>Logical Translation Prompt for All: Please use the provided propositions to translate each expression into a complete sentence .</h1>
<p>A represents the negation of proposition A , the arrow () represents the causal relationship , and AB represents if A , then B .</p>
<p>Only output the sentences in a paragraph !</p>
<h1></h1>
<p>Declare variables: animals = EnumSort([great_ape, animal]) abilities = Function([animals]  [bool]) possesses = Function([animals]  [bool]) # Constraints: 1. ForAll([a:animals], Implies(abilities(a), possesses(a))) 2. ForAll([a:animals], Implies(possesses(a), abilities(a))) 3. ForAll([a:animals], Implies(possesses(a), abilities(a))) # Which one of the following follows logically from the information above?(A)is_valid(ForAll([a:animals], Implies(abilities(a), possesses(a)))) (B)is_valid(ForAll([a:animals], Implies(Not(abilities(a)), Not(a == great_ape)))) (C)is_valid(ForAll([a:animals], Implies(Not(a == great_ape), Not(abilities(a))))) (D)is_valid(ForAll([a:animals], Implies(Not(possesses(a)), Not(abilities(a)))))Information mistake: Mistakenly using "abilities" to represent "can" from the first sentence, and causing the semantic errors of constraints.Confusing 'has a sense of self' and 'has a sense of the minds of others' together.Only use "possesses" to represent.</p>
<p>Figure 3 :
3
Figure 3: A comparative case of SatLM and LoT from the ReClor dataset.</p>
<p>Figure 4 :
4
Figure 4: Comparison between SatLM and LoT in the ReClor dataset.</p>
<ol>
<li>6 Figure 5 :
65
Figure 5: Comparison between ToT and LoT+ToT in the ProofWriter dataset.</li>
</ol>
<p>Figure 7 :
7
Figure 7: Errors in Logic Extraction using LoT.</p>
<p>3 .
3
If a person gains knowledge , they become smarter .----A: a person reads a book , B : person gains knowledge , C : become smarter A  B, B  C ## Logic Extension: A  C ## Logic Translation: If a person reads a book , that person become smarter .## Extended context: 1. Books contain tons of knowledge .2. When a person reads a book , that person gains knowledge .3. If a person gains knowledge , they become smarter .4. Harry read the book " Walden " by Henry Thoreau . 5. If a person reads a book , that person become smarter .</p>
<p>Figure 2presents an overview of LoT, which consists of three phases.Firstly, in the Logic Extraction phase, propositions and logical relations are extracted from the input context using LLMs to output logical expressions.Secondly, in the Logic Extension phase, the logical expressions are expanded through Python-implemented logical rules.Thirdly, in the Logic Translation If you have no keyboarding skills at all, you will not be able to use a computer.And if you are not able to use a computer, you will not be able to write your essays using a word processing program.Question:If the statements above are true, which one of the following must also be true on the basis of them?
Context:Logical Propositions: A: have keyboarding skills;B: be able to use a computer;C: be able to write your essays using a wordprocessing programOptions:A. If you are not able to write your essays using a wordprocessing program, you have no keyboarding skills.B. If you are able to write your essays using a word processingprogram, you have at least some keyboarding skills.C. If you are not able to write your essays using a wordprocessing program, you are not able to use a computer.D. If you have some keyboarding skills, you will be able towrite your essays using a word processing program.3 Methodology If you have no keyboarding skills at all, you will not be able to use a computer. And if you are not able to use a computer, you will not be able to write your essays using a word processing program. Question: ...... Options: Overview. Context: ......</p>
<p>Extended Context:</p>
<p>If you have no keyboarding skills at all, you will not be able to use a computer.And if you are not able to use a computer, you will not be able to write your essays using a word processing program.If you are able to write your essays using a word processing program, then you have keyboarding skills.</p>
<p>symbol A symbol B symbol C Logical Expressions: A  B , B C Input Logic Extraction Logic Extension Logic Translation Output Question:</p>
<p>If the statements above are true, which one of the following must also be true on the basis of them?Options: A. If you are not able to write your essays using a word processing program, you have no keyboarding skills.B. If you are able to write your essays using a word processing program, then you have at least some keyboarding skills. C. If you are not able to write your essays using a word processing program, you are not able to use a computer.D. If you have some keyboarding skills, you will be able to write your essays using a word processing program.</p>
<p>Table 1 :
1
9.82 35.300.70 61.40 2.60 59.50 5.00 75.00 0.80 77.98 5.81 57.60 2.80 65.30 2.70 66.70 2.00 86.00 3.00 Main results of combining LoT with various prompting methods.The number in green indicates an enhancement in performance, while the number in red signifies a decline in performance.For comprehensive details with standard deviation, refer to the Appendix A.
MethodGPT-3.5-turboGPT-4ReClorLogiQARuleTaker ProofWriter FOLIOReClorLogiQARuleTaker ProofWriter FOLIODirect46.2034.6058.8054.5074.2072.1754.8062.6064.7083.00LoT 56.02 CoT 52.1734.0060.7058.8078.0077.3955.4062.4075.9084.40LoT + CoT56.52 4.35 36.50 2.50 61.60 0.90 61.50 2.7078.00 0.00 79.13 1.74 57.50 2.10 65.60 3.20 76.80 0.9084.80 0.40SC(5)56.5236.6059.0057.5076.0073.9155.5065.5067.5085.00LoT + SC(5)58.70 2.18 38.00 1.40 60.00 1.00 60.00 2.5078.60 2.60 80.43 6.52 58.50 3.00 64.50 1.00 66.00 1.5088.00 3.00CoT-SC(5)58.7034.5065.5061.5080.0080.4356.5063.5080.5084.00LoT + CoT-SC(5) 60.87 2.17 39.50 5.00 65.50 0.00 67.50 6.0083.00 3.00 82.61 2.18 61.00 4.50 66.50 3.00 80.50 0.0086.00 2.00</p>
<p>Table 2 :
2
Comparison of reasoning states between LoT and LoT+ToT.
MethodTotal StatesSuccessful States Full Reasoning(%)ToT18.707.7090LoT+ToT 19.10 2.13%8.09 5.06%92 2%
, SatLM induces information mistakes and loss.Specifically, during</p>
<p>Table 3 :
3
Main results of combining LoT with various prompting methods on GPT-3.5-turbo.The standard deviation is presented and the best results are bold.
GPT-3.5-turboReClorLogiQARuleTaker ProofWriterFOLIODirect46.202.68 34.601.83 58.802.58 54.502.35 74.202.14CoT52.174.56 34.001.50 60.702.69 58.802.06 78.003.85SC(5)56.520.00 36.600.00 59.000.00 57.500.00 76.000.00CoT-SC(5)58.700.00 34.500.00 65.500.00 61.500.00 80.000.00LoT56.023.20 35.301.44 61.401.07 59.502.12 75.002.83LoT + CoT56.522.71 36.501.92 61.601.53 61.501.21 78.002.15LoT + SC(5)58.700.00 38.000.00 60.000.00 60.000.00 78.600.00LoT + CoT-SC(5) 60.870.00 39.500.00 65.500.00 67.500.00 83.000.00MethodGPT-4ReClorLogiQARuleTaker ProofWriterFOLIODirect74.202.14 72.172.24 54.800.93 62.601.02 64.701.21CoT77.392.94 55.401.88 62.401.43 75.901.24 84.401.85SC(5)73.910.00 55.500.00 65.500.00 67.500.00 85.000.00CoT-SC(5)80.430.00 56.500.00 63.500.00 80.500.00 84.000.00LoT77.983.88 57.601.22 65.300.51 66.701.50 86.001.26LoT + CoT79.133.98 57.501.05 65.601.24 76.802.11 84.800.40LoT + SC(5)80.430.00 58.500.00 64.500.00 66.000.00 88.000.00LoT + CoT-SC(5) 82.610.00 61.000.00 66.500.00 80.500.00 86.000.00</p>
<p>Table 4 :
4
Main results of combining LoT with various prompting methods on GPT-4.The standard deviation is presented and the best results are bold.</p>
<p>Bob is rough.Charlie is nice.Charlie is rough.Charlie is white.Dave is not white.Gary is cold.Gary is quiet.Rough, white things are quiet.If Charlie is round and Charlie is quiet then Charlie is rough.All young things are round.Nice things are white.Rough, nice things are cold.If something is round and rough then it is nice.If something is rough then it is young.If things are rough, then things are round.Bob is rough.Charlie is nice.Charlie is rough.Charlie is white.Dave is not white.Gary is cold.Gary is quiet.Rough, white things are quiet.If Charlie is round and Charlie is quiet then Charlie is rough.All young things are round.Nice things are white.Rough, nice things are cold.If something is round and rough then it is nice.If something is rough then it is young.Bob is rough.Charlie is nice.Charlie is rough.Charlie is white.Dave is not white.Gary is cold.Gary is quiet.Rough, white things are quiet.If Charlie is round and Charlie is quiet then Charlie is rough.All young things are round.Nice things are white.Rough, nice things are cold.If something is round and rough then it is nice.If something is rough then it is young.Bob is rough.Charlie is nice.Charlie is rough.Charlie is white.Dave is not white.Gary is cold.Gary is quiet.Rough, white things are quiet.If Charlie is round and Charlie is quiet then Charlie is rough.All young things are round.Nice things are white.Rough, nice things are cold.If something is round and rough then it is nice.If something is rough then it is young.If things are rough, then things are round.If Charlie is round, then Charlie is young and nice.Charlie is not young.If Charlie is quiet and round, then Charlie is young.If Charlie is round and rough, then Charlie is white.Case study of state exploration in ToT and LoT+ToT.C An Error Case of LoT Figure 7 illustrates an instance of inaccuracies in extracting logical information during the Logic Extraction process, leading to erroneous logical expressions and errors in the final outcome.When LoT selects sentences with logical relationships, there are biases in the information extracted by LLMs.The sentences "Today is Easter, but Cindy's hair is still braided", "Cindy's hair is braided, which means it must be a special occasion", and "If it's a holiday, Cindy will most likely have her hair braided" all exhibit inaccuracies.Subsequently, extracted logical expressions, such as D  B, B  C, C  D exhibit errors.These accumulated errors result in erroneous generated logical descriptions and incorrect final outcomes.Cindy only braids her hair on Christmas.-This cannot be true because the given context states that Cindy always braids her hair on Christmas, but today is Easter and her hair is still braided.B. Cindy dislikes braiding her hair since it takes too long.-This could be true, as there is no information given about Cindy's feelings towards braiding her hair.C. Cindy only braids her hair on holidays.-This cannot be true because the given context states that Cindy always braids her hair on Christmas, which is a holiday, but today is Easter and her hair is still braided.D. Cindy never braids her hair during July or August.-This could be true, as there is no information given about Cindy braiding her hair during July or August.Therefore, the correct answer is C. Cindy only braids her hair on holidays.
Failed state Successful state ToT LLMs Answer: False Correct answer: True ToT inference ending ToT Figure 6: Extended expressions: Extended context: Conclusion: Charlie is quiet. New premises of successful states: 1. If Charlie is round, then Charlie is young and nice. 2. Charlie is not young. 3. If Charlie is quiet and round, then Charlie is young. 4. If Charlie is round and rough, then Charlie is white. ToT 1 2 3 4 Extended context: Conclusion: Charlie is quiet. LLMs ToT inference ending Answer: True Correct answer: True LoT+ToT AC, AD, AB, AD, BD, CB 1. If it's Christmas, Cindy will always braid her hair. 2. Today is Easter, but Cindy's hair is still braided. 3. Cindy's hair is braided, which means it must be a special occasion. 4. If it's a holiday, Cindy will most likely have her hair braided. A. Sentences containing conditional reasoning relationships: Context: Conclusion: Charlie is quiet. Context: Conclusion: Charlie is quiet. Answer: 5. Even though it's not Christmas, Cindy's hair is still braided.</p>
<p>In the example, LoT first extracts propositions A, B, and C from the context and identifies the relationships A  B and B  C.Then, it extends to a new expression A  C.This new expression is translated into additional logical information "If a person reads a book, that person becomes smarter".This logical information directly links "Harry read the book" and "become smarter" in the context, helping LLMs correctly infer the answer.CoT's reasoning process involves first deriving proposition B from proposition A based on the second sentence, then deriving proposition C from proposition B based on the third sentence, ultimately arriving at the answer.We can see that both CoT and LoT handle this problem by linking conditional statements and reasoning step by step, indicating that CoT and LoT sometimes have overlapping capabilities.Books contain tons of knowledge .2. When a person reads a book , that person gains knowledge .3. If a person gains knowledge , they become smarter .4. Harry read the book " Walden " by Henry Thoreau .</p>
<h1>Context:1.</h1>
<p>E.3 SatLM Prompt# SatLM Prompt for LSAT: Nine different treatments are available for a certain illness : three antibiotics (F , G , and H ) three dietary regimens (M , N , and O ) and three physical therapies (U , V , and W ) .For each case of the illness , a doctor will prescribe exactly five of the treatments , in accordance with the following conditions : If two of the antibiotics are prescribed , the remaining antibiotic cannot be prescribed .There must be exactly one dietary regimen prescribed .If O is not prescribed , F cannot be prescribed .If W is prescribed , F cannot be prescribed .G cannot be prescribed if both N and U are prescribed .V cannot be prescribed unless both H and M are prescribed .Question : If O is prescribed for a given case , which one of the following is a pair of treatments both of which must also be prescribed for that case ?Implies ( prescribed ( W ) , Not ( prescribed ( F ) ) ) Implies ( And ( prescribed ( N ) , prescribed ( U ) ) , Not ( prescribed ( G ) ) ) Implies ( prescribed ( V ) , And ( prescribed ( H ) , prescribed ( M ) ) ) solve ( Implies ( prescribed ( O ) , And ( prescribed ( U ) , prescribed ( V ) ) ) ) # ( A ) solve ( Implies ( prescribed ( O ) , And ( prescribed ( G ) , prescribed ( V ) ) ) ) # ( B ) solve ( Implies ( prescribed ( O ) , And ( prescribed ( N ) , prescribed ( U ) ) ) ) # ( C ) solve ( Implies ( prescribed ( O ) , And ( prescribed ( U ) , prescribed ( V ) ) ) ) # ( D ) solve ( Implies ( prescribed ( O ) , And ( prescribed ( U ) , prescribed ( W ) ) ) ) # ( E )E.4 ToT Prompt# ToT Prompt used for Final Conclusion: {{# system }} Suppose you are one of the greatest AI scientists , logicians and mathematicians .Let us think step by step .Read and analyze the " Premises " first , then judge whether the " Hypothesis " is True , False .Please make sure your reasoning is directly deduced from the " Premises " and " Propositions " other than introducing unsourced common knowledge and unsourced information by common sense reasoning .----{{/ system }} {{~# each examples }} {{# user }} ---" Premises ": "{{ this .premises }}" " Hypothesis ": "{{ this .conclusion }}" {{/ user }} {{# assistant }} " Thoughts ": " Let us think step by step .From the premises , we can deduce propositions : {{ this .propositions }}" {{/ assistant }} {{# assistant }} " Reasoning ": " Let us think step by step , {{ this .reasoning }}" {{/ assistant }} {{# assistant }} " Recall the Hypothesis ": "{{ this .conclusion }}" {{/ assistant }} {{# assistant }} " Judgement ": " Now we know that the Hypothesis is {{ this .judgement }}{{/ assistant }} {{~/ each }} {{# user }} ---" Premises ": "{{ premises }}" " Hypothesis ": "{{ conclusion }}" {{/ user }} {{# assistant }} " Thoughts ": " Let us think step by step .From the premises , we can deduce propositions : {{ propositions }}" {{/ assistant }} {{# assistant }} " Recall the Hypothesis ": "{{ conclusion }}" {{/ assistant }} {{# assistant }} " Reasoning ": " Let us think step by step , {{/ assistant }} {{# assistant }} {{ gen " reasoning " temperature =0.7 max_tokens =300 stop =[ ' textbackslash n ']}}{{/ assistant }} {{# assistant }} " Recall the Hypothesis ": "{{ conclusion }}" {{/ assistant }} {{# assistant }} " Judgement ": " Now we know that the Hypothesis is {{/ assistant }} {{# assistant }} {{ gen " judgement " temperature = temperature max_tokens =1 stop = ' textbackslash n '}} {{/ assistant }} # ToT Prompt used for Generate Proposition: {{# system }} Suppose you are one of the greatest AI scientists , logicians and mathematicians .Let us think step by step .Please use Logical Reasoning Rules ( LRR ) to deduce a " Proposition " from two given " Premises " and the proposition does not include " if ". Logical Reasoning Rules ( LRR ) : 1. " Two premises ": " If A , then B .A is true ."then " Proposition ": " B is true ." 2. " Two premises ": " If A , then B .B is not true ."then " Proposition ": " A is not true " 3. " Two premises ": " A is either C or D .A is not C ." then " Proposition ": " A is D ."Please make sure that the " Proposition " is logically correct .Please make sure that the " Proposition " is not a duplicate of the " Premises ".Please make sure your reasoning is directly deduced from the " Premises " and " Propositions " other than introducing unsourced common knowledge and unsourced information by common sense reasoning .Please remember that your " Proposition " should be useful to determine whether the " Hypothesis " is True , False .----{{# system }} {{~# each examples }} {{# user }} ---'' Premises ": "{{ this .premises }}" We want to deduce more propositions to determine the correctness of the following " Hypothesis ": '' Hypothesis ": "{{ this .conclusion }}" Can you deduce a new " Proposition " from at least two given " Premises "? {{# user }} {{# assistant }} " Proposition ": "{{ this .propositions }}" {{/ assistant }} {{~/ each }} {{# user }} ---Premises ": "{{ this .premises }}" We want to deduce more propositions to determine the correctness of the following " Hypothesis ": '' Hypothesis ": "{{ this .conclusion }}" Can you deduce a new " Proposition " from at least two given " Premises "? {{# user }} {{# assistant }} " Proposition ": " {{/ assistant }} {{# assistant }} {{ gen " proposition " temperature = temperature max_tokens =50 stop = ' textbackslash \n '}} {{/ assistant }} # ToT Prompt used for Validate Deduction: {{# system }} Suppose you are one of the greatest AI scientists , logicians and mathematicians .Let us think step by step .Please use the Logical Reasoning Rules ( LRR ) to determine whether the deduction of the given " Premises " to a " Proposition " is valid or not , reply with True or False .Logical Reasoning Rules ( LRR ) : 1. " Two premises ": " If A , then B .A is true ."then " Proposition ": " B is true ." 2. " Two premises ": " If A , then B .If B , then C ." then " Proposition ": " If A , then C ." 3. " Two premises ": " If A , then B .B is not true ."then " Proposition ": " A is not true " 4. " Two premises ": " A is either C or D .A is not C ." then " Proposition ": " A is D ."----{{/ system }} {{~# each examples }} {{# user }} ---" Premises ": "{{ this .premises }}" " Proposition ": "{{ this .propositions }}" {{/ user }} {{# assistant }} " Judgement ": " Is this deduction valid ?{{ this .validation }}" {{/ assistant }} {{~/ each }} {{# user }} ---" Premises ": "{{ premises }}" " Proposition ": "{{ propositions }}" {{/ user }} {{# assistant }} " Judgement ": " Is this deduction valid ?{{/ assistant }} {{# assistant }} {{ gen " validation " temperature = temperature max_tokens =1 stop = ' textbackslash n '}} {{/ assistant }} # ToT Prompt used for sourced deduction: {{# system }} Suppose you are one of the greatest AI scientists , logicians and mathematicians .Let us think step by step .Please determine whether the " Proposition " is directly deduced from the " Premises " with certainty other than introducing unsourced information by common sense reasoning , reply with True or False .----{{/ system }} {{~# each examples }} {{# user }} ---" Premises ": "{{ this .premises }}" " Proposition ": "{{ this .propositions }}" {{/ user }} {{# assistant }} " Judgement ": " Is this proposition directly deduced from the premises ?{{ this .sourced }}" {{/ assistant }} {{~/ each }} {{# user }} ---" Premises ": "{{ premises }}" " Proposition ": "{{ propositions }}" {{/ user }} {{# assistant }} " Judgement ": " Is this proposition directly deduced from the premises ?{{/ assistant }} {{# assistant }} {{ gen " sourced " temperature = temperature max_tokens =1 stop = ' textbackslash n '}}{{/ assistant }}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.104032023Palm 2 technical report. arXiv preprint</p>
<p>Konstantine Arkoudas, arXiv:2308.03762Gpt-4 can't reason. 2023arXiv preprint</p>
<p>Llms with chain-of-thought are non-causal reasoners. Guangsheng Bao, Hongbo Zhang, Linyi Yang, Cunxiang Wang, Yue Zhang, arXiv:2402.160482024arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwaniewski, Jrgen Mller, Lukas Gianinazzi, arXiv:2401.14295Topologies of reasoning: Demystifying chains, trees, and graphs of thoughts. 2024arXiv preprint</p>
<p>Propositional logic: deduction and algorithms. Hans Kleine, Bning , Theodor Lettmann, 1999Cambridge University Press48</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence. the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Natural language reasoning with first-order logic. 2022arXiv preprint</p>
<p>Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran, arXiv:2212.13894Lambada: Backward chaining for automated reasoning in natural language. 2022arXiv preprint</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Measuring faithfulness in chain-of-thought reasoning. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, arXiv:2307.137022023arXiv preprint</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023arXiv preprint</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yue Zhang, arXiv:2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023arXiv preprint</p>
<p>Logiqa: A challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, arXiv:2007.081242020arXiv preprint</p>
<p>Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, Chris Callison-Burch, arXiv:2301.13379Faithful chain-of-thought reasoning. 2023arXiv preprint</p>
<p>Show your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.001142021arXiv preprint</p>
<p>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Alex Theo X Olausson, Benjamin Gu, Cedegao E Lipkin, Armando Zhang, Joshua B Solar-Lezama, Roger Tenenbaum, Levy, arXiv:2310.151642023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Yang, Wang , The 2023 Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Automatic prompt augmentation and selection with chain-of-thought from labeled data. Kashun Shum, Shizhe Diao, Tong Zhang, arXiv:2302.128222023arXiv preprint</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Llama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023arXiv preprint</p>
<p>Language models don't always say what they think: unfaithful explanations in chain-of-thought prompting. Miles Turpin, Julian Michael, Ethan Perez, Samuel Bowman, Advances in Neural Information Processing Systems. 202436</p>
<p>Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-Tse Huang, Pinjia He, Wenxiang Jiao, Michael R Lyu, arXiv:2401.00757A &amp; b== b &amp; a: Triggering logical reasoning failures in large language models. 2024arXiv preprint</p>
<p>Logic-driven context extension and data augmentation for logical reasoning of text. Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, Nan Duan, Findings of the Association for Computational Linguistics: ACL 2022. 2022</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Satlm: Satisfiability-aided language models using declarative prompting. Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett, Advances in Neural Information Processing Systems. 202436</p>
<p>Yang Zhang Yifan, Yuan Jingqin, Yao Yang, Chi-Chih Andrew, arXiv:2308.04371Cumulative reasoning with large language models. 2024arXiv preprint</p>
<p>Reclor: A reading comprehension dataset requiring logical reasoning. Weihao Yu, Zihang Jiang, Yanfei Dong, Jiashi Feng, arXiv:2002.043262020arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Jesse Mu, Noah Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Guiding large language models with divide-and-conquer program for discerning problem solving. Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu, arXiv:2402.053592024arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schrli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, arXiv:2205.106252022arXiv preprint</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, arXiv:2211.019102022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
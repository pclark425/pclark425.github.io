<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-400 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-400</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-400</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-274281342</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.17438v1.pdf" target="_blank">Object-centric proto-symbolic behavioural reasoning from pixels</a></p>
                <p><strong>Paper Abstract:</strong> Autonomous intelligent agents must bridge computational challenges at disparate levels of abstraction, from the low-level spaces of sensory input and motor commands to the high-level domain of abstract reasoning and planning. A key question in designing such agents is how best to instantiate the representational space that will interface between these two levels -- ideally without requiring supervision in the form of expensive data annotations. These objectives can be efficiently achieved by representing the world in terms of objects (grounded in perception and action). In this work, we present a novel, brain-inspired, deep-learning architecture that learns from pixels to interpret, control, and reason about its environment, using object-centric representations. We show the utility of our approach through tasks in synthetic environments that require a combination of (high-level) logical reasoning and (low-level) continuous control. Results show that the agent can learn emergent conditional behavioural reasoning, such as $(A \to B) \land (\neg A \to C)$, as well as logical composition $(A \to B) \land (A \to C) \vdash A \to (B \land C)$ and XOR operations, and successfully controls its environment to satisfy objectives deduced from these logical rules. The agent can adapt online to unexpected changes in its environment and is robust to mild violations of its world model, thanks to dynamic internal desired goal generation. While the present results are limited to synthetic settings (2D and 3D activated versions of dSprites), which fall short of real-world levels of complexity, the proposed architecture shows how to manipulate grounded object representations, as a key inductive bias for unsupervised learning, to enable behavioral reasoning.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e400.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e400.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OBR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Object-centric Behavioural Reasoner</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A brain‑inspired hybrid neural architecture that learns object-centric latent representations from pixels and uses a proto-symbolic preference network plus model-based control to produce conditional, logic-like behaviors (e.g., if-then, XOR, composition) via active inference and variational objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Object-centric Behavioural Reasoner (OBR)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>OBR is a modular hybrid reasoning system with two tightly coupled modules: (1) a perceptual inference module built from K weight-sharing iterative variational autoencoders (itVAEs) that produce object-centric latent 'proto-symbol' slots (means and variances over state and derivative), segmentation masks and predicted sub-images; and (2) an action / reasoning module that (a) maps object-latent beliefs to object-wise preference distributions (desired future latent states) using a set-structured encoder/context/decoder MLP (the 'preference network'), and (b) converts the discrepancy between current and preferred latent trajectories into continuous control using a closed-form model-based planner derived from linearized latent dynamics and a path-integral variational free-energy minimization. Perception and action are trained with a composite ELBO (variational free energy) loss; the preference network is trained self-supervised (map seed frames to target latent frame). Integration is through the shared object-centric latent space and the common variational objective (active inference paradigm).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Proto-symbolic, declarative-like component: object-centric latent 'protosymbols' (slot-based representations) combined with an explicit preference distribution per object (Gaussian μ(k), σ(k) over future s†) produced by a set-structured MLP; these preferences encode conditional rules (if-then style) and act as soft goals (attractors) — a form of declarative goal/prior over states rather than an explicit logic-program or rule-engine.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Imperative/procedural components: neural itVAE perceptual modules (iterative amortized inference with refinement networks and spatial-broadcast decoders), an action-inference refinement network, and a model-based continuous controller (closed-form planner using linearized latent dynamics D and matrix projection U; actions represented as pixel-space action fields and inferred via sampling/Gumbel-softmax), trained with gradient-based optimization (Adam) under the ELBO objective.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular, latent-space integration under a shared variational/free-energy objective: perceptual itVAEs produce object-wise latent beliefs q(s†(k)), which are the sole interface for the preference network that outputs object-wise preferred distributions p(s†) = ∏ N(μ(k),σ(k)^2). Planning minimizes a KL(path q || p(preferences)) in closed form by projecting precision-weighted latent errors onto a pseudo-inverse of U (constructed from linearized dynamics D and horizon T). Action likelihoods are computed by mapping pixel-space action fields to object actions via sampled segmentation assignments (Gumbel-Softmax). Training is staged: world-model (perception + dynamics) is trained unsupervised from videos; the preference network is trained self-supervised on seed→target latent mappings; all losses are ELBO-based (composite over inference iterations). The overall control loop follows Active Inference (variational free energy minimization) rather than conventional reward maximization.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Emergent proto-symbolic conditional reasoning (if-then rules), logical composition ((A→B) ∧ (A→C) ⊢ A→(B∧C)), XOR‑style behavior, and the conversion of inferred symbolic-like goals into continuous control without explicit symbolic programs. Additional emergent behaviors: online adaptation to object substitution, robustness to mild model mismatches, ability to generalize to differing numbers of objects and longer video horizons (with small fine-tuning), interpretable imagined goals decoded back to pixel space, and disentangled latent factors (position, color, size) observed by latent traversals.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Interactive conditional behavioural reasoning in the custom Active dSprites environments (2D and 3D), where tasks require learning conditional rules from pixels and producing continuous control to move objects to target configurations (examples: IfHeart rule, IfHalfTorus, Heart XOR Square, composition tasks, object-substitution adaptation experiments). Also compared on a single conditional rule vs baselines (LQR oracle, SAC, PPO) under full and partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Qualitative and comparative performance reported: OBR successfully learns and executes conditional rules from pixels, showing substantially better performance than pixel-input SAC/PPO baselines (which fail under partial observability), while being less precise than oracles with full state access. Exact numeric MSE time-series are plotted in the paper figures but explicit scalar summary numbers are not reported in the text; therefore numeric values are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td>Reported baselines: (a) Oracle controllers with full object-state access (LQR / model-based) solve the tasks and define an upper bound; (b) SAC and PPO with full observability (object states) solve the conditional task with performance similar to oracle; (c) SAC and PPO with pixel inputs (partial observability) fail to learn the conditional rule (performance degrades). Exact numeric MSEs are shown in figures but not listed as explicit scalar values in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Trained with 3-object episodes and tested on 2–5 objects: OBR generalizes across object counts with a modest drop in accuracy as object number increases; latent-slot architecture permits adding/removing slots without weight changes. Generalizes across 2D↔3D complexity (works in both setups though 3D is harder). Logical composition emerges when separately trained rules are combined at test time. Robust to unexpected mid-episode object substitution (online adaptation), recovering to satisfy preferences after perceptual update. Longer-horizon robustness: trained on 4-frame videos, generalizes to 12-frame videos with slight drop which is remedied by a few epochs of extra training.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Interpretability stems from object-centric slots and preference decoding: object segmentation masks, decoded 'imagined goals' (preference network outputs decoded to pixel space), and latent traversals that reveal interpretable latent dimensions (position, color, size). The declarative-like preferences supply an explicit desired-state distribution per object that can be inspected; however, the 'rules' are emergent in the preference network and not emitted as explicit symbolic formulas or human-readable logic programs, so explanation is partial (visualizable goals and masks, but no symbolic trace of rule application).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Dependence on perceptual module accuracy (reasoner sensitive to perceptual errors); iterative inference is computationally expensive and slower than amortized alternatives; dynamics are linearized in latent space (actions effect modeled linearly via D), limiting planning in highly nonlinear contact/collision regimes and preventing explicit planning through complex collisions; experiments limited to synthetic Active dSprites (2D/3D) and not demonstrated on real-world visual complexity; preference network is trained separately (not end-to-end with task supervision), constraining some forms of fast task adaptation; some causal combinations of behaviors depend on the particular latent features learned and may not emerge reliably; expressivity lower than large symbolic/LLM reasoners for complex language-like reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Grounded in the Free Energy Principle / Active Inference (variational free-energy / ELBO minimization) and an emergentist probabilistic view: object-centric latent slots are treated as proto-symbols that the agent manipulates by minimizing variational free energy with respect to a learned preference distribution; planning is cast as a path-integral minimization (KL between future state beliefs and preference prior) solved in closed form under linearized dynamics. The approach frames declarative preferences as priors (soft goals) and imperative control as minimizing discrepancy under a probabilistic generative model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Object-centric proto-symbolic behavioural reasoning from pixels', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Multi-object representation learning with iterative variational inference <em>(Rating: 2)</em></li>
                <li>Systematic visual reasoning through object-centric relational abstraction <em>(Rating: 2)</em></li>
                <li>Object-centric scene representations using active inference <em>(Rating: 2)</em></li>
                <li>Object-centric compositional imagination for visual abstract reasoning <em>(Rating: 2)</em></li>
                <li>Visual interaction networks: Learning a physics simulator from video <em>(Rating: 1)</em></li>
                <li>Active inference: the free energy principle in mind, brain, and behavior <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-400",
    "paper_id": "paper-274281342",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "OBR",
            "name_full": "Object-centric Behavioural Reasoner",
            "brief_description": "A brain‑inspired hybrid neural architecture that learns object-centric latent representations from pixels and uses a proto-symbolic preference network plus model-based control to produce conditional, logic-like behaviors (e.g., if-then, XOR, composition) via active inference and variational objectives.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Object-centric Behavioural Reasoner (OBR)",
            "system_description": "OBR is a modular hybrid reasoning system with two tightly coupled modules: (1) a perceptual inference module built from K weight-sharing iterative variational autoencoders (itVAEs) that produce object-centric latent 'proto-symbol' slots (means and variances over state and derivative), segmentation masks and predicted sub-images; and (2) an action / reasoning module that (a) maps object-latent beliefs to object-wise preference distributions (desired future latent states) using a set-structured encoder/context/decoder MLP (the 'preference network'), and (b) converts the discrepancy between current and preferred latent trajectories into continuous control using a closed-form model-based planner derived from linearized latent dynamics and a path-integral variational free-energy minimization. Perception and action are trained with a composite ELBO (variational free energy) loss; the preference network is trained self-supervised (map seed frames to target latent frame). Integration is through the shared object-centric latent space and the common variational objective (active inference paradigm).",
            "declarative_component": "Proto-symbolic, declarative-like component: object-centric latent 'protosymbols' (slot-based representations) combined with an explicit preference distribution per object (Gaussian μ(k), σ(k) over future s†) produced by a set-structured MLP; these preferences encode conditional rules (if-then style) and act as soft goals (attractors) — a form of declarative goal/prior over states rather than an explicit logic-program or rule-engine.",
            "imperative_component": "Imperative/procedural components: neural itVAE perceptual modules (iterative amortized inference with refinement networks and spatial-broadcast decoders), an action-inference refinement network, and a model-based continuous controller (closed-form planner using linearized latent dynamics D and matrix projection U; actions represented as pixel-space action fields and inferred via sampling/Gumbel-softmax), trained with gradient-based optimization (Adam) under the ELBO objective.",
            "integration_method": "Modular, latent-space integration under a shared variational/free-energy objective: perceptual itVAEs produce object-wise latent beliefs q(s†(k)), which are the sole interface for the preference network that outputs object-wise preferred distributions p(s†) = ∏ N(μ(k),σ(k)^2). Planning minimizes a KL(path q || p(preferences)) in closed form by projecting precision-weighted latent errors onto a pseudo-inverse of U (constructed from linearized dynamics D and horizon T). Action likelihoods are computed by mapping pixel-space action fields to object actions via sampled segmentation assignments (Gumbel-Softmax). Training is staged: world-model (perception + dynamics) is trained unsupervised from videos; the preference network is trained self-supervised on seed→target latent mappings; all losses are ELBO-based (composite over inference iterations). The overall control loop follows Active Inference (variational free energy minimization) rather than conventional reward maximization.",
            "emergent_properties": "Emergent proto-symbolic conditional reasoning (if-then rules), logical composition ((A→B) ∧ (A→C) ⊢ A→(B∧C)), XOR‑style behavior, and the conversion of inferred symbolic-like goals into continuous control without explicit symbolic programs. Additional emergent behaviors: online adaptation to object substitution, robustness to mild model mismatches, ability to generalize to differing numbers of objects and longer video horizons (with small fine-tuning), interpretable imagined goals decoded back to pixel space, and disentangled latent factors (position, color, size) observed by latent traversals.",
            "task_or_benchmark": "Interactive conditional behavioural reasoning in the custom Active dSprites environments (2D and 3D), where tasks require learning conditional rules from pixels and producing continuous control to move objects to target configurations (examples: IfHeart rule, IfHalfTorus, Heart XOR Square, composition tasks, object-substitution adaptation experiments). Also compared on a single conditional rule vs baselines (LQR oracle, SAC, PPO) under full and partial observability.",
            "hybrid_performance": "Qualitative and comparative performance reported: OBR successfully learns and executes conditional rules from pixels, showing substantially better performance than pixel-input SAC/PPO baselines (which fail under partial observability), while being less precise than oracles with full state access. Exact numeric MSE time-series are plotted in the paper figures but explicit scalar summary numbers are not reported in the text; therefore numeric values are not provided here.",
            "declarative_only_performance": null,
            "imperative_only_performance": "Reported baselines: (a) Oracle controllers with full object-state access (LQR / model-based) solve the tasks and define an upper bound; (b) SAC and PPO with full observability (object states) solve the conditional task with performance similar to oracle; (c) SAC and PPO with pixel inputs (partial observability) fail to learn the conditional rule (performance degrades). Exact numeric MSEs are shown in figures but not listed as explicit scalar values in the text.",
            "has_comparative_results": true,
            "generalization_properties": "Trained with 3-object episodes and tested on 2–5 objects: OBR generalizes across object counts with a modest drop in accuracy as object number increases; latent-slot architecture permits adding/removing slots without weight changes. Generalizes across 2D↔3D complexity (works in both setups though 3D is harder). Logical composition emerges when separately trained rules are combined at test time. Robust to unexpected mid-episode object substitution (online adaptation), recovering to satisfy preferences after perceptual update. Longer-horizon robustness: trained on 4-frame videos, generalizes to 12-frame videos with slight drop which is remedied by a few epochs of extra training.",
            "interpretability_properties": "Interpretability stems from object-centric slots and preference decoding: object segmentation masks, decoded 'imagined goals' (preference network outputs decoded to pixel space), and latent traversals that reveal interpretable latent dimensions (position, color, size). The declarative-like preferences supply an explicit desired-state distribution per object that can be inspected; however, the 'rules' are emergent in the preference network and not emitted as explicit symbolic formulas or human-readable logic programs, so explanation is partial (visualizable goals and masks, but no symbolic trace of rule application).",
            "limitations_or_failures": "Dependence on perceptual module accuracy (reasoner sensitive to perceptual errors); iterative inference is computationally expensive and slower than amortized alternatives; dynamics are linearized in latent space (actions effect modeled linearly via D), limiting planning in highly nonlinear contact/collision regimes and preventing explicit planning through complex collisions; experiments limited to synthetic Active dSprites (2D/3D) and not demonstrated on real-world visual complexity; preference network is trained separately (not end-to-end with task supervision), constraining some forms of fast task adaptation; some causal combinations of behaviors depend on the particular latent features learned and may not emerge reliably; expressivity lower than large symbolic/LLM reasoners for complex language-like reasoning.",
            "theoretical_framework": "Grounded in the Free Energy Principle / Active Inference (variational free-energy / ELBO minimization) and an emergentist probabilistic view: object-centric latent slots are treated as proto-symbols that the agent manipulates by minimizing variational free energy with respect to a learned preference distribution; planning is cast as a path-integral minimization (KL between future state beliefs and preference prior) solved in closed form under linearized dynamics. The approach frames declarative preferences as priors (soft goals) and imperative control as minimizing discrepancy under a probabilistic generative model.",
            "uuid": "e400.0",
            "source_info": {
                "paper_title": "Object-centric proto-symbolic behavioural reasoning from pixels",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Multi-object representation learning with iterative variational inference",
            "rating": 2,
            "sanitized_title": "multiobject_representation_learning_with_iterative_variational_inference"
        },
        {
            "paper_title": "Systematic visual reasoning through object-centric relational abstraction",
            "rating": 2,
            "sanitized_title": "systematic_visual_reasoning_through_objectcentric_relational_abstraction"
        },
        {
            "paper_title": "Object-centric scene representations using active inference",
            "rating": 2,
            "sanitized_title": "objectcentric_scene_representations_using_active_inference"
        },
        {
            "paper_title": "Object-centric compositional imagination for visual abstract reasoning",
            "rating": 2,
            "sanitized_title": "objectcentric_compositional_imagination_for_visual_abstract_reasoning"
        },
        {
            "paper_title": "Visual interaction networks: Learning a physics simulator from video",
            "rating": 1,
            "sanitized_title": "visual_interaction_networks_learning_a_physics_simulator_from_video"
        },
        {
            "paper_title": "Active inference: the free energy principle in mind, brain, and behavior",
            "rating": 1,
            "sanitized_title": "active_inference_the_free_energy_principle_in_mind_brain_and_behavior"
        }
    ],
    "cost": 0.011942999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Object-centric proto-symbolic behavioural reasoning from pixels
November 27, 2024</p>
<p>Ruben Van Bergen 
Donders Institute
Radboud University
NijmegenThe Netherlands</p>
<p>Justus Hübotter 
Donders Institute
Radboud University
NijmegenThe Netherlands</p>
<p>Pablo Lanillos 
Donders Institute
Radboud University
NijmegenThe Netherlands</p>
<p>Cajal International Neuroscience Center
Spanish National Research Council
MadridSpain</p>
<p>Object-centric proto-symbolic behavioural reasoning from pixels
November 27, 20249789C1A2C620B3E4973195FD9140769EarXiv:2411.17438v1[cs.AI]Preprint submitted to under reviewObject-centric reasoningBrain-inspired perception and controlDeep learning architectures
Autonomous intelligent agents must bridge computational challenges at disparate levels of abstraction, from the low-level spaces of sensory input and motor commands to the high-level domain of abstract reasoning and planning.A key question in designing such agents is how best to instantiate the representational space that will interface between these two levels-ideally without requiring supervision in the form of expensive data annotations.These objectives can be efficiently achieved by representing the world in terms of objects (grounded in perception and action).In this work, we present a novel, brain-inspired, deep-learning architecture that learns from pixels to interpret, control, and reason about its environment, using object-centric representations.We show the utility of our approach through tasks in synthetic environments that require a combination of (high-level) logical reasoning and (low-level) continuous control.Results show that the agent can learn emergent conditional behavioural reasoning, such as (A → B) ∧ (¬A → C), as well as logical composition (A → B) ∧ (A → C) ⊢ A → (B ∧ C) and XOR operations, and successfully controls its environment to satisfy objectives deduced from these logical rules.The agent can adapt online to unexpected changes in its environment and is robust to mild violations of its world model, thanks to dynamic internal desired goal generation.While the present results are limited to synthetic settings (2D and 3D activated versions of dSprites), which fall short of real-world levels of complexity, the proposed architecture shows how to manipulate grounded object representations, as a key inductive bias for unsupervised learning, to enable behavioral reasoning.</p>
<p>Introduction</p>
<p>A one-year old infant, before language is expressed [1], learns the efferent-afferent patterns of sensory stimulation and motor commands to a neural representation of the environment.From the sensorium, to abstract thought, and back to the minutiae of the sensorimotor domain, this interaction defines the feats of higher-order cognition and are the backdrop of the different levels of abstraction at which intelligent agents must operate, when they interact with the environment, which is not fully observable [2].How sub-symbolic computations transform into higher-level cognitive representations of structures is far from being understood [3].However, there is common accepted idea that suggest that there are intermediate representations that bridge cognitive reasoning and behaviour [4].Hence, in the design of artificial agents, a key challenge is first to find a representational space that provides an effective interface between these disparate demands and second a mechanism that makes proper use of this representation to interact with the environment and produce behaviour.This is, we define reasoning from a behavioral perspective where the output of a reasoning process is always an action that physically interacts with the environment.</p>
<p>To address these challenges in a single, comprehensive system, here, we introduce a novel, brain-inspired neural network architecture that spans the domains of cognitive reasoning, perceptual inference, planning and continuous control.We follow both the emergentist and probabilistic approach to cognition, where the representational format, and the machinery to transform sensory observations into this space, is learned unsupervised but allows reasoning as an inference process.This also avoids the dependency on ground-truth labels furnished by human annotators, as these are highly labor-intensive to produce, especially when labels must cover all the relevant variables in a scene.The proposed architecture leverages the core inductive bias that the environment can be partitioned into discrete entities, or objects, that obey many useful symmetries and invariances [5].Object-based representations and reasoning are a key tenet of perception and cognition in humans [6].Objects constitute the environment's separable, movable parts, as well as the logical units of reasoning and planning.They naturally take on properties of symbols, as different objects obey the same laws of physics and possess similar or analogous properties [7].At the same time, each object representation is tethered to the sensorimotor domain via explicit attention maps.Objects, thus, are an ideal level of abstraction to interface between the disparate levels of computation that we require.We show the promise of our approach by evaluating it in tasks that require both high-level reasoning and continuous control in synthetically generated environments.</p>
<p>Related work, challenges and contribution</p>
<p>Structured representation learning appeared as a powerful way to introduce inductive biases and scale artificial intelligence to high-level cognition [8] and exploit symmetries and invariances that can be leveraged by decomposing scenes into their natural constituents [9].Particularly, object-centric representations provide a natural interface between bottom-up (e.g., emergentist, connectionist) and top-down (e.g., graph based, probabilistic) approaches [7].We summarize the closed-related works that influenced the proposed architecture classified in four topics 1 : scene understanding (e.g., visual segmentation), physics-informed and interaction (e.g., simulation), behaviour (e.g., robotics) and reasoning (e.g., object relations).We analyse these works from the prism of behavioural reasoning.</p>
<p>Visual segmentation and tracking.Visual segmentation methods have reached maturity with high accuracy on segmentation in both static images [10] and videos [11].From IODINE [10] that used IAI, recent methods evolved to fully amortized slot-attention architectures [12] and diffusion approaches [13].These methods, which can track 2D and 3D objects in cluttered scenes [14], capture object collisions and discover new objects [15], are designed for perception and not for reasoning or control the segmented objects.</p>
<p>Physics-informed and interaction.Physics methods have also shown a great potential in handling complex non-linear interactions.These are strongly influenced by the simulation approach to cognition [4].For instance, interaction [16] and propagation [17] networks are able to understand complex scene dynamics with multiple objects and even control them to reach a desired known state.Unfortunately, these methods require full observation of the objects states.Approaches that can work directly on pixels make use of ground-truth segmentation masks [18] or Visual interaction networks [19].While these approaches have some kind of physical "reasoning" a full-fledged architecture that performs 1st order logic reasoning from pixels and transforms to behaviour (control) is still not fully accomplished.</p>
<p>Robotics.These works focus on moving the objects meaningfully and usually have full access to the objects states (e.g.[20]).OP3 [21] is an outstanding exception.However, it has two key limitations.First, it is restricted to a discrete action space of picking up and placing objects, with no continuous control.Second, it has no ability to learn tasks-it can only plan actions towards objectives specified ad hoc by means of a goal image that shows exactly what the scene in question ought to look like.Novel approaches use ReFs to describe the 3D representation of the object and control is solved by RRTs in the latent prediction dynamics and Model Predictive Control (MPC) to execute the actions [22].Still, ground truth objects mask are used for training and inference.Some methods have overcome this constraint through entity-based segmentation-as a simplification of object-centric representation that uses the center of the object as the location of the entity-and RL to overcome with the generation of meaningful behaviours [23].Unfortunately, this restricts the shape of objects to "point mass" like.Besides, there are object-centric approaches that instead of focusing on the interactive behaviour between the agent and the objects, they focus on view point matching [24] and agent navigation [25].</p>
<p>Reasoning.Leaving out the literature on visual scene understanding with language, reasoning with object-centric representations as neurosymbols is, while the most promising, the least investigated area [26].There are just a few object-centric studies on visual reasoning in static images [27].Furthermore, there are relevant works from Large Language Models research (e.g., [28]), but they do not adhere to the grounding paradigm proposed, where reasoning should appear as an emergent property of learning a world physical model [29].For instance in [30] the reasoning capabilities are much more expressive than our proposed approach but they use pre-trained skill, which are conditioned on the language generated by the LLM.Conversely, our architecture learns the world dynamics and interaction through unsupervised learning, harnessing the construction of the proto-symbols while interacting.This does not prevent the possibility of connecting proposed architecture to a LLMs similarly to [31], but through the preference network, which is already grounded.</p>
<p>Current challenges and architecture decisions</p>
<p>There are two major challenges at the unsupervised learning with object-centric representations: working with complex naturalistic scene images [13] and the generation of meaningful physical actions (i.e., interacting through continuous control) derived from the reasoning process when the input is high-dimensional (i.e., image) and the environment is partially-observable.This work focus on the latter.We show that simple conditional reasoning problems, such as, "if there is a heart move squares to the left, otherwise move squares to the right" ((A → B) ∧ (¬A → C)) is already unsolvable for non-object-centric representations SOTA algorithms-See Sec. 4.An underlying problem for both challenges is to achieve high performance in world state dynamic estimation (world model learning), but allowing adaptation and generalization.We designed the object-centric backbone architecture to incorporate adaptation, following the free energy principle theory of information brain processing [32,33], where the brain estimates the world by continuously approximating its internal model, learn by experience, to the real world through approximate Bayesian computation.To implement it, we opted for an iterative variational inference approach inspired by the IODINE [10] architecture.Whilst iterative methods have shown less performance than amortized approaches (e.g., slot-attention architecture [12]), at least at perceptual tasks, it provides Bayesian filtering and smoothing, thus allowing better counterfactual reasoning.Finally, another relevant challenge, which is not addressed in this work, is to deal with complex non-linearities of physical contact, such as collisions.Physics-based simulation based approaches [18] and Deep Reinforcement Learning (RL) [23] is showing the best results but they still provide the ground truth object masks or entail oversimplified segmentation in the visual input respectively.</p>
<p>Contribution</p>
<p>This work provides a neural network architecture (Fig. 1) that can learn 1st order conditional behavioural reasoning where proto-symbols2 take the form of object-centic representations and perception, control and preferences (desired internal state of the agent) are dynamical process computed through approximate Bayesian inference, in an unsupervised learning scheme from pixels.Our approach draws inspiration on natural intelligence, where the agent function is to generate behaviours through reasoning conditioned on perceptual cues by optimizing a single quantity: the variational free energy (or evidence lower bound) [32].To this end, we combine bottom-up and top-down object-centric approaches [35,36] following the emergentist approach (with unsupervised learning from pixels) but allowing probabilistic inference on the structured representation.This permits the agent to learn: i) what is an object (scene understanding), ii) how to mentally manipulate it through object-centric rules (reasoning) and iii) generate physical behavior (i.e., continuous control).The agent can only perceive through visual input (2D image projection) of a synthetic world composed of 2D or 3D objects (see Fig. 2a for schematic).It can interact with the environment by applying forces to locations in the image, which corresponds to forces in the 2D/3D world.This architecture allows the agent to learn complex conditional rules, such as
(A → B ∧ C) ∧ (¬A → D ∧ E), perform logical composition (A → B) ∧ (A → C) ⊢ A → (B ∧ C
) by learning two different rules separately but then they emerge combined during execution, and XOR operations, such as (A ∨ B) → C) ∧ (A ∧ B → D).Once the agent learns the rules, reasoning transforms into a behavioural response that changes the environment towards its internal preference.Furthermore, the agent, using the proposed architecture, shows online adaptation to unexpected changes in its environment, is robust to mild violations of its world model, and generalize to the number of objects.</p>
<p>Proposed neural network model</p>
<p>The Object-centric Behavioural Reasoner (OBR) proposed, depicted in Fig. 1, is an object-centric deep learning architecture consisting of two interconnected modules: i) perceptual inference, in charge of learning object representations (properties and dynamics) and producing top-down attention and ii) action inference, with the reasoning and control, in charge of learning proto-symbolic rules at the level of object representations and transforming them into meaningful behaviors (i.e., control commands).</p>
<p>Figure 1: OBR model architecture.Illustration of an example instance of OBR perceptual and reasoning interconnected modules with K = 3 slots, and an inference window of two time points.The perceptual module exploits iterative amortized inference (through a refinement network).The action module reasons what is the internal state preference (through online goal imagination) and generates the continuous control actions accordingly to obtain the desired state in the real world (through the minimization of the variational free energy).OBR uses object-centric representations, dynamics, reasoning, and control.For clarity, the computation of object action beliefs is not included here (see Appendix 6).</p>
<p>Perceptual inference and generative model</p>
<p>The perception module infers the object state and action beliefs from incoming sensory data, e.g., one RGB image (frame) per timepoint, and the agent's own motor efferents (see Appendix 6.1).</p>
<p>Object-centric representation.We define the k-th object at time t as a state vector s † (k) t .This vector is expressed in second-order generalized coordinates -that is, it is a concatenation of the current state and its derivative:
s † (k) t = s (k) t T , s ′ (k) t T T
. Objects are influenced by actions from the agent.The action (or action-effect) on object k at time t is denoted by a (k)  t .OBR represents its knowledge about object states and actions through variational beliefs {q(s † (k) t )} and {q(a (k) t )}, which are mean-field Gaussian distributions parameterized by a mean and variance for each latent dimension.</p>
<p>To learn object-centric representations and perform inference from pixels, we use a set of K weight-sharing iterative variational autoencoders (itVAEs) [37,10], where K is the number of object representations to be inferred.These itVAEs together invert a generative model in which image pixels are drawn from a Gaussian mixture distribution:
p(o i |{s (k) } k∈1:K ) = k mik N g i (s (k) ), σ 2 o (1) mik = p(m i |{s (k) } k∈1:K ) = Cat Softmax {π i (s (k) )} k∈1:K (2)
where o i is the value of the i-th image pixel, g i (•) is a decoder function that translates an object state to a predicted mean value at pixel i, σ 2 o is the variability of pixels around their mean values, and π i (•) maps an object state to a logprobability at pixel i, which defines the predicted probability mik that the pixel belongs to that object (its segmentation mask).We implement g i (•) and π i (•) jointly in the decoder of the itVAE, which thus outputs 4 channels per pixel (3 RGB color values + 1 mask logit).Each itVAE thereby predicts its object's subimage and segmentation mask, and a full image prediction can be obtained as a segmentation-weighted superposition of subimages.</p>
<p>Object dynamics.To complete the world model, this per-frame observation model is combined with linearized object-centric dynamics p(s †(k) t |s †(k) t−1 , a (k) t−1 ) defined by the following equations:
s ′(k) t = s ′(k) t−1 + D(a (k) t−1 ) + σ s ϵ 1 t , s (k) t = s (k) t−1 + s ′(k) t + σ s ϵ 2 t(3)
where ϵ 1 t and ϵ 2 t are noise realizations drawn from a Normal distribution.Note that we are assuming that the latent representation dynamics can be captured with a 2nd-order generalized coordinates with additive noise, but this is not imposed to the environment 3 The action a (k) t on object k at time t is a (2-D or 3-D) vector that specifies the control command (e.g., acceleration) on the object in environment coordinates.Finally, D is a learned function that transforms the control command to its effect in the model's latent space.</p>
<p>Learning and inference.Inference for each object is performed within a sliding window spanning the present and a number of past time points.Each itVAE infers beliefs for a single object and time point, taking a total of 8 inference iterations to refine these beliefs to their optimal (minimum-ELBO) values.Inference is coupled between itVAEs through information flow between represented objects and between time points in the inference window (see Appendix 6).OBR optimizes a composite ELBO loss L comp = N iter n=1 n N iter L (n) for both learning and inference, where n indexes inference iterations and L is:
L = − T t=0 H q {s †(k) t , a (k) t } Complexity + βE q({s (k) t }) [log p(o t |{s (k) t })]
Reconstruction accuracy
+ k E q(a (k) t ) [log p(a (k) t |Ψ t )]
Action inference accuracy
+ k E q s †(k) t ,s †(k) t−1 ,a (k) t−1 [log p(s †(k) t |s †(k) t−1 , a (k) t−1 )]
Temporal consistency (4)</p>
<p>Reasoning and control</p>
<p>OBR's generative model allows it to infer the current state of the world, as well as predict future states that would arise as the result of the agent's actions.In a (model-based) RL framework, we could now define rewards and learn a value function on which to base a behavioral policy.Here, we take a slightly different approach, which is seminal in the proposed architecture, based on the influential computational neuroscience framework of Active Inference [39,33].It posits that agents, through their actions, aim to minimize a variational free energy with respect to a probability distribution over states that they prefer to find themselves in [40,41,42].This preference distribution entails a biased (conditional) prior on environmental states, which acts as an attractor for planning future behavior.Once this preference (or soft goal) is learnt the rest of the architecture machinery will drive the system (agent-world) to generate object-centric actions that minimize the discrepancy of the current and the desired state.</p>
<p>Preference network.We assume that the preference distribution is given by p({s
† (k) T &gt;t } k∈1:K |{λ (k) t } k∈1:K ) = k N( μ(k) , σ(k) 2
), where T is some time horizon and t denotes the present.Note that this distribution is conditioned on the agent's internal beliefs, via the variational parameters.We implement this mapping through a set-structured MLP network ϕ (architectural details in Appendix 6.1), which preserves the order of objects from input to output, and is invariant to the number of objects:
ν (k) = ϕ enc (λ (k) ), c = 1 K k W ctx ν (k) + b ctx (5) μ(k) , σ(k) = ϕ dec ν (k) , c(6)
In words, each λ (k) is projected into an embedding space by an encoder network.Object-wise embedding vectors {ν (k) } are then linearly transformed and aggregated into a global context vector c, which is appended to each object embedding.Finally, the concatenated object+context embeddings are passed through a decoder network, to obtain object-wise preference statistics.Conceptually, since OBR determines its current goals by repeatedly applying the same local operation to each object representation (rather than a global operation on the full state of the environment), this may be interpreted as a form of (proto-)symbolic reasoning.Note that the preference network depends on the world model in an unsupervised learning scheme.The latent space geometry that the world model ends up learning is unknown a priori.Therefore, the preference network cannot learn a mapping within this latent space (from current to desired states), before this latent space has been defined.Interestingly, multiple preference models can be trained "on top of" the same world model, allowing fast acquisition of novel tasks within the same environment.Practically, the one main difference to a common value network or critic, is that our preference network does not assign scores to states, but rather furnishes the agent with a desired state conditioned on its current context.This obviates the need to unroll (many) possible futures to evaluate which of these would be more desirable.</p>
<p>Control.Given a preference distribution p(s † ) 4 , the linearized dynamics model enables the agent to plan actions efficiently in closed form (without the need of rollouts).Specifically, the action plan π = a t+1 , . . ., a t+T T is computed by minimizing the path integral of the variational free energy [41] over some future time horizon (similarly to model predictive control) 5 :
π * = argmin π T τ=1 D KL q(s † t+τ |π)|| p(s † ) = (U T LU + λ a I) −1 U T Le(7)
Importantly, this optimization can be performed in closed form, by computing the discrepancy between the preferred state and the sequence of states that will unfold if no action is taken, and projecting this discrepancy (e) onto the pseudo-inverse of a matrix U that maps actions within the planning horizon to their (cumulative) effects over that time window.
L = diag( σ(k) −2
) is a diagonal matrix containing the precisions (inverse variances) of the preference distributions, and λ a controls the strength with which actions are regularized (shrunk) towards to a zero-mean prior.Further details are provided in Appendix 7.1.</p>
<p>Training</p>
<p>The perception module is trained using pre-generated 4-frame videos with sparse actions, to minimize a bespoke ELBO loss that drives the model to reconstruct video frames accurately, while employing representations that are consistent with the dynamics model.Full details on the training procedure may be found in Appendix 6.3.The preference network is trained separately, in a self-supervised fashion, to learn the state-preference mapping-OBR does not copy any behavior from a teacher agent.The model is shown example task episodes where objects start in a random configuration with random velocities, and are simulated for a few frames without any goal-directed interference.In the final two frames, the objects are moved (by an oracle agent) to their target locations (with a final velocity of 0).The perception module encodes each video frame in the latent space, and the preference network is trained to minimize the discrepancy (KL divergence) between its preference prediction for the "seed frames", and the representation of the target frame.</p>
<p>Results</p>
<p>We evaluated OBR qualitatively and quantitatively focusing on analyzing its ability to perform conditional behavioural reasoning using visual cues (Figure 2a).This is, to evaluate the capacity of the agent to learn protosymbolic rules that follow 1st order logic.An exemplary conditional rule is "If there is a Half-torus move Boxes to the Top-Left and cones to the Bottom-Left and Half-torus to Middle-Right otherwise move Boxes and Cones to the Right" 6 .Furthermore, we analyzed its generalization for the number of objects and the adaptation to changes in the environment.As extra result, although it is not the focus of this work, an analysis of world model learning (i.e., segmentation and video prediction) can be found in the Appendix 6.1.1.Once the agent learns the rules, reasoning transforms into a behavioural response that changes the environment towards its internal preference (Fig. 2a). Figure 2c shows an OBR agent solving several conditional reasoning instances with different number of objects in the scene.</p>
<p>Environment</p>
<p>We developed, the Active dSprites, which is an "activated" version of the various multi-dSprites datasets that have been used in previous work on object-based visual inference (e.g.[10,12]).Not only does it include (continuous) dynamics, but these dynamics can can be acted on by an agent (through continuous control actions that accelerate the objects).Thus, active dSprites is an interactive environment, rather than a dataset.We implemented two different scenarios the active dSprites environment.The first, objects in can be 2.5-D shapes (e.g., squares, ellipses and hearts)they have no depth dimension of their own, but can occlude each other within the depth dimension of the image.The second objects are 3D shapes (e.g., boxes, half-torus, cones) that can move in a 10 × 10 × 10 m cubic space.These objects are much more complex to learn than the 2D version, as they have lighting and shadows, producing color gradients that need to be encoded.To randomize the environment, when an active dSprites instance is intialized, object shapes, positions, sizes and colors are all sampled uniformly at random.Initial velocities are drawn from a Normal distribution.Shape colors are sampled at discrete intervals spanning the full range of RGB-colors, while a background color is drawn from a set of evenly spaced grayscale values between black and white.Shapes are presented in random depth order.In the 3D environment the lighting sources are fixed.Code for the active dSprites environment can be found at github.com/neuro-ai-robotics/OBR.</p>
<p>Problem complexity analysis through baselines comparison</p>
<p>We evaluated the problem solving complexity of a basic conditional rule "If Heart then move Boxes to the Right, otherwise move Ellipses to the Left" comparing different oracle and baseline algorithms with our approach 3. We evaluated different algorithms with full observability (i.e., with access to object locations): Linear-Quadratic Regulator control [43,44], (implemented in the Python 3 control systems library), Soft-Actor-Critic (SAC) [45] and Proximal Policy Optimization (PPO) [46] (both used from the stable baselines 3 library [47]), and our OBR with perfect access to the true environmental state).And with partial observability (i.e., the agent only has access to the RGB image (pixels): SAC, PPO and OBR.For RL algorithms we gave dense rewards using the object distance between the current state and the desired state.For the OBR we only allow actions after frame 5.Here objects are 2.5 dimensional and during training the shape, color and positions are randomized and velocities are sampled from a Normal distribution with mean 0. Results are shown in Fig. 3.The left panel shows the task accuracy (Minimum Squarred Error, MSE) of the current object locations and the desired state in projected image) 7 .All methods with full observability can solve the task, having all of them similar performance.When restricting the input to RGB images (pixels) standard SAC or PPO cannot learn the conditional rule.We can see that the MSE increases along the time.OBR, while it is not as accurate as the oracle it can learn the rule.The right panel shows the final performance of the compared algorithms.Oracle algorithms shows the upperbound performance as it is constrained by the dynamics of the environment.</p>
<p>a b</p>
<p>These results first show, in line with previous works [21], that manipulating objects in the environment cannot be solved through non object-centric approaches.And highlights an important weakness of current pixel-based RL baselines.Second, it shows that a basic reasoning rule that implies interaction with the environment is already enough complex.Thus, learning multiple rules and composition in this active dSprite synthetic environment is relevant.</p>
<p>Abstract behavioural reasoning with conditional rules</p>
<p>We evaluated the capacity of the architecture to learn proto-symbolic behavioural rules and its generalization to the number of objects.We evaluated both 2D and 3D scenarios.Figure 4 shows an OBR agent execution for an"IfHeart" conditional (A → B) ∧ (¬A → C) and the XOR rules, and two randomized scenarios for each rule.In Fig. 4a, the first scenario, there is no heart, and boxes should be moved to the upper-left and ellipses to the middle-left.The second one, there is a heart present, and boxes, ellipses and hearts should be moved to the left top, middle and down respectively.The agent is passive until the 5th frame.The environment row shows the visual input and the red arrows describe the forces applied.The imagined goal row, describes the output from the preference network at each frame.Note that the preference is in encoded only in the latent representation and here, for visualization purposes, we show its reconstruction using the decoder network.We let the agent to perform three frames of iterative inference until the proposed goal is shown.The true goal is the idealized solution of this "IfHeart" task.Analogously, in Fig. 4b, the OBR agent, which has learnt the Heart XOR Square task, solves two 2d active dSprites scenarios.Figure 4c,d shows the statistical performance of the OBR for different number of objects for the two conditional rules.The network is trained with three objects and then evaluated on two, three, four and five objects in the scene.Again here, the agent is passive until the fifth frame.The error curves shows that OBR interpret and manipulate the environment using the learnt conditional rule.</p>
<p>Figure 5 shows an OBR similar analysis but in a 3D environment.The 3D scenario is considerably much more complex than the 2D one as objects have an extra dimension to move and colors have lighting.Here, the conditional rule learnt depends on the appearance of a HalfTorus object.The OBR preference network is also decoded to show its visual interpretation.Relevantly, the second instance shows that OBR can deal with the absence of multiples appearances of objects in the scene.There are two HalfTorus and no Cones in Fig. 5a bottom instance.Figure 5b shows the statistical performance of the OBR for different number of objects.The network is again trained with three objects and then evaluated on two, three, four and five objects in the scene.The agent is passive until the fifth frame.The error curves (towards zero) shows that OBR interpret and manipulate the environment using the learnt conditional rule.Figure 2c describes the 16 randomized OBR agent executions in 3D for 2,3,4 and 5 objects in the scence.For completeness, several randomized executions in 3D with different rules can be inspected at the github repository as animations.</p>
<p>Adaptation to changes in the environment</p>
<p>We evaluated the capacity of the OBR agent to recover from unexpected changes in the environment.For that purpose, we generate instances of the active dSprites that perform an object substitution in the middle of the execution.For instance, a Heart is changed by a Square during a conditional rule execution.Figure 6a describes two instances of the adaptation environment and Fig. 6b the statistical performance over 100 randomized instances.The object substitution experiment test the perception module, which needs to adapt fast to estimate the new situation, the preference network, which has to adapt the imagined goal depending on the objects that appear in the scene, and the control module that should provide the correct continuous actions to correct for the previously generated object movements.The true goal describes the idealized desired objects location after object substitution.The task error shows a decrease of performance until the system is able to recover and generate the proper behaviour to fulfill the conditional rule.</p>
<p>Logical composition emergence</p>
<p>We further evaluated the capacity of the agent to do logical composition (A → B) ∧ (A → C) ⊢ A → (B ∧ C), as described in Fig. 7.We trained the preference reasoning module on two rules separately and then we evaluated in the testing phase the capacity of the agent to combine both.</p>
<p>Discussion</p>
<p>We described a brain-inspired deep learning architecture that allows grounded conditional abstract reasoning and behaviour by leveraging object-centric learned representation as the units of "mental" manipulation.Both perceptual and proto-symbolic goals were learnt in an unsupervised fashion from visual information (pixels).Results show that the system can learn and resolve conditional behaviours similar to 1st order logic: simple
A → B, complex (A → B ∧ C) ∧ (¬A → D ∧ E), XOR operation (A ∨ B → C), A ∧ B → D) and logical composition (A → B, A → C ⊢ A → (B ∧ C).
We analysed the model generalization to different number of objects (both in 2D and 3D), objects properties (e.g., color, shapes) and, critically, adaptation to changes in the environment (e.g., object substitution) thanks to the iterative inference and the preference network.OBR shows a promising direction in objects-based behavioral reasoning from pixels, mainly placing the foundations of grounded object-centric representations as a key inductive bias during learning, perception, resoning and control.</p>
<p>Grounded protosymbols and preferences in the latent space</p>
<p>While the agent behaviours from this work are much less complex from by current SOTA LLMs/VLM approaches such as RT architectures [48], researchers in the field are still struggling on how to perform goal-conditioned behavioural reasoning [49].One of the key features of OBR is that it learns grounded representations and manipulate them in the latent manifold as well as it generates the agent preferences (goals) in the object-centric latent representation.This tights agent intentions to its own learning during interaction and constrained by its own learnt world model, bringing benefits but also restrictions as it forces the system to do embodied reasoning: the agents goals are tight to the world and the actions that it can perform in it.</p>
<p>Limitations and future work</p>
<p>Following the brain inspiration, we made some important decisions on the OBR architecture that derived into shortcomings.First, we employed an iterative inference procedure in order to benefit from postdictive inference ("smoothing") in a sliding window approach.However, experimental results showed that the reasoner was highly dependent on the accuracy of the perceptual module.Thus, replacing the perceptual module by amortized methods could improve segmentation performance and reduce training time.Secondly, in the experiments, the effect of the real actions in the latent representation is assumed to be linear.While this could fit a hierarchical interpretation of cognition, it prevents to properly manipulate complex non-linear dynamics.While OBR can recover from collisions, it cannot plan ahead with them.Further work should investigate complex collisions, for instance, by incorporating a brain-inspired version of interaction networks.Furthermore, unsupervised learning of protosymbolic rules is dependant on the object learnt features.This implies that some combination of causal learnt behaviours are not properly resolved in execution.A holistic engineering solution for these limitations may be to incorporate a higher-layer that uses OBR as the interface between symbolic (e.g., graph-based, language) and subsymbolic representations.This will also expand the expressivity of the reasoning, which is currently limited, when comparing to LLMs reasoners.Finally, and very relevant to operationalize this approach for robotics applications the agent needs to include the body restrictions of performing an action in the world [50,29].This can be done by changing the action field mapping by a robotic arm controller.</p>
<p>Appendix</p>
<p>Network architecture</p>
<p>OBR's percpetual inference network consists of two separate Iterative Amortized Inference (IAI) modules, each of which in turn contains a refinement and a decoder module.The first IAI module concerns the inference of the state beliefs q({s †(k) }) -we term this the perceptual inference module.The second IAI module infers the object action beliefs q({a (k) }), and we refer to this as the action inference module.In addition, OBR comprises a preference network that maps the current latent state beliefs to a predicted preference distribution over future object states.Code for the OBR architecture can be found at github.com/neuro-ai-robotics/OBR.</p>
<p>Perceptual inference module</p>
<p>This module used a latent dimension of 16.Note that, in the output of the refinement network, this number is doubled once as each latent belief is encoded by a mean and variance, and then doubled again as we represent (and infer) both the states and their first-order derivatives.In the decoder, the latent dimension is doubled only once, as the state derivatives do not enter into the reconstruction of a video frame.As in [10], we use a spatial broadcast decoder, meaning that the latent beliefs are copied along a spatial grid with the same dimensions as a video frame, and each latent vector is concatenated with the (x, y) coordinate of its grid location, before passing through a stack of transposed convolution layers.Decoder and refinement network architectures are summarized in the tables below.The refinement network takes in 16 image-sized inputs, which are identical to those used in [10], except that we omit the leave-one-out likelihoods.Vector-sized inputs join the network after the convolutional stage (which processes only the image-sized inputs), and consist of the variational parameters and (stochastic estimates of) their gradients.Segmentation and prediction.Figure 8 describes the internal mechanism of the perceptual module with the attentive masks segmentation and the objects prediction in the environment.</p>
<p>Decoder (states)</p>
<p>Prediction robustness.To enable action planning across episodes of non-trivial length, it is important that OBR's perceptual inference and predictive abilities remain stable across longer time windows.In addition, a great benefit of OBR's slot-based architecture is that object slots can be added or removed at will, without having to learn additional connection weights [36].However, it is not a given that performance will be robust to such variations.Here, we test the robustness of OBR's world model, as measured by segmentation and reconstruction accuracy, to both of these dimensions (Fig. 9).Having been trained on 4-frame videos with 3 objects, OBR generalizes well to longer videos with fewer or more objects present in the scene.A slight drop-off in performance towards later frames in the 12-frame testing videos is easily remedied by a very short bout (3 epochs) of additional training with an additional set of longer (also 12-frame) training videos.Generalization to different numbers of objects is characterized by a slight drop in performance as the number increases, but this might also be (partly) attributed to increased complexity of the resulting images.</p>
<p>Action inference module</p>
<p>A key challenge in multi-object environments such as the evaluated environment, is that an agent's internal representation of objects has an unknown (and possibly imperfect) correspondence to the objects in the environment.Even if the object features are inferred with perfect accuracy, the order of the objects in the representation is arbitrary.To solve this correspondence problem, we define an action space in which accelerations can be placed at pixel locations within the environment's image grid, and objects receive the sum of all accelerations that coincide with their visible pixels.Specifically, we introduce the notion of an action field Ψ = [ψ 1 , ..., ψ M ] T : an [M × 2] matrix (with M the number of pixels in an image or video frame), such that the i-th row in this matrix (ψ i ) specifies the (x,y,z)-acceleration applied at pixel i.The action on the k-th object is then given by:
a (k) t = i [m i = k]ψ i(8)
where m i is a categorical variable that indicates which object pixel i belongs to 8 .This definition of actions in pixel space provides an unambiguous interface for the agent to interact with its environment.The action inference module does not incorporate a decoder network, as the quality of the action beliefs is computed by evaluating equation 8 and plugging this into the ELBO loss from equation 4. While this requires some additional sampling operations (see Appendix 7), no neural network is required for this.This module does include a (shallow) refinement network, which is summarized in the table below.This network takes as input the current variational parameters λ a (k) (2 means and 2 variances), their gradients, and the 'expected object action', i mik ψ i .</p>
<p>Refinement network (actions)</p>
<p>No extra training  The preference network consists of encoder and decoder networks that operate on single object representations, as well as a context module that aggregates object embeddings produced by the encoder into a single context vector, which is then broadcast and appended to the object embeddings that are fed into the decoder.Details for the two-layer encoder and decoder networks are listed below.The context module consists of a single Linear layer with input and output sizes both equal to 64, and no activation function.</p>
<p>Minimal stability training</p>
<p>Encoder</p>
<p>Inference procedure</p>
<p>Inference is performed in a recurrent algorithm that loops through the decoder and refinement subnetworks.For a single video frame, the inference starts by initializing the object-wise latent beliefs.For the first frame in an episode, beliefs are initialized to a learned default vector of variational parameters λ 0 .For subsequent frames, beliefs are initialized by extrapolating the inferred dynamics from the previous frame.</p>
<p>From each belief q(s † (k) ), we then (as in [10]) sample a state vector (using the reparameterization trick) and run this through the decoder to obtain, for each object, a predicted sub-image o (k) and segmentation logits m(k) .These decoder outputs are then fed, separately for each object, into the refinement subnetwork, along with additional inputs.Image-sized inputs are fed into the bottom convolutional layer, while vector-sized inputs are appended to the output of the final convolutional layer and processed by the final LSTM and Linear layers.We use the same selection of auxiliary inputs as [10], except that we omit the leave-one-out likelihoods.</p>
<p>Inference across multiple video frames is performed in a sliding window.This window slides "into" and "out of" the full video or episode.That is, the first window includes only the first video frame, which is processed for L iterations, before moving the inference window ahead by one frame.Thus, it takes L × F inference steps before the window reaches its full extent, comprising F frames.The reverse happens when we reach the end of an episode or video.The lagging end of the window keeps advancing until there is only a single frame left within the window, which gets processed for a final L iterations.Thus, each video frame gets processed for exactly L × F inference steps.</p>
<p>Training procedure</p>
<p>The above network architecture was trained on pre-generated experience with the active dSprites environment.The main training set, used to train the world model, comprised 50,000 videos of 4 frames each.An additional validation set of 10,000 videos was sampled identically and independently to the training set.Each video was generated as follows.</p>
<p>First, an instance of the active dSprites environment was randomly initialized, with three objects.Object shapes (square, ellipse or heart) were drawn uniformly.Colors were uniformly sampled from 5 evenly spaced values, independently for the R, G and B channels (a uniform grayscale background color was sampled from the same 5 intensity values).Orientations were sampled uniformly from the interval [0, 2π].Object sizes were sampled uniformly from [ 1  6 ], 1  3 ], where 1 is the size of the image frame.Positions were sampled uniformly from the interval [0.2, 0.8] 2 (where 0 and 1 are the edges of the frame), while velocities were drawn from a Normal distribution with mean 0 and standard deviation 0.0625.Video frames were then generated by simulating 4 frames of these objects' dynamics.Between the 2nd and 3rd frames, a single action (acceleration) was randomly sampled for each object, from a Normal distribution with mean 0 and s.d.0.0625, and placed at a random pixel location within the object's segmentation mask.If an object happened to be completely invisible (e.g. because it was fully occluded by another), then it received no action.An additional action was sampled for a random background pixel, to encourage the model to learn to accurately segment the background into a separate object slot (rather than grouping the background with one of the objects).Training with these 4-frame videos was followed by a brief period of training with longer videos (12 frames, with actions after every second frame) that were otherwise identically sampled.</p>
<p>Additional sets of 50,000 training videos (one for each task; as well as accompanying sets of 10,000 identically and independently sampled validation videos) were used to train OBR's preference network.Each of these videos was 8 frames long and generated as follows.First, an instance of the active dSPrites environment was randomly initialized as before (but with a different procedure for sampling shapes -see below).Then, 6 frames of this environment were simulated with random actions inserted before the 3rd and 5th frames.To prevent objects from leaving the frame (which is likely in longer videos and becomes problematic in this setting), we additionally placed actions as needed to prevent this from happening (specifically, if an object was on course to move to a coordinate outside the range [0.05, 0.95] 2 in the next frame, an action was generated that resulted in the object appearing to "bounce" against an invisible wall instead).Before the 7th and 8th frames, actions were generated that brought the objects to their target positions and velocities within the context of a task (see below).The first of these actions brought the objects to their target positions, while the second decelerated them to 0 velocity.Thus, in the final frame (and only then), the objects reached their target configuration.Each frame was encoded (using the normal inference procedure) as a set of latent beliefs in OBR's latent space, and the preference network was then tasked to map the representation of the first 7 "seed frames" to that of the final target frame, by minimizing the following KL-divergence loss:
L pref = T −1 t=1 k D KL pt s † (k) ||q s † (k) T (9) pt s † (k) = N s † (k) ; μ(k) (λ t ), σ(k) (λ t ) 2(10)
where λ t are the parameters of the variational beliefs for frame t, and we make explicit the fact that the parameters (means and variances) of the predicted preference distribution are a function (via the preference network) of these beliefs about prior frames.Object target positions depended on the task.Specifically, the vertical target coordinates of any square, ellipse and heart shapes in the scene were fixed to 0.2, 0.5 and 0.8, respectively.If, under the task, objects were to go to the left of the frame, their target horizontal coordinate was 0.2 -if they were to go to the right, it was 0.8.Object shapes were pseudo-randomized to evenly sample task conditions.In the IfHeart task, we ensured that training episodes had a 50% probability of containing at least one heart.In the HeartXORSquare task, we sampled four conditions with equal probability: (1) there being neither a heart or a square in the scene; (2) at least one heart but no squares present; (3) at least one square but no hearts present; and (4) at least one heart and one square present in the scene.Outside of these restrictions, shapes were sampled randomly.</p>
<p>Training was performed using the ADAM optimizer [51] with default parameters and an initial learning rate of 3 × 10 −4 .This learning rate was reduced automatically by a factor 3 whenever the validation loss had not decreased in the last 10 training epochs, down to a minimum learning rate of 3 × 10 −5 .Training was performed with a batch size of 64 (16 × 4 GPUs).OBR's world model was deemed to have converged after 241 epochs, which required approximately 24 hours to train on 4 Nvidia A100 GPUs.Preference networks for the IfHeart and HeartXORSquare tasks were trained for 90 and 120 epochs, respectively (18 and 24 hours).</p>
<p>ELBO loss</p>
<p>OBR optimizes the following (weighted) ELBO loss for both learning and inference:
L = − T t=0 H q {s †(k) t , a (k) t } Complexity + βE q({s (k) t }) [log p(o t |{s (k) t })] Reconstruction accuracy + k E q(a (k) t ) [log p(a (k) t |Ψ t )]
Action inference accuracy
+ k E q s †(k) t ,s †(k) t−1 ,a (k) t−1 [log p(s †(k) t |s †(k) t−1 , a (k) t−1 )]
Temporal consistency (11) where H(•) denotes entropy.Similar to previous work (e.g.[10]) we up-weight the reconstruction accuracy term in this loss by a factor β. We train the network to minimize not just the loss at the end of the inference iterations through the network, but a composite loss that also includes the loss after earlier iterations.Let L (n) β be the loss after n inference iterations, then the composite loss is given by:
L comp = N iter n=1 n N iter L (n) (12)
6.3.2.Hyperparameters OBR includes a total of 4 hyperparameters: (1) the loss-reweighting coefficient β (see above); (2) the variance of the pixels around their predicted values, σ 2 o ; (3) the variance of the noise in the latent space dynamics, σ 2 s ; and (4) the variance of the noise in the object actions, σ 2 ψ .The results described in the current work were achieved with the following settings:
Param. Value β 5.0 σ o 0.3 σ s 0.1 σ ψ 0.3 7. Computing E q(a (k) ) [log p(a (k) |Ψ)]
The expectation under q(a (k) ) of log p(a (k) |Ψ), which appears in the ELBO loss (eq.4), cannot be computed in closed form, because the latter log probability requires us to marginalize over all possible configurations of the pixel-to-object assignments, and to do so inside of the logarithm.That is:
log p(a (k) |Ψ) = log        m p(a (k) |Ψ, m)p(m|{s (k) })        (13) = log E p(m|{s (k) }) <a href="14">p(a (k) |Ψ, m)</a>
However, note that within the ELBO loss, we want to maximize the expected value of this quantity (as its negative appears in the ELBO, which we want to minimize).From Jensen's inequality, we have:
E p(m|{s (k) }) [log p(a (k) |Ψ, m)] ≤ log E p(m|{s (k) }) <a href="15">p(a (k) |Ψ, m)</a>
Therefore, the l.h.s. of this equation provides a lower bound on the quantity we want to maximize.Thus, we can approximate our goal by maximizing this lower bound instead.This is convenient, because this lower bound, and its expectation under q(a (k) ) can be approximated through sampling:
E q(a (k) ) E p(m|{s (k) }) [log p(a (k) |Ψ, m)] ≈ 1 N samples j log p(a (k) * j |Ψ, m * j ) (16) = 1 N samples j log N        a (k) * j ; i m * (i) jk ψ i , σ 2 ψ I        (17) m * (i) j ∼ p(m i |{s (k) }), a (k) * j ∼ q(a (k) ), s (k) * j ∼ q(s (k) )(18)
where we slightly abuse notation in the sampling of the pixel assignments, as a vector is sampled from a distribution over a categorical variable.The reason this results in a vector is because this sampling step uses the Gumbel-Softmax trick [52], which is a differentiable method for sampling categorical variables as "approximately one-hot" vectors.Thus, for every pixel i, we sample a vector m * (i) j , such that the k-th entry of this vector, m * (i) jk , denotes the "soft-binary" condition of whether pixel i belongs to object k.In practice, we use N samples = 1, based on the intuition that this will still yield a good approximation over many training instances, and that we rely on the refinement network to learn to infer good beliefs.The Gumbel-Softmax sampling method depends on a temperature τ, which we gradually reduce across training epochs, so that the samples gradually better approximate the ideal one-hot vectors.</p>
<p>It is worth noting that, as the entropy of p(m|{s (k) }) decreases (i.e. as object slots "become more certain" about which pixels are theirs), the bound in equation 15 becomes tighter.In the limit as the entropy goes to 0, the network is perfectly certain about the pixel assignments, and so the distribution collapses to a point mass.The expectation then becomes trivial, and so the two sides of eq. 15 become equal.Sampling the pixel assignments is equally trivial in this case, as the distribution has collapsed to permit only a single value for each assignment.In short, at this extreme point, the procedure becomes entirely deterministic.In our data, we typically observe very low entropy for p(m|{s (k) }), and so we likely operate in a regime close to the deterministic one, where the approximation is very accurate.</p>
<p>Planning</p>
<p>OBR's linearized dynamics model allows us to find an optimal action plan π * (minimizing equation 7) in closed form, as follows:
d (T ) =                  1 2 . . . T                  , Ω (T ) d =                                           1                                          (19)U = (Ω (T ) d ⊗ D), L = diag(1 ⊗ σ) −2(20)π * = (U T LU + λ a I) −1 U T L 1 ⊗ ( μ − µ s † t ) − d (T ) ⊗ µ s ′ t 0 (21)
where 1 denotes a column vector of 1s of length T , ⊗ denotes the Kronecker product, and µ s † t and µ s ′ t denote, respectively, the means of the variational beliefs about the full object state in generalized coordinates (s † ), and the means of the beliefs about the derivatives (s ′ ) only.In words, U is a matrix that maps actions within the planning horizon to their (cumulative) effects over that time window.These effects are translated to the network's latent space, through the multiplication by D in equation 20.The optimal actions are obtained by projecting the current (precision-weighted) error (i.e., the discrepancy between the desired state and the sequence of states that will unfold if no action is taken) onto the (precision-weighted) pseudoinverse of U.This projection is optionally shrunk towards a zero-mean Gaussian prior over actions, with precision λ a , to regularize the scale of the actions.</p>
<p>In the experiments reported in this work, we used a planning horizon of T = 3 and an action regularization strength λ a = 0.1.We also set L = I as we empirically find this leads to better and more stable performance.</p>
<p>Latent space traversals</p>
<p>OBR is trained without supervision, and so the structure of its latent representational space is a priori unknown.Here, we explore this space by systematically traversing it one dimension at a time.We first let OBR encode an image into its latent space.Subsequently, for each object representation, we increment the target dimension's encoded (mean) value with a range of 20 evenly spaced values between [−1, 1].We then feed the resulting, perturbed latent vectors through OBR's decoder to obtain 20 images that vary systematically (and for all objects) along the target dimension.The images in Fig. 10 show the results of this procedure for three different scenes, for each of the 16 latent dimensions of the trained OBR model.Of particular note are dimensions 3 and 9, which have learned to encode the objects' positions (along an approximately orthogonal set of axes at an arbitrary angle to the pixel grid).Other recognizable variations can be seen in color (dimensions 4, 6 and 11) and size (14).Shape and orientation appear to be entangled along multiple dimensions.Other dimensions do not obviously encode anything -they may be truly non-coding, or their role might not be visible in combination with the other latent that we happened to sample here.In particular, the depth value of an object is almost certainly encoded in one or more latent dimensions, but this is not apparent from these traversals, as they target all objects simultaneously, and thus should not affect their occlusion patterns (which object is in front of which other object(s)).</p>
<p>Figure 2 :
2
Figure 2: Conditional behavioural reasoning experiments.(a) 3D Active dSprites.The agent's visual input is the RGB image of the projected objects.Objects have 1st order dynamics with friction and the agent can apply forces.The behavioural reasoning is conditional on the objects present in the environment and the learnt rules.This is, the agent should move the objects differently depending on the proto-symbolic rules and the presence of objects.(b) Type of conditional rules that the agent should be able to learn: Simple and complex conditional rules, composition of two learnt rules and advanced logic XOR rules.</p>
<p>Figure</p>
<p>Figure 2b details the type of rules that the agent can learn.Conditional rules, such as (A → B) ∧ (¬A → C): "If there is a Half-torus move Boxes to the Top-left and cones to the Center-left, otherwise move Boxes to the Topright".It can also perform logical composition (A → B) ∧ (A → C) ⊢ A → (B ∧ C) by learning two different rules separately but then they emerge combined during execution.Furthermore, it can also learn XOR operations, such as (A ∨ B) → C) ∧ (A ∧ B → D).Once the agent learns the rules, reasoning transforms into a behavioural response that changes the environment towards its internal preference (Fig.2a).Figure2cshows an OBR agent solving several conditional reasoning instances with different number of objects in the scene.</p>
<p>Figure 3 :
3
Figure 3: Complexity analysis through baseline comparison.Oracle algorithms have access to the objects location and dynamics, and define the upper-bound performance.RL algorithms (PPO and SAC) are tested with full observability and partial observability (RGB image pixels) (a) Algorithms performance at each frame of execution for a learnt single conditional rule (A → B) ∧ (¬A → C).Colored lines are the Mean Squared Error (MSE) between the objects location and the idealized goal location for 100 tested instances after training.Error bars are the standard deviation.(b) Average performance of the algorithms at the final frame.The lower the better.</p>
<p>Figure 4 :
4
Figure 4: Behavioural reasoning with 2D objects.(a) Two instances of the active 2D dSprites solved by the OBR agent which has learnt the "IfHeart" conditional rule: (Heart → Squares to top-right ∧ Ellipses to middle-right) ∧ (¬ Heart → Squares to top-left ∧ Ellipses to middle-left ∧ Hearts to down-left).Environment row shows the real scenario with the objects and Imagined goal row show the desired preference of the agent.The red arrows are the control actions applied to the objects in every frame.The agent is passive until the 5th frame and the preference network waits three frames to provide an output (this allows perception to be stable).The true goal is the idealized desired location of the objects.(b) Two instances of the active 2D dSprites solved by the OBR agent which has learnt the Heart XOR Square task.(c and d) Statistical evaluation of both tasks accuracy (MSE with respect to the idealized true goal) for different number of objects in the scene.The training was performed with 3 objects.Colored lines describe the the MSE between ground-truth goals and actual object positions and velocities.Error bars reflect +/-1 SEM.</p>
<p>Figure 5 :
5
Figure 5: Behavioural reasoning with 3D objects.(a) Two instances of the active 3D dSprites solved by the OBR agent which has learnt the "IfHalfTorus" conditional rule: (HalfTorus → Boxes to top-right-middle ∧ Cones to middle-right-middle) ∧ (¬ HalfTorus → Boxes to top-left-front ∧ Cones to middle-left-front ∧ HalfTorus to down-left-front).Note that in the second instance there are only Boxes and HalfTorus and no Cones but the OBR solves the conditional task without troubles.Environment row shows the real scenario with the 3D objects and Imagined goal row show the desired preference of the agent.The red arrows are the control actions applied to the objects in every frame.The agent is passive until the 5th frame and the preference network waits three frames to provide an output (this allows perception to be stable).The true goal is the idealized desired location of the objects.(b) Statistical evaluation of the task accuracy (MSE with respect to the idealized true goal) for different number of objects in the scene.Blue lines describe the mean and the error bars denote the standard deviation.(c) Instances of the OBR agent sequential behaviour with an IfHalfTorus rule with different number of objects in the scene.The goal represents the ideal location of the objects given the rule.The desired goals of the agent (preferent internal state) are not shown and are learnt through unsupervised learning using the preference network.</p>
<p>Figure 6 :
6
Figure 6: Adaptation.(a) Two instances of the active dSprites solved by the OBR agent which has learnt the "IfHeart" conditional rule: (Heart → Squares to top-right ∧ Ellipses to middle-right) ∧ (¬ Heart → Squares to top-left ∧ Ellipses to middle-left ∧ Hearts to down-left).At 4th frame one of the objects (in this case the heart is substituted by a square, forcing not only to change the object-centric perception but the conditional behaviour, as the resulting reasoning depends on the appearance of the Heart.(b) Statistical performance evaluation of 100 instances of the object substitution experiment.Blue line is the MSE and the error bars are the standard deviation.</p>
<p>For instance, Rule 1, If Heart move Squares to Top-left, otherwise, to Middle-right and Rule 2, If Heart move Ellipses to Middle-left, otherwise, to Middle-right.In the testing phase, when there are both squares and ellipses the agent should compose both rules into: If Heart move Squares to Top-left and Ellipses to Middle-left, otherwise to Middle-right.Fig 7 shows the task performance, computed as the MSE to idealized desired location of the objects, as if the two rules where being computed.The OBR agent is passive until the 5th frame.The plot shows that the performed actions on the objects actually solve the task, confirming the that agent reasoning combines both rules.</p>
<p>Figure 7 :
7
Figure 7: Logical composition.The OBR agent preference reasoning module is trained with two distinct rules and then tested in execution if the composed rule emerges.Task accuracy over 100 instances shows that OBR is combining both rules in the test phase.</p>
<p>Figure 8 :
8
Figure 8: Object-centric segmentation and prediction.(a) Observation model (b) Three examples of attentive masks output by the OBR perceptual module with an IAI backbone.(c) Dynamics model.(d) Two examples of the perception predicting ahead in time the dynamics of the environment.</p>
<p>Figure 9 :
9
Figure 9: Robustness to variations in video length and number of objects.Segmentation accuracy (F-ARI score) and reconstruction error (MSE) across video frames that extend beyond the duration of the training videos (4 frames), for scenes with various numbers of objects (training videos contained 3 objects).Left panels show the performance of the network after training on 4-frame videos only.Right panels show the performance after just 3 epochs of additional training with 12-frame videos.Error bars depict the mean +/-1 SEM.</p>
<p>Figure 10
10
Figure 10</p>
<p>(a) Latent dimension 8 (
8
b) Latent dimension 9 (c) Latent dimension 10 (d) Latent dimension 11 (e) Latent dimension 12 (f) Latent dimension 13 (g) Latent dimension 14 (h) Latent dimension 15</p>
<p>Some of the works may have overlapping features in other topics.
We use the term proto-symbols as in visual segmentation we use for proto-objects[34], thus describing candidates for symbols and objects.
Robotic experiments have shown that 2nd order generalized coordinates are enough to track a dynamical system[38] depending on the nature of the noise.
Note that in this notation, we do not condition the preference on the current state belief as above, as the planning procedure described here can be applied to any preference distribution.
Note that the notation in this section omits the object index k for legibility, as actions can be planned independently between objects (a strength of our approach).
While we use words to describe the rule the agent learns the rule through unsupervised learning and there is no textual description of them
Note that this error is over the ground truth goal as we can use the environment information to obtain the true image projection of the goal.
Note the use of Iverson-bracket notation; the bracket term is binary and evaluates to 1 iff the expression inside the brackets is true.
AcknowledgmentsThis work has been funded by the SPIKEFERENCE project, co-funded by the Human Brain Project (HBP) Specific Grant Agreement 3 (ID: 945539).
How human infants deal with symbol grounding. S J Cowley, Interaction Studies. 812007</p>
<p>This ai learnt language by seeing the world through a baby's eyes. E Gibney, Nature. 62679992024</p>
<p>The computational origin of representation, Minds and machines 31. S T Piantadosi, 2021</p>
<p>Simulation as an engine of physical scene understanding. P W Battaglia, J B Hamrick, J B Tenenbaum, Proceedings of the National Academy of Sciences. 110452013</p>
<p>K Greff, S Van Steenkiste, J Schmidhuber, arXiv:2012.05208On the binding problem in artificial neural networks. 2020arXiv preprint</p>
<p>Capturing the objects of vision with neural networks. B Peters, N Kriegeskorte, Nature Human Behaviour. 52021</p>
<p>Probabilistic models of cognition: Exploring representations and inductive biases. T L Griffiths, N Chater, C Kemp, A Perfors, J B Tenenbaum, Trends in cognitive sciences. 1482010</p>
<p>Inductive biases for deep learning of higher-level cognition. A Goyal, Y Bengio, Proceedings of the Royal Society A. 478202100682266. 2022</p>
<p>Attend, infer, repeat: Fast scene understanding with generative models. S Eslami, N Heess, T Weber, Y Tassa, D Szepesvari, G E Hinton, Advances in neural information processing systems. 292016</p>
<p>Multi-object representation learning with iterative variational inference. K Greff, R L Kaufman, R Kabra, N Watters, C Burgess, D Zoran, L Matthey, M Botvinick, A Lerchner, International conference on machine learning. PMLR2019</p>
<p>Savi++: Towards end-to-end object-centric learning from real-world videos. G Elsayed, A Mahendran, S Van Steenkiste, K Greff, M C Mozer, T Kipf, Advances in Neural Information Processing Systems. 352022</p>
<p>Object-centric learning with slot attention. F Locatello, D Weissenborn, T Unterthiner, A Mahendran, G Heigold, J Uszkoreit, A Dosovitskiy, T Kipf, Advances in Neural Information Processing Systems. 332020</p>
<p>Object-centric slot diffusion. J Jiang, F Deng, G Singh, S Ahn, arXiv:2303.108342023arXiv preprint</p>
<p>Learning what and where: Disentangling location and identity tracking without supervision. M Traub, S Otte, T Menge, M Karlbauer, J Thuemmel, M V Butz, arXiv:2205.133492022arXiv preprint</p>
<p>. H.-X , </p>
<p>Unsupervised discovery of object radiance fields. L J Yu, J Guibas, Wu, International Conference on Learning Representations. 2022</p>
<p>Interaction networks for learning about objects, relations and physics. P Battaglia, R Pascanu, M Lai, D Jimenez Rezende, Advances in neural information processing systems. 292016</p>
<p>Propagation networks for model-based control under partial observation. Y Li, J Wu, J.-Y Zhu, J B Tenenbaum, A Torralba, R Tedrake, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Intuitive physics learning in a deep-learning model inspired by developmental psychology. L S Piloto, A Weinstein, P Battaglia, M Botvinick, Nature human behaviour. 692022</p>
<p>Visual interaction networks: Learning a physics simulator from video, Advances in neural information processing systems. N Watters, D Zoran, T Weber, P Battaglia, R Pascanu, A Tacchetti, 201730</p>
<p>Curious exploration via structured world models yields zero-shot object manipulation. C Sancaktar, S Blaes, G Martius, Advances in Neural Information Processing Systems. 352022</p>
<p>Entity abstraction in visual model-based reinforcement learning. R Veerapaneni, J D Co-Reyes, M Chang, M Janner, C Finn, J Wu, J Tenenbaum, S Levine, Conference on Robot Learning. PMLR2020</p>
<p>Learning multi-object dynamics with compositional neural radiance fields. D Driess, Z Huang, Y Li, R Tedrake, M Toussaint, Conference on robot learning. PMLR2023</p>
<p>Entity-centric reinforcement learning for object manipulation from pixels. D Haramati, T Daniel, A Tamar, arXiv:2404.012202024arXiv preprint</p>
<p>Object-centric scene representations using active inference. T Van De Maele, T Verbelen, P Mazzaglia, S Ferraro, B Dhoedt, Neural Computation. 3642024</p>
<p>Learning geometry-enhanced visual representation with slot attention for vision-and-language navigation. J Huo, Q Sun, B Jiang, H Lin, Y Fu, Geovln , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>
<p>Object-centric compositional imagination for visual abstract reasoning. R Assouel, P Rodriguez, P Taslakian, D Vazquez, Y Bengio, ICLR2022 Workshop on the Elements of Reasoning: Objects, Structure and Causality. 2022</p>
<p>Systematic visual reasoning through object-centric relational abstraction. T Webb, S S Mondal, J D Cohen, Advances in Neural Information Processing Systems. 362024</p>
<p>D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXiv:2303.03378Palm-e: An embodied multimodal language model. 2023arXiv preprint</p>
<p>World models and predictive coding for cognitive and developmental robotics: Frontiers and challenges. T Taniguchi, S Murata, M Suzuki, D Ognibene, P Lanillos, E Ugur, L Jamone, T Nakamura, A Ciria, B Lara, Advanced Robotics. 37132023</p>
<p>M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, C Fu, K Gopalakrishnan, K Hausman, arXiv:2204.01691Do as i can, not as i say: Grounding language in robotic affordances. 2022arXiv preprint</p>
<p>Y Xu, W Li, P Vaezipoor, S Sanner, E B Khalil, arXiv:2305.18354Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations. 2023arXiv preprint</p>
<p>The free-energy principle: a unified brain theory?. K Friston, Nature reviews neuroscience. 1122010</p>
<p>P Lanillos, C Meo, C Pezzato, A A Meera, M Baioumy, W Ohata, A Tschantz, B Millidge, M Wisse, C L Buckley, arXiv:2112.01871Active inference in robotics and artificial agents: Survey and challenges. 2021arXiv preprint</p>
<p>Modeling attention to salient proto-objects. D Walther, C Koch, Neural networks. 1992006</p>
<p>Attention-based active visual search for mobile robots. A Rasouli, P Lanillos, G Cheng, J K Tsotsos, Autonomous Robots. 4422020</p>
<p>Object-based active inference. R S Van Bergen, P Lanillos, Active Inference: Third International Workshop. Grenoble, FranceSpringerSeptember 19, 2022. 20232022Revised Selected Papers</p>
<p>J Marino, Y Yue, S Mandt, Iterative amortized inference, 35th International Conference on Machine Learning, ICML 2018. 20188</p>
<p>Free energy principle for state and input estimation of a quadcopter flying in wind. F Bos, A A Meera, D Benders, M Wisse, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Active inference: the free energy principle in mind, brain, and behavior. T Parr, G Pezzulo, K J Friston, 2022MIT Press</p>
<p>Deep active inference for partially observable mdps. O Van Der Himst, P Lanillos, Active Inference: First International Workshop. Ghent, Belgium2020. September 14, 2020, Proceedings 1, Springer, 20202020</p>
<p>Whence the expected free energy?. B Millidge, A Tschantz, C L Buckley, Neural Computation. 3322021</p>
<p>Deep active inference agents using monte-carlo methods. Z Fountas, N Sajid, P Mediano, K Friston, Advances in neural information processing systems. 332020</p>
<p>Contributions to the theory of optimal control. R E Kalman, Bol. soc. mat. mexicana. 521960</p>
<p>Optimal control theory: an introduction, Courier Corporation. D E Kirk, 2004</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. T Haarnoja, A Zhou, P Abbeel, S Levine, International conference on machine learning. PMLR2018</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Stable-baselines3: Reliable reinforcement learning implementations. A Raffin, A Hill, A Gleave, A Kanervisto, M Ernestus, N Dormann, Journal of Machine Learning Research. 222682021</p>
<p>J Gu, S Kirmani, P Wohlhart, Y Lu, M G Arenas, K Rao, W Yu, C Fu, K Gopalakrishnan, Z Xu, arXiv:2311.01977Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. 2023arXiv preprint</p>
<p>P Sundaresan, Q Vuong, J Gu, P Xu, T Xiao, S Kirmani, T Yu, M Stark, A Jain, K Hausman, arXiv:2403.02709Rt-sketch: Goal-conditioned imitation learning from hand-drawn sketches. 2024arXiv preprint</p>
<p>Neuroscience-inspired perception-action in robotics: applying active inference for state estimation, control and self-perception. P Lanillos, M Van Gerven, arXiv:2105.042612021arXiv preprint</p>
<p>D P Kingma, J Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>E Jang, S Gu, B Poole, arXiv:1611.01144Categorical reparameterization with gumbel-softmax. 2016arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
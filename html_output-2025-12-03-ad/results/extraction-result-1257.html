<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1257 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1257</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1257</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-240354728</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2111.00210v2.pdf" target="_blank">Mastering Atari Games with Limited Data</a></p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 194.3% mean human performance and 109.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at https://github.com/YeWR/EfficientZero. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1257.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1257.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EfficientZero-model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EfficientZero learned latent environment model (representation + dynamics + predictor)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The latent world model used inside EfficientZero: an encoder (H) that maps image observations to a compact hidden state, a recurrent dynamics function (G) that predicts next hidden states conditioned on actions, and prediction heads for reward (value-prefix), value and policy; trained end-to-end with an added temporal self-supervised consistency loss and an LSTM value-prefix head to improve fidelity and robustness under limited data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EfficientZero latent environment model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space recurrent dynamics model: input images (stacked 4 frames -> 96x96x12) are encoded by a convolutional residual encoder H into a hidden tensor s_t (spatial 6x6 feature maps with 64 channels after pooling). A dynamics network G (recurrent inference with residual link; reduced residual blocks from MuZero) takes s_t and discrete action a_t and produces predicted next latent state ŝ_{t+1}. Prediction heads take latent states (and an LSTM over unrolled latents) to output a value-prefix (sum of discounted rewards predicted via an LSTM), a scalar value V(s), and a policy prior P(s). The model is trained end-to-end with task losses (reward/value/policy) plus a SimSiam-style temporal consistency loss that pulls ŝ_{t+k} close to H(o_{t+k}) for k up to 5.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (Atari 100k benchmark) and selected DMControl visual tasks (DMControl 100k)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Several metrics: (1) reconstruction quality of decoded latent states (visual reconstruction via a decoder D), (2) L1 error of value targets vs Monte Carlo ground-truth values (value-target fidelity), (3) prediction error of unrolled next states averaged over 1–5 steps; losses used include negative cosine similarity (SimSiam loss) and L2 between projected features.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Quantified examples in paper: value-target L1 errors on UpNDown (Table 4) — without off-policy correction: current-state 0.765, unrolled next-5 avg 0.636, all-states avg 0.657; with correction: current-state 0.533, unrolled next-5 avg 0.576, all-states avg 0.569. Visual reconstruction: predicted unrolled states from model with consistency could be reconstructed into recognizable observations whereas model without consistency could not (Fig.4). No single scalar MSE for pixel predictions reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: latent representations are amenable to visualization (a decoder D reconstructs images from latent states), enabling qualitative assessment of what the model encodes; however the latent features are neural-network learned and not mapped to symbolic/explicit physical quantities, so the model remains largely a black-box beyond reconstruction visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Decoder-based latent visualization (decode s_t and ŝ_{t+k} to images); comparison of reconstruction quality for models trained with/without the temporal consistency loss; inspection of distributional shift between representation and dynamics outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Implementation-level costs: trained EfficientZero on Atari 100k with 4 GPUs in ~7 hours (per paper). MCTS uses N_sim = 50 simulations per root. Architectural sizes: LSTM hidden size 512, projector/predictor MLPs with hidden 512 and output 1024; input stacked frames produce 96x96x12 tensors; dynamics network reduced to 1 residual block (vs MuZero's 16). Reanalyze (off-policy correction) doubles some reanalyze computations; MCTS implemented in C++ with batched inference in Python/Cython to improve throughput.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Data efficiency: EfficientZero uses 100k environment steps (≈2 hours gameplay) and achieves comparable/close performance to methods trained on orders of magnitude more data (paper states ~500x less data than DQN at 200M frames). Compute efficiency: authors claim EfficientZero can be trained for Atari-100k on 4 GPUs in ~7 hours, while MuZero (original) reportedly required 64 TPUs for 12 hours in prior work — indicating large reductions in compute needed for the limited-data regime; however MCTS still incurs significant search cost (50 sims).</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On Atari 100k benchmark: mean human-normalized score 194.3% and median 109.0% across 26 games (using 100k steps). EfficientZero outperforms prior SoTA (SPR) substantially and surpasses human mean/median in this low-data regime; on DMControl 100k, EfficientZero matches or outperforms some baselines and is comparable to state-based SAC on the tested low-dimensional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The latent world model is central to MCTS planning and sample efficiency: improving latent fidelity (via temporal consistency) yields better predicted unrolled states that can be used by MCTS, producing higher-quality search priors and improved policies; the end-to-end value-prefix predictor improves Q estimates under state aliasing and yields better search performance; model-based imagined rollouts (used in off-policy correction) reduce off-policy bias and improve target-value fidelity, which in turn improves downstream policy/value learning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Trade-offs discussed: (1) deeper/unconstrained unrolls increase compounding error (state aliasing) which hurts reward prediction and MCTS search; (2) using imagined rollouts to correct off-policy bias risks model exploitation and increases compute (reanalyze step doubles some computation), so they shorten dynamic horizon l for older trajectories to balance bias vs model-exploitation; (3) larger models (MuZero default) are unnecessary in limited-data regime and smaller networks were chosen to reduce overfitting and compute; (4) adding temporal consistency increases supervision (improves fidelity) but adds an auxiliary loss and projector/predictor MLPs.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key design choices: (1) add SimSiam-style temporal self-supervised consistency loss between G(s_t,a) and H(o_{t+1}) (and recurrently up to 5 steps) to provide strong supervision for dynamics; (2) predict value-prefix end-to-end using an LSTM over unrolled latent states instead of per-step reward prediction; (3) model-based off-policy correction: use a dynamic horizon l (smaller for older trajectories) then run MCTS from s_{t+l} to compute ν_MCTS and form z_t = sum_{i<l} γ^i u_{t+i} + γ^l ν_MCTS_{t+l}; (4) reduce model size (few residual blocks) for limited-data efficiency; (5) implement batched C++ MCTS with Python inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to MuZero baseline (their reimplementation with same hyperparams), EfficientZero substantially improves sample efficiency in the 100k-step regime (ablation shows each of the three components helps; consistency removal yields largest drop). Compared to model-free sample-efficient methods (SPR, CURL, DrQ), EfficientZero achieves much higher mean/median human-normalized scores on Atari 100k. Compared to Dreamer / DreamerV2 and other world-model methods, paper notes those methods can achieve high final performance but are typically less sample-efficient in this extremely low-data (100k) regime.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends for limited-data image-based RL: (1) use a self-supervised temporally consistent environment model to give rich supervision to dynamics, (2) predict value-prefix end-to-end (LSTM) to mitigate state-aliasing/compounding error, and (3) use model-based off-policy correction with a dynamic horizon and MCTS root-value re-evaluation; additionally, use a smaller network capacity and batch/batched MCTS/C++ implementation to balance fidelity vs compute in low-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Atari Games with Limited Data', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1257.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1257.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuZero-model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuZero learned environment model (reward + dynamics implicitly via value/policy heads)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MuZero's internal world model uses an abstract latent state representation plus learned dynamics, reward, policy and value predictors, and it is used as the model for MCTS planning; MuZero does not explicitly supervise pixel-level next-state predictions in its original formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari, go, chess and shogi by planning with a learned model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MuZero latent model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Abstract-state latent model: an encoder produces hidden states s_t from observations, and a dynamics network predicts next hidden states and rewards; separate prediction heads produce policy priors and value estimates that serve as heuristics for MCTS. The original MuZero trains dynamics implicitly via losses on reward, value and policy predictions rather than pixel-level next-state supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (implicit dynamics trained via task-prediction losses)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Board games (Go/Chess/Shogi) and Atari planning; general RL planning tasks with MCTS</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>In MuZero, fidelity of the model is indirectly measured via task losses (reward/value/policy prediction error) and downstream policy performance rather than explicit next-state reconstruction metrics; in this paper MuZero baseline is evaluated by task performance on Atari 100k.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>In this paper MuZero (their implementation, same hyperparams) performs poorly in the limited-data (100k) regime relative to EfficientZero; no explicit pixel MSE numbers for MuZero dynamics are reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>MuZero's latent model is largely a black-box; the paper notes MuZero does not explicitly learn an environment model (in the sense of next-observation supervision), which leads to insufficient supervision signal for the dynamics in sparse-reward and limited-data regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Not much interpretability used in MuZero in this paper; authors use decoder visualization on EfficientZero (and contrast to MuZero-like models without consistency) to show distributional shift between representation H and dynamics G when no consistency loss is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>MuZero (original reported) required very large compute in high-data settings: authors mention 'MuZero needs 64 TPUs to train 12 hours for one agent on Atari games' (prior work). In the authors' reimplementation MuZero is used as a baseline but details of compute are not re-stated beyond being heavier than EfficientZero in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>In the limited-data regime, MuZero without the additional components proposed here is less sample-efficient and achieves worse performance than EfficientZero. MuZero-style models trained without explicit consistency supervision suffer from dynamics prediction distributional shift when data is scarce.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>As a baseline in this paper on Atari 100k, MuZero (their implementation) underperforms EfficientZero and prior SoTA sample-efficient methods; specific per-game numbers are in the paper tables but aggregate mean/median are lower than EfficientZero.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>MuZero's implicit model suffices in high-data regimes (as prior work shows), but in the 100k-step low-data setting the lack of explicit supervision of next-state predictions (and consequent poor dynamics fidelity) harms MCTS effectiveness and policy improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>MuZero's approach reduces explicit modeling burden by focusing on task-related predictions (value/reward/policy), but in sparse-reward and low-data regimes this trades off too much fidelity in latent dynamics, producing distributional shift between H and G and hurting planning utility.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>MuZero trains dynamics via reward/value/policy losses rather than supervised next-state losses; uses latent states and MCTS for planning; the paper uses MuZero as the base and modifies it by adding temporal consistency, value-prefix LSTM, and off-policy correction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared with EfficientZero, MuZero lacks temporal self-supervised supervision and value-prefix prediction, leading to poorer sample efficiency in the 100k-step Atari benchmark; compared to pure model-free approaches, MuZero can produce strong policies given ample data but struggles in extremely low-data regimes without the proposed modifications.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper implies MuZero-style models can be improved for low-data regimes by adding explicit temporal consistency supervision to align dynamics outputs with encoder outputs, adding end-to-end value-prefix prediction, and applying model-based off-policy correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Atari Games with Limited Data', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1257.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1257.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temporal-Consistency (SimSiam-style)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-supervised temporal consistency loss (SimSiam-inspired applied to transitions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A self-supervised auxiliary loss that enforces consistency between predicted latent next states (ŝ_{t+k} from dynamics G) and actual encoded latent states (s_{t+k}=H(o_{t+k})) across short rollouts (k=1..5), implemented with a SimSiam-style predictor/projector and negative cosine similarity loss; used to provide stronger supervision for the learned dynamics in image-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Temporal self-supervised consistency (SimSiam-style) for dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applies a SimSiam architecture to adjacent observations: H(o_t) is unrolled via G to produce ŝ_{t+1..t+5}; both ŝ and s_{t+k}=H(o_{t+k}) pass through a common projector; one branch is treated as target (no gradient) and the other with predictor is trained to minimize negative cosine similarity (L2-like in projected space). The loss L_similarity pulls dynamics outputs toward encoder outputs, reducing distributional shift.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>self-supervised representation/dynamics regularizer (auxiliary supervision for latent world model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari image-based RL (100k) and general image-based model-based RL tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Measured via downstream indicators: reconstruction quality of decoded unrolled latent states and reduced prediction/value errors; loss is negative cosine similarity on projected features between s_{t+k} and ŝ_{t+k}.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Ablation: removing consistency causes the largest drop in performance (normed mean drops from 1.943 to 0.881, normed median 1.090 -> 0.340). Qualitatively, models with consistency produce reconstructable unrolled states while models without do not (Fig.4).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Improves interpretability indirectly by making predicted latent states closer to encoder-produced latents, enabling meaningful decoding and visualization of unrolled states; still no explicit disentanglement reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Decoder visualization of latents; comparison of reconstruction quality and distributional shift analyses between representation and dynamics outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Adds projector/predictor MLPs (P1: 3-layer MLP, P2: 2-layer MLP, hidden dim 512, proj output 1024) and the extra loss computation across up to 5 unroll steps; modest extra compute relative to MCTS cost.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Provides large gains in sample efficiency for limited-data regime (largest single-component ablation drop when removed); data augmentation alone is insufficient — the loss itself is main contributor according to ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Critical to achieving EfficientZero's Atari 100k results: ablation without consistency shows large performance degradation across most games; consistency accelerates early learning and improves final performance under data scarcity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>By aligning predicted and encoded latent trajectories, the consistency loss provides richer supervision to dynamics G, leading to better unrolled latent states for MCTS and better policy improvement; thus improved fidelity directly translates to better task performance in data-limited settings.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Adds auxiliary loss and parameters but reduces sample complexity and improves generalization; no large compute penalty reported compared to benefits.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use SimSiam-style asymmetric stop-gradient design; apply augmentations to observations; choose representation/dynamics hidden states for the loss rather than projector outputs; unroll dynamics up to 5 steps for the consistency loss.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to SPR/BYOL-style temporal consistency used in prior model-free work, this approach uses SimSiam and applies the consistency in a model-based/unrolled setting, producing stronger supervision for dynamics and better sample efficiency in planning-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests using SimSiam-like asymmetric predictor with stop-gradient, projectors with 1024-d outputs, unrolling dynamics for up to 5 steps, and applying small image augmentations; consistency coefficient λ3 = 2 in hyperparams.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Atari Games with Limited Data', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1257.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1257.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimPLe / Kaiser</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-based reinforcement learning for Atari (action-conditioned video prediction / SimPLe)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier model-based approach that learns an action-conditioned video prediction model and trains a policy (PPO) inside the learned video model; shown to achieve reasonable performance with a few hundred thousand frames but inferior to later methods in many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modelbased reinforcement learning for atari</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SimPLe action-conditioned video prediction model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Action-conditioned video-prediction world model that predicts next-frame pixels conditioned on actions; used to generate imagined trajectories to train a policy (e.g., PPO) inside the learned simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>pixel-space video prediction world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari games (sample-efficient RL)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Measured by how well predicted video sequences lead to policies that perform in the real environment; prior works report pixel MSE and downstream policy returns but not directly used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>SimPLe achieved the first strong results on Atari with as little as 400k frames (cited by this paper), but its median normalized score is low compared to later work (paper cites SimPLe median 0.144 human-normalized in prior comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Pixel predictions are interpretable visually (frame sequences), but video-prediction models are prone to compounding error and high-dimensional prediction difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visual inspection of predicted frames; not used in this paper beyond citation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Video prediction models can be computationally heavy due to pixel-space predictions; specific compute numbers not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>SimPLe is sample-efficient relative to older large-data RL but underperforms the method proposed here (EfficientZero) on the 100k benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>On Atari, SimPLe performs well at 400k frames but achieves lower median normalized scores on the 100k benchmark than EfficientZero.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Pixel-space video prediction gives directly interpretable imagined rollouts but is brittle due to compounding errors; this paper argues latent-space, self-supervised-consistent models with planning (MCTS) give better sample efficiency in the 100k regime.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Pixel-space fidelity and interpretability vs severe compounding error and lower ultimate policy performance under tight data budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Action-conditioned video predictors operating in pixel space vs latent-space recurrent predictors; EfficientZero favors latent-space with temporal self-supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to SimPLe, EfficientZero achieves much higher normalized scores at 100k steps and avoids direct pixel-space rollouts to reduce compounding error.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper's recommendations imply that latent-space models with temporal consistency and planning are preferable to pixel-space video prediction for extremely low-data Atari.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Atari Games with Limited Data', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1257.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1257.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer-family</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreamer / DreamerV2 / discrete world models (Hafner et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of latent world-model methods that learn stochastic + deterministic latent dynamics and perform policy learning inside latent 'imagined' rollouts (Dreamer, DreamerV2, and discrete world models), shown to achieve strong performance on many image-based control tasks though typically requiring more data in Atari compared to EfficientZero.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer / DreamerV2 latent world models (and discrete world models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent dynamics models with both stochastic and deterministic components (e.g., RSSM) trained via reconstruction/prediction losses; policies and value functions are learned using imagined trajectories sampled from the latent model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model with stochastic/deterministic components (recurrent state-space model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Atari, continuous control (DMControl), image-based RL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Typically evaluated via reconstruction/prediction losses in latent space and downstream policy performance in environments; not numerically reported in this paper beyond qualitative positioning.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Paper notes Dreamer V2 can achieve super-human performance in some settings but 'are not sample efficient' in the strict 100k-steps Atari benchmark relative to EfficientZero.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent representations can sometimes be decoded for qualitative inspection but remain largely neural and not explicitly interpretable.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Prior works decode latents or visualize imagined trajectories; this paper references them but does not apply their interpretability methods.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Dreamer-family involves training generative latent models and imagined rollouts; compute depends on model size and rollout lengths; specific numbers not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Dreamer/DreamerV2 reach high performance but typically need more environment data than EfficientZero to reach comparable Atari scores; EfficientZero is more sample-efficient in the 100k regime.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Prior Dreamer variants achieve strong control performance on DMControl and some Atari tasks (in higher-data regimes); in the 100k Atari benchmark they are characterized here as less sample-efficient.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Imagined-latent rollouts provide a way to generate training data without environment interactions (reducing sample complexity in some settings), but carefully designed supervision (temporal consistency, planning) matters in the extreme low-data Atari regime.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Dreamer-style latent generative objectives encourage good long-horizon imagination but can require more data and careful tuning; EfficientZero instead emphasizes temporal self-supervision and planning for sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Dreamer uses stochastic state-space models and policy/value learning in imagined trajectories; EfficientZero uses MCTS over learned latents with SimSiam-style consistency and value-prefix prediction as different choices for the low-data regime.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared to EfficientZero, Dreamer variants are powerful but (per the authors) not as sample-efficient on 100k Atari; on DMControl EfficientZero can outperform pixel-based SAC and be comparable to state-based SAC on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper implies that combining strong latent-model training (temporal self-supervision) with planning and off-policy correction is an effective configuration for the 100k-step Atari setting, rather than relying solely on long imagined rollouts used by Dreamer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mastering Atari Games with Limited Data', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Mastering atari, go, chess and shogi by planning with a learned model <em>(Rating: 2)</em></li>
                <li>Modelbased reinforcement learning for atari <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination <em>(Rating: 2)</em></li>
                <li>Mastering atari with discrete world models <em>(Rating: 2)</em></li>
                <li>Exploring simple siamese representation learning <em>(Rating: 2)</em></li>
                <li>Data-efficient reinforcement learning with self-predictive representations <em>(Rating: 1)</em></li>
                <li>Combating the compounding-error problem with a multi-step model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1257",
    "paper_id": "paper-240354728",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "EfficientZero-model",
            "name_full": "EfficientZero learned latent environment model (representation + dynamics + predictor)",
            "brief_description": "The latent world model used inside EfficientZero: an encoder (H) that maps image observations to a compact hidden state, a recurrent dynamics function (G) that predicts next hidden states conditioned on actions, and prediction heads for reward (value-prefix), value and policy; trained end-to-end with an added temporal self-supervised consistency loss and an LSTM value-prefix head to improve fidelity and robustness under limited data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "EfficientZero latent environment model",
            "model_description": "Latent-space recurrent dynamics model: input images (stacked 4 frames -&gt; 96x96x12) are encoded by a convolutional residual encoder H into a hidden tensor s_t (spatial 6x6 feature maps with 64 channels after pooling). A dynamics network G (recurrent inference with residual link; reduced residual blocks from MuZero) takes s_t and discrete action a_t and produces predicted next latent state ŝ_{t+1}. Prediction heads take latent states (and an LSTM over unrolled latents) to output a value-prefix (sum of discounted rewards predicted via an LSTM), a scalar value V(s), and a policy prior P(s). The model is trained end-to-end with task losses (reward/value/policy) plus a SimSiam-style temporal consistency loss that pulls ŝ_{t+k} close to H(o_{t+k}) for k up to 5.",
            "model_type": "latent world model (recurrent latent dynamics)",
            "task_domain": "Atari games (Atari 100k benchmark) and selected DMControl visual tasks (DMControl 100k)",
            "fidelity_metric": "Several metrics: (1) reconstruction quality of decoded latent states (visual reconstruction via a decoder D), (2) L1 error of value targets vs Monte Carlo ground-truth values (value-target fidelity), (3) prediction error of unrolled next states averaged over 1–5 steps; losses used include negative cosine similarity (SimSiam loss) and L2 between projected features.",
            "fidelity_performance": "Quantified examples in paper: value-target L1 errors on UpNDown (Table 4) — without off-policy correction: current-state 0.765, unrolled next-5 avg 0.636, all-states avg 0.657; with correction: current-state 0.533, unrolled next-5 avg 0.576, all-states avg 0.569. Visual reconstruction: predicted unrolled states from model with consistency could be reconstructed into recognizable observations whereas model without consistency could not (Fig.4). No single scalar MSE for pixel predictions reported.",
            "interpretability_assessment": "Partially interpretable: latent representations are amenable to visualization (a decoder D reconstructs images from latent states), enabling qualitative assessment of what the model encodes; however the latent features are neural-network learned and not mapped to symbolic/explicit physical quantities, so the model remains largely a black-box beyond reconstruction visualizations.",
            "interpretability_method": "Decoder-based latent visualization (decode s_t and ŝ_{t+k} to images); comparison of reconstruction quality for models trained with/without the temporal consistency loss; inspection of distributional shift between representation and dynamics outputs.",
            "computational_cost": "Implementation-level costs: trained EfficientZero on Atari 100k with 4 GPUs in ~7 hours (per paper). MCTS uses N_sim = 50 simulations per root. Architectural sizes: LSTM hidden size 512, projector/predictor MLPs with hidden 512 and output 1024; input stacked frames produce 96x96x12 tensors; dynamics network reduced to 1 residual block (vs MuZero's 16). Reanalyze (off-policy correction) doubles some reanalyze computations; MCTS implemented in C++ with batched inference in Python/Cython to improve throughput.",
            "efficiency_comparison": "Data efficiency: EfficientZero uses 100k environment steps (≈2 hours gameplay) and achieves comparable/close performance to methods trained on orders of magnitude more data (paper states ~500x less data than DQN at 200M frames). Compute efficiency: authors claim EfficientZero can be trained for Atari-100k on 4 GPUs in ~7 hours, while MuZero (original) reportedly required 64 TPUs for 12 hours in prior work — indicating large reductions in compute needed for the limited-data regime; however MCTS still incurs significant search cost (50 sims).",
            "task_performance": "On Atari 100k benchmark: mean human-normalized score 194.3% and median 109.0% across 26 games (using 100k steps). EfficientZero outperforms prior SoTA (SPR) substantially and surpasses human mean/median in this low-data regime; on DMControl 100k, EfficientZero matches or outperforms some baselines and is comparable to state-based SAC on the tested low-dimensional tasks.",
            "task_utility_analysis": "The latent world model is central to MCTS planning and sample efficiency: improving latent fidelity (via temporal consistency) yields better predicted unrolled states that can be used by MCTS, producing higher-quality search priors and improved policies; the end-to-end value-prefix predictor improves Q estimates under state aliasing and yields better search performance; model-based imagined rollouts (used in off-policy correction) reduce off-policy bias and improve target-value fidelity, which in turn improves downstream policy/value learning.",
            "tradeoffs_observed": "Trade-offs discussed: (1) deeper/unconstrained unrolls increase compounding error (state aliasing) which hurts reward prediction and MCTS search; (2) using imagined rollouts to correct off-policy bias risks model exploitation and increases compute (reanalyze step doubles some computation), so they shorten dynamic horizon l for older trajectories to balance bias vs model-exploitation; (3) larger models (MuZero default) are unnecessary in limited-data regime and smaller networks were chosen to reduce overfitting and compute; (4) adding temporal consistency increases supervision (improves fidelity) but adds an auxiliary loss and projector/predictor MLPs.",
            "design_choices": "Key design choices: (1) add SimSiam-style temporal self-supervised consistency loss between G(s_t,a) and H(o_{t+1}) (and recurrently up to 5 steps) to provide strong supervision for dynamics; (2) predict value-prefix end-to-end using an LSTM over unrolled latent states instead of per-step reward prediction; (3) model-based off-policy correction: use a dynamic horizon l (smaller for older trajectories) then run MCTS from s_{t+l} to compute ν_MCTS and form z_t = sum_{i&lt;l} γ^i u_{t+i} + γ^l ν_MCTS_{t+l}; (4) reduce model size (few residual blocks) for limited-data efficiency; (5) implement batched C++ MCTS with Python inference.",
            "comparison_to_alternatives": "Compared to MuZero baseline (their reimplementation with same hyperparams), EfficientZero substantially improves sample efficiency in the 100k-step regime (ablation shows each of the three components helps; consistency removal yields largest drop). Compared to model-free sample-efficient methods (SPR, CURL, DrQ), EfficientZero achieves much higher mean/median human-normalized scores on Atari 100k. Compared to Dreamer / DreamerV2 and other world-model methods, paper notes those methods can achieve high final performance but are typically less sample-efficient in this extremely low-data (100k) regime.",
            "optimal_configuration": "Paper recommends for limited-data image-based RL: (1) use a self-supervised temporally consistent environment model to give rich supervision to dynamics, (2) predict value-prefix end-to-end (LSTM) to mitigate state-aliasing/compounding error, and (3) use model-based off-policy correction with a dynamic horizon and MCTS root-value re-evaluation; additionally, use a smaller network capacity and batch/batched MCTS/C++ implementation to balance fidelity vs compute in low-data regimes.",
            "uuid": "e1257.0",
            "source_info": {
                "paper_title": "Mastering Atari Games with Limited Data",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "MuZero-model",
            "name_full": "MuZero learned environment model (reward + dynamics implicitly via value/policy heads)",
            "brief_description": "MuZero's internal world model uses an abstract latent state representation plus learned dynamics, reward, policy and value predictors, and it is used as the model for MCTS planning; MuZero does not explicitly supervise pixel-level next-state predictions in its original formulation.",
            "citation_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "mention_or_use": "use",
            "model_name": "MuZero latent model",
            "model_description": "Abstract-state latent model: an encoder produces hidden states s_t from observations, and a dynamics network predicts next hidden states and rewards; separate prediction heads produce policy priors and value estimates that serve as heuristics for MCTS. The original MuZero trains dynamics implicitly via losses on reward, value and policy predictions rather than pixel-level next-state supervision.",
            "model_type": "latent world model (implicit dynamics trained via task-prediction losses)",
            "task_domain": "Board games (Go/Chess/Shogi) and Atari planning; general RL planning tasks with MCTS",
            "fidelity_metric": "In MuZero, fidelity of the model is indirectly measured via task losses (reward/value/policy prediction error) and downstream policy performance rather than explicit next-state reconstruction metrics; in this paper MuZero baseline is evaluated by task performance on Atari 100k.",
            "fidelity_performance": "In this paper MuZero (their implementation, same hyperparams) performs poorly in the limited-data (100k) regime relative to EfficientZero; no explicit pixel MSE numbers for MuZero dynamics are reported here.",
            "interpretability_assessment": "MuZero's latent model is largely a black-box; the paper notes MuZero does not explicitly learn an environment model (in the sense of next-observation supervision), which leads to insufficient supervision signal for the dynamics in sparse-reward and limited-data regimes.",
            "interpretability_method": "Not much interpretability used in MuZero in this paper; authors use decoder visualization on EfficientZero (and contrast to MuZero-like models without consistency) to show distributional shift between representation H and dynamics G when no consistency loss is applied.",
            "computational_cost": "MuZero (original reported) required very large compute in high-data settings: authors mention 'MuZero needs 64 TPUs to train 12 hours for one agent on Atari games' (prior work). In the authors' reimplementation MuZero is used as a baseline but details of compute are not re-stated beyond being heavier than EfficientZero in practice.",
            "efficiency_comparison": "In the limited-data regime, MuZero without the additional components proposed here is less sample-efficient and achieves worse performance than EfficientZero. MuZero-style models trained without explicit consistency supervision suffer from dynamics prediction distributional shift when data is scarce.",
            "task_performance": "As a baseline in this paper on Atari 100k, MuZero (their implementation) underperforms EfficientZero and prior SoTA sample-efficient methods; specific per-game numbers are in the paper tables but aggregate mean/median are lower than EfficientZero.",
            "task_utility_analysis": "MuZero's implicit model suffices in high-data regimes (as prior work shows), but in the 100k-step low-data setting the lack of explicit supervision of next-state predictions (and consequent poor dynamics fidelity) harms MCTS effectiveness and policy improvement.",
            "tradeoffs_observed": "MuZero's approach reduces explicit modeling burden by focusing on task-related predictions (value/reward/policy), but in sparse-reward and low-data regimes this trades off too much fidelity in latent dynamics, producing distributional shift between H and G and hurting planning utility.",
            "design_choices": "MuZero trains dynamics via reward/value/policy losses rather than supervised next-state losses; uses latent states and MCTS for planning; the paper uses MuZero as the base and modifies it by adding temporal consistency, value-prefix LSTM, and off-policy correction.",
            "comparison_to_alternatives": "Compared with EfficientZero, MuZero lacks temporal self-supervised supervision and value-prefix prediction, leading to poorer sample efficiency in the 100k-step Atari benchmark; compared to pure model-free approaches, MuZero can produce strong policies given ample data but struggles in extremely low-data regimes without the proposed modifications.",
            "optimal_configuration": "Paper implies MuZero-style models can be improved for low-data regimes by adding explicit temporal consistency supervision to align dynamics outputs with encoder outputs, adding end-to-end value-prefix prediction, and applying model-based off-policy correction.",
            "uuid": "e1257.1",
            "source_info": {
                "paper_title": "Mastering Atari Games with Limited Data",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Temporal-Consistency (SimSiam-style)",
            "name_full": "Self-supervised temporal consistency loss (SimSiam-inspired applied to transitions)",
            "brief_description": "A self-supervised auxiliary loss that enforces consistency between predicted latent next states (ŝ_{t+k} from dynamics G) and actual encoded latent states (s_{t+k}=H(o_{t+k})) across short rollouts (k=1..5), implemented with a SimSiam-style predictor/projector and negative cosine similarity loss; used to provide stronger supervision for the learned dynamics in image-based RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Temporal self-supervised consistency (SimSiam-style) for dynamics",
            "model_description": "Applies a SimSiam architecture to adjacent observations: H(o_t) is unrolled via G to produce ŝ_{t+1..t+5}; both ŝ and s_{t+k}=H(o_{t+k}) pass through a common projector; one branch is treated as target (no gradient) and the other with predictor is trained to minimize negative cosine similarity (L2-like in projected space). The loss L_similarity pulls dynamics outputs toward encoder outputs, reducing distributional shift.",
            "model_type": "self-supervised representation/dynamics regularizer (auxiliary supervision for latent world model)",
            "task_domain": "Atari image-based RL (100k) and general image-based model-based RL tasks",
            "fidelity_metric": "Measured via downstream indicators: reconstruction quality of decoded unrolled latent states and reduced prediction/value errors; loss is negative cosine similarity on projected features between s_{t+k} and ŝ_{t+k}.",
            "fidelity_performance": "Ablation: removing consistency causes the largest drop in performance (normed mean drops from 1.943 to 0.881, normed median 1.090 -&gt; 0.340). Qualitatively, models with consistency produce reconstructable unrolled states while models without do not (Fig.4).",
            "interpretability_assessment": "Improves interpretability indirectly by making predicted latent states closer to encoder-produced latents, enabling meaningful decoding and visualization of unrolled states; still no explicit disentanglement reported.",
            "interpretability_method": "Decoder visualization of latents; comparison of reconstruction quality and distributional shift analyses between representation and dynamics outputs.",
            "computational_cost": "Adds projector/predictor MLPs (P1: 3-layer MLP, P2: 2-layer MLP, hidden dim 512, proj output 1024) and the extra loss computation across up to 5 unroll steps; modest extra compute relative to MCTS cost.",
            "efficiency_comparison": "Provides large gains in sample efficiency for limited-data regime (largest single-component ablation drop when removed); data augmentation alone is insufficient — the loss itself is main contributor according to ablations.",
            "task_performance": "Critical to achieving EfficientZero's Atari 100k results: ablation without consistency shows large performance degradation across most games; consistency accelerates early learning and improves final performance under data scarcity.",
            "task_utility_analysis": "By aligning predicted and encoded latent trajectories, the consistency loss provides richer supervision to dynamics G, leading to better unrolled latent states for MCTS and better policy improvement; thus improved fidelity directly translates to better task performance in data-limited settings.",
            "tradeoffs_observed": "Adds auxiliary loss and parameters but reduces sample complexity and improves generalization; no large compute penalty reported compared to benefits.",
            "design_choices": "Use SimSiam-style asymmetric stop-gradient design; apply augmentations to observations; choose representation/dynamics hidden states for the loss rather than projector outputs; unroll dynamics up to 5 steps for the consistency loss.",
            "comparison_to_alternatives": "Compared to SPR/BYOL-style temporal consistency used in prior model-free work, this approach uses SimSiam and applies the consistency in a model-based/unrolled setting, producing stronger supervision for dynamics and better sample efficiency in planning-based RL.",
            "optimal_configuration": "Paper suggests using SimSiam-like asymmetric predictor with stop-gradient, projectors with 1024-d outputs, unrolling dynamics for up to 5 steps, and applying small image augmentations; consistency coefficient λ3 = 2 in hyperparams.",
            "uuid": "e1257.2",
            "source_info": {
                "paper_title": "Mastering Atari Games with Limited Data",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "SimPLe / Kaiser",
            "name_full": "Model-based reinforcement learning for Atari (action-conditioned video prediction / SimPLe)",
            "brief_description": "An earlier model-based approach that learns an action-conditioned video prediction model and trains a policy (PPO) inside the learned video model; shown to achieve reasonable performance with a few hundred thousand frames but inferior to later methods in many tasks.",
            "citation_title": "Modelbased reinforcement learning for atari",
            "mention_or_use": "mention",
            "model_name": "SimPLe action-conditioned video prediction model",
            "model_description": "Action-conditioned video-prediction world model that predicts next-frame pixels conditioned on actions; used to generate imagined trajectories to train a policy (e.g., PPO) inside the learned simulator.",
            "model_type": "pixel-space video prediction world model",
            "task_domain": "Atari games (sample-efficient RL)",
            "fidelity_metric": "Measured by how well predicted video sequences lead to policies that perform in the real environment; prior works report pixel MSE and downstream policy returns but not directly used in this paper.",
            "fidelity_performance": "SimPLe achieved the first strong results on Atari with as little as 400k frames (cited by this paper), but its median normalized score is low compared to later work (paper cites SimPLe median 0.144 human-normalized in prior comparisons).",
            "interpretability_assessment": "Pixel predictions are interpretable visually (frame sequences), but video-prediction models are prone to compounding error and high-dimensional prediction difficulty.",
            "interpretability_method": "Visual inspection of predicted frames; not used in this paper beyond citation.",
            "computational_cost": "Video prediction models can be computationally heavy due to pixel-space predictions; specific compute numbers not provided in this paper.",
            "efficiency_comparison": "SimPLe is sample-efficient relative to older large-data RL but underperforms the method proposed here (EfficientZero) on the 100k benchmark.",
            "task_performance": "On Atari, SimPLe performs well at 400k frames but achieves lower median normalized scores on the 100k benchmark than EfficientZero.",
            "task_utility_analysis": "Pixel-space video prediction gives directly interpretable imagined rollouts but is brittle due to compounding errors; this paper argues latent-space, self-supervised-consistent models with planning (MCTS) give better sample efficiency in the 100k regime.",
            "tradeoffs_observed": "Pixel-space fidelity and interpretability vs severe compounding error and lower ultimate policy performance under tight data budgets.",
            "design_choices": "Action-conditioned video predictors operating in pixel space vs latent-space recurrent predictors; EfficientZero favors latent-space with temporal self-supervision.",
            "comparison_to_alternatives": "Compared to SimPLe, EfficientZero achieves much higher normalized scores at 100k steps and avoids direct pixel-space rollouts to reduce compounding error.",
            "optimal_configuration": "Paper's recommendations imply that latent-space models with temporal consistency and planning are preferable to pixel-space video prediction for extremely low-data Atari.",
            "uuid": "e1257.3",
            "source_info": {
                "paper_title": "Mastering Atari Games with Limited Data",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "Dreamer-family",
            "name_full": "Dreamer / DreamerV2 / discrete world models (Hafner et al.)",
            "brief_description": "A family of latent world-model methods that learn stochastic + deterministic latent dynamics and perform policy learning inside latent 'imagined' rollouts (Dreamer, DreamerV2, and discrete world models), shown to achieve strong performance on many image-based control tasks though typically requiring more data in Atari compared to EfficientZero.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination",
            "mention_or_use": "mention",
            "model_name": "Dreamer / DreamerV2 latent world models (and discrete world models)",
            "model_description": "Latent dynamics models with both stochastic and deterministic components (e.g., RSSM) trained via reconstruction/prediction losses; policies and value functions are learned using imagined trajectories sampled from the latent model.",
            "model_type": "latent world model with stochastic/deterministic components (recurrent state-space model)",
            "task_domain": "Atari, continuous control (DMControl), image-based RL",
            "fidelity_metric": "Typically evaluated via reconstruction/prediction losses in latent space and downstream policy performance in environments; not numerically reported in this paper beyond qualitative positioning.",
            "fidelity_performance": "Paper notes Dreamer V2 can achieve super-human performance in some settings but 'are not sample efficient' in the strict 100k-steps Atari benchmark relative to EfficientZero.",
            "interpretability_assessment": "Latent representations can sometimes be decoded for qualitative inspection but remain largely neural and not explicitly interpretable.",
            "interpretability_method": "Prior works decode latents or visualize imagined trajectories; this paper references them but does not apply their interpretability methods.",
            "computational_cost": "Dreamer-family involves training generative latent models and imagined rollouts; compute depends on model size and rollout lengths; specific numbers not provided here.",
            "efficiency_comparison": "Dreamer/DreamerV2 reach high performance but typically need more environment data than EfficientZero to reach comparable Atari scores; EfficientZero is more sample-efficient in the 100k regime.",
            "task_performance": "Prior Dreamer variants achieve strong control performance on DMControl and some Atari tasks (in higher-data regimes); in the 100k Atari benchmark they are characterized here as less sample-efficient.",
            "task_utility_analysis": "Imagined-latent rollouts provide a way to generate training data without environment interactions (reducing sample complexity in some settings), but carefully designed supervision (temporal consistency, planning) matters in the extreme low-data Atari regime.",
            "tradeoffs_observed": "Dreamer-style latent generative objectives encourage good long-horizon imagination but can require more data and careful tuning; EfficientZero instead emphasizes temporal self-supervision and planning for sample efficiency.",
            "design_choices": "Dreamer uses stochastic state-space models and policy/value learning in imagined trajectories; EfficientZero uses MCTS over learned latents with SimSiam-style consistency and value-prefix prediction as different choices for the low-data regime.",
            "comparison_to_alternatives": "Compared to EfficientZero, Dreamer variants are powerful but (per the authors) not as sample-efficient on 100k Atari; on DMControl EfficientZero can outperform pixel-based SAC and be comparable to state-based SAC on some tasks.",
            "optimal_configuration": "Paper implies that combining strong latent-model training (temporal self-supervision) with planning and off-policy correction is an effective configuration for the 100k-step Atari setting, rather than relying solely on long imagined rollouts used by Dreamer.",
            "uuid": "e1257.4",
            "source_info": {
                "paper_title": "Mastering Atari Games with Limited Data",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "rating": 2,
            "sanitized_title": "mastering_atari_go_chess_and_shogi_by_planning_with_a_learned_model"
        },
        {
            "paper_title": "Modelbased reinforcement learning for atari",
            "rating": 2,
            "sanitized_title": "modelbased_reinforcement_learning_for_atari"
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Mastering atari with discrete world models",
            "rating": 2,
            "sanitized_title": "mastering_atari_with_discrete_world_models"
        },
        {
            "paper_title": "Exploring simple siamese representation learning",
            "rating": 2,
            "sanitized_title": "exploring_simple_siamese_representation_learning"
        },
        {
            "paper_title": "Data-efficient reinforcement learning with self-predictive representations",
            "rating": 1,
            "sanitized_title": "dataefficient_reinforcement_learning_with_selfpredictive_representations"
        },
        {
            "paper_title": "Combating the compounding-error problem with a multi-step model",
            "rating": 1,
            "sanitized_title": "combating_the_compoundingerror_problem_with_a_multistep_model"
        }
    ],
    "cost": 0.0213245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Mastering Atari Games with Limited Data</p>
<p>Weirui Ye 
Tsinghua University
† UC Berkeley, ‡ Shanghai Qi Zhi Institute</p>
<p>Shaohuai Liu 
Tsinghua University
† UC Berkeley, ‡ Shanghai Qi Zhi Institute</p>
<p>Thanard Kurutach 
Tsinghua University
† UC Berkeley, ‡ Shanghai Qi Zhi Institute</p>
<p>Pieter Abbeel 
Tsinghua University
† UC Berkeley, ‡ Shanghai Qi Zhi Institute</p>
<p>Yang Gao 
Tsinghua University
† UC Berkeley, ‡ Shanghai Qi Zhi Institute</p>
<p>Mastering Atari Games with Limited Data</p>
<p>Reinforcement learning has achieved great success in many applications. However, sample efficiency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been significant progress in sample efficient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efficient model-based visual RL algorithm built on MuZero, which we name EfficientZero. Our method achieves 194.3% mean human performance and 109.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the first time an algorithm achieves super-human performance on Atari games with such little data. EfficientZero's performance is also close to DQN's performance at 200 million frames while we consume 500 times less data. EfficientZero's low sample complexity and high performance can bring RL closer to real-world applicability. We implement our algorithm in an easy-to-understand manner and it is available at https://github.com/YeWR/EfficientZero. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.Figure 1: Our proposed method EfficientZero is 176% and 163% better than the previous SoTA performance in mean and median human normalized score and is the first to outperform the average human performance on the Atari 100k benchmark. The high sample efficiency and performance of EfficientZero can bring RL closer to the real-world applications. * manipulation, healthcare, and advertisement recommendation systems, achieving high performance while maintaining low sample complexity is the key to viability.People have made a lot of progress in sample efficient RL in the past years[10,12,40,25,23,37,20]. Among them, model-based methods have attracted a lot of attention, since both the data from real environments and the "imagined data" from the model can be used to train the policy, making these methods particularly sample-efficient[10,12]. However, most of the successes are in state-based environments. In image-based environments, some model-based methods such as MuZero[32]and Dreamer V2 [16] achieve super-human performance, but they are not sample efficient; other methods such as SimPLe [20] is quite efficient but achieve inferior performance (0.144 human normalized median scores). Recently, data-augmented and self-supervised methods applied to modelfree methods have achieved more success in the data-efficient regime[37]. However, they still fail to achieve the levels which can be expected of a human.Therefore, for improving the sample efficiency as well as keeping superior performance, we find the following three components are essential to the model-based visual RL agent: a self-supervised environment model, a mechanism to alleviate the model compounding error, and a method to correct the off-policy issue. In this work, we propose EfficientZero, a model-based RL algorithm that achieves high performance with limited data. Our proposed method is built on MuZero. We make three critical changes: (1) use self-supervised learning to learn a temporally consistent environment model, (2) learn the value prefix in an end-to-end manner, thus helping to alleviate the compounding error in the model, (3) use the learned model to correct off-policy value targets.</p>
<p>Introduction</p>
<p>Reinforcement learning has achieved great success on many challenging problems. Notable work includes DQN [27], AlphaGo [38] and OpenAI Five [7]. However, most of these works come at the cost of a large number of environmental interactions. For example, AlphaZero [39] needs to play 21 million games at training time. On the contrary, a professional human player can only play around 5 games per day, meaning it would take a human player 11,500 years to achieve the same amount of experience. The sample complexity might be less of an issue when applying RL algorithms in simulation and games. However, when it comes to real-world problems, such as robotic As illustrated as Figure 1, our model achieves state-of-the-art performance on the widely used Atari [6] 100k benchmark and it achieves super-human performance with only 2 hours of real-time gameplay. More specifically, our model achieves 194.3% mean human normalized performance and 109.0% median human normalized performance. As a reference, DQN [27] achieves 220% mean human normalized performance, and 96% median human normalized performance, at the cost of 500 times more data (200 million frames). To further verify the effectiveness of EfficientZero, we conduct experiments on some simulated robotics environments of the DeepMind Control (DMControl) suite. It achieves state-of-the-art performance and outperforms the state SAC which directly learns from the ground truth states. Our sample efficient and high-performance algorithm opens the possibility of having more impact on many real-world problems.</p>
<p>Related Work</p>
<p>Sample Efficient Reinforcement Learning</p>
<p>Sample efficiency has attracted significant work in the past. In RL with image inputs, model-based approaches [15,14] which model the world with both a stochastic and a deterministic component, have achieved promising results for simulated robotic control. Kaiser et al. [20] propose to use an action-conditioned video prediction model, along with a policy learning algorithm. It achieves the first strong performance on Atari games with as little as 400k frames. However, Kielak [21] and van Hasselt et al. [46] argue that this is not necessary to achieve strong results with model-based methods, and they show that when tuned appropriately, Rainbow [18] can achieve comparable results.</p>
<p>Recent advances in self-supervised learning, such as SimCLR [8], MoCo [17], SimSiam [9] and BYOL [13] have inspired representation learning in image-based RL. Srinivas et al. [40] propose to use contrastive learning in RL algorithms and their work achieves strong performance on image-based continuous and discrete control tasks. Later, Laskin et al. [25] and Kostrikov et al. [23] find that contrastive learning is not necessary, but with data augmentations alone, they can achieve better performance. Some researchers propose to use contrastive learning to enforce action equivariance on the learned representations [44]. Schwarzer et al. [37] propose a temporal consistency loss, which is combined with data augmentations and achieves state-of-the-art performance. Notably, our self-supervised consistency loss is quite similar to SPR [37], except we use SimSiam [8] while they use BYOL [13] as the base self-supervised learning framework. However, they only apply the learned representations in a model-free manner, while we combine the learned model with model-based exploration and policy improvement, thus leading to more efficient use of the environment model.</p>
<p>Despite the recent progress in the sample-efficient RL, today's RL algorithms are still well behind human performance when the amount of data is limited. Although traditional model-based RL is considered more sample efficient than model-free ones, current model-free methods dominate in terms of performance for image-input settings. In this paper, we propose a model-based RL algorithm that for the first time, achieves super-human performance on Atari games with limited data.</p>
<p>Reinforcement Learning with MCTS</p>
<p>Temporal difference learning [27,45,47,18] and policy gradient based methods [28,26,34,36] are two types of popular reinforcement learning algorithms. Recently, Silver et al. [38] propose to use MCTS as a policy improvement operator and has achieved great success in many board games, such as Go, Chess, and Shogi [39]. Later, the algorithm is adapted to learn the world model at the same time [32]. It has also been extended to deal with continuous action spaces [19] and offline data [33]. These MCTS RL algorithms are a hybrid of model-based learning and model-free learning.</p>
<p>However, most of them are trained with a lot of environmental samples. Our method is built on top of MuZero [32], and we demonstrate that our method can achieve higher sample efficiency while still achieving competitive performance on the Atari 100k benchmark. de Vries et al. [11] have studied the potential of using auxiliary loss similar to our self-supervised consistency loss. However, they only test on two low dimensional state-based environments and find the auxiliary loss has mixed effects on the performance. On the contrary, we find that the consistency loss is critical in most environments with high dimensional observations and limited data.</p>
<p>Multi-Step Value Estimation</p>
<p>In Q-learning [48], the target Q value is computed by one step backup. In practice, people find that incorporating multiple steps of rewards at once, i.e. z t = k−1 i=0 γ i u t+i + γ k v t+k , where u t+i is the reward from the replay buffer, v t+k is the value estimation from the target network, to compute the value target z t leads to faster convergence [27,18]. However, the use of multi-step value has off-policy issues, since u t+i are not generated by the current policy. In practice, this issue is usually ignored when there is a large amount of data since the data can be thought as approximately on-policy. TD(λ) [41] and GAE [35] improve the value estimation by better trading off the bias and the variance, but they do not deal with the off-policy issue. Recently, image input model-based algorithms such as Kaiser et al. [20] and Hafner et al. [14] use model imaginary rollouts to avoid the off-policy issue. However, this approach has the risk of model exploitation. Asadi et al. [3] proposed a multi-step model to combat the compounding error. Our proposed model-based off-policy correction method starts from the rewards in the real-world experience and uses model-based value estimate to bootstrap. Our approach balances between the off-policy issue and model exploitation.</p>
<p>Background</p>
<p>MuZero</p>
<p>Our method is built on top of the MuZero Reanalyze [32] algorithm. For brevity, we refer to it as MuZero throughout the paper. MuZero is a policy learning method based on the Monte-Carlo Tree Search (MCTS) algorithm. The MCTS algorithm operates with an environment model, a prior policy function, and a value function. The environment model is represented as the reward function R and the dynamic function G: r t = R(s t , a t ),ŝ t+1 = G(s t , a t ), which are needed when MCTS expands a new node. In MuZero, the environment model is learned. Thus the reward and the next state are approximated. Besides, the predicted policy p t = acts as a search prior over actions of a node. It helps the MCTS focus on more promising actions when expanding the node. MCTS also needs a value function V(s t ) that measures the expected return of the node s t , which provides a long-term evaluation of the tree's leaf node without further search. MCTS will output an action visit distribution π t over the root node, which is potentially a better policy, compared to the current neural network. Thus, the MCTS algorithm can be thought of as a policy improvement operator.</p>
<p>In practice, the environment model, policy function, and value function operate on a hidden abstract state s t , both for computational efficiency and ease of environment modeling. The abstract state is extracted by a representation function H on observations o t : s t = H(o t ). All of the mentioned models above are usually represented as neural networks. During training, the algorithm collects roll-out data in the environment using MCTS, resulting in potentially higher quality data than the current neural network policy. The data is stored in a replay buffer. The optimizer minimizes the following loss on the data sampled from the replay buffer:
L(u t , r t ) + λ 1 L(π t , p t ) + λ 2 L(z t , v t )(1)
Here, u t is the reward from the environment, r t = R(s t , a t ) is the predicted reward, π t is the output visit count distribution of the MCTS, p t = P(s t ) is the predicted policy, z t = k−1 i=0 γ i u t+i +γ k v t+k is the bootstrapped value target and v t = V(s t ) is the predicted value. Specifically, the reward function R, policy function P, value function V, the representation function H and the dynamics function G are trainable neural networks. It is worth noting that MuZero does not explicitly learn the environment model. Instead, it solely relies on the reward, value, and policy prediction to learn the model.</p>
<p>Monte-Carlo Tree Search</p>
<p>Monte-Carlo Tree Search [1,38,39,16], or MCTS, is a heuristic search algorithm. In our setup, MCTS is used to find an action policy that is better than the current neural network policy.</p>
<p>More specifically, MCTS needs an environment model, including the reward function and the dynamics function. It also needs a value function and a policy function, which act as heuristics during search. MCTS operates by expanding a search tree from the current node. It saves computation by selectively expanding nodes. To find a high-quality decision, the expansion process has to balance between exploration versus exploitation, i.e. balance between expanding a node that is promising with more visits versus a node with lower performance but fewer visits. MCTS employs the UCT [30,22] rule, i.e. UCB [4] on trees. At every node expansion step, UCT will select a node as follows [16]:
a k = arg max a Q(s, a) + P (s, a) b N (s, b) 1 + N (s, a) c 1 + log b N (s, b) + c 2 + 1 c 2(2)
where, Q(s, a) is the current estimate of the Q-value, P (s, a) is the current neural network policy for selecting this action, helping the MCTS prioritize exploring promising part of the tree. During training time, P (s, a) is usually perturbed by noises to allow explorations. N (s, a) denotes how many times this state-action pair is visited in the tree search, and N (s, b) denote that of a's siblings. Thus this term will encourage the search to visit the nodes whose siblings are visited often, but itself less visited. Finally, the last term gives a weights to the previous terms.</p>
<p>After expanding the nodes for a pre-defined number of times, the MCTS will return how many times each action under the root node is visited, as the improved policy to the root node. Thus, MCTS can be considered as a policy improvement operator in the RL setting.</p>
<p>EfficientZero</p>
<p>Model-based algorithms have achieved great success in sample-efficient learning from lowdimensional states. However, current visual model-based algorithms either require large amounts of training data or exhibit inferior performance to model-free algorithms in data-limited settings [37]. Many previous works even suspect whether model-based algorithms can really offer data efficiency when using image observations [46]. We provide a positive answer here. We propose the Effi-cientZero, a model-based algorithm built on the MCTS, that achieves super-human performance on the 100k Atari benchmark, outperforming the previous SoTA to a large degree.</p>
<p>When directly running MCTS-based RL algorithms such as MuZero, we find that they do not perform well on the limited-data benchmark. Through our ablations, we confirm the following three issues which pose challenges to algorithms like MuZero in data-limited settings.</p>
<p>Lack of supervision on environment model. First, the learned model in the environment dynamics is only trained through the reward, value and policy functions. However, the reward is only a scalar signal and in many scenarios, the reward will be sparse. Value functions are trained with bootstrapping, and thus are noisy. Policy functions are trained with the search process. None of the reward, value and policy losses can provide enough training signals to learn the environment model.</p>
<p>Hardness to deal with aleatoric uncertainty. Second, we find that even with enough data, the predicted rewards still have large prediction errors. This is caused by the aleatoric uncertainty of the underlying environment. For example, when the environment is hard to model, the reward prediction errors will accumulate when expanding the MCTS tree to a large depth.</p>
<p>Off-policy issues of multi-step value. Lastly, as for value targets, MuZero uses the multi-step reward observed in the environment. Although it allows rewards to be propagated to the value function faster, it suffers from severe off-policy issues and hinders convergence given limited data.</p>
<p>To address the above issues, we propose the following three critical modifications, which can greatly improve performance when samples are limited. In previous MCTS RL algorithms, the environment model is either given or only trained with rewards, values, and policies, which cannot provide sufficient training signals due to their scalar nature. The problem is more severe when the reward is sparse or the bootstrapped value is not accurate. The MCTS policy improvement operator heavily relies on the environment model. Thus, it is vital to have an accurate one. We notice that the outputŝ t+1 from the dynamic function G should be the same as s t+1 , i.e. the output of the representation function H with input of the next observation o t+1 (Fig. 2). This can help to supervise the predicted next stateŝ t+1 using the actual s t+1 , which is a tensor with at least a few hundred dimensions. This providesŝ t+1 with much more training signals than the default scalar reward and value.</p>
<p>Self-Supervised Consistency Loss</p>
<p>Notably, in MCTS RL algorithms, the consistency between the hidden states and the predicted states can be shaped through the dynamics function directly without extra models. More specifically, we adopt the recently proposed SimSiam [9] self-supervised framework. SimSiam [9] is a self-supervised method that takes two augmentation views of the same image and pulls the output of the second branch close to that of the first branch, where the first branch is an encoder network without gradient, and the second is the same encoder network with the gradient and a predictor head. The head can simply be an MLP.</p>
<p>Note that SimSiam only learns the representation of individual images, and is not aware of how different images are connected. The learned image representations of SimSiam might not be a good candidate for learning the transition function, since adjacent observations might be encoded to very different representation encodings. We propose a self-supervised method that learns the transition function, along with the image representation function in an end-to-end manner, as Figure 2 shows. Since we aim to learn the transition between adjacent observations, we pull o t and o t+1 close to each other. The transition function is applied after the representation of o t , such that s t is transformed toŝ t+1 , which now represents the same entity as the other branch. Then both of s t+1 andŝ t+1 go through a common projector network. Since s t+1 is potentially a more accurate description of o t+1 compared toŝ t+1 , we make the o t+1 branch as the target branch. It is common in self-supervised learning that the second or the third layer from the last is chosen as the features for some reason.</p>
<p>Here, we choose the outputs from the representation network or the dynamics network as the hidden states rather than those from the projector or the predictor. The two adjacent observations provide two views of the same entity. In practice, we find that applying augmentations to observations on the image helps to further improve the learned representation quality [40,37]. We also unroll the dynamic function recurrently for 5 further steps and also pullŝ t+k close to s t+k (k = 1, ..., 5). Please see App.A.1 for more details. We note that our temporal consistency loss is similar to SPR [37], an unsupervised representation method applied on rainbow. However, the consistency loss in our case is applied in a model-based manner, and we use the SimSiam loss function.</p>
<p>End-To-End Prediction of the Value Prefix</p>
<p>In model-based learning, the agent needs to predict the future states conditioned on the current state and a series of hypothetical actions. The longer the prediction, the harder to predict it accurately, due to the compounding error in the recurrent rollouts. This is called the state aliasing problem. The environment model plays an important role in MCTS. The state aliasing problem harms the MCTS expansion, which will result in sub-optimal exploration as well as sub-optimal action search. t t+10 t+20 Figure 3: A sample trajectory from the Atari Pong game. In this case, the right player didn't move and missed the ball.</p>
<p>Predicting the reward from an aliased state is a hard problem. For example, as shown in Figure  3, the right agent loses the ball. If we only see the first observation, along with future actions, it is very hard both for an agent and a human to predict at which exact future timestep the player would lose a point. However, it is easy to predict the agent will miss the ball after a sufficient number of timesteps if he does not move. In practice, a human will never try to predict the exact step that he loses the point but will imagine over a longer horizon and thus get a more confident prediction.</p>
<p>Inspired by this intuition, we propose an end-to-end method to predict the value prefix. We notice that the predicted reward is always used in the estimation of the Q-value Q(s, a) in UCT of Equation 2
Q(s t , a) = k−1 i=0 γ i r t+i + γ k v t+k(3)
, where r t+i is the reward predicted from unrolled stateŝ t+i . We name the sum of rewards k−1 i=0 γ i r t+i as the value prefix, since it is used as a prefix in the later Q-value computation. We propose to predict value prefix from the unrolled states (s t ,ŝ t+1 , · · · ,ŝ t+k−1 ) in an end-to-end manner, i.e. value-prefix = f (s t ,ŝ t+1 , · · · ,ŝ t+k−1 ). Here f is some neural network architecture that takes in a variable number of inputs and outputs a scalar. We choose the LSTM in our experiment. During the training time, the LSTM is supervised at every time step, since the value prefix can be computed whenever a new state comes in. This per-step rich supervision allows the LSTM can be trained well even with limited data. Compared with the naive per step reward prediction and summation approach, the end-to-end value prefix prediction is more accurate, because it can automatically handle the intermediate state aliasing problem. See Experiment Section 5.3 for empirical evaluations. As a result, it helps the MCTS to explore better, and thus increases the performance. See App.A.1 for architectural details.</p>
<p>Model-Based Off-Policy Correction</p>
<p>In MCTS RL algorithms, the value function fits the value of the current neural network policy. However, in practice as MuZero Reanalyze does, the value target is computed by sampling a trajectory from the replay buffer and computing: z t = k−1 i=0 γ i u t+i + γ k v t+k . This value target suffers from off-policy issues, since the trajectory is rolled out using an older policy, and thus the value target is no longer accurate. When data is limited, we have to reuse the data sampled from a much older policy, thus exaggerating the inaccurate value target issue.</p>
<p>In previous model-free settings, there is no straightforward approach to fix this issue. On the contrary, since we have a model of the environment, we can use the model to imagine an "online experience". More specifically, we propose to use rewards of a dynamic horizon l from the old trajectory, where l &lt; k and l should be smaller if the trajectory is older. This reduces the policy divergence by fewer rollout steps. Further, we redo an MCTS search with the current policy on the last state s t+l and compute the empirical mean value at the root node. This effectively corrects the off policy issue using imagined rollouts with current policy and reduces the increased bias caused by setting l less than k. Formally, we propose to use the following value target:
z t = l−1 i=0 γ i u t+i + γ l ν MCTS t+l (4)
where l &lt;= k and the older the sampled trajectory, the smaller the l. ν MCTS (s t+l ) is the root value of the MCTS tree expanded from s t+l with the current policy, as MuZero non-Reanalyze does. See App.A.4 for how to choose l. In practice, the computation cost of the correction is two times on the reanalyzed side. However, the training will not be affected due to the parallel implementation.</p>
<p>Experiments</p>
<p>In this section, we aim to evaluate the sample efficiency of the proposed algorithm. Here, the sample efficiency is measured by the performance of each algorithm at a common, small amount of environment transitions, i.e. the better the performance, the higher the sample efficiency. More specifically, we use the Atari 100k benchmark. Intuitively, this benchmark asks the agent to learn to play Atari games within two hours of real-world game time. Additionally, we conduct some ablation studies to investigate and analyze each component on Atari 100k. To further show the sample efficiency, we apply EfficientZero to some simulated robotics environments on the DMControl 100k benchmark, which contains the same 100k environment steps.</p>
<p>Environments</p>
<p>Atari 100k Atari 100k was first proposed by the SimPLe [20] method, and is now used by many sample-efficient RL works, such as Srinivas et al. [40], Laskin et al. [25], Kostrikov et al. [23], Schwarzer et al. [37]. The benchmark contains 26 Atari games, and the diverse set of games can effectively measure the performance of different algorithms. The benchmark allows the agent to interact with 100 thousand environment steps, i.e. 400 thousand frames due to a frameskip of 4, with each environment. 100k steps roughly correspond to 2 hours of real-time gameplay, which is far less than the usual RL settings. For example, DQN [27] uses 200 million frames, which is around 925 hours of real-time gameplay. Note that the human player's performance is tested after allowing the human to get familiar with the game after 2 hours as well. We report the raw performance on each game, as well as the mean and median of the human normalized score. The human normalized score is defined as: (score agent − score random )/(score human − score random ).</p>
<p>We compare our method to the following baselines. (1) SimPLe [20], a model-based RL algorithm that learns an action conditional video prediction model and trains PPO within the learned environment.</p>
<p>(2) OTRainbow [21], which tunes the hyper-parameters of the Rainbow [18] method to achieve higher sample efficiency. (3) CURL [40], which uses contrastive learning as a side task to improve the image representation quality. (4) DrQ [23], which adds data augmentations to the input images while learning the original RL objective. (5) SPR [37], the previous SoTA in Atari 100k which proposes to augment the Rainbow [18] agent with data augmentations as well as a multi-step consistency loss using BYOL-style self-supervision. (6) MuZero [32] with our implementations and the same hyper-parameters as EfficientZero. (7) Random Agent (8) Human performance.</p>
<p>DeepMind Control 100k Tassa et al. [42] propose the DMControl suite, which includes some challenging visual robotics tasks with continuous action space. And some works [14,40] have benchmarked for the sample efficiency on the DMControl 100k which contains 100k environment steps data. Since the MCTS-based methods cannot deal with tasks with continuous action space, we discretize each dimension into 5 discrete slots in MuZero [32] and EfficientZero. To avoid the dimension explosion, we evaluate EfficientZero in three low-dimensional tasks.</p>
<p>We compare our method to the following baselines. (1) Pixel SAC, which applies SAC directly to pixels. (2) SAC-AE [50], which combines the SAC and an auto-encoder to handle image-based inputs. (3) State SAC, which applies SAC directly to ground truth low dimensional states rather than the pixels. (4) Dreamer [14], which learns a world model and is trained in dreamed scenarios. (5) CURL [40], the previous SoTA in DMControl 100k. (6) MuZero [32] with action discretizations. Table 1 shows the results of EfficientZero on the Atari 100k benchmark. Normalizing our score with the score of human players, EfficientZero achieves a mean score of 1.904 and a median score of 1.160. As a reference, DQN [27] achieves a mean and median performance of 2.20 and 0.959 on these 26 games. However, it is trained with 500 times more data (200 million frames). For the first time, an agent trained with only 2 hours of game data can outperform the human player in terms of the mean and median performance. Among all games, our method outperforms the human in 14 out of 26 games. Compared with the previous state-of-the-art method (SPR [37]), we are 170% and 180% better in terms of mean and median score respectively. As for more robust results, we record the aggregate metrics in App.A.5 with statistical tools proposed by Agarwal et al. [2].</p>
<p>Results</p>
<p>Apart from the Atari games, EffcientZero achieves remarkable results in the simulated tasks. As shown in Table 2, EffcientZero outperforms CURL, the previous SoTA, to a considerable degree and keeps a smaller variance but MuZero cannot work well. Notably, EfficientZero achieves comparable results to the state SAC, which consumes the ground truth states and is considered as the oracles.  </p>
<p>Ablations</p>
<p>In Section 4, we discuss three issues that prevent MuZero from achieving high performance when data is limited: (1) the lack of environment model supervision, (2) the state aliasing issue, and (3) the off-policy target value issue. We propose three corresponding approaches to fix those issues and demonstrate the usefulness of the combination of those approaches on a wide range of 26 Atari games. In this section, we will analyze each component individually.</p>
<p>Each Component Firstly, we do an ablation study by removing the three components from our full model one at a time. As shown in Table 3, we find that removing any one of the three components will lead to a performance drop compared to our full model. Furthermore, the richer learning signals are the aspect Muzero lacks most in the low-data regime as the largest performance drop is from the version without consistency supervision. As for the performance in the high-data regime, We find that the temporal consistency can significantly accelerate the training. The value prefix seems to be  Figure 4: Evaluations of image reconstructions based on latent states extracted from the model with or without self-supervised consistency. The predicted next states with consistency can basically be reconstructed into observations while the ones without consistency cannot.</p>
<p>helpful during the early learning process, but not as much in the later stage. The off-policy correction is not necessary as it is specifically designed under limited data.</p>
<p>Temporal Consistency As the version without self-supervised consistency cannot work well in most of the games, we attempt to dig into the reason for such phenomenon. We design a decoder D to reconstruct the original observations, taking the latent states as inputs. Specifically, the architecture of D and the H are symmetrical, which means that all the convolutional layers are replaced by deconvolutional layers in D and the order of the layers are reversed in D. Therefore, H is an encoder to obtain state s t from observation o t and D tries to decode the o t from s t . In this ablation, we freeze all parameters of the trained EfficientZero network with or without consistency respectively and the reconstructed results are shown in different columns of Figure 4. We regard the decoder as a tool to visualize the current states and unrolled states, shown in different rows of Figure 4. Here we note that M con is the trained EfficientZero model with consistency and M non is the one without consistency. As shown in Figure 4, as for the current state s t , the observation is reconstructed well enough in the two versions. However, it is remarkable that the the decoder given M non can not reconstruct images from the unrolled predicted statesŝ t+k while the one given M con can reconstruct basic observations.</p>
<p>To sum up, there are some distributional shifts between the latent states from the representation network and the states from the dynamics function without consistency. The consistency component can reduce the shift and provide more supervision for training the dynamics network.</p>
<p>Value Prefix</p>
<p>We further validate our assumptions in the end-to-end learning of value prefix, i.e. the state aliasing problem will cause difficulty in predicting the reward, and end-to-end learning of value prefix can alleviate this phenomenon.</p>
<p>To fairly compare directly predicting the reward versus end-to-end learning of the value prefix, we need to control for the dataset that both methods are trained on. Since during the RL training, the dataset distribution is determined by the method, we opt to load a half-trained Pong model and rollout total 100k steps as the common static dataset. We split this dataset into a training set and a validation set. Then we run both the direct reward prediction and the value prefix method on the training split. As shown in Figure 5, we find that the direct reward prediction method has lower losses on the training set. However, the value prefix's validation error is much smaller when unrolled for 5 steps. This shows that the value prefix method avoids overfitting the hard reward prediction problem, and thus it can reduce the state aliasing problem, reaching a better generalization performance.  Table 4. The error of unrolled next 5 states means the average error of the unrolled 1-5 states with dynamics network from current states. The error is smaller in both current states and the unrolled states with off-policy correction. Thus, the correction component does reduce the bias caused by the off-policy issue. Furthermore, we also ablate the value error of the trajectories at distinct stages in Table 5. We can find that the value error becomes smaller as the trajectories are fresher. This indicates that the off-policy issue is severe due to the staleness of the data. More significantly, the off-policy correction can provide more accurate target value estimation for the trajectories at distinct time-steps as all the errors with correction shown in the table are smaller than those without correction at the same stage. </p>
<p>Off-Policy Correction</p>
<p>Discussion</p>
<p>In this paper, we propose a sample-efficient model-based method EfficientZero. It achieves superhuman performance on the Atari games with as little as 2 hours of the gameplay experience and state-of-the-art performance on some DMControl tasks. Apart from the full results, we do detailed ablation studies to examine the effectiveness of the proposed components. This work is one step towards running RL in the physical world with complex sensory inputs. In the future, we plan to extend it to more directions, such as a better design for the continuous action space. And we also plan to study the acceleration of MCTS and how to combine this framework with life-long learning.</p>
<p>A Appendix</p>
<p>A.1 Models and Hyper-parameters</p>
<p>As for the architecture of the networks, there are three parts in our model pipeline: the representation part, the dynamics part, and the prediction part. The architecture of the representation part is as follows:</p>
<p>• 1 convolution with stride 2 and 32 output planes, output resolution 48x48. (BN + ReLU)</p>
<p>• 1 residual block with 32 planes.</p>
<p>• 1 residual downsample block with stride 2 and 64 output planes, output resolution 24x24.</p>
<p>• 1 residual block with 64 planes.</p>
<p>• Average pooling with stride 2, output resolution 12x12. (BN + ReLU)</p>
<p>• 1 residual block with 64 planes.</p>
<p>• Average pooling with stride 2, output resolution 6x6. (BN + ReLU)</p>
<p>• 1 residual block with 64 planes.</p>
<p>, where the kernel size is 3 × 3 for all operations.</p>
<p>As for the dynamics network, we follow the architecture of MuZero [32] but reduce the residual blocks from 16 to 1. Furthermore, we add an extra residual link in the dynamics part to keep the information of historical hidden states during recurrent inference. The design of the dynamics network is listed here:</p>
<p>• Concatenate the input states and input actions into 65 planes. In the prediction part, we use two-layer MLPs with batch normalization to predict the reward, value, or policy. Considering the stability of the prediction part, we set the weights and bias of the last layer to zero in prediction networks. As for the reward prediction network, it predicts the sum of the rewards, namely value prefix: r t , h t+1 = R(ŝ t+1 , h t ), where r t is the predicted sum of rewards, h 0 is zero-initialized and hidden size of LSTM is 512. The architecture of the value prediction network is as follows: The horizontal length of the LSTM during training is limited to the unrolled steps l unroll = 5, but it will be larger in MCTS as the dynamics process can go deeper. Therefore, we reset the hidden state of LSTM after ζ = 5 steps of recurrent inference, where ζ is the valid horizontal length.</p>
<p>The design of the reward and policy prediction networks are the same except for the dimension of the outputs:</p>
<p>• 1 residual block with 64 planes.  , where D = 601 in the reward prediction network and D is equal to the action space in the policy prediction network.</p>
<p>Here is the brief introduction of the training pipeline, taking one-step rollout as an example.
s t = H(o t ) s t+1 = H(o t+1 ) s t+1 = G(s t , a t ) v t = V(s t ) p t = P(s t ) r t , h t+1 = R(ŝ t+1 , h t ) = R(G(s t , a t ), h t )(5)
, where H is the representation network, G is the dynamics network, V is the value prediction network, P is the policy prediction network, R is the reward (value prefix) prediction network. o t , s t , a t are observations, states and actions. h t is the hidden states in recurrent neural networks.</p>
<p>Here is the training loss, taking one-step rollout as an example:</p>
<p>L similarity (s t+1 ,ŝ t+1 ) = L 2 (sg(P 1 (s t+1 )), P 2 (P 1 (ŝ t+1 ))) L t (θ) = L(u t , r t ) + λ 1 L(π t , p t ) + λ 2 L(z t , v t )
+ λ 3 L similarity (s t+1 ,ŝ t+1 ) + c||θ|| 2 L(θ) = 1 l unroll lunroll−1 i=0 L t+i (θ)(6)
, where L is the total loss of the unrolled l unroll steps, L 1 is the Cross-Entropy loss, and L 2 is the negtive cosine similarity loss. Besides, P 1 is a 3-layer MLP while P 2 is a 2-layer MLP. The dimension of the hidden layers is 512 and the dimension of the output layers is 1024. We add batch normalization between every two layers in those MLP except the final layer. sg(P 1 ) means stopping gradients.</p>
<p>We stack 4 historical frames, with an interval of 4 frames-skip. Thus the input effectively covers 16 frames of the game history. We stack the input images on the channel dimension, resulting in a 96 × 96 × 12 tensor. We do not use any extra state normalization besides the batch norm and we choose reward clipping to keep better scales in the searching process.</p>
<p>Generally, compared with MuZero [32], we reduce the number of residual blocks and the number of planes as we find that there is no capability issue caused by much smaller networks in our EfficientZero with limited data. In another word, such a tiny network can acquire good performance in the limited setting.</p>
<p>For other details, we provide hyper-parameters in Table 6. It is notable that we train the model for 120k steps where we only collect data during the first 100k steps. In this way, the latter trajectories can be fully used in training. Besides, the learning rate will drop after every 100k training steps (from 0.2 to 0.02 at 100k).</p>
<p>A.2 More Ablations</p>
<p>In the experiment section , we list some ablation studies to prove the effectiveness of each component. In this section, we will display more results for the ablation study.</p>
<p>Firstly, the detailed results of the ablation study of each component are listed in Table 7. In this table, We find that the full version of EfficientZero outperforms the others without any one of the components. Furthermore, for those environments EfficientZero can already solve, the performance is similar between the full version and the version without off-policy correction, such as Breakout, Pong, etc. In such a case, the off-policy issue is not severe, which is the reason for this phenomenon. Besides, for some environments with sparse rewards, the value prefix component matters, such as Pong; and for those with dense rewards, the state aliasing problem has less negative effects for the reward signals are sufficient, such as Qbert. As for the version without self-supervised consistency, the results of all the environments are much poorer. In addition, we do the ablation study for the data augmentation technique in the consistency component to examine the effect of data augmentations. We apply a random small shift of 0-4 pixels as well as the change of the intensity as the augmentation techniques. Here we choose several Atari games and train the model for 100k steps. The results are shown in Table 8. We can find that the version without data augmentation has similar performances while the version without consistency component is worse. This indicates that the improvement of the consistency component is basically from the self-supervised learning loss rather than the data augmentation. Finally, we also do the ablation study for the MCTS root value and the dynamic horizon in the off-policy correction component. Here we choose several Atari games and train the model for 100k steps. As shown in Table 9, the version without dynamic horizon has poorer results than that without the MCTS root value. In the off-policy correction component, the dynamic horizon seems more important. </p>
<p>A.3 MCTS Details</p>
<p>Our policy searching approach is based on Monte-Carlo tree search (MCTS). We follow the procedure in MuZero [32], which includes three stages and repeats the searching process for N sim = 50 simulations. Here are some brief introductions for each stage.</p>
<p>Selection In the selection part, it targets at choosing an appropriate unvisited node while balancing exploration and exploitation with UCT:
a k = arg max a Q(s, a) + P (s, a) b N (s, b) 1 + N (s, a) c 1 + log b N (s, b) + c 2 + 1 c 2(7)
, where Q(s, a) is the average Q values after simulations, N (s, a) is the total visit counts at state s by selecting action a, and P (s, a) is the policy prior set in the expansion process. In each simulation, the MCTS starts from the root node s 0 . And for each time-step k = 1...l of the simulation, the algorithm will select the action a k according to the UCT. Usually, c 1 = 1.25 and c 2 = 19652 according to the literature [38,39,16].</p>
<p>However, the default Q value of the unvisted node is set to 0, which indicates the worst state. To give a better Q-value estimation of the unvisited nodes, we evaluate a mean Q value mechanism in each simulation for tree nodes, similar to the implementation of Elf OpenGo [43].
Q(s root ) = 0 Q(s) =Q (s parent ) + b 1 N (s,b)&gt;0 Q(s, b) 1 + b 1 N (s,b)&gt;0 Q(s, a) : = Q(s, a) N (s, a) &gt; 0 Q(s) N (s, a) = 0(8)
, whereQ(s) is the estimated Q value for unvisited nodes to make better selections considering exploration and exploitation. s root is the state of the root node and s parent is the state of the parent node of s. In experiments, we find that the mean Q value mechanism gives a better exploration than the default one.</p>
<p>Expansion Then the newly selected node will be expanded with the predicted reward and policy as its prior. Furthermore, when the root node is to expand, we apply the Dirichlet noise to the policy prior during the self-play stage and the reanalyzing stage to give more explorations.
P (s, a) := (1 − ρ)P (s, a) + ρN D (ξ)(9)
, where N D (ξ) is the Dirichlet noise distribution, ρ, ξ is set to 0.25 and 0.3 respectively. However, we do not use any noise and set ρ to 0 instead for those non-root node or during evaluations.</p>
<p>Backup After selecting and expanding a new node, we need to backup along the current searching trajectory to update the Q(s, a). Considering the scales of values in distinct environments, we compute a normalized Q-value by using the minimum-maximum values calculated along with all visited tree nodes, which is applied in MuZero [32]. However, when the data is limited, the small difference between the minimum and maximum values will result in overconfidence in UCT calculation. For example, when all the Q-values in those visited tree nodes are in a range of 0 to 10 −4 , the normalized Q-value of 10 −5 and 5 × 10 −5 will make a huge difference as one is normalized to 0.1 and another is 0.5. Therefore, we set a threshold here to reduce overconfidence in such occasions, which is called the soft minimum-maximum updates: 
Q(s k−1 , a k ) = Q(s k−1 , a k ) − min (
, where , the threshold to give a smooth range of the min-max bound, is set to 0.01.</p>
<p>After all the expansions in the MCTS, we will obtain average value and visit count distributions of the root node. Here, the root value can be applied in off-policy correction and the visit count distribution is the target policy distribution:
π(s, a) = N (s, a) 1/T b N (s, b) 1/T(11)
We decay the temperature of the MCTS output policy distribution here twice during training, at 50% and 75% of the training progress to 0.5 and 0.25 respectively.</p>
<p>A.4 Training Details</p>
<p>In this subsection, we will introduce more training details.</p>
<p>Pipeline As for the code implementation of EfficientZero, we design a paralleled architecture with a double buffering mechanism in Pytorch and Ray, as shown in Figure 6.</p>
<p>Intuitively, we will describe the training process in a synchronized way. Firstly, the data workers called self-play actors are aimed at doing self-play with the given model updated within 600 training steps and then they will send the rolled-out trajectories into the replay buffer. Then the CPU rollout workers attempt to prepare the contexts of those batch transitions sampled from the replay buffer, in which way only CPU resources are required. Afterward, the GPU batch workers reanalyze those past data with the given contexts by the given target model, and most of the time-consuming parts in this procedure are in GPUs. Considering the frequent utilization of CPUs and GPUs in MCTS, the searching process is assigned for those GPU workers. Finally, the learner will obtain the reanalyzed batch and begin to train the agent.  The learner, all the data workers, CPU workers, and GPU workers start in parallel. The data workers and CPU workers share the replay buffer to sample data while the CPU and GPU workers share a context queue for reanalyzing data. Besides, the learner and the GPU workers use a batch queue to communicate. In such a design, we can utilize the CPU and GPU as much as possible.</p>
<p>Self-play During self-play, the priorities of the transition to collect are set to the max of the whole priorities in replay buffer. We also update the priority in EfficientZero according to MuZero [32]:
P (i) = p α i k p α k ,
where p i is the L1 error of the value during training. And the we scale with important sampling ratio w i = ( 1 N ×P (i) ) β . We set α to 0.6 and anneal β from 0.4 to 1.0, following prioritized replay [31]. However, we find the priority mechanism only improves a little with limited data. Considering the long horizons in atari games, we collect the intermediate sequences of 400 moves.</p>
<p>Reanalyze The reanalyzed part is introduced in MuZero [32], which revisits the past trajectories and re-executes the data with lasted target model to obtain a fresher value and policy with model inference as well as MCTS.</p>
<p>For the off-policy correction, the target values are reanalyzed as follows:
z t = l−1 i=0 γ i u t+i + γ l ν MCTS t+l , l = (k − T current − T st τ T total ).clip(1, k), l ∈ <a href="12">1, k</a>
, where k is the TD steps here, and is set to 5; T current is the current training steps, T st is the training steps of collecting the data s t , T total is the total training steps (100k), and τ is a coefficient which is set to 0.3. Intuitively, l is to define how fresh the collected data s t is. When the trajectory is stale, we need to unroll less to estimate the target values for the sake of the gaps between current model predictions and the stale trajectory rollouts. Besides, we replace the predicted value v t+k with the averaged root value from MCTS ν MCTS t+l to alleviate the off-policy bias.</p>
<p>Notably, we re-sample Dirichlet noise into the MCTS procedure in reanalyzed part to improve the sample efficiency with a more diverse searching process. Besides, we reanalyze the policy among 99% of the data and reanalyze the value among 100% data.</p>
<p>A.5 Evaluation</p>
<p>We evaluate the EfficientZero on Atari 100k benchmark with a total of 26 games. Here are the evaluation curves during training, as shown in Figure 7. The average of the total rewards among 32 evaluation seeds for 3 runs is show on the y-axis and the number of total training steps is 120,000, shown on the x-axis. Figure 8: Aggregate metrics on Atari 100k benchmark with 95% CIs. Here the higher mean, median and IQM scores and lower optimality gap indicate better performance. The CIs are estimated by the percentile bootstrap with stratified sampling. All results except EfficientZero are from Agarwal et al. [2]. And all the methods are based on 10 runs per game except SimPLe with 5 runs and EfficientZero with 3 runs. EfficientZero significantly outperforms the other methods concerning the four metrics.</p>
<p>Besides, we also report the scores for 3 runs (different seeds) with 32 evaluation seeds across the 26 Atari games, which is shown in Table 10.</p>
<p>Recently, Agarwal et al. [2] propose to use statistical tools to present more robust and efficient aggregate metrics. Here we display the corresponding results based on its open-sourced codebase. Figure 8 illustrates that EfficientZero significantly outperforms the other methods on Atari 100k benchmark concerning all the metrics.</p>
<p>A.6 Open Source EfficientZero Implementation</p>
<p>MCTS-based RL algorithms present a promising future research direction: to achieve strong performance with model-based methods. However, two major practical obstacles prevent them from being widely used currently. First, there are no high-quality open-source implementations of these algorithms. Existing implementations [49,24] can only deal with simple state-based environments, such as CartPole [5]. Accurately scaling to complex image input environments requires non-trivial engineering efforts. Second, MCTS RL algorithms such as MuZero [32] require a large number of computations. For example, MuZero needs 64 TPUs to train 12 hours for one agent on Atari games. The high computational costs pose problems both for the future development of such methods as well as practical applications.</p>
<p>We think our open-source implementation of EfficientZero can drastically accelerate the research in MCTS RL algorithms. Our implementation is computationally friendly. To train an Atari agent for 100k steps, it only needs 4 GPUs to train 7 hours. Our framework could potentially have a large impact on many real-world applications, such as robotics since it requires significantly fewer samples.</p>
<p>Our open-source framework aims to provide an easy way to understand the implementation while keeping relatively high compute efficiency. As shown in Fig. 9, the system is composed of four components: the replay buffer, the experience sampling actor, the reanalyze training target preparation module, and the training component.  To make sure the framework is easy to use, we implement them based on Ray [29], and the four components are implemented as ray actors which run in parallel. The main computation bottleneck is in the reanalyze module, which samples from the replay, and runs an MCTS search on each observation. To accelerate the reanalyze module, we split the reanalyze computation into the CPU part and the GPU part, such that computation on CPU and GPU are run in parallel. We use a different number of actors between CPU and GPU to match their total throughput. To increase the throughput on GPU, we also collocate multiple batch computation threads on one GPU, as in Tian et al. [43]. We also implement the MCTS in C++ to avoid performance issues with Python on large amounts of atomic computations.</p>
<p>We implement the MCTS by a couple of important techniques, which are quite crucial to improve the efficiency of the MCTS process. On the one hand, we implement batch MCTS to allow the agent to search a batch of trees in parallel, to enlarge the throughput of MCTS during self-play and reanalyzing targets. On the other, we choose C++ in the MCTS process. However, the process of MCTS needs to do searching as well as model inference, which needs to communicate with Pytorch. Therefore, we use Python to do model inference, C++ to do other atomic computations, and Cython to communicate between Python contexts and C++ contexts. In another word, we use pure C++ to do selection, expansion, and backup while using neural networks in Python. Meanwhile, we build a database to store the hidden states in Python while storing the corresponding data index during the searching process in C++. For more details of the implementation, please refer to https://github.com/YeWR/EfficientZero.</p>
<p>Figure 2 :
2The self-supervised consistency loss.</p>
<p>Figure 5 :
5Training and validation losses of direct reward prediction method and the value prefix method.</p>
<p>To prove the effectiveness of the off-policy correction component, we compare the error between the target values and the ground truth values with or without off-policy correction. Specifically, the ground truth values are estimated by Monte Carlo sampling.We train a model for the game UpNDown with total 100k training steps, and collect the trajectories at different training stages respectively (20k, 40k, ..., 100k steps). Then we calculate the ground truth values with the final model. We choose the trajectories at the same stage (20k) and use the final model to evaluate the target values with or without off-policy correction, following the Equation 4. We evaluate the L1 error of the target values and the ground truth, as shown in</p>
<p>• 1 convolution with stride 2 and 64 output planes. (BN) • A residual link: add up the output and the input states. (ReLU) • 1 residual block with 64 planes.</p>
<p>with 512 hidden size. (BN + ReLU) • 1 fully connected layers and 32 output dimensions. (BN + ReLU) • 1 fully connected layers and 601 output dimensions.</p>
<p>• 1 1x1convolution and 16 output planes. (BN + ReLU) • Flatten. • 1 fully connected layers and 32 output dimensions. (BN + ReLU) • 1 fully connected layers and D output dimensions.</p>
<p>s,a)∈T ree Q(s, a) max(max (s,a)∈T ree Q(s, a) − min (s,a)∈T ree Q(s, a), )</p>
<p>Figure 6 :
6Pipeline of the EfficientZero implementation.</p>
<p>Figure 7 :
7Evaluation curves of EfficientZero on Atari 100k benchmark for individual games.</p>
<p>Figure 9 :
9EfficientZero implementation overview.</p>
<p>Table 1 :
1Scores on the Atari 100k benchmark (3 runs with 32 seeds). EfficientZero achieves superhuman performance with only 2 hours of real-time game play. Our method is 176% and 163% better than the previous SoTA performance, in mean and median human normalized score respectively.Game 
Random Human SimPLe OTRainbow 
CURL 
DrQ 
SPR 
MuZero 
Ours </p>
<p>Alien 
227.8 
7127.7 
616.9 
824.7 
558.2 
771.2 
801.5 
530.0 
808.5 
Amidar 
5.8 
1719.5 
88.0 
82.8 
142.1 
102.8 
176.3 
38.8 
148.6 
Assault 
222.4 
742.0 
527.2 
351.9 
600.6 
452.4 
571.0 
500.1 
1263.1 
Asterix 
210.0 
8503.3 
1128.3 
628.5 
734.5 
603.5 
977.8 
1734.0 
25557.8 
Bank Heist 
14.2 
753.1 
34.2 
182.1 
131.6 
168.9 
380.9 
192.5 
351.0 
BattleZone 
2360.0 
37187.5 
5184.4 
4060.6 
14870.0 12954.0 16651.0 
7687.5 
13871.2 
Boxing 
0.1 
12.1 
9.1 
2.5 
1.2 
6.0 
35.8 
15.1 
52.7 
Breakout 
1.7 
30.5 
16.4 
9.8 
4.9 
16.1 
17.1 
48.0 
414.1 
ChopperCmd 
811.0 
7387.8 
1246.9 
1033.3 
1058.5 
780.3 
974.8 
1350.0 
1117.3 
Crazy Climber 
10780.5 35829.4 62583.6 
21327.8 
12146.5 20516.5 42923.6 56937.0 83940.2 
Demon Attack 
152.1 
1971.0 
208.1 
711.8 
817.6 
1113.4 
545.2 
3527.0 
13003.9 
Freeway 
0.0 
29.6 
20.3 
25.0 
26.7 
9.8 
24.4 
21.8 
21.8 
Frostbite 
65.2 
4334.7 
254.7 
231.6 
1181.3 
331.1 
1821.5 
255.0 
296.3 
Gopher 
257.6 
2412.5 
771.0 
778.0 
669.3 
636.3 
715.2 
1256.0 
3260.3 
Hero 
1027.0 
30826.4 
2656.6 
6458.8 
6279.3 
3736.3 
7019.2 
3095.0 
9315.9 
Jamesbond 
29.0 
302.8 
125.3 
112.3 
471.0 
236.0 
365.4 
87.5 
517.0 
Kangaroo 
52.0 
3035.0 
323.1 
605.4 
872.5 
940.6 
3276.4 
62.5 
724.1 
Krull 
1598.0 
2665.5 
4539.9 
3277.9 
4229.6 
4018.1 
3688.9 
4890.8 
5663.3 
Kung Fu Master 
258.5 
22736.3 17257.2 
5722.2 
14307.8 
9111.0 
13192.7 18813.0 30944.8 
Ms Pacman 
307.3 
6951.6 
1480.0 
941.9 
1465.5 
960.5 
1313.2 
1265.6 
1281.2 
Pong 
-20.7 
14.6 
12.8 
1.3 
-16.5 
-8.5 
-5.9 
-6.7 
20.1 
Private Eye 
24.9 
69571.3 
58.3 
100.0 
218.4 
-13.6 
124.0 
56.3 
96.7 
Qbert 
163.9 
13455.0 
1288.8 
509.3 
1042.4 
854.4 
669.1 
3952.0 
13781.9 
Road Runner 
11.5 
7845.0 
5640.6 
2696.7 
5661.0 
8895.1 
14220.5 
2500.0 
17751.3 
Seaquest 
68.4 
42054.7 
683.3 
286.9 
384.5 
301.2 
583.1 
208.0 
1100.2 
Up N Down 
533.4 
11693.2 
3350.3 
2847.6 
2955.2 
3180.8 
28138.5 
2896.9 
17264.2 </p>
<p>Normed Mean 
0.000 
1.000 
0.443 
0.264 
0.381 
0.357 
0.704 
0.562 
1.943 
Normed Median 
0.000 
1.000 
0.144 
0.204 
0.175 
0.268 
0.415 
0.227 
1.090 </p>
<p>Table 2 :
2Scores achieved by EfficientZero (mean &amp; standard deviation for 10 seeds) and some baselines on some low-dimensional environments on the DMControl 100k benchmark. EfficientZero achieves state-of-art performance and comparable results to the state-based SAC.Task 
CURL 
Dreamer 
MuZero 
SAC-AE Pixel SAC State SAC EfficientZero </p>
<p>Cartpole, Swingup 582± 146 
326±27 
218.5 ± 122 311±11 
419±40 
835±22 
813±19 
Reacher, Easy 
538± 233 
314±155 
493 ± 145 
274±14 
145±30 
746±25 
952±34 
Ball in cup, Catch 
769± 43 
246 ± 174 
542 ± 270 
391± 82 
312± 63 
746±91 
942±17 </p>
<p>Table 3 :
3Ablations of the self-supervised consistency, end-to-end value prefix and model-based off-policy correction. We remove one component at a time and evaluate the corresponding version on the 26 Atari games. Each component matters and the consistency one is the most significant. The detailed results are attached in App.A.2 .Game 
Full 
w.o. consistency w.o. value prefix w.o. off-policy correction </p>
<p>Normed Mean 
1.943 
0.881 
1.482 
1.475 
Normed Median 1.090 
0.340 
0.552 
0.836 </p>
<p>Table 4 :
4Ablations of the off-policy correction: L1 error of the target values versus the ground truth values. Take UpNDown as an example.States 
Current state Unrolled next 5 states (Avg.) All states (Avg.) </p>
<p>Value error without correction 
0.765 
0.636 
0.657 
Value error with correction 
0.533 
0.576 
0.569 </p>
<p>Table 5 :
5Ablations of the off-policy correction: Average L1 error of the values of the trajectories at distinct stages. Take UpNDown as an example.Stages of trajectories 
20k 
40k 
60k 
80k 
100k </p>
<p>Value error without correction 0.657 0.697 0.628 0.574 0.441 
Value error with correction 
0.569 0.552 0.537 0.488 0.397 </p>
<p>Table 6 :
6Hyper-parameters for EfficientZero on Atari gamesParameter 
Setting </p>
<p>Observation down-sampling 
96 × 96 
Frames stacked 
4 
Frames skip 
4 
Reward clipping 
True 
Terminal on loss of life 
True 
Max frames per episode 
108K 
Discount factor 
0.997 4 
Minibatch size 
256 
Optimizer 
SGD 
Optimizer: learning rate 
0.2 
Optimizer: momentum 
0.9 
Optimizer: weight decay (c) 
0.0001 
Learning rate schedule 
0.2 → 0.02 
Max gradient norm 
5 
Priority exponent (α) 
0.6 
Priority correction (β) 
0.4 → 1 
Training steps 
120K 
Evaluation episodes 
32 
Min replay size for sampling 
2000 
Self-play network updating inerval 
100 
Target network updating interval 
200 
Unroll steps (lunroll) 
5 
TD steps (k) 
5 
Policy loss coefficient (λ1) 
1 
Value loss coefficient (λ2) 
0.25 
Self-supervised consistency loss coefficient (λ3) 
2 
LSTM horizontal length (ζ) 
5 
Dirichlet noise ratio (ξ) 
0.3 
Number of simulations in MCTS (Nsim) 
50 
Reanalyzed policy ratio 
0.99 </p>
<p>Table 7 :
7Ablations of the self-supervised consistency, end-to-end value prefix and model-based off-policy correction on more Atari games. (Scores on the Atari 100k benchmark)Game 
Full 
w.o. consistency w.o. value prefix w.o. off-policy correction </p>
<p>Alien 
808.5 
961.3 
558 
619.4 
Amidar 
148.6 
32.2 
31.0 
256.3 
Assault 
1263.1 
572.9 
955.0 
1190.4 
Asterix 
25557.8 
2065.6 
7330.0 
13525.0 
Bank Heist 
351.0 
165.6 
273.0 
297.5 
BattleZone 
13871.2 
14063.0 
9900.0 
16125.0 
Boxing 
52.7 
6.1 
60.2 
30.5 
Breakout 
414.1 
237.4 
379.2 
400.3 
ChopperCommand 
1117.3 
1138.0 
1280 
1487.5 
Crazy Climber 
83940.2 
75550.0 
106090.0 
70681.0 
Demon Attack 
13003.9 
5973.8 
6818.5 
8640.6 
Freeway 
21.8 
21.8 
21.8 
21.8 
Frostbite 
296.3 
248.8 
235.2 
227.5 
Gopher 
3260.3 
1155 
2792.0 
2275.0 
Hero 
9315.9 
5824.4 
3167.5 
9053.0 
Jamesbond 
517.0 
154.7 
380.0 
356.3 
Kangaroo 
724.1 
375.0 
200.0 
687.5 
Krull 
5663.3 
4178.625 
4527.6 
3635.6 
Kung Fu Master 
30944.8 
19312.5 
25980.0 
25025.0 
Ms Pacman 
1281.2 
1090.0 
1475.0 
1297.2 
Pong 
20.1 
-1.5 
16.8 
19.5 
Private Eye 
96.7 
100.0 
100.0 
100.0 
Qbert 
13781.9 
5340.7 
6360.0 
13637.5 
Road Runner 
17751.3 
2700.0 
3010.0 
9856.0 
Seaquest 
1100.2 
460.0 
468.0 
843.8 
Up N Down 
17264.2 
3040.0 
7656.0 
4897.2 </p>
<p>Normed Mean 
1.943 
0.881 
1.482 
1.475 
Normed Median 
1.090 
0.340 
0.552 
0.836 </p>
<p>Table 8 :
8Ablations of the data augmentation technique in the consistency component. Results show 
that the data augmentation has limited improvement in EfficientZero and the self-supervised training 
loss is more significant. </p>
<p>Game 
Full 
w.o. consistency w.o. data augmentation </p>
<p>Asterix 
6218.8 
1350.0 
13884.0 
Breakout 
388.8 
12.0 
365.2 
Demon Attack 10536.6 
5973.8 
8730.0 
Gopher 
2828.8 
1155.0 
1823.75 
Pong 
19.8 
-8.5 
13.9 
Qbert 
15268.8 
2304.7 
14286.0 
Seaquest 
1321.0 
460.0 
1125.0 
Up N Down 
10238.1 
3040.0 
16380.0 </p>
<p>Table 9 :
9Ablations of the techniques (the MCTS root value and the dynamic horizon) in the off-policy correction component. The dynamic horizon seems more important than the MCTS root value when data is limited.Game 
Full 
w.o. off-policy correction w.o. dynamic horizon w.o. MCTS root value </p>
<p>Asterix 
6218.8 
2706.3 
3263.0 
6288.0 
Breakout 
388.8 
468.6 
427.0 
387.8 
Demon Attack 10536.6 
8640.6 
9211.1 
10063.0 
Gopher 
2828.8 
2275.0 
2459.2 
2651.0 
Pong 
19.8 
19.5 
19.2 
14.5 
Qbert 
15268.8 
3948.4 
7945 
14738.0 
Seaquest 
1321.0 
1248.0 
1292.0 
876.0 
Up N Down 
10238.1 
3240.0 
4772.0 
9925.6 </p>
<p>Table 10 :
10Scores reported for 3 random seeds for each of the above games, with the last two columns being the mean and standard deviation across the runs. Each run is evaluated with 32 different seeds.Game 
Seed 0 
Seed 1 
Seed 2 
Mean 
Std </p>
<p>Alien 
1093.1 
622.2 
710.3 
808.5 
204.4 
Amidar 
198.7 
116.4 
130.6 
148.6 
35.9 
Assault 
1436.3 
1150.8 
1202.2 
1263.1 
124.3 
Asterix 
18421.9 43220.2 15031.3 25557.8 12565.7 
Bank Heist 
362.6 
336.3 
354.0 
351.0 
10.9 
Battle Zone 
11812.5 13100.8 16700.4 13871.2 
2068.5 
Boxing 
45.9 
49.9 
62.4 
52.7 
7.0 
Breakout 
432.8 
418.7 
390.9 
414.1 
17.4 
ChopperCommand 
1190.9 
1360.9 
800.0 
1117.3 
234.8 
Crazy Climber 
98640.2 65520.4 87660.1 83940.2 13774.6 
Demon Attack 
11517.5 14323.3 13170.8 13003.9 
1151.5 
Freeway 
21.8 
21.8 
21.8 
21.8 
0.0 
Frostbite 
407.1 
225.5 
256.3 
296.3 
79.4 
Gopher 
3002.6 
2744.2 
4034.1 
3260.3 
557.2 
Hero 
12349.1 
8006.5 
7592.0 
9315.9 
2151.5 
Jamesbond 
530.7 
600.3 
420.1 
517.0 
74.2 
Kangaroo 
980.2 
460.7 
731.3 
724.1 
212.1 
Krull 
4839.5 
5548.5 
6602.0 
5663.3 
724.1 
Kung Fu Master 
28493.1 36840.7 27500.5 30944.8 
4188.7 
Ms Pacman 
1465.0 
1203.4 
1175.3 
1281.2 
130.4 
Pong 
20.6 
18.8 
21.0 
20.1 
1.0 
Private Eye 
100.0 
90.0 
100.0 
96.7 
4.7 
Qbert 
15458.1 14577.5 13310.0 14448.5 
881.7 
Road Runner 
17843.8 20140.0 15270.2 17751.3 
1989.2 
Seaquest 
1038.1 
1078.2 
1184.4 
1100.2 
61.7 
Up N Down 
22717.5 
8095.6 
20979.4 17264.2 
6521.9 </p>
<p>Acknowledgments and Disclosure of Funding
The expected-outcome model of two-player games. Bruce Abramson, Morgan KaufmannBruce Abramson. The expected-outcome model of two-player games. Morgan Kaufmann, 2014.</p>
<p>Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G Bellemare, Advances in Neural Information Processing Systems. Deep reinforcement learning at the edge of the statistical precipiceRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc G Belle- mare. Deep reinforcement learning at the edge of the statistical precipice. Advances in Neural Information Processing Systems, 2021.</p>
<p>Combating the compounding-error problem with a multi-step model. Kavosh Asadi, Dipendra Misra, Seungchan Kim, Michel L Littman, arXiv:1905.13320arXiv preprintKavosh Asadi, Dipendra Misra, Seungchan Kim, and Michel L Littman. Combating the compounding-error problem with a multi-step model. arXiv preprint arXiv:1905.13320, 2019.</p>
<p>Finite-time analysis of the multiarmed bandit problem. Peter Auer, Nicolo Cesa-Bianchi, Paul Fischer, Machine learning. 472Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235-256, 2002.</p>
<p>Neuronlike adaptive elements that can solve difficult learning control problems. G Andrew, Richard S Barto, Charles W Sutton, Anderson, IEEE transactions on systems, man, and cybernetics. 5Andrew G Barto, Richard S Sutton, and Charles W Anderson. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE transactions on systems, man, and cybernetics, (5):834-846, 1983.</p>
<p>The arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 47Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.</p>
<p>Dota 2 with large scale deep reinforcement learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, arXiv:1912.06680arXiv preprintChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław Dębiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.</p>
<p>A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, International conference on machine learning. PMLRTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 1597-1607. PMLR, 2020.</p>
<p>Exploring simple siamese representation learning. Xinlei Chen, Kaiming He, arXiv:2011.10566arXiv preprintXinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint arXiv:2011.10566, 2020.</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Kurtland Chua, Roberto Calandra, Rowan Mcallister, Sergey Levine, arXiv:1805.12114arXiv preprintKurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforce- ment learning in a handful of trials using probabilistic dynamics models. arXiv preprint arXiv:1805.12114, 2018.</p>
<p>Visualizing muzero models. Ken S Joery A De Vries, Voskuil, M Thomas, Aske Moerland, Plaat, arXiv:2102.12924arXiv preprintJoery A de Vries, Ken S Voskuil, Thomas M Moerland, and Aske Plaat. Visualizing muzero models. arXiv preprint arXiv:2102.12924, 2021.</p>
<p>Pilco: A model-based and data-efficient approach to policy search. Marc Deisenroth, Carl E Rasmussen, Proceedings of the 28th International Conference on machine learning (ICML-11). the 28th International Conference on machine learning (ICML-11)CiteseerMarc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pages 465-472. Citeseer, 2011.</p>
<p>Bootstrap your own latent: A new approach to self-supervised learning. Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, H Pierre, Elena Richemond, Carl Buchatskaya, Bernardo Doersch, Zhaohan Daniel Avila Pires, Mohammad Gheshlaghi Guo, Azar, arXiv:2006.07733arXiv preprintJean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, arXiv:1912.01603arXiv preprintDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.</p>
<p>Learning latent dynamics for planning from pixels. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, James Davidson, International Conference on Machine Learning. PMLRDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In International Conference on Machine Learning, pages 2555-2565. PMLR, 2019.</p>
<p>Mastering atari with discrete world models. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, Jimmy Ba, arXiv:2010.02193arXiv preprintDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. arXiv preprint arXiv:2010.02193, 2020.</p>
<p>Momentum contrast for unsupervised visual representation learning. Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, Ross Girshick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729-9738, 2020.</p>
<p>Rainbow: Combining improvements in deep reinforcement learning. Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, David Silver, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence32Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improve- ments in deep reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.</p>
<p>Learning and planning in complex action spaces. Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, David Silver, arXiv:2104.06303arXiv preprintThomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Mohammadamin Barekatain, Simon Schmitt, and David Silver. Learning and planning in complex action spaces. arXiv preprint arXiv:2104.06303, 2021.</p>
<p>Modelbased reinforcement learning for atari. Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, H Roy, Konrad Campbell, Dumitru Czechowski, Chelsea Erhan, Piotr Finn, Sergey Kozakowski, Levine, arXiv:1903.00374arXiv preprintLukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model- based reinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.</p>
<p>Do recent advancements in model-based deep reinforcement learning really improve data efficiency?. Kacper Kielak, arXiv:2003.10181arXiv preprintKacper Kielak. Do recent advancements in model-based deep reinforcement learning really improve data efficiency? arXiv preprint arXiv:2003.10181, 2020.</p>
<p>Bandit based monte-carlo planning. Levente Kocsis, Csaba Szepesvári, European conference on machine learning. SpringerLevente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European conference on machine learning, pages 282-293. Springer, 2006.</p>
<p>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. Ilya Kostrikov, Denis Yarats, Rob Fergus, arXiv:2004.13649arXiv preprintIlya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020.</p>
<p>Pytorch implementation of muzero. Anurag Koul, Anurag Koul. Pytorch implementation of muzero. https://github.com/koulanurag/ muzero-pytorch, 2019.</p>
<p>Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, arXiv:2004.14990Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. arXiv preprintMichael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas. Reinforcement learning with augmented data. arXiv preprint arXiv:2004.14990, 2020.</p>
<p>P Timothy, Jonathan J Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. arXiv preprintTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015.</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, G Marc, Alex Bellemare, Martin Graves, Andreas K Riedmiller, Georg Fidjeland, Ostrovski, nature. 5187540Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529-533, 2015.</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, International conference on machine learning. PMLRVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforce- ment learning. In International conference on machine learning, pages 1928-1937. PMLR, 2016.</p>
<p>A distributed framework for emerging {AI} applications. Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, 13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18). Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I Jordan, et al. Ray: A distributed framework for emerging {AI} applications. In 13th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 18), pages 561-577, 2018.</p>
<p>Multi-armed bandits with episode context. Christopher D Rosin, Annals of Mathematics and Artificial Intelligence. 613Christopher D Rosin. Multi-armed bandits with episode context. Annals of Mathematics and Artificial Intelligence, 61(3):203-230, 2011.</p>
<p>. Tom Schaul, John Quan, Ioannis Antonoglou, David Silver, arXiv:1511.05952Prioritized experience replay. arXiv preprintTom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015.</p>
<p>Mastering atari, go, chess and shogi by planning with a learned model. Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Nature. 5887839Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Si- mon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et al. Mastering atari, go, chess and shogi by planning with a learned model. Nature, 588(7839):604-609, 2020.</p>
<p>Online and offline reinforcement learning by planning with a learned model. Julian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, David Silver, arXiv:2104.06294arXiv preprintJulian Schrittwieser, Thomas Hubert, Amol Mandhane, Mohammadamin Barekatain, Ioannis Antonoglou, and David Silver. Online and offline reinforcement learning by planning with a learned model. arXiv preprint arXiv:2104.06294, 2021.</p>
<p>Trust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, International conference on machine learning. PMLRJohn Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, pages 1889-1897. PMLR, 2015.</p>
<p>Highdimensional continuous control using generalized advantage estimation. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel, arXiv:1506.02438arXiv preprintJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High- dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Data-efficient reinforcement learning with self-predictive representations. Max Schwarzer, Ankesh Anand, Rishab Goel, Devon Hjelm, Aaron Courville, Philip Bachman, Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip Bachman. Data-efficient reinforcement learning with self-predictive representations.</p>
<p>Mastering the game of go with deep neural networks and tree search. David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den, Julian Driessche, Ioannis Schrittwieser, Veda Antonoglou, Marc Panneershelvam, Lanctot, nature. 5297587David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driess- che, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mas- tering the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.</p>
<p>Mastering the game of go without human knowledge. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, nature. 5507676David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354-359, 2017.</p>
<p>Curl: Contrastive unsupervised representations for reinforcement learning. Aravind Srinivas, Michael Laskin, Pieter Abbeel, arXiv:2004.04136arXiv preprintAravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised represen- tations for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.</p>
<p>. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego De Las, David Casas, Budden, arXiv:1801.00690Abbas Abdolmaleki. Andrew LefrancqarXiv preprintJosh Merel. et al. Deepmind control suiteYuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.</p>
<p>Elf opengo: An analysis and open reimplementation of alphazero. Yuandong Tian, Jerry Ma, Qucheng Gong, Shubho Sengupta, Zhuoyuan Chen, James Pinkerton, Larry Zitnick, International Conference on Machine Learning. PMLRYuandong Tian, Jerry Ma, Qucheng Gong, Shubho Sengupta, Zhuoyuan Chen, James Pinkerton, and Larry Zitnick. Elf opengo: An analysis and open reimplementation of alphazero. In International Conference on Machine Learning, pages 6244-6253. PMLR, 2019.</p>
<p>Plannable approximations to mdp homomorphisms: Equivariance under actions. Elise Van Der Pol, Thomas Kipf, A Frans, Max Oliehoek, Welling, arXiv:2002.11963arXiv preprintElise van der Pol, Thomas Kipf, Frans A Oliehoek, and Max Welling. Plannable approximations to mdp homomorphisms: Equivariance under actions. arXiv preprint arXiv:2002.11963, 2020.</p>
<p>Deep reinforcement learning with double q-learning. Arthur Hado Van Hasselt, David Guez, Silver, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence30Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.</p>
<p>When to use parametric models in reinforcement learning?. Matteo Hado Van Hasselt, John Hessel, Aslanides, arXiv:1906.05243arXiv preprintHado van Hasselt, Matteo Hessel, and John Aslanides. When to use parametric models in reinforcement learning? arXiv preprint arXiv:1906.05243, 2019.</p>
<p>Dueling network architectures for deep reinforcement learning. Ziyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, Nando Freitas, International conference on machine learning. PMLRZiyu Wang, Tom Schaul, Matteo Hessel, Hado Hasselt, Marc Lanctot, and Nando Freitas. Dueling network architectures for deep reinforcement learning. In International conference on machine learning, pages 1995-2003. PMLR, 2016.</p>
<p>Learning from delayed rewards. Christopher John Cornish Hellaby Watkins, Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.</p>
<p>Muzero general: Open reimplementation of muzero. Aurèle Hainaut Werner Duvaud, Aurèle Hainaut Werner Duvaud. Muzero general: Open reimplementation of muzero. https: //github.com/werner-duvaud/muzero-general, 2019.</p>
<p>Improving sample efficiency in model-free reinforcement learning from images. Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, Rob Fergus, arXiv:1910.01741arXiv preprintDenis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving sample efficiency in model-free reinforcement learning from images. arXiv preprint arXiv:1910.01741, 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
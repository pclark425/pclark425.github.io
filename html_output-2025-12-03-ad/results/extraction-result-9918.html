<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9918 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9918</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9918</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-d6077b167684567cfc35017709872b298a96a92d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d6077b167684567cfc35017709872b298a96a92d" target="_blank">UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work presents UMBRELA (a recursive acronym that stands for UMbrela is the Bing RELevance Assessor), an open-source toolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o model and adds more nuance to the original paper.</p>
                <p><strong>Paper Abstract:</strong> Copious amounts of relevance judgments are necessary for the effective training and accurate evaluation of retrieval systems. Conventionally, these judgments are made by human assessors, rendering this process expensive and laborious. A recent study by Thomas et al. from Microsoft Bing suggested that large language models (LLMs) can accurately perform the relevance assessment task and provide human-quality judgments, but unfortunately their study did not yield any reusable software artifacts. Our work presents UMBRELA (a recursive acronym that stands for UMbrela is the Bing RELevance Assessor), an open-source toolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o model and adds more nuance to the original paper. Across Deep Learning Tracks from TREC 2019 to 2023, we find that LLM-derived relevance judgments correlate highly with rankings generated by effective multi-stage retrieval systems. Our toolkit is designed to be easily extensible and can be integrated into existing multi-stage retrieval and evaluation pipelines, offering researchers a valuable resource for studying retrieval evaluation methodologies. UMBRELA will be used in the TREC 2024 RAG Track to aid in relevance assessments, and we envision our toolkit becoming a foundation for further innovation in the field. UMBRELA is available at https://github.com/castorini/umbrela.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9918.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9918.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UMBRELA (LLM-as-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UMBRELA: LLM-based Bing RELevance Assessor (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source toolkit that uses an LLM (GPT-4o) with DNA zero-shot prompting to produce passage-level relevance judgments for TREC Deep Learning tracks and compares those judgments to human (NIST) qrels to evaluate differences in judgement quality and system-ranking agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Passage relevance assessment / information retrieval evaluation (TREC Deep Learning Tracks 2019–2023)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>OpenAI GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Zero-shot DNA (Descriptive, Narrative, Aspects) prompt per Figure 1: given query+passage, LLM computes intent, measures match (M) and trustworthiness (T), then outputs an integer 0–3 in format '##final score: X'; parameters: temperature=0, top_p=1, frequency/presence penalties set per paper.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>NIST human assessors (TREC Deep Learning Tracks) provided gold qrels using a 4-point scale (0: irrelevant, 1: related, 2: highly relevant, 3: perfectly relevant); judgments include near-duplicate propagation handled by NIST (paper removes propagated duplicates for experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Cohen's κ (four-scale): DL2019=0.3613, DL2020=0.3506, DL2021=0.3730, DL2022*=0.3362, DL2023*=0.3081; Cohen's κ (binary: {0,1} vs {2,3}): DL2019=0.4989, DL2020=0.4496, DL2021=0.4917, DL2022*=0.4217, DL2023*=0.4176. System-ranking correlations (nDCG@10): Kendall τ: DL2019=0.8926, DL2020=0.9435, DL2021=0.9343, DL2022*=0.8728, DL2023*=0.9107; Spearman ρ: DL2019=0.9736, DL2020=0.9923, DL2021=0.9915, DL2022*=0.9729, DL2023*=0.9857. Confusion-matrix per-label accuracy (approx.): non-relevant ≈75%, related ≈50%, highly relevant ≈30%, perfectly relevant ≈45%. (All values from Table 2 and Figure 2 / Section 4.)</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using an LLM judge degrades fine-grained agreement with human labels (only 'fair' Cohen's κ ~0.31–0.37 on four-scale labels) and reduces per-label accuracy for mid/high relevance categories (notably 'highly relevant' and 'perfectly relevant'), leading to misclassification and loss of granularity in label assignment compared to human assessors.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Case 1 (TREC DL 2019): Query 'what is the daily life of thai people' — human qrel labeled a passage about the Thai flag as 3 (perfectly relevant) while the LLM labeled it 0; confusion indicates LLM/human disagreement where LLM often under-predicts high relevance. Case 2: Query 'medicare's definition of mechanical ventilation' — passage about CPAP was judged by humans as relevant but LLM marked it non-relevant; paper's spot-checks show many such mismatches where LLM disagreed with human high-relevance labels. Also see per-label accuracies from Figure 2: related (~50%), highly relevant (~30%), perfectly relevant (~45%) accuracy vs. human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Despite label-level losses, LLM judgments lead to very high agreement in system-level evaluations: Spearman ρ up to ~0.99 and Kendall τ ~0.87–0.94 for nDCG@10, indicating retrieval system rankings are preserved. The authors also note cases where human labels appear ambiguous or unwarranted and the LLM judgment seems more accurate (authors' spot-checks). The paper further highlights that LLMs can 'fill holes' from incomplete human judgments and may outperform assessors with limited understanding in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>Abstract; Introduction; Section 3.2 (prompting); Section 4 (Comparing LLM-based assessments; Confusion matrices; Case study); Table 2; Figure 2; Figure 3; concluding remarks in Section 5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large Language Models Can Accurately Predict Searcher Preferences <em>(Rating: 2)</em></li>
                <li>Perspectives on Large Language Models for Relevance Judgment <em>(Rating: 2)</em></li>
                <li>Can Large Language Models be an Alternative to Human Evaluations? <em>(Rating: 2)</em></li>
                <li>LLMs Can Patch Up Missing Relevance Judgments in Evaluation <em>(Rating: 2)</em></li>
                <li>One-Shot Labeling for Automatic Relevance Estimation <em>(Rating: 1)</em></li>
                <li>G-EVAL: NLG Evaluation Using GPT-4 with Better Human Alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9918",
    "paper_id": "paper-d6077b167684567cfc35017709872b298a96a92d",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "UMBRELA (LLM-as-judge)",
            "name_full": "UMBRELA: LLM-based Bing RELevance Assessor (GPT-4o)",
            "brief_description": "An open-source toolkit that uses an LLM (GPT-4o) with DNA zero-shot prompting to produce passage-level relevance judgments for TREC Deep Learning tracks and compares those judgments to human (NIST) qrels to evaluate differences in judgement quality and system-ranking agreement.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Passage relevance assessment / information retrieval evaluation (TREC Deep Learning Tracks 2019–2023)",
            "llm_judge_model": "OpenAI GPT-4o",
            "llm_judge_setup": "Zero-shot DNA (Descriptive, Narrative, Aspects) prompt per Figure 1: given query+passage, LLM computes intent, measures match (M) and trustworthiness (T), then outputs an integer 0–3 in format '##final score: X'; parameters: temperature=0, top_p=1, frequency/presence penalties set per paper.",
            "human_evaluation_setup": "NIST human assessors (TREC Deep Learning Tracks) provided gold qrels using a 4-point scale (0: irrelevant, 1: related, 2: highly relevant, 3: perfectly relevant); judgments include near-duplicate propagation handled by NIST (paper removes propagated duplicates for experiments).",
            "agreement_metric": "Cohen's κ (four-scale): DL2019=0.3613, DL2020=0.3506, DL2021=0.3730, DL2022*=0.3362, DL2023*=0.3081; Cohen's κ (binary: {0,1} vs {2,3}): DL2019=0.4989, DL2020=0.4496, DL2021=0.4917, DL2022*=0.4217, DL2023*=0.4176. System-ranking correlations (nDCG@10): Kendall τ: DL2019=0.8926, DL2020=0.9435, DL2021=0.9343, DL2022*=0.8728, DL2023*=0.9107; Spearman ρ: DL2019=0.9736, DL2020=0.9923, DL2021=0.9915, DL2022*=0.9729, DL2023*=0.9857. Confusion-matrix per-label accuracy (approx.): non-relevant ≈75%, related ≈50%, highly relevant ≈30%, perfectly relevant ≈45%. (All values from Table 2 and Figure 2 / Section 4.)",
            "losses_identified": "Using an LLM judge degrades fine-grained agreement with human labels (only 'fair' Cohen's κ ~0.31–0.37 on four-scale labels) and reduces per-label accuracy for mid/high relevance categories (notably 'highly relevant' and 'perfectly relevant'), leading to misclassification and loss of granularity in label assignment compared to human assessors.",
            "examples_of_loss": "Case 1 (TREC DL 2019): Query 'what is the daily life of thai people' — human qrel labeled a passage about the Thai flag as 3 (perfectly relevant) while the LLM labeled it 0; confusion indicates LLM/human disagreement where LLM often under-predicts high relevance. Case 2: Query 'medicare's definition of mechanical ventilation' — passage about CPAP was judged by humans as relevant but LLM marked it non-relevant; paper's spot-checks show many such mismatches where LLM disagreed with human high-relevance labels. Also see per-label accuracies from Figure 2: related (~50%), highly relevant (~30%), perfectly relevant (~45%) accuracy vs. human labels.",
            "counterexamples_or_caveats": "Despite label-level losses, LLM judgments lead to very high agreement in system-level evaluations: Spearman ρ up to ~0.99 and Kendall τ ~0.87–0.94 for nDCG@10, indicating retrieval system rankings are preserved. The authors also note cases where human labels appear ambiguous or unwarranted and the LLM judgment seems more accurate (authors' spot-checks). The paper further highlights that LLMs can 'fill holes' from incomplete human judgments and may outperform assessors with limited understanding in some cases.",
            "paper_reference": "Abstract; Introduction; Section 3.2 (prompting); Section 4 (Comparing LLM-based assessments; Confusion matrices; Case study); Table 2; Figure 2; Figure 3; concluding remarks in Section 5.",
            "uuid": "e9918.0",
            "source_info": {
                "paper_title": "UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large Language Models Can Accurately Predict Searcher Preferences",
            "rating": 2
        },
        {
            "paper_title": "Perspectives on Large Language Models for Relevance Judgment",
            "rating": 2
        },
        {
            "paper_title": "Can Large Language Models be an Alternative to Human Evaluations?",
            "rating": 2
        },
        {
            "paper_title": "LLMs Can Patch Up Missing Relevance Judgments in Evaluation",
            "rating": 2
        },
        {
            "paper_title": "One-Shot Labeling for Automatic Relevance Estimation",
            "rating": 1
        },
        {
            "paper_title": "G-EVAL: NLG Evaluation Using GPT-4 with Better Human Alignment",
            "rating": 1
        }
    ],
    "cost": 0.007095249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>UMBRELA: UMbrela is the (Open-Source Reproduction of the) Bing RELevance Assessor</h1>
<p>Shivani Upadhyay<br>sjupadhyay@uwaterloo.ca University of Waterloo Waterloo, Canada</p>
<p>Ronak Pradeep<br>rpradeep@uwaterloo.ca<br>University of Waterloo<br>Waterloo, Canada</p>
<p>Nandan Thakur<br>nandan.thakur@uwaterloo.ca<br>University of Waterloo<br>Waterloo, Canada</p>
<p>Nick Craswell<br>nickcr@microsoft.com<br>Microsoft<br>Seattle, USA</p>
<p>Jimmy Lin<br>jimmylin@uwaterloo.ca<br>University of Waterloo<br>Waterloo, Canada</p>
<h4>Abstract</h4>
<p>Copious amounts of relevance judgments are necessary for the effective training and accurate evaluation of retrieval systems. Conventionally, these judgments are made by human assessors, rendering this process expensive and laborious. A recent study by Thomas et al. from Microsoft Bing suggested that large language models (LLMs) can accurately perform the relevance assessment task and provide human-quality judgments, but unfortunately their study did not yield any reusable software artifacts. Our work presents UMBRELA (a recursive acronym that stands for UMbrela is the Bing RELevance Assessor), an open-source toolkit that reproduces the results of Thomas et al. using OpenAI's GPT-4o model and adds more nuance to the original paper. Across Deep Learning Tracks from TREC 2019 to 2023, we find that LLMderived relevance judgments correlate highly with rankings generated by effective multi-stage retrieval systems. Our toolkit is designed to be easily extensible and can be integrated into existing multi-stage retrieval and evaluation pipelines, offering researchers a valuable resource for studying retrieval evaluation methodologies. UMBRELA will be used in the TREC 2024 RAG Track to aid in relevance assessments, and we envision our toolkit becoming a foundation for further innovation in the field. UMBRELA is available at https://github.com/castorini/umbrela.</p>
<h2>1 INTRODUCTION</h2>
<p>Accurate relevance labels are crucial for training and evaluating retrieval systems. Typically, these labels are assigned by trained human assessors, for example, retired intelligence analysts in the case of TREC or editors hired by a web search engine company. Given a search query, a set of results, and a description of the information need, these humans assessors determine the relevance of those results. However, human assessments can be time-consuming, laborintensive, and costly. Even after allocating these resources, assessments often remain inaccurate and misaligned with the searcher's intentions, particularly in the case where the information need did not arise from the assessor. They may fail to grasp the intent behind a search, resulting in flawed relevance judgments.</p>
<p>The emergence of large language models (LLMs) such as GPT4 [17] and Gemini [20] has presented a distinctive opportunity to automate the intricate task of data annotation [1, 3, 12, 13, 15, 19, 22, 23]. Thomas et al. [21] showcased capabilities of LLMs to understand searcher intent and to assign relevant labels as "accurately"</p>
<h2>as human assessors. These models have the potential to serve as an efficient and cost-effective alternative for manual relevance assessment. Moreover, LLMs may actually "outperform" assessors with limited knowledge, who might not fully grasp the search intent, thereby leading to imprecise relevance judgments. In addition, relevance judgments facilitated by LLMs can be employed to "fill holes" left by incomplete judgments, thereby contributing to a more thorough and accurate evaluation of retrieval systems [16, 24].</h2>
<p>In this paper, we reproduced the work of Thomas et al. [21] and repackage our efforts into UMBRELA, an open-source toolkit for using LLMs as relevance assessors. UMbrela is a recursive acronym that stands for UMbrela is the Bing RELevance Assessor. In our study, we carefully follow the zero-shot DNA (Descriptive, Narrative, and Aspects) prompting technique described by Kojima et al. [14] to reproduce the original setup as faithfully as possible.</p>
<p>Our experiments with the TREC Deep Learning (DL) Tracks from 2019-2023 [4-8] verified the claims by Thomas et al. [21] about LLM assessment being an effective alternative to humanbased assessment, using OpenAI's latest model GPT-4o. We have expanded our validation study to include correlation statistics that compare the effectiveness of retrieval systems based on human and LLM judgments. High correlations affirm the effectiveness of LLMs as relevance assessors. As highlighted by Thomas et al. [21], LLMs possess the ability to understand user needs and assign relevance labels with veracity comparable to human assessors.</p>
<p>At a high level, UMBRELA takes a query and a set of passages as input, and based on the LLM configuration, attempts to "understand" the search intent and labels the passages with different relevance grades. Our main contributions are as follows:</p>
<ul>
<li>We reproduced results from Thomas et al. [21], which lacked reusable software components, and created UMBRELA, an opensource toolkit that we share with the broader community.</li>
<li>Our tool will be used in the TREC 2024 RAG Track ${ }^{1}$ to showcase its practical utility for automatic relevance assessment. Through this deployment, we hope to contribute to advances in retrieval evaluation methodologies.</li>
</ul>
<h2>2 RELATED WORK</h2>
<p>The substantial effort required for manual test data preparation in large-scale testing of retrieval systems has consistently encouraged</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">qrels</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Total counts for</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">track</td>
<td style="text-align: center;">runs</td>
<td style="text-align: center;">topics</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">label</td>
<td style="text-align: center;">label</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\mathbf{0}$</td>
<td style="text-align: center;">$\mathbf{1}$</td>
<td style="text-align: center;">$\mathbf{2}$</td>
<td style="text-align: center;">$\mathbf{3}$</td>
</tr>
<tr>
<td style="text-align: left;">DL 2019</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">5158</td>
<td style="text-align: center;">1601</td>
<td style="text-align: center;">1804</td>
<td style="text-align: center;">697</td>
</tr>
<tr>
<td style="text-align: left;">DL 2020</td>
<td style="text-align: center;">59</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">7780</td>
<td style="text-align: center;">1940</td>
<td style="text-align: center;">1020</td>
<td style="text-align: center;">646</td>
</tr>
<tr>
<td style="text-align: left;">DL 2021</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">53</td>
<td style="text-align: center;">4338</td>
<td style="text-align: center;">3063</td>
<td style="text-align: center;">2341</td>
<td style="text-align: center;">1086</td>
</tr>
<tr>
<td style="text-align: left;">DL 2022</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">286459</td>
<td style="text-align: center;">52218</td>
<td style="text-align: center;">46080</td>
<td style="text-align: center;">1659</td>
</tr>
<tr>
<td style="text-align: left;">DL 2022*</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">12892</td>
<td style="text-align: center;">6192</td>
<td style="text-align: center;">3053</td>
<td style="text-align: center;">1385</td>
</tr>
<tr>
<td style="text-align: left;">DL 2023</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">13866</td>
<td style="text-align: center;">4372</td>
<td style="text-align: center;">2259</td>
<td style="text-align: center;">1830</td>
</tr>
<tr>
<td style="text-align: left;">DL 2023*</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">11618</td>
<td style="text-align: center;">3774</td>
<td style="text-align: center;">1942</td>
<td style="text-align: center;">1544</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary statistics for the Deep Learning Tracks from TREC 2019-2023. The total count of topics represents only those that have qrels; * represents counts after applying the pre-processing step to remove near-duplicate passages, as explained in Section 3.1.
researchers to develop automated techniques [9, 10]. In previous work, automated IR evaluations using LLMs have combined various prompting techniques such as zero-shot, one-shot, or fewshot learning. By providing detailed instruction, defining roles and multiple judges, researchers have showcased a continuous increase in the alignment between human and the predicted assessments [11, 16, 21, 24]. LLMs, when provided with proper guidance, can effectively "understand" the relation between a given query and passage and accurately determine its relevance.</p>
<p>In the domain of text annotations in general, both proprietary and open-source LLMs have shown remarkable effectiveness [1, 2, $13,18,25]$. The enhanced capabilities of LLMs and various prompting techniques have presented a strong substitute for manual text annotations.</p>
<h2>3 METHODOLOGY</h2>
<p>The main idea behind UMBRELA is to leverage LLMs for relevance assessment, reproducing the work of Thomas et al. [21].</p>
<h3>3.1 DL TREC Judgments</h3>
<p>To demonstrate LLM capabilities for generating relevance assessment, we took human relevance judgments (also known as qrels) from the Deep Learning Tracks in TREC 2019-2023 [4-8] and reassessed them with UMBRELA. These judgments were originally provided by human NIST assessors who were given the query and the following relevance scale [7]:</p>
<p>0 Irrelevant: The passage has nothing to do with the query
1 Related: The passage seems related to the query but does not answer it
2 Highly relevant: The passage has some answer for the query, but the answer may be a bit unclear, or hidden amongst extraneous information
3 Perfectly relevant: The passage is dedicated to the query and contains the exact answer
Based on their understanding and general knowledge, the human assessors assigned relevance labels to passages. Thus, we consider these as "gold" labels. See Table 1 for summary statistics.</p>
<div class="codehilite"><pre><span></span><code><span class="nv">Given</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">query</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">passage</span>,<span class="w"> </span><span class="nv">you</span><span class="w"> </span><span class="nv">must</span><span class="w"> </span><span class="nv">provide</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">score</span><span class="w"> </span><span class="nv">on</span><span class="w"> </span><span class="nv">an</span>
<span class="nv">integer</span><span class="w"> </span><span class="nv">scale</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">following</span><span class="w"> </span><span class="nv">meanings</span>:
<span class="mi">0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">represent</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">passage</span><span class="w"> </span><span class="nv">has</span><span class="w"> </span><span class="nv">nothing</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="k">do</span><span class="w"> </span><span class="nv">with</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">query</span>,
<span class="mi">1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">represents</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">passage</span><span class="w"> </span><span class="nv">seems</span><span class="w"> </span><span class="nv">related</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">query</span><span class="w"> </span><span class="nv">but</span>
<span class="nv">does</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">it</span>.
<span class="mi">2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">represents</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">passage</span><span class="w"> </span><span class="nv">has</span><span class="w"> </span><span class="nv">some</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">query</span>,
<span class="nv">but</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">may</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">bit</span><span class="w"> </span><span class="nv">unclear</span>,<span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="nv">hidden</span><span class="w"> </span><span class="nv">amongst</span><span class="w"> </span><span class="nv">extraneous</span>
<span class="nv">information</span><span class="w"> </span><span class="nv">and</span>
<span class="mi">3</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">represents</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">passage</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">dedicated</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">query</span><span class="w"> </span><span class="nv">and</span>
<span class="nv">contains</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">exact</span><span class="w"> </span><span class="nv">answer</span>.
</code></pre></div>

<p>Important Instruction: Assign category 1 if the passage is somewhat related to the topic but not completely, category 2 if passage presents something very important related to the entire topic but also has some extra information and category 3 if the passage only and entirely refers to the topic. If none of the above satisfies give it category 0.</p>
<p>Query: {query}
Passage: {passage}
Split this problem into steps:
Consider the underlying intent of the search.
Measure how well the content matches a likely intent of the query (M).
Measure how trustworthy the passage is (T).
Consider the aspects above and the relative importance of each, and decide on a final score (0). Final score must be an integer value only.
Do not provide any code in result. Provide each score in the format of: ##final score: score without providing any reasoning.</p>
<p>Figure 1: Prompt used for relevance assessment.
Near-duplicate handling for DL 2022-23. The qrels for the TREC 2022 and 2023 Deep Learning Tracks include relevance label propagation for near-duplicate passages that exist within the corpus. Thus, the number of relevance judgments that were actually assigned by humans is much smaller than the statistics in Table 1 suggest (we provide figures both with and without de-duplication). For the purposes of this study, we have excluded these near-duplicate passages from both the judgments and the submission results prior to conducting our calculations. To accomplish this, we consulted the list of near-duplicate passages compiled by NIST, which provides the near-duplicates in the form of clusters and a "canonical" passage for each cluster. Specifically, we retained only the "canonical" passages; all others were eliminated both from the qrels and the retrieved runs.</p>
<h3>3.2 Prompting Details</h3>
<p>Following the prompting techniques described by Thomas et al. [21], we also utilize the DNA prompting technique. Figure 1 shows the exact prompt used in UMBRELA for relevance assessment.</p>
<p>The DNA prompt consists of three essential sections: descriptive, narrative, and aspects. The descriptive and narrative sections help the LLM understand the user query and the passage that it is supposed to label, whereas the aspects section details the step-by-step procedure for guiding the LLM's "thinking process". This procedure essentially breaks down the relevance labeling task into smaller ones and instructs the LLM to develop a better interpretation of it. The prompt additionally informs the LLM of the expected result structure to be followed.</p>
<p>Following the original paper, we utilize a zero-shot prompting technique for performing our assessments using GPT-40. Providing</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Confusion matrices comparing the original human labels with those generated by the LLM.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">qrels track</th>
<th style="text-align: center;">Cohen κ</th>
<th style="text-align: center;">Correlation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">four-scale</td>
<td style="text-align: center;">binary</td>
<td style="text-align: center;">Kendall <br> ( $\tau$ )</td>
<td style="text-align: center;">Spearman <br> ( $\rho$ )</td>
</tr>
<tr>
<td style="text-align: center;">DL 2019</td>
<td style="text-align: center;">0.3613</td>
<td style="text-align: center;">0.4989</td>
<td style="text-align: center;">0.8926</td>
<td style="text-align: center;">0.9736</td>
</tr>
<tr>
<td style="text-align: center;">DL 2020</td>
<td style="text-align: center;">0.3506</td>
<td style="text-align: center;">0.4496</td>
<td style="text-align: center;">0.9435</td>
<td style="text-align: center;">0.9923</td>
</tr>
<tr>
<td style="text-align: center;">DL 2021</td>
<td style="text-align: center;">0.3730</td>
<td style="text-align: center;">0.4917</td>
<td style="text-align: center;">0.9343</td>
<td style="text-align: center;">0.9915</td>
</tr>
<tr>
<td style="text-align: center;">DL 2022*</td>
<td style="text-align: center;">0.3362</td>
<td style="text-align: center;">0.4217</td>
<td style="text-align: center;">0.8728</td>
<td style="text-align: center;">0.9729</td>
</tr>
<tr>
<td style="text-align: center;">DL 2023*</td>
<td style="text-align: center;">0.3081</td>
<td style="text-align: center;">0.4176</td>
<td style="text-align: center;">0.9107</td>
<td style="text-align: center;">0.9857</td>
</tr>
</tbody>
</table>
<p>Table 2: Cohen $\kappa$ scores (left) and Kendall and Spearman's correlations between TREC submissions evaluated with groundtruth judgments and UMBRELA (right). We use nDCG@10 as the evaluation metric. Here, * represents the use of a pre-processing step to remove near-duplicate passages as explained in Section 3.1.
precise instructions assists the LLM in understanding the semantic relation between the query and the passage and thus assigning accurate labels.</p>
<h2>4 RESULTS</h2>
<p>We performed experiments with OpenAI's latest GPT-4o model (via Microsoft Azure) following the parameter configurations provide in Thomas et al. [21]. Temperature is set to 0 , top $p=1$, without using any stopwords, frequency and presence penalty set as 0.5 and 0 , respectively.</p>
<p>Comparing LLM-based assessments. Table 2 (left) presents Cohen $\kappa$ scores to capture agreement between human and LLM assessments. These agreement scores are demonstrated in two variants, the four-scale form which considers all four relevance labels and the binarized form following Faggioli et al. [11], where non-relevant
(0) and related (1) are merged under "non-relevant" and highly relevant (2) and perfectly relevant (3) are merged under the "relevant" label. These correlation values demonstrate fair agreement between human assessors and LLMs judgments.</p>
<p>Figure 2 further compares human-provided ground-truth labels with LLM-based labels using label-wise confusion matrices for all five tracks. As expected, LLMs are able to assign appropriate labels to the majority of the judgments across all tracks. Across these datasets, the LLM was able to predict non-relevant labels with roughly $75 \%$ accuracy, whereas for labels "related", "highly relevant", and "perfectly relevant", the accuracy drops to roughly $50 \%, 30 \%$, and $45 \%$, respectively (with respect to the human labels).</p>
<p>System evaluation using LLM-based assessments. Table 2 (right) shows the results of Kendall $\tau$ and Spearman $\rho$ correlations between evaluations performed with human ground-truth judgments and judgments provided by the LLM. In all cases, we compared nDCG@10. The high correlations among these judgment evaluations show alignment in the retrieval systems' rankings. Following Faggioli et al. [11], Figure 3 demonstrates the effectiveness of TREC DL runs with scatter plots comparing evaluations made using human and LLM judgments.</p>
<p>Case Study: TREC DL 2019. We further analyzed the discrepancies between LLM judgments and human judgments for TREC DL 2019. As shown in Figure 2(a), we observe a high number of misclassifications in the judgments, predicted as label 0 but where their original (human) label is 3. Based on manual spot-checking, we encountered the following interesting case:</p>
<p>Query: what is the daily life of thai people
Passage: Thai Flag Meaning: The red stripes represent the blood spilt to maintain Thailand's independence. The white stands for purity and is the color of Buddhism which is the country's main</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Scatter plots for comparing evaluations performed using original human assessments and LLM assessments.
religion.roportions: 2:3. Thai Flag Description: The flag of Thailand consists of five horizontal stripes. The top and bottom are equal-sized red stripes, the middle stripe is blue which is lined above and below by equal-sized white stripes. The blue stripe is double the size of the other four.</p>
<p>The passage mainly talks about the Thai flag but the query asks about the "regular life of Thai people". Here the topic seems to be somewhat vague and the pair fails to justify (based on our opinion) the highly relevant label assigned by the human assessors. A few other judgments for this query exhibit similar ambiguity and thus we feel the label 3 to be unwarranted. In this case, the LLM judgment appears to be more accurate.</p>
<p>Consider another case:
Query: medicare's definition of mechanical ventilation
Passage: Continuous Positive Airway Pressure (CPAP) Continuous positive airway pressure - also called CPAP - is a treatment in which a mask is worn over the nose and/or mouth while you sleep. The mask is hooked up to a machine that delivers a continuous flow of air into the nose. This air flow helps keep the airways open so that breathing is regular.</p>
<p>The query requests a definition for "mechanical ventilation", but the passage provides details regarding "Continuous Positive Airway Pressure". In our opinion, the passage fails to define "mechanical ventilation" and thus does not justify the human-assigned label. We notice many judgments similarly failing the classification for this query.</p>
<p>Both the above-mentioned cases demonstrate ambiguity that could have been the result of either inaccurate assessments or incomplete information regarding the user intent.</p>
<h2>5 CONCLUSION</h2>
<p>Continuous efforts are underway to automate the generation of relevance assessments, intending to reduce the cost of manual assessments. In our research, we have successfully replicated the results from Thomas et al. [21], thereby validating the capabilities of OpenAI's latest model, GPT-4o, in performing relevance assessments automatically. Through our experiments with the Deep Learning Tracks from TREC 2019-2023, we have demonstrated the prediction quality of an LLM. Moreover, we have demonstrated significant correlations between human and LLM-assessed judgments by evaluating submitted runs to the TREC evaluations. Finally, we have repackaged this methodology in an open-source tool, referred to as UMBRELA, that we share with the community.</p>
<h2>ACKNOWLEDGMENTS</h2>
<p>This research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada. We'd also like to thank Microsoft for providing access to OpenAI LLMs on Azure via the Accelerating Foundation Models Research program.</p>
<h2>REFERENCES</h2>
<p>[1] Meysam Alizadeh, Maël Kubli, Zeynab Samei, Shirin Dehghani, Juan Diego Bermeo, Maria Korobeynikovo, and Fabrizio Gilardi. 2023. Open-source Large Language Models Outperform Crowd Workers and Approach ChatGPT in TextAnnotation Tasks. arXiv:2307.02179 [cs.CL]
[2] Meysam Alizadeh, Maël Kubli, Zeynab Samei, Shirin Dehghani, Mohammadmasiha Zahedivafa, Juan Diego Bermeo, Maria Korobeynikova, and Fabrizio Gilardi. 2024. Open-Source LLMs for Text Annotation: A Practical Guide for Model Setting and Fine-Tuning. arXiv:2307.02179 [cs.CL]
[3] Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models be an Alternative to Human Evaluations? 15607-15631.</p>
<p>[4] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 Deep Learning Track. arXiv:2102.07662 [cs.IR]
[5] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Jimmy Lin. 2022. Overview of the TREC 2021 Deep Learning Track. In Text REtrieval Conference (TREC). NIST, TREC.
[6] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff. 2023. Overview of the TREC 2022 Deep Learning Track. In Text REtrieval Conference (TREC). NIST, TREC.
[7] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2020. Overview of the TREC 2019 Deep Learning Track. arXiv:2003.07820 [cs.IR]
[8] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Hossein A. Rahmani, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff. 2024. Overview of the TREC 2023 Deep Learning Track. In Text REtrieval Conference (TREC). NIST, TREC.
[9] Laura Dietz, Shubham Chatterjee, Connor Lennox, Sumanta Kashyapi, Pooja Oza, and Ben Gamari. 2022. Wikimarks: Harvesting Relevance Benchmarks from Wikipedia. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain). Association for Computing Machinery, 3005-3012.
[10] Laura Dietz and Jeff Dalton. 2020. Humans Optional? Automatic Large-Scale Test Collections for Entity, Passage, and Entity-Passage Retrieval. DatenbankSpektrum 20, 1 (2020), 17-28.
[11] Guglielmo Faggioli, Laura Dietz, Charles Clarke, Gianluca Demartini, Matthias Hagen, Claudia Hauff, Noriko Kando, Evangelos Kanoulas, Martin Potthast, Benno Stein, and Henning Wachsmuth. 2023. Perspectives on Large Language Models for Relevance Judgment. arXiv:2304.09161 [cs.IR]
[12] Fabrizio Gilardi, Meysam Alizadeh, and Maél Kubli. 2023. ChatGPT Outperforms Crowd Workers for Text-Annotation Tasks. arXiv:2303.15056 [cs.CL]
[13] Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, and Weizhu Chen. 2024. AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators. arXiv:2303.16854 [cs.CL]
[14] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large Language Models are Zero-Shot Reasoners. arXiv:2205.11916 [cs.CL]
[15] Yang Liu, Dan Iter, Yichong xu amd Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-EVAL: NLG Evaluation Using GPT-4 with Better Human Alignment. arXiv:2303.16634v3 [cs.CL]
[16] Sean MacAvaney and Luca Soldaini. 2023. One-Shot Labeling for Automatic Relevance Estimation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '23).
[17] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
[18] Jaromir Savelka, Kevin D Ashley, Morgan A Gray, Hannes Westermann, and Huihui Xu. 2023. Can GPT-4 Support Analysis of Textual Data in Tasks Requiring Highly Specialized Domain Expertise? (2023). arXiv:2306.13906 [cs.CL]
[19] Zhen Tan, Alimohammad Beigi, Song Wang, Ruocheng Guo, Amrita Bhattacharjee, Bohan Jiang, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. 2024. Large Language Models for Data Annotation: A Survey. arXiv:2402.13446 [cs.CL]
[20] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hasth, Katie Millican, David Silver, et al. 2024. Gemini: A Family of Highly Capable Multimodal Models. arXiv:2312.11805 [cs.CL]
[21] Paul Thomas, Seth Spidman, Nick Craswell, and Bhaskar Mitra. 2024. Large Language Models Can Accurately Predict Searcher Preferences. In 2024 International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM.
[22] Petter Törnberg. 2023. ChatGPT-4 Outperforms Experts and Crowd Workers in Annotating Political Twitter Messages with Zero-Shot Learning. arXiv:2304.06588 [cs.CL]
[23] Petter Törnberg. 2024. Best Practices for Text Annotation with Large Language Models. arXiv:2402.05129 [cs.CL]
[24] Shivani Upadhyay, Ehsan Kamalloo, and Jimmy Lin. 2024. LLMs Can Patch Up Missing Relevance Judgments in Evaluation. arXiv:2405.04727 [cs.IR]
[25] Yiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui, and Gareth Tyson. 2023. Can ChatGPT Reproduce Human-Generated Labels? a Study of Social Computing Tasks. (2023). arXiv:2304.10145 [cs.AI]</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://cs.uwaterloo.ca/ jimmylin/publications/Lin_etal_TREC2023-planning.pdf&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
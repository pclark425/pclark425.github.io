<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7567 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7567</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7567</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-4f85cf06753035f58248384d3244bc2a89b32220</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4f85cf06753035f58248384d3244bc2a89b32220" target="_blank">CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain</a></p>
                <p><strong>Paper Venue:</strong> ACM Transactions on Privacy and Security</p>
                <p><strong>Paper TL;DR:</strong> A language model specifically tailored to the cysec domain that can serve as a basic building block for cybersecurity systems and is compared on 15 tasks, showing that the model performs best in cybersecurity scenarios.</p>
                <p><strong>Paper Abstract:</strong> The field of cysec is evolving fast. Security professionals are in need of intelligence on past, current and —ideally — upcoming threats, because attacks are becoming more advanced and are increasingly targeting larger and more complex systems. Since the processing and analysis of such large amounts of information cannot be addressed manually, cysec experts rely on machine learning techniques. In the textual domain, pre-trained language models such as Bidirectional Encoder Representations from Transformers (BERT) have proven to be helpful as they provide a good baseline for further fine-tuning. However, due to the domain-knowledge and the many technical terms in cysec, general language models might miss the gist of textual information. For this reason, we create a high-quality dataset1 and present a language model2 specifically tailored to the cysec domain that can serve as a basic building block for cybersecurity systems. The model is compared on 15 tasks: Domain-dependent extrinsic tasks for measuring the performance on specific problems, intrinsic tasks for measuring the performance of the internal representations of the model, as well as general tasks from the SuperGLUE benchmark. The results of the intrinsic tasks show that our model improves the internal representation space of domain words compared with the other models. The extrinsic, domain-dependent tasks, consisting of sequence tagging and classification, show that the model performs best in cybersecurity scenarios. In addition, we pay special attention to the choice of hyperparameters against catastrophic forgetting, as pre-trained models tend to forget the original knowledge during further training.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Towards the detection of inconsistencies in public security vulnerability reports <em>(Rating: 2)</em></li>
                <li>OVANA: An Approach to Analyze and Improve the Information Quality of Vulnerability Databases <em>(Rating: 2)</em></li>
                <li>CyBERT: Contextualized Embeddings for the Cybersecurity Domain <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7567",
    "paper_id": "paper-4f85cf06753035f58248384d3244bc2a89b32220",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Towards the detection of inconsistencies in public security vulnerability reports",
            "rating": 2
        },
        {
            "paper_title": "OVANA: An Approach to Analyze and Improve the Information Quality of Vulnerability Databases",
            "rating": 2
        },
        {
            "paper_title": "CyBERT: Contextualized Embeddings for the Cybersecurity Domain",
            "rating": 1
        }
    ],
    "cost": 0.00551725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain</h1>
<p>MARKUS BAYER*, PEASEC, Technical University of Darmstadt, Germany<br>PHILIPP KUEHN, PEASEC, Technical University of Darmstadt, Germany<br>RAMIN SHANEHSAZ, PEASEC, Technical University of Darmstadt, Germany<br>CHRISTIAN REUTER, PEASEC, Technical University of Darmstadt, Germany</p>
<p>The field of cybersecurity is evolving fast. Experts need to be informed about past, current and - in the best case - upcoming threats, because attacks are becoming more advanced, targets bigger and systems more complex. As this cannot be addressed manually, cybersecurity experts need to rely on machine learning techniques. In the texutual domain, pre-trained language models like BERT have shown to be helpful, by providing a good baseline for further fine-tuning. However, due to the domain-knowledge and many technical terms in cybersecurity general language models might miss the gist of textual information, hence doing more harm than good. For this reason, we create a high-quality dataset and present a language model specifically tailored to the cybersecurity domain, which can serve as a basic building block for cybersecurity systems that deal with natural language. The model is compared with other models based on 15 different domain-dependent extrinsic and intrinsic tasks as well as general tasks from the SuperGLUE benchmark. On the one hand, the results of the intrinsic tasks show that our model improves the internal representation space of words compared to the other models. On the other hand, the extrinsic, domain-dependent tasks, consisting of sequence tagging and classification, show that the model is best in specific application scenarios, in contrast to the others. Furthermore, we show that our approach against catastrophic forgetting works, as the model is able to retrieve the previously trained domain-independent knowledge. The used dataset and trained model are made publicly available.</p>
<h2>ACM Reference Format:</h2>
<p>Markus Bayer, Philipp Kuehn, Ramin Shanehsaz, and Christian Reuter. 2022. CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain. In Proceedings of (Conference acronym 'XX). ACM, New York, NY, USA, 13 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn</p>
<h2>1 INTRODUCTION</h2>
<p>Cybersecurity evolved rapidely in recent years. One of the offspring technologies is cyber threat intelligence (CTI). Already with the first adoption in research and practise, which invited open discussions and collaboration on threat indicators, it became clear that the sheer volume of information could not be managed without automated support [12]. Meanwhile, the number of cyber attacks a day is steadily increasing [37], while the COVID-19 pandamic ${ }^{1}$ and the war in Ukraine [10] intensifies this development.</p>
<p>Analyzing attacks is mostly manual work which includes reverse engineering and forensics. The formats used to publish the resulting information differ as well. They could be published in structured form as indicator of compromise (IoC), or in natural language in blog posts and news articles. The latter is the gist of CTI [32, 35]. Generally, this is a manual, labour intensive task in which experts extract actually relevant, evidence-based knowledge [34]. This led to the idea of using natural language processing (NLP) to undertake this work and extract cyber threat information.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Recently, analyzing data by using neural network inspired language models has gained attention and has become an important part of modern NLP systems [6]. Particularly word embedding methods that use a sparse vector space to represent words are prominent instances [24]. In this context, models such as bidirectional encoder representations from transformers (BERT) [8] have become the standard basis models in all machine learning tasks that have natural language as input. These models are already generally pre-trained and can be adapted to the task at hand, which is called fine-tuning. Research has shown that the full potential of such models cannot be realized when applying them to domain-specific tasks [1, 5, 11, 18]. This is intuitive because these models try to cover as many domains as possible, and specific domain knowledge is lost due to capacity constraints, especially in normal-sized models, or because the knowledge is not even included in the training data. To gain domain-specific knowledge, these models can be further trained on domain-specific corpora to achieve even better results in this domain [18].</p>
<p>Models trained on general domain corpora like Wikipedia have inherent problems with domainspecific purposes [25]. They either have never seen domain-specific words, like new vulnerability names, or differentiate words with multiple meanings in multiple domains. An example is the word virus which might lead to a general model's understanding of a disease instead of some type of malware [25]. This is troublesome for automated CTI since it misses fundamental threats, when searching for cyber threats.</p>
<p>In this paper, we propose CySecBERT, a word embedding model based on BERT [8] for analyzing cybersecurity texts. Our aim is to enable state of the art NLP for the security domain and with that provide a model highly suitable for practical cybersecurity use cases and a solid base for further research in this field. By evaluating our resulting model on different tasks (i.e., intrinsic and extrinsic tasks) we ensure that it indeed enriches the cybersecurity domain as well as not forgetting too much of its preceding learnings. In this study, we will pre-train a model on a thoroughly chosen cybersecurity corpus consisting of different datasets, such as scientific papers, Twitter, webpages, and the national vulnerability database. A well-performing model for this use case may supersede a lot of manual work done by researchers. Although there are well performing models for various specific purposes in this domain [19, 26], the importance of a general cybersecurity model is undeniable. The following contributions are achieved in this paper:</p>
<ul>
<li>A pre-trained, general purpose cybersecurity language model based on BERT called CySecBERT (C1).</li>
<li>A sensibly chosen cybersecurity dataset containing all the data instances the model is trained on (C2).</li>
<li>An evaluation of CySecBERT based on several tasks tailored to the cybersecurity domain, including intrinsic and extrinsic tasks, and general benchmark, to measure if and the degree to which the model forgets past knowledge (C3).</li>
<li>A comparison of our model to a related cybersecurity model, as well as the original BERT model and a discussion about its shortcomings and improvements (C4).</li>
</ul>
<h1>2 RELATED WORK</h1>
<p>This subsection gives an overview of relevant work on the topic of BERT models. We outline models adapted to different domains that have emerged with the publication of BERT. We also summarise work that already proposes BERT-like language models for the cybersecurity domain. Finally, we indicate the research gap we are willing to fill.</p>
<h3>2.1 BERT Models in Different Domains</h3>
<p>In various publications, the researchers were able to show that it is possible to achieve good results on domain-specific text corpora with pre-trained models such as BERT. Of interest here is the method of domain-adaptive pre-training (DAPT) [11], which describes the process of training an already pre-trained language model on a domain-specific, domain-dependent dataset in the same way as the pre-training was done. This differs from classical fine-tuning in that the model is not specialised for just one task, but serves as a building block for many tasks in the field. It is done in several other domains since the introduction of BERT [5, 18, 27]. A prominent example is BioBERT, introduced by Lee et al. [18], where BERT was adapted to a biomedical corpus. It was initialized with weights from Devlin et al. [8]'s BERT model and then pre-trained once again, this time with a large biomedical dataset, where the dataset was more than five times larger than BERT's. Using a subsequent fine-tuning process on three different biomedical text mining tasks, which are named entity recognition (NER), Relation Extraction (RE), and Question Answering (QA), Lee et al. [18] were able to largely outperform BERT and previous state-of-the-art models on these aforementioned tasks. Similar approaches present models that address other domains. SciBERT [5] for example focuses on scientific publications whereas DA-RoBERTa, introduced by Krieger et al. [15] covers media bias. Gururangan et al. [11] underpin our method of additional pre-training on BERT by yielding good results applying this approach on RoBERTa [21], a variant of BERT using the same transformer-based architecture. In contrast to studies such as BioBERT by Lee et al. [18], in which only a single domain at a time is considered, Gururangan et al. [11] covered a wide range of variations for pre-trained models to domains and tasks within those domains.</p>
<p>Similarly, researchers have also explored BERT models for the cybersecurity domain. For example, Ranade et al. [27] propose a BERT model for this domain called CyBERT. Although, the paper states that fine-tuning on BERT took place, in fact, the process is a further pre-training of BERT for the cybersecurity domain. Fine-tuning is performed atop of this pre-trained cybersecurity model and is primarily used for application. In general, they have a similar research goal as we do.</p>
<p>There are also further cybersecurity BERT models, which, however, are fine-tuned instead of continued pre-training in case of true DAPT, which makes them less suitable for other task of the cybersecurity domain. MalBERT [26] is a BERT-based model from the cybersecurity domain focusing on the detection of malicious software. Another security-related work is CatBERT, introduced by Lee et al. [19]. They replaced some transformer blocks with adapters and fine-tuned the BERT model for the detection of phishing emails. Mendsaikhan et al. [23] introduced a BERTbased Natural Language filter for identifying and classifying cyber threat-related information from publicly available information sources with high accuracy.</p>
<p>An overview of the approaches with their domains and how they are trained can be found in Table 1. These works are related to our work because the approach of adapting BERT to a specific domain is similar to our work and differs mainly in the target domain. So, all in all, the different pre-trained BERT approaches can be used in our work as an orientation and also for comparing our results to theirs w.r.t. the performance. Notwithstanding the fact that BERT has achieved great results in various domains, the full potentialities for the cybersecurity domain have yet to be exploited.</p>
<h3>2.2 Research Gap</h3>
<p>The research gap has led us to develop a model with the aim of achieving satisfactory performance for cybersecurity textual material in various tasks. BERT has already been transferred to different domains, resulting in domain-specific models (BioBERT [18], SciBERT [5]) and even specific domains in the cybersecurity domain, leading to models like MalBERT [26] or CatBERT [19].</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model / Paper</th>
<th style="text-align: left;">Domain / Use Case</th>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Model Base</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BioBERT [18]</td>
<td style="text-align: left;">biomedical</td>
<td style="text-align: left;">PT (+ FT)</td>
<td style="text-align: left;">BERT</td>
</tr>
<tr>
<td style="text-align: left;">SciBERT [5]</td>
<td style="text-align: left;">scientific</td>
<td style="text-align: left;">PT (+ FT)</td>
<td style="text-align: left;">BERT</td>
</tr>
<tr>
<td style="text-align: left;">[11]</td>
<td style="text-align: left;">Papers (bio. +</td>
<td style="text-align: left;">PT (+ FT)</td>
<td style="text-align: left;">RoBERTA</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">CS), news, reviews</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">MaLbert [26]</td>
<td style="text-align: left;">malware</td>
<td style="text-align: left;">FT</td>
<td style="text-align: left;">BERT,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">RoBERTA,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">DistilBERT</td>
</tr>
<tr>
<td style="text-align: left;">CatBERT [19]</td>
<td style="text-align: left;">phishing</td>
<td style="text-align: left;">FT</td>
<td style="text-align: left;">DistilBERT</td>
</tr>
<tr>
<td style="text-align: left;">ExBERT</td>
<td style="text-align: left;">exploit prediction</td>
<td style="text-align: left;">FT</td>
<td style="text-align: left;">BERT</td>
</tr>
<tr>
<td style="text-align: left;">[23]</td>
<td style="text-align: left;">CTI</td>
<td style="text-align: left;">FT</td>
<td style="text-align: left;">BERT</td>
</tr>
</tbody>
</table>
<p>Table 1. Overview over relevant existing BERT models for special domains. The method explains if the model was only fine-tuned (FT) or also pre-trained (PT).</p>
<p>As outlined in the introduction, there are a multitude of research problems in the field of cybersecurity based on the essential part of information extraction. A solid method to address this can improve research in this field at a stroke. Furthermore, it enables extensibility and additional layers can be applied on top of the model, such as CRF [33], (Bi)LSTM, or both combined [13].</p>
<p>Ranade et al. [27] also addresses the delineated research gap to some extent. Unfortunately, there was no juxtaposition with the results of BERT as the baseline but only a presentation of their model's outcome. We compare our CySecBERT with theirs, which is varied in the model training and the corpus [27]. Furthermore, in delimitation to their work, we also evaluate a whole span of different cybersecurity tasks, ranging from classification to NER and clustering tasks and we include the results of BERT for comparison reasons. The latter results from the fact that we also take into account the phenomenon of catastrophic forgetting, where the pre-trained model forgets its already acquired knowledge in the new training phase, which has not been studied in other work in this area. The similarity in both works results from the nature of the research task and also underlines the importance of the approach. It is encouraging and important at the same time that there is such attention for this research gap. Multiple works addressing a similar objective can be complementary and, therefore, expedite filling the gap in research. Nonetheless, our work is distinguished from this paper at several points including the evaluation step, the applied data, and overall the extent of our work.</p>
<h1>3 METHODOLOGY</h1>
<p>This section presents a short background on domain adaptive pre-training, including the planned training process, the dataset used to adapt our proposed language model to the cybersecurity domain, and the evaluation process.</p>
<h3>3.1 Domain Adaptive Pre-Training</h3>
<p>DAPT of language models to a specific domain is a common method to achieve an advanced domain-specific language model (cf. §2). It has been shown to increase model performance in several ways, from model performance on downstream tasks, and hence better evaluation results, to reduced training time for such tasks due to smaller datasets for the training process to achieve similar performance. These prospects lead us to expect that the cybersecurity domain will benefit</p>
<table>
<thead>
<tr>
<th style="text-align: left;">#Tokens</th>
<th style="text-align: right;">$\mathrm{Min}^{5}$</th>
<th style="text-align: right;">$\mathrm{Max}^{5}$</th>
<th style="text-align: right;">Sum</th>
<th style="text-align: right;">Median</th>
<th style="text-align: right;">Mean</th>
<th style="text-align: right;">Entries</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Blogs</td>
<td style="text-align: right;">44</td>
<td style="text-align: right;">0.1 M</td>
<td style="text-align: right;">169 M</td>
<td style="text-align: right;">710</td>
<td style="text-align: right;">1.1 k</td>
<td style="text-align: right;">151 k</td>
</tr>
<tr>
<td style="text-align: left;">arXiv</td>
<td style="text-align: right;">533</td>
<td style="text-align: right;">0.7 M</td>
<td style="text-align: right;">167 M</td>
<td style="text-align: right;">8.2 k</td>
<td style="text-align: right;">9.9 k</td>
<td style="text-align: right;">16 k</td>
</tr>
<tr>
<td style="text-align: left;">NVD</td>
<td style="text-align: right;">5</td>
<td style="text-align: right;">1.9 k</td>
<td style="text-align: right;">12 M</td>
<td style="text-align: right;">58</td>
<td style="text-align: right;">71</td>
<td style="text-align: right;">171 k</td>
</tr>
<tr>
<td style="text-align: left;">Twitter</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">500</td>
<td style="text-align: right;">179 M</td>
<td style="text-align: right;">39</td>
<td style="text-align: right;">45</td>
<td style="text-align: right;">4 M</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0.7 M</td>
<td style="text-align: right;">528 M</td>
<td style="text-align: right;">40</td>
<td style="text-align: right;">122</td>
<td style="text-align: right;">4.3 M</td>
</tr>
</tbody>
</table>
<p>Table 2. Statistics of the number of tokens based on the subset of our training dataset.
greatly from a domain-adapted, pre-trained language model for every possible task, e.g., NER and relevance classification, to name a few.</p>
<p>We aim to adapt BERT to the cybersecurity domain based on domain specific text corpora (cf. §3.2) [11]. Our DAPT pipeline is build with Huggingface ${ }^{2}$ and Weights and Biases ${ }^{3}$. The final domainadapted pre-trained model is based on bert-base-uncased. Likewise, the text corpus is tokenized using the bert-base-uncased model. The training itself is done on the Lichtenberg Cluster ${ }^{4}$.</p>
<p>During the training phase, we try to mitigate the problem of catastrophic forgetting [22] by reducing the learning rate, the training steps, and the size of the dataset compared to BERT pre-training. In this way, the susceptibility to catastrophic forgetting should be greatly reduced because the new learning process is subordinated. Nevertheless, we test whether the problem also occurs with the created model by evaluating it on a non-cybersecurity task. While we expect no improvements, we want to analyse to what extent the old knowledge has altered.</p>
<h1>3.2 Text Corpus</h1>
<p>When creating the text corpus, we paid a lot of attention to the quality of the data, as this quality transfers to the model [4]. The text corpus is composed of different sub-corpora: (i) blog data, (ii) arXiv data, (iii) national vulnerability database (NVD) data, and (iv) Twitter data. This decision is based on the kind of information, that can be found in either source and the fact, that the information is used in recent publications regarding machine learning. Furthermore, they vary widely in their structure. While the NVD contains short, objective and precise language with semi-structured information, Twitter consists of short posts with objective, subjective, emotional, on- as well as off-topic, etc. content, arXiv encompasses long papers with highly educational language, and blog posts are typically longer articles with less formal language.</p>
<p>The blog posts build a solid foundation for different writing styles and practical information in information security, including vulnerability and exploit information, threat notifications [20], and foundational knowledge. We crawled 38 different blogs, filtered duplicates and instances shorter than 300 characters $^{6}$, resulting in over 151 k blog posts.</p>
<p>Second, we use arXiv papers from the category Cryptography and Security ${ }^{7}$ [18]. Due to errors during the text extraction process, we ignored papers with lower length than 3000 characters, resulting in over 16 k papers.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1. Training loss of the CySecBERT model.</p>
<p>Third, we use vulnerability descriptions of the NVD [9, 16]. Experts curate those texts ${ }^{8}$, so they need no further processing. Hence, we do neither filter nor pre-process these information.</p>
<p>Lastly we use Twitter as information source [7, 29, 30]. We crawled datasets with the following keywords:</p>
<ul>
<li>infosec OR security OR threat OR vulnerability OR cyber OR cybersec OR infrasec OR netsec OR hacking OR siem OR soc OR offsec OR osing OR bugbounty
Additionally, we also crawled dedicated datasets of data breaches, as, for example, the Microsoft Exchange Server Data Breach. Overall, we managed to crawl nearly 4M tweets with over 179M tokens in total.</li>
</ul>
<p>A summary of all datasets is depicted in Tab. 2.</p>
<h1>3.3 Setup</h1>
<p>While the authors of BERT trained the model with a learning rate of $1 \times 10^{-4}$ for about 40 epochs, we adapted BERT to the cybersecurity domain with a learning rate of $2 \times 10^{-5}$ and 30 epochs on a dataset that is $10 \%$ of the size of the original BERT dataset. We trained our proposed model with a batch size of 64 on 4 Nvidia Tesla V100 GPUs. For the other hyperparameters, we followed the original BERT work, i.e., we used a weight decay of 0.01 , a dropout rate of $0.1,10000$ warm-up steps, and ADAM as the optimisation algorithm [14]. The training loss of the CySecBERT model can be seen in Fig. 1. It shows that the loss decreases logarithmically and only improves very slowly after 300k steps.</p>
<h2>4 EVALUATION</h2>
<p>In this section, the evaluation process and the corresponding results are presented in detail. We cover a short description of the evaluation tasks in $\S 4.1$. After this the presentation and interpretation of the results follow in $\S 4.2$.</p>
<h3>4.1 Experiments and Tasks</h3>
<p>Since our goal is to publish a model that is highly usable for the cybersecurity domain, we evaluate it against the current standard method of the domain (BERT) and another cybersecurity language model (CyBERT from Ranade et al. [27]). We use different types of tasks, i.e. intrinsic and extrinsic</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>evaluation tasks, which are reasonably chosen in the field of cybersecurity. While the extrinsic tasks measure how well the trained model performs on downstream tasks, i.e. measure real-world application, the intrinsic tasks measure the model itself without any kind of additional classifier, e.g. by measuring the representations of the model and showing an overall fit to the domain.</p>
<p>As intrinsic tasks we use a word similarity task using parts of the dataset by Mumtaz et al. [25] and a Twitter dataset for clustering evaluation. The clustering dataset is based on a random sample of Log4j Twitter posts. For this task, the posts are converted into latent representations with the different BERT models. The latent representation consists of the concatenation of the last four layers of the model output and then the mean values over all words in the post. A KMeans clustering algorithm with k-values from 5 to 9 is executed on the gathered and transformed posts. The evaluation scores are measured with the Silhouette Coefficient (the higher the better). This is an internal clustering metric that analyses the clusters created and does not require gold labels. This is important because there can be many solutions and gold labels can be misleading in the case of clustering [2]. The cybersecurity word similarity dataset consists of words with their similar words, all from the field of cybersecurity. The dataset is an extension of the public cybersecurity word similarity dataset from Mumtaz et al. [25] and contains over 300 word pairs. Normally, the evaluation is based on the cosine similarity of the word embeddings when static embeddings are used. However, BERT is context-dependent, so the cosine similarity of BERT-encoded words is not a good metric for scoring. For this reason, we developed a novel word similarity evaluation method that asks the model to predict whether two given words are similar in a zero-shot learning setting. Similar to the works on zero-shot learning, we create a meaningful cloze task for the model consisting of a sentence with a masked word that the model fills in, which is implicitly the answer to the similarity question. The task is written in the following way:
"Are word 1 and word 2 similar? [MASK]", where [MASK] can be either "Yes" or "No", which represent the masked words that the model has to fill.</p>
<p>Example: "Are virus and malware similar? [MASK]"
Since we also want to show that the model does not only predict that every word is similar to every other word, we also randomly take word pairs from the dataset that are not similar and add them to the evaluation. This dataset is then used and an F1 score is calculated for the model's predictions.</p>
<p>As part of the extrinsic tasks, we use two cybersecurity classification tasks from Riebe et al. [29] and Bayer et al. [3]. In the first task, the classifier has to decide whether a Twitter post is related to the field of cybersecurity, and in the second task, it has to predict whether a post might be relevant to experts in the field during a major cybersecurity event. Furthermore, we use the sequence tagging dataset by Kuehn et al. [16]. Sequence tagging is the task of finding specific words in a text and means that each word in a text is tagged, often as IOB: Inside, outside, and beginning, referring to the specific words being searched for. Specifically, the dataset consists of several NER tasks for predicting relevant details of NVD descriptions. We chose the task of predicting the name and version of the software and the attack vector. These tasks were chosen for their different performances in Kuehn et al. [16]'s work, in order to analyse how well the models perform on different difficulties of the tasks.</p>
<p>Furthermore, we evaluate CySecBERT and BERT on the SuperGLUE benchmark [36]. This is a common NLP benchmark used in this work to identify signs of catastrophic forgetting [22]. We assume that our cybersecurity model is not able to achieve a better or even equivalent score, but still think that the scores should not be too bad, as this would indicate that some basic knowledge would have been forgotten during the domain training phase.</p>
<p>Due to the scatter of results, all extrinsic experiments were performed five times, and the mean values as well as the standard deviation are given if they are informative.</p>
<h1>4.2 Results</h1>
<p>As stated in $\S 4.1$, we aim to evaluate CySecBERT, as well as BERT and CyBERT [27], on different tasks, mainly settled in the cybersecurity (CySec) domain, both intrinsic and extrinsic evaluation tasks. Additionally, we run the SuperGLUE task to test our model for catastrophic forgetting.
4.2.1 Intrinsic Tasks. To measure the representation quality of the model, we evaluate it using intrinsic tasks from CySec: clustering and word similarity.</p>
<p>Clustering. The results of the clustering task are given in Table 3. While the CyBERT model of Ranade et al. [27], is only better than the baseline when forming 5-7 clusters, our approach is better in every constellation, according to the Silhouette Score. CySecBERT actually outperforms Ranade et al. [27]'s cybersecurity model by a considerable margin for each number of clusters, with consistent improvements ranging from +0.002 to +0.059 points. Our model shows the highest improvement when 9 clusters are formed. On the one hand, these results show that we can obtain more coherent clusters thanks to our trained language model. On the other hand, from a more general perspective, the results show that the model is better able to represent the given instances in a meaningful latent space.</p>
<p>Nevertheless, even better results can be expected if we use an approach like SentenceBERT by Reimers and Gurevych [28] for our model, as they prove to be much better for representing complete documents, like tweets in our case.</p>
<p>Word Similarity. The word similarity task results are shown in Tab. 4. The baseline model has the worst performance with a F1-score of 0.44 . This is to be expected, as most domain-specific words were not or only very rarely included in the standard BERT training. Our CySecBERTmodel is clearly superior to the other two approaches, which is very interesting to see and confirms the previous intrinsic results. However, we would like to point out that this task is different from other word similarity tasks and does not reflect the word similarities directly through the word representations, but by questioning the model in a cloze fashion (see 4.1).</p>
<table>
<thead>
<tr>
<th style="text-align: left;"># Clusters</th>
<th style="text-align: left;">BERT</th>
<th style="text-align: left;">$[27]$</th>
<th style="text-align: left;">CySecBERT</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: left;">0.114</td>
<td style="text-align: left;">0.141</td>
<td style="text-align: left;">$\mathbf{0 . 1 4 3}$</td>
</tr>
<tr>
<td style="text-align: left;">6</td>
<td style="text-align: left;">0.115</td>
<td style="text-align: left;">0.124</td>
<td style="text-align: left;">$\mathbf{0 . 1 5 0}$</td>
</tr>
<tr>
<td style="text-align: left;">7</td>
<td style="text-align: left;">0.118</td>
<td style="text-align: left;">0.133</td>
<td style="text-align: left;">$\mathbf{0 . 1 6 7}$</td>
</tr>
<tr>
<td style="text-align: left;">8</td>
<td style="text-align: left;">0.125</td>
<td style="text-align: left;">0.117</td>
<td style="text-align: left;">$\mathbf{0 . 1 6 3}$</td>
</tr>
<tr>
<td style="text-align: left;">9</td>
<td style="text-align: left;">0.130</td>
<td style="text-align: left;">0.113</td>
<td style="text-align: left;">$\mathbf{0 . 1 7 2}$</td>
</tr>
</tbody>
</table>
<p>Table 3. Silhouette Score of the first intrinsic task, clustering the data of a Log4J dataset. The best values are marked.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tasks</th>
<th style="text-align: left;">Word Similarity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">0.4382</td>
</tr>
<tr>
<td style="text-align: left;">$[27]$</td>
<td style="text-align: left;">0.4861</td>
</tr>
<tr>
<td style="text-align: left;">CySecBERT</td>
<td style="text-align: left;">$\mathbf{0 . 6 3 8 2}$</td>
</tr>
</tbody>
</table>
<p>Table 4. Overview of the results of the word similarity task where the scores are indicated by the F1 score.
4.2.2 Extrinsic Tasks. Now that we have shown that the model produces meaningful representations of cybersecurity words and data, we want to check whether our model is also more suitable for real-world applications, i.e. for extrinsic tasks, the so-called downstream tasks of machine learning. The tasks that we have chosen for the cybersecurity domain are (i) NER (ii) general relevance classification, and (iii) CTI classification .</p>
<p>NER. The results of the NER task are shown in Tab. 5. While we can see that the basic BERT model and the CyBERT model of Ranade et al. [27] are more similar, e.g. in terms of software</p>
<table>
<thead>
<tr>
<th style="text-align: left;">CVSS NER</th>
<th style="text-align: center;">SV</th>
<th style="text-align: center;">SN</th>
<th style="text-align: center;">AC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">$0.9247(0.0064)$</td>
<td style="text-align: center;">$0.8837(0.0037)$</td>
<td style="text-align: center;">$0.3323(0.0135)$</td>
</tr>
<tr>
<td style="text-align: left;">$[27]$</td>
<td style="text-align: center;">$0.9298(0.0019)$</td>
<td style="text-align: center;">$0.8834(0.0029)$</td>
<td style="text-align: center;">$0.3336(0.0214)$</td>
</tr>
<tr>
<td style="text-align: left;">CySecBERT</td>
<td style="text-align: center;">$\mathbf{0 . 9 3 0 2}(0.0066)$</td>
<td style="text-align: center;">$\mathbf{0 . 8 8 7 1}(0.0025)$</td>
<td style="text-align: center;">$\mathbf{0 . 3 4 7 2}(0.0116)$</td>
</tr>
</tbody>
</table>
<p>Table 5. Named entity recognition score based on tagged software versions (SV), software names (SN), and attack complexities (AC) of NVD descriptions. The results are given as F1 scores and the best values are marked.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">MS Exchange</th>
<th style="text-align: left;">CySecAlert</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: left;">$0.8599(0.0193)$</td>
<td style="text-align: left;">$0.8779(0.0084)$</td>
</tr>
<tr>
<td style="text-align: left;">$[27]$</td>
<td style="text-align: left;">$0.8766(0.0153)$</td>
<td style="text-align: left;">$0.8647(0.0095)$</td>
</tr>
<tr>
<td style="text-align: left;">CySecBERT</td>
<td style="text-align: left;">$\mathbf{0 . 8 8 6 9 ( 0 . 0 0 2 6 )}$</td>
<td style="text-align: left;">$\mathbf{0 . 8 8 8 3 ( 0 . 0 0 6 4 )}$</td>
</tr>
</tbody>
</table>
<p>Table 6. Classification results of the MS Exchange and CySecAlert dataset, given as F1 scores. The best values are marked.
naming (SN), our model consistently outperforms both. Only in the tagging of the software version (SV) does the CyBERT model of Ranade et al. [27] perform significantly better than the baseline BERT model, while our model nevertheless improves this result. One can speculate that the CyBERT training data from Ranade et al. [27] contained software versions at a higher frequency than the normal BERT data. However, the CyBERT model deteriorates the results for software names (SN), which could indicate that either a large number of software names are missing in high frequency in their dataset or they have been neglected due to errors in the training process. The highest improvements of our model are seen in attack complexity (AC), with 0.0136 points better than the CyBERT model of Ranade et al. [27]. Nevertheless, the results of this particular task are not very satisfactory, which has already been discussed by Kuehn et al. [16] and is related to the problem of too little data in this task.</p>
<p>Relevance Classification (CySecAlert). In the first classification task of our experiments, the models are trained to predict whether a Twitter post is related to the cybersecurity domain (see Tab. 6). This can be considered a general cybersecurity task, as the model only needs to identify cybersecurityrelated words. In this context, it is particularly interesting to see that the Ranade et al. [27] model is worse than the basic BERT model. Our model significantly improves the base model and the CyBERT model by 0.0104 and 0.0236 points in the F1 score, respectively. All models have a relatively low standard deviation, indicating that the fine-tuning process is stable across runs.</p>
<p>Specialised CTI Classification (MS Exchange). The second classification task is about finding specialised CTI where very specific words are needed to classify the instances. The results are also presented in Tab. 6. Surprisingly, unlike the other tasks, the CyBERT model of Ranade et al. [27] is able to improve the baseline, showing that while it does not improve the more general tasks, it could be beneficial in more specific tasks. Here, there is a high improvement of our model compared to the baseline observation (+0.027), which we expected since this task focuses on very domain-dependent language and specific words. Moreover, although the CyBERT model of Ranade et al. [27] is advantageous for this task, our model still improves the results significantly by +0.0103 F1 points. Here we also see that our model has a significantly lower standard deviation than the other two models, which again indicates a very stable training process.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">record</th>
<th style="text-align: center;">rte</th>
<th style="text-align: center;">wic</th>
<th style="text-align: center;">wsc</th>
<th style="text-align: center;">boolq</th>
<th style="text-align: center;">cb</th>
<th style="text-align: center;">copa</th>
<th style="text-align: center;">multirc</th>
<th style="text-align: center;">total_score</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: center;">0.6416</td>
<td style="text-align: center;">0.5949</td>
<td style="text-align: center;">0.6476</td>
<td style="text-align: center;">0.5538</td>
<td style="text-align: center;">0.6760</td>
<td style="text-align: center;">0.3704</td>
<td style="text-align: center;">0.606</td>
<td style="text-align: center;">0.4067</td>
<td style="text-align: center;">0.600994</td>
</tr>
<tr>
<td style="text-align: left;">CySecBERT</td>
<td style="text-align: center;">0.6137</td>
<td style="text-align: center;">0.5545</td>
<td style="text-align: center;">0.5887</td>
<td style="text-align: center;">0.5404</td>
<td style="text-align: center;">0.6752</td>
<td style="text-align: center;">0.5551</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.3915</td>
<td style="text-align: center;">0.546831</td>
</tr>
</tbody>
</table>
<p>Table 7. Results of the SuperGLUE benchmark, each indicated in the evaluation metric proposed in the benchmark.</p>
<p>Catastrophic Forgetting. In the last part of our evaluation we address the problem of catastrophic forgetting. To this end, we evaluate our model with the SuperGLUE benchmark to see if the model degrades the results too much, which would indicate that the model has forgotten the initial knowledge acquired in the BERT training phase. The results of this task and a comparison to the BERT model is shown in Table 7. As expected, we can see that our model reduces almost every task outcome. Nevertheless, the worse results do not indicate catastrophic forgetting, as the differences are still relatively small, with a mean drop of about -0.05 points. This shows that although the model has lost some of its knowledge, it still has most of it left. It is interesting to see that the cb task has even increased and the result of the boolq task has remained almost the same.</p>
<p>Conclusion. In our evaluation, we have shown that the model developed in this work has very good cybersecurity capabilities. The tasks showed that the CySecBERT model is able to outperform the BERT baseline and the CyBERT model by Ranade et al. [27] consistently across all cybersecurity tasks. We evaluated these models on intrinsic cybersecurity tasks where we showed how well the models represent documents and words in latent space. These tasks assess the fundamental quality of the language model. In addition, we evaluated the three models using extrinsic cybersecurity tasks that demonstrate the practicality of the model in most real-world application contexts. Our model improves the results of these tasks by up to 0.027 F1 points compared to the other two models and achieves its highest improvement on an in-depth cybersecurity task where very specific language differences have to be considered. In addition, we also analysed the phenomenon of catastrophic forgetting by evaluating our model on standard NLP tasks. Although there is a deterioration in performance in these tasks, it is only within the expected range of decline. We can say with confidence that our model is capable of handling a wide range of cybersecurity tasks while retaining the original language modelling knowledge.</p>
<h1>5 DISCUSSION, CONCLUSION, AND OUTLOOK</h1>
<p>In this work we propose a novel state of the art cybersecurity language model based on BERT [8]. We perform DAPT on this model with a sensibly chosen cybersecurity corpus. The corpus consists of a variety of source data structures, such as blogs, paper, as well as Twitter data. The data and frequencies in the sources were selected to be appropriate for cybersecurity research and practice. Furthermore, the size of the dataset and the structure of the training process were chosen to prevent catastrophic forgetting on the one hand, and on the other hand, so that the model learns enough to contribute to the general field and specific niches of cybersecurity. We explore this through a thorough evaluation of various tasks and in comparison to the BERT baseline as well as the current state of the art of cybersecurity language models. First, we evaluate the models on two intrinsic tasks, where we show that the quality of our model improves in terms of the learned representation space, i.e. how well the cybersecurity-specific instances (words and texts) can be distinguished from each other. Table 3 and 4 show the substantial performance increases by our model. Second, we evaluated the model together with the other two models for cybersecurity-specific classification and NER tasks to show the usefulness and practicality of the model in application contexts. Our</p>
<p>model outperforms the other models in every task, which can be seen in Table 5 and 6 . The greatest improvement is observed in the special CTI classification dataset, suggesting that the model is particularly beneficial when dealing with very specific cybersecurity language that a normal BERT model did not have in the training dataset. Our evaluation concludes with a focus on catastrophic forgetting by assessing the performance of our models against a general NLP benchmark. While these results (table 7) show that our model does indeed degrade the results, they also show that, as intended, there is no catastrophic forgetting and that the final model has combined much of its original knowledge with the new knowledge about cybersecurity.</p>
<p>While we are aware that the current state of the art on research in language modeling and NLP generally tends to focus on larger language models, like GPT-3 by Brown et al. [6], we have chosen the BERT model on purpose. Most of the cybersecurity research and especially practice does not have the necessary resources to apply large language models. In most cases, the BERT model can still be considered the standard model in such ML application contexts as the cybersecurity domain. In this way, our work benefits most for the research landscape and practice.</p>
<p>Practical and Theoretical Implications. Our work contributes to research and pratice through a novel, state-of-the-art cybersecurity model called CYSecBERT, which is published. We also publish the associated dataset so that it can contribute to further work. Thus, our work has several implications for practice and research:</p>
<p>A novel, state-of-the-art language model for cybersecurity that is useful for various tasks. With the research around the model, we aimed to find a solution to increase the performance of machine learning in as many cybersecurity language tasks as possible. Our model provides utility for a large number of tasks, as it can be estimated based on the success in extrinsic task scores as well as inferred from intrinsic task scores, which show that the representation space is better for the domain-dependent language with our model. With the release of the model, we are paving the way for better cybersecurity tools, as practitioners can easily use the new model in existing pipelines, for example, in alert aggregation [17], detection of phishing websites [38, 39], or even maleware detection [31]. The better tools will then also be the result of new research derived from the model and will even improve the results in various tasks by incorporating further research ideas. This can be done on a smaller scale, where the model is not the focus but serves as a foundation on which further techniques such as data augmentation, meaningful data selection, few-shot learning or specific applications are built. But it can also be done on a larger scale where the model is the subject of research, for example by analysing its results in explainable AI approaches.</p>
<p>A sensibly chosen cybersecurity dataset containing most relevant sources. When creating the dataset, care was taken to include many different sources so that the model can be used for a wide range of cybersecurity tasks. The publication of this dataset can be used in further work analysing the content, e.g. if there is a bias in the data. The dataset can also serve as a basis for training other language models. Although we have deliberately chosen this size of dataset for BERT training to prevent catastrophic forgetting, it might be useful to expand the dataset, which can easily be done by collecting more data sources that we have already selected.</p>
<p>Ethical Considerations. We would like to emphasise that we did not explicitly focus on and analyse social biases in the data or the resulting model. While this may not be so damaging for most application contexts, there are certainly applications that rely heavily on these biases, and including any kind of discrimination can have serious consequences. As authors, we would like to express our warnings regarding the use of the model in such contexts. Nonetheless, we aim for an open source mentality, seeing the great impact it can have, and therefore transfer the thinking to the user of the model, drawing on the many previous discussions in the open source community.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>We thank all anonymous reviewers of this work. This research work has been funded by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE and by the German Federal Ministry for Education and Research (BMBF) in the project CYWARN (13N15407). The calculations for this research were conducted on the Lichtenberg high performance computer of the TU Darmstadt.</p>
<h2>REFERENCES</h2>
<p>[1] Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew McDermott. 2019. Publicly available clinical BERT embeddings. arXiv preprint arXiv:1904.03323 (2019).
[2] Markus Bayer. 2021. Information Overload in Crisis Management: Bilingual Evaluation of Embedding Models for Clustering Social Media Posts in Emergencies. (2021), 19.
[3] Markus Bayer, Tobias Frey, and Christian Reuter. 2022. Multi-Level Fine-Tuning, Data Augmentation, and Few-Shot Learning for Specialized Cyber Threat Intelligence. http://arxiv.org/abs/2207.11076 arXiv:2207.11076 [cs].
[4] Markus Bayer, Marc-André Kaufhold, and Christian Reuter. 2022. A Survey on Data Augmentation for Text Classification. Comput. Surveys (June 2022), 3544558. https://doi.org/10.1145/3544558
[5] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676 (2019).
[6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. http://arxiv.org/abs/2005.14165
[7] Haipeng Chen, Rui Liu, Noseong Park, and V. S. Subrahmanian. 2019. Using twitter to predict when vulnerabilities will be exploited. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM Press, New York, New York, USA, 3143-3152. https://doi.org/10.1145/3292500.3330742
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
[9] Ying Dong, Wenbo Guo, Yueqi Chen, Xinyu Xing, Yuqing Zhang, and Gang Wang. 2019. Towards the detection of inconsistencies in public security vulnerability reports. In 28th {USENIX} Security Symposium ({USENIX} Security 19). $869-885$.
[10] SONG Tae Eun. [n. d.]. Cyber Warfare in the Russo-Ukrainian War: Assessment and Implications. ([n. d.]), 2.
[11] Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A Smith. 2020. Don't Stop Pretraining: Adapt Language Models to Domains and Tasks. arXiv preprint arXiv:2004.10964 (2020).
[12] Eoin Hinchy. 2022. Voice of the SOC Analyst. Technical Report. Tines. 39 pages. https://www.tines.com/reports/voice-of-the-soc-analyst/
[13] Shaohua Jiang, Shan Zhao, Kai Hou, Yang Liu, Li Zhang, et al. 2019. A BERT-BiLSTM-CRF model for Chinese electronic medical records named entity recognition. In 2019 12th International Conference on Intelligent Computation Technology and Automation (ICICTA). IEEE, 166-169.
[14] Diederik P. Kingma and Jimmy Ba. 2017. Adam: A Method for Stochastic Optimization. https://doi.org/10.48550/ arXiv.1412.6980 Number: arXiv:1412.6980 arXiv:1412.6980 [cs].
[15] Jan-David Krieger, Timo Spinde, Terry Ruas, Juhi Kulshrestha, and Bela Gipp. 2022. A domain-adaptive pre-training approach for language bias detection in news. arXiv preprint arXiv:2205.10773 (2022).
[16] Philipp Kuehn, Markus Bayer, Marc Wendelborn, and Christian Reuter. 2021. OVANA: An Approach to Analyze and Improve the Information Quality of Vulnerability Databases. In Proceedings of the 16th International Conference on Availability, Reliability and Security. ACM, 11. https://doi.org/10.1145/3465481.3465744
[17] Max Landauer, Florian Skopik, Markus Wurzenberger, and Andreas Rauber. 2022. Dealing with Security Alert Flooding: Using Machine Learning for Domain-independent Alert Aggregation. ACM Transactions on Privacy and Security 25, 3 (Aug. 2022), 1-36. https://doi.org/10.1145/3510581
[18] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36, 4 (2020), 1234-1240.
[19] Younghoo Lee, Joshua Saxe, and Richard Harang. 2020. CATBERT: Context-Aware Tiny BERT for Detecting Social Engineering Emails. arXiv preprint arXiv:2010.03484 (2020).</p>
<p>[20] Xiaojing Liao, Kan Yuan, Xiaofeng Wang, Zhou Li, Luyi Xing, and Raheem Beyah. 2016. Acing the IOC game: Toward automatic discovery and analysis of open-source cyber threat intelligence. In Proceedings of the ACM Conference on Computer and Communications Security, Vol. 24-28-Octo. ACM Press, New York, New York, USA, 755-766. https: //doi.org/10.1145/2976749.2978315
[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).
[22] Michael McCloskey and Neal J Cohen. 1989. Catastrophic interference in connectionist networks: The sequential learning problem. In Psychology of learning and motivation. Vol. 24. Elsevier, 109-165.
[23] Otgonpurev Mendsaikhan, Hirokazu Hasegawa, Yukiko Yamaguchi, Hajime Shimada, and Enkhbold Bataa. 2020. Identification of cybersecurity specific content using different language models. Journal of Information Processing 28 (2020), 623-632.
[24] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In 1st International Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings.
[25] Sara Mumtaz, Carlos Rodriguez, Boualem Benatallah, Mortada Al-Banna, and Shayan Zamanirad. 2020. Learning Word Representation for the Cyber Security Vulnerability Domain. In 2020 International Joint Conference on Neural Networks (IJCNN). IEEE, 1-8.
[26] Abir Rahali and Moulay A Akhloufi. 2021. MalBERT: Using Transformers for Cybersecurity and Malicious Software Detection. arXiv preprint arXiv:2103.03806 (2021).
[27] Priyanka Ranade, Aritran Piplai, Anupam Joshi, Tim Finin, et al. 2021. CyBERT: Contextualized Embeddings for the Cybersecurity Domain. In IEEE International Conference on Big Data.
[28] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese BERT-Networks. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). https://doi.org/10.18653/v1/d19-1410
[29] Thea Riebe, Tristan Wirth, Markus Bayer, Philipp Kuehn, Marc-André Kaufhold, Volker Knauthe, Stefan Guthe, and Christian Reuter. 2021. CySecAlert: An Alert Generation System for Cyber Security Events Using Open Source Intelligence Data. In International Conference on Information and Communications Security (ICICS).
[30] Carl Sabottke, Octavian Suciu, and Tudor Dumitras. 2015. Vulnerability disclosure in the age of social media: Exploiting twitter for predicting real-world exploits. Proceedings of the 24th USENIX Security Symposium (2015), 1041-1056.
[31] Aleseldin Salem, Sebastian Banescu, and Alexander Pretschner. 2021. Maat: Automatically Analyzing VirusTotal for Accurate Labeling and Effective Malware Detection. ACM Transactions on Privacy and Security 24, 4 (Nov. 2021), 1-35. https://doi.org/10.1145/3465361
[32] Bhavna Soman. 2019. Death to the IOC: What's Next in Threat Intelligence. https://www.blackhat.com/us-19/briefings/ schedule/#death-to-the-ioc-whats-next-in-threat-intelligence-15392. Online; accessed 28-December-2020.
[33] Fábio Souza, Rodrigo Nogueira, and Roberto Lotufo. 2019. Portuguese named entity recognition using BERT-CRF. arXiv preprint arXiv:1909.10649 (2019).
[34] Wiem Tounsi and Helmi Rais. 2018. A survey on technical threat intelligence in the age of sophisticated cyber attacks. Computers \&amp; Security 72 (Jan 2018), 212-233. https://doi.org/10.1016/j.cose.2017.09.001
[35] Thomas D. Wagner, Khaled Mahbub, Esther Palomar, and Ali E. Abdallah. 2019. Cyber threat intelligence sharing: Survey and research directions. Computers \&amp; Security 87 (Nov 2019), 101589. https://doi.org/10.1016/j.cose.2019.101589
[36] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2020. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. arXiv:1905.00537 [cs] (Feb. 2020). http://arxiv.org/abs/1905.00537 arXiv: 1905.00537.
[37] Suzanne Widup, Dave Hylender, Gabriel Bassett, Philippe Langlois, and Alex Pinto. 2020. 2020 Verizon Data Breach Investigations Report. (05 2020). https://doi.org/10.13140/RG.2.2.21300.48008
[38] Guang Xiang, Jason Hong, Carolyn P. Rose, and Lorrie Cranor. 2011. CANTINA+: A Feature-Rich Machine Learning Framework for Detecting Phishing Web Sites. ACM Transactions on Information and System Security 14, 2 (Sept. 2011), 1-28. https://doi.org/10.1145/2019599.2019606
[39] Peng Yang, Guangzhen Zhao, and Peng Zeng. 2019. Phishing Website Detection Based on Multidimensional Features Driven by Deep Learning. IEEE Access 7 (2019), 15196-15209. https://doi.org/10.1109/ACCESS.2019.2892066</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ https://www.cve.org/ResourcesSupport/FAQs#pc_cve_recordscve_record_descriptions_created&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
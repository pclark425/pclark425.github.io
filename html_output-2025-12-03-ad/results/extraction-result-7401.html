<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7401 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7401</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7401</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-274306423</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.17976v4.pdf" target="_blank">The importance of visual modelling languages in generative software engineering</a></p>
                <p><strong>Paper Abstract:</strong> Multimodal GPTs represent a watershed in the interplay between Software Engineering and Generative Artificial Intelligence. GPT-4 accepts image and text inputs, rather than simply natural language. We investigate relevant use cases stemming from these enhanced capabilities of GPT-4. To the best of our knowledge, no other work has investigated similar use cases involving Software Engineering tasks carried out via multimodal GPTs prompted with a mix of diagrams and natural language.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7401.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7401.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multimodal diagram+text prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal prompting combining UML diagrams (images) with natural language instructions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using image attachments (UML sketches/PlantUML diagrams) together with concise natural-language instructions to prompt multimodal LLMs (Copilot/GPT-4o, Gemini) to generate, implement, or reverse-engineer software artefacts; authors report practical success across multiple nontrivial software tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Copilot (GPT-4o) and Gemini (gemini-1.5-flash)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal large language models used via Microsoft Copilot (based on GPT-4o, web interface with image attachment support) and a Gemini 1.5 flash pipeline implemented in a Jupyter Notebook; default LLM parameters were used and no role/context prompt was set.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Diagram-guided software development (class/interaction/behavior → code)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Implement classes, methods and full applications guided by UML diagrams (class diagrams, sequence/communication/state/activity diagrams) and short natural-language prompts; also reverse-engineer diagrams from code.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multimodal: image (UML diagram) + single-sentence natural language prompts in a chat (prompt chaining); images provided as attachments (PlantUML/hand-drawn diagrams).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompts were short (single sentence) and often chained across multiple chat rounds (Prompt Chaining). All LLM parameters left at defaults; no role/context instructions provided. Authors used both direct single multimodal prompts and iterative prompts that feed back stack traces or corrected outputs. Examples include attaching a class diagram and asking to implement specific classes, or attaching a sequence diagram and asking for code reflecting the interaction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>prompts-to-completion / successful implementation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported: mathematical expressions evaluator — 1 multimodal prompt produced a complete implementation (plus 1 additional prompt to implement Visitor pattern); tic-tac-toe — complete application obtained in 10 short prompts (one-sentence each); checkers — complete application obtained in 20 short prompts (including ~10 prompts for debugging/fixes).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Not quantified against a natural-language-only baseline; authors claim qualitative gains in information-transfer efficiency and compression when using diagrams vs pure natural language prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Copilot web-based (GPT-4o) with image attachment support; an equivalent Gemini (gemini-1.5-flash) pipeline was developed for reproducibility; default decoding/temperature; interactions used prompt chaining and fed stack traces when debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The importance of visual modelling languages in generative software engineering', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7401.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7401.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt Chaining / iterative chat prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt Chaining: multi-round chat interactions to incrementally solve multipart programming/design tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-prompt chat strategy where tasks are broken into steps and the model is given successive prompts (often brief) that build on prior outputs; used both for stepwise implementation and for debugging by feeding stack traces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Copilot (GPT-4o); Gemini (gemini-1.5-flash)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Chat-based multimodal LLM interfaces used with default parameter settings; iterative conversation state preserved across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Incremental implementation and debugging of software from UML diagrams</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Start from low-complexity classes and progressively request implementations of dependent classes; when runtime errors occur, feed full stack traces and request fixes in subsequent prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Chat-style short prompts (one sentence each) chained across multiple rounds; error traces supplied as prompt payloads for debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors used sequences of short prompts (Prompt Chaining). Example counts: tic-tac-toe involved seven prompts to implement classes, two prompts to correct execution errors, and one final prompt for board visualization; checkers involved nine prompts to implement classes, one prompt to develop drawing logic, and ten additional prompts to fix exceptions and conceptual errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>number of prompts to reach working implementation / number of debugging prompts</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Tic-tac-toe: total 10 prompts (7 implementation + 2 debugging + 1 visualization). Checkers: 20 prompts total (≈10 implementation/debugging + others).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>No numeric comparison to single-shot or different prompting styles; authors present prompt counts to illustrate efficiency and practicality of iterative prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Default LLM parameters; conversation history retained across prompts; stack traces included verbatim when debugging.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The importance of visual modelling languages in generative software engineering', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7401.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7401.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code→diagram vs Diagram→PlantUML translation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparative behaviour of LLM when generating PlantUML diagrams from source code versus generating PlantUML from an attached diagram</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors observed that asking Copilot to create an activity diagram from Python code produced minor PlantUML syntax errors (in while loops), whereas asking it to generate PlantUML by translating a provided diagram produced no errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Copilot (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Web-based Copilot instance (GPT-4o) with image attachment and code-to-diagram translation capability; defaults used.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Generation of PlantUML activity diagrams</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce a PlantUML activity diagram either by (a) translating Python method source code into PlantUML, or (b) translating a provided activity diagram image into PlantUML text.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Input modality varied: (a) code snippet (text) → PlantUML output; (b) diagram image (or diagram representation) → PlantUML output.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / prompt source</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Two different input sources were compared qualitatively: direct code-to-diagram translation vs diagram-to-PlantUML translation. In the code→diagram case, the returned PlantUML had minor syntax issues for while loops that authors fixed by hand; in the diagram→PlantUML case, output had no errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>syntactic correctness of PlantUML output (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Code→diagram: returned PlantUML contained minor syntax errors (specifically in two while loops). Diagram→PlantUML: returned PlantUML produced no syntax errors.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative improvement (fewer syntax errors) when translating from diagram input vs from code input; no quantitative significance reported.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Copilot default settings; small manual fixes applied to PlantUML outputs where necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The importance of visual modelling languages in generative software engineering', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7401.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7401.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Diagram-based prompting (theoretical claim)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Authors' claim that diagram-based prompting yields higher information-transfer efficiency (compression) than natural language prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper argues, supported by a citation to 'Understanding is compression' (Li et al., 2024b), that diagrams leverage background knowledge in their visual elements and structure, producing compression and therefore improved information-transfer efficiency compared to pure natural language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (multimodal) (general claim)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal LLMs that accept image+text inputs (e.g., GPT-4 multimodal / GPT-4o); claim is conceptual and not tied to an exact experimental model configuration in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Information transfer / prompt efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Conceptual argument that diagrams can encode complex structured information compactly and thereby improve the effectiveness/efficiency of prompts to LLMs versus equivalent natural-language descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Contrast between diagram-based multimodal prompts and natural-language-only prompts (conceptual).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / input modality</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Paper cites compression theory and suggests diagrams compress domain knowledge into visual form, reducing verbosity required in prompts; no controlled experiments or numeric comparisons provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>qualitative efficiency / information-transfer</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Claimed qualitative advantage for diagrams (no numeric effect sizes reported).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The importance of visual modelling languages in generative software engineering', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7401.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7401.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot with design-pattern diagrams</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot prompting that includes UML diagram examples of new design patterns to operationalise them</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Authors propose using Few-Shot prompting that attaches UML diagrams representing a newly created design pattern (or examples) to guide the LLM in implementing or adapting that pattern; suggested as practical when the pattern is novel.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (multimodal) / Copilot (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multimodal LLMs capable of receiving image+text few-shot examples in the prompt; suggestion rather than an empirically tested protocol in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Operationalising novel design patterns via few-shot multimodal prompts</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide example diagrams (and possibly example implementations) in the prompt to teach the model a new design pattern and have it produce code or designs that follow the pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot multimodal prompt: attached UML diagram(s) as examples + a request to implement or adapt the pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Authors suggest attaching a UML diagram representing the new pattern and using few-shot prompting to obtain a working implementation. This is described as a practical technique but the paper contains no empirical comparison or numeric evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The importance of visual modelling languages in generative software engineering', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>From image to uml: First results of image based uml diagram generation using llms <em>(Rating: 2)</em></li>
                <li>On the assessment of generative ai in modeling tasks: an experience report with chatgpt and uml <em>(Rating: 2)</em></li>
                <li>Understanding is compression <em>(Rating: 2)</em></li>
                <li>The importance of visual modelling languages in generative software engineering <em>(Rating: 1)</em></li>
                <li>Sketches and diagrams in practice <em>(Rating: 1)</em></li>
                <li>Guiding large language models via directional stimulus prompting <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7401",
    "paper_id": "paper-274306423",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Multimodal diagram+text prompting",
            "name_full": "Multimodal prompting combining UML diagrams (images) with natural language instructions",
            "brief_description": "Using image attachments (UML sketches/PlantUML diagrams) together with concise natural-language instructions to prompt multimodal LLMs (Copilot/GPT-4o, Gemini) to generate, implement, or reverse-engineer software artefacts; authors report practical success across multiple nontrivial software tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Copilot (GPT-4o) and Gemini (gemini-1.5-flash)",
            "model_description": "Multimodal large language models used via Microsoft Copilot (based on GPT-4o, web interface with image attachment support) and a Gemini 1.5 flash pipeline implemented in a Jupyter Notebook; default LLM parameters were used and no role/context prompt was set.",
            "model_size": null,
            "task_name": "Diagram-guided software development (class/interaction/behavior → code)",
            "task_description": "Implement classes, methods and full applications guided by UML diagrams (class diagrams, sequence/communication/state/activity diagrams) and short natural-language prompts; also reverse-engineer diagrams from code.",
            "problem_format": "Multimodal: image (UML diagram) + single-sentence natural language prompts in a chat (prompt chaining); images provided as attachments (PlantUML/hand-drawn diagrams).",
            "format_category": "input modality",
            "format_details": "Prompts were short (single sentence) and often chained across multiple chat rounds (Prompt Chaining). All LLM parameters left at defaults; no role/context instructions provided. Authors used both direct single multimodal prompts and iterative prompts that feed back stack traces or corrected outputs. Examples include attaching a class diagram and asking to implement specific classes, or attaching a sequence diagram and asking for code reflecting the interaction.",
            "performance_metric": "prompts-to-completion / successful implementation",
            "performance_value": "Reported: mathematical expressions evaluator — 1 multimodal prompt produced a complete implementation (plus 1 additional prompt to implement Visitor pattern); tic-tac-toe — complete application obtained in 10 short prompts (one-sentence each); checkers — complete application obtained in 20 short prompts (including ~10 prompts for debugging/fixes).",
            "baseline_performance": null,
            "performance_change": "Not quantified against a natural-language-only baseline; authors claim qualitative gains in information-transfer efficiency and compression when using diagrams vs pure natural language prompting.",
            "experimental_setting": "Copilot web-based (GPT-4o) with image attachment support; an equivalent Gemini (gemini-1.5-flash) pipeline was developed for reproducibility; default decoding/temperature; interactions used prompt chaining and fed stack traces when debugging.",
            "statistical_significance": null,
            "uuid": "e7401.0",
            "source_info": {
                "paper_title": "The importance of visual modelling languages in generative software engineering",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Prompt Chaining / iterative chat prompting",
            "name_full": "Prompt Chaining: multi-round chat interactions to incrementally solve multipart programming/design tasks",
            "brief_description": "A multi-prompt chat strategy where tasks are broken into steps and the model is given successive prompts (often brief) that build on prior outputs; used both for stepwise implementation and for debugging by feeding stack traces.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Copilot (GPT-4o); Gemini (gemini-1.5-flash)",
            "model_description": "Chat-based multimodal LLM interfaces used with default parameter settings; iterative conversation state preserved across prompts.",
            "model_size": null,
            "task_name": "Incremental implementation and debugging of software from UML diagrams",
            "task_description": "Start from low-complexity classes and progressively request implementations of dependent classes; when runtime errors occur, feed full stack traces and request fixes in subsequent prompts.",
            "problem_format": "Chat-style short prompts (one sentence each) chained across multiple rounds; error traces supplied as prompt payloads for debugging.",
            "format_category": "prompt style",
            "format_details": "Authors used sequences of short prompts (Prompt Chaining). Example counts: tic-tac-toe involved seven prompts to implement classes, two prompts to correct execution errors, and one final prompt for board visualization; checkers involved nine prompts to implement classes, one prompt to develop drawing logic, and ten additional prompts to fix exceptions and conceptual errors.",
            "performance_metric": "number of prompts to reach working implementation / number of debugging prompts",
            "performance_value": "Tic-tac-toe: total 10 prompts (7 implementation + 2 debugging + 1 visualization). Checkers: 20 prompts total (≈10 implementation/debugging + others).",
            "baseline_performance": null,
            "performance_change": "No numeric comparison to single-shot or different prompting styles; authors present prompt counts to illustrate efficiency and practicality of iterative prompting.",
            "experimental_setting": "Default LLM parameters; conversation history retained across prompts; stack traces included verbatim when debugging.",
            "statistical_significance": null,
            "uuid": "e7401.1",
            "source_info": {
                "paper_title": "The importance of visual modelling languages in generative software engineering",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Code→diagram vs Diagram→PlantUML translation",
            "name_full": "Comparative behaviour of LLM when generating PlantUML diagrams from source code versus generating PlantUML from an attached diagram",
            "brief_description": "Authors observed that asking Copilot to create an activity diagram from Python code produced minor PlantUML syntax errors (in while loops), whereas asking it to generate PlantUML by translating a provided diagram produced no errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Copilot (GPT-4o)",
            "model_description": "Web-based Copilot instance (GPT-4o) with image attachment and code-to-diagram translation capability; defaults used.",
            "model_size": null,
            "task_name": "Generation of PlantUML activity diagrams",
            "task_description": "Produce a PlantUML activity diagram either by (a) translating Python method source code into PlantUML, or (b) translating a provided activity diagram image into PlantUML text.",
            "problem_format": "Input modality varied: (a) code snippet (text) → PlantUML output; (b) diagram image (or diagram representation) → PlantUML output.",
            "format_category": "input modality / prompt source",
            "format_details": "Two different input sources were compared qualitatively: direct code-to-diagram translation vs diagram-to-PlantUML translation. In the code→diagram case, the returned PlantUML had minor syntax issues for while loops that authors fixed by hand; in the diagram→PlantUML case, output had no errors.",
            "performance_metric": "syntactic correctness of PlantUML output (qualitative)",
            "performance_value": "Code→diagram: returned PlantUML contained minor syntax errors (specifically in two while loops). Diagram→PlantUML: returned PlantUML produced no syntax errors.",
            "baseline_performance": null,
            "performance_change": "Qualitative improvement (fewer syntax errors) when translating from diagram input vs from code input; no quantitative significance reported.",
            "experimental_setting": "Copilot default settings; small manual fixes applied to PlantUML outputs where necessary.",
            "statistical_significance": null,
            "uuid": "e7401.2",
            "source_info": {
                "paper_title": "The importance of visual modelling languages in generative software engineering",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Diagram-based prompting (theoretical claim)",
            "name_full": "Authors' claim that diagram-based prompting yields higher information-transfer efficiency (compression) than natural language prompting",
            "brief_description": "The paper argues, supported by a citation to 'Understanding is compression' (Li et al., 2024b), that diagrams leverage background knowledge in their visual elements and structure, producing compression and therefore improved information-transfer efficiency compared to pure natural language prompts.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (multimodal) (general claim)",
            "model_description": "Multimodal LLMs that accept image+text inputs (e.g., GPT-4 multimodal / GPT-4o); claim is conceptual and not tied to an exact experimental model configuration in this paper.",
            "model_size": null,
            "task_name": "Information transfer / prompt efficiency",
            "task_description": "Conceptual argument that diagrams can encode complex structured information compactly and thereby improve the effectiveness/efficiency of prompts to LLMs versus equivalent natural-language descriptions.",
            "problem_format": "Contrast between diagram-based multimodal prompts and natural-language-only prompts (conceptual).",
            "format_category": "prompt style / input modality",
            "format_details": "Paper cites compression theory and suggests diagrams compress domain knowledge into visual form, reducing verbosity required in prompts; no controlled experiments or numeric comparisons provided in this paper.",
            "performance_metric": "qualitative efficiency / information-transfer",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": "Claimed qualitative advantage for diagrams (no numeric effect sizes reported).",
            "experimental_setting": null,
            "statistical_significance": null,
            "uuid": "e7401.3",
            "source_info": {
                "paper_title": "The importance of visual modelling languages in generative software engineering",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Few-shot with design-pattern diagrams",
            "name_full": "Few-shot prompting that includes UML diagram examples of new design patterns to operationalise them",
            "brief_description": "Authors propose using Few-Shot prompting that attaches UML diagrams representing a newly created design pattern (or examples) to guide the LLM in implementing or adapting that pattern; suggested as practical when the pattern is novel.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "model_name": "GPT-4 (multimodal) / Copilot (GPT-4o)",
            "model_description": "Multimodal LLMs capable of receiving image+text few-shot examples in the prompt; suggestion rather than an empirically tested protocol in this paper.",
            "model_size": null,
            "task_name": "Operationalising novel design patterns via few-shot multimodal prompts",
            "task_description": "Provide example diagrams (and possibly example implementations) in the prompt to teach the model a new design pattern and have it produce code or designs that follow the pattern.",
            "problem_format": "Few-shot multimodal prompt: attached UML diagram(s) as examples + a request to implement or adapt the pattern.",
            "format_category": "prompt style / few-shot",
            "format_details": "Authors suggest attaching a UML diagram representing the new pattern and using few-shot prompting to obtain a working implementation. This is described as a practical technique but the paper contains no empirical comparison or numeric evaluation.",
            "performance_metric": null,
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": null,
            "statistical_significance": null,
            "uuid": "e7401.4",
            "source_info": {
                "paper_title": "The importance of visual modelling languages in generative software engineering",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "From image to uml: First results of image based uml diagram generation using llms",
            "rating": 2,
            "sanitized_title": "from_image_to_uml_first_results_of_image_based_uml_diagram_generation_using_llms"
        },
        {
            "paper_title": "On the assessment of generative ai in modeling tasks: an experience report with chatgpt and uml",
            "rating": 2,
            "sanitized_title": "on_the_assessment_of_generative_ai_in_modeling_tasks_an_experience_report_with_chatgpt_and_uml"
        },
        {
            "paper_title": "Understanding is compression",
            "rating": 2,
            "sanitized_title": "understanding_is_compression"
        },
        {
            "paper_title": "The importance of visual modelling languages in generative software engineering",
            "rating": 1,
            "sanitized_title": "the_importance_of_visual_modelling_languages_in_generative_software_engineering"
        },
        {
            "paper_title": "Sketches and diagrams in practice",
            "rating": 1,
            "sanitized_title": "sketches_and_diagrams_in_practice"
        },
        {
            "paper_title": "Guiding large language models via directional stimulus prompting",
            "rating": 1,
            "sanitized_title": "guiding_large_language_models_via_directional_stimulus_prompting"
        }
    ],
    "cost": 0.011035999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The importance of visual modelling languages in generative software engineering</p>
<p>Roberto Rossi roberto.rossi@ed.ac.uk 
Business School
University of Edinburgh
UK</p>
<p>The importance of visual modelling languages in generative software engineering
BE59A4F3385D997D455961CE0E22797A
Multimodal GPTs represent a watershed in the interplay between Software Engineering and Generative Artificial Intelligence.GPT-4 accepts image and text inputs, rather than simply natural language.We investigate relevant use cases stemming from these enhanced capabilities of GPT-4.To the best of our knowledge, no other work has investigated similar use cases involving Software Engineering tasks carried out via multimodal GPTs prompted with a mix of diagrams and natural language.</p>
<p>Introduction</p>
<p>Software engineering (SE) applies engineering principles to the development, operation, and maintenance of software systems, ensuring they are reliable, efficient, and meet user requirements.Generative Artificial Intelligence (GenAI) models, such as pre-trained models [Devlin et al., 2019;Vaswani et al., 2017] and large language models (LLMs), are revolutionising fields like computer vision and natural language processing through their ability to generate novel and contextually-appropriate content.In recent times, the interplay between SE and GenAI has been receiving increasing attention.A vast array of applications of pre-trained models and LLMs are surveyed in [Huang et al., 2024;Hou et al., 2024].However, when it comes to the type of data used in existing studies, it appears that all studies surveyed focused on text-based datasets, with the most prevalent type of data utilised in training LLMs for SE tasks bring programming tasks/problems expressed in natural language [Hou et al., 2024].Similar conclusions are reached in [Huang et al., 2024], which focused on seven sub-tasks of SE: requirements generation, code generation, code summarisation, test generation, patch generation, code optimisation, and code translation.In all cases, the pipeline typically begins with instructions in natural language that need to be used in the context of one or more of these seven sub-tasks.</p>
<p>In SE it is often the case that communication among developers, and between developers and customers, occurs in the form of sketches and diagrams [Baltes and Diehl, 2014] and not just via natural language.The reason for this can be easily understood once we consider the following excerpt taken from [Tufte, 2003].</p>
<p>A TALK, which proceeds at a pace of 100 to 160 spoken words per minute, is not an especially highresolution method of data transmission.Rates of transmitting visual evidence can be far higher.The artist Ad Reinhardt said, "As for a picture, if it isn't worth a thousand words, the hell with it."People can quickly look over tables with hundreds of numbers in the financial or sports pages in newspapers.People read 300 to 1,000 printed words a minute, and find their way around a printed map or a 35mm slide displaying 5 to 40 MB in the visual field.Often the visual channel is an intensely highresolution channel.</p>
<p>It is then not surprising that the ambition of automatically turning sketches and diagrams into working code, or to reverse engineer working code into diagrams, has existed for a very long time in SE.Not only sketches and diagrams represent a high-resolution communication channel, but when they are drawn following a standard, they become a form of technical languages.The importance of technical languages is that they are denotative: they say one thing and one thing only.Conversely, natural language is connotative: the meaning of a statement is context dependent.It is the intrinsic ambiguity of natural language that makes it not well suited for programming, or for communicating programming-related matters.</p>
<p>In this work, we argue that the advent of multimodal GPTs, such as GPT-4 [OpenAI, 2024], may represent a watershed in the interplay between SE and GenAI.GPT-4 accepts image and text inputs, and it can therefore receive prompts that contain sketches and diagrams, rather than simply natural language.We therefore investigate relevant use cases stemming from these enhanced capabilities of GPT-4.To the best of our knowledge, no other work has investigated similar use cases involving SE tasks carried out via multimodal GPTs prompted with a mix of diagrams and natural language.</p>
<p>The rest of this paper is organised as follows: Section 2 provides relevant background in GenAI and SE.Section 3 outlines the methodology adopted in our study.Section 4 investigates the role of multimodal GPTs in software development by illustrating a selection of use cases.This section provides relevant background in GenAI and SE.</p>
<p>Software Engineering and UML</p>
<p>The Unified Modelling Language (UML) is a generalpurpose standardised visual language for designing systems.In SE, UML is utilised in three main areas: use case development, static analysis, and dynamic analysis of the software system.Use case development is a key step of requirement analysis.Use cases 1 describe how a user interacts with a system or product to achieve a specific goal.Static analysis of a software system describes the structure of a system by showing its classes, their attributes, operations (or methods), and the relationships among objects.Dynamic analysis expresses and model the behaviour of the system over time.More specifically, in UML, use case diagrams support use case development.Class diagrams support static analysis; and interaction (sequence, activity, collaboration) diagrams support dynamic analysis.However, it should be noted that UML is a vast language that finds countless applications to go beyond the three areas here considered.</p>
<p>Despite UML being the standard visual language for designing systems, recent studies suggest that that most software developers favour informal hand-drawn diagrams and do not use UML; those using UML, tend to use it informally and selectively [Baltes and Diehl, 2014].The authors further advocated the development of suitable tools to make better use of such sketches.Our study argues that GenAI may represent such missing tool.</p>
<p>Generative Artificial Intelligence</p>
<p>GenAI refers to a class of AI models that can create new content, such as text, diagrams, or code, based on the patterns they learn from existing data [Eapen et al., 2023].Large Language Models (LLMs) are a specific category of GenAI models that focus on language-related tasks, such as text generation, translation, summarisation, and question answering.LLMs are typically based on a neural network architecture called a Transformer [Vaswani et al., 2017], which is pretrained -hence the name Generative Pretrained Transformer (GPT) -on a massive dataset of text and code, and which allows them to process and generate text by considering longrange dependencies and contextual information [Brown et al., 2020].The latest GPTs, such as GPT-4 [OpenAI, 2024], are multimodal: they accept image and text inputs, and produce image and text outputs, leading to new applications.</p>
<p>Prompts are instructions or input given to a large language model (LLM) to generate a specific response.Prompt engineering -the design prompting strategies to query language models -offers a cost-effective way to adapt pre-trained models without full fine-tuning.</p>
<p>Single Prompt Techniques.Zero-Shot Prompting involves providing tasks with natural language without additional context, relying on the model's pre-existing capabilities.Few-Shot Prompting includes providing examples within the prompt to guide the model, enhancing its performance on complex tasks by exposing it to the input and output patterns 1 User stories in Agile Software Development [Beck et al., 2001] [ Brown et al., 2020].Chain of Thought Prompting [Wei et al., 2024] aids in breaking down reasoning tasks into smaller steps to improve outcome accuracy, using either zero-shot or few-shot methods to encourage step-by-step thinking.</p>
<p>Multiple Prompt Techniques.</p>
<p>Voting/self-consistency [Wang et al., 2023] involves generating multiple responses and selecting the most common result, which can improve accuracy, especially for complex reasoning tasks.Divide and Conquer methods split tasks into subtasks handled in sequence for improved manageability and precision, seen in Directional Stimulus Prompting [Li et al., 2024a], Generated Knowledge [Liu et al., 2021], and Prompt Chaining.Selfevaluation asks the model to verify output accuracy, exemplified by Reflexion [Shinn et al., 2024] and Tree of Thoughts [Yao et al., 2024], enabling iterative improvement.</p>
<p>Retrieval-Augmented Generation (RAG) [Fan et al., 2024] and ReAct [Yao et al., 2023] combine LLMs with external systems to improve context handling and output relevance.</p>
<p>The role of GenAI in Software Engineering</p>
<p>In recent times, SE has become one of the important application areas for GenAI.We focus on two recent surveys [Huang et al., 2024;Hou et al., 2024] investigating the interplay between SE and GenAI over a large body of recent works.</p>
<p>Several studies, e.g.[Arora et al., 2024;White et al., 2024], investigated the use of GenAI in the context of the requirement engineering sub-task.GenAI does play a role in this sub-task, which is mainly concerned in turning requirements expressed in natural language by the customer into suitable user stories and/or conceptual diagrams [Robeer et al., 2016], but our focus in this study will not be on this subtask.Conversely, SE sub-tasks of interest in the context of the present study include: software design, software development, and code summarisation.</p>
<p>Application of LLMs in software design remains relatively sparse: [Huang et al., 2024] does not include any study within this sub-task; while [Hou et al., 2024] only report 4 works [Kolthoff et al., 2023;Mandal et al., 2023;White et al., 2024;Zhang et al., 2024], none of which overlaps with the content of the present study; they also stress that by expanding the use of LLMs to this under-explored area it is possible to improve how software designs are conceptualised.</p>
<p>Both surveys identify a plethora of works concerned with software development (including code generation, test case generation, patch generation, and code optimisation) and code summarisation.</p>
<p>Code generation has been object of investigation for a long time in the AI community.Early works used symbolic and neural-semiotic approaches [Alur et al., 2013].However, recent neurolinguistic models, such as GPT-4 [Liu et al., 2024] and Copilot [Ma et al., 2023], can generate code directly from natural language descriptions.While there are several works and benchmarks in the literature concerned with method-level code generation, to the best of our knowledge there is only one study and benchmark on class-level code generation [Du et al., 2023], and none on diagram-level code generation.Moreover, none of the studies listed in the above surveys focus on code generation leveraging multimodal prompts that include sketches &amp; diagrams.</p>
<p>Code summarisation [Ahmed et al., 2024] aims to automatically generate descriptions of a given source code.This technique improves code comprehension, documentation, and collaboration by providing clear summaries.Existing studies in code summarisation focus on analysing code structures and contexts to generate informative natural language summaries.None of the studies listed in the above surveys focus on code summarisation producing a diagram as its output.</p>
<p>Finally, while there exist a few studies that investigated the generation of UML diagram with support from LLMs [Conrardy and Cabot, 2024;Wang et al., 2024;Cámara et al., 2023], none of these studies go as far as investigating the generation of working code from UML diagrams, as well as the reverse engineering of relevant UML diagrams from existing code.Perhaps the most interesting study among the three listed is [Cámara et al., 2023], which focuses on building UML class diagrams in PlantUML notation, which we also adopt in this work, by using ChatGPT as a modelling assistant prompted with instructions in natural language.</p>
<p>Methodology</p>
<p>We develop a portfolio of novel GenSE use cases that, to the best of our knowledge, have not been previously investigated in the literature.</p>
<p>We utilise Microsoft Copilot in its web-based version, which is based on GPT-4o and allows image attachments.To ensure reproducibility of the discussion in Sections 4.1-4.4,we have also developed an equivalent pipeline in Gemini, formalised in a Jupyter Notebook based on gemini-1.5-flash,which is included in the supplementary material (SM). 2 In both cases, we left all LLM parameters to their default settings and we did not specify any role or context instructions.</p>
<p>Table 1 maps use cases discussed in the rest of this work to relevant SE sub-tasks of interest.All use cases are based on a duly documented interaction with the LLMs that takes the form of a chat comprising multiple rounds of questions and responses (Prompt Chaining), which allow users to step incrementally towards answers and thus get help with multipart problems [Google AI, 2025].The core principle underpinning all our use cases is to illustrate possible strategies to leverage UML diagrams in order to guide the software development process.More specifically, in Section 4.1 we leverage class diagrams to guide the LLM in the context of implementing relevant classes, attributes, and operations; in Section 4.2 we leverage interaction diagrams to guide the LLM in the context of implementing the desired system behaviour; in Section 4.3 we leverage hand-drawn activity diagrams to guide the LLM in the context of implementing a given method; in Section 4.4 we leverage design patterns to influence the design of a given system.Finally, in Section 4.5 we present three additional case studies: in the first, we leverage hand-drawn diagrams and design patterns to implement a mathematical expressions evaluator; in the second and third, which feature a higher degree of complexity, we leverage class diagrams to implement a tic tac toe game and a game of checkers, respectively.</p>
<p>Use case</p>
<p>Section</p>
<p>Multimodal GPTs in software development</p>
<p>In what follows, we will assume that a preliminary requirement analysis has been carried out, which has produced a portfolio of initial user stories.While GPTs may in principle support automated elicitation of user stories and fully automated translation of user stories into working software, as things stand today this level of automation in the realm of code generation is hardly found in SE practice; nor we suspect it would be beneficial, as it may conflict with some of the principles of Agile.What we find in practice are teams of software developers collaborating to translate user stories into working software.Our focus is to illustrate novel use cases of GPTs in this specific context.The role of sketches and diagrams in the daily work of software developers has been investigated in [Baltes and Diehl, 2014].This study found that most practitioners produced informal hand-drawn diagrams and did not use UML; those using UML, tended to use it informally and selectively.We argue that this stems from the fact that the adoption of a "formal" notation, at present, bears no advantage with respect to an "informal" one.Over several decades, firms have repeatedly tried to develop tools (e.g.IBM Rational) that could automatically generate working code from UML diagrams, or reverse engineer UML diagram from existing code.While these technologies still exist, it is rare to find developers who routinely develop a complete set of UML diagrams and then translate this to code using such tools.The lacklustre success of these tools is likely due to a fundamental misunderstanding of what UML is: a language for capturing and communicating conceptual requirements, not one aimed at describing a complete system.In other words, UML diagrams are most useful when they are high level and sufficiently abstract.Generation of complete working code requires such diagrams to reach a level of detail that is equivalent to the working code itself; but generating diagrams at this level of abstraction would be a complete waste of time: why then not generating the code itself directly?</p>
<p>Albeit there are already good reasons for practitioners to produce sketches and diagrams in certain circumstances, we believe the advent of multimodal GPTs, such as GPT-4 [Ope-nAI, 2024], will substantially increase the associated use cases.To exemplify this, in what follows we outline a set of use cases illustrating how practitioners may combine multimodal GPTs and UML diagrams to innovate SE practices.</p>
<p>Static modelling</p>
<p>As a motivating example to illustrate our use cases, we consider the introductory case study in Chapter 3 of [Stevens and Pooley, 2006].In this section, we focus on static modelling; in particular, we assume that the developer has already converted the relevant requirements into a preliminary class diagram such as that shown in Figure 1.The developer now We can then use PlantUML to visualise the corresponding UML class diagram, which is shown in Figure 2 and, incidentally, only partly matches the original design in Figure 1.This is due to the fact that the Python implementation, whilst not incorrect, does not fully capture all cardinality constraints in the original diagram.</p>
<p>Dynamic modelling</p>
<p>We may now want to proceed and describe the dynamic behaviour of the system.Rather than using natural language, we may describe specific aspect of such behaviour by leveraging suitable UML interaction diagrams, such as the sequence diagram in Figure 3.To achieve this, we will attach Figure 3 The resulting code is presented in the SM.</p>
<p>Next, we may want to explore the behaviour of the various entities involved in a method call by obtaining a communication diagram for it.Copilot can generate this diagram (Figure 4) via the following prompt chained to previous outputs.Generate a UML communication diagram in Plan-tUML notation illustrating the behaviour of the following code: member.borrowcopy(copy1).</p>
<p>In the code generated by Copilot, the state of the book object changes when a copy of the book is successfully borrowed.In particular, the book may change from being borrowable (there is a copy of it in the library) to not borrowable (all copies are out on loan or reserved).This behaviour can be represented via a state diagram.Rather than drawing such state diagram, we generate one via the following prompt.The resulting code is presented in the SM.</p>
<p>Finally, we can ask Copilot to generate a new state diagram in PlantUML notation that reflects the behaviour of the updated code.The resulting diagram is shown in Figure 7 and matches the behaviour in Figure 6.Copilot correctly recognises the fact that that the activity diagram represents a sorting algorithm (bubble sort), and returns a method implementing it.</p>
<p>Alternatively, assuming the code for our sorting algorithm is already available to us, we can ask Copilot to convert it to an activity diagram via the following prompt.</p>
<p>Design patterns</p>
<p>Design Patterns [Gamma et al., 1994] are reusable solutions to commonly occurring problems in software design.They are not specific implementations but rather general templates that can be adapted to various situations.By using design patterns, developers can create more flexible, maintainable, and efficient software systems.There exists a wealth of existing patterns available in the SE literature; In addition, new patterns can be created and illustrated via appropriate UML diagrams.For instance, we may consider the "Adapter" pattern, which converts the interface of a class into another interface clients expect.This pattern is illustrated in Figure 10.Design   pattern is well known, and there is no need to provide a diagram for it while prompting.However, if the user intends to utilise a brand new pattern, then Few-Shot prompting can be leveraged to obtain the desired result.In practice, this would entail attaching a UML diagram representing the new design pattern that needs to be operationalised in the context of the given coding task.</p>
<p>Implementation of complex systems</p>
<p>In this section, we discuss the implementation of three more complex applications.In all cases, the interaction takes the form of a chat comprising multiple rounds of questions and responses (Prompt Chaining), which allows the user to step incrementally towards a working implementation.</p>
<p>Evaluating mathematical expressions</p>
<p>In this first application, we consider a class diagram (provided in our SM) that conceptually captures a system that evaluates mathematical expressions comprising additions and subtractions.The system comprises five interrelated classes.We were able to obtain a complete implementation with a single multimodal prompt (diagram + instruction to implement the diagram).An additional prompt was required to implement an evaluator by leveraging the Visitor design pattern.</p>
<p>Tic tac toe</p>
<p>We consider the class diagram4 for a tic tac toe game.The diagram comprises over twenty interrelated classes.We use this diagram to guide the development of a tic tac toe Python application.The detailed interaction with Copilot is illustrated in our SM.This interaction involves seven prompts in which Copilot is asked to fully implement specific classes in the diagram.In line with traditional SE practices, we start from classes featuring low complexity and connectivity, and we then move towards other classes that depend on those already implemented.This is followed by two prompts to correct errors encountered while trying to execute the application.In each of these prompts the full stack trace of the error is fed to Copilot.Finally, an additional prompt is used to tailor board visualisation.A complete application that neatly matches the conceptual model presented in the class diagram is hence obtained in ten short prompts comprising a single sentence each.</p>
<p>Checkers</p>
<p>We consider the class diagram5 for a checkers game.The diagram comprises over ten interrelated classes and a complex underpinning logic.We use this diagram to guide the development of a checkers Python application.The detailed interaction with Copilot is illustrated in our SM.This interaction involves nine prompts in which Copilot is asked to fully implement specific classes in the diagram.This is followed by an additional prompt to develop a text-based drawing logic.Ten additional prompts are then required to correct errors encountered while trying to execute the application and play the game.These errors include both exceptions (e.g. a class attribute is missing) as well as conceptual errors that affect the game logic (a piece is not removed from the board after being jumped over) or the user interaction (e.g. the user is requested to input a move, but it is not stated if they are playing as black or white).A complete application that matches the conceptual model presented in the class diagram is hence obtained in twenty short prompts comprising a single sentence each.</p>
<p>In our SM we provide a more detailed description of each case study, as well as Jupyter Notebooks reporting the complete interactions with Copilot.remains a persistent issue in SE.However, GenAI has already started to change this picture via automatic generation of code comments.And yet, if sketches and diagram (both formal and informal) can be transformed, as we have shown, into a preliminary draft of the desired source code, then producing reliable visual descriptions of the software system under development suddenly becomes appealing.Likewise, if it is possible to selectively and cheaply reverse engineer part of such software system to visually inspect the behaviour of classes and methods, the developer becomes equipped with a formidable arsenal of high resolution communication tools to enhance code understanding, boost team communication, and resolve the long lasting conflict between investing time in developing working software or writing comprehensive documentation.These are all important and novel directions of enquiry that should be investigated in future studies.</p>
<p>Figure 1 :
1
Figure 1: Class model of a library</p>
<p>Figure 2: The reconstructed class diagram</p>
<p>Figure 3 :
3
Figure 3: A sequence diagram illustrating an interaction</p>
<p>Figure 4 :
4
Figure 4: A communication diagram</p>
<p>Figure 5 :
5
Figure 5: The reconstructed state diagram Assume now that the desired behaviour, illustrated in Figure 6, is slightly different from that obtained in this preliminary draft of our code.We can ask Copilot to amend the code</p>
<p>Figure 6 :
6
Figure 6: The desired state diagram by chaining the following prompt.</p>
<p>Figure 7 :
7
Figure 7: The revised state diagram</p>
<p>Figure 8 :
8
Figure 8: An activity diagram</p>
<p>Create an activity diagram in PlantUML notation for the following method in Python: [insert Python code].In this specific instance, Copilot returned a diagram with a small number of syntax errors.The errors were minor issues with the PlantUML syntax of the two while loops in the diagram, and could be easily fixed by hand.The resulting activity diagram is shown in Figure 9. Conversely, a second prompt asking Copilot to generate a diagram in PlantUML notation by translating directly the diagram in Figure8produced no errors.Since the focus of our discussion is facilitating -and not fully automating -SE activities, the presence of minor errors is of no concern, as these can easily be addressed in follow up prompts.</p>
<p>Figure 9 :
9
Figure 9: A PlantUML activity diagram of a sorting algorithm</p>
<p>Figure 10 :
10
Figure 10: The "Adapter" Pattern</p>
<p>Figure 11 :
11
Figure 11: The data transfer system</p>
<p>Figure 12 :
12
Figure 12: The data transfer system with an adaptor.</p>
<p>Section 5 provides concluding remarks by reflecting on how the former use cases address several research gaps in Generative Software Engineering (GenSE), as well as some longstanding issues in SE, and open new research directions.
2 BackgroundarXiv:2411.17976v4 [cs.SE] 20 Aug 2025</p>
<p>Table 1 :
1
Use cases investigated in the rest of this work (SD: software design; DE: software development; CS: code summarisation)
SE sub-tasks
https://github.com/gwr3n/gense
https://github.com/pelensky/JavaTTT/blob/master/UML.pdf
https://app.genmymodel.com/api/repository/wanex505/ checkers 5 Discussion and conclusionsIn this section, we reflect on how the use cases discussed in Section 4, which are made possible thanks to the enhanced capabilities of GPT-4, address several research gaps in GenSE, as well as some long standing issues in SE. We will also discuss how the same use cases open new research directions that can be investigated in future studies.We shall next focus on the research gaps we bridged in GenSE. First, our study addresses the lack of applications of GenAI to the software design sub-task of SE, a literature gap identified in[Huang et al., 2024;Hou et al., 2024]. Second, to the best of our knowledge, our study represents the first application of multimodal GPTs (diagram + prompt) to the software development sub-task of SE, which in GenSE is generally carried out via natural languagebased prompts. We argue that diagram-based prompting may be advantageous, in terms of information transfer efficiency, compared to pure natural language prompting, as diagrams leverage background knowledge associated with their visual elements and structure, thereby leading to compression[Li et al., 2024b]. Third, by reverse engineering code into diagrams, we have contributed to the code summarisation sub-task of SE, which is again predominantly carried out via natural language summaries. Fourth, while there exist studies that investigated the production of PlantUML code from hand-drawn diagrams[Conrardy and Cabot, 2024;  Wang et al., 2024], we believe this is the first study in which we cover multiple elements of the SE stack, from conceptual modelling -which includes static and dynamic analysis of the software system -to implementation. Moreover, to the best of our knowledge, this is the first study that focus on diagram-level (rather than method-level or class-level) code generation. Finally, the use of existing (or new) design patterns in the context of Few-Shot prompting for software system design does not seem to have been investigated before in the literature, and hence merits attention in future studies.Next, we shall look at which long standing issues in SE may be addressed by the use cases discussed in our work. In[Baltes and Diehl, 2014], the authors suggested that sketches could supplement often outdated and poorly written documentation, advocating for tools to archive and retrieve these sketches. No such tool exists and poorly documented code</p>
<p>Automatic semantic augmentation of language model prompts (for code summarization). Ahmed , Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, ICSE '24. Kunal Suresh Ahmed, Premkumar Pai, Earl Devanbu, Barr, the IEEE/ACM 46th International Conference on Software Engineering, ICSE '24New York, NY, USAAssociation for Computing Machinery2024. 2024</p>
<p>Armando Solar-Lezama, Emina Torlak, and Abhishek Udupa. Syntax-guided synthesis. Alur, 2013 Formal Methods in Computer-Aided Design. 2013. 2013</p>
<p>Advancing requirements engineering through generative ai: Assessing the role of llms. Arora, Generative AI for Effective Software Development. Anh Nguyen-Duc, Pekka Abrahamsson, Foutse Khomh, ChamSpringer Nature Switzerland2024. 2024</p>
<p>Sketches and diagrams in practice. Diehl Baltes, Sebastian Baltes, Stephan Diehl, Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. the 22nd ACM SIGSOFT International Symposium on Foundations of Software EngineeringACM2014. November 2014SIGSOFT/FSE'14</p>
<p>Manifesto for agile software development. Beck, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M F Balcan, H Lin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc2001. 2001. 2020. 202033Ilya Sutskever, and Dario Amodei</p>
<p>From image to uml: First results of image based uml diagram generation using llms. Cabot Conrardy, Aaron Conrardy, Jordi Cabot, arXiv:2404.113762024. 2024Technical Report</p>
<p>Tat-Seng Chua, and Qing Li. A survey on rag meeting llms: Towards retrievalaugmented large language models. Cámara, arXiv:2308.01861On the assessment of generative ai in modeling tasks: an experience report with chatgpt and uml. Software and Systems Modeling. Long and Short Papers. Jill Burstein, Christy Doran, Thamar Solorio, Minneapolis, MN, USA; Dawei Yin; New York, NY, USA; Richard Helm, Ralph Johnson; Boston, MAAddison Wesley2023. May 2023. 2019. June 2-7, 2019. 2019. 2023. 2023. 2023. July 2023. 2024. October 199422Technical ReportProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD '24. Gamma et al., 1994] Erich Gamma. and John Vlissides. Design patterns</p>
<p>Large language models for software engineering: A systematic literature review. Ai ; Google, A I Google, Gemini, Hou, arXiv:2403.02583ACM Trans. Softw. Eng. Methodol. Kolthoff et al., 2023] Kristian Kolthoff, Christian Bartelt, and Simone Paolo Ponzetto3382025. 2025. 2024. December 2024. 2024. 2024. March 2023Technical ReportData-driven prototyping via natural-language-based gui retrieval. Automated Software Engineering</p>
<p>Guiding large language models via directional stimulus prompting. Li, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2024a. 2024</p>
<p>Understanding is compression. Li, arXiv:2407.077232024b. 2024Technical Report</p>
<p>Generated knowledge prompting for commonsense reasoning. Liu, Annual Meeting of the Association for Computational Linguistics. 2021. 2021</p>
<p>Is your code generated by ChatGPT really correct? Rigorous evaluation of large language models for code generation. Liu, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2024. 2024</p>
<p>Large language models based automatic synthesis of software specifications. Ma, arXiv:2306.05153arXiv:2304.091812023. 2023. 2023. 2023Technical ReportIs AI the better programming partner? Human-Human Pair Programming vs</p>
<p>Automated extraction of conceptual models from user stories via nlp. Openai ; Robeer, arXiv:2303.08774Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USAAddison-Wesley2024. 2024. 2016. September 2016. 2024. 2024. 2006. 2006Technical ReportUsing UML -software engineering with objects and components. Second Edition. Addison Wesley object technology series</p>
<p>Chatgpt prompt patterns for improving code quality, refactoring, requirements elicitation, and software design. ; E R Tufte, Tufte, Vaswani, Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS '22. Anh Nguyen-Duc, Pekka Abrahamsson, Foutse Khomh, the 36th International Conference on Neural Information Processing Systems, NIPS '22Los Alamitos, CA, USA; Red Hook, NY, USA; ChamSpringer Nature Switzerland2003. 2003. 2017. 2017. 2023. 2023. 2024. July 2024. 2024. 2024. 2024. 2023. 202330The Eleventh International Conference on Learning Representations</p>
<p>Tree of thoughts: deliberate problem solving with large language models. Yao, Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23. the 37th International Conference on Neural Information Processing Systems, NIPS '23Red Hook, NY, USACurran Associates Inc2024. 2024</p>
<p>Experimenting a new programming practice with llms. Zhang, arXiv:2401.010622024. 2024Technical Report</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1138 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1138</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1138</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-17413151</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1509.04064v1.pdf" target="_blank">Benchmarking for Bayesian Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> In the Bayesian Reinforcement Learning (BRL) setting, agents try to maximise the collected rewards while interacting with their environment while using some prior knowledge that is accessed beforehand. Many BRL algorithms have already been proposed, but the benchmarks used to compare them are only relevant for specific cases. The paper addresses this problem, and provides a new BRL comparison methodology along with the corresponding open source library. In this methodology, a comparison criterion that measures the performance of algorithms on large sets of Markov Decision Processes (MDPs) drawn from some probability distributions is defined. In order to enable the comparison of non-anytime algorithms, our methodology also includes a detailed analysis of the computation time requirement of each algorithm. Our library is released with all source code and documentation: it includes three test problems, each of which has two different prior distributions, and seven state-of-the-art RL algorithms. Finally, our library is illustrated by comparing all the available algorithms and the results are discussed.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1138.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1138.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OPPS-DS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Offline, Prior-based Policy Search (Discrete Strategy space)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An offline prior-based policy search that formulates selection among a discrete set of exploration/exploitation (E/E) strategies as a multi-armed bandit (UCB1) problem, using posterior rollouts on sampled MDPs to evaluate candidate strategies and selecting the best for online use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning exploration/exploitation strategies for single trajectory Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>OPPS-DS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Offline policy-search agent: enumerates an (index-based) discrete strategy space F_n of formula-based E/E strategies, treats each strategy as an arm of a k-armed bandit, evaluates arms by sampling MDPs from the prior and simulating one trajectory per draw, and uses UCB1 to allocate evaluation budget β to identify a top strategy; at runtime it executes the chosen strategy and can update it online with observed transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Multi-armed bandit (UCB1) driven offline experimental design / policy search</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by allocating offline evaluation budget β across candidate E/E strategies using UCB1: repeatedly sample an MDP from the prior, simulate each candidate strategy on that MDP to obtain returns, update empirical means and UCB indices, and concentrate evaluation on promising strategies; the selected strategy is then used online (with small online updates).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Generalised Chain (GC), Generalised Double-Loop (GDL), Grid</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transition function (drawn from FDM Dirichlet priors), fully observable discrete-state MDPs; stochastic transitions; deterministic bounded rewards; episodic with truncated horizon for discounting approximation; prior may be accurate or inaccurate.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>GC: 5 states × 3 actions; GDL: 9 states × 2 actions; Grid: 25 states × 4 actions; horizon truncated to guarantee approximation error ≤ 0.01 (T computed from γ and R_max); experiments use many sampled MDPs (N) and one trajectory per sampled MDP for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Accurate priors: GDL 3.10 ± 0.07; Grid 7.03 ± 0.30 (best or among best when allowed required offline compute). Inaccurate priors: GDL 2.99 ± 0.08; Grid 1.09 ± 0.17 (best overall in inaccurate case on two of three tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random baseline: GDL 2.79 ± 0.07 (accurate), Grid 0.22 ± 0.06 (accurate); epsilon-Greedy sometimes close in specific tasks but overall OPPS-DS outperforms baselines when given offline budget.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Requires offline evaluation budget β (tested β ∈ {50,...,1e6}); performance increases rapidly with modest offline time (authors report poor performance after a few seconds but strong improvement after ~1 minute of offline computation); online cost is small compared with some planning methods.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Separation of concerns: exploration strategy chosen offline by evaluating candidate E/E strategies against prior; online behavior follows chosen strategy which encodes its E/E tradeoff (index-based formula); UCB1 ensures adaptive allocation of offline experiments to discover high-performing strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against Random, ε-Greedy, Soft-max, BAMCP, BFS3, SBOSS, BEB.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>OPPS-DS consistently appears among the best algorithms across experiments, especially when sufficient offline computation is available; it dominated other algorithms in the inaccurate-prior case (best in 2/3 tasks) and matched or exceeded others in accurate-prior settings when allowed its minimal offline compute budget.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires non-negligible offline computation budget to reach top performance (very poor if only a few seconds are available); quality limited by the chosen discrete strategy space F_n (no guarantee that best strategy exists in the considered space); variability across strategy complexity (more complex formulas increase online cost slightly).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking for Bayesian Reinforcement Learning', 'publication_date_yy_mm': '2016-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1138.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1138.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BAMCP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayes-Adaptive Monte Carlo Planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sample-based planning algorithm that adapts online by performing Monte Carlo Tree Search (UCT-like) over the belief-augmented MDP using models sampled from the posterior, with sparse sampling to make planning tractable and theoretical convergence to Bayes optimality as samples grow.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient Bayes-adaptive Reinforcement Learning using sample-based search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BAMCP</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Online planning agent: builds a Monte-Carlo Tree Search over belief-augmented states by sampling models from the posterior and performing UCT-style tree search; uses parameters K (nodes per step) and depth (max tree depth) to control computational effort and applies sparse sampling (sample a model at root and reuse it down the tree) to reduce cost.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Adaptive planning via Monte Carlo Tree Search with posterior model sampling (UCT-like), i.e., adaptive experimental design by targeted simulation of future trajectories under sampled models.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each decision, samples models from current posterior and grows an MCTS tree guided by UCT selection/values; rollout and value estimates use sampled models to concentrate computation on belief-relevant branches; increases planning depth/number of nodes adaptively (via parameters) to focus simulation where it matters.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Generalised Chain (GC), Generalised Double-Loop (GDL), Grid</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown transition dynamics (FDM Dirichlet priors), discrete fully-observable MDPs, stochastic transitions, deterministic rewards; tested with both accurate and inaccurate priors.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same benchmarks: GC (5 states, 3 actions), GDL (9×2), Grid (25×4); computational budget controlled by K (tested up to 25k) and depth (15/25/50).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Accurate priors: GDL 3.11 ± 0.07; Grid 6.43 ± 0.30 (very good on Grid with high online budget); Inaccurate priors: GDL 2.85 ± 0.07; Grid 0.51 ± 0.09 (weaker in Grid under inaccurate prior). Performance depends strongly on online compute (K, depth).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baselines: Random and simple policies perform worse (e.g. Random Grid 0.22 ± 0.06 accurate), but BAMCP can outperform baselines when given large online computation. No explicit ablation without posterior sampling is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Sample efficiency is governed by number of simulated tree nodes (K) and depth; higher K/depth improve performance but at higher per-decision cost; authors report BAMCP behaved poorly with low compute in some tasks but achieved top scores in others when given large online compute.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implicitly balanced by planning under the posterior: sampling from posterior captures epistemic uncertainty and MCTS planning chooses actions that trade off potential long-term reward under sampled models — no explicit exploration bonus but exploration arises from planning over uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to OPPS-DS, BFS3, SBOSS, BEB, ε-Greedy, Soft-max, Random.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BAMCP achieved very strong performance in some tasks (notably Grid in the accurate-prior case) when granted substantial online computation; performance is highly sensitive to available online computational budget (K, depth); converges toward Bayes-optimal given enough samples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>High online computation cost required for top performance; can perform poorly with small online budgets; sensitive to prior accuracy (performance degraded in some inaccurate-prior Grid cases).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking for Bayesian Reinforcement Learning', 'publication_date_yy_mm': '2016-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1138.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1138.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BFS3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Forward Search Sparse Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An online planning algorithm that applies Forward Search Sparse Sampling (FSSS) to the belief-augmented MDP by sampling models from the posterior and using upper/lower bounds to prune search, with guarantees of convergence to Bayes optimality as samples increase.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Approaching Bayes-optimality using Monte-Carlo tree search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BFS3</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Online planning agent: samples a model from the posterior and uses a sparse sampling forward-search (FSSS-style) procedure with maintained value bounds (upper/lower) to guide and prune search; parameters include number of nodes K, branching factor C and depth to control computational effort.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Adaptive forward-search planning with posterior model sampling and bound-based pruning (sparse sampling / FSSS applied to belief MDP).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Adapts by sampling models from the posterior at search time and using computed value bounds to prune low-value branches and concentrate search effort on promising actions and trajectories; parameters tune how much compute is devoted, and more samples deepen/expand search selectively.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Generalised Chain (GC), Generalised Double-Loop (GDL), Grid</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown stochastic transitions (FDM Dirichlet), discrete fully-observable MDPs, deterministic bounded rewards; environments vary in size from 5 to 25 states.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>GC 5×3, GDL 9×2, Grid 25×4; parameters tested: K up to 10000, C (branching) up to 15, depth up to 50.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Accurate priors: GDL 2.90 ± 0.07; Grid 3.46 ± 0.23 (moderate performance); Inaccurate priors: GDL 2.85 ± 0.07; Grid 0.42 ± 0.09. Performance sometimes improved with high compute but generally less competitive than BAMCP/OPPS in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random baselines as above; BFS3 outperforms random but is typically below best methods unless given favorable compute/time trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Performance increases with samples (K, branching C, depth), but BFS3 required substantial compute in some tasks to approach top methods; authors note convergence guarantees as sampling increases.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Treated via forward planning under sampled models: search assesses long-term value and prunes branches that are unlikely to be rewarding under posterior uncertainty, thereby implicitly balancing exploration and exploitation via planned rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to OPPS-DS, BAMCP, SBOSS, BEB, ε-Greedy, Soft-max, Random.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BFS3 can converge toward Bayes-optimality with sufficient samples and performed well in some settings, but overall was less competitive than OPPS-DS and BAMCP in the reported benchmarks, particularly with constrained online budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Needs substantial online computation to reach competitive performance; in experiments BFS3 was only top in limited cases and often overtaken by BAMCP or BEB depending on the task and time budget.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking for Bayesian Reinforcement Learning', 'publication_date_yy_mm': '2016-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1138.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1138.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SBOSS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Smarter Best Of Sampled Set</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian RL algorithm that adaptively chooses how many models to sample from the posterior based on Dirichlet-model uncertainty bounds, builds a merged MDP from sampled models, and executes the optimal policy of that merged MDP, dynamically reducing sampling frequency as transitions become well observed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Smarter sampling in model-based bayesian reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SBOSS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-sampling agent: assumes Dirichlet-distributed transitions, derives uncertainty bounds on state-action values, dynamically decides the number of posterior model samples K_t and resampling frequency δ based on uncertainty thresholds, merges sampled models into an aggregated MDP and computes its optimal policy for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Adaptive model sampling driven by uncertainty estimates (count/variance-based adaptive sampling) and merged-model planning.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Monitors posterior uncertainty per (state,action); when uncertainty ∆(x,u) exceeds threshold, decides how many transition vectors to sample and whether to resample; reduces sampling and planning as transitions are observed enough — adaptively focuses sampling where it's most uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Generalised Chain (GC), Generalised Double-Loop (GDL), Grid</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown stochastic transitions with Dirichlet priors (FDM), discrete fully observable MDPs, deterministic rewards; tested with accurate and inaccurate priors.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Benchmarks as above (GC 5×3, GDL 9×2, Grid 25×4); parameters include sampling parameter and δ controlling resampling frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Accurate priors: GDL 2.81 ± 0.10; Grid 4.5 ± 0.33; Inaccurate priors: GDL 2.86 ± 0.07; Grid 0.29 ± 0.07. SBOSS excelled as a low online-time algorithm and improved relative performance in the inaccurate-prior case, sometimes surpassing more computation-heavy planners.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random baseline and simple policies generally worse (e.g., Random Grid 0.22 ±0.06); SBOSS typically outperforms random with minimal online compute but trails best-performing methods when those have sufficient compute.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Designed to be sample-efficient in low-compute regimes by adaptively focusing sampling on uncertain (state,action) pairs; authors report SBOSS is among first algorithms to appear in top rankings when online time bounds are small.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Exploration is encouraged by sampling models where uncertainty is high; as transitions become observed, SBOSS reduces sampling (exploitation) and reuses computed policies, balancing E/E via uncertainty thresholds and dynamic sampling counts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to OPPS-DS, BAMCP, BFS3, BEB, ε-Greedy, Soft-max, Random.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>SBOSS is a strong choice under tight online-time constraints and in some inaccurate-prior settings; it often appears earliest in top-performance vs time trade-offs because of its low per-decision cost and adaptive sampling schedule.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Never achieved best absolute performance in large-online-budget settings in these experiments; performance depends on assumptions of Dirichlet priors and the heuristics used to set sampling/resampling thresholds (parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking for Bayesian Reinforcement Learning', 'publication_date_yy_mm': '2016-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1138.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1138.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BEB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian Exploration Bonus</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based Bayesian RL algorithm that adds an exploration bonus to the reward proportional to inverse visit counts, solves the mean MDP of the posterior with that bonus, and updates incrementally, providing theoretical convergence guarantees.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Near-Bayesian exploration in polynomial time</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>BEB</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Incremental model-based agent: computes mean MDP from posterior at each step, augments the reward function with a bonus β / c_t(x,u,y) (where c_t is visit count of transition), solves resulting MDP to get policy and executes one step, repeating as posterior updates; β controls exploration strength.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Count-based exploration bonus (intrinsic reward) derived from posterior counts; adaptively increases reward for low-count transitions to encourage informative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses current posterior visit counts c_t for each transition to compute an exploration bonus that decays as transitions are observed; solving the bonus-augmented mean MDP biases action selection towards transitions with high uncertainty/unobserved counts, adapting exploration over time as counts increase.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Generalised Chain (GC), Generalised Double-Loop (GDL), Grid</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown stochastic transitions (FDM Dirichlet), discrete fully-observable MDPs, deterministic bounded rewards; experiments run with accurate and inaccurate priors.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>GC 5×3, GDL 9×2, Grid 25×4; tested β values ∈ {0.25,0.5,1,...,16}; online per-step cost is low to moderate (solve mean MDP each step).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Accurate priors: GDL 3.09 ± 0.07; Grid 6.76 ± 0.30 (competitive, often among top when medium online budget available). Inaccurate priors: GDL 2.88 ± 0.07; Grid 0.29 ± 0.05 (performance dropped in inaccurate priors, especially Grid).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Random baseline and low-compute greedy policies often worse; BEB often matched or outperformed several baselines in accurate-prior medium-budget settings.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Relatively sample-efficient for medium online compute budgets because exploration is directly driven by counts; theoretical polynomial-time guarantees exist for near-Bayesian exploration, but empirical performance sensitive to β and prior accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit explicit tradeoff through bonus β: larger β favors exploration by inflating reward for infrequently observed transitions; bonus decays with counts, shifting to exploitation as transitions become known.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to OPPS-DS, BAMCP, BFS3, SBOSS, ε-Greedy, Soft-max, Random.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>BEB reached performance similar to the best methods (OPPS-DS) in accurate-prior experiments for medium online budgets, but its advantage vanished or reversed in the inaccurate-prior case; sensitivity to prior accuracy and β setting was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance degrades when prior is inaccurate (Grid inaccurate-case poor); choice of β critical (too small → under-exploration, too large → wasted exploration); solves mean MDP each step which costs computation and may be suboptimal when posterior multimodality matters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking for Bayesian Reinforcement Learning', 'publication_date_yy_mm': '2016-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient Bayes-adaptive Reinforcement Learning using sample-based search <em>(Rating: 2)</em></li>
                <li>Smarter sampling in model-based bayesian reinforcement learning <em>(Rating: 2)</em></li>
                <li>A Bayesian sampling approach to exploration in Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Near-Bayesian exploration in polynomial time <em>(Rating: 2)</em></li>
                <li>Learning exploration/exploitation strategies for single trajectory Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Bayes Adaptive Reinforcement Learning versus Off-line Prior-based Policy Search: an Empirical Comparison <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1138",
    "paper_id": "paper-17413151",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "OPPS-DS",
            "name_full": "Offline, Prior-based Policy Search (Discrete Strategy space)",
            "brief_description": "An offline prior-based policy search that formulates selection among a discrete set of exploration/exploitation (E/E) strategies as a multi-armed bandit (UCB1) problem, using posterior rollouts on sampled MDPs to evaluate candidate strategies and selecting the best for online use.",
            "citation_title": "Learning exploration/exploitation strategies for single trajectory Reinforcement Learning",
            "mention_or_use": "use",
            "agent_name": "OPPS-DS",
            "agent_description": "Offline policy-search agent: enumerates an (index-based) discrete strategy space F_n of formula-based E/E strategies, treats each strategy as an arm of a k-armed bandit, evaluates arms by sampling MDPs from the prior and simulating one trajectory per draw, and uses UCB1 to allocate evaluation budget β to identify a top strategy; at runtime it executes the chosen strategy and can update it online with observed transitions.",
            "adaptive_design_method": "Multi-armed bandit (UCB1) driven offline experimental design / policy search",
            "adaptation_strategy_description": "Adapts by allocating offline evaluation budget β across candidate E/E strategies using UCB1: repeatedly sample an MDP from the prior, simulate each candidate strategy on that MDP to obtain returns, update empirical means and UCB indices, and concentrate evaluation on promising strategies; the selected strategy is then used online (with small online updates).",
            "environment_name": "Generalised Chain (GC), Generalised Double-Loop (GDL), Grid",
            "environment_characteristics": "Unknown transition function (drawn from FDM Dirichlet priors), fully observable discrete-state MDPs; stochastic transitions; deterministic bounded rewards; episodic with truncated horizon for discounting approximation; prior may be accurate or inaccurate.",
            "environment_complexity": "GC: 5 states × 3 actions; GDL: 9 states × 2 actions; Grid: 25 states × 4 actions; horizon truncated to guarantee approximation error ≤ 0.01 (T computed from γ and R_max); experiments use many sampled MDPs (N) and one trajectory per sampled MDP for evaluation.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Accurate priors: GDL 3.10 ± 0.07; Grid 7.03 ± 0.30 (best or among best when allowed required offline compute). Inaccurate priors: GDL 2.99 ± 0.08; Grid 1.09 ± 0.17 (best overall in inaccurate case on two of three tasks).",
            "performance_without_adaptation": "Random baseline: GDL 2.79 ± 0.07 (accurate), Grid 0.22 ± 0.06 (accurate); epsilon-Greedy sometimes close in specific tasks but overall OPPS-DS outperforms baselines when given offline budget.",
            "sample_efficiency": "Requires offline evaluation budget β (tested β ∈ {50,...,1e6}); performance increases rapidly with modest offline time (authors report poor performance after a few seconds but strong improvement after ~1 minute of offline computation); online cost is small compared with some planning methods.",
            "exploration_exploitation_tradeoff": "Separation of concerns: exploration strategy chosen offline by evaluating candidate E/E strategies against prior; online behavior follows chosen strategy which encodes its E/E tradeoff (index-based formula); UCB1 ensures adaptive allocation of offline experiments to discover high-performing strategies.",
            "comparison_methods": "Compared against Random, ε-Greedy, Soft-max, BAMCP, BFS3, SBOSS, BEB.",
            "key_results": "OPPS-DS consistently appears among the best algorithms across experiments, especially when sufficient offline computation is available; it dominated other algorithms in the inaccurate-prior case (best in 2/3 tasks) and matched or exceeded others in accurate-prior settings when allowed its minimal offline compute budget.",
            "limitations_or_failures": "Requires non-negligible offline computation budget to reach top performance (very poor if only a few seconds are available); quality limited by the chosen discrete strategy space F_n (no guarantee that best strategy exists in the considered space); variability across strategy complexity (more complex formulas increase online cost slightly).",
            "uuid": "e1138.0",
            "source_info": {
                "paper_title": "Benchmarking for Bayesian Reinforcement Learning",
                "publication_date_yy_mm": "2016-06"
            }
        },
        {
            "name_short": "BAMCP",
            "name_full": "Bayes-Adaptive Monte Carlo Planning",
            "brief_description": "A sample-based planning algorithm that adapts online by performing Monte Carlo Tree Search (UCT-like) over the belief-augmented MDP using models sampled from the posterior, with sparse sampling to make planning tractable and theoretical convergence to Bayes optimality as samples grow.",
            "citation_title": "Efficient Bayes-adaptive Reinforcement Learning using sample-based search",
            "mention_or_use": "use",
            "agent_name": "BAMCP",
            "agent_description": "Online planning agent: builds a Monte-Carlo Tree Search over belief-augmented states by sampling models from the posterior and performing UCT-style tree search; uses parameters K (nodes per step) and depth (max tree depth) to control computational effort and applies sparse sampling (sample a model at root and reuse it down the tree) to reduce cost.",
            "adaptive_design_method": "Adaptive planning via Monte Carlo Tree Search with posterior model sampling (UCT-like), i.e., adaptive experimental design by targeted simulation of future trajectories under sampled models.",
            "adaptation_strategy_description": "At each decision, samples models from current posterior and grows an MCTS tree guided by UCT selection/values; rollout and value estimates use sampled models to concentrate computation on belief-relevant branches; increases planning depth/number of nodes adaptively (via parameters) to focus simulation where it matters.",
            "environment_name": "Generalised Chain (GC), Generalised Double-Loop (GDL), Grid",
            "environment_characteristics": "Unknown transition dynamics (FDM Dirichlet priors), discrete fully-observable MDPs, stochastic transitions, deterministic rewards; tested with both accurate and inaccurate priors.",
            "environment_complexity": "Same benchmarks: GC (5 states, 3 actions), GDL (9×2), Grid (25×4); computational budget controlled by K (tested up to 25k) and depth (15/25/50).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Accurate priors: GDL 3.11 ± 0.07; Grid 6.43 ± 0.30 (very good on Grid with high online budget); Inaccurate priors: GDL 2.85 ± 0.07; Grid 0.51 ± 0.09 (weaker in Grid under inaccurate prior). Performance depends strongly on online compute (K, depth).",
            "performance_without_adaptation": "Baselines: Random and simple policies perform worse (e.g. Random Grid 0.22 ± 0.06 accurate), but BAMCP can outperform baselines when given large online computation. No explicit ablation without posterior sampling is reported.",
            "sample_efficiency": "Sample efficiency is governed by number of simulated tree nodes (K) and depth; higher K/depth improve performance but at higher per-decision cost; authors report BAMCP behaved poorly with low compute in some tasks but achieved top scores in others when given large online compute.",
            "exploration_exploitation_tradeoff": "Implicitly balanced by planning under the posterior: sampling from posterior captures epistemic uncertainty and MCTS planning chooses actions that trade off potential long-term reward under sampled models — no explicit exploration bonus but exploration arises from planning over uncertainty.",
            "comparison_methods": "Compared to OPPS-DS, BFS3, SBOSS, BEB, ε-Greedy, Soft-max, Random.",
            "key_results": "BAMCP achieved very strong performance in some tasks (notably Grid in the accurate-prior case) when granted substantial online computation; performance is highly sensitive to available online computational budget (K, depth); converges toward Bayes-optimal given enough samples.",
            "limitations_or_failures": "High online computation cost required for top performance; can perform poorly with small online budgets; sensitive to prior accuracy (performance degraded in some inaccurate-prior Grid cases).",
            "uuid": "e1138.1",
            "source_info": {
                "paper_title": "Benchmarking for Bayesian Reinforcement Learning",
                "publication_date_yy_mm": "2016-06"
            }
        },
        {
            "name_short": "BFS3",
            "name_full": "Bayesian Forward Search Sparse Sampling",
            "brief_description": "An online planning algorithm that applies Forward Search Sparse Sampling (FSSS) to the belief-augmented MDP by sampling models from the posterior and using upper/lower bounds to prune search, with guarantees of convergence to Bayes optimality as samples increase.",
            "citation_title": "Approaching Bayes-optimality using Monte-Carlo tree search",
            "mention_or_use": "use",
            "agent_name": "BFS3",
            "agent_description": "Online planning agent: samples a model from the posterior and uses a sparse sampling forward-search (FSSS-style) procedure with maintained value bounds (upper/lower) to guide and prune search; parameters include number of nodes K, branching factor C and depth to control computational effort.",
            "adaptive_design_method": "Adaptive forward-search planning with posterior model sampling and bound-based pruning (sparse sampling / FSSS applied to belief MDP).",
            "adaptation_strategy_description": "Adapts by sampling models from the posterior at search time and using computed value bounds to prune low-value branches and concentrate search effort on promising actions and trajectories; parameters tune how much compute is devoted, and more samples deepen/expand search selectively.",
            "environment_name": "Generalised Chain (GC), Generalised Double-Loop (GDL), Grid",
            "environment_characteristics": "Unknown stochastic transitions (FDM Dirichlet), discrete fully-observable MDPs, deterministic bounded rewards; environments vary in size from 5 to 25 states.",
            "environment_complexity": "GC 5×3, GDL 9×2, Grid 25×4; parameters tested: K up to 10000, C (branching) up to 15, depth up to 50.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Accurate priors: GDL 2.90 ± 0.07; Grid 3.46 ± 0.23 (moderate performance); Inaccurate priors: GDL 2.85 ± 0.07; Grid 0.42 ± 0.09. Performance sometimes improved with high compute but generally less competitive than BAMCP/OPPS in these experiments.",
            "performance_without_adaptation": "Random baselines as above; BFS3 outperforms random but is typically below best methods unless given favorable compute/time trade-offs.",
            "sample_efficiency": "Performance increases with samples (K, branching C, depth), but BFS3 required substantial compute in some tasks to approach top methods; authors note convergence guarantees as sampling increases.",
            "exploration_exploitation_tradeoff": "Treated via forward planning under sampled models: search assesses long-term value and prunes branches that are unlikely to be rewarding under posterior uncertainty, thereby implicitly balancing exploration and exploitation via planned rollouts.",
            "comparison_methods": "Compared to OPPS-DS, BAMCP, SBOSS, BEB, ε-Greedy, Soft-max, Random.",
            "key_results": "BFS3 can converge toward Bayes-optimality with sufficient samples and performed well in some settings, but overall was less competitive than OPPS-DS and BAMCP in the reported benchmarks, particularly with constrained online budgets.",
            "limitations_or_failures": "Needs substantial online computation to reach competitive performance; in experiments BFS3 was only top in limited cases and often overtaken by BAMCP or BEB depending on the task and time budget.",
            "uuid": "e1138.2",
            "source_info": {
                "paper_title": "Benchmarking for Bayesian Reinforcement Learning",
                "publication_date_yy_mm": "2016-06"
            }
        },
        {
            "name_short": "SBOSS",
            "name_full": "Smarter Best Of Sampled Set",
            "brief_description": "A Bayesian RL algorithm that adaptively chooses how many models to sample from the posterior based on Dirichlet-model uncertainty bounds, builds a merged MDP from sampled models, and executes the optimal policy of that merged MDP, dynamically reducing sampling frequency as transitions become well observed.",
            "citation_title": "Smarter sampling in model-based bayesian reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "SBOSS",
            "agent_description": "Model-sampling agent: assumes Dirichlet-distributed transitions, derives uncertainty bounds on state-action values, dynamically decides the number of posterior model samples K_t and resampling frequency δ based on uncertainty thresholds, merges sampled models into an aggregated MDP and computes its optimal policy for action selection.",
            "adaptive_design_method": "Adaptive model sampling driven by uncertainty estimates (count/variance-based adaptive sampling) and merged-model planning.",
            "adaptation_strategy_description": "Monitors posterior uncertainty per (state,action); when uncertainty ∆(x,u) exceeds threshold, decides how many transition vectors to sample and whether to resample; reduces sampling and planning as transitions are observed enough — adaptively focuses sampling where it's most uncertain.",
            "environment_name": "Generalised Chain (GC), Generalised Double-Loop (GDL), Grid",
            "environment_characteristics": "Unknown stochastic transitions with Dirichlet priors (FDM), discrete fully observable MDPs, deterministic rewards; tested with accurate and inaccurate priors.",
            "environment_complexity": "Benchmarks as above (GC 5×3, GDL 9×2, Grid 25×4); parameters include sampling parameter and δ controlling resampling frequency.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Accurate priors: GDL 2.81 ± 0.10; Grid 4.5 ± 0.33; Inaccurate priors: GDL 2.86 ± 0.07; Grid 0.29 ± 0.07. SBOSS excelled as a low online-time algorithm and improved relative performance in the inaccurate-prior case, sometimes surpassing more computation-heavy planners.",
            "performance_without_adaptation": "Random baseline and simple policies generally worse (e.g., Random Grid 0.22 ±0.06); SBOSS typically outperforms random with minimal online compute but trails best-performing methods when those have sufficient compute.",
            "sample_efficiency": "Designed to be sample-efficient in low-compute regimes by adaptively focusing sampling on uncertain (state,action) pairs; authors report SBOSS is among first algorithms to appear in top rankings when online time bounds are small.",
            "exploration_exploitation_tradeoff": "Exploration is encouraged by sampling models where uncertainty is high; as transitions become observed, SBOSS reduces sampling (exploitation) and reuses computed policies, balancing E/E via uncertainty thresholds and dynamic sampling counts.",
            "comparison_methods": "Compared to OPPS-DS, BAMCP, BFS3, BEB, ε-Greedy, Soft-max, Random.",
            "key_results": "SBOSS is a strong choice under tight online-time constraints and in some inaccurate-prior settings; it often appears earliest in top-performance vs time trade-offs because of its low per-decision cost and adaptive sampling schedule.",
            "limitations_or_failures": "Never achieved best absolute performance in large-online-budget settings in these experiments; performance depends on assumptions of Dirichlet priors and the heuristics used to set sampling/resampling thresholds (parameters).",
            "uuid": "e1138.3",
            "source_info": {
                "paper_title": "Benchmarking for Bayesian Reinforcement Learning",
                "publication_date_yy_mm": "2016-06"
            }
        },
        {
            "name_short": "BEB",
            "name_full": "Bayesian Exploration Bonus",
            "brief_description": "A model-based Bayesian RL algorithm that adds an exploration bonus to the reward proportional to inverse visit counts, solves the mean MDP of the posterior with that bonus, and updates incrementally, providing theoretical convergence guarantees.",
            "citation_title": "Near-Bayesian exploration in polynomial time",
            "mention_or_use": "use",
            "agent_name": "BEB",
            "agent_description": "Incremental model-based agent: computes mean MDP from posterior at each step, augments the reward function with a bonus β / c_t(x,u,y) (where c_t is visit count of transition), solves resulting MDP to get policy and executes one step, repeating as posterior updates; β controls exploration strength.",
            "adaptive_design_method": "Count-based exploration bonus (intrinsic reward) derived from posterior counts; adaptively increases reward for low-count transitions to encourage informative experiments.",
            "adaptation_strategy_description": "Uses current posterior visit counts c_t for each transition to compute an exploration bonus that decays as transitions are observed; solving the bonus-augmented mean MDP biases action selection towards transitions with high uncertainty/unobserved counts, adapting exploration over time as counts increase.",
            "environment_name": "Generalised Chain (GC), Generalised Double-Loop (GDL), Grid",
            "environment_characteristics": "Unknown stochastic transitions (FDM Dirichlet), discrete fully-observable MDPs, deterministic bounded rewards; experiments run with accurate and inaccurate priors.",
            "environment_complexity": "GC 5×3, GDL 9×2, Grid 25×4; tested β values ∈ {0.25,0.5,1,...,16}; online per-step cost is low to moderate (solve mean MDP each step).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Accurate priors: GDL 3.09 ± 0.07; Grid 6.76 ± 0.30 (competitive, often among top when medium online budget available). Inaccurate priors: GDL 2.88 ± 0.07; Grid 0.29 ± 0.05 (performance dropped in inaccurate priors, especially Grid).",
            "performance_without_adaptation": "Random baseline and low-compute greedy policies often worse; BEB often matched or outperformed several baselines in accurate-prior medium-budget settings.",
            "sample_efficiency": "Relatively sample-efficient for medium online compute budgets because exploration is directly driven by counts; theoretical polynomial-time guarantees exist for near-Bayesian exploration, but empirical performance sensitive to β and prior accuracy.",
            "exploration_exploitation_tradeoff": "Explicit explicit tradeoff through bonus β: larger β favors exploration by inflating reward for infrequently observed transitions; bonus decays with counts, shifting to exploitation as transitions become known.",
            "comparison_methods": "Compared to OPPS-DS, BAMCP, BFS3, SBOSS, ε-Greedy, Soft-max, Random.",
            "key_results": "BEB reached performance similar to the best methods (OPPS-DS) in accurate-prior experiments for medium online budgets, but its advantage vanished or reversed in the inaccurate-prior case; sensitivity to prior accuracy and β setting was reported.",
            "limitations_or_failures": "Performance degrades when prior is inaccurate (Grid inaccurate-case poor); choice of β critical (too small → under-exploration, too large → wasted exploration); solves mean MDP each step which costs computation and may be suboptimal when posterior multimodality matters.",
            "uuid": "e1138.4",
            "source_info": {
                "paper_title": "Benchmarking for Bayesian Reinforcement Learning",
                "publication_date_yy_mm": "2016-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient Bayes-adaptive Reinforcement Learning using sample-based search",
            "rating": 2,
            "sanitized_title": "efficient_bayesadaptive_reinforcement_learning_using_samplebased_search"
        },
        {
            "paper_title": "Smarter sampling in model-based bayesian reinforcement learning",
            "rating": 2,
            "sanitized_title": "smarter_sampling_in_modelbased_bayesian_reinforcement_learning"
        },
        {
            "paper_title": "A Bayesian sampling approach to exploration in Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "a_bayesian_sampling_approach_to_exploration_in_reinforcement_learning"
        },
        {
            "paper_title": "Near-Bayesian exploration in polynomial time",
            "rating": 2,
            "sanitized_title": "nearbayesian_exploration_in_polynomial_time"
        },
        {
            "paper_title": "Learning exploration/exploitation strategies for single trajectory Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "learning_explorationexploitation_strategies_for_single_trajectory_reinforcement_learning"
        },
        {
            "paper_title": "Bayes Adaptive Reinforcement Learning versus Off-line Prior-based Policy Search: an Empirical Comparison",
            "rating": 1,
            "sanitized_title": "bayes_adaptive_reinforcement_learning_versus_offline_priorbased_policy_search_an_empirical_comparison"
        }
    ],
    "cost": 0.017070000000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Benchmarking for Bayesian Reinforcement Learning Benchmarking for Bayesian Reinforcement Learning
14 Sep 2015</p>
<p>Michaël Castronovo m.castronovo@ulg.ac.be 
Montefiore Institute University of Liège
B-4000LiègeBelgium</p>
<p>Damien Ernst dernst@ulg.ac.be 
Montefiore Institute University of Liège
B-4000LiègeBelgium</p>
<p>Adrien Couëtoux acouetoux@ulg.ac.be 
Montefiore Institute University of Liège
B-4000LiègeBelgium</p>
<p>Raphaël Fonteneau raphael.fonteneau@ulg.ac.be 
Montefiore Institute University of Liège
B-4000LiègeBelgium</p>
<p>Benchmarking for Bayesian Reinforcement Learning Benchmarking for Bayesian Reinforcement Learning
14 Sep 201574644FF2B86FDDEB9B87D31DFBADC182arXiv:1509.04064v1[cs.AI]Bayesian Reinforcement LearningBenchmarkingBBRL libraryOffline LearningReinforcement Learning
In the Bayesian Reinforcement Learning (BRL) setting, agents try to maximise the collected rewards while interacting with their environment while using some prior knowledge that is accessed beforehand.Many BRL algorithms have already been proposed, but even though a few toy examples exist in the literature, there are still no extensive or rigorous benchmarks to compare them.The paper addresses this problem, and provides a new BRL comparison methodology along with the corresponding open source library.In this methodology, a comparison criterion that measures the performance of algorithms on large sets of Markov Decision Processes (MDPs) drawn from some probability distributions is defined.In order to enable the comparison of non-anytime algorithms, our methodology also includes a detailed analysis of the computation time requirement of each algorithm.Our library is released with all source code and documentation: it includes three test problems, each of which has two different prior distributions, and seven state-of-the-art RL algorithms.Finally, our library is illustrated by comparing all the available algorithms and the results are discussed.</p>
<p>Introduction</p>
<p>Reinforcement Learning (RL) agents aim to maximise collected rewards by interacting over a certain period of time in initially unknown environments.Actions that yield the highest performance according to the current knowledge of the environment and those that maximise the gathering of new knowledge on the environment may not be the same.This is the dilemma known as Exploration/Exploitation (E/E).In such a context, using prior knowledge of the environment is extremely valuable, since it can help guide the decision-making process in order to reduce the time spent on exploration.Model-based Bayesian Reinforcement Learning (BRL) (Dearden et al. (1999); Strens (2000)) specifically targets RL problems for which such a prior knowledge is encoded in the form of a probability distribution (the "prior") over possible models of the environment.As the agent interacts with the actual model, this probability distribution is updated according to the Bayes rule into what is known as "posterior distribution".The BRL process may be divided into two learning phases: the offline learning phase refers to the phase when the prior knowledge is used to warm-up the agent for its future interactions with the real model.The online learning phase, on the other hand, refers to the actual interactions between the agent and the model.In many applications, interacting with the actual environment may be very costly (e.g.medical experiments).In such cases, the experiments made during the online learning phase are likely to be much more expensive than those performed during the offline learning phase.</p>
<p>In this paper, we investigate how the way BRL algorithms use the offline learning phase may impact online performances.To properly compare Bayesian algorithms, the first comprehensive BRL benchmarking protocol is designed, following the foundations of Castronovo et al. (2014)."Comprehensive BRL benchmark" refers to a tool which assesses the performance of BRL algorithms over a large set of problems that are actually drawn according to a prior distribution.In previous papers addressing BRL, authors usually validate their algorithm by testing it on a few test problems, defined by a small set of predefined MDPs.For instance, BAMCP (Guez et al. (2012)), SBOSS (Castro and Precup (2010)), and BFS3 (Asmuth and Littman (2011)) are all validated on a fixed number of MDPs.In their validation process, the authors select a few BRL tasks, for which they choose one arbitrary transition function, which defines the corresponding MDP.Then, they define one prior distribution compliant with the transition function.This type of benchmarking is problematic in the sense that the authors actually know the hidden transition function of each test case.It also creates an implicit incentive to over-fit their approach to a few specific transition functions, which should be completely unknown before interacting with the model.In this paper, we compare BRL algorithms in several different tasks.In each task, the real transition function is defined using a random distribution, instead of being arbitrarily fixed.Each algorithm is thus tested on an infinitely large number of MDPs, for each test case.To perform our experiments, we developed the BBRL library, whose objective is to also provide other researchers with our benchmarking tool.This paper is organised as follows: Section 2 presents the problem statement.Section 3 formally defines the experimental protocol designed for this paper.Section 4 briefly presents the library.Section 5 shows a detailed application of our protocol, comparing several wellknow BRL algorithms on three different benchmarks.Section 6 concludes the study.</p>
<p>Problem Statement</p>
<p>This section is dedicated to the formalisation of the different tools and concepts discussed in this paper.</p>
<p>Reinforcement Learning</p>
<p>Let M = (X, U, f (•), ρ M , p M,0 (•), γ) be a given unknown MDP, where X = {x (1) , . . ., x (n X ) } denotes its finite state space and U = {u (1) , . . ., u (n U ) } refers to its finite action space.When the MDP is in state x t at time t and action u t is selected, the agent moves instantaneously to a next state x t+1 with a probability of P (x t+1 |x t , u t ) = f (x t , u t , x t+1 ).An instantaneous deterministic, bounded reward r t = ρ M (x t , u t , x t+1 ) ∈ [R min , R max ] is observed.</p>
<p>Let h t = (x 0 , u 0 , r 0 , x 1 , • • • , x t−1 , u t−1 , r t−1 , x t ) ∈ H denote the history observed until time t.An E/E strategy is a stochastic policy π which, given the current state x t , returns an action u t ∼ π(h t ).Given a probability distribution over initial states p M,0 (•), the expected return of a given E/E strategy π with respect to the MDP M can be defined as follows:
J π M = E x 0 ∼p M,0 (•) [R π M (x 0 )],
where R π M (x 0 ) is the stochastic sum of discounted rewards received when applying the policy π, starting from an initial state x 0 :
R π M (x 0 ) = +∞ t=0 γ t r t .
RL aims to learn the behaviour that maximises J π M , i.e. learning a policy π * defined as follows:
π * ∈ arg max π J π M .</p>
<p>Prior Knowledge</p>
<p>In this paper, the actual MDP is assumed to be initially unknown.Model-based Bayesian Reinforcement Learning (BRL) proposes to the model the uncertainty, using a probability distribution p 0 M (•) over a set of candidate MDPs M. Such a probability distribution is called a prior distribution and can be used to encode specific prior knowledge available before interaction.Given a prior distribution p 0 M (•), the expected return of a given E/E strategy π is defined as:
J π p 0 M (•) = E M ∼p 0 M (•) [J π M ] ,
In the BRL framework, the goal is to maximise
J π p 0 M (•)
, by finding π * , which is called "Bayesian optimal policy" and defined as follows:
π * ∈ arg max π J π p 0 M (•) .</p>
<p>Computation time characterisation</p>
<p>Most BRL algorithms rely on some properties which, given sufficient computation time, ensure that their agents will converge to an optimal behaviour.However, it is not clear to know beforehand whether an algorithm will satisfy fixed computation time constraints while providing good performances.</p>
<p>The parameterisation of the algorithms makes the selection even more complex.Most BRL algorithms depend on parameters (number of transitions simulated at each iteration, etc.) which, in some way, can affect the computation time.In addition, for one given algorithm and fixed parameters, the computation time often varies from one simulation to another.These features make it nearly impossible to compare BRL algorithms under strict computation time constraints.In this paper, to address this problem, algorithms are run with multiple choices of parameters, and we analyse their time performance a posteriori.</p>
<p>Furthermore, a distinction between the offline and online computation time is made.Offline computation time corresponds to the moment when the agent is able to exploit its prior knowledge, but cannot interact with the MDP yet.One can see it as the time given to take the first decision.In most algorithms concerned in this paper, this phase is generally used to initialise some data structure.On the other hand, online computation time corresponds to the time consumed by an algorithm for taking each decision.</p>
<p>There are many ways to characterise algorithms based on their computation time.One can compare them based on the average time needed per step or on the offline computation time alone.To remain flexible, for each run of each algorithm, we store its computation times (B i ) −1≤i , with i indexing the time step, and B −1 the offline learning time.Then a feature function φ((B i ) −1≤i ) is extracted from this data.This function is used as a metric to characterise and discriminate algorithms based on their time requirements.</p>
<p>In our protocol, which is detailed in the next section, two types of characterisation are used.For a set of experiments, algorithms are classified based on their offline computation time only, i.e. we use φ((B i ) −1≤i ) = B −1 .Afterwards, the constraint is defined as φ((B i ) −1≤i ) ≤ K, K &gt; 0 in case it is required to only compare the algorithms that have an offline computation time lower than K.</p>
<p>For another set of experiments, algorithms are separated according to their empirical average online computation time.In this case, φ((B i ) −1≤i ) = 1 n 0≤i<n B i .Algorithms can then be classified based on whether or not they respect the constraint φ(
(B i ) −1≤i ) ≤ K, K > 0.
This formalisation could be used for any other computation time characterisation.For example, one could want to analyse algorithms based on the longest computation time of a trajectory, and define φ((B i ) −1≤i ) = max −1≤i B i .</p>
<p>A new Bayesian Reinforcement Learning benchmark protocol</p>
<p>A comparison criterion for BRL</p>
<p>In this paper, a real Bayesian evaluation is proposed, in the sense that the different algorithms are compared on a large set of problems drawn according to a test probability distribution.This is in contrast with the Bayesian literature (Guez et al. (2012); Castro and Precup (2010); Asmuth and Littman (2011)), where authors pick a fixed number of MDPs on which they evaluate their algorithm.</p>
<p>Our criterion to compare algorithms is to measure their average rewards against a given random distribution of MDPs, using another distribution of MDPs as a prior knowledge.In our experimental protocol, an experiment is defined by a prior distribution p 0 M (•) and a test distribution p M (•).Both are random distributions over the set of possible MDPs, not stochastic transition functions.To illustrate the difference, let us take an example.Let (x, u, x ) be a transition.Given a transition function f : X × U × X → [0; 1], f (x, u, x ) is the probability of observing x if we chose u in x.In this paper, this function f is assumed to be the only unknown part of the MDP that the agent faces.Given a certain test case, f corresponds to a unique MDP M ∈ M. A Bayesian learning problem is then defined by a probability distribution over a set M of possible MDPs.We call it a test distribution, and denote it p M (•).Prior knowledge can then be encoded as another distribution over M, and denoted p 0 M (•).We call "accurate" a prior which is identical to the test distribution (p 0 M (•) = p M (•)), and we call "inaccurate" a prior which is different (p 0 M (•) = p M (•)).In previous Bayesian literature, authors select a fixed number of MDPs M 1 , ..., M n , train and test their algorithm on them.Doing so does not guarantee any generalisation capabilities.To solve this problem, a protocol that allows rigorous comparison of BRL algorithms is designed.Training and test data are separated, and can even be generated from different distributions (in what we call the inaccurate case).</p>
<p>More precisely, our protocol can be described as follows: Each algorithm is first trained on the prior distribution.Then, their performances are evaluated by estimating the expectation of the discounted sum of rewards, when they are facing MDPs drawn from the test distribution.Let J π(p 0 M ) p M be this value:
J π(p 0 M ) p M = E M ∼p M J π(p 0 M ) M
, where π(p 0 M ) is the algorithm π trained offline on p 0 M .In our Bayesian RL setting, we want to find the algorithm π * which maximises J
π(p 0 M ) p M
for the p 0 M , p M experiment:
π * ∈ arg max π J π(p 0 M ) p M .
In addition to the performance criterion, we also measure the empirical computation time.In practice, all problems are subject to time constraints.Hence, it is important to take this parameter into account when comparing different algorithms.</p>
<p>The experimental protocol</p>
<p>In practice, we can only sample a finite number of trajectories, and must rely on estimators to compare algorithms.In this section our experimental protocol is described, which is based on our comparison criterion for BRL and provides a detailed computation time analysis.</p>
<p>An experiment is defined by (i) a prior distribution p 0 M and (ii) a test distribution p M .Given these, an agent is evaluated π as follows:</p>
<ol>
<li>Train π offline on p 0 M ., the expected return of agent π trained offline on p 0 M , one trajectory is sampled on the MDP M , and the cumulated return is computed Jπ(p 0</li>
</ol>
<p>Sample
M ) M i = R π(p 0 M ) M (x 0 ).
To estimate this return, each trajectory is truncated after T steps.Therefore, given an MDP M and its initial state x 0 , we observe Rπ(p 0
M ) M (x 0 ), an approximation of R π(p 0 M ) M (x 0 ): Rπ(p 0 M ) M (x 0 ) = T t=0 γ t r t .
If R max denotes the maximal instantaneous reward an agent can receive when interacting with an MDP drawn from p M , then choosing T as guarantees the approximation error is bounded by &gt; 0:
T = log( × (1−γ) Rmax ) log γ .
= 0.01 is set for all experiments, as a compromise between measurement accuracy and computation time.</p>
<p>Finally, to estimate our comparison criterion J
π(p 0 M ) p M
, the empirical average of the algorithm performance is computed over N different MDPs, sampled from p M :
Jπ(p 0 M ) p M = 1 N 0≤i&lt;N Jπ(p 0 M ) M i = 1 N 0≤i&lt;N Rπ(p 0 M ) M i (x 0 )(1)
For each agent π, we retrieve µ π = Jπ M and σ π , the empirical mean and standard deviation of the results observed respectively.This gives us the following statistical confidence interval at 95% for J π M :
J π M ∈ Jπ M − 2σ π N ; Jπ M + 2σ π N .
The values reported in the following figures and tables are estimations of the interval within which J π M is, with probability 0.95.</p>
<p>As introduced in Section 2.3, in our methodology, a function φ of computation times is used to classify algorithms based on their time performance.The choice of φ depends on the type of time constraints that are the most important to the user.In this paper, we reflect this by showing three different ways to choose φ.These three choices lead to three different ways to look at the results and compare algorithms.The first one is to classify algorithms based on their offline computation time, the second one is to classify them based on the algorithms average online computation time.The third is a combination of the first two choices of φ, that we denote
φ of f ((B i ) −1≤i ) = B −1 and φ on ((B i ) −1≤i ) = 1 n 0≤i&lt;n B i . The objective is that for each pair of constraints φ of f ((B i ) −1≤i ) &lt; K 1 and φ on ((B i ) −1≤i ) &lt; K 2 , K 1 , K 2 &gt; 0,
we want to identify the best algorithms that respect these constraints.In order to achieve this: (i) All agents that do not satisfy the constraints are discarded; (ii) for each algorithm, the agent leading to the best performance in average is selected; (iii) we build the list of agents whose performances are not significantly different1 .</p>
<p>The results will help us to identify, for each experiment, the most suitable algorithm(s) depending on the constraints the agents must satisfy.This protocol is an extension of the one presented in Castronovo et al. (2014).</p>
<p>BBRL library</p>
<p>BBRL2 is a C++ open-source library for Bayesian Reinforcement Learning (discrete state/action spaces).This library provides high-level features, while remaining as flexible and documented as possible to address the needs of any researcher of this field.To this end, we developed a complete command-line interface, along with a comprehensive website: https://github.com/mcastron/BBRLBBRL focuses on the core operations required to apply the comparison benchmark presented in this paper.To do a complete experiment with the BBRL library, follow these five steps:</p>
<ol>
<li>We create a test and a prior distribution.Those distributions are represented by Flat Dirichlet Multinomial distributions (FDM), parameterised by a state space X, an action space U , a vector of parameters θ, and reward function ρ.For more information about the FDM distributions, check Section 5.2.
./ BBRL -DDS --m d p _ d i s t r i b _ g e n e r a t i o n \ --name &lt; name &gt; \ --short_name &lt; short name &gt; \ --n_states <n X > --n_actions <n U > \ --ini_state <x 0 > \ --t ra ns it io n_ we ig ht s \ &lt;θ(1)&gt; • • • &lt;θ(n X n U n X )&gt; \ --reward_type " RT_CONSTANT " \ --reward_means \ &lt;ρ(x (1) , u (1) , x (1) )&gt; • • • &lt;ρ(x (n X ) , u (n U ) , x (n X ) )&gt; \ --output &lt; output file &gt; A distribution file is created.</li>
<li>We create an experiment.An experiment is defined by a set of N MDPs, drawn from a test distribution defined in a distribution file, a discount factor γ and a horizon limit T .</li>
</ol>
<p>.
/ BBRL -DDS --new_experiment \ --name &lt; name &gt; \ --mdp_distribution " D i r M u l t i D i s t r i b u t i o n " \ --m d p _ d i s t r i b u t i o n _ f i l e &lt; distribution file &gt; \ --n_mdps <N > --n _ s i m u l a t i o n s _ p e r _ m d p 1 \ --discount_factor &lt;γ &gt; --horizon_limit <T > \ --compress_output \ --output &lt; output file &gt;
An experiment file is created and can be used to conduct the same experiment for several agents.</p>
<ol>
<li>We create an agent.An agent is defined by an algorithm alg, a set of parameters ψ, and a prior distribution defined in a distribution file, on which the created agent will be trained.</li>
</ol>
<p>.
/ BBRL -DDS --offline_learning \ --agent <alg > [ &lt; parameters ψ &gt;]\ --mdp_distribution " D i r M u l t i D i s t r i b u t i o n " \ --m d p _ d i s t r i b u t i o n _ f i l e &lt; distribution file &gt; \ --output &lt; output file &gt;
An agent file is created.The file also stores the computation time observed during the offline training phase.</p>
<p>A result file is created.This file contains a set of all transitions encountered during each trajectory.Additionally, the computation times we observed are also stored in this file.It is often impossible to measure precisely the computation time of a single decision.This is why only the computation time of each trajectory is reported in this file.</p>
<ol>
<li>Our results are exported.After each experiment has been performed, a set of K result files is obtained.We need to provide all agent files and result files to export the data.</li>
</ol>
<p>.
/ BBRL -export --agent <alg (1) > \ --agent_file &lt; agent file #1 &gt; \ --experiment \ --experiment_file &lt; result file #1 &gt; \ ... --agent <alg (K) > \ --agent_file &lt; agent file #K &gt; \ --experiment \ --experiment_file &lt; result file #K &gt;
BBRL will sort the data automatically and produce several files for each experiment.</p>
<p>• A graph comparing offline computation cost w.r.t.performance;</p>
<p>• A graph comparing online computation cost w.r.t.performance;</p>
<p>• A graph where the X-axis represents the offline time bound, while the Y-axis represents the online time bound.A point of the space corresponds to set of bounds.An algorithm is associated to a point of the space if its best agent, satisfying the constraints, is among the best ones when compared to the others;</p>
<p>• A table reporting the results of each agent.</p>
<p>BBRL will also produce a report file in L A T E X gathering the 3 graphs and the table for each experiment.</p>
<p>More than 2.000 commands have to be entered in order to reproduce the results of this paper.We decided to provide several Lua script in order to simplify the process.By completing some configuration files, the user can define the agents, the possible values of their parameters and the experiments to conduct.</p>
<p>local agents = {</p>
<p>--e -Greedy { name = " EGreedyAgent " , params = { { opt = " --epsilon " , values = { 0.0 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 , 0.6 , 0.7 , 0.8 , 0.9 , 1.0 } } } , olOptions = { " --compress_output " } , memory = { ol = "1000 M " , re = "1000 M " } , duration = { ol = "01:00:00" , re = "01:00:00" } } , ... }  Those configuration files are then used by a script called make_scripts.sh,included within the library, whose purpose is to generate four other scripts:
• 0-init.sh
Create the experiment files, and create the formulas sets required by OPPS agents.</p>
<p>• 1-ol.shCreate the agents and train them on the prior distribution(s).</p>
<p>• 2-re.shRun all the experiments.</p>
<p>• 3-export.shGenerate the L A T E X reports.</p>
<p>Due to the high computation power required, we made those scripts compatible with workload managers such as SLURM.In this case, each cluster should provide the same amount of CPU power in order to get consistent time measurements.To sum up, when the configuration files are completed correctly, one can start the whole process by executing the four scripts, and retrieve the results in nice L A T E X reports.</p>
<p>It is worth noting that there is no computation budget given to the agents.This is due to the diversity of the algorithms implemented.No algorithm is "anytime" natively, in the sense that we cannot stop the computation at any time and receive an answer from the agent instantly.Strictly speaking, it is possible to develop an anytime version of some of the algorithms considered in BBRL.However, we made the choice to stay as close as possible to the original algorithms proposed in their respective papers for reasons of fairness.In consequence, although computation time is a central parameter in our problem statement, it is never explicitly given to the agents.We instead let each agent run as long as necessary and analyse the time elapsed afterwards.</p>
<p>Another point which needs to be discussed is the impact of the implementation of an algorithm on the comparison results.For each algorithm, many implementations are possible, some being better than others.Even though we did our best to provide the best possible implementations, BBRL does not compare algorithms but rather the implementations of each algorithms.Note that this issue mainly concerns small problems, since the complexity of the algorithms is preserved.</p>
<p>Illustration</p>
<p>This section presents an illustration of the protocol presented in Section 3. We first describe the algorithms considered for the comparison in Section 5.1, followed by a description of the benchmarks in Section 5.2.Section 5.3 shows and analyses the results obtained.</p>
<p>Compared algorithms</p>
<p>In this section, we present the list of the algorithms considered in this study.The pseudocode of each algorithm can be found in Appendix A. For each algorithm, a list of "reasonable" values is provided to test each of their parameters.When an algorithm has more than one parameter, all possible parameter combinations are tested.</p>
<p>Random</p>
<p>At each time-step t, the action u t is drawn uniformly from U .</p>
<p>-Greedy</p>
<p>The -Greedy agent maintains an approximation of the current MDP and computes, at each time-step, its associated Q-function.The selected action is either selected randomly (with a probability of (1 ≥ ≥ 0), or greedily (with a probability of 1 − ) with respect to the approximated model.</p>
<p>Tested values:</p>
<p>• ∈ {0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0}.</p>
<p>Soft-max</p>
<p>The Soft-max agent maintains an approximation of the current MDP and computes, at each time-step, its associated Q-function.The selected action is selected randomly, where the probability to draw an action u is proportional to Q(x t , u).The temperature parameter τ allows to control the impact of the Q-function on these probabilities (τ → 0 + : greedy selection; τ → +∞: random selection).</p>
<p>Tested values:</p>
<p>• τ ∈ {0.05, 0.10, 0.20, 0.33, 0.50, 1.0, 2.0, 3.0, 5.0, 25.0}.</p>
<p>OPPS</p>
<p>Given a prior distribution p 0 M (.) and an E/E strategy space S (either discrete or continuous), the Offline, Prior-based Policy Search algorithm (OPPS) identifies a strategy π * ∈ S which maximises the expected discounted sum of returns over MDPs drawn from the prior.</p>
<p>The OPPS for Discrete Strategy spaces algorithm (OPPS-DS) (Castronovo et al. (2012(Castronovo et al. ( , 2014))) formalises the strategy selection problem as a k-armed bandit problem, where k = |S|.Pulling an arm amounts to draw an MDP from p 0 M (.), and play the E/E strategy associated to this arm on it for one single trajectory.The discounted sum of returns observed is the return of this arm.This multi-armed bandit problem has been solved by using the UCB1 algorithm (Auer et al. (2002); Audibert et al. (2007)).The time budget is defined by a variable β, corresponding to the total number of draws performed by the UCB1.</p>
<p>The E/E strategies considered by Castronovo et.al are index-based strategies, where the index is generated by evaluating a small formula.A formula is a mathematical expression, combining specific features (Q-functions of different models) by using standard mathematical operators (addition, subtraction, logarithm, etc.).The discrete E/E strategy space is the set of all formulas which can be built by combining at most n features/operators (such a set is denoted by F n ).</p>
<p>OPPS-DS does not come with any guarantee.However, the UCB1 bandit algorithm used to identify the best E/E strategy within the set of strategies provides statistical guarantees that the best E/E strategies are identified with high probability after a certain budget of experiments.However, it is not clear that the best strategy of the E/E strategy space considered yields any high-performance strategy regardless the problem.</p>
<p>Tested values:
• S ∈ {F 2 , F 3 , F 4 , F 5 , F 6 } 3 ,
• β ∈ {50, 500, 1250, 2500, 5000, 10000, 100000, 1000000}.</p>
<p>BAMCP</p>
<p>Bayes-adaptive Monte Carlo Planning (BAMCP) (Guez et al. (2012)) is an evolution of the Upper Confidence Tree (UCT) algorithm (Kocsis and Szepesvári (2006)), where each transition is sampled according to the history of observed transitions.The principle of this algorithm is to adapt the UCT principle for planning in a Bayes-adaptive MDP, also called the belief-augmented MDP, which is an MDP obtained when considering augmented states made of the concatenation of the actual state and the posterior.The BAMCP algorithm is made computationally tractable by using a sparse sampling strategy, which avoids sampling a model from the posterior distribution at every node of the planification tree.Note that the BAMCP also comes with theoretical guarantees of convergence towards Bayesian optimality.</p>
<p>In practice, the BAMCP relies on two parameters: (i) Parameter K which defines the number of nodes created at each time-step, and (ii) Parameter depth which defines the depth of the tree from the root.</p>
<p>Tested values:</p>
<p>• K ∈ {1, 500, 1250, 2500, 5000, 10000, 25000},</p>
<p>• depth ∈ {15, 25, 50}.The Bayesian Forward Search Sparse Sampling (BFS3) (Asmuth and Littman (2011)) is a Bayesian RL algorithm whose principle is to apply the principle of the FSSS (Forward Search Sparse Sampling, see Kearns et al. (2002)) algorithm to belief-augmented MDPs.</p>
<p>It first samples one model from the posterior, which is then used to sample transitions.</p>
<p>The algorithm then relies on lower and upper bounds on the value of each augmented state to prune the search space.The authors also show that BFS3 converges towards Bayesoptimality as the number of samples increases.</p>
<p>In practice, the parameters of BFS3 are used to control how much computational power is allowed.The parameter K defines the number of nodes to develop at each time-step, C defines the branching factor of the tree and depth controls its maximal depth.</p>
<p>Tested values:</p>
<p>• K ∈ {1, 500, 1250, 2500, 5000, 10000},</p>
<p>• C ∈ {2, 5, 10, 15},</p>
<p>• depth ∈ {15, 25, 50}.</p>
<p>SBOSS</p>
<p>The Smarter Best of Sampled Set (SBOSS) (Castro and Precup (2010)) is a Bayesian RL algorithm which relies on the assumption that the model is sampled from a Dirichlet distribution.From this assumption, it derives uncertainty bounds on the value of state action pairs.It then uses those bounds to decide how many models to sample from the posterior, and how often the posterior should be updated in order to reduce the computational cost of Bayesian updates.The sampling technique is then used to build a merged MDP, as in Asmuth et al. (2009), and to derive the corresponding optimal action with respect to that MDP.In practice, the number of sampled models is determined dynamically with a parameter .The re-sampling frequency depends on a parameter δ.</p>
<p>Tested values:
• ∈ {1.0, 1e − 1, 1e − 2, 1e − 3, 1e − 4, 1e − 5, 1e − 6}, • δ ∈ {9, 7, 5, 3, 1, 1e − 1, 1e − 2, 1e − 3n1e − 4, 1e − 5, 1e − 6}.</p>
<p>BEB</p>
<p>The Bayesian Exploration Bonus (BEB) (Kolter and Ng (2009)) is a Bayesian RL algorithm which builds, at each time-step t, the expected MDP given the current posterior.Before solving this MDP, it computes a new reward function ρ
(t) BEB (x, u, y) = ρ M (x, u, y) + β c (t) <x,u,y>
, where c (t) <x,u,y> denotes the number of times transition &lt; x, u, y &gt; has been observed at time-step t.This algorithm solves the mean MDP of the current posterior, in which we replaced ρ M (•, •, •) by ρ (t) BEB (•, •, •), and applies its optimal policy on the current MDP for one step.The bonus β is a parameter controlling the E/E balance.BEB comes with theoretical guarantees of convergence towards Bayesian optimality.</p>
<p>Tested values:</p>
<p>• β ∈ {0.25, 0.5, 1, 1.5, 2, 2.5, 3, 4, 8, 16}.</p>
<p>Computation times variance</p>
<p>Each algorithm has one or more parameters that can affect the number of sampled transitions from a given state, or the length of each simulation.This, in turn, impacts the computation time requirement at each step.Hence, for some algorithms, no choice of parameters can bring the computation time below or over certain values.In other words, each algorithm has its own range of computation time.Note that, for some methods, the computation time is influenced concurrently by several parameters.We present a qualitative description of how computation time varies as a function of parameters in Table 1.</p>
<p>Offline phase duration</p>
<p>Online phase duration Random Almost instantaneous.Almost instantaneous.-Greedy 4 Almost instantaneous.</p>
<p>Varies in inverse proportion to .Can vary a lot from one step to another.OPPS-DS Varies proportionally to β. Varies proportionally to the number of features implied in the selected E/E strategy.BAMCP 5 Almost instantaneous.</p>
<p>Varies proportionally to K and depth.BFS3 6</p>
<p>Almost instantaneous.</p>
<p>Varies proportionally to K, C and depth.SBOSS 7</p>
<p>Almost instantaneous.</p>
<p>Varies in inverse proportion to and δ.Can vary a lot from one step to another, with a general decreasing tendency.</p>
<p>BEB</p>
<p>Almost instantaneous.Constant.</p>
<p>Table 1: Influence of the algorithm and their parameters on the offline and online phases duration.</p>
<ol>
<li>If a random decision is chosen, the model is not solved.5. K defines the number of nodes to develop at each step, and depth defines the maximal depth of the tree.6.K defines the number of nodes to develop at each step, C the branching factor of the tree and depth its maximal depth.7. The number of models sampled is inversely proportional to , while the frequency at which the models are sampled is inversely proportional to δ.When an MDP has been sufficiently explored, the number of models to sample and the frequency of the sampling will decrease.</li>
</ol>
<p>Benchmarks</p>
<p>In our setting, the transition matrix is the only element which differs between two MDPs drawn from the same distribution.For each &lt; state, action &gt; pair &lt; x, u &gt;, we define a Dirichlet distribution, which represents the uncertainty about the transitions occurring from &lt; x, u &gt;.A Dirichlet distribution is parameterised by a set of concentration parameters α</p>
<p>(1)
<x,u> , • • • , α (n X )
<x,u> .We gathered all concentration parameters in a single vector θ.Consequently, our MDP distributions are parameterised by ρ M (the reward function) and several Dirichlet distributions, parameterised by θ.Such a distribution is denoted by p ρ M ,θ (•).In the Bayesian Reinforcement Learning community, these distributions are referred to as Flat Dirichlet Multinomial distributions (FDMs).</p>
<p>We chose to study two different cases:</p>
<p>• Accurate case: the test distribution is fully known (p 0 M (.) = p M (.)),</p>
<p>• Inaccurate case: the test distribution is unknown (p 0 M (.) = p M (.)).</p>
<p>In the inaccurate case, we have no assumption on the transition matrix.We represented this lack of knowledge by a uniform FDM distribution, where each transition has been observed one single time
(θ = [1, • • • , 1]).
Sections 5.2.1, 5.2.2 and 5.2.3 describes the three distributions considered for this study.</p>
<p>Generalised Chain distribution (p ρ GC ,θ GC (•))</p>
<p>The Generalised Chain (GC) distribution is inspired from the five-state chain problem (5 states, 3 actions) (Dearden et al. (1998)).The agent starts at State 1, and has to go through State 2, 3 and 4 in order to reach the last state (State 5), where the best rewards are.The agent has at its disposal 3 actions.An action can either let the agent move from State x (n)  to State x (n+1) or force it to go back to State x (1) .The transition matrix is drawn from a FDM parameterised by θ GC , and the reward function is denoted by ρ GC .More details can be found in Appendix B.1.</p>
<p>Generalised Double-Loop distribution (p ρ GDL ,θ GDL (•))</p>
<p>The Generalised Double-Loop (GDL) distribution is inspired from the double-loop problem (9 states, 2 actions) (Dearden et al. (1998)).Two loops of 5 states are crossing at State 1, where the agent starts.One loop is a trap: if the agent enters it, it has no choice to exit but crossing over all the states composing it.Exiting this loop provides a small reward.The other loop is yielding a good reward.However, each action of this loop can either let the agent move to the next state of the loop or force it to return to State 1 with no reward.The transition matrix is drawn from an FDM parameterised by θ GDL , and the reward function is denoted by ρ GDL .More details can be found in Appendix B.2.</p>
<p>Grid distribution (p ρ
Grid ,θ Grid (•))
The Grid distribution is inspired from the Dearden's maze problem (25 states, 4 actions) (Dearden et al. (1998)).The agent is placed at a corner of a 5x5 grid (the S cell), and has to reach the opposite corner (the G cell).When it succeeds, it returns to its initial state and receives a reward.The agent can perform 4 different actions, corresponding to the 4 directions (up, down, left, right).However, depending on the cell on which the agent is, each action has a certain probability to fail, and can prevent the agent to move in the selected direction.The transition matrix is drawn from an FDM parameterised by θ Grid , and the reward function is denoted by ρ Grid .More details can be found in Appendix B.3.</p>
<p>Discussion of the results</p>
<p>Accurate case</p>
<p>GDL Experiment</p>
<p>Agent Score Random 2.79 ± 0.07 e-Greedy ( = 0.1) 3.05 ± 0.07 Soft-Max (τ = 0.1) 2.79
± 0.1 OPPS-DS (max(Q 0 (x, u), |Q 2 (x, u)|)) 3.1 ± 0.07 BAMCP (K = 10000, depth = 15) 3.11 ± 0.07 BFS3 (K = 1, C = 15, depth = 25) 2.9 ± 0.07 SBOSS ( = 1, δ = 1)
2.81 ± 0.1 BEB (β = 0.5) 3.09 ± 0.07</p>
<p>Grid Experiment</p>
<p>Agent Score Random 0.22 ± 0.06 e-Greedy ( = 0) 6.9 ± 0.31 Soft-Max (τ = 0.05)
0 ± 0 OPPS-DS (Q 0 (x, u) + Q 2 (x, u)) 7.03 ± 0.3 BAMCP (K = 25000, depth = 15) 6.43 ± 0.3 BFS3 (K = 500, C = 15, depth = 50) 3.46 ± 0.23 SBOSS ( = 0.1, δ = 7)
4.5 ± 0.33 BEB (β = 0.5) 6.76 ± 0.3</p>
<p>t Performance</p>
<p>As it can be seen in Figure 6, OPPS is the only algorithm whose offline time cost varies.In the three different settings, OPPS can be launched after a few seconds, but behaves very poorly.However, its performances increased very quickly when given at least one minute of computation time.Algorithms that do not use offline computation time have a wide range of different scores.This variance represents the different possible configurations for these algorithms, which only lead to different online computation time.</p>
<p>On Figure 7, BAMCP, BFS3 and SBOSS have variable online time costs.BAMCP behaved poorly on the first experiment, but obtained the best score on the second one and was pretty efficient on the last one.BFS3 was good only on the second experiment.SBOSS was never able to get a good score in any cases.Note that OPPS online time cost varies slightly depending on the formula's complexity.</p>
<p>If we take a look at the top-right point in Figure 8, which defines the less restrictive bounds, we notice that OPPS-DS and BEB were always the best algorithms in every experiment.-Greedy was a good candidate in the two first experiments.BAMCP was also a very good choice except for the first experiment.On the contrary, BFS3 and SBOSS were only good choices in the first experiment.</p>
<p>If we look closely, we can notice that OPPS-DS was always one of the best algorithm since we have met its minimal offline computation time requirements.</p>
<p>Moreover, when we place our offline-time bound right under OPPS-DS minimal offline time cost, we can see how the top is affected from left to right:</p>
<p>GC:</p>
<p>(Random), (SBOSS), (BEB, -Greedy), (BEB, BFS3, -Greedy), GDL: (Random), (Random, SBOSS), ( -Greedy), (BEB, -Greedy), (BAMCP, BEB, -Greedy),</p>
<p>Grid: (Random), (SBOSS), ( -Greedy), (BEB, -Greedy).</p>
<p>We can clearly see that SBOSS was the first algorithm to appear on the top, with a very small online computation cost, followed by -Greedy and BEB.Beyond a certain online time bound, BFS3 emerged in the first experiment while BAMCP emerged in the second experiment.Neither of them was able to compete with BEB or -Greedy in the last experiment.Soft-max was never able to reach the top regardless the configuration.Figure 9 reports the best score observed for each algorithm, disassociated from any time measure.Note that the variance is very similar for all algorithms in GDL and Grid experiments.On the contrary, the variance oscillates between 1.0 and 2.0.However, OPPS seems to be the less stable algorithm in the three cases.</p>
<p>Inaccurate case</p>
<p>GDL Experiment</p>
<p>Agent Score Random 2.76 ± 0.08 e-Greedy ( = 0.3) 2.88 ± 0.07 Soft-Max (τ = 0.05) 2.76
± 0.1 OPPS-DS (max(Q 0 (x, u), Q 1 (x, u))
2.99 ± 0.08 BAMCP (K= 10000, depth = 50) 2.85 ± 0.07 BFS3 (K = 1250, C = 15, depth = 50) 2.85 ± 0.07 SBOSS ( = 0.1, δ = 0.001) 2.86 ± 0.07 BEB (β = 2.5) 2.88 ± 0.07</p>
<p>Grid Experiment</p>
<p>Agent Score Random 0.23 ± 0.06 e-Greedy ( = 0.2) 0.63 ± 0.09 Soft-Max (τ = 0.05)
0 ± 0 OPPS-DS (Q 1 (x, u) + Q 2 (x, u)))
1.09 ± 0.17 BAMCP (K = 25000, depth = 25) 0.51 ± 0.09 BFS3 (K = 1, C = 15, depth = 50) 0.42 ± 0.09 SBOSS ( = 0.001, δ = 0.1) 0.29 ± 0.07 BEB (β = 0.25) 0.29 ± 0.05</p>
<p>t Performance</p>
<p>As seen in the accurate case, Figure 10 also shows impressive performances for OPPS-DS, which has beaten all other algorithms in every experiment.We can also notice that, as observed in the accurate case, in the Grid experiment, the OPPS-DS agents scores are very close.However, only a few were able to significantly surpass the others, contrary to the accurate case where most OPPS-DS agents were very good candidates.</p>
<p>Surprisingly, SBOSS was a very good alternative to BAMCP and BFS3 in the two first experiments as shown in Figure 11.It was able to surpass both algorithms on the first one while being very close to BAMCP performances in the second.Relative performances of BAMCP and BFS3 remained the same in the inaccurate case, even if the BAMCP advantage is less visible in the second experiment.BEB was no longer able to compete with OPPS-DS and was even beaten by BAMCP and BFS3 in the last experiment.-Greedy was still a decent choice except in the first experiment.As observed in the accurate case, Soft-max was very bad in every case.</p>
<p>In Figure 12, if we take a look at the top-right point, we can see OPPS-DS is the best choice in the second and third experiment.BEB, SBOSS and -Greedy share the first place with OPPS-DS in the first one.</p>
<p>If we place our offline-time bound right under OPPS-DS minimal offline time cost, we can see how the top is affected from left to right:</p>
<p>GC:</p>
<p>(Random), (Random, SBOSS), (SBOSS), (BEB, SBOSS, -Greedy), (BEB, BFS3, SBOSS, -Greedy), GDL: (Random), (Random, SBOSS), (BAMCP, Random, SBOSS), (BEB, SBOSS, -Greedy), (BEB, BFS3, SBOSS, -Greedy), (BAMCP, BEB, BFS3, SBOSS, -Greedy),</p>
<p>Grid: (Random), (Random, SBOSS), (BAMCP, BEB, BFS3, Random, SBOSS), ( -Greedy).</p>
<p>SBOSS is again the first algorithm to appear in the rankings.-Greedy is the only one which could reach the top in every case, even when facing BAMCP and BFS3 fed with high online computation cost.BEB no longer appears to be undeniably better than the others.Besides, the two first experiments show that most algorithms obtained similar results, except for BAMCP which does not appear on the top in the first experiment.In the last experiment, -Greedy succeeded to beat all other algorithms.</p>
<p>Figure 13 does not bring us more information than those we observed in the accurate case.</p>
<p>Summary</p>
<p>In the accurate case, OPPS-DS was always among the best algorithms, at the cost of some offline computation time.When the offline time budget was too constrained for OPPS-DS, different algorithms were suitable depending on the online time budget:</p>
<p>• Low online time budget: SBOSS was the fastest algorithm to make better decisions than a random policy.</p>
<p>• Medium online time budget 8 : BEB reached performances similar to OPPS-DS on each experiment.</p>
<p>• High online time budget 9 : In the first experiment, BFS3 managed to catch up BEB and OPPS-DS when given sufficient time.In the second experiment, it was BAMCP which has achieved this result.Neither BFS3 nor BAMCP was able to compete with BEB and OPPS-DS in the last experiment.</p>
<p>The results obtained in the inaccurate case were very interesting.BEB was not as good as it seemed to be in the accurate case, while SBOSS improved significantly compared to the others.For its part, OPPS-DS obtained the best overall results in the inaccurate case by outperforming all the other algorithms in two out of three experiments while remaining among the best ones in the last experiment.</p>
<p>Conclusion</p>
<p>We have proposed a new extensive BRL comparison methodology which takes into account both performance and time requirements for each algorithm.In particular, our benchmarking protocol shows that no single algorithm dominates all other algorithms on all scenarios.The protocol we introduced can compare any time algorithm to non-anytime algorithms while measuring the impact of inaccurate offline training.By comparing algorithms on large sets of problems, we avoid over fitting to a single problem.Our methodology is associated with an open-source library, BBRL, and we hope that it will help other researchers to design algorithms whose performances are put into perspective with computation times, that may be critical in many applications.This library is specifically designed to handle new algorithms easily, and is provided with a complete and comprehensive documentation website.</p>
<p>Appendix A. Pseudo-code of the algorithms Algorithm 1 -Greedy 1: procedure offline-learning(p 0 M (.))</p>
<p>2:</p>
<p>M ← "Build an initial model based on p 0 M (.)" 3: end procedure {Select an action randomly, with a probability proportional to Q * M (x, u)} 10:
Q * M ← "Compute the optimal Q-function of M " 11: for 1 ≤ i ≤ |U | do 12: if r &lt; j≤i exp Q * M (x,u (j) ) τ u exp Q * M (x,u ) τ then 13:
return u (i)</p>
<p>14:</p>
<p>end if 15:</p>
<p>end for 16: end function {Initialise the k arms of UCB1} 3:
for 1 ≤ i ≤ k do 4: M ∼ p 0 M (.) 5:
R π i M ← "Simulate strategy π i on MDP M over a single trajectory" 6:
µ(i) ← R π i M 7: θ(i) ← 1 8:
end for
u ← arg max u Q( x, h , u) + c ( log(N ( x,h )) N ( x,h ,u ) ) 27: "Sample x ,"Update N ( x, h ), N ( x, h , u), Q( x, h , u)" 33: return R 34: end function Algorithm 5 BAMCP (2/2) 1: procedure rollout( x, h , M, d) 2: if γ d R max &lt;
then {Truncate the trajectory if precision has been reached} {Update the current Q-function} 3:</p>
<p>M mean ← "Compute the mean MDP of p t M (.)."</p>
<p>4:</p>
<p>for all u ∈ U do 5:</p>
<p>for 1 ≤ i ≤ C do 6:</p>
<p>{Draw y and r from the mean MDP of the posterior} 7:</p>
<p>y ∼ P Mmean 8:</p>
<p>r ← ρ M (x, u, y)</p>
<p>9:</p>
<p>10:</p>
<p>{Update the Q-value in (x, u) by using F SSS algorithm} 11: for Similarly to the GC distribution, we can also identify two possibly optimal behaviours:
Q(x, u) ← Q(x, u) + 1 C r + γ FSSS(y,&lt; x, u, y &gt;∈ X × U × X do ρ M (x, u, y) ← ρ M (x, u, y) + β c
• The agent enters the "good" loop and tries to stay in it until the end;</p>
<p>• The agent gives up and chooses to enter the "bad" loop as frequently as possible.</p>
<p>B.2.1 Formal description X = {1, 2, 3, 4, 5, 6, 7, 8, 9}, U = {1, 2} ∀u ∈ U :</p>
<p>θ GDL 1,u = [0, 1, 0, 0, 0, 1, 0, 0, 0] θ GDL 2,u = [0, 0, 1, 0, 0, 0, 0, 0, 0] θ GDL 3,u = [0, 0, 0, 1, 0, 0, 0, 0, 0] θ GDL 4,u = [0, 0, 0, 0, 1, 0, 0, 0, 0] θ GDL 5,u = [1, 0, 0, 0, 0, 0, 0, 0, 0] θ GDL 6,u = [1, 0, 0, 0, 0, 0, 1, 0, 0] θ GDL 7,u = [1, 0, 0, 0, 0, 0, 0, 1, 0] θ GDL 8,u = [1, 0, 0, 0, 0, 0, 0, 0, 1] θ GDL 9,u = [1, 0, 0, 0, 0, 0, 0, 0, 0] ∀u ∈ U :</p>
<p>ρ GDL (5, u, 1) = 1.0 ρ GDL (9, u, 1) = 2.0 ρ GDL (x, u, y) = 0.0, ∀x ∈ X, ∀y ∈ X : y = 1</p>
<p>B.3 Grid distribution</p>
<p>MDPs drawn from the Grid distribution are 2-dimensional grids.Since the agents considered do not manage multi-dimensional state spaces, the following bijection was defined:</p>
<p>{1, 2, 3, 4, 5} × {1, 2, 3, 4, 5} → X = {1, 2, • • • , 25} : n(i, j) = 5(i − 1) + j where i and j are the row and column indexes of the cell on which the agent is.</p>
<p>When the agent reaches the G cell (in (5, 5)), it is directly moved to (1, 1), and will perceive its reward of 10.In consequence, State (5, 5) is not reachable.</p>
<p>To move inside the Grid, the agent can perform four actions: U = {up, down, lef t, right}.Those actions only move the agent to one adjacent cell.However, each action has a certain probability to fail (depending on the cell on which the agent is).In case of failure, the agent does not move at all.Besides, if the agent tries to move out of the grid, it will not move either.Discovering a reliable (and short) path to reach the G cell will determine the success of the agent.(n(i − 1, j)) = 1, (i − 1) ≥ 1 θ Grid n(i,j),down (n(i + 1, j)) = 1, (i + 1) ≤ 5, (i, j) = (4, 5) θ Grid n(i,j),lef t (n(i, j − 1)) = 1, (j − 1) ≥ 1 θ Grid n(i,j),right (n(i, j + 1)) = 1, (j + 1) ≤ 5, (i, j) = (5, 4)</p>
<p>θ Grid n(4,5),down (n(1, 1)) = 1</p>
<p>θ Grid n(5,4),right (n(1, 1)) = 1</p>
<p>θ Grid n(i,j),u</p>
<p>(n(k, l)) = 0, else ρ Grid ((4, 5),down,(1, 1)) = 10.0 ρ Grid ((5, 4),right,(1, 1)) = 10.0 ρ Grid ((i, j), u, (k, l)) = 0.0, ∀u ∈ U</p>
<p>Step 1 -Hypothesis</p>
<p>We compute the mean and the standard deviation of the differences between the two sample sets, denoted by xd and sd , respectively.
xd = 1 N N i=1 R π A M i − R π B M i sd = 1 N N i=1 (x d − (R π A M i − R π B M i )) 2
If N ≥ 30, sd is a good estimation of σ d , the standard deviation of the differences between the two populations (s d ≈ σ d ).In order words, σ d is the standard deviation we should observe when testing the two algorithms on a number of MDPs tending towards infinity.This was always the case in our experiments.We now set Hypothesis H 0 and Hypothesis H α :
H 0 : µ d = 0 H α : µ d &gt; 0
Our goal is to determine if µ d , the mean of the differences between the two populations, is equal or greater than 0.More expressly, we want to know if the differences between the two agents' performances is significant (H α is correct) or not (H 0 correct).Only one of those hypotheses can be true.</p>
<p>Step 2 -Test statistic</p>
<p>The test statistic consists to compute a certain value Z:
Z = xd σ d √ N
This value will help us to determine if we should accept (or reject) hypothesis H α .</p>
<p>Step 3 -Rejection region</p>
<p>Assuming we want our decision to be correct with a probability of failure of α, we will have to compare Z with Z α , a value of a Gaussian curve.If Z &gt; Z α , it means we are in the rejection region (R.R.) with a probability equal to 1 − α.For a confidence of 95%, Z α should be equal to 1.645.</p>
<p>Being in the R.R. means we have to reject Hypothesis H 0 (and accept Hypothesis H α ).In the order case, we have to accept Hypothesis H 0 (and reject Hypothesis H α ).</p>
<p>Figure 1 :
1
Figure 1: Example of a configuration file for the agents.</p>
<p>Figure 2: Example of a configuration file for the experiments.</p>
<ol>
<li>The number of arms k is always equal to the number of strategies in the given set.For your information: |F2| = 12, |F3| = 43, |F4| = 226, |F5| = 1210, |F6| = 7407 5.1.6BFS3</li>
</ol>
<p>Figure 3 :
3
Figure 3: Illustration of the GC distribution.</p>
<p>Figure 4 :
4
Figure 4: Illustration of the GDL distribution.</p>
<p>Figure 5 :
5
Figure 5: Illustration of the Grid distribution.</p>
<p>Figure</p>
<p>Figure 7: Online computation cost Vs.Performance</p>
<p>Figure 9 :
9
Figure 9: Best algorithms w.r.t Performance</p>
<p>Figure</p>
<p>Figure 11: Online computation cost Vs.Performance</p>
<p>Figure 13 :
13
Figure 13: Best algorithms w.r.t Performance</p>
<p>online-learning(x, u, y, r)    19:"Update model M w.r.t.transition &lt; x, u, y, r &gt;"</p>
<p>online-learning(x, u, y, r)    19:"Update model M w.r.t.transition &lt; x, u, y, r &gt;" 20: end procedure Algorithm 3 OPPS-DS 1: procedure offline-learning(p 0 M (.))2:</p>
<p>9 :"
9
for k + 1 ≤ b ≤ β do 12:a ← arg max a µ(a ) + 2 log(b) Simulate strategy π a on MDP M over a single trajectory" search(x, h) 25: return u ∼ π OP P S (x, h) 26: end function 27: 28: procedure online-learning(x, u, y, r) 29: "Update strategy π OP P S w.r.t.transition &lt; x, u, y, r &gt;" {Return the best action w.r.t.Q(., .)}return arg max u Q( x, h , u) 10: end function 11: 12: function simulate( x, h , M, d) 13:if N ( x, h ) = 0 then {New node reached} 14:"Initialise N ( x, h , u), Q( x, h , u)" 15:u ∼ π 0 ( x, h ) the score of this node by using the rollout policy} 19: R ← r + γ Rollout( x , hux , P, d) Update N ( x, h ), N ( x, h , u), Q( x, h , u)"</p>
<p>{Simulate a single transition from M and continue the rollout process} 10:y ∼ P M 11: r ← ρ M (x, u, y) 12:return r + γ rollout( y, huy , M, d + 1) 13: end procedure 14:15: procedure online-learning(x, u, y, r) 16:"Update the posterior w.r.t.transition &lt; x, u, y, r &gt;" 17: end procedure Algorithm 6 BFS3 1: function search(x, h) 2:</p>
<p>the optimal action in x w.r.t.π * M } 12: return u ∼ π * M (x) 13: end procedure 14: 15: procedure online-learning(x, u, y, r) 16:"Update the posterior w.r.t.transition &lt; x, u, y, r &gt;" 17: end procedure• The agent gives up to reach State 5 and tries to return to State 1 as often as possible.B.1.1Formal descriptionX = {1, 2, 3, 4, 5}, U = {1, = [1, 1, 0, 0, 1] ∀x, u ∈ X × U : ρ GC (x, u, 1) = 2.0 ρ GC (x, u, 5) = 10.0 ρ GC (x, u, y) = 0.0, ∀y ∈ X \ {1, 5} B.2 Generalised Double-Loop distribution</p>
<p>B.3.1 Formal descriptionX = {1, 2, • • • , 25}, U = {up, down, lef t, right} ∀(i, j) ∈ {1, 2, 3, 4, 5} × {1, 2, 3, 4, 5} ∀(k, l) ∈ {1, 2, 3, 4, 5} × {1, 2, 3, 4, 5} :</p>
<p>N</p>
<p>MDPs from the test distribution p M .
π(p 0 M ) M To estimate J3. For each sampled MDP M , compute estimateJπ(p 0 M ) Mof J M π(p 0 M ).4. Use these values to compute an estimateJπ(p 0 M ) p M.</p>
<p Return="Return" SBOSS="SBOSS" action="action" in="in" optimal="optimal" the="the" w.r.t.π="w.r.t.π" x="x">d, t)
Algorithm 9 SBOSS (1/2) Algorithm 10 SBOSS (2/2)1: function search(x, h) 1: function fit-action-space(π  *  M # )2: 2:{Compute the transition matrix of the mean MDP of the posterior} for all x ∈ X do3: 3:M mean ← "Compute the mean MDP of p t M (.)." π(x) ← π  *  M # (x) mod |U |4: 4:P t ← P Mmean end for5: 5:6: 6:{Update the policy to follow if necessary} return π7: 7: end function ∀(x, u) : ∆(x, u) = y∈X 8: if t = 1 or ∃(x , u ) : ∆(x , u ) &gt; δ then |Pt(x,u,y)−P lastU pdate (x,u,y)| σ(x,u,y) 8:9: 9: procedure online-learning(x, u, y, r) {Sample some transition vectors for each state-action pair}10: 10:S ← {} "Update the posterior w.r.t. transition &lt; x, u, y, r &gt;"12: 11: 11: end procedure for all (x, u) ∈ X × U do end for 13: 12: {Compute the number of transition vectors to sample for (x, u)} end for 13: σ 2 (x,u,y) Algorithm 11 BEB K t (x, u) ← max y 14: 15: {Return the action u with the maximal Q-value in x} 16: 1: procedure search(x, h) 14: 15: {Sample K t (x, u) transition vectors from &lt; x, u &gt;, sampled from 2: M ← "Compute the mean MDP of p t M (.)." return arg max u Q(x, u) the posterior} 3: 17: end function 16: for 1 ≤ k ≤ K t (x, u) do 4: {Add a bonus reward to all transitions}17: 5:S ← S ∪ "A transition vector from &lt; x, u &gt;, sampled from theposterior"18: Algorithm 7 FSSS (1/2) end for 19: end for1: function FSSS(x, d, t) 20:2: 21:{Develop a MCTS and compute bounds on V (x)} M # ← "Build a new MDP by merging all transitions from S"3: 4: 5: 22: 23: 24:for 1 ≤ i ≤ t do rollout(s, d, 0) π  *  M # ←value-iteration(M # ) π SBOSS ←fit-action-space(π  *  M # ) end for P lastU pdate ← P t6: 25:end if7: 8: 26: 9: 27: 28: 10: end function {Make an optimistic estimation of V (x)} V (x) ← max u U d (x, u) return V (x) return u ∼ π SBOSS (x) 29: end function</p>
<p>. A paired sampled Z-test with a confidence level of 95% has been used to determine when two agents are statistically equivalent (more details in Appendix C).
. BBRL stands for Benchmaring tools for Bayesian Reinforcement Learning.
. We run the experiment. We need to provide an experiment file, an algorithm alg and an agent file../ BBRL -DDS --run_experiment \ --experiment \ --experiment_file &lt; experiment file &gt; \ --agent <alg > \ --agent_file &lt; agent file &gt; \ --n_threads 1 \ --compress_output \ --safe_simulations \ --re fresh_ freque ncy 60 \ --backup_frequency 900 \ --output &lt; output file &gt;
AcknowledgmentsMichaël Castronovo acknowledges the financial support of the FRIA.Raphael Fonteneau is a postdoctoral fellow of the F.R.S.-FNRS (Belgian Funds for Scientifique Research).for all u ∈ U do 9:Appendix B. MDP distributions in detailIn this section, we describe the MDPs drawn from the considered distributions in more detail.In addition, we also provide a formal description of the corresponding θ (parameterising the FDM used to draw the transition matrix) and ρ M (the reward function).B.1 Generalised Chain distributionOn those MDPs, we can identify two possibly optimal behaviours:• The agent tries to move along the chain, reaches the last state, and collect as many rewards as possible before returning to State 1;Appendix C. Paired sampled Z-testLet π A and π B be the two agents we want to compare.We played the two agents on the same N MDPs, denoted byM i be the scores we observed for the two agents on M i .Step 4 -Decision At this point, we have either accepted Hypothesis H 0 or Hypothesis H α .• Accepting Hypothesis H 0 (Z &lt; Z α ): The two algorithms π A and π B are not significantly different.• Accepting Hypothesis H α (Z ≥ Z α ): The two algorithms π A and π B are significantly different.Therefore, the algorithm with the greatest mean is definitely better with 95% confidence.
± 100 times more than the low online time budget. </p>
<p>Approaching Bayes-optimalilty using Monte-Carlo tree search. M Littman, Proceedings of the 21st International Conference on Automated Planning and Scheduling. the 21st International Conference on Automated Planning and Scheduling2011± 100 times more than the medium online time budget References J. Asmuth and</p>
<p>A Bayesian sampling approach to exploration in Reinforcement Learning. J Asmuth, L Li, M L Littman, A Nouri, D Wingate, Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI). the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI)AUAI Press2009</p>
<p>Tuning bandit algorithms in stochastic environments. J Y Audibert, R Munos, C Szepesvári, Algorithmic Learning Theory. Springer2007</p>
<p>Finite-time analysis of the multiarmed bandit problem. P Auer, N Cesa-Bianchi, P Fischer, Machine learning. 4722002</p>
<p>Smarter sampling in model-based bayesian reinforcement learning. P S Castro, D Precup, Machine Learning and Knowledge Discovery in Databases. Springer2010</p>
<p>Learning exploration/exploitation strategies for single trajectory Reinforcement Learning. M Castronovo, F Maes, R Fonteneau, D Ernst, Journal of Machine Learning Research. 2012</p>
<p>M Castronovo, R Fonteneau, D Ernst, Bayes Adaptive Reinforcement Learning versus Off-line Prior-based Policy Search: an Empirical Comparison. 23rd annual machine learning conference of Belgium and the Netherlands (BENELEARN 2014). 2014</p>
<p>Bayesian Q-learning. R Dearden, N Friedman, S Russell, Proceedings of Fifteenth National Conference on Artificial Intelligence (AAAI). Fifteenth National Conference on Artificial Intelligence (AAAI)AAAI Press1998</p>
<p>Model based Bayesian exploration. R Dearden, N Friedman, D Andre, Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI). the Fifteenth Conference on Uncertainty in Artificial Intelligence (UAI)Morgan Kaufmann1999</p>
<p>Efficient Bayes-adaptive Reinforcement Learning using sample-based search. A Guez, D Silver, P Dayan, Neural Information Processing Systems (NIPS). 2012</p>
<p>A sparse sampling algorithm for near-optimal planning in large Markov decision processes. M Kearns, Y Mansour, A Y Ng, Machine Learning. 200249</p>
<p>Bandit based Monte-Carlo planning. L Kocsis, C Szepesvári, European Conference on Machine Learning (ECML). 2006</p>
<p>Near-Bayesian exploration in polynomial time. J , Zico Kolter, Andrew Y Ng, Proceedings of the 26th Annual International Conference on Machine Learning. the 26th Annual International Conference on Machine Learning2009</p>
<p>A Bayesian framework for Reinforcement Learning. M Strens, Proceedings of the Seventeenth International Conference on Machine Learning (ICML). the Seventeenth International Conference on Machine Learning (ICML)ICML2000</p>            </div>
        </div>

    </div>
</body>
</html>
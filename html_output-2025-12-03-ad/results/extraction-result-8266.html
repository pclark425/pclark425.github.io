<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8266 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8266</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8266</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-277103755</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.14495v1.pdf" target="_blank">Temporal Consistency for LLM Reasoning Process Error Identification</a></p>
                <p><strong>Paper Abstract:</strong> Verification is crucial for effective mathematical reasoning. We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment. Unlike one-round verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy. Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods. When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1. Our codes are available at https://github.com/jcguo123/Temporal-Consistency</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8266.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8266.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TemporalConsistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporal Consistency (iterative self-checking with isolated multi-agent majority)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training-free, test-time verification method where K isolated LLM verifiers iteratively self-check their own previous verifications over multiple rounds; the algorithm outputs the majority identification only after stability/growing-consensus criteria are met.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-R1-Llama-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deepseek-R1 fine-tuned variant of Llama-family (used in experiments); 8B parameter-scale distilled / R1 models also evaluated (Deepseek distilled Qwen/Llama variants reported separately).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['iterative self-reflection / self-check', 'multi-agent independent verification', 'majority voting aggregation across agents with temporal stability criterion']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Each of K agents runs an initial verification (loc, reasoning). In subsequent rounds each agent receives only its own previous (loc, reasoning) and re-evaluates (self-check). After each round the algorithm computes a majority identification loc_t and proportion p_t; it stops when majority stability and non-decreasing consensus hold for q consecutive rounds (q=3 in experiments) or T rounds reached. Agents are isolated (do not see other agents' answers) to avoid persuasive but incorrect influences.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Uses multiple agents (K=5) each performing the same iterative self-check method (so multiple agents but a single repeated self-check procedure). Experimental comparisons include (1) single-agent greedy decoding, (2) majority voting (multiple independent single-round verifications), and (3) debate-based multi-agent exchange (agents see others' answers). Ablation study includes Temporal Consistency without multi-agent (single agent iterative), self-checking without iterative generation (single-round self-check), and the full multi-agent iterative method.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>MathCheck* (balanced GSM8K/MathCheck process-judging problems), ProcessBench (3,400 math problems from GSM8K, MATH, OlympiadBench, Omni-MATH), PRM800K (process-labeled problems from MATH). Step-level process error identification (identify earliest incorrect solution step; metric: F1 on correct/incorrect samples).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Deepseek-R1-Llama-8B: Temporal Consistency F1 = 82.5% (MathCheck*), 67.2% (ProcessBench), 50.2% (PRM800K). Compared to baselines on same model: greedy decoding = 35.9/29.3/21.2 (MathCheck*/ProcessBench/PRM800K), majority voting = 35.5/56.7/48.9, multi-model debate = 57.6/41.7/46.7. For Deepseek-R1-Distill-Qwen-7B Temporal Consistency achieves 89.5%/71.3%/57.7% on MathCheck*/ProcessBench/PRM800K respectively, outperforming reported 70B/72B baselines and GPT-4o on ProcessBench. (All numbers are F1 percentages as reported in paper tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Temporal stability (consensus that persists and grows over rounds) correlates with correctness; isolating agents prevents propagation of persuasive but incorrect long justifications (a problem for debate methods); enforcing stricter stability requirement q increases F1 (0→3 increases ProcessBench F1 from 48.9% to 67.2%). Iterative vertical scaling (over time) yields large gains especially for distilled/smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Temporal consistency across iterative self-checks is a reliable indicator of correct verifications and yields substantial, training-free improvements over single-shot greedy decoding, majority voting, and multi-model debate; it allows small distilled models to match or exceed much larger models on process error identification benchmarks by leveraging time-based refinement rather than purely more parallel samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Consistency for LLM Reasoning Process Error Identification', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8266.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8266.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MajorityVoting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Majority Voting (parallel independent verifiers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A common training-free ensemble technique where multiple agents independently produce verifications and the final decision is the majority of generated outputs (self-consistency / best-of-n style aggregation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-R1-Llama-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same base model as used in experiments; majority voting implemented with K=5 independent agents/samples.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['parallel independent verifications', 'aggregation via majority vote']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Multiple agents independently generate a single-round verification (initial verification prompt). The final label is the most frequent label across agents (argmax count). This scales horizontally by increasing the number of parallel samples.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Baseline in experiments with K=5 parallel agents; compared to greedy single-agent, debate (agents exchange answers), and Temporal Consistency (iterative self-checking). Ablation: compared against Temporal Consistency and single-agent variants to measure contribution of multi-agent aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Same process error identification benchmarks: MathCheck*, ProcessBench, PRM800K.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Deepseek-R1-Llama-8B majority voting F1 = 35.5% (MathCheck*), 56.7% (ProcessBench), 48.9% (PRM800K). For Deepseek-R1-Distill-Qwen-7B majority voting F1 = 89.3%/64.8%/64.8% (MathCheck*/ProcessBench/PRM800K) in Table 2. Majority voting improves over greedy in some cases but is outperformed by Temporal Consistency on all reported benchmarks for the primary evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Majority voting is effective when many agents independently find the correct answer, but it fails when only a minority spot the error (minority-correct cases); it also lacks temporal refinement and can produce unstable identifications across rounds.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Parallel aggregation via majority voting is useful but inferior to temporal iterative self-checking for process error identification; Temporal Consistency leverages temporal stability in addition to consensus across agents to reduce unstable incorrect identifications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Consistency for LLM Reasoning Process Error Identification', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8266.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8266.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MultiModelDebate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-model Debate / Debate-based Verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent interactive protocol where agents see other agents' answers and generate rebuttals/updated verifications; intended to surface flaws via critique and discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-R1-Llama-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Debate adaptation implemented following Du et al. (2023) with two rounds: initial verification then a debate round where agents receive others' solutions and re-evaluate.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['multi-agent exchange and critique (debate)', 'iterative two-round interaction']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Agents first produce independent verifications; in the debate round they are provided the other agents' answers as context and asked to analyze again, potentially changing identifications. The paper adapted Du et al.'s debate prompt to the verification task.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Debate baseline used K=5 agents with two rounds (initial + debate). Compared to Temporal Consistency (agents isolated, iterative self-check) and majority voting (independent, single-round). The paper includes an example where debate causes agents to shift toward persuasive but incorrect long justifications (asymmetric behavior in math proofs).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>ProcessBench / MathCheck* / PRM800K process error identification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Deepseek-R1-Llama-8B multi-model debate F1 = 57.6% (MathCheck*), 41.7% (ProcessBench), 46.7% (PRM800K) per Table 2. Temporal Consistency outperforms debate (e.g., 82.5% vs 57.6% on MathCheck* for same base model).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Debate can be negatively affected by asymmetry in mathematical reasoning: incorrect reasoning paths produce long, apparently logical justifications that can persuade other agents, while correct concise solutions may be overshadowed. This can lead debate to favor incorrect justifications; isolating agents (Temporal Consistency) mitigates this effect.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>While debate enables agents to see additional perspectives, openness to others' arguments can cause propagation of persuasive but incorrect reasoning in mathematical verification; maintaining agent isolation combined with iterative self-checking produces more stable and accurate identifications.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Consistency for LLM Reasoning Process Error Identification', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8266.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8266.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GreedyDecoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greedy Decoding (single-shot deterministic verification)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single-agent, single-round deterministic verification where one model produces a verification with greedy decoding (no ensemble or iterative refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-R1-Llama-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-agent deterministic generation (temperature/seed set to produce a single output), used as a baseline for verification performance.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['single-shot deterministic reasoning / verification']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>A single LLM runs the verification prompt and outputs the earliest incorrect-step index (or -1) deterministically (greedy decoding). No parallel sampling, debate, or iterative self-checking is used.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Baseline single-agent method; compared directly with majority voting (multiple independent samples), multi-model debate, and Temporal Consistency. Used across same datasets to measure absolute baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>ProcessBench / MathCheck* / PRM800K process error identification.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Deepseek-R1-Llama-8B greedy decoding F1 = 35.9% (MathCheck*), 29.3% (ProcessBench), 21.2% (PRM800K). For Deepseek-R1-Distill-Qwen-7B greedy = 86.0%/54.8%/46.2% (MathCheck*/ProcessBench/PRM800K) per Table 2. Greedy is consistently worse than Temporal Consistency and often worse than majority voting or debate depending on model and dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Single-shot deterministic verification is fast but brittle; many process errors are not identified without multiple samples or iterative re-evaluation. Greedy outputs lack the stability signal that temporal refinements exploit.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Greedy single-shot verification is a weak baseline for step-level process error identification; test-time aggregation (majority voting) or iterative temporal refinement yields much higher F1, especially for smaller/distilled models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Consistency for LLM Reasoning Process Error Identification', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8266.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8266.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ablation_IterVsMulti</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation: iterative generation vs multi-agent contribution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A targeted ablation, performed on ProcessBench, that isolates the contributions of iterative self-checking and multi-agent aggregation to the full Temporal Consistency performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deepseek-R1-Llama-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Ablation applied to the same base model(s) used in the main experiments to measure per-component gains.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['single-agent iterative self-check (no multi-agent)', 'multi-agent single-round self-check (no iterative generation)', 'full multi-agent iterative Temporal Consistency']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Four configurations: (1) greedy decoding (single-shot), (2) Temporal Consistency without multi-agent (single agent iterative), (3) self-checking without iterative generation (multi-agent but only single re-check, i.e., no iteration), and (4) full Temporal Consistency (multi-agent iterative). Measured F1 on ProcessBench to estimate each component's effect.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both (experimentally varied)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>Direct ablation on ProcessBench. The study quantifies contributions from (A) iterative refinement (vertical/time scaling) and (B) multi-agent aggregation (horizontal scaling).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>ProcessBench (difficulty-split analysis and aggregated F1 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Ablation results reported show that both components contribute substantially: the two components individually contribute improvements of ~24.2% and ~25.8% (numbers reported in paper) and their combination yields the best F1 (67.2% on ProcessBench for full Temporal Consistency with K=5 and q=3).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Both iterative self-checking and multi-agent aggregation are necessary: iterative self-checking provides temporal stability and correction of initial misidentifications, while multi-agent voting provides redundancy; removing either substantially reduces performance. The combination produces synergistic gains larger than either alone.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Ablation confirms that iterative temporal refinement and multi-agent aggregation independently improve verification accuracy by large margins, and their combination (Temporal Consistency) produces the largest performance gain on process error identification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Temporal Consistency for LLM Reasoning Process Error Identification', 'publication_date_yy_mm': '2025-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>ProcessBench: Identifying process errors in mathematical reasoning <em>(Rating: 2)</em></li>
                <li>PRM800K: Let's verify step by step (Process Reward Models) <em>(Rating: 2)</em></li>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 1)</em></li>
                <li>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8266",
    "paper_id": "paper-277103755",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "TemporalConsistency",
            "name_full": "Temporal Consistency (iterative self-checking with isolated multi-agent majority)",
            "brief_description": "A training-free, test-time verification method where K isolated LLM verifiers iteratively self-check their own previous verifications over multiple rounds; the algorithm outputs the majority identification only after stability/growing-consensus criteria are met.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Deepseek-R1-Llama-8B",
            "model_description": "Deepseek-R1 fine-tuned variant of Llama-family (used in experiments); 8B parameter-scale distilled / R1 models also evaluated (Deepseek distilled Qwen/Llama variants reported separately).",
            "reasoning_methods": [
                "iterative self-reflection / self-check",
                "multi-agent independent verification",
                "majority voting aggregation across agents with temporal stability criterion"
            ],
            "reasoning_methods_description": "Each of K agents runs an initial verification (loc, reasoning). In subsequent rounds each agent receives only its own previous (loc, reasoning) and re-evaluates (self-check). After each round the algorithm computes a majority identification loc_t and proportion p_t; it stops when majority stability and non-decreasing consensus hold for q consecutive rounds (q=3 in experiments) or T rounds reached. Agents are isolated (do not see other agents' answers) to avoid persuasive but incorrect influences.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "Uses multiple agents (K=5) each performing the same iterative self-check method (so multiple agents but a single repeated self-check procedure). Experimental comparisons include (1) single-agent greedy decoding, (2) majority voting (multiple independent single-round verifications), and (3) debate-based multi-agent exchange (agents see others' answers). Ablation study includes Temporal Consistency without multi-agent (single agent iterative), self-checking without iterative generation (single-round self-check), and the full multi-agent iterative method.",
            "task_or_benchmark": "MathCheck* (balanced GSM8K/MathCheck process-judging problems), ProcessBench (3,400 math problems from GSM8K, MATH, OlympiadBench, Omni-MATH), PRM800K (process-labeled problems from MATH). Step-level process error identification (identify earliest incorrect solution step; metric: F1 on correct/incorrect samples).",
            "performance_results": "Deepseek-R1-Llama-8B: Temporal Consistency F1 = 82.5% (MathCheck*), 67.2% (ProcessBench), 50.2% (PRM800K). Compared to baselines on same model: greedy decoding = 35.9/29.3/21.2 (MathCheck*/ProcessBench/PRM800K), majority voting = 35.5/56.7/48.9, multi-model debate = 57.6/41.7/46.7. For Deepseek-R1-Distill-Qwen-7B Temporal Consistency achieves 89.5%/71.3%/57.7% on MathCheck*/ProcessBench/PRM800K respectively, outperforming reported 70B/72B baselines and GPT-4o on ProcessBench. (All numbers are F1 percentages as reported in paper tables.)",
            "qualitative_findings": "Temporal stability (consensus that persists and grows over rounds) correlates with correctness; isolating agents prevents propagation of persuasive but incorrect long justifications (a problem for debate methods); enforcing stricter stability requirement q increases F1 (0→3 increases ProcessBench F1 from 48.9% to 67.2%). Iterative vertical scaling (over time) yields large gains especially for distilled/smaller models.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Temporal consistency across iterative self-checks is a reliable indicator of correct verifications and yields substantial, training-free improvements over single-shot greedy decoding, majority voting, and multi-model debate; it allows small distilled models to match or exceed much larger models on process error identification benchmarks by leveraging time-based refinement rather than purely more parallel samples.",
            "uuid": "e8266.0",
            "source_info": {
                "paper_title": "Temporal Consistency for LLM Reasoning Process Error Identification",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MajorityVoting",
            "name_full": "Majority Voting (parallel independent verifiers)",
            "brief_description": "A common training-free ensemble technique where multiple agents independently produce verifications and the final decision is the majority of generated outputs (self-consistency / best-of-n style aggregation).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Deepseek-R1-Llama-8B",
            "model_description": "Same base model as used in experiments; majority voting implemented with K=5 independent agents/samples.",
            "reasoning_methods": [
                "parallel independent verifications",
                "aggregation via majority vote"
            ],
            "reasoning_methods_description": "Multiple agents independently generate a single-round verification (initial verification prompt). The final label is the most frequent label across agents (argmax count). This scales horizontally by increasing the number of parallel samples.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Baseline in experiments with K=5 parallel agents; compared to greedy single-agent, debate (agents exchange answers), and Temporal Consistency (iterative self-checking). Ablation: compared against Temporal Consistency and single-agent variants to measure contribution of multi-agent aggregation.",
            "task_or_benchmark": "Same process error identification benchmarks: MathCheck*, ProcessBench, PRM800K.",
            "performance_results": "Deepseek-R1-Llama-8B majority voting F1 = 35.5% (MathCheck*), 56.7% (ProcessBench), 48.9% (PRM800K). For Deepseek-R1-Distill-Qwen-7B majority voting F1 = 89.3%/64.8%/64.8% (MathCheck*/ProcessBench/PRM800K) in Table 2. Majority voting improves over greedy in some cases but is outperformed by Temporal Consistency on all reported benchmarks for the primary evaluated models.",
            "qualitative_findings": "Majority voting is effective when many agents independently find the correct answer, but it fails when only a minority spot the error (minority-correct cases); it also lacks temporal refinement and can produce unstable identifications across rounds.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Parallel aggregation via majority voting is useful but inferior to temporal iterative self-checking for process error identification; Temporal Consistency leverages temporal stability in addition to consensus across agents to reduce unstable incorrect identifications.",
            "uuid": "e8266.1",
            "source_info": {
                "paper_title": "Temporal Consistency for LLM Reasoning Process Error Identification",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "MultiModelDebate",
            "name_full": "Multi-model Debate / Debate-based Verification",
            "brief_description": "A multi-agent interactive protocol where agents see other agents' answers and generate rebuttals/updated verifications; intended to surface flaws via critique and discussion.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Deepseek-R1-Llama-8B",
            "model_description": "Debate adaptation implemented following Du et al. (2023) with two rounds: initial verification then a debate round where agents receive others' solutions and re-evaluate.",
            "reasoning_methods": [
                "multi-agent exchange and critique (debate)",
                "iterative two-round interaction"
            ],
            "reasoning_methods_description": "Agents first produce independent verifications; in the debate round they are provided the other agents' answers as context and asked to analyze again, potentially changing identifications. The paper adapted Du et al.'s debate prompt to the verification task.",
            "reasoning_diversity": "diverse",
            "reasoning_diversity_experimental_setup": "Debate baseline used K=5 agents with two rounds (initial + debate). Compared to Temporal Consistency (agents isolated, iterative self-check) and majority voting (independent, single-round). The paper includes an example where debate causes agents to shift toward persuasive but incorrect long justifications (asymmetric behavior in math proofs).",
            "task_or_benchmark": "ProcessBench / MathCheck* / PRM800K process error identification.",
            "performance_results": "Deepseek-R1-Llama-8B multi-model debate F1 = 57.6% (MathCheck*), 41.7% (ProcessBench), 46.7% (PRM800K) per Table 2. Temporal Consistency outperforms debate (e.g., 82.5% vs 57.6% on MathCheck* for same base model).",
            "qualitative_findings": "Debate can be negatively affected by asymmetry in mathematical reasoning: incorrect reasoning paths produce long, apparently logical justifications that can persuade other agents, while correct concise solutions may be overshadowed. This can lead debate to favor incorrect justifications; isolating agents (Temporal Consistency) mitigates this effect.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "While debate enables agents to see additional perspectives, openness to others' arguments can cause propagation of persuasive but incorrect reasoning in mathematical verification; maintaining agent isolation combined with iterative self-checking produces more stable and accurate identifications.",
            "uuid": "e8266.2",
            "source_info": {
                "paper_title": "Temporal Consistency for LLM Reasoning Process Error Identification",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "GreedyDecoding",
            "name_full": "Greedy Decoding (single-shot deterministic verification)",
            "brief_description": "A single-agent, single-round deterministic verification where one model produces a verification with greedy decoding (no ensemble or iterative refinement).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Deepseek-R1-Llama-8B",
            "model_description": "Single-agent deterministic generation (temperature/seed set to produce a single output), used as a baseline for verification performance.",
            "reasoning_methods": [
                "single-shot deterministic reasoning / verification"
            ],
            "reasoning_methods_description": "A single LLM runs the verification prompt and outputs the earliest incorrect-step index (or -1) deterministically (greedy decoding). No parallel sampling, debate, or iterative self-checking is used.",
            "reasoning_diversity": "similar",
            "reasoning_diversity_experimental_setup": "Baseline single-agent method; compared directly with majority voting (multiple independent samples), multi-model debate, and Temporal Consistency. Used across same datasets to measure absolute baseline performance.",
            "task_or_benchmark": "ProcessBench / MathCheck* / PRM800K process error identification.",
            "performance_results": "Deepseek-R1-Llama-8B greedy decoding F1 = 35.9% (MathCheck*), 29.3% (ProcessBench), 21.2% (PRM800K). For Deepseek-R1-Distill-Qwen-7B greedy = 86.0%/54.8%/46.2% (MathCheck*/ProcessBench/PRM800K) per Table 2. Greedy is consistently worse than Temporal Consistency and often worse than majority voting or debate depending on model and dataset.",
            "qualitative_findings": "Single-shot deterministic verification is fast but brittle; many process errors are not identified without multiple samples or iterative re-evaluation. Greedy outputs lack the stability signal that temporal refinements exploit.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Greedy single-shot verification is a weak baseline for step-level process error identification; test-time aggregation (majority voting) or iterative temporal refinement yields much higher F1, especially for smaller/distilled models.",
            "uuid": "e8266.3",
            "source_info": {
                "paper_title": "Temporal Consistency for LLM Reasoning Process Error Identification",
                "publication_date_yy_mm": "2025-03"
            }
        },
        {
            "name_short": "Ablation_IterVsMulti",
            "name_full": "Ablation: iterative generation vs multi-agent contribution",
            "brief_description": "A targeted ablation, performed on ProcessBench, that isolates the contributions of iterative self-checking and multi-agent aggregation to the full Temporal Consistency performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Deepseek-R1-Llama-8B",
            "model_description": "Ablation applied to the same base model(s) used in the main experiments to measure per-component gains.",
            "reasoning_methods": [
                "single-agent iterative self-check (no multi-agent)",
                "multi-agent single-round self-check (no iterative generation)",
                "full multi-agent iterative Temporal Consistency"
            ],
            "reasoning_methods_description": "Four configurations: (1) greedy decoding (single-shot), (2) Temporal Consistency without multi-agent (single agent iterative), (3) self-checking without iterative generation (multi-agent but only single re-check, i.e., no iteration), and (4) full Temporal Consistency (multi-agent iterative). Measured F1 on ProcessBench to estimate each component's effect.",
            "reasoning_diversity": "both (experimentally varied)",
            "reasoning_diversity_experimental_setup": "Direct ablation on ProcessBench. The study quantifies contributions from (A) iterative refinement (vertical/time scaling) and (B) multi-agent aggregation (horizontal scaling).",
            "task_or_benchmark": "ProcessBench (difficulty-split analysis and aggregated F1 reported).",
            "performance_results": "Ablation results reported show that both components contribute substantially: the two components individually contribute improvements of ~24.2% and ~25.8% (numbers reported in paper) and their combination yields the best F1 (67.2% on ProcessBench for full Temporal Consistency with K=5 and q=3).",
            "qualitative_findings": "Both iterative self-checking and multi-agent aggregation are necessary: iterative self-checking provides temporal stability and correction of initial misidentifications, while multi-agent voting provides redundancy; removing either substantially reduces performance. The combination produces synergistic gains larger than either alone.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Ablation confirms that iterative temporal refinement and multi-agent aggregation independently improve verification accuracy by large margins, and their combination (Temporal Consistency) produces the largest performance gain on process error identification.",
            "uuid": "e8266.4",
            "source_info": {
                "paper_title": "Temporal Consistency for LLM Reasoning Process Error Identification",
                "publication_date_yy_mm": "2025-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "ProcessBench: Identifying process errors in mathematical reasoning",
            "rating": 2,
            "sanitized_title": "processbench_identifying_process_errors_in_mathematical_reasoning"
        },
        {
            "paper_title": "PRM800K: Let's verify step by step (Process Reward Models)",
            "rating": 2,
            "sanitized_title": "prm800k_lets_verify_step_by_step_process_reward_models"
        },
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 1,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
            "rating": 2,
            "sanitized_title": "deepseekr1_incentivizing_reasoning_capability_in_llms_via_reinforcement_learning"
        }
    ],
    "cost": 0.014653,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Temporal Consistency for LLM Reasoning Process Error Identification
18 Mar 2025</p>
<p>Jiacheng Guo 
Department of Electrical &amp; Computer Engineering
Princeton University</p>
<p>Yue Wu 
AI Lab
Princeton University</p>
<p>Jiahao Qiu 
Department of Electrical &amp; Computer Engineering
Princeton University</p>
<p>Kaixuan Huang 
Department of Electrical &amp; Computer Engineering
Princeton University</p>
<p>Xinzhe Juan 
Department of Computer Science &amp; Engineering
University of Michigan</p>
<p>Ling Yang 
Department of Electrical &amp; Computer Engineering
Princeton University</p>
<p>AI Lab
Princeton University</p>
<p>Mengdi Wang 
Department of Electrical &amp; Computer Engineering
Princeton University</p>
<p>Temporal Consistency for LLM Reasoning Process Error Identification
18 Mar 202509A384BB4997D841C97F9C573004099CarXiv:2503.14495v1[cs.CL]
Verification is crucial for effective mathematical reasoning.We present a new temporal consistency method where verifiers iteratively refine their judgments based on the previous assessment.Unlike oneround verification or multi-model debate approaches, our method leverages consistency in a sequence of self-reflection actions to improve verification accuracy.Empirical evaluations across diverse mathematical process error identification benchmarks (Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements over baseline methods.When applied to the recent DeepSeek R1 distilled models, our method demonstrates strong performance, enabling 7B/8B distilled models to outperform all 70B/72B models and GPT-4o on ProcessBench.Notably, the distilled 14B model with our method achieves performance comparable to Deepseek-R1.Our codes are available at https://github.com/jcguo123/Temporal-Consistency</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown impressive capabilities in reasoning tasks [Grattafiori et al., 2024, Yang et al., 2024a, Jaech et al., 2024, Guo et al., 2025, Yang et al., 2025], but still often make mistakes when generating complex multi-step solutions.To address this issue, Process Reward Models (PRMs) [Lightman et al., 2023, Luo et al., 2024a] have been introduced to guide generations.Instead of providing feedback solely on the final answer, PRMs evaluate every intermediate step in the reasoning chain, thereby aligning the model's chain of thought with correct logical sequences.Convergence Check Output Self-check Figure 2: Overview of our Temporal Consistency approach, where each LLM iteratively examines its own verification results until reaching a stable result (stopping criteria defined in Section 2).The self-checking mechanism allows LLMs to refine their judgments based on previous verifications, potentially correcting initial misidentification.</p>
<p>However, existing PRMs face several key limitations that hinder their broader applicability.First, training a PRM requires large-scale, high-quality annotated datasets, making the process highly data-intensive and costly to scale [Guo et al., 2025].Second, PRMs exhibit poor out-of-domain generalization; models trained on specific problem distributions often struggle to accurately evaluate reasoning steps when confronted with diverse problem types [Zeng et al., 2025, Lin et al., 2024].Finally, the effectiveness of PRMs is intrinsically limited by the capability of the base model [Luo et al., 2024b].These challenges highlight the need for further research to develop more scalable process supervision techniques in LLMs.</p>
<p>An alternative way is to adopt some training-free approaches like majority voting [Wang et al., 2022] or debate-based approaches [Du et al., 2023], which have shown effectiveness in aggregating opinions and resolving conflicts between multiple reasoning trajectories.</p>
<p>Nevertheless, we found that both methods show limitations when applied to mathematical process error identification tasks.Majority voting often fails when errors are identified by only a minority of LLMs [Huang et al., 2024].Debatebased approaches sometimes struggle due to an asymmetry in mathematical reasoning: erroneous reasoning paths tend to generate lengthy, seemingly logical justifications, while correct reasoning paths provide only simple justifications.This asymmetry can cause debate methods to favor incorrect justifications, as more elaborate (though flawed) arguments may overshadow simple (but correct) justifications.</p>
<p>To address these limitations, we develop a simple but effective training-free approach to enhance process error identification capabilities.The intuition is to leverage the consistency between a sequence of self-reflection actions because the LLMs should be more likely to remain consistent and confident when asked to review correct validations.As shown in Figure 2, we propose the Temporal Consistency method, where each LLM iteratively checks its identifications, and the final output is only produced when multiple LLMs demonstrate consistent self-checking over time, effectively reducing unstable incorrect identifications.</p>
<p>We further evaluate our approach across three annotated mathematical step datasets, PRM800K [Lightman et al., 2023], ProcessBench [Zheng et al., 2024a], and MathCheck *1 [Zhou et al., 2024].Our experiments demonstrated consistent performance gains across different models, benchmarks, and difficulty levels.We then conducted experiments on R1 distilled models [Guo et al., 2025], where our method achieved remarkable improvements: as shown in Figure 1 for Deepseek-R1-Distill-Llama-8B, improvements of 46.6% on MathCheck * , 37.9% on ProcessBench, and 29.0% on PRM800K; for Deepseek-R1-Distill-Qwen-7B, improvements of 3.5% on MathCheck * , 16.5% on ProcessBench, and 11.5% on PRM800K; for Deepseek-R1-Distill-Qwen-14B, improvements of 3.7% on MathCheck * , 10.6% on ProcessBench, and 8.6% on PRM800K.Notably, our method enables distilled 7B/8B models to achieve 71.3%/67.2% on ProcessBench, surpassing all existing 70B/72B models and GPT-4o reported in Zheng et al. [2024a].With our method applied, the distilled 14B model demonstrates performance comparable to Deepseek-R1's.As shown in Figure 3, our Temporal Consistency method establishes a new type of test-time scaling law.Unlike conventional approaches that scale by increasing the number of parallel samples, our method scales through iterative refinement over time (temporal dimension).</p>
<p>Methodology</p>
<p>In this section, we introduce our method that utilizes multiple rounds of validation to improve identification accuracy.We begin by defining the process error identification task.Task Definition Given a problem P and its step-by-step solution S = {s 0 , s 1 , . . ., s n−1 }, where each s i represents the i-th solution step, our objective is to identify the first incorrect step, if any, and output a location index loc ∈ {−1, 0, . . ., n − 1}.Here, loc = −1 indicating that all steps are correct, while for loc ≥ 0, s loc represents the first incorrect step.</p>
<p>We now introduce the Temporal Consistency algorithm.This method adds a temporal dimension to the verification process by having each LLM consider its own previous assessment, leveraging consistency in a sequence of selfreflection.We employ K LLMs as verifiers, denoted by LLM 1 , . . ., LLM K .The algorithm has three phases: Initial Verification Phase For each i ∈ {1, . . ., K}, given a problem P , a solution S, and a designated process error identification prompt X Verify , LLM i examines the solution step by step.It identifies the location of the first incorrect step loc i 1 , and provides the corresponding reasoning response res i 1 : (loc i 1 , res i 1 ) = LLM i (P, S, X Verify ) These initial verifications establish a set of independent assessments.</p>
<p>Iterative Self-checking Phase For time steps t ≥ 2, let (loc i t−1 , res i t−1 ) represent the verification results from the previous iteration for each i ∈ {1, . . ., K}.With a designated self-verification prompt X Self-check , LLM i performs a subsequent self-assessment:</p>
<p>(loc i t , res i t ) = LLM i (P, S, X Self-check , loc i t−1 , res i t−1 ).The distinction between the initial verification phase and the self-checking phase is incorporating previous verification results to provide additional context.This temporal dependency enables the LLMs to potentially correct initial misidentifications.Figure 4 illustrates the self-checking mechanism.</p>
<p>Julia was preparing for a dinner party at her house, where she intended to serve stew.She noticed that she was out of plastic spoons, so she bought a new package of spoons.Later, her husband also bought a package of 5 new spoons and gave them to Julia… First, initially, Julia had no spoons.She then bought a new package of spoons.Her husband also bought a package of 5 new spoons and gave them to her.Self−checking Convergence Check After each iteration t, the algorithm determines the majority identification loc t by applying a majority voting function:
X Self-check X Self-check X Self-check X Self-check X Self-checkMajorityVote(loc 1 t , . . . , loc K t ) = argmax loc∈{−1,••• ,n−1} {i : loc i t = loc} .(1)
This function aggregates the verification outcomes from K different LLMs and returns the error step that is most frequently identified.Specifically, {i : loc i = loc} counts the number of LLMs that have identified step loc as incorrect.The algorithm then evaluates the stability of these identifications across all LLMs.Let p t be the proportion of agents supporting loc t , formally defined as
p t = {i : loc i t = loc t } K .(2)
When sufficient stability and consensus are reached, the algorithm terminates and outputs the final identification.Detailed stopping conditions defined with loc t and p t are provided in Section 2.1.</p>
<p>This approach leverages the strengths of multiple independent verifications and consistency across the temporal dimension.By allowing each LLM to build on its previous assessments while remaining isolated from others, the algorithm minimizes the risk of reinforcing arguments that appear plausible but are incorrect.The complete algorithm is detailed in Algorithm 1.</p>
<p>Stopping Criteria</p>
<p>In practice, most agents converge to an identification within just a few rounds, making further self-checks computationally redundant.To enhance efficiency, we propose a heuristic stopping criterion that permits early termination for "high confidence" problems while allowing continued self-checking for "low confidence" problems.</p>
<p>For any round t ∈ {1, . . ., T }, let loc t denote the majority identification defined in equation 1, and p t be the proportion of agents supporting loc t defined in equation 2. Based on these definitions, we design two stopping conditions over q consecutive rounds, where q is a given consistency requirement:</p>
<ol>
<li>Majority Stability:
loc t−q+1 = loc t−q+2 = • • • = loc t , 2. Growing Consensus: p t−q+1 ≤ p t−q+2 ≤ • • • ≤ p t .
The majority stability condition requires that the majority identification remains unchanged over the past q rounds, ensuring a consistent outcome in majority voting.Concurrently, the growing consensus condition needs the proportion of agents supporting the majority identification to not decrease across these q rounds.The underlying intuition is that the correct answer should be identified with "increasing confidence" over the past q rounds.</li>
</ol>
<p>The algorithm terminates when both conditions are satisfied or when the maximum number of rounds T is reached.The consistency requirement q is a parameter that can be adjusted according to task-specific requirements.</p>
<p>Algorithm 1 Temporal Consistency</p>
<p>Input: Problem P , solution S, number of LLMs K, initial verification prompt X Verify , self-checking prompt X Self-check , consistency requirement q, max rounds T ./<em> Initial Verification Phase </em>/ for i = 1 to K in parallel do (loc i 1 , res i 1 ) ← LLM i (P, S, X Verify ) end for /<em> Iterative Self-checking Phase </em>/ for round t = 2 to T do for LLM i = 1 to K in parallel do Figure 5: Performance comparison across three datasets (Mathcheck * , ProcessBench, and PRM800K).Our Temporal Consistency approach (green) consistently outperforms baseline methods, including greedy decoding (yellow), majority voting (orange), and multi-model debate (red).
(loc i t , res i t ) ← LLM i (P, S, X Self-check , loc i t−1 , res i t−1 ) end for loc t ← MajorityVote(loc 1 t , ..., loc K t ) p t ← |{i : loc i t = loc t }|/K if t ≥ q then stable ← q−2 j=0 (loc t−j = loc t−q+1 ) growing ← q−2 j=0 (p t−j ≥ p t−j−</p>
<p>Comparison with Existing Methods</p>
<p>Existing majority voting approaches [Cobbe et al., 2021, Li et al., 2022, Wang et al., 2022] perform multiple generations simultaneously, essentially scaling horizontally to enhance stability.In contrast, our method allows each LLM to build upon its previous assessments, achieving vertical scaling over time.This sequential self-reflection enables each verification to benefit from prior insights.</p>
<p>Moreover, our approach differs from multi-model debate methods [Du et al., 2023] in treating LLM independence.</p>
<p>Although debate methods allow models to exchange information, thus enabling them to see other agents' answers and gain additional perspectives, this openness risks influence from persuasive yet incorrect arguments.For further illustration, an example can be found in Appendix B. In contrast, our method maintains strict isolation between LLMs.Each LLM focuses solely on its own reasoning process, thereby reducing the risk of propagating elaborate but erroneous arguments.3 Experiments</p>
<p>Experimental Setup</p>
<p>Dataset We evaluate our method on ProcessBench [Zheng et al., 2024a], a comprehensive dataset combining multiple mathematical problem-solving benchmarks.The dataset consists of 3,400 problems from four sources: 400 from GSM8K [Cobbe et al., 2021], 1,000 from MATH dataset [Hendrycks et al., 2021], 1,000 from OlympiadBench [He et al., 2024], and 1,000 from Omni-MATH [Gao et al., 2024].Each problem includes both generated solutions and humanannotated processes.Additionally, we incorporate 516 process judging problems based on GSM8K from MathCheck [Zhou et al., 2024] and 300 randomly selected problems based on MATH dataset from PRM800K [Lightman et al., 2023].Since the process judging problem in MathCheck only contains incorrect solutions, we combine it with the GSM8K problems with correct steps from ProcessBench to create a balanced dataset, which we denote as MathCheck * .For PRM800K, we consider both 0 and 1 annotations as correct steps and -1 as incorrect steps.We evaluate the F1 score for all benchmarks, which is the harmonic mean of the accuracies on incorrect and correct samples.Baseline Methods We compare our approach against three baseline methods:</p>
<p>Model</p>
<p>(1) Verification with greedy decoding [Zhang et al., 2022], where a single agent generates a verification deterministically, (2) Majority voting among multiple agents [Wang et al., 2022], where multiple agents independently generate verifications, and the final decision is made based on majority voting and (3) Verification with debate-based reasoning [Du et al., 2023], where multiple agents generate verifications independently, and they will receive the answer from the other agents and then generate a new identification.</p>
<p>Parameter Setting To ensure a fair comparison, we employ 5 parallel agents in each of the three methods: majority voting, debate-based verification, and our Temporal Consistency approach.Following Du et al. [2023], the debate method proceeds in two rounds: an initial verification round followed by a debate round.Our method implements convergence criteria requiring stability across 3 consecutive rounds, with a maximum of 10 rounds.We use Deepseek-R1-Llama-8B [Guo et al., 2025] in all our experiments except those in Table 1, Table 2 and Figure 1.Appendix A shows complete experimental configurations and implementation details.</p>
<p>Main Results</p>
<p>Improvement over Diverse Dataset Figure 5 presents the performance comparison across three datasets for Deepseek-R1-Llama-8B. Our Temporal Consistency approach consistently outperforms baseline methods across all evaluation settings.</p>
<p>On Mathcheck * , our method achieves an F1 score of 82.5%, showing an improvement of 46.6% over greedy decoding and 25.8% over multi-model debate.For ProcessBench, we observe consistent improvements with our method achieving 67.2% F1 score, compared to 29.3% for greedy decoding and 57.6% for multi-model debate.On PRM800K, our method maintains its advantage with 50.2% F1 score, showing a 29.0%improvement over greedy decoding.</p>
<p>Improvement over Different Base Models</p>
<p>To demonstrate the generalizability of our approach, we conducted experiments across different language models, including GPT-4o mini, GPT-4o [Hurst et al., 2024], Llama 3.1 8B Instruct [Grattafiori et al., 2024] and Mistral 7B Instruct [Jiang et al., 2023].We evaluated these models on Mathcheck * , ProcessBench, and PRM800K.As shown in Table 1, our Temporal Consistency method consistently outperforms baseline methods across all benchmarks.This consistent performance across different models demonstrates the effectiveness of our approach.</p>
<p>Improvement for Distilled Models We further evaluate our method and the baseline methods on the recently released Deepseek R1 distilled models [Guo et al., 2025], including DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B.As shown in Table 2, our Temporal Consistency method demonstrates remarkable effectiveness on 7B/8B-scale models, achieving 71.3% and 67.2% accuracy on ProcessBench with DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B respectively, surpassing GPT-4o (69.1%) and all 70B/72B models reported in Zheng et al. [2024a], including Llama-3.3-70B-Instruct (58.0%),Qwen2.5-Math-72B-Instruct(45.5%) and Qwen2.5-72B-Instruct(61.2%) [Yang et al., 2024b].</p>
<p>Additional Analysis</p>
<p>Different Choice of Consistency Requirement We investigated the impact of different consistency requirements on model performance using ProcessBench.As shown in Figure 6, we experimented with consistency requirements ranging from 0 to 3, where higher values indicate stricter requirements for output stability.The F1 score demonstrates a consistent upward trend as the consistency requirement increases, starting from 48.9% without the self-checking requirement (parameter = 0) and reaching 67.2% with the strictest stability requirement (parameter = 3).This correlation suggests that requiring more stable outputs through multiple verification rounds leads to more accurate results.</p>
<p>Performance Across Problem Difficulty To analyze our method's effectiveness across varying complexity levels, we categorized ProcessBench problems into two groups following the difficulty definition in Zheng et al. [2024a]: Easy (derived from GSM8K and MATH) and Hard (derived from OlympiadBench and Omni-MATH). Figure 8 illustrates the performance comparison across these categories.</p>
<p>All methods demonstrate strong performance on easy problems, with our approach achieving 70.2</p>
<p>Cost-Performance Analysis To understand the trade-offs between computational resources and verification performance, we conducted experiments with various parameter configurations of our method.Figure 7 illustrates how performance scales with increased computational budget across different parameter settings.We observe a general trend where higher computational investment yields better verification results.</p>
<p>Ablation Study To understand the contribution of each component in our approach, we conducted an ablation study on ProcessBench, with results shown in Figure 9.We evaluated four configurations: the greedy decoding method, Temporal Consistency without multi-agent, self-checking without iterative generation, and our method.The results demonstrate that both components contribute to the overall performance.contributing improvements of 24.2% and 25.8%, respectively.The combination achieves the best performance with an F1 score of 67.2%.</p>
<p>Related Work</p>
<p>Datasets and Benchmarks for Process Error Detection Process error detection in mathematical reasoning requires annotations at the step level, currently available in three major datasets.PRM800K [Lightman et al., 2023] pioneered this direction by providing human-annotated reasoning steps based on the MATH dataset [Hendrycks et al., 2021], focusing on high school and college-level mathematics.MathCheck [Zhou et al., 2024] extends this approach to elementary mathematics by synthesizing solutions with incorrect steps from GSM8K problems [Cobbe et al., 2021], offering a systematic evaluation of step-by-step verification.Most recently, ProcessBench [Zheng et al., 2024a] expands the coverage of mathematical difficulty by providing expert-annotated solution steps across four distinct datasets: GSM8K, MATH, and notably, OlympiadBench [He et al., 2024] and Omni-MATH [Gao et al., 2024] for competition and olympiad-level challenges.Our experimental evaluation across these benchmarks provides comprehensive insights into our method's effectiveness from basic arithmetic to advanced mathematical reasoning.</p>
<p>Process Error Identification Methods Approaches to error detection in language models can be categorized into two main streams.The first focuses on training specialized verification models, such as process reward models [Lightman et al., 2023, Luo et al., 2024a, Setlur et al., 2024, Wang et al., 2024, Zhang et al., 2024] and finetuned language models [Cobbe et al., 2021, Kang et al., 2024, Zheng et al., 2024b, Yang et al., 2024c, Tang et al., 2025, Luo et al., 2025, Guan et al., 2025].While these training-based methods have shown promising results, they require additional training data and significant computational resources, especially for larger models.The second stream explores inference-time verification through prompting techniques like self-reflection [Miao et al., 2023, Madaan et al., 2024].Recent work has demonstrated that language models often struggle to correct errors without external feedback [Huang et al., 2023, Kamoi et al., 2024].Similar to self-reflection work [Madaan et al., 2024, Yang et al., 2024c], which iteratively generates improvement suggestions, our method employs an iterative process.</p>
<p>Rather than training new models, we focus on utilizing existing models more effectively.However, our Self-check method can also be applied to trained verification models to improve their accuracy potentially.</p>
<p>More General Reasoning Methods The broader field of reasoning in language models has explored various frameworks to enhance problem-solving capabilities and solution reliability.Chain-of-Thought prompting [Wang et al., 2022] and its variants like Tree-of-Thought [Yao et al., 2024] and Buffer-of-Thought [Yang et al., 2024d] have demonstrated that explicitly articulating intermediate reasoning steps improves model performance on complex reasoning tasks, and Zhang et al. [2024] further validates the effectiveness of reasoning in verification tasks.Predesigned reasoning structures [Zhang et al., 2022, Besta et al., 2024, Yang et al., 2024c] have also shown promise in improving mathematical capabilities by guiding LLMs to think along predefined trajectories.Multi-agent approaches such as debate mechanisms [Du et al., 2023, Subramaniam et al., 2025] enable models to critically examine solutions through structured discussions, while majority voting methods [Wang et al., 2022] generate multiple independent solutions and aggregate them through majority voting to enhance reliability.While each approach offers unique advantages, they demonstrate the importance of structured reasoning processes in improving model performance.</p>
<p>Test Time Scaling Recent studies have demonstrated that leveraging multiple samples during inference can significantly enhance model performance [Hurst et al., 2024, Guo et al., 2025, Yang et al., 2025].Through iterative refinement, models incorporate feedback from previous generations to guide subsequent outputs [Snell et al., 2024, Hou et al., 2025, Lee et al., 2025].While early approaches focused on simple majority voting strategies [Wang et al., 2022], subsequent research has advanced towards more sophisticated techniques, particularly in search-based methods [Khanov et al., 2024, Wan et al., 2024, Yang et al., 2025].The field has evolved with hybrid frameworks that seamlessly integrate tree-based search with sequential approaches [Wu et al., 2024, Snell et al., 2024, Qiu et al., 2024, Gandhi et al., 2024].Liu et al. [2025] conducted a study on optimizing test-time computation scaling across various policy models and problem complexities.Most closely related to our approach, Muennighoff et al. [2025] achieved substantial improvements in competition math questions by implementing parallel self-reflection on historical interactions.</p>
<p>Conclusion</p>
<p>We presented an Temporal Consistency approach for improving mathematical process error identification in language models.Our method leverages temporal consistency patterns in verification behavior, allowing LLMs to recheck their judgments through multiple rounds.We demonstrated how this approach effectively improves verification accuracy across different models and problem types through empirical evaluation.</p>
<p>Our key insight is that the temporal stability of verifications can serve as a reliable indicator of correctness.This finding opens new directions for developing methods focusing on consistency over time rather than agreement across agents.Our results suggest that incorporating temporal dynamics can enhance the reliability of mathematical reasoning methods.</p>
<p>Limitations</p>
<p>While our Temporal Consistency approach demonstrates consistent improvements across different settings, it has several limitations.First, the method requires multiple verification rounds, leading to increased computational costs.Second, our method has been evaluated in the context of mathematical tasks, and it may not hold true for other reasoning tasks.</p>
<p>A Implementation Details</p>
<p>We use the gpt-4o-2024-08-06 API for GPT-4o and gpt-4o-mini API for GPT-4o-mini.We use Together API for the Deepseek-R1 model.All experiments can be performed on a single NVIDIA H100 GPU.</p>
<p>In the first round of all methods, the generation process was conducted using a temperature setting 0.7.The subsequent rounds vary slightly between closed-source and open-source models, with the following specifics:</p>
<p>• Closed-source models: For the debate method and our approach in later rounds, the temperature was set to 1.</p>
<p>• Open-source models: We used a fixed random seed of 42 throughout the experiments.For the debate method and subsequent rounds of our approach, the temperature was set to 0.7, and top-p=0.8,top-k=40.</p>
<p>A.1 Prompting Strategy for Initial Verification</p>
<p>In the first round of all methods, we utilized the verification prompts provided in Zheng et al. [2024a].The prompt format for the initial generation was:</p>
<p>The following is a math problem and a solution (split into paragraphs, enclosed with tags, and indexed from 0):
[Math Problem] {problem} [Solution]
{tagged_response} Your task is to review and critique the solution paragraph by paragraph.Once you identify an error in a paragraph, return the index where the earliest error occurs.Otherwise, return the index of -1 (which typically denotes "not found").</p>
<p>Please put your final answer (i.e., the index) in \boxed{}.</p>
<p>A.2 Debate Method Prompt Adaptation</p>
<p>The debate method is not designed for the verification task.To adapt it to our context, we combined the prompts for initial verification with those described in the appendix of Du et al. [2023].The adapted prompt is as follows:</p>
<p>These are the solutions to the problem from other agents: One agent solution: {res['reason']} Using the solutions from other agents as additional information, please analyze this solution and end with the earliest error index in \boxed{}, or -1 in \boxed{} if no errors.</p>
<p>A.3 Prompt for Secondary Verification</p>
<p>For the subsequent verification rounds in our approach, we designed a custom prompt to ensure thorough checking of the initial LLM's results.The prompt used is:</p>
<p>You are a secondary LLM for math problem solutions.The first verifier's task was to review math solutions paragraph by paragraph and identify the earliest error (if any), returning -1 if no errors were found.Problem: Julia was preparing for a dinner party at her house, where she intended to serve stew.She noticed that she was out of plastic spoons, so she bought a new package of spoons.Later, her husband also bought a package of 5 new spoons and gave them to Julia.While Julia was making the stew, she used three of the spoons to sample her stew.Later, when she went to set the table, she had a total of 12 spoons.How many spoons were in the package that Julia bought?Solution Steps:</p>
<p>Step 0: Let's break down the problem step by step: First, initially, Julia had no spoons.She then bought a new package of spoons.Her husband also bought a package of 5 new spoons and gave them to her.</p>
<p>Step 1: Second, in total, she received 5 + 5 = 10 new spoons from both her purchase and her husband's purchase.</p>
<p>First Error Step</p>
<p>Step 2: Third, while making the stew, she used 3 of the spoons to sample it, leaving her with 10 -3 = 7 spoons.</p>
<p>Step 3: Fourth, when she went to set the table, she had a total of 12 spoons.We need to find out how many spoons were in the package that Julia bought initially.</p>
<p>Step 4: We know she received 10 more spoons after her husband gave her the additional 5 spoons.Since she had 7 spoons left after sampling and ended up with 12 spoons in total, the number of spoons she started with must have been 12 -7 = 5.</p>
<p>Step 5: Therefore, the number of spoons in the package that Julia bought is 5. Agent Analysis: Agent 1:</p>
<p>• Paragraph &lt;0&gt; assumes Julia started with no spoons which isn't stated in problem • This is an incorrect assumption as we don't know initial spoon count • First error occurs in paragraph &lt;0&gt;</p>
<p>• Therefore labels error as paragraph 0</p>
<p>Agent 2:</p>
<p>• Paragraph &lt;0&gt; contains unstated assumption about initial spoon count • Making assumptions not given in original problem is an error • First error identified in step &lt;0&gt;</p>
<p>• Labels error as paragraph 0</p>
<p>Agent 3:</p>
<p>• Paragraph &lt;0&gt; correctly states initial conditions • Paragraph &lt;1&gt; makes incorrect assumption about total spoons being 10 • First error found in calculating total spoons without knowing Julia's package • Labels error as paragraph 1</p>
<p>Example for Change of Opinion After Debate (3 agents) Problem: Amy is taking a history test.She correctly answers 80% of the multiple-choice questions, 90% of the true/false questions, and 60% of the long-answer questions.The multiple-choice and true/false questions are worth 1 point each, and the long answer questions are worth 5 points each.How many points does Amy score if there are 10 multiple-choice questions, 20 true/false questions, and 5 long answer questions?Solution Steps:</p>
<p>Step 0: To calculate the total points that Amy scores on her history test, we need to determine how many points she earns from each type of question and then sum those points.</p>
<p>Step 1: First, for the multiple-choice questions: -There are 10 multiple-choice questions, each worth 1 point.-Amy answers 80% of them correctly.-Points from multiple-choice questions = 10 × 0.80 = 8 points.</p>
<p>Step 2: Second, for the true/false questions: -There are 20 true/false questions, each worth 1 point.-Amy answers 90% of them correctly.-Points from true/false questions = 20 × 0.90 = 18 points.</p>
<p>Step 3: Third, for the long-answer questions: -There are 5 long-answer questions, each worth 5 points.-Amy answers 60% of them correctly.-Points from long-answer questions = 5 × 0.60 × 5 = 15 points.</p>
<p>Step 4: Finally, to find the total points: -Total points = Points from multiple-choice + Points from true/false + Points from long-answer -Total points = 8 + 18 + 15 -Total points = 41</p>
<p>Step 5: Therefore, Amy scores a total of 41 points on her history test.</p>
<p>No Error (loc = −1) Agent Analysis: First Round:</p>
<p>• Agent 1: "After reviewing each paragraph step-by-step, no computational errors or mistakes in the logic of the solution are found.Therefore, index -1."• Agent 2: "Upon reviewing each paragraph, no errors are found.While the calculation method in paragraph 3 is unconventional, it yields the correct result.Therefore, index -1."• Agent 3: "Here, there is an error.The calculation should be 5 × 5 × 0.60 = 15 points, which is correct.</p>
<p>However, the error is in the explanation that wrongly states the operation order as 5 × 0.60 × 5.While the final result is correct, the explanation could be misleading.The first minor error in explanation is found in Paragraph &lt;3&gt;.Therefore, index 3."</p>
<p>Second Round (After Discussion):</p>
<p>• Agent 1: "Upon reviewing the solution and considering the feedback from other agents, I maintain that there are no computational errors affecting the final result.Index -1." • Agent 2: "After reviewing the solutions from other agents, I now see that while the calculation gives the correct result, the explanation in paragraph 3 can be misleading.As Agent 3 pointed out, the operation order should be more clearly stated.Therefore, I revise my assessment to index 3." • Agent 3: "Upon reviewing the solutions provided by other agents, I maintain my position.The calculation in paragraph 3, while giving the correct result, has a misleading explanation that could cause confusion.Index 3."</p>
<p>C Break Down of Evaluation Results</p>
<p>In this section, we provide the break down evaluation results in Table 1 and Table 2. Table 3 is the results for Mathcheck * , Table 4 are the results for PRM800K, Table 5 are the results for ProcessBench.</p>
<p>Figure 1 :
1
Figure 1: Performance improvements for various models on process error identification benchmarks.</p>
<p>Figure 3 :
3
Figure 3: Cost v.s.Performance across different methods and models on ProcessBench.The x-axis (logarithmic scale) shows the cost per problem in dollars (based on OpenRouter pricing 2 ), while the y-axis shows the F1 Score percentage.</p>
<p>... This step is correct… Paragraph 1... The first verifier is correct here.This is an error due to an incorrect assumption.Paragraph 0:...It does not calculate yet, so the first verifier is incorrect.Paragraph 1:...This paragraph is indeed incorrect because ... The earliest error index is 1.Paragraph 0:...The first verifier is wrong because ...Paragraph 1:...This paragraph incorrectly calculates... the first error index is 1</p>
<p>Figure 4 :
4
Figure 4: Example of the self-checking process: The first error occurred in step 1.Initially, two LLMs incorrectly identified the first incorrect step, while one correctly located the first incorrect step.After self-checking, all LLMs achieve the correct identification.</p>
<p>Figure 9 :
9
Figure9: Ablation study results for ProcessBench demonstrating the effectiveness of both iterative generation and multi-agent components, with their combination yielding the best performance.</p>
<p>→ 1 .
1
may have made mistakes.Your job is to carefully check their work.You will receive: The original math problem 2. The solution steps 3. The first verifier's generated label (paragraph index where they found the first error, or -1) → B Examples for Various Methods Example for Majority Voting (3 agents)</p>
<p>Table 1 :
1
Performance comparison across different models.Numbers represent F1 score (%).The best performance for each model is highlighted in bold.Our method consistently outperforms baselines across all models and benchmarks.
ModelMethodMathcheck  *  ProcessBench PRM800KGreedy Decoding78.852.934.0GPT-4o miniMajority Voting Multi-Model Debate80.4 79.954.2 54.637.9 38.0Temporal Consistency (Ours)84.858.239.0Greedy Decoding87.362.541.6GPT-4oMajority Voting Multi-Model Debate89.0 90.865.9 66.842.6 50.7Temporal Consistency (Ours)91.869.151.6Greedy Decoding13.36.42.4Llama 3.1 8B InstructMajority Voting Multi-Model Debate5.9 6.85.1 5.66.8 2.6Temporal Consistency (Ours)60.235.522.1Greedy Decoding26.420.313.0Mistral 7B Instruct v0.3Majority Voting Multi-Model Debate26.3 26.217.6 17.712.1 12.1Temporal Consistency (Ours)37.422.513.3</p>
<p>Table 2 :
2
Performance comparison of Deepseek R1 distilled models on three benchmarks.Numbers represent F1 score (%).The best performance for each model is highlighted in bold.
MethodMathcheck  *  ProcessBench PRM800KGreedy Decoding86.054.846.2Deepseek-R1-Qwen-7BMajority Voting Multi-Model Debate89.3 84.864.8 61.755.1 51.2Temporal Consistency (Ours)89.571.357.7Greedy Decoding35.929.321.2Deepseek-R1-Llama-8BMajority Voting Multi-Model Debate35.5 56.748.9 57.641.7 46.7Temporal Consistency (Ours)82.567.250.2
We use MathCheck * to denote a balanced dataset that combines MathCheck's process judging problems (containing only incorrect solutions) with problems with correct solutions from ProcessBench.
https://openrouter.ai
4. The first verifier's reasoning IMPORTANT:1.You must check each paragraph carefully, as if you are re-solving the problem from scratch.2. If you find any error-no matter how minor-locate the earliest paragraph containing that error.→ 3.If the solution is correct throughout, only then do you output -1.4. The first verifier may be wrong.You cannot just accept their result.Always verify carefully and do not hesitate to disagree.
The llama 3 herd of models. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, arXiv-2407202417arXiv e-prints</p>
<p>An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. 2024a</p>
<p>Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, arXiv:2412.16720Openai o1 system card. 2024arXiv preprint</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025. 1, 2, 7, 9arXiv preprint</p>
<p>Ling Yang, Zhaochen Yu, Bin Cui, Mengdi Wang, Reasonflux, arXiv:2502.06772Hierarchical llm reasoning via scaling thought templates. 202519arXiv preprint</p>
<p>Vineet Hunter Lightman, Yura Kosaraju, Harri Burda, John Edwards ; Leike, Schulman, arXiv:2305.20050Ilya Sutskever, and Karl Cobbe. Let's verify step by step. Bowen Baker, Teddy LeeJan. 2023. 1, 2, 689arXiv preprint</p>
<p>Improve mathematical reasoning in language models by automated process supervision. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, arXiv:2406.065922024a19arXiv preprint</p>
<p>Versaprm: Multi-domain process reward model via synthetic reasoning data. Thomas Zeng, Shuibai Zhang, Shutong Wu, Christian Classen, Daewon Chae, Ethan Ewer, Minjae Lee, Heeju Kim, Wonjun Kang, Jackson Kunde, Ying Fan, Jungtaek Kim, Hyung Il Koo, Kannan Ramchandran, Dimitris Papailiopoulos, Kangwook Lee, 2025</p>
<p>On the limited generalization capability of the implicit reward model induced by direct preference optimization. Yong Lin, Skyler Seto, Maartje Ter Hoeve, Katherine Metcalf, Barry-John Theobald, Xuan Wang, Yizhe Zhang, Chen Huang, Tong Zhang, 2024</p>
<p>Improve mathematical reasoning in language models by automated process supervision. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, Abhinav Rastogi, 2024b</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712022. 2, 569arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.143252023. 2, 5, 6, 7, 913arXiv preprint</p>
<p>Siyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Zhouhan Lin, arXiv:2410.10857Mirror-consistency: Harnessing inconsistency in majority voting. 2024arXiv preprint</p>
<p>Processbench: Identifying process errors in mathematical reasoning. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, Junyang Lin, 2024a713</p>
<p>Is your model really a good math reasoner? evaluating mathematical reasoning with checklist. Zihao Zhou, Shudong Liu, Maizhen Ning, Wei Liu, Jindong Wang, Derek F Wong, Xiaowei Huang, Qiufeng Wang, Kaizhu Huang, arXiv:2407.087332024. 2, 6, 8arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021. 5, 689arXiv preprint</p>
<p>Competition-level code generation with alphacode. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Science. 37866242022</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.0387420216arXiv preprint</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, arXiv:2402.14008202469arXiv preprint</p>
<p>Omni-math: A universal olympiad level mathematic benchmark for large language models. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, arXiv:2410.07985202469arXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.03493202269arXiv preprint</p>
<p>Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, Akila Ostrow, Alan Welihinda, Alec Hayes, Radford, arXiv:2410.21276Gpt-4o system card. 202479arXiv preprint</p>
<p>. Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.068252023Mistral 7b. arXiv preprint</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.151152024b5 technical report. arXiv preprint</p>
<p>Rewarding progress: Scaling automated process verifiers for llm reasoning. Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, Aviral Kumar, arXiv:2410.081462024arXiv preprint</p>
<p>Mathshepherd: Verify and reinforce llms step-by-step without human annotations. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, Zhifang Sui, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal, arXiv:2408.15240Generative verifiers: Reward modeling as next-token prediction. 2024arXiv preprint</p>
<p>Enhancing math reasoning in pre-trained llms at inference time. Jikun Kang, Xin Zhe Li, Xi Chen, Amirreza Kazemi, Qianyi Sun, Boxing Chen, Dong Li, Xu He, Quan He, Feng Wen, arXiv:2405.162652024arXiv preprint</p>
<p>Critic-cot: Boosting the reasoning abilities of large language model via chain-of-thoughts critic. Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han, Debing Zhang, Le Sun, arXiv:2408.163262024barXiv preprint</p>
<p>Supercorrect: Supervising and correcting language models with error-driven insights. Ling Yang, Zhaochen Yu, Tianjun Zhang, Minkai Xu, Joseph E Gonzalez, Bin Cui, Shuicheng Yan, arXiv:2410.090082024carXiv preprint</p>
<p>Enabling scalable oversight via self-evolving critic. Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, arXiv:2501.057272025arXiv preprint</p>
<p>Ruilin Luo, Zhuofan Zheng, Yifan Wang, Yiyao Yu, Xinzhe Ni, Zicheng Lin, Jin Zeng, Yujiu Yang, arXiv:2501.04686Understanding and verifying chain-of-thought reasoning in multimodal mathematics. Ursa2025arXiv preprint</p>
<p>rstar-math: Small llms can master math reasoning with self-evolved deep thinking. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang, arXiv:2501.045192025arXiv preprint</p>
<p>Selfcheck: Using llms to zero-shot check their own step-by-step reasoning. Ning Miao, Yee Whye Teh, Tom Rainforth, 2023</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Advances in Neural Information Processing Systems. 202436</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, arXiv:2310.017982023arXiv preprint</p>
<p>When can llms actually correct their own mistakes? a critical survey of self-correction of llms. Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, Rui Zhang, Transactions of the Association for Computational Linguistics. 1292024</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Buffer of thoughts: Thought-augmented reasoning with large language models. Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E Gonzalez, Bin Cui, Advances in Neural Information Processing Systems. 2024d</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Vighnesh Subramaniam, Yilun Du, Joshua B Tenenbaum, Antonio Torralba, Shuang Li, Igor Mordatch, arXiv:2501.05707Multiagent finetuning: Self improvement with diverse reasoning chains. 2025arXiv preprint</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. Charlie Snell, Jaehoon Lee, Kelvin Xu, Aviral Kumar, 2024</p>
<p>Advancing language model reasoning through reinforcement learning and inference scaling. Zhenyu Hou, Xin Lv, Rui Lu, Jiajie Zhang, Yujiang Li, Zijun Yao, Juanzi Li, Jie Tang, Yuxiao Dong, arXiv:2501.116512025arXiv preprint</p>
<p>Kuang-Huei Lee, Ian Fischer, Yueh-Hua Wu, Dave Marwood, Shumeet Baluja, Dale Schuurmans, Xinyun Chen, arXiv:2501.09891Evolving deeper llm thinking. 2025arXiv preprint</p>
<p>Maxim Khanov, Jirayu Burapacheep, Yixuan Li, arXiv:2402.01694Args: Alignment as reward-guided search. 2024arXiv preprint</p>
<p>Alphazerolike tree-search can guide large language model decoding and training. Ziyu Wan, Xidong Feng, Muning Wen, Stephen Marcus Mcaleer, Ying Wen, Weinan Zhang, Jun Wang, Forty-first International Conference on Machine Learning. 2024</p>
<p>Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, Yiming Yang, arXiv:2408.007242024arXiv preprint</p>
<p>Treebon: Enhancing inference-time alignment with speculative tree-search and best-of-n sampling. Jiahao Qiu, Yifu Lu, Yifan Zeng, Jiacheng Guo, Jiayi Geng, Huazheng Wang, Kaixuan Huang, Yue Wu, Mengdi Wang, arXiv:2410.160332024arXiv preprint</p>
<p>Stream of search (sos): Learning to search in language. Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, Noah D Goodman, arXiv:2404.036832024arXiv preprint</p>
<p>Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, Bowen Zhou, arXiv:2502.067032025arXiv preprint</p>
<p>Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang , Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto, arXiv:2501.19393Simple test-time scaling. 20251arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
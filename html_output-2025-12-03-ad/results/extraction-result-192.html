<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-192 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-192</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-192</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-10.html">extraction-schema-10</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <p><strong>Paper ID:</strong> paper-984d4a1d41bfc8184fb77b8aa0eb8e96d536d048</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048" target="_blank">Trusting Your Evidence: Hallucinate Less with Context-aware Decoding</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, significantly improves the faithfulness of different LM families.</p>
                <p><strong>Paper Abstract:</strong> Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA, and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model’s prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential. Our code is publicly released at https://github.com/xhan77/context-aware-decoding.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e192.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e192.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context-aware Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An inference-time contrastive decoding method that upweights tokens that become more likely when a provided context is included by multiplying the original distribution with a PMI-like factor between context and output (implemented as a logit adjustment with hyperparameter α).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OPT, GPT-Neo, LLaMA, FLAN-T5 (evaluated across several families)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (3B–30B reported)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Summarization and knowledge-conflict question answering where outputs should be faithful to provided context.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Context documents or instructions containing factual statements or contradictory information (retrieved/modified documents, explicit instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Ground-truth or modified/supporting documents and prompt instructions provided as input context.</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>mixed (often contradictory in knowledge-conflict settings, sometimes aligned in standard QA/summarization)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Baseline (regular decoding) scores vary by model and task; examples: LLaMA-30B on CNN-DM ROUGE-L 25.8, factKB 76.8; GPT-Neo 20B on NQ-SWAP EM 16.1; MemoTrap GPT-Neo 3B EM 22.5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>With CAD (α>0) scores improved substantially in most settings; examples: LLaMA-30B CNN-DM ROUGE-L 31.8 (+6.0), factKB 87.8 (+11.0); GPT-Neo 20B NQ-SWAP EM 36.8 (+20.7); GPT-Neo 3B MemoTrap EM 47.3 (+24.8). (All numbers are percentages where applicable.)</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive — adding context as evidence and using CAD increased accuracy/factuality metrics and caused the model to prefer outputs supported by provided evidence (large increases in factuality and EM in knowledge-conflict datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>CAD factors out the model's prior (parametric) distribution by boosting token probabilities that show high PMI with the context (implemented as (1+α)*logit_with_context - α*logit_without_context), thereby demoting memorized priors and encouraging generation that matches the provided evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Directly providing context often does not suffice because models can over-rely on parametric priors; CAD (contrastive, PMI-like logit adjustment) reliably increases attention to context and substantially improves factuality and correctness across summarization and knowledge-conflict QA tasks without additional training.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Trusting Your Evidence: Hallucinate Less with Context-aware Decoding', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e192.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e192.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior-dominance / context-ignoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model reliance on parametric prior instead of input context</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed behavior where pretrained LMs ignore up-to-date or contradictory context evidence and continue to produce outputs consistent with outdated or memorized parametric knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Trusting Your Evidence: Hallucinate Less with Context-aware Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (example given)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unspecified in anecdote (examples include 13B/30B evaluated elsewhere)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Question answering where the context contains updated facts contradicting the model's training-time knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Factual statements in the context that contradict model's stored knowledge (e.g., updated news text).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Provided context document (ground-truth/current document).</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>contradictory (context contradicts model's prior knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Model answers according to prior knowledge (example: LLaMA predicted 'Two' World Cups won by Argentina based on parametric knowledge).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>Despite the context asserting 'Argentina won ... 1978, 1986 and 2022', the model still predicted 'Two' — i.e., the provided evidence did not change the prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>negative/neutral — adding the (contradictory) evidence did not change the model's output; the model continued to prefer its prior.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Insufficient attention to context: the model overweights parametric memorized knowledge learned during pretraining and underweights new contextual signals presented in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Raw inclusion of up-to-date or corrective evidence can fail to alter LM predictions because the parametric prior can dominate; larger models can be more prone to this prior reliance in conflict scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Trusting Your Evidence: Hallucinate Less with Context-aware Decoding', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e192.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e192.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NQ-SWAP results (contrast effect)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NQ-SWAP knowledge-conflict QA performance with and without context-aware decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical measurements showing that providing a modified supportive document (contradictory evidence) alone often yields low EM; applying CAD substantially increases EM, indicating better incorporation of provided evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Trusting Your Evidence: Hallucinate Less with Context-aware Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>20B</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Knowledge-conflict QA (NQ-SWAP): answer questions given a document whose gold answer entity has been replaced (the model should output the replaced entity).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Modified supportive document containing a contradictory (replaced) answer entity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Synthetic modification of gold supportive documents (NQ-SWAP dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>contradictory (document contradicts model's memorized gold entity)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Regular decoding EM = 16.1%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>With CAD EM = 36.8%</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive — accuracy (EM) increased markedly (~+20.7 percentage points), indicating stronger adherence to the document evidence when CAD is applied.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>CAD demotes the influence of the model's prior (via subtracting logits from the context-free distribution) and amplifies tokens that the context makes more probable, enabling the model to prefer the replaced entity in the document.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>When evidence in the document contradicts model priors, baseline decoding often fails; contrastive decoding (CAD) substantially improves fidelity to the provided evidence in knowledge-conflict QA.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Trusting Your Evidence: Hallucinate Less with Context-aware Decoding', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e192.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e192.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoTrap results</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoTrap instruction-following under memorization trap</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical result showing models often produce memorized/common completions (ignoring the prompt instruction) unless encouraged to follow context via CAD; CAD greatly increases correct adherence to the prompt instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Trusting Your Evidence: Hallucinate Less with Context-aware Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Instruction completion where the instruction requests a non-memorized ending for a well-known proverb (MemoTrap).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Instruction/context explicitly stating the desired (non-standard) ending word.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Human-designed prompt instructions (MemoTrap dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>contradictory (desired output contradicts the common memorized completion)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Regular decoding EM = 22.5%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>With CAD EM = 47.3%</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>positive — adding the instruction-context and applying CAD increased correct adherence to the instruction by ~24.8 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>By contrasting logits with and without the instruction context, CAD upweights tokens that the instruction makes relatively more likely and downweights memorized defaults.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Standard decoding frequently falls back to memorized/common completions despite explicit instruction context; CAD substantially mitigates this, improving instruction-following accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Trusting Your Evidence: Hallucinate Less with Context-aware Decoding', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e192.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e192.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models respond to evidence provided in prompts, particularly cases where evidence affects model confidence, probability assessments, or truthfulness judgments, including both positive and negative effects of adding evidence.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5 degradation on NQ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Small negative effect of CAD on instruction-finetuned FLAN-T5 for non-conflict NQ</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minor performance decrease when applying CAD to an instruction-finetuned model (FLAN-T5) on a standard (non-conflict) NQ set, suggesting CAD can slightly harm performance when the context aligns with strong parametric knowledge or training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Trusting Your Evidence: Hallucinate Less with Context-aware Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B (FLAN-T5 XL) and 11B (XXL) reported; example uses 3B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Standard QA (NQ) where context aligns with the model's training/finetuning data (non-conflict).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>Supportive context/document aligned with model's parametric knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Original gold supportive documents (NQ dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>parametric_knowledge_alignment</strong></td>
                            <td>aligned</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_evidence</strong></td>
                            <td>Regular decoding accuracy (EM) for FLAN 3B on NQ = 81.8%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_evidence</strong></td>
                            <td>With CAD accuracy = 80.3% (a slight decrease of 1.5 percentage points).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_effect</strong></td>
                            <td>mixed/negative (small negative effect) — CAD slightly decreased accuracy on this aligned, non-conflict dataset for an instruction-finetuned model.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_decreased_confidence</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>proposed_mechanism</strong></td>
                            <td>Possible over-adjustment: CAD can reweight distributions even when the model's parametric knowledge is already correct or closely aligned to the context (and instruction-finetuned models may already strongly incorporate context), leading to slight degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Although CAD broadly improves adherence to context in conflict settings, it can slightly harm performance on some instruction-finetuned models and non-conflict tasks, indicating α selection and model/task calibration matter.</td>
                        </tr>
                        <tr>
                            <td><strong>counterintuitive_behavior</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Trusting Your Evidence: Hallucinate Less with Context-aware Decoding', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Entity-based knowledge conflicts in question answering <em>(Rating: 2)</em></li>
                <li>The memotrap dataset <em>(Rating: 2)</em></li>
                <li>Context-faithful prompting for large language models <em>(Rating: 2)</em></li>
                <li>Coherence boosting: When your pretrained language model is not paying enough attention <em>(Rating: 1)</em></li>
                <li>DExperts: Decoding-time controlled text generation with experts and anti-experts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-192",
    "paper_id": "paper-984d4a1d41bfc8184fb77b8aa0eb8e96d536d048",
    "extraction_schema_id": "extraction-schema-10",
    "extracted_data": [
        {
            "name_short": "CAD",
            "name_full": "Context-aware Decoding",
            "brief_description": "An inference-time contrastive decoding method that upweights tokens that become more likely when a provided context is included by multiplying the original distribution with a PMI-like factor between context and output (implemented as a logit adjustment with hyperparameter α).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "OPT, GPT-Neo, LLaMA, FLAN-T5 (evaluated across several families)",
            "model_size": "various (3B–30B reported)",
            "task_description": "Summarization and knowledge-conflict question answering where outputs should be faithful to provided context.",
            "evidence_type": "Context documents or instructions containing factual statements or contradictory information (retrieved/modified documents, explicit instructions).",
            "evidence_source": "Ground-truth or modified/supporting documents and prompt instructions provided as input context.",
            "parametric_knowledge_alignment": "mixed (often contradictory in knowledge-conflict settings, sometimes aligned in standard QA/summarization)",
            "performance_without_evidence": "Baseline (regular decoding) scores vary by model and task; examples: LLaMA-30B on CNN-DM ROUGE-L 25.8, factKB 76.8; GPT-Neo 20B on NQ-SWAP EM 16.1; MemoTrap GPT-Neo 3B EM 22.5.",
            "performance_with_evidence": "With CAD (α&gt;0) scores improved substantially in most settings; examples: LLaMA-30B CNN-DM ROUGE-L 31.8 (+6.0), factKB 87.8 (+11.0); GPT-Neo 20B NQ-SWAP EM 36.8 (+20.7); GPT-Neo 3B MemoTrap EM 47.3 (+24.8). (All numbers are percentages where applicable.)",
            "evidence_effect": "positive — adding context as evidence and using CAD increased accuracy/factuality metrics and caused the model to prefer outputs supported by provided evidence (large increases in factuality and EM in knowledge-conflict datasets).",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "CAD factors out the model's prior (parametric) distribution by boosting token probabilities that show high PMI with the context (implemented as (1+α)*logit_with_context - α*logit_without_context), thereby demoting memorized priors and encouraging generation that matches the provided evidence.",
            "key_findings": "Directly providing context often does not suffice because models can over-rely on parametric priors; CAD (contrastive, PMI-like logit adjustment) reliably increases attention to context and substantially improves factuality and correctness across summarization and knowledge-conflict QA tasks without additional training.",
            "counterintuitive_behavior": false,
            "uuid": "e192.0",
            "source_info": {
                "paper_title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Prior-dominance / context-ignoring",
            "name_full": "Model reliance on parametric prior instead of input context",
            "brief_description": "Observed behavior where pretrained LMs ignore up-to-date or contradictory context evidence and continue to produce outputs consistent with outdated or memorized parametric knowledge.",
            "citation_title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
            "mention_or_use": "use",
            "model_name": "LLaMA (example given)",
            "model_size": "unspecified in anecdote (examples include 13B/30B evaluated elsewhere)",
            "task_description": "Question answering where the context contains updated facts contradicting the model's training-time knowledge.",
            "evidence_type": "Factual statements in the context that contradict model's stored knowledge (e.g., updated news text).",
            "evidence_source": "Provided context document (ground-truth/current document).",
            "parametric_knowledge_alignment": "contradictory (context contradicts model's prior knowledge)",
            "performance_without_evidence": "Model answers according to prior knowledge (example: LLaMA predicted 'Two' World Cups won by Argentina based on parametric knowledge).",
            "performance_with_evidence": "Despite the context asserting 'Argentina won ... 1978, 1986 and 2022', the model still predicted 'Two' — i.e., the provided evidence did not change the prediction.",
            "evidence_effect": "negative/neutral — adding the (contradictory) evidence did not change the model's output; the model continued to prefer its prior.",
            "evidence_decreased_confidence": null,
            "proposed_mechanism": "Insufficient attention to context: the model overweights parametric memorized knowledge learned during pretraining and underweights new contextual signals presented in the prompt.",
            "key_findings": "Raw inclusion of up-to-date or corrective evidence can fail to alter LM predictions because the parametric prior can dominate; larger models can be more prone to this prior reliance in conflict scenarios.",
            "counterintuitive_behavior": true,
            "uuid": "e192.1",
            "source_info": {
                "paper_title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "NQ-SWAP results (contrast effect)",
            "name_full": "NQ-SWAP knowledge-conflict QA performance with and without context-aware decoding",
            "brief_description": "Empirical measurements showing that providing a modified supportive document (contradictory evidence) alone often yields low EM; applying CAD substantially increases EM, indicating better incorporation of provided evidence.",
            "citation_title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
            "mention_or_use": "use",
            "model_name": "GPT-Neo",
            "model_size": "20B",
            "task_description": "Knowledge-conflict QA (NQ-SWAP): answer questions given a document whose gold answer entity has been replaced (the model should output the replaced entity).",
            "evidence_type": "Modified supportive document containing a contradictory (replaced) answer entity.",
            "evidence_source": "Synthetic modification of gold supportive documents (NQ-SWAP dataset).",
            "parametric_knowledge_alignment": "contradictory (document contradicts model's memorized gold entity)",
            "performance_without_evidence": "Regular decoding EM = 16.1%",
            "performance_with_evidence": "With CAD EM = 36.8%",
            "evidence_effect": "positive — accuracy (EM) increased markedly (~+20.7 percentage points), indicating stronger adherence to the document evidence when CAD is applied.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "CAD demotes the influence of the model's prior (via subtracting logits from the context-free distribution) and amplifies tokens that the context makes more probable, enabling the model to prefer the replaced entity in the document.",
            "key_findings": "When evidence in the document contradicts model priors, baseline decoding often fails; contrastive decoding (CAD) substantially improves fidelity to the provided evidence in knowledge-conflict QA.",
            "counterintuitive_behavior": false,
            "uuid": "e192.2",
            "source_info": {
                "paper_title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "MemoTrap results",
            "name_full": "MemoTrap instruction-following under memorization trap",
            "brief_description": "Empirical result showing models often produce memorized/common completions (ignoring the prompt instruction) unless encouraged to follow context via CAD; CAD greatly increases correct adherence to the prompt instruction.",
            "citation_title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
            "mention_or_use": "use",
            "model_name": "GPT-Neo",
            "model_size": "3B",
            "task_description": "Instruction completion where the instruction requests a non-memorized ending for a well-known proverb (MemoTrap).",
            "evidence_type": "Instruction/context explicitly stating the desired (non-standard) ending word.",
            "evidence_source": "Human-designed prompt instructions (MemoTrap dataset).",
            "parametric_knowledge_alignment": "contradictory (desired output contradicts the common memorized completion)",
            "performance_without_evidence": "Regular decoding EM = 22.5%",
            "performance_with_evidence": "With CAD EM = 47.3%",
            "evidence_effect": "positive — adding the instruction-context and applying CAD increased correct adherence to the instruction by ~24.8 percentage points.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "By contrasting logits with and without the instruction context, CAD upweights tokens that the instruction makes relatively more likely and downweights memorized defaults.",
            "key_findings": "Standard decoding frequently falls back to memorized/common completions despite explicit instruction context; CAD substantially mitigates this, improving instruction-following accuracy.",
            "counterintuitive_behavior": false,
            "uuid": "e192.3",
            "source_info": {
                "paper_title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "FLAN-T5 degradation on NQ",
            "name_full": "Small negative effect of CAD on instruction-finetuned FLAN-T5 for non-conflict NQ",
            "brief_description": "A minor performance decrease when applying CAD to an instruction-finetuned model (FLAN-T5) on a standard (non-conflict) NQ set, suggesting CAD can slightly harm performance when the context aligns with strong parametric knowledge or training distribution.",
            "citation_title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
            "mention_or_use": "use",
            "model_name": "FLAN-T5",
            "model_size": "3B (FLAN-T5 XL) and 11B (XXL) reported; example uses 3B)",
            "task_description": "Standard QA (NQ) where context aligns with the model's training/finetuning data (non-conflict).",
            "evidence_type": "Supportive context/document aligned with model's parametric knowledge.",
            "evidence_source": "Original gold supportive documents (NQ dataset).",
            "parametric_knowledge_alignment": "aligned",
            "performance_without_evidence": "Regular decoding accuracy (EM) for FLAN 3B on NQ = 81.8%",
            "performance_with_evidence": "With CAD accuracy = 80.3% (a slight decrease of 1.5 percentage points).",
            "evidence_effect": "mixed/negative (small negative effect) — CAD slightly decreased accuracy on this aligned, non-conflict dataset for an instruction-finetuned model.",
            "evidence_decreased_confidence": false,
            "proposed_mechanism": "Possible over-adjustment: CAD can reweight distributions even when the model's parametric knowledge is already correct or closely aligned to the context (and instruction-finetuned models may already strongly incorporate context), leading to slight degradation.",
            "key_findings": "Although CAD broadly improves adherence to context in conflict settings, it can slightly harm performance on some instruction-finetuned models and non-conflict tasks, indicating α selection and model/task calibration matter.",
            "counterintuitive_behavior": true,
            "uuid": "e192.4",
            "source_info": {
                "paper_title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Entity-based knowledge conflicts in question answering",
            "rating": 2,
            "sanitized_title": "entitybased_knowledge_conflicts_in_question_answering"
        },
        {
            "paper_title": "The memotrap dataset",
            "rating": 2,
            "sanitized_title": "the_memotrap_dataset"
        },
        {
            "paper_title": "Context-faithful prompting for large language models",
            "rating": 2,
            "sanitized_title": "contextfaithful_prompting_for_large_language_models"
        },
        {
            "paper_title": "Coherence boosting: When your pretrained language model is not paying enough attention",
            "rating": 1,
            "sanitized_title": "coherence_boosting_when_your_pretrained_language_model_is_not_paying_enough_attention"
        },
        {
            "paper_title": "DExperts: Decoding-time controlled text generation with experts and anti-experts",
            "rating": 1,
            "sanitized_title": "dexperts_decodingtime_controlled_text_generation_with_experts_and_antiexperts"
        }
    ],
    "cost": 0.01284925,
    "model_str": null
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Trusting Your Evidence: Hallucinate Less with Context-aware Decoding</h1>
<p>Weijia Shi ${ }^{1,2 <em>}$ Xiaochuang Han ${ }^{1,2 </em>}$<br>Mike Lewis ${ }^{2}$ Yulia Tsvetkov ${ }^{1}$ Luke Zettlemoyer ${ }^{1,2}$ Wen-tau Yih ${ }^{2}$<br>${ }^{1}$ University of Washington, Seattle, WA, ${ }^{2}$ FAIR, Meta<br>{swj0419, xhan77}@uw.edu</p>
<h4>Abstract</h4>
<p>Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLANT5 for summarization tasks (e.g., $14.3 \%$ gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential. Our code is publicly released at https://github.com/ xhan77/context-aware-decoding.</p>
<h2>1 Introduction</h2>
<p>Language models (LMs) are effective in generating fluent continuations of a prompt or document prefix. During generation, they rely on two sources of knowledge: (1) prior knowledge, which is learned during pretraining and stored implicitly within the model parameters; (2) context knowledge, which is passed as inputs in the prefix context (Chan et al., 2022). However, it remains an open question how a pretrained LM, particularly a vanilla LM without task-specific finetuning, balances these two knowledge sources during generation.</p>
<p>Previous research shows that LMs can fail to pay enough attention to new information introduced in the context knowledge. This can lead to hallucination in summarization (Maynez et al., 2020; Pagnoni et al., 2021), where the generated summaries include facts not present in the input doc-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of context-aware decoding.
ument. Insufficient attention to context is especially problematic when the context knowledge contradicts with the prior knowledge (Longpre et al., 2021; Zhou et al., 2023). For instance, when LLaMA (Touvron et al., 2023) is presented with a latest document "Argentina won the FIFA World Cups in 1978, 1986 and 2022 ..." in its context (Figure 1), it still predicts "Two" in response to the question "How many World Cups have Argentina won?", due in part to the outdated training data.</p>
<p>In this work, we present a simple context-aware decoding (CAD) method to encourage the LM to attend to its context during generation. As shown in Figure 1, CAD samples from a new output distribution, which amplifies the difference between output probabilities with and without the context document. This provides a new form of contrastive decoding (Li et al., 2023), which effectively downweights the prior knowledge when more relevant contextual information is provided. CAD can be used with off-the-shelf pretrained language models without any additional training.</p>
<p>Experimental results from summarization tasks show that context-aware decoding significantly enhances the generation faithfulness of vanilla LMs including OPT (Zhang et al., 2022), GPTNeo (Black et al., 2021), LLaMA (Touvron et al., 2023) and instruction-finetuned LMs such as FLAN (Chung et al., 2022). For instance, when applied to LLaMA-30B in CNN-DM, CAD leads to</p>
<p>substantial improvement in both ROUGE-L (21\%) and factuality evaluation metrics ( $14.3 \%$ ). More notably, CAD is especially beneficial for knowledge conflicting tasks, where the context contains information contradictory to the model's prior knowledge. CAD brings a 2.9 x improvement to LLaMA30B on a knowledge conflicts QA dataset (Longpre et al., 2021). Furthermore, we observe that this gain brought by CAD increases as the model size grows in knowledge conflicts tasks. These results demonstrate the potential of CAD in mitigating hallucinations in text generation and overriding prior knowledge with reliable and trusted information.</p>
<h2>2 Method</h2>
<h3>2.1 Background</h3>
<p>Given a LM $\theta$, an input query $\boldsymbol{x}$, and a context $\boldsymbol{c}$ that contains some external knowledge unfamiliar or in conflict to the model's prior knowledge, we ask our model $\theta$ to generate a response $\boldsymbol{y}$ given the the query and context. The response can be directly sampled (autoregressively) from the probability distribution conditioned on query $\boldsymbol{x}$ and context $\boldsymbol{c}$ :</p>
<p>$$
\begin{aligned}
&amp; y_{t} \sim p_{\theta}\left(y_{t} \mid \boldsymbol{c}, \boldsymbol{x}, \boldsymbol{y}<em _theta="\theta">{&lt;t}\right) \
&amp; \propto \operatorname{exp} \operatorname{logit}</em>\right)
\end{aligned}
$$}\left(y_{t} \mid \boldsymbol{c}, \boldsymbol{x}, \boldsymbol{y}_{&lt;t</p>
<p>However, in cases where the context $\boldsymbol{c}$ contains knowledge that is out-of-distribution with respect to $\theta$, we hypothesize that the model can struggle to effectively attend to $\boldsymbol{c}$ and overly rely on the prior knowledge encoded in $\theta$. For instance, as illustrated in Figure 1, when the context $\boldsymbol{c}$ states "Argentina won the FIFA World Cups in 1978, 1986 and 2022 ...", it contradicts the LM's outdated prior knowledge that Argentina has won the World Cup twice. The language model may still incorrectly predict "Two" even when presented with the context $\boldsymbol{c}$ and the query $\boldsymbol{x}$.</p>
<h3>2.2 Context-aware Decoding</h3>
<p>To mitigate such issues, we factor out the prior knowledge from the model's original output distribution contrastively. Here, we model the prior knowledge as $p_{\theta}\left(y_{t} \mid \boldsymbol{x}, \boldsymbol{y}<em t="t">{&lt;t}\right)$ and adjust the model's original output probability distribution using the pointwise mutual information (PMI) between the context $\boldsymbol{c}$ and the generation $y</em>$. Formally, we have:}$, conditioned on $\boldsymbol{x}, \boldsymbol{y}_{&lt;t</p>
<p>$$
\begin{aligned}
&amp; y_{t} \sim \tilde{p}<em t="t">{\theta}\left(y</em>} \mid \boldsymbol{c}, \boldsymbol{x}, \boldsymbol{y<em _theta="\theta">{&lt;t}\right) \
&amp; \propto p</em>}\left(y_{t} \mid \boldsymbol{c}, \boldsymbol{x}, \boldsymbol{y<em _theta="\theta">{&lt;t}\right)\left(\frac{p</em>}\left(y_{t} \mid \boldsymbol{c}, \boldsymbol{x}, \boldsymbol{y<em _theta="\theta">{&lt;t}\right)}{p</em>
\end{aligned}
$$}\left(y_{t} \mid \boldsymbol{x}, \boldsymbol{y}_{&lt;t}\right)}\right)^{\alpha</p>
<p>where the output probability is a product-of-experts of the original output probability and PMI weighted by $\alpha$. Essentially, outputs that become much more likely when the context is included are preferred (Figure 1).</p>
<p>This expression is not a valid probability distribution and needs to be normalized across all possible values of $y_{t}$. By rearranging the terms, we obtain the final form:</p>
<p>$$
\begin{gathered}
y_{t} \sim \operatorname{softmax}\left[(1+\alpha) \operatorname{logit}<em t="t">{\theta}\left(y</em>} \mid \boldsymbol{c}, \boldsymbol{x}, \boldsymbol{y<em _theta="\theta">{&lt;t}\right)\right. \
\left.-\alpha \operatorname{logit}</em>\right)\right]
\end{gathered}
$$}\left(y_{t} \mid \boldsymbol{x}, \boldsymbol{y}_{&lt;t</p>
<p>Larger $\alpha$ means more weight on our adjustment ( $\alpha=0$ reduces to regular decoding). ${ }^{1}$ We refer to this simple method as context-aware decoding. From the adjusted output distribution $\tilde{p}$, we can apply various sampling strategies, such as nucleus sampling (Holtzman et al., 2020).</p>
<p>Essentially, context-aware decoding is just a contrastive ensemble between the logits of $p_{\theta}\left(y_{t}\right.$ $c, \boldsymbol{x}, \boldsymbol{y}<em _theta="\theta">{&lt;t}$ ) and $p</em>}\left(y_{t} \mid \boldsymbol{x}, \boldsymbol{y<em _theta="\theta">{&lt;t}\right)$. A similar contrastive objective is universal in image generation, where classifier-free diffusion models (Ho and Salimans, 2022) predict diffusion noise with $(1+\alpha) \epsilon</em>$ being a control to the image. In text generation, Malkin et al. (2022) propose coherence boosting with the same intuition, with a focus on contrasting the full input and a short premise-free input, promoting coherence w.r.t. the long context. Instead of using a single model $\theta$ in this work, different models can also be used in the distribution adjustments to demote unwanted model behaviors or distill expert model's capability (Liu et al., 2021; Li et al., 2023). We further discuss related works in $\S 6$ and $\S$ A.2.}(\boldsymbol{x}, \boldsymbol{c})-\alpha \epsilon_{\theta}(\boldsymbol{x})$, with $\boldsymbol{c</p>
<h2>3 Experimental Setup</h2>
<p>We perform evaluation on tasks that require LMs to read and reason over contexts and produce outputs that are faithful to the contexts. Following prior work (Zhang et al., 2024; Zhou et al., 2023), we evaluate the models using prompting.</p>
<h3>3.1 Datasets and Metrics</h3>
<p>Summarization We conduct summarization experiments on CNN-DM (See et al., 2017) and XSUM (Narayan et al., 2018). We use ROUGEL (Lin, 2004) to evaluate summarization quality.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To measure the factual consistency of summaries, we adopt state-of-the-art factuality evaluation metrics: BERT-Precision (Pagnoni et al., 2021) and FactKB (Feng et al., 2023), which has been demonstrated to achieve high correlations with human judgment on the summarization datasets, outperforming other metrics such as FACTCC (Kryscinski et al., 2020) and SUMMAC (Laban et al., 2022).</p>
<p>Knowledge Conflicts We evaluate performance on two knowledge conflict datasets: MemoTrap (Liu and Liu, 2023) and NQ-Swap (Longpre et al., 2021). MemoTrap is created to investigate whether language models fall into memorization traps. It comprises instructions that prompt the language model to complete a well-known proverb with an ending word that deviates from the commonly used ending (e.g., Write a quote that ends in the word "early": Better late than __). NQ-Swap is based on a QA dataset, natural questions (NQ) (Kwiatkowski et al., 2019), where the objective is to answer questions based on a gold document. To generate NQSwap, Longpre et al. (2021) identify questions in NQ with named entity answers, find the supportive document for each question and replace the gold answer entity in the document with a random entity. A faithful LM should generate the replaced entity as the answer when given the question and modified document. We also include the original NQ dataset with the question and original document for evaluation. We use Exact Match (EM) as the evaluation metric for NQ-Swap, NQ and MemoTrap.</p>
<p>In Table 1, we show illustrative examples of the contexts we aim to upweight for the model and the queries across different datasets. We hope LMs pay more attention to the source document in XSUM and NQ-Swap. On the other hand, we hope LMs focus more on the instruction in MemoTrap.</p>
<h3>3.2 Models and Baselines</h3>
<p>We apply CAD to pretrained language models including OPT (Zhang et al., 2022), GPT-Neo (Black et al., 2021), LLaMA (Touvron et al., 2023) and instruction-finetuned LMs such as FLAN-T5 (Chung et al., 2022).</p>
<p>CAD introduces a hyperparameter $\alpha$ to control the adjustment level. We set $\alpha=0.5$ for all models evaluated on the summarization datasets and $\alpha=1$ for all models evaluated on the knowledge conflict datasets. We observed that $\alpha=0.5$ generally yielded good results across all settings and all datasets, but a slightly higher $\alpha$ is more effec-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">XSUM</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">c</td>
<td style="text-align: center;">Article: Prison Link Cymru had 1,099 referrals in 2015-16 and said some ex-offenders were living rough for up to a year before finding suitable accommodation ...</td>
</tr>
<tr>
<td style="text-align: center;">$x$</td>
<td style="text-align: center;">Summarize the article in one sentence. Summary:</td>
</tr>
<tr>
<td style="text-align: center;">NQ-SWAP</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">c</td>
<td style="text-align: center;">Tesla CEO Elon Musk is now in charge of Twitter, CNBC has learned ...</td>
</tr>
<tr>
<td style="text-align: center;">$x$</td>
<td style="text-align: center;">Who is Twitter CEO now?</td>
</tr>
<tr>
<td style="text-align: center;">MemoTrap</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">c</td>
<td style="text-align: center;">Write a quote that ends in the word "early":</td>
</tr>
<tr>
<td style="text-align: center;">$x$</td>
<td style="text-align: center;">Better late than</td>
</tr>
</tbody>
</table>
<p>Table 1: An illustation of the inputs to CAD applied to each dataset. CAD upweights the context $\boldsymbol{c}$ (in red) by sampling each token from $\operatorname{softmax}\left[(1+\alpha) \operatorname{logit}<em t="t">{\theta}\left(y</em>} \mid\right.\right.$ $\left.\boldsymbol{c}, \boldsymbol{x}, \boldsymbol{y<em _theta="\theta">{&lt;t}\right)-\alpha \operatorname{logit}</em>\right)\right]$.
tive in the knowledge conflict setting, where the prior knowledge needs to be factored out more. We investigate the effect of $\alpha$ in Section 5.}\left(y_{t} \mid \boldsymbol{x}, \boldsymbol{y}_{&lt;t</p>
<p>For the baselines, we use the regular decoding methods following prior work (Longpre et al., 2021; Kwiatkowski et al., 2019): greedy decoding for knowledge conflict tasks and top- $p$ sampling with $p=0.9$ for summarization tasks (Holtzman et al., 2020). For CAD, we use the same sampling strategies on top of the adjusted output probability distribution.</p>
<h2>4 Results</h2>
<p>Summarization Table 2 reports the results on CNN-DM and XSUM. We observe that CAD outperforms the standard decoding algorithm by a large margin in all eight models across both datasets. Specifically, when applied to LLaMA30B in CNN-DM, CAD leads to $21 \%$ increase in ROUGE-L, $14.3 \%$ increase in factKB and $7.8 \%$ increase in BERT-P. This result demonstrates that CAD could effectively improve the quality and factuality of the generated summaries from a diverse set of language models.
Knowledge Conflicts Our results for the knowledge conflict datasets, NQ-SWAP and MemoTrap, as well as the original NQ are detailed in Table 3. CAD is significantly better than the regular decoding in all settings, with the exception of a minor decrease observed for FLAN-T5 on the non-conflict NQ dataset. ${ }^{2}$ Despite this, CAD achieves better per-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CNN-DM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">XSUM</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Decoding</td>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">factKB</td>
<td style="text-align: center;">BERT-P</td>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">factKB</td>
<td style="text-align: center;">BERT-P</td>
</tr>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">Regular</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">86.5</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">47.2</td>
<td style="text-align: center;">85.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">18.2</td>
<td style="text-align: center;">64.9</td>
<td style="text-align: center;">87.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">Regular</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">81.7</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">38.2</td>
<td style="text-align: center;">86.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">89.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT-Neo</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">Regular</td>
<td style="text-align: center;">24.3</td>
<td style="text-align: center;">80.5</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">17.6</td>
<td style="text-align: center;">54.0</td>
<td style="text-align: center;">86.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">90.6</td>
<td style="text-align: center;">18.1</td>
<td style="text-align: center;">65.1</td>
<td style="text-align: center;">89.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20B</td>
<td style="text-align: center;">Regular</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">68.3</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">14.9</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">85.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">90.6</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">Regular</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">53.5</td>
<td style="text-align: center;">87.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">90.8</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">21.1</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">91.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">Regular</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">88.5</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">47.7</td>
<td style="text-align: center;">87.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">31.8</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">22.0</td>
<td style="text-align: center;">66.4</td>
<td style="text-align: center;">90.3</td>
</tr>
<tr>
<td style="text-align: center;">FLAN</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">Regular</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">18.8</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">88.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">92.1</td>
<td style="text-align: center;">19.5</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">88.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">Regular</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">88.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">35.0</td>
<td style="text-align: center;">88.8</td>
</tr>
</tbody>
</table>
<p>Table 2: CAD consistently outperform the regular decoding method in terms of both summary quality metric (ROUGE-L) and summary factuality (factKB and BERT-P). The best scores for each setting are boldfaced. FLAN 3B and 11B refer to FLAN-T5 XL and FLAN-T5 XXL respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Decoding</th>
<th style="text-align: center;">Memo.</th>
<th style="text-align: center;">NQ</th>
<th style="text-align: center;">NQ-SWAP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">OPT</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">Reg.</td>
<td style="text-align: center;">32.5</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">18.8</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">32.2</td>
<td style="text-align: center;">36.9</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">Reg.</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">14.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">41.0</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">29.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT.</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">Reg.</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">31.9</td>
<td style="text-align: center;">19.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">41.2</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20B</td>
<td style="text-align: center;">Reg.</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">16.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">57.3</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">36.8</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA</td>
<td style="text-align: center;">13B</td>
<td style="text-align: center;">Reg.</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">11.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">57.1</td>
<td style="text-align: center;">33.6</td>
<td style="text-align: center;">36.7</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">30B</td>
<td style="text-align: center;">Reg.</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">9.6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">50.6</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">37.7</td>
</tr>
<tr>
<td style="text-align: center;">FLAN</td>
<td style="text-align: center;">3B</td>
<td style="text-align: center;">Reg.</td>
<td style="text-align: center;">69.2</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">72.2</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">73.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">11B</td>
<td style="text-align: center;">Reg.</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">73.0</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">77.1</td>
</tr>
</tbody>
</table>
<p>Table 3: CAD outperforms the regular decoding method (Reg.) in all settings except for FLAN-T5 on NQ.
formance on the knowledge conflict datasets, e.g., CAD improve GPT-Neo 20B by $54.4 \%$ on Memotrap and by $128 \%$ on NQ-SWAP. This substantial improvement suggests that context-aware decoding is particularly beneficial for LMs to adhere to the given context, in scenarios where the model's prior knowledge contradicts with the context knowledge.</p>
<h2>5 Analysis</h2>
<p>CAD brings consistent improvement to LMs with different sizes. In Tables 2 and 3, we show that CAD could be used to enhance a diverse set of LM families, including OPT, GPT-Neo, LLaMA, and FLAN-T5. We further investigate whether</p>
<p>CAD is effective in improving language models of different sizes. Specifically, we focus on OPT models across a range of sizes: $125 \mathrm{M}, 350 \mathrm{M}, 1.3 \mathrm{~B}$, $2.7 \mathrm{~B}, 6.7 \mathrm{~B}, 13 \mathrm{~B}, 30 \mathrm{~B}$. We observe that the performance gain brought by CAD stays consistent with different model sizes in CNN-DM. In Memotrap and NQ-SWAP, this gain increases as the model size grows, indicating that larger LMs can have a greater tendency to rely on their prior knowledge instead of reading the contexts, thereby benefiting more from CAD. In Figure 2, we observe that the performance gain brought by CAD stays consistent with different OPT model sizes in CNN-DM. In Memotrap and NQ-SWAP, this gain increases as the model size grows, indicating that larger LMs can have a greater tendency to rely on their prior knowledge instead of reading the contexts, thereby benefiting more from CAD.
Effect of adjustment level $\alpha$ We then investigate the effect of different adjustment level $\alpha$ (a small $\alpha$ makes the distribution closer to the original next token distribution). We conduct experiments with various values of $\alpha$ and present the results in Figure 3. Across all three datasets, we find $\alpha=0.5$ consistently provide robust improvements over regular decoding.</p>
<h2>6 Related Work</h2>
<p>Summarization factuality Summarization models have shown a tendency to generate hallucinated texts (Maynez et al., 2020; Pagnoni et al., 2021). This has led to growing efforts to improve the factual consistency, including applying attentions to</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: OPT models of varying sizes consistently benefit from CAD. The x-axis indicates the size of language models and the y-axis is the performance.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Effect of the adjustment level α. The y-axis is the performance and the x-axis is α.</p>
<p>fact triples extracted from source documents (Cao et al., 2018; Zhu et al., 2021), optimizing summarization models towards a factual consistency metrics (Nan et al., 2021; Cao and Wang, 2021), learning a post-editing error corrector (Dong et al., 2020) and removing noisy training samples (Kang and Hashimoto, 2020; Goyal and Durrett, 2021). These methods require additional fine-tuning and are not directly suitable for zero-shot and few-shot prompting scenarios. King et al. (2022) and Sridhar and Visser (2022) propose to alleviate the issue by constraining beam search algorithms.</p>
<p>Knowledge conflicts When presented with an updated document with conflicting knowledge, we expect language models to generate responses based on the provided contexts rather than relying solely on outdated parametric knowledge. This setting is especially valuable to retrieval-augmented language models (Khandelwal et al., 2020; Shi et al., 2024; Min et al., 2023; Yasunaga et al., 2023), where documents retrieved from external databases are used as additional input to provide LMs additional knowledge. However, simply adding documents does not always change the model predictions, as current LMs often overlook the contexts and rely heavily on their prior parametric knowledge (Longpre et al., 2021; Chen et al., 2022). Existing approaches for improving model's faithfulness to the context, such as the prompting-based method (Zhou et al., 2023), are limited in that they could only apply to large-scale instruction-finetuned LMs like OpenAI's text-davinci-003. In contrast, our work investigates a decoding strategy to tackle this problem, applicable to any LM.</p>
<h2>7 Conclusion</h2>
<p>Language models suffer from an insufficient attention to the given context compared to its prior knowledge, leading to an unfaithful generation to the input context. We present CAD, a simple inference-time method that downweights an output probability associated with the model's prior knowledge to promote models' attention to the context. We experiment on two families of tasks that require a strong attention to the context and show that CAD provides more faithful outputs across different language models of various sizes.</p>
<h2>Limitations</h2>
<p>Our proposed CAD method requires the output logits from language models in order to contrastively calculate the probability distribution with and without contexts. However, API-based language models like ChatGPT and GPT-4 may not provide output logits. Consequently, it is not feasible for CAD to be directly applied to such fully black-box models. Furthermore, CAD introduces a hyperparameter $\alpha$, which serves to regulate the level of contrastive adjustment. While we have observed that $\alpha=0.5$ yields consistent enhancements compared to regular decoding, different models applied to various tasks may have distinct optimal values for $\alpha$. If there exists a very small demonstration set of in-domain examples, we would consider the selection of $\alpha$ similar to other decoding parameters like the top-p or temperature values.</p>
<h2>Acknowledgement</h2>
<p>We thank Alisa Liu and Jiacheng Liu for providing insights during discussions of the project. This research is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-22072200004. This material is also funded in part by the DARPA Grant under Contract No. HR001120C0124. We also gratefully acknowledge support from NSF CAREER Grant No. IIS2142739, NSF Grants No. IIS2125201, IIS2203097, and the Alfred P. Sloan Foundation Fellowship. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p>
<h2>References</h2>
<p>Sid Black, Gao Leo, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow.</p>
<p>Shuyang Cao and Lu Wang. 2021. CLIFF: Contrastive learning for improving faithfulness and factuality in abstractive summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6633-6649, Online and</p>
<p>Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 4784-4791. AAAI Press.</p>
<p>Stephanie Chan, Adam Santoro, Andrew Lampinen, Jane Wang, Aaditya Singh, Pierre Richemond, James McClelland, and Felix Hill. 2022. Data distributional properties drive emergent in-context learning in transformers. In Advances in Neural Information Processing Systems, volume 35, pages 18878-18891. Curran Associates, Inc.</p>
<p>Hung-Ting Chen, Michael Zhang, and Eunsol Choi. 2022. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2292-2307, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. ArXiv preprint, abs/2210.11416.</p>
<p>Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173-1178, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie Chi Kit Cheung, and Jingjing Liu. 2020. Multifact correction in abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9320-9331, Online. Association for Computational Linguistics.</p>
<p>Shangbin Feng, Vidhisha Balachandran, Yuyang Bai, and Yulia Tsvetkov. 2023. FactKB: Generalizable factuality evaluation using language models enhanced with factual knowledge. In Proceedings of</p>
<p>the 2023 Conference on Empirical Methods in Natural Language Processing, pages 933-952, Singapore. Association for Computational Linguistics.</p>
<p>Tanya Goyal and Greg Durrett. 2021. Annotating and modeling fine-grained factuality in summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1449-1462, Online. Association for Computational Linguistics.</p>
<p>Meiqi Guo, Rebecca Hwa, and Adriana Kovashka. 2023. Decoding symbolism in language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3311-3324, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. ArXiv preprint, abs/2207.12598.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. 2021. Surface form competition: Why the highest probability answer isn't always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7038-7051, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Daniel Kang and Tatsunori B. Hashimoto. 2020. Improved natural language generation via loss truncation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 718-731, Online. Association for Computational Linguistics.</p>
<p>Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>Daniel King, Zejiang Shen, Nishant Subramani, Daniel S. Weld, Iz Beltagy, and Doug Downey. 2022. Don't say what you don't know: Improving the consistency of abstractive summarization by constraining beam search. In Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM), pages 555-571, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical</p>
<p>Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computational Linguistics.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-visiting NLIBased models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163-177.</p>
<p>Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110-119, San Diego, California. Association for Computational Linguistics.</p>
<p>Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2023. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12286-12312, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Alisa Liu and Jiacheng Liu. 2023. The memotrap dataset. https://github. com/inverse-scaling/prize/blob/main/ data-release/README.md.</p>
<p>Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. 2021. DExperts: Decoding-time controlled text generation with experts and anti-experts. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6691-6706, Online. Association for Computational Linguistics.</p>
<p>Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference</p>
<p>on Empirical Methods in Natural Language Processing, pages 7052-7063, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Nikolay Malkin, Zhen Wang, and Nebojsa Jojic. 2022. Coherence boosting: When your pretrained language model is not paying enough attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8214-8236, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wentau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2023. Nonparametric masked language modeling. In Findings of the Association for Computational Linguistics: ACL 2023, pages 2097-2118, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Kathleen McKeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, and Bing Xiang. 2021. Improving factual consistency of abstractive summarization via question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6881-6894, Online. Association for Computational Linguistics.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4812-4829, Online. Association for Computational Linguistics.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10731083, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and</p>
<p>Wen tau Yih. 2024. REPLUG: Retrieval-augmented black-box language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p>
<p>Arvind Krishna Sridhar and Erik Visser. 2022. Improved beam search for hallucination mitigation in abstractive summarization. ArXiv preprint, abs/2212.02712.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. LLaMA: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971.</p>
<p>Liam van der Poel, Ryan Cotterell, and Clara Meister. 2022. Mutual information alleviates hallucinations in abstractive summarization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5956-5965, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2023. Retrievalaugmented multimodal language modeling. In International Conference on Machine Learning (ICML).</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. ArXiv preprint, abs/2205.01068.</p>
<p>Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. 2024. Benchmarking Large Language Models for News Summarization. Transactions of the Association for Computational Linguistics, 12:39-57.</p>
<p>Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2023. Context-faithful prompting for large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14544-14556, Singapore. Association for Computational Linguistics.</p>
<p>Chenguang Zhu, William Hinthorn, Ruochen Xu, Qingkai Zeng, Michael Zeng, Xuedong Huang, and Meng Jiang. 2021. Enhancing factual consistency of abstractive summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 718-733, Online. Association for Computational Linguistics.</p>
<h2>A Appendix</h2>
<h2>A. 1 Qualitative Analyais</h2>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">XSUM</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Article</td>
<td style="text-align: center;">He passed away peacefully in hospital on Tuesday after a short illness. Born in Tourmakeady, County Mayo, he worked as a teacher before securing a part in the premiere of the Brian Friel play Translations in 1980. Lally became a household name in Ireland for his role as Miley Byrne in the RTE soap opera Glenroe and later starred in the BBC series Ballykissangel. He also appeared in the Hollywood movie Alexander and provided the voice for the Oscarnominated, animated Irish film, The Secret of Kells. As a fluent Irish speaker and advocate of the language, Lally had roles in several Irish language films ...</td>
</tr>
<tr>
<td style="text-align: center;">Regular</td>
<td style="text-align: center;">Westminister actor Pat Lally died in hospital on Tuesday night aged 82</td>
</tr>
<tr>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">Actor Lally, best known for Glenroe and Ballykissangel, has died in hospital on Tuesday</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">MemoTrap</td>
</tr>
<tr>
<td style="text-align: center;">Input</td>
<td style="text-align: center;">Write a quote that ends in the word "early". Better late than</td>
</tr>
<tr>
<td style="text-align: center;">Regular</td>
<td style="text-align: center;">never</td>
</tr>
<tr>
<td style="text-align: center;">CAD</td>
<td style="text-align: center;">early</td>
</tr>
</tbody>
</table>
<p>Table 4: Qualitative examples of contrast-aware decoding. The nonfactual or inconsistent texts are highlighted in yellow.</p>
<p>We provide qualitative examples for XSUM and Memotrap in Table 4. In XSUM, the regular decoding generates texts that is not mentioned in the article, whereas CAD produces output exclusively based on the information in the input article. For MemoTrap, the standard decoding disregards the instruction and generates the memorized ending, while CAD adheres to the instruction within the given context and produces the desired output.</p>
<h2>A. 2 Additional Related Work</h2>
<p>Contrastive decoding methods Contrastive decoding methods have been extensively explored for text generation. Coherence boosting (Malkin et al., 2022) and CPMI (van der Poel et al., 2022) demote a short context from a full context, focusing on the longer-range context for coherence and overall better generation quality. MMI-based decoding (Li et al., 2016) uses a contrastive formulation to improve output diversity in dialog generation. In this work, we adopt a same intuition and focus on analyzing the knowledge conflict scenarios where the faithfulness to the context is particularly important but difficult for the regular decoding methods.</p>
<p>We also extensively experiment the setup with a diverse set of language models and scales. DExperts (Liu et al., 2021) demotes the output distribution of an anti-expert (e.g., exposed to toxic language) to help lead the generations free from the unwanted attributes. Contrastive decoding (Li et al., 2023) demotes an amateur model (e.g., models with a very small number of parameters) to help distill the expert knowledge learned in the larger, more competitive models. In general, contrastive decoding has shown to be a general way to control model outputs, which we reinforce by considering the new case of factual consistency with the textual context.</p>
<p>Pointwise mutual information in text classification The concept of Pointwise Mutual Information (PMI) is extensively examined in text classification and reranking, serving to adjust the weighting of various classification choices based on the increased likelihood of an answer given a question within a specific task domain. Past research has applied it to zero-shot multiple-choice tasks (Holtzman et al., 2021), as well as the reranking of candidates for commonsense and symbolic knowledge extraction (Guo et al., 2023; Davison et al., 2019).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ The slight decline in performance can be attributed to the NQ dataset being included in the instruction-finetuning sets used by FLAN-T5.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
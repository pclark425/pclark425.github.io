<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1857 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1857</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1857</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-6fa25c94e41a0c90e3aabe80cf60f59ec9ff0a52</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6fa25c94e41a0c90e3aabe80cf60f59ec9ff0a52" target="_blank">Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training</a></p>
                <p><strong>Paper Venue:</strong> Computer Vision and Pattern Recognition</p>
                <p><strong>Paper TL;DR:</strong> This paper presents the first pre-training and fine-tuning paradigm for vision-and-language navigation (VLN) tasks, which leads to significant improvement over existing methods, achieving a new state of the art.</p>
                <p><strong>Paper Abstract:</strong> Learning to navigate in a visual environment following natural-language instructions is a challenging task, because the multimodal inputs to the agent are highly variable, and the training data on a new task is often limited. In this paper, we present the first pre-training and fine-tuning paradigm for vision-and-language navigation (VLN) tasks. By training on a large amount of image-text-action triplets in a self-supervised learning manner, the pre-trained model provides generic representations of visual environments and language instructions. It can be easily used as a drop-in for existing VLN frameworks, leading to the proposed agent PREVALENT. It learns more effectively in new tasks and generalizes better in a previously unseen environment. The performance is validated on three VLN tasks. On the Room-to-Room benchmark, our model improves the state-of-the-art from 47\% to 51\% on success rate weighted by path length. Further, the learned representation is transferable to other VLN tasks. On two recent tasks, vision-and-dialog navigation and ``Help, Anna!'', the proposed PREVALENT leads to significant improvement over existing methods, achieving a new state of the art.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1857.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1857.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PREVALENT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PREVALENT (Pre-trained Vision-and-Language Navigator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer-based vision-language encoder pre-trained on large-scale image-text-action triplets (masked language modeling attended to images + action prediction) and transferred to multiple 3D embodied VLN tasks (R2R, CVDN, HANNA), providing generic image-text representations that speed adaptation and improve generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PREVALENT</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Backbone: two single-modal Transformer encoders (vision and text) followed by cross-modal Transformer layers; visual input is panoramic (36 views) with ResNet features + orientation embedding; pretraining heads: image-attended Masked Language Modeling (MLM) and Action Prediction (AP) from [CLS]. Pretrained encoder is used either feature-based or fine-tuned (two-stage fine-tuning) and plugged into an LSTM Seq2Seq or cross-attention decoder for downstream navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>language instructions grounded with images and actions (image-text-action triplets, synthesized instructions + human instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Built on Matterport3D/Matterport simulator using: (i) original R2R training set (≈104K image-text-action triplets) and (ii) synthetic instructions produced by a speaker seq2seq model for shortest-path trajectories (≈1,020K synthesized instructions for trajectories producing ≈6,482K triplets). Total pretraining set ≈6.582M image-text-action triplets.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Room-to-Room (R2R), Cooperative Vision-and-Dialog Navigation (CVDN/NDH), HANNA (Help, Anna!)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>3D photorealistic indoor navigation tasks in Matterport3D simulator: R2R — navigate from start to goal following a single natural-language instruction; CVDN/NDH — navigate using dialog history (multi-turn Q/A) with indirect/underspecified instructions; HANNA — interactive imitation setting where an assistant issues intermediate language+image subtasks and the agent requests helps to find objects.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>During pretraining data collection the 'text' side is natural-language instructions (no explicit discrete commands); the pretraining action labels are the discrete panoramic navigation actions from R2R trajectories (next-view choices).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete panoramic navigation actions (choose one of navigable viewpoints among panoramic candidates; includes a special STOP action). Each panoramic view contains 36 view images (12 headings × 3 elevations).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Pretraining: an AP head (FC on encoder [CLS]) predicts the next discrete panoramic action conditioned on current panoramic image features and instruction (no trajectory history). Transfer: the pre-trained encoder's contextualized word and visual embeddings (z_t) are fed into downstream decoders (LSTM seq2seq follower or cross-attention decoder); in two-stage fine-tuning the cross-attention layers are further fine-tuned (20k iterations) to map encoder representations to navigation actions.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>RGB panoramic vision (36 views per state) represented by ResNet-2048 features concatenated with 128-d orientation encoding; no object-level Fast R-CNN features used due to panoramic view setup.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>R2R: paper reports improving prior SPL SOTA from 47% → 51% (abstract); best reported PREVALENT numbers in paper include Test-Unseen SR up to 59% and SPL up to 56% in one reported setting, and Test-Unseen SPL=51% in the abstract/primary claim. CVDN (Goal Progress): PREVALENT achieves higher GP (e.g., Navigator setting GP ≈2.99 on val unseen vs Seq2Seq 1.98). HANNA: Test-unseen SR=52.91%, SPL=28.72%, NE=5.29 (PREVALENT with AP+MLM) — better SPL and NE than baseline ANNA on unseen.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines: Seq2Seq and EnvDrop (random init / non-pretrained) achieve substantially lower metrics: e.g., on R2R prior SPL SOTA reported ~47% (paper cites improvement from 47% to 51%); on CVDN Seq2Seq GP ~1.98 (val unseen Navigator) vs PREVALENT 2.99. HANNA ANNA baseline Test-unseen SPL ≈25.50 and NE ≈7.67 (PREVALENT is better on these). Exact baseline values vary by setting and are reported in paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>The paper reports qualitatively faster adaptation and better early-stage learning curves (Figures show PREVALENT converging faster on seen/unseen curves). Pretraining used 6.582M triplets; fine-tuning strategy often feature-based then fine-tune cross-attention for 20k iterations (batch sizes for fine-tuning reported: e.g., batch size 10 for cross-attention at lr 2e-6). No explicit numeric claim of 'samples-to-threshold' is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not explicitly quantified; paper shows comparison learning curves showing non-pretrained (random init) EmbDrop/Seq2Seq learning more slowly and converging to lower performance, but exact sample counts to reach thresholds are not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative improvement: faster early-stage adaptation and better final generalization to unseen environments; no precise multiplicative sample-efficiency factor reported (paper provides learning curves rather than exact sample-count reductions).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Key factors: (1) image-attended MLM grounds language tokens in visual context, (2) Action Prediction objective injects action-conditioned grounding during pretraining, (3) large-scale synthetic instruction augmentation (speaker) provides scale (≈6.6M triplets), (4) panoramic view-level visual features matching downstream action space (36-view panoramic representation) reduces perception/action mismatch, (5) Transformer cross-modal architecture yields transferable joint representations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations noted: pretraining on VLN-style data (panoramic ResNet features + actions) is necessary — generic VLP methods that use Fast R-CNN object features do not directly apply; remaining gaps include environment-specific biases and remaining SR/SPL gaps to human performance. No catastrophic negative transfer reported; ablations show removing AP weakens transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretraining a vision-language encoder on image-text-action triplets with image-attended MLM plus an Action Prediction objective, scaled via synthetic instruction generation, yields transferable joint representations that (i) speed fine-tuning and early learning, (ii) improve generalization to unseen Matterport3D environments, and (iii) transfer across different embodied VLN tasks (R2R, dialog-guided NDH/CVDN, and interactive HANNA); the AP objective and image-attended MLM are both important for transfer, and PREVALENT outperforms language-only pretraining (BERT) and non-pretrained baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1857.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1857.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRESS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRESS (Robust navigation with language pretraining and stochastic sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior VLN agent that uses off-the-shelf BERT language pretraining (language-only) to improve language understanding for navigation, with stochastic sampling for robustness; used as a competitive baseline in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust navigation with language pretraining and stochastic sampling</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>PRESS (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Uses off-the-shelf BERT for instruction encoding (language-only pretraining) integrated into navigation models (speaker-follower style augmentation and stochastic sampling). Does not pretrain a joint vision-language-action encoder from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Generic language pretraining (BERT-style corpora) — language-only</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>PRESS uses off-the-shelf BERT (pretrained on large text corpora); the paper being analyzed did not retrain PRESS but cites it as a baseline. Exact corpora for off-the-shelf BERT are not detailed in this paper (see BERT original paper).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Room-to-Room (R2R) (used as baseline task)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>R2R panoramic-view navigation in Matterport3D; PRESS is evaluated on same R2R benchmarks (seen/unseen/test splits).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Language instructions (natural sentences); no explicit text-world action commands.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete panoramic navigation actions (same as R2R standard: choose navigable viewpoints among panoramic candidates plus STOP).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>BERT-derived language embeddings are integrated into sequence-to-sequence navigation decoders (follower), using stochastic sampling and data augmentation pipelines; no joint vision-language-action pretraining was performed in PRESS according to this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Panoramic RGB visual features (ResNet view-level representations used in downstream models), same simulator setup as R2R.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported in paper Table 1 (as baseline): PRESS Test-Unseen example row: NE≈4.53m, SR≈57%, SPL≈53% (specific numbers depend on evaluation setting shown in table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Non-pretrained seq2seq/EnvDrop baselines in table show lower performance (e.g., Seq2Seq / EnvDrop SPL and SR lower than PRESS); exact numbers vary by baseline and setting.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not explicitly reported in this paper for PRESS; PRESS benefits from BERT embeddings but numeric sample-efficiency metrics are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not quantified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Language pretraining improves instruction understanding and robustness (semantic representations from BERT) which helps downstream navigation when integrated with visual encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Because PRESS uses language-only pretraining, it lacks image-grounded pretraining and action-conditioned learning; the paper shows PREVALENT (vision-language-action pretraining) reduces unseen/seen gap more effectively than PRESS.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language-only pretraining (BERT) helps navigation vs random init, but jointly pretraining vision-language-action representations (PREVALENT) yields further improvements in generalization and unseen environment performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1857.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1857.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (applied)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT: Pre-training of deep bidirectional transformers for language understanding (applied as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The canonical language-only pre-trained Transformer (BERT) used in ablation experiments: the authors evaluated both re-pretraining BERT on R2R text and fine-tuning off-the-shelf BERT on downstream VLN tasks to compare against image-attended MLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BERT: Pre-training of deep bidirectional transformers for language understanding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>BERT (off-the-shelf and re-pretrained variants used as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Standard BERT MLM training on text only (no image attention) used either (i) re-pretrained on R2R instruction text or (ii) off-the-shelf BERT fine-tuned on downstream CVDN/HANNA; compared to PREVALENT's image-attended MLM and AP objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Generic large-scale text corpora (off-the-shelf) or R2R instruction text when re-pretrained in ablation</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Ablation variants in paper: (a) 'BERT pre-training' — original MLM applied to the R2R pretraining dataset (the authors applied BERT MLM on their R2R pretraining set); (b) 'BERT fine-tuning' — off-the-shelf BERT fine-tuned directly on CVDN. Exact corpora for off-the-shelf BERT follow Devlin et al. (not detailed in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>CVDN (NDH) and HANNA (used in ablation comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>CVDN: dialog-based navigation (longer text inputs). HANNA: interactive assistance task. BERT variants were used as baselines to test effectiveness of image-attended MLM+AP.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language instructions and dialog text (no explicit actions in pretraining text).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete panoramic navigation actions during downstream evaluation (same as VLN tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>BERT provides contextualized language embeddings which are fed into the downstream navigation architecture (e.g., LSTM decoder) to predict discrete navigation actions; BERT lacks joint visual-action pretraining so mapping relies on downstream supervised/fine-tuning signals.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>When used in VLN pipelines, BERT is paired with ResNet panoramic visual features; BERT itself is language-only.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Ablations show BERT variants perform worse than PREVALENT: e.g., on CVDN Goal Progress (Table 4) BERT pre-training yields GP values lower than PREVALENT (e.g., BERT pre-train Navigator ≈2.71 vs PREVALENT ≈3.01 in some settings). On HANNA (Table 7) BERT (feature-based) SR and SPL on unseen are much lower (e.g., SR≈24.12%, SPL≈11.50%) compared to PREVALENT (SR≈52.91%, SPL≈28.72%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Seq2Seq baselines and other non-BERT methods in tables; BERT fine-tuning sometimes outperforms naive baselines but is still below PREVALENT when image-attended MLM+AP are used.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Not numerically specified; ablations indicate BERT-based methods converge less effectively and provide inferior early-stage performance on unseen environments relative to PREVALENT.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not provided numerically in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Not quantified numerically; qualitative result: image-attended pretraining (PREVALENT) yields better sample efficiency than BERT-based baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong language representations from BERT help instruction understanding, but lack of image-attended language grounding and action-conditioned supervision limits its effectiveness for embodied VLN tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Absence of visual grounding in pretraining and lack of action prediction objective cause reduced transfer performance compared to PREVALENT's joint V+L+A pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Language-only pretraining (BERT) gives some benefit over random initialization but is inferior to image-attended, action-aware pretraining; grounding language in images and including action prediction are important for transferring to 3D embodied navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1857.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1857.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Speaker model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Speaker seq2seq model (instruction synthesizer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An auto-regressive seq2seq 'speaker' model trained on R2R that generates natural-language instructions conditioned on agent trajectories; used to synthesize large-scale instruction data for PREVALENT pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Speaker-follower models for vision-and-language navigation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>Speaker (seq2seq instruction generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A sequence-to-sequence auto-regressive model that takes a trajectory (sequence of actions and visual images) as input and generates language instructions describing the trajectory; trained on the R2R dataset and then used to produce synthetic instructions for shortest-path trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Language instruction synthesis model trained on R2R (language conditioned on trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Trained on the R2R human-annotated instruction-trajectories; then used to synthesize ≈1,020K instructions for shortest-path trajectories, producing ≈6,482K image-text-action triplets used for PREVALENT pretraining (synthetic data comprises ≈98.4% of pretraining data).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used to create pretraining data for PREVALENT which is transferred to R2R / CVDN / HANNA</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Not an embodied agent itself; it produces natural-language instructions conditioned on simulated trajectories in Matterport3D, enabling scale for joint vision-language-action pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Outputs natural-language instructions (text) describing trajectory-level actions; internal training uses discrete panoramic actions from R2R trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A for the speaker itself; the trajectories used by the speaker come from discrete panoramic navigation action space in Matterport3D.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>The speaker maps sequences of view-level visual observations + actions (trajectory) to sequences of text tokens via seq2seq decoding; synthetic text labels are paired with the original trajectories for PREVALENT pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Trained on ResNet panoramic features (same input representation as follower agents); requires trajectory (views + actions) supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>The speaker enabled scaling PREVALENT's pretraining dataset to ≈6.6M triplets; PREVALENT trained with synthetic+real data showed improved downstream performance vs training only on the original 104K R2R examples (qualitative and quantitative improvements shown in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>If speaker-generated synthetic data were not used, pretraining data would be ~104K triplets (R2R only), which is an order of magnitude smaller; the paper shows that synthetic augmentation improves pretraining effectiveness (synthetic samples constitute ≈98.4% of pretraining set).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>With synthetic augmentation the pretraining set grew to ≈6.582M triplets enabling stable pretraining for 20 epochs on 8×V100; exact downstream sample-efficiency gains reported qualitatively (faster learning), not as exact sample counts.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Not numerically specified; training only on the 104K R2R triplets would be substantially smaller and less effective per the authors' discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Enables scaling pretraining by ≈63× (6.582M / 104K ≈ 63), which materially enabled the PREVALENT pretraining regime; downstream sample-efficiency gains are qualitative in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Massive scale of synthesized instruction data, alignment between generated language and environment trajectories, and use of generated data in the same panoramic-action representation used by downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Synthetic instructions may carry biases of the speaker model; however the paper reports that synthesized data helps rather than harms pretraining (no major failure reported).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a speaker seq2seq to generate large-scale language supervision for simulator trajectories is a practical and effective strategy to scale image-text-action pretraining for embodied VLN; the synthetic data dominated the pretraining corpus and materially contributed to PREVALENT's transfer performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training', 'publication_date_yy_mm': '2020-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Robust navigation with language pretraining and stochastic sampling <em>(Rating: 2)</em></li>
                <li>Speaker-follower models for vision-and-language navigation <em>(Rating: 2)</em></li>
                <li>BERT: Pre-training of deep bidirectional transformers for language understanding <em>(Rating: 2)</em></li>
                <li>Vision-and-dialog navigation <em>(Rating: 2)</em></li>
                <li>Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning <em>(Rating: 2)</em></li>
                <li>VL-BERT: Pre-training of generic visual-linguistic representations <em>(Rating: 1)</em></li>
                <li>LXMERT: Learning cross-modality encoder representations from transformers <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1857",
    "paper_id": "paper-6fa25c94e41a0c90e3aabe80cf60f59ec9ff0a52",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "PREVALENT",
            "name_full": "PREVALENT (Pre-trained Vision-and-Language Navigator)",
            "brief_description": "A Transformer-based vision-language encoder pre-trained on large-scale image-text-action triplets (masked language modeling attended to images + action prediction) and transferred to multiple 3D embodied VLN tasks (R2R, CVDN, HANNA), providing generic image-text representations that speed adaptation and improve generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_agent_name": "PREVALENT",
            "model_agent_description": "Backbone: two single-modal Transformer encoders (vision and text) followed by cross-modal Transformer layers; visual input is panoramic (36 views) with ResNet features + orientation embedding; pretraining heads: image-attended Masked Language Modeling (MLM) and Action Prediction (AP) from [CLS]. Pretrained encoder is used either feature-based or fine-tuned (two-stage fine-tuning) and plugged into an LSTM Seq2Seq or cross-attention decoder for downstream navigation.",
            "pretraining_data_type": "language instructions grounded with images and actions (image-text-action triplets, synthesized instructions + human instructions)",
            "pretraining_data_details": "Built on Matterport3D/Matterport simulator using: (i) original R2R training set (≈104K image-text-action triplets) and (ii) synthetic instructions produced by a speaker seq2seq model for shortest-path trajectories (≈1,020K synthesized instructions for trajectories producing ≈6,482K triplets). Total pretraining set ≈6.582M image-text-action triplets.",
            "embodied_task_name": "Room-to-Room (R2R), Cooperative Vision-and-Dialog Navigation (CVDN/NDH), HANNA (Help, Anna!)",
            "embodied_task_description": "3D photorealistic indoor navigation tasks in Matterport3D simulator: R2R — navigate from start to goal following a single natural-language instruction; CVDN/NDH — navigate using dialog history (multi-turn Q/A) with indirect/underspecified instructions; HANNA — interactive imitation setting where an assistant issues intermediate language+image subtasks and the agent requests helps to find objects.",
            "action_space_text": "During pretraining data collection the 'text' side is natural-language instructions (no explicit discrete commands); the pretraining action labels are the discrete panoramic navigation actions from R2R trajectories (next-view choices).",
            "action_space_embodied": "Discrete panoramic navigation actions (choose one of navigable viewpoints among panoramic candidates; includes a special STOP action). Each panoramic view contains 36 view images (12 headings × 3 elevations).",
            "action_mapping_method": "Pretraining: an AP head (FC on encoder [CLS]) predicts the next discrete panoramic action conditioned on current panoramic image features and instruction (no trajectory history). Transfer: the pre-trained encoder's contextualized word and visual embeddings (z_t) are fed into downstream decoders (LSTM seq2seq follower or cross-attention decoder); in two-stage fine-tuning the cross-attention layers are further fine-tuned (20k iterations) to map encoder representations to navigation actions.",
            "perception_requirements": "RGB panoramic vision (36 views per state) represented by ResNet-2048 features concatenated with 128-d orientation encoding; no object-level Fast R-CNN features used due to panoramic view setup.",
            "transfer_successful": true,
            "performance_with_pretraining": "R2R: paper reports improving prior SPL SOTA from 47% → 51% (abstract); best reported PREVALENT numbers in paper include Test-Unseen SR up to 59% and SPL up to 56% in one reported setting, and Test-Unseen SPL=51% in the abstract/primary claim. CVDN (Goal Progress): PREVALENT achieves higher GP (e.g., Navigator setting GP ≈2.99 on val unseen vs Seq2Seq 1.98). HANNA: Test-unseen SR=52.91%, SPL=28.72%, NE=5.29 (PREVALENT with AP+MLM) — better SPL and NE than baseline ANNA on unseen.",
            "performance_without_pretraining": "Baselines: Seq2Seq and EnvDrop (random init / non-pretrained) achieve substantially lower metrics: e.g., on R2R prior SPL SOTA reported ~47% (paper cites improvement from 47% to 51%); on CVDN Seq2Seq GP ~1.98 (val unseen Navigator) vs PREVALENT 2.99. HANNA ANNA baseline Test-unseen SPL ≈25.50 and NE ≈7.67 (PREVALENT is better on these). Exact baseline values vary by setting and are reported in paper tables.",
            "sample_complexity_with_pretraining": "The paper reports qualitatively faster adaptation and better early-stage learning curves (Figures show PREVALENT converging faster on seen/unseen curves). Pretraining used 6.582M triplets; fine-tuning strategy often feature-based then fine-tune cross-attention for 20k iterations (batch sizes for fine-tuning reported: e.g., batch size 10 for cross-attention at lr 2e-6). No explicit numeric claim of 'samples-to-threshold' is provided.",
            "sample_complexity_without_pretraining": "Not explicitly quantified; paper shows comparison learning curves showing non-pretrained (random init) EmbDrop/Seq2Seq learning more slowly and converging to lower performance, but exact sample counts to reach thresholds are not reported.",
            "sample_complexity_gain": "Qualitative improvement: faster early-stage adaptation and better final generalization to unseen environments; no precise multiplicative sample-efficiency factor reported (paper provides learning curves rather than exact sample-count reductions).",
            "transfer_success_factors": "Key factors: (1) image-attended MLM grounds language tokens in visual context, (2) Action Prediction objective injects action-conditioned grounding during pretraining, (3) large-scale synthetic instruction augmentation (speaker) provides scale (≈6.6M triplets), (4) panoramic view-level visual features matching downstream action space (36-view panoramic representation) reduces perception/action mismatch, (5) Transformer cross-modal architecture yields transferable joint representations.",
            "transfer_failure_factors": "Limitations noted: pretraining on VLN-style data (panoramic ResNet features + actions) is necessary — generic VLP methods that use Fast R-CNN object features do not directly apply; remaining gaps include environment-specific biases and remaining SR/SPL gaps to human performance. No catastrophic negative transfer reported; ablations show removing AP weakens transfer.",
            "key_findings": "Pretraining a vision-language encoder on image-text-action triplets with image-attended MLM plus an Action Prediction objective, scaled via synthetic instruction generation, yields transferable joint representations that (i) speed fine-tuning and early learning, (ii) improve generalization to unseen Matterport3D environments, and (iii) transfer across different embodied VLN tasks (R2R, dialog-guided NDH/CVDN, and interactive HANNA); the AP objective and image-attended MLM are both important for transfer, and PREVALENT outperforms language-only pretraining (BERT) and non-pretrained baselines.",
            "uuid": "e1857.0",
            "source_info": {
                "paper_title": "Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "PRESS",
            "name_full": "PRESS (Robust navigation with language pretraining and stochastic sampling)",
            "brief_description": "A prior VLN agent that uses off-the-shelf BERT language pretraining (language-only) to improve language understanding for navigation, with stochastic sampling for robustness; used as a competitive baseline in this paper.",
            "citation_title": "Robust navigation with language pretraining and stochastic sampling",
            "mention_or_use": "mention",
            "model_agent_name": "PRESS (baseline)",
            "model_agent_description": "Uses off-the-shelf BERT for instruction encoding (language-only pretraining) integrated into navigation models (speaker-follower style augmentation and stochastic sampling). Does not pretrain a joint vision-language-action encoder from scratch.",
            "pretraining_data_type": "Generic language pretraining (BERT-style corpora) — language-only",
            "pretraining_data_details": "PRESS uses off-the-shelf BERT (pretrained on large text corpora); the paper being analyzed did not retrain PRESS but cites it as a baseline. Exact corpora for off-the-shelf BERT are not detailed in this paper (see BERT original paper).",
            "embodied_task_name": "Room-to-Room (R2R) (used as baseline task)",
            "embodied_task_description": "R2R panoramic-view navigation in Matterport3D; PRESS is evaluated on same R2R benchmarks (seen/unseen/test splits).",
            "action_space_text": "Language instructions (natural sentences); no explicit text-world action commands.",
            "action_space_embodied": "Discrete panoramic navigation actions (same as R2R standard: choose navigable viewpoints among panoramic candidates plus STOP).",
            "action_mapping_method": "BERT-derived language embeddings are integrated into sequence-to-sequence navigation decoders (follower), using stochastic sampling and data augmentation pipelines; no joint vision-language-action pretraining was performed in PRESS according to this paper.",
            "perception_requirements": "Panoramic RGB visual features (ResNet view-level representations used in downstream models), same simulator setup as R2R.",
            "transfer_successful": true,
            "performance_with_pretraining": "Reported in paper Table 1 (as baseline): PRESS Test-Unseen example row: NE≈4.53m, SR≈57%, SPL≈53% (specific numbers depend on evaluation setting shown in table).",
            "performance_without_pretraining": "Non-pretrained seq2seq/EnvDrop baselines in table show lower performance (e.g., Seq2Seq / EnvDrop SPL and SR lower than PRESS); exact numbers vary by baseline and setting.",
            "sample_complexity_with_pretraining": "Not explicitly reported in this paper for PRESS; PRESS benefits from BERT embeddings but numeric sample-efficiency metrics are not provided here.",
            "sample_complexity_without_pretraining": "Not provided in this paper.",
            "sample_complexity_gain": "Not quantified in this paper.",
            "transfer_success_factors": "Language pretraining improves instruction understanding and robustness (semantic representations from BERT) which helps downstream navigation when integrated with visual encoders.",
            "transfer_failure_factors": "Because PRESS uses language-only pretraining, it lacks image-grounded pretraining and action-conditioned learning; the paper shows PREVALENT (vision-language-action pretraining) reduces unseen/seen gap more effectively than PRESS.",
            "key_findings": "Language-only pretraining (BERT) helps navigation vs random init, but jointly pretraining vision-language-action representations (PREVALENT) yields further improvements in generalization and unseen environment performance.",
            "uuid": "e1857.1",
            "source_info": {
                "paper_title": "Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "BERT (applied)",
            "name_full": "BERT: Pre-training of deep bidirectional transformers for language understanding (applied as baseline)",
            "brief_description": "The canonical language-only pre-trained Transformer (BERT) used in ablation experiments: the authors evaluated both re-pretraining BERT on R2R text and fine-tuning off-the-shelf BERT on downstream VLN tasks to compare against image-attended MLM.",
            "citation_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "mention_or_use": "use",
            "model_agent_name": "BERT (off-the-shelf and re-pretrained variants used as baselines)",
            "model_agent_description": "Standard BERT MLM training on text only (no image attention) used either (i) re-pretrained on R2R instruction text or (ii) off-the-shelf BERT fine-tuned on downstream CVDN/HANNA; compared to PREVALENT's image-attended MLM and AP objectives.",
            "pretraining_data_type": "Generic large-scale text corpora (off-the-shelf) or R2R instruction text when re-pretrained in ablation",
            "pretraining_data_details": "Ablation variants in paper: (a) 'BERT pre-training' — original MLM applied to the R2R pretraining dataset (the authors applied BERT MLM on their R2R pretraining set); (b) 'BERT fine-tuning' — off-the-shelf BERT fine-tuned directly on CVDN. Exact corpora for off-the-shelf BERT follow Devlin et al. (not detailed in this paper).",
            "embodied_task_name": "CVDN (NDH) and HANNA (used in ablation comparisons)",
            "embodied_task_description": "CVDN: dialog-based navigation (longer text inputs). HANNA: interactive assistance task. BERT variants were used as baselines to test effectiveness of image-attended MLM+AP.",
            "action_space_text": "Natural language instructions and dialog text (no explicit actions in pretraining text).",
            "action_space_embodied": "Discrete panoramic navigation actions during downstream evaluation (same as VLN tasks).",
            "action_mapping_method": "BERT provides contextualized language embeddings which are fed into the downstream navigation architecture (e.g., LSTM decoder) to predict discrete navigation actions; BERT lacks joint visual-action pretraining so mapping relies on downstream supervised/fine-tuning signals.",
            "perception_requirements": "When used in VLN pipelines, BERT is paired with ResNet panoramic visual features; BERT itself is language-only.",
            "transfer_successful": true,
            "performance_with_pretraining": "Ablations show BERT variants perform worse than PREVALENT: e.g., on CVDN Goal Progress (Table 4) BERT pre-training yields GP values lower than PREVALENT (e.g., BERT pre-train Navigator ≈2.71 vs PREVALENT ≈3.01 in some settings). On HANNA (Table 7) BERT (feature-based) SR and SPL on unseen are much lower (e.g., SR≈24.12%, SPL≈11.50%) compared to PREVALENT (SR≈52.91%, SPL≈28.72%).",
            "performance_without_pretraining": "Seq2Seq baselines and other non-BERT methods in tables; BERT fine-tuning sometimes outperforms naive baselines but is still below PREVALENT when image-attended MLM+AP are used.",
            "sample_complexity_with_pretraining": "Not numerically specified; ablations indicate BERT-based methods converge less effectively and provide inferior early-stage performance on unseen environments relative to PREVALENT.",
            "sample_complexity_without_pretraining": "Not provided numerically in paper.",
            "sample_complexity_gain": "Not quantified numerically; qualitative result: image-attended pretraining (PREVALENT) yields better sample efficiency than BERT-based baselines.",
            "transfer_success_factors": "Strong language representations from BERT help instruction understanding, but lack of image-attended language grounding and action-conditioned supervision limits its effectiveness for embodied VLN tasks.",
            "transfer_failure_factors": "Absence of visual grounding in pretraining and lack of action prediction objective cause reduced transfer performance compared to PREVALENT's joint V+L+A pretraining.",
            "key_findings": "Language-only pretraining (BERT) gives some benefit over random initialization but is inferior to image-attended, action-aware pretraining; grounding language in images and including action prediction are important for transferring to 3D embodied navigation.",
            "uuid": "e1857.2",
            "source_info": {
                "paper_title": "Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        },
        {
            "name_short": "Speaker model",
            "name_full": "Speaker seq2seq model (instruction synthesizer)",
            "brief_description": "An auto-regressive seq2seq 'speaker' model trained on R2R that generates natural-language instructions conditioned on agent trajectories; used to synthesize large-scale instruction data for PREVALENT pretraining.",
            "citation_title": "Speaker-follower models for vision-and-language navigation",
            "mention_or_use": "use",
            "model_agent_name": "Speaker (seq2seq instruction generator)",
            "model_agent_description": "A sequence-to-sequence auto-regressive model that takes a trajectory (sequence of actions and visual images) as input and generates language instructions describing the trajectory; trained on the R2R dataset and then used to produce synthetic instructions for shortest-path trajectories.",
            "pretraining_data_type": "Language instruction synthesis model trained on R2R (language conditioned on trajectories)",
            "pretraining_data_details": "Trained on the R2R human-annotated instruction-trajectories; then used to synthesize ≈1,020K instructions for shortest-path trajectories, producing ≈6,482K image-text-action triplets used for PREVALENT pretraining (synthetic data comprises ≈98.4% of pretraining data).",
            "embodied_task_name": "Used to create pretraining data for PREVALENT which is transferred to R2R / CVDN / HANNA",
            "embodied_task_description": "Not an embodied agent itself; it produces natural-language instructions conditioned on simulated trajectories in Matterport3D, enabling scale for joint vision-language-action pretraining.",
            "action_space_text": "Outputs natural-language instructions (text) describing trajectory-level actions; internal training uses discrete panoramic actions from R2R trajectories.",
            "action_space_embodied": "N/A for the speaker itself; the trajectories used by the speaker come from discrete panoramic navigation action space in Matterport3D.",
            "action_mapping_method": "The speaker maps sequences of view-level visual observations + actions (trajectory) to sequences of text tokens via seq2seq decoding; synthetic text labels are paired with the original trajectories for PREVALENT pretraining.",
            "perception_requirements": "Trained on ResNet panoramic features (same input representation as follower agents); requires trajectory (views + actions) supervision.",
            "transfer_successful": true,
            "performance_with_pretraining": "The speaker enabled scaling PREVALENT's pretraining dataset to ≈6.6M triplets; PREVALENT trained with synthetic+real data showed improved downstream performance vs training only on the original 104K R2R examples (qualitative and quantitative improvements shown in experiments).",
            "performance_without_pretraining": "If speaker-generated synthetic data were not used, pretraining data would be ~104K triplets (R2R only), which is an order of magnitude smaller; the paper shows that synthetic augmentation improves pretraining effectiveness (synthetic samples constitute ≈98.4% of pretraining set).",
            "sample_complexity_with_pretraining": "With synthetic augmentation the pretraining set grew to ≈6.582M triplets enabling stable pretraining for 20 epochs on 8×V100; exact downstream sample-efficiency gains reported qualitatively (faster learning), not as exact sample counts.",
            "sample_complexity_without_pretraining": "Not numerically specified; training only on the 104K R2R triplets would be substantially smaller and less effective per the authors' discussion.",
            "sample_complexity_gain": "Enables scaling pretraining by ≈63× (6.582M / 104K ≈ 63), which materially enabled the PREVALENT pretraining regime; downstream sample-efficiency gains are qualitative in the paper.",
            "transfer_success_factors": "Massive scale of synthesized instruction data, alignment between generated language and environment trajectories, and use of generated data in the same panoramic-action representation used by downstream tasks.",
            "transfer_failure_factors": "Synthetic instructions may carry biases of the speaker model; however the paper reports that synthesized data helps rather than harms pretraining (no major failure reported).",
            "key_findings": "Using a speaker seq2seq to generate large-scale language supervision for simulator trajectories is a practical and effective strategy to scale image-text-action pretraining for embodied VLN; the synthetic data dominated the pretraining corpus and materially contributed to PREVALENT's transfer performance.",
            "uuid": "e1857.3",
            "source_info": {
                "paper_title": "Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training",
                "publication_date_yy_mm": "2020-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Robust navigation with language pretraining and stochastic sampling",
            "rating": 2
        },
        {
            "paper_title": "Speaker-follower models for vision-and-language navigation",
            "rating": 2
        },
        {
            "paper_title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "rating": 2
        },
        {
            "paper_title": "Vision-and-dialog navigation",
            "rating": 2
        },
        {
            "paper_title": "Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning",
            "rating": 2
        },
        {
            "paper_title": "VL-BERT: Pre-training of generic visual-linguistic representations",
            "rating": 1
        },
        {
            "paper_title": "LXMERT: Learning cross-modality encoder representations from transformers",
            "rating": 1
        }
    ],
    "cost": 0.0193535,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training</h1>
<p>Weituo Hao ${ }^{1 \dagger \ddagger}$, Chunyuan $\mathrm{Li}^{2 \dagger} \uparrow$ Xiujun $\mathrm{Li}^{2}$, Lawrence Carin ${ }^{1}$, Jianfeng Gao ${ }^{2}$<br>${ }^{1}$ Duke University ${ }^{2}$ Microsoft Research, Redmond<br>{weituo.hao, lcarin}@duke.edu {chuny1,xiul,jfgao}@microsoft.com</p>
<h4>Abstract</h4>
<p>Learning to navigate in a visual environment following natural-language instructions is a challenging task, because the multimodal inputs to the agent are highly variable, and the training data on a new task is often limited. We present the first pre-training and fine-tuning paradigm for vision-and-language navigation (VLN) tasks. By training on a large amount of image-text-action triplets in a self-supervised learning manner, the pre-trained model provides generic representations of visual environments and language instructions. It can be easily used as a drop-in for existing VLN frameworks, leading to the proposed agent Prevalent ${ }^{\dagger}$. It learns more effectively in new tasks and generalizes better in a previously unseen environment. The performance is validated on three VLN tasks. On the Room-to-Room [3] benchmark, our model improves the state-of-the-art from $47 \%$ to $51 \%$ on success rate weighted by path length. Further, the learned representation is transferable to other VLN tasks. On two recent tasks, vision-and-dialog navigation [30] and "Help, Anna!" [22], the proposed Prevalent leads to significant improvement over existing methods, achieving a new state of the art.</p>
<h2>1. Introduction</h2>
<p>Learning to navigate in a photorealistic home environment based on natural language instructions has attracted increasing research interest [23, 14, 7, 3, 6], as it provides insight into core scientific questions about multimodal representations. It also takes a step toward real-world applications, such as personal assistants and in-home robots. Vision-and-language navigation (VLN) presents a challenging reasoning problem for agents, as the multimodal inputs are highly variable, inherently ambiguous, and often underspecified.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Most previous methods build on the sequence-tosequence architecture [26], where the instruction is encoded as a sequence of words, and the navigation trajectory is decoded as a sequence of actions, enhanced with attention mechanisms [3, 32, 18] and beam search [9]. While a number of methods $[20,21,33]$ have been proposed to improve language understanding, common to all existing work is that the agent learns to understand each instruction from scratch or in isolation, without collectively leveraging prior visiongrounded domain knowledge.</p>
<p>However, each instruction in practice only loosely aligns with the desired navigation path, making it imperfect for the existing paradigm of learning to understand the instruction from scratch. This is because (i) every instruction only partially characterizes the trajectory. It can be ambiguous to interpret the instructions, without grounding on the visual states. (ii) The objects in visual states and language instructions may share various common forms/relationships, and therefore it is natural to build an informative joint representation beforehand, and use this "common knowleldge" for transfer learning in downstream tasks.</p>
<p>To address this natural ambiguity of instructions more effectively, we propose to pre-train an encoder to align language instructions and visual states for joint representations. The image-text-action triplets at each time step are independently fed into the model, which is trained to predict the masked word tokens and next actions, thus formulating the VLN pre-training in the self-learning paradigm. The complexity of VLN learning can then be reduced by eliminating language understandings which lack consensus from visual states. The pre-trained model plays the role of providing generic image-text representations, and is applicable to most existing approaches to VLN, leading to our agent Prevalent. We consider three VLN scenarios as downstream tasks: Room-to-room (R2R) [3], cooperative vision-and-dialog navigation (CVDN) [30], and "Help, Anna!" (HANNA) [22]. The overall pre-training and finetuning pipeline is shown in Figure 1.</p>
<p>Comprehensive experiments demonstrate strong empirical performance of Prevalent. The proposed Preva-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of the proposed pre-training and fine-tuning paradigm for VLN. The image-text-action triplets are collected from the R2R dataset. The model is pre-trained with two self-supervised learning objectives, and fine-tuned for three tasks: R2R, CVND and HANNA. R2R is an in-domain task, where the language instruction is given at the beginning, describing the full navigation path. CVND and HANNA are out-of-domain tasks; the former is to navigate based on dialog history, while the latter is an interactive environment, where intermediate instructions are given in the middle of navigation.</p>
<p>LENT achieves a new state of the art on all three tasks <sup>2</sup>. Comparing with existing methods, it adapts faster, and generalizes better to unseen environments and new tasks. Our code and pre-trained model is released on GitHub <sup>3</sup>.</p>
<h2>2. Related Work</h2>
<h3>Vision-language pre-training</h3>
<p>Vision-Language Pre-training (VLP) is a rapidly growing research area. The existing approaches employ BERT-like objectives [8] to learn cross-modal representation for various vision-language problems, such as visual question-answering, image-text retrieval and image captioning <em>etc</em>. [25, 27, 17, 34, 24, 15]. However, these VLP works focus on learning representations only for vision-language domains. This paper presents the first pre-trained models, grounding vision-language understanding with actions in a reinforcement learning setting. Further, existing VLP methods require faster R-CNN features as visual inputs [10, 2], which are not readily applicable to VLN. State-of-the-art VLN systems are based on panoramic views (<em>e.g</em>., 36 images per view for R2R), and therefore it is computationally infeasible to extract region features for all views and feed them into the agent.</p>
<h3>Vision-and-language navigation</h3>
<p>Various methods have been proposed for learning to navigate based on vision-language cues. In [9] a panoramic action space and a "speaker" model were introduced for data augmentation. A novel neural decoding scheme was proposed in [12] with search, to balance global and local information. To improve the alignment of the instruction and visual scenes, a visual-textual co-grounding attention mechanism was proposed in [18], which is further improved with a progress monitor [19]. To improve the generalization of the learned policy to unseen environments, reinforcement learning has been considered, including planning [33], and exploration of unseen environments using a off-policy method [32]. An environment dropout was proposed [28] to generate more environments based on the limited data, so that it can generalize well to unseen environments. These methods are specifically designed for particular tasks, and hard to generalize for new tasks. In this paper, we propose the first generic agent that is pre-trained to effectively understand vision-language inputs for a broad range of navigation tasks, and can quickly adapt to new tasks. The most related agent to ours is PRESS [16]. However, our work is different from [16] from two perspectives: (<em>i</em>) PRESS employs an off-the-shelf BERT [8] model for language instruction understanding, while we pre-train a vision-language encoder from scratch, specifically for the navigation tasks. (<em>ii</em>) PRESS only focuses on the R2R task, while we verify the effectiveness of our pre-trained model on three tasks, including two out-of-domain navigation tasks.</p>
<h2>3. Background</h2>
<p>The VLN task can be formulated as a Partially Observable Markov Decision Process (POMDP) $$M = \langle \mathcal{S}, \mathcal{A}, P_s, r \rangle$$, where $$\mathcal{S}$$ is the visual state space, $$\mathcal{A}$$ is a discrete action space, $$P_s$$ is the unknown environment distribution from which we draw the next state, and $$r \in \mathbb{R}$$ is the reward function. At each time step $$t$$, the agent first observes an RGB image $$s_t \in \mathcal{S}$$, and then takes an action $$a_t \in \mathcal{A}$$. This leads the simulator to generate a new image observation $$s_{t+1} \sim P_s(\cdot | s_t, a_t)$$ as the next state. The agent interacts with the environment sequentially, and generates a trajectory of length $$T$$. The episode ends when the agent selects the special STOP action, or when a pre-defined maximum trajectory length is reached. The navigation is successfully completed if the trajectory $$\tau$$ terminates at the intended target location.</p>
<p>In a typical VLN setting, the instructions are represented as a set $$\mathcal{X} = { x_i }<em i_1="i,1">{ i=1}^M$$, where $$M$$ is the number of alternative instructions, and each instruction $$x_i$$ consists of a sequence of $$L_i$$ word tokens, $$x_i = [x</em>_E = { \tau, x } $$, consists of pairs of the instruction $$x$$ together with its corresponding expert trajectory $$\tau$$. The agent then learns to navigate via performing maximum likelihood estimation (MLE) of the policy $$\pi$$, based on the}, x_{i,2}, ..., x_{i,L_i}]$$. The training dataset $$\mathcal{D</p>
<p><sup>2</sup>Among <em>all</em> public results at the time of this submission.</p>
<p><sup>3</sup>https://github.com/weituo12321/PREVALENT</p>
<p>individual sequences:</p>
<p>$\max_{\boldsymbol{\theta}} \mathcal{L}<em _boldsymbol_theta="\boldsymbol{\theta">{\boldsymbol{\theta}}(\boldsymbol{\tau}, \boldsymbol{x})=\log \pi</em>}}(\boldsymbol{\tau} \mid \boldsymbol{x})=\sum_{t=1}^{T} \log \pi_{\boldsymbol{\theta}}\left(\boldsymbol{a<em t="t">{t} \mid \boldsymbol{s}</em>\right),$
where $\boldsymbol{\theta}$ are the policy parameters. The policy is usually parameterized as an attention-based Seq2Seq model [3, 9], trained in the teacher-forcing fashion, i.e., the ground-truth states $\boldsymbol{s}}, \boldsymbol{x<em _boldsymbol_theta="\boldsymbol{\theta">{t}$ are provided at every step in training. This allows reparameterization of the policy as an encoder-decoder architecture, by considering a function decomposition $\pi</em>}}=$ $f_{\boldsymbol{\theta<em _boldsymbol_theta="\boldsymbol{\theta">{E}} \circ f</em>$ :}_{D}</p>
<ul>
<li>A vision-language encoder $f_{\boldsymbol{\theta}<em t="t">{E}}:\left{\boldsymbol{s}</em>}, \boldsymbol{x}\right} \rightarrow \boldsymbol{z<em t="t">{t}$, where a joint representation $\boldsymbol{z}</em>$.}$ at time step $t$ is learned over the visual state $\boldsymbol{s}_{t}$ and the language instruction $\boldsymbol{x</li>
<li>An action decoder $f_{\boldsymbol{\theta}<em t="t">{D}}:\left{\boldsymbol{s}</em>}, \boldsymbol{z<em t="t">{t}\right} \rightarrow \boldsymbol{a}</em>}$. For each joint representation $\boldsymbol{s<em t="t">{t}$, we ground it with $\boldsymbol{s}</em>$.
Successful navigation largely depends on precise joint understanding of natural language instructions and the visual states [29]. We isolate the encoder stage, and focus on pre-training a generic vision-language encoder for various navigation tasks.}$ via neural attention, and decode into actions $\boldsymbol{a}_{t</li>
</ul>
<h2>4 Pre-training Models</h2>
<p>Our pre-training model aims to provide joint representations for image-text inputs in VLN.</p>
<h3>4.1 Input Embeddings</h3>
<p>The input embedding layers convert the inputs (i.e., panoramic views and language instruction) into two sequences of features: image-level visual embeddings and word-level sentence embeddings.</p>
<p>Visual Embedding Following [9], we employ panoramic views as visual inputs to the agent. Each panoramic view consists of 36 images in total ( 12 angles, and 3 camera poses per angle): $s=\left[s_{1}, \cdots, s_{36}\right]$. Each image is represented as a 2176-dimensional feature vector $s=$ $\left[s_{v}, s_{p}\right]$, as a result of the concatenation of two vectors: (i) The 2048-dimensional visual feature $s_{v}$ output by a Residual Network (ResNet) of the image [11]; (ii) the 128-dimensional orientation feature vector $s_{p}$ that repeats $[\sin \psi ; \cos \psi ; \sin \omega ; \cos \omega] 32$ times, where $\psi$ and $\omega$ are the heading and elevation poses, respectively [9]. The embedding for each image is:</p>
<p>$$
\boldsymbol{h}=\operatorname{Layer} \text {-Norm }\left(\mathbf{W}<em e="e">{e} s+\boldsymbol{b}</em>\right))
$$</p>
<p>where $\mathbf{W}<em h="h">{e} \in \mathbb{R}^{d</em>} \times 2176}$ is a weight matrix, and $\boldsymbol{b<em h="h">{e} \in \mathbb{R}^{d</em>=768$ in our experiments. Layer normalization (LN) [4] is used on the output of this fully connected (FC) layer. An illustration of the visual embedding is shown in Figure 2(a).
}}$ is the bias term; $d_{h<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration for the representation procedure of (a) visual embedding and (b) text embedding. FC is the fullyconnected layer, and LN is the layer-normalization layer.</p>
<p>Text Embedding The embedding layer for the language instruction follows the standard Transformer, where LN is applied to the summation of the token embedding and position embedding. An illustration of the text embedding is shown in Figure 2(b).</p>
<h3>4.2 Encoder Architecture</h3>
<p>Our backbone network has three principal modules: two single-modal encoders (one for each modality), followed by a cross-modal encoder. All modules are based on a multilayer Transformer [31]. For the $\ell$-th Transformer layer, its output is</p>
<p>$$
\mathbf{H}<em _ell-1="\ell-1">{\ell}=\mathcal{T}\left(\mathbf{H}</em>\right)
$$}, \mathbf{H}^{\prime}, \mathbf{M</p>
<p>where $\mathbf{H}<em h="h">{l-1} \in \mathbb{R}^{L \times d</em>}}$ is the previous layer's features ( $L$ is the sequence length), $\mathbf{H}^{\prime} \in \mathbb{R}^{L^{\prime} \times d_{h}}$ is the feature matrix to attend, and $\mathbf{M} \in \mathbb{R}^{L \times L^{\prime}}$ is the mask matrix, determining whether a pair of tokens can be attended to each other. More specifically, in each Transformer block, the output vector is the concatenation of multiple attention heads $\mathbf{H<em 1="1" _ell_="\ell,">{\ell}=\left[\mathbf{A}</em>$ is computed via:}, \cdots, \mathbf{A}_{\ell, h}\right]$ ( $h$ is the number of heads). One attention head $\mathbf{A</p>
<p>$$
\begin{aligned}
&amp; \mathbf{A}<em k="k">{\ell}=\operatorname{Softmax}\left(\frac{\mathbf{Q K}^{\top}}{\sqrt{d</em> \
&amp; \mathbf{M}}}}+\mathbf{M}\right) \mathbf{V<em _ell="\ell">{i j}=\left{\begin{array}{cc}
0, &amp; \text { allow to attend } \
-\infty, &amp; \text { not to attend }
\end{array}\right. \
&amp; \mathbf{Q}=\mathbf{W}</em>}^{Q} \mathbf{H}^{\prime}, \mathbf{K}=\mathbf{W<em l-1="l-1">{\ell}^{K} \mathbf{H}</em>}, \mathbf{V}=\mathbf{W<em l-1="l-1">{\ell}^{V} \mathbf{H}</em>
\end{aligned}
$$</p>
<p>where $\mathbf{H}<em _ell="\ell">{l-1}$ and $\mathbf{H}^{\prime}$ are linearly projected to a triple of queries, keys and values using parameter matrices $\mathbf{W}</em>}^{Q}, \mathbf{W<em _ell="\ell">{\ell}^{K}, \mathbf{W}</em>$ is the projection dimension. In the following, we use different mask}^{V} \in \mathbb{R}^{d_{k} \times d_{k}}$, respectively; $d_{k</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of the proposed pre-training model. In this example, two learning objectives are considered: (<em>i</em>) image-attended masked language modeling is performed on the masked word right in the instruction; (<em>ii</em>) action prediction is performed to make the decision to navigate toward direction 180. Only the language features are used for fine-tuning in downstream tasks.</p>
<p>matrices <strong>M</strong> and attended feature matrices <strong>H</strong><sup>0</sup> to construct the contextualized representation for each module.</p>
<h3>Single-modal Encoder</h3>
<p>The standard self-attention layer is used in the single-modal encoder. All of the keys, values, and queries come from the output of the previous layer in the encoder. Each position in the encoder can attend to all positions that belong to its own modality in the previous layer. Specifically, <strong>M</strong> is a full-zero matrix, and <strong>H</strong><sup>0</sup> = <strong>H</strong><sub><em>l</em>−1</sub>. Similar to the self-attention encoder module in the standard Transformer, the position-wise feed-forward network (FFN) is used.</p>
<h3>Cross-modal Encoder</h3>
<p>To fuse the features from both modalities, a cross-attention layer is considered. The queries <strong>H</strong><sup>0</sup> come from the previous layer of the other modality, and the memory keys and values come from the output <strong>H</strong><sub><em>l</em>−1</sub> of the current modality. It allows every position in the encoder to attend over all positions in the different modality. This mimics the typical encoder-decoder attention mechanisms in the Transformer, but here we consider two different modalities, rather than input-output sequences. This cross-attention layer is followed by a self-attention layer and an FFN layer.</p>
<p>The overall model architecture is illustrated in Figure 3. Following [27], <em>L</em><sub>text</sub> = 9, <em>L</em><sub>vision</sub> = 1, and <em>L</em><sub>cross</sub> = 3. The last layer output of the encoder is denoted as <em>z</em> = <em>h</em><sub><em>L</em><sub>cross</sub></sub>, which is used as the features in the downstream tasks.</p>
<h3>4.3 Pre-training Objectives</h3>
<p>We introduce two main tasks to pre-train our model: Image-attended masked language modeling (MLM) and action prediction (AP). For an instruction-trajectory pair {<em>x</em>, <em>τ</em>} from the training dataset <strong>D<sub>E</sub></strong>, we assume a state-action pair from the trajectory follows an independent identical distribution given the instruction in the pre-training stage: (<em>s</em><sub><em>t</em></sub>, <em>α</em><sub><em>t</em></sub>)<sup>ind</sup> <em>p</em>(<em>τ</em>).</p>
<h3>Attended Masked Language Modeling</h3>
<p>We randomly mask out the input words with probability 15%, and replace the masked ones <em>x</em><sub><em>i</em></sub> with special token [MASK]. The goal is to predict these masked words based on the observation of their surrounding words <em>x</em><sub>⊥<em>i</em></sub> and all images <em>s</em> by minimizing the negative log-likelihood:</p>
<p>$$
\mathcal{L}<em _boldsymbol_s="\boldsymbol{s">{\text{MLM}} = -\mathbb{E}</em>} \sim p(\boldsymbol{\tau}), (\boldsymbol{\tau}, \boldsymbol{x}) \sim \mathcal{D<em _backslash="\backslash" i="i">{E}} \log p(x_i | \boldsymbol{x}</em>
$$}, \boldsymbol{s}) \tag{7</p>
<p>This is in analogy to the cloze task in BERT, where the masked word is recovered from surrounding words, but with additional image information to attend. It helps the learned word embeddings to be grounded in the context of visual states. This is particularly important for VLN tasks, where the agent is required to monitor the progress of completed instruction by understanding the visual images.</p>
<h3>Action Prediction</h3>
<p>The output on the special token [CLS] indicates the fused representation of both modalities. We apply an FC layer on top of the encoder output of [CLS] to predict the action. It scores how well the agent can make the correct decision conditioned on the current visual image and the instruction, without referring to the trajectory history. During training, we sample a state-action pair (<em>s</em>, <em>α</em>) from the trajectory <em>τ</em> at each step, and then apply a cross-entropy loss for optimization:</p>
<p>$$
\mathcal{L}<em _boldsymbol_a="(\boldsymbol{a">{\text{AP}} = -\mathbb{E}</em>}, \boldsymbol{s}) \sim p(\boldsymbol{\tau}), (\boldsymbol{\tau}, \boldsymbol{x}) \sim \mathcal{D<em _text_CLS="[\text{CLS">{E}} \log p(\boldsymbol{a}| x</em>
$$}]}, \boldsymbol{s}). \tag{8</p>
<p>The full pre-training objective is:</p>
<p>$$
\mathcal{L}<em _text_MLM="\text{MLM">{\text{Pre-training}} = \mathcal{L}</em>
$$}} + \mathcal{L}_{\text{AP}}. \tag{9</p>
<p>Discussion Other loss designs can be considered for the pre-training objective. Our results on masked image modeling did not show better results, and thus are excluded in the experiments.</p>
<h3>4.4 Pre-training Datasets</h3>
<p>We construct our pre-training dataset based on the Matterport3D Simulator, a photo-realistic visual reinforcement learning (RL) simulation environment for the development of intelligent agents based on the Matterport3D dataset [5]. Specifically, it consists of two sets: (i) The training datasets of R2R, which has 104K image-text-action triplets; (ii) we employed the Speaker model in [9] to synthesize 1,020K instructions for the shortest-path trajectories on the training environments. This leads to 6,482K image-text-action triplets. Therefore, the pre-training dataset size is 6,582K.</p>
<h2>5 Adapting to new tasks</h2>
<p>We focus on three downstream VLN tasks that are based on the Matterport3D simulator. Each task poses a very different challenge to evaluate the agent. (i) The R2R task is used as an in-domain task; it can verify the agent’s generalization capability to unseen environments. (ii) CVDN and HANNA are considered as out-of-domain tasks, to study the generalization ability of the agent to new tasks. More specifically, CVDN considers indirect instructions (i.e., dialog history), and HANNA is an interactive RL task.</p>
<h3>5.1 Room-to-Room</h3>
<p>In R2R, the goal is to navigate from a starting position to a target position with the minimal trajectory length, where the target is explicitly informed via language instruction. To use the pre-trained model for fine-tuning in R2R, the attended contextualized word embeddings are fed into an LSTM encoder-decoder framework, as in [9, 16]. In prior work, random initialization is used in [9], and BERT is used in [16]. In contrast, our word embeddings are pre-trained from scratch with VLN data and tasks.</p>
<h3>5.2 Cooperative Vision-and-Dialogue Navigation</h3>
<p>In the CVDN environment, the Navigation from Dialog History (NDH) is defined, where the agent searches an environment for a goal location, based on the dialog history that consists of multiple turns of question-answering interactions between the the agent and to its partner. The partner has privileged access to the best next steps that the agent should take according to a shortest path planner. CVDN is more challenging than R2R, in that the instructions from the dialog history are often ambiguous, under-specified, and indirect to the final target. The fine-tuning model architecture for CVDN is the same as R2R, except that CVND usually has much longer text input. We limit the sequence length to 300. Words that are longer than 300 in a dialog history are removed.</p>
<h3>5.3 HANNA: Interactive Imitation Learning</h3>
<p>HANNA simulates a scenario where a human requester asks an agent via language to find an object in an indoor environment, without specifying the process of how to complete the task. The only source of help the agent can leverage in the environment is the assistant, who helps the agent by giving subtasks in the form of (i) a natural language instruction that guides the agent to a specific location, and (ii) an image of the view at that location. When the help mode is triggered, we use our pre-trained model to encode the language instructions, and the features are used for the rest of their system.</p>
<h2>6 Experimental Results</h2>
<h3>6.1 Training details</h3>
<p>Pre-training We pre-train the proposed model on eight V100 GPUs, and the batch size for each GPU is 96. The AdamW optimizer [13] is used, and the learning rate is $5 \times$ $10^{-5}$. The total number of training epochs is 20 .</p>
<p>Fine-tuning The fine-tuning is performed on NVIDIA 1080Ti GPU. For the R2R task, we follow the same learning schedule as [28]. When training the augmented listener, we use batch size 20. We continue to fine-tune the crossattention encoder for 20k iterations, with the batch size 10 and learning rate $2 \times 10^{-6}$. For the NDH task, we follow the same learning schedule as in [30], and choose the batch size as 15 and learning rate as $5 \times 10^{-4}$. For HANNA, the training schedule is the same as [22]. The batch size is 32 and learning rate is $1 \times 10^{-4}$.</p>
<h3>6.2 Room-to-Room</h3>
<p>Dataset The R2R dataset [3] consists of 10,800 panoramic views (each panoromic view has 36 images) and 7,189 trajectories. Each trajectory is paired with three natural language instructions. The R2R dataset consists of four splits: train, validation seen and validation unseen, test unseen. The challenge of R2R is to test the agent's generalization ability in unseen environments.</p>
<p>Evaluation Metrics The performance of different agents is evaluated using the following metrics:
TL Trajectory Length measures the average length of the navigation trajectory.
NE Navigation Error is the mean of the shortest path distance in meters between the agent's final location and the target location.</p>
<table>
<thead>
<tr>
<th>Agent</th>
<th>Validation Seen</th>
<th></th>
<th></th>
<th></th>
<th>Validation Unseen</th>
<th></th>
<th></th>
<th></th>
<th>Test Unseen</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>TL $\downarrow$</td>
<td>NE $\downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>TL $\downarrow$</td>
<td>NE $\downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>TL $\downarrow$</td>
<td>NE $\downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
</tr>
<tr>
<td>RANDOM</td>
<td>9.58</td>
<td>9.45</td>
<td>16</td>
<td>-</td>
<td>9.77</td>
<td>9.23</td>
<td>16</td>
<td>-</td>
<td>9.93</td>
<td>9.77</td>
<td>13</td>
<td>12</td>
</tr>
<tr>
<td>SEQ2SEQ</td>
<td>11.33</td>
<td>6.01</td>
<td>39</td>
<td>-</td>
<td>8.39</td>
<td>7.81</td>
<td>22</td>
<td>-</td>
<td>8.13</td>
<td>7.85</td>
<td>20</td>
<td>18</td>
</tr>
<tr>
<td>RPA</td>
<td>-</td>
<td>5.56</td>
<td>43</td>
<td>-</td>
<td>-</td>
<td>7.65</td>
<td>25</td>
<td>-</td>
<td>9.15</td>
<td>7.53</td>
<td>25</td>
<td>23</td>
</tr>
<tr>
<td>SPEAKER-FOLLOWER</td>
<td>-</td>
<td>3.36</td>
<td>66</td>
<td>-</td>
<td>-</td>
<td>6.62</td>
<td>35</td>
<td>-</td>
<td>14.82</td>
<td>6.62</td>
<td>35</td>
<td>28</td>
</tr>
<tr>
<td>SMNA</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>18.04</td>
<td>5.67</td>
<td>48</td>
<td>35</td>
</tr>
<tr>
<td>RCM+SIL(TRAIN)</td>
<td>10.65</td>
<td>3.53</td>
<td>67</td>
<td>-</td>
<td>11.46</td>
<td>6.09</td>
<td>43</td>
<td>-</td>
<td>11.97</td>
<td>6.12</td>
<td>43</td>
<td>38</td>
</tr>
<tr>
<td>REGRETFUL</td>
<td>-</td>
<td>3.23</td>
<td>69</td>
<td>63</td>
<td>-</td>
<td>5.32</td>
<td>50</td>
<td>41</td>
<td>13.69</td>
<td>5.69</td>
<td>48</td>
<td>40</td>
</tr>
<tr>
<td>FAST</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>21.17</td>
<td>4.97</td>
<td>56</td>
<td>43</td>
<td>22.08</td>
<td>5.14</td>
<td>54</td>
<td>41</td>
</tr>
<tr>
<td>ENVDrop</td>
<td>11.00</td>
<td>3.99</td>
<td>62</td>
<td>59</td>
<td>10.70</td>
<td>5.22</td>
<td>52</td>
<td>48</td>
<td>11.66</td>
<td>5.23</td>
<td>51</td>
<td>47</td>
</tr>
<tr>
<td>PRESS</td>
<td>10.57</td>
<td>4.39</td>
<td>58</td>
<td>55</td>
<td>10.36</td>
<td>5.28</td>
<td>49</td>
<td>45</td>
<td>10.77</td>
<td>5.49</td>
<td>49</td>
<td>45</td>
</tr>
<tr>
<td>PREVALENT (ours)</td>
<td>10.32</td>
<td>3.67</td>
<td>69</td>
<td>65</td>
<td>10.19</td>
<td>4.71</td>
<td>58</td>
<td>53</td>
<td>10.51</td>
<td>5.30</td>
<td>54</td>
<td>51</td>
</tr>
<tr>
<td>PRESS</td>
<td>10.35</td>
<td>3.09</td>
<td>71</td>
<td>67</td>
<td>10.06</td>
<td>4.31</td>
<td>59</td>
<td>55</td>
<td>10.52</td>
<td>4.53</td>
<td>57</td>
<td>53</td>
</tr>
<tr>
<td>PREVALENT</td>
<td>10.31</td>
<td>3.31</td>
<td>67</td>
<td>63</td>
<td>9.98</td>
<td>4.12</td>
<td>60</td>
<td>57</td>
<td>10.21</td>
<td>4.52</td>
<td>59</td>
<td>56</td>
</tr>
<tr>
<td>Human</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>11.85</td>
<td>1.61</td>
<td>86</td>
<td>76</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison with the state-of-the-art methods on R2R. Blue indicates the best value in a given setting. $\mathbf{S}$ indicates the single-instruction setting, $\mathbf{M}$ indicates the multiple-instruction setting.</p>
<p>SR Success Rate is the percentage of the agent's final location that is less than 3 meters away from the target location. SPL Success weighted by Path Length [1] trades-off SR against TL. A higher score represents more efficiency in navigation. Among these metrics, SPL is the recommended primary metric, and other metrics are considered as auxiliary measures.</p>
<p>Baselines We compare our approach with nine recently published systems:</p>
<ul>
<li>RANDOM: an agent that randomly selects a direction and moves five step in that direction [3].</li>
<li>S2S-ANDERSON: a sequence-to-sequence model using a limited discrete action space [3].</li>
<li>RPA [33]: an agent that combines model-free and model-based reinforcement learning, using a lookahead module for planning.</li>
<li>SPEAKER-FOLLOWER [9]: an agent trained with data augmentation from a speaker model on the panoramic action space.</li>
<li>SMNA [18]: an agent trained with a visual-textual co-grounding module and a progress monitor on the panoramic action space.</li>
<li>RCM+SIL [32]: an agent trained with cross-modal grounding locally and globally via RL.</li>
<li>REGRETFUL [19]: an agent with a trained progress monitor heuristic for search that enables backtracking.</li>
<li>FAST [12]: an agent that uses a fusion function to score and compare partial trajectories of different lengths, which enables the agent to efficiently backtrack after a mistake.</li>
<li>EnvDROP [28]: an agent is trained with environment dropout, which can generate more environments based on the limited seen environments.</li>
<li>PRESS [16]: an agent is trained with pre-trained language models and stochastic sampling to generalize well in the unseen environment.</li>
</ul>
<p>Comparison with SoTA Table 1 compares the performance of our agent against the existing published top systems. ${ }^{4}$. Our agent Prevalent outperforms the existing models on SR and SPL by a large margin. On both validation seen and unseen environments, Prevalent outperforms other agents on nearly all metrics.</p>
<p>In PRESS [16], multiple introductions are used. To have a fair comparison, we follow [16], and report Prevalent results. We see that testing SPL is improved. Further, the gap between seen and unseen environments of Prevalent is smaller than PRESS, meaning that image-attended language understanding is more effective to help the agent generalize better to an unseen environment.</p>
<h3>6.3. Cooperative Vision-and-Dialogue Navigation</h3>
<p>Dataset \&amp; Evaluation Metric The CVDN dataset has 2050 human-human navigation dialogs, comprising over 7 K navigation trajectories punctuated by question-answer exchanges, across 83 MatterPort houses [5]. The metrics for R2R can be readily used for the CVDN dataset. Further, one new metric is proposed for the NDH task: GP Goal Progress measures the difference between completed distance and left distance to the goal. Larger</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup> <sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: ${ }^{4}$ The full list of leaderboard is publicly available: https:// evalai.cloudcv.org/web/challenges/challenge-page/ 97/leaderboard/270</p>
<table>
<thead>
<tr>
<th></th>
<th>Validation Unseen</th>
<th></th>
<th></th>
<th>Test Unseen</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Agent</td>
<td>Oracle</td>
<td>Navigator</td>
<td>Mixed</td>
<td>Oracle</td>
<td>Navigator</td>
<td>Mixed</td>
</tr>
<tr>
<td>RANDOM</td>
<td>1.09</td>
<td>1.09</td>
<td>1.09</td>
<td>0.83</td>
<td>0.83</td>
<td>0.83</td>
</tr>
<tr>
<td>SEQ2SEQ</td>
<td>1.23</td>
<td>1.98</td>
<td>2.10</td>
<td>1.25</td>
<td>2.11</td>
<td>2.35</td>
</tr>
<tr>
<td>PREVALENT (Ours)</td>
<td>2.58</td>
<td>2.99</td>
<td>3.15</td>
<td>1.67</td>
<td>2.39</td>
<td>2.44</td>
</tr>
<tr>
<td>SHORTEST PATH AGENT</td>
<td>8.36</td>
<td>7.99</td>
<td>9.58</td>
<td>8.06</td>
<td>8.48</td>
<td>9.76</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on CVDN measured by Goal Progress. Blue indicates the best value in a given setting.</p>
<table>
<thead>
<tr>
<th></th>
<th>SEEN-ENV</th>
<th></th>
<th></th>
<th></th>
<th>UNSEEN-ALL</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Agent</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>NE $\downarrow$</td>
<td>#R $\downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>NE $\downarrow$</td>
</tr>
<tr>
<td>$\stackrel{\text { ® }}{\sim}$</td>
<td>RANDOM WALK</td>
<td>0.54</td>
<td>0.33</td>
<td>15.38</td>
<td>0.0</td>
<td>0.46</td>
<td>0.23</td>
<td>15.34</td>
</tr>
<tr>
<td></td>
<td>FORWARD 10</td>
<td>5.98</td>
<td>4.19</td>
<td>14.61</td>
<td>0.0</td>
<td>6.36</td>
<td>4.78</td>
<td>13.81</td>
</tr>
<tr>
<td></td>
<td>No ASSISTANCE</td>
<td>17.21</td>
<td>13.76</td>
<td>11.48</td>
<td>0.0</td>
<td>8.10</td>
<td>4.23</td>
<td>13.22</td>
</tr>
<tr>
<td></td>
<td>ANNA</td>
<td>88.37</td>
<td>63.92</td>
<td>1.33</td>
<td>2.9</td>
<td>47.45</td>
<td>25.50</td>
<td>7.67</td>
</tr>
<tr>
<td></td>
<td>PREVALENT (Ours)</td>
<td>83.82</td>
<td>59.38</td>
<td>1.47</td>
<td>3.4</td>
<td>52.91</td>
<td>28.72</td>
<td>5.29</td>
</tr>
<tr>
<td></td>
<td>$\stackrel{\text { ® }}{\sim}$ SHORTEST</td>
<td>100.00</td>
<td>100.00</td>
<td>0.00</td>
<td>0.0</td>
<td>100.00</td>
<td>100.00</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>Perfect assistance</td>
<td>90.99</td>
<td>68.87</td>
<td>0.91</td>
<td>2.5</td>
<td>83.56</td>
<td>56.88</td>
<td>1.83</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on test splits of HANNA. The agent with "perfect assistance" uses the teacher navigation policy to make decisions when executing a subtask from the assistant. Blue indicates the best value.
values indicate a more efficient agent.
Three settings are considered, depending on which ground-truth action/path is employed [30]. Oracle indicates the shortest path, and Navigator indicates the path taken by the navigator. The Mixed supervision path means to take the navigator path if available, otherwise the shortest path. The results are in Table 2. The proposed Prevalent significantly outperforms the Seq2Seq baseline on both validation and testing unseen environments in all settings, leading to the top position on the leaderboard ${ }^{5}$. Note that our encoder is pre-trained on R2R dataset. We observe that it can provide significant improvement when used the new task built on the CVDN dataset. This shows that the pre-trained model can adapt well on new tasks, and yields better generalization.</p>
<h3>6.4 HANNA</h3>
<p>Dataset \&amp; Evaluation Metric The HANNA dataset features 289 object types; the language instruction vocabulary contains 2,332 words. The numbers of locations on the shortest paths to the requested objects are restricted to be between 5 and 15 . With an average edge length of 2.25 meters, the agent has to travel about 9 to 32 meters to reach its goals. Similar to R2R, SR, SPL and NE are used to evaluate the navigation. Further, one new metric is considered for this interactive task:</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>#R Number of requests measures how many helps are requested by the agent.
The results are shown in Table 3. Two rule-based methods and two skyline methods are reported as references; see [22] for details. Our Prevalent outperforms the baseline agent ANNA on the test unseen environments in terms of SR, SPL and NE, while requesting a slightly higher number of helps (#R). When measuring the performance gap between seen and unseen environments, we see that Prevalent shows a significantly smaller difference than ANNA, e.g., (59.38-28.72=30.66) vs (63.92-25.50=38.42) for SPL. This means that the pre-trained joint representation by Prevalent can reduce over-fitting, and generalise better to unseen environments.</p>
<h3>6.5 Ablation Studies</h3>
<p>Is pre-training with actions helpful? Our pre-training objective in (9) includes two losses, $\mathcal{L}<em _mathrm_MLM="\mathrm{MLM">{\mathrm{PA}}$ and $\mathcal{L}</em>}}$. To study the impact of each loss, we pre-train two model variants: one is based on the full objective $\mathcal{L<em _mathrm_MLM="\mathrm{MLM">{\mathrm{PA}}+\mathcal{L}</em>$. To verify its impact on new tasks, we consider CVDN first, and the results are shown in Table 4. Three types of text inputs are considered: Navigation QA, Orcale Answer, and All (a combination of both). More details are provided in the Appendix.}}$, the other only uses $\mathcal{L}_{\mathrm{MLM}</p>
<p>When $\mathcal{L}_{\mathrm{PA}}$ is employed in the objective, we see consistent improvement on nearly all metrics and settings. Note that our MLM is different from BERT in that the attention over images is used in the cross-layer. To verify whether the image-attended learning is necessary, we consider BERT in</p>
<table>
<thead>
<tr>
<th></th>
<th>Navigation QA</th>
<th></th>
<th></th>
<th>Oracle Answer</th>
<th></th>
<th></th>
<th></th>
<th>All</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Methods</td>
<td>Oracle</td>
<td>Navigator</td>
<td>Mixed</td>
<td>Oracle</td>
<td>Navigator</td>
<td>Mixed</td>
<td>Oracle</td>
<td>Navigator</td>
<td>Mixed</td>
<td></td>
<td></td>
</tr>
<tr>
<td>$\mathcal{L}<em _text_MLM="\text{MLM">{\text{PA}}+\mathcal{L}</em>$}</td>
<td>2.80</td>
<td>3.01</td>
<td>3.28</td>
<td>2.78</td>
<td>3.44</td>
<td>3.38</td>
<td>2.58</td>
<td>2.99</td>
<td>3.15</td>
<td></td>
<td></td>
</tr>
<tr>
<td>$\mathcal{L}_{\text{MLM}}$</td>
<td>2.69</td>
<td>3.00</td>
<td>3.25</td>
<td>2.84</td>
<td>3.35</td>
<td>3.19</td>
<td>2.52</td>
<td>2.98</td>
<td>3.14</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BERT pre-trainig</td>
<td>2.26</td>
<td>2.71</td>
<td>2.94</td>
<td>2.70</td>
<td>2.68</td>
<td>3.06</td>
<td>2.46</td>
<td>2.74</td>
<td>2.64</td>
<td></td>
<td></td>
</tr>
<tr>
<td>BERT fine-tuning</td>
<td>2.39</td>
<td>2.03</td>
<td>2.51</td>
<td>2.23</td>
<td>2.41</td>
<td>2.52</td>
<td>2.32</td>
<td>2.93</td>
<td>2.28</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 4: Ablation study of the pre-training objectives on CVDN, measured by Goal Progress. Blue indicates the best value.</p>
<table>
<thead>
<tr>
<th></th>
<th>Validation Seen</th>
<th></th>
<th></th>
<th></th>
<th>Validation Unseen</th>
<th></th>
<th></th>
<th></th>
<th>Test Unseen</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Methods</td>
<td>TL $\downarrow$</td>
<td>NE $\downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>TL $\downarrow$</td>
<td>NE $\downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
<td>TL $\downarrow$</td>
<td>NE $\downarrow$</td>
<td>SR $\uparrow$</td>
<td>SPL $\uparrow$</td>
</tr>
<tr>
<td>Two-stage</td>
<td>10.32</td>
<td>3.67</td>
<td>0.69</td>
<td>0.66</td>
<td>10.19</td>
<td>4.71</td>
<td>0.58</td>
<td>0.53</td>
<td>10.51</td>
<td>5.30</td>
<td>0.54</td>
<td>0.51</td>
</tr>
<tr>
<td>Feature-based</td>
<td>10.13</td>
<td>3.98</td>
<td>0.66</td>
<td>0.64</td>
<td>9.70</td>
<td>5.01</td>
<td>0.54</td>
<td>0.51</td>
<td>9.99</td>
<td>5.54</td>
<td>0.52</td>
<td>0.49</td>
</tr>
</tbody>
</table>
<p>Table 5: Ablation study on R2R: feature-based vs fine-tuning. Blue indicates the better value.
two ways. (i) BERT pre-training: we apply the original MLM loss in BERT on our R2R pre-training dataset. The newly pre-trained BERT is used for fine-tuning on CVDN. (ii) BERT fine-tuning: we directly fine-tune off-the-shelf BERT on CVDN. Their performances are lower than the two variants of the proposed PREVALENT. This means our image-attended MLM is more effective for navigation tasks. More ablation studies on the pre-training objectives are conducted for HANNA, with results shown in the Appendix.</p>
<p>Feature-based vs Fine-tuning The pre-trained encoder can be used in two modes: (i) fine-tuning approach, where a task-specific layer is added to the pre-trained model, and all parameters are jointly updated on a downstream task. (ii) feature-based approach, where fixed features are extracted from the pre-trained model, and only the task-specific layer is updated. In this paper, all PREVALENT presented results generally have used the feature-based approach, as there are major computational benefits to pre-computing an expensive representation of the training data once, and then running many experiments with cheaper models on top of this representation. In the R2R dataset, we consider a twostage scheme, where we fine-tune the cross-attention layers of the agent, after training via the feature-based approach. The results are reported in Table 5. We observe notable improvement with this two-stage scheme on nearly all metrics, expect the trajectory length.</p>
<p>How does pre-training help generalization? We plot the learning curves on the seen/unseen environments for R2R in Figure 4(a), and CVDN in Figure 4(b). Compared with the random initialized word embeddings in EnvDrop <em>[28]</em>, the pre-trained word embeddings can adapt faster (especially in the early stage), and converge to higher performance in unseen environments. This is demonstrated by the SPL values in the Figure 4(a). By comparing the learning curves in Figure 4(b), we see a much smaller gap between seen and un-
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Learning curves on (a) R2R and (b) CVDN.
seen environments for PREVALENT than the Seq2Seq baseline <em>[30]</em>, meaning pre-training is an effective tool to help reduce over-fitting in learning.</p>
<h2>7 Conclusions</h2>
<p>We present Prevalent, a new pre-training and finetuning paradigm for vision-and-language navigation problems. This allows for more effective use of limited training data to improve generalization to previously unseen environments, and new tasks. The pre-trained encoder can be easily plugged into existing models to boost their performance. Empirical results on three benchmarks (R2R, CVDN and HANNA) demonstrate that Prevalent significantly improves over existing methods, achieving new state-of-the-art performance.</p>
<h2>References</h2>
<p>[1] Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, and Amir Zamir. On evaluation of embodied navigation agents. arXiv preprint arXiv:1807.06757, 2018. 6
[2] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018. 2
[3] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR, volume 2, 2018. 1, 3, 5, 6
[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3
[5] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Nießner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGBD data in indoor environments. International Conference on 3D Vision (3DV), 2017. 5, 6
[6] Howard Chen, Alane Shur, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. CVPR, 2010. 1
[7] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In CVPR, 2018. 1
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. NAACL, 2019. 2, 11
[9] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor BergKirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. NIPS, 2018. 1, 2, 3, 5, 6, 11
[10] Ross Girshick. Fast R-CNN. In CVPR, 2015. 2
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 3
[12] Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, and Siddhartha Srinivasa. Tactical rewind: Self-correction via backtracking in vision-and-language navigation. CVPR, 2019. 2, 6
[13] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5
[14] Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An interactive 3D environment for visual AI. arXiv preprint arXiv:1712.05474, 2017. 1
[15] Gen Li, Nan Duan, Yuejian Fang, Daxin Jiang, and Ming Zhou. Unicoder-VL: A universal encoder for vision and language by cross-modal pre-training. arXiv preprint arXiv:1908.06066, 2019. 2
[16] Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah Smith, and Yejin Choi. Robust navigation with language pretraining and stochastic sampling. EMNLP, 2019. 2, 5, 6
[17] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. VilBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. NIPS, 2019. 2
[18] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. Selfmonitoring navigation agent via auxiliary progress estimation. ICLR, 2019. 1, 2, 6
[19] Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, and Zsolt Kira. The regretful agent: Heuristic-aided navigation through progress estimation. CVPR, 2019. 2, 6
[20] Dipendra Misra, John Langford, and Yoav Artzi. Mapping instructions and visual observations to actions with reinforcement learning. EMNLP, 2017. 1
[21] Will Monroe, Robert XD Hawkins, Noah D Goodman, and Christopher Potts. Colors in context: A pragmatic neural model for grounded language understanding. TACL, 2017. 1
[22] Khanh Nguyen and Hal Daumé III. Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. EMNLP, 2019. 1, 5,7
[23] Manolis Savva, Angel X Chang, Alexey Dosovitskiy, Thomas Funkhouser, and Vladlen Koltun. MINOS: Multimodal indoor simulator for navigation in complex environments. arXiv preprint arXiv:1712.03931, 2017. 1
[24] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. VL-BERT: Pre-training of generic visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019. 2
[25] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. VideoBERT: A joint model for video and language representation learning. ICCV, 2019. 2
[26] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In NIPS, 2014. 1
[27] Hao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from transformers. EMNLP, 2019. 2, 4
[28] Hao Tan, Licheng Yu, and Mohit Bansal. Learning to navigate unseen environments: Back translation with environmental dropout. EMNLP, 2019. 2, 5, 6, 8
[29] Jesse Thomason, Daniel Gordon, and Yonatan Bisk. Shifting the baseline: Single modality performance on visual navigation \&amp; qa. In NAACL, 2019. 3
[30] Jesse Thomason, Michael Murray, Maya Cakmak, and Luke Zettlemoyer. Vision-and-dialog navigation. CoRL, 2019. 1, $5,7,8$
[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. 3
[32] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and selfsupervised imitation learning for vision-language navigation. CVPR, 2019. 1, 2, 6</p>
<p>[33] Xin Wang, Wenhan Xiong, Hongmin Wang, and William Yang Wang. Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-and-language navigation. ECCV, 2018. 1, 2, 6
[34] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J Corso, and Jianfeng Gao. Unified vision-language pretraining for image captioning and VQA. AAAI, 2020. 2, 11</p>
<h1>Supplementary Material: Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-training</h1>
<p>Summary of Contributions. Weituo implemented the algorithm, made the model work, and ran all experiments. Chunyuan initiated the idea of pre-training the first generic agent for VLN, led and completed the manuscript writing. Xiujun provided the codebase and helped implementation. Lawrence and Jianfeng edited the final manuscript.</p>
<h2>A. Pre-training Dataset Preparation</h2>
<p>We found that the largest VLN training dataset R2R contains only 104 K samples, an order magnitude smaller than the pre-training datasets typically used in language [8] or vision-and-language pre-training [34]. This renders a case where pre-training can be degraded due to insufficient training data, while harvesting such samples with human annotations is expensive. Fortunately, we can resort to generative models to synthesize the samples. We first train an seq2seq auto-regressive model (i.e., a speaker model [9]) that can produce language instructions conditioned on the agent trajectory (a sequence of actions and visual images) on R2R dataset; then collect a large number of shortest trajectories using
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The percentage of pre-training datasets. The synthesized dataset occupies $98.4 \%$.
the Matterport 3D Simulator, and synthesize their corresponding instructions using the speaker model. This leads to 6482 K new training samples. The two datasets are compared in Figure 4(b). The agent is pre-trained on the combined dataset. Our results show that synthetic samples produced by generative models can be incorporated into the pre-training data and helps self-supervised learning.</p>
<h2>B. Experiments</h2>
<p>Three types of inputs on CVDN We illustrate the naming of three types of text inputs on CVDN in Table 6.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">$V$</th>
<th style="text-align: center;">$t_{0}$</th>
<th style="text-align: center;">$A_{i}$</th>
<th style="text-align: center;">$Q_{i}$</th>
<th style="text-align: center;">$Q_{1: i-1} \&amp; A_{1: i-1}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Oracle Answer</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Navigation QA</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 6: Three types of inputs on CVDN. $t_{0}$ is the target object, $V$ is the ResNet feature. $Q_{i}$ and $A_{i}$ are the question and answers in the $i$-th turn. $Q_{1: i-1} \&amp; A_{1: i-1}$ are the question \&amp; answer pairs before the $i$-th turn.</p>
<p>Ablation Study Results on HANNA Table 7 shows the results with different pre-training objectives. We see that the $\mathcal{L}<em _mathrm_MLM="\mathrm{MLM">{\mathrm{PA}}+\mathcal{L}</em>$ yields the best performance among all variants.}</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Agent</th>
<th style="text-align: center;">SEEN-ENV</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">UNSEEN-ALL</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
<td style="text-align: center;">NE $\downarrow$</td>
<td style="text-align: center;">#R $\downarrow$</td>
<td style="text-align: center;">SR $\uparrow$</td>
<td style="text-align: center;">SPL $\uparrow$</td>
<td style="text-align: center;">NE $\downarrow$</td>
<td style="text-align: center;">#R $\downarrow$</td>
</tr>
<tr>
<td style="text-align: center;">Prevalent $\left(\mathcal{L}<em _mathrm_MLM="\mathrm{MLM">{\mathrm{PA}}+\mathcal{L}</em>\right)$}</td>
<td style="text-align: center;">83.82</td>
<td style="text-align: center;">59.38</td>
<td style="text-align: center;">1.47</td>
<td style="text-align: center;">3.4</td>
<td style="text-align: center;">52.91</td>
<td style="text-align: center;">28.72</td>
<td style="text-align: center;">5.29</td>
<td style="text-align: center;">6.6</td>
</tr>
<tr>
<td style="text-align: center;">Prevalent $\left(\mathcal{L}_{\mathrm{MLM}}\right)$</td>
<td style="text-align: center;">78.75</td>
<td style="text-align: center;">54.68</td>
<td style="text-align: center;">1.82</td>
<td style="text-align: center;">4.3</td>
<td style="text-align: center;">44.29</td>
<td style="text-align: center;">24.27</td>
<td style="text-align: center;">6.33</td>
<td style="text-align: center;">8.1</td>
</tr>
<tr>
<td style="text-align: center;">BERT (feature-based)</td>
<td style="text-align: center;">57.54</td>
<td style="text-align: center;">34.33</td>
<td style="text-align: center;">4.71</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">24.12</td>
<td style="text-align: center;">11.50</td>
<td style="text-align: center;">9.55</td>
<td style="text-align: center;">11.3</td>
</tr>
<tr>
<td style="text-align: center;">BERT (fine-tuning)</td>
<td style="text-align: center;">80.75</td>
<td style="text-align: center;">57.46</td>
<td style="text-align: center;">1.97</td>
<td style="text-align: center;">4.0</td>
<td style="text-align: center;">26.36</td>
<td style="text-align: center;">12.66</td>
<td style="text-align: center;">9.1</td>
<td style="text-align: center;">8.3</td>
</tr>
</tbody>
</table>
<p>Table 7: Ablation study of pre-training objectives on test splits of HANNA.</p>
<h1>C. Comparison with Related Work</h1>
<p>Comparison with Press. The differences are summarized in Table 8 (a). Empirically, we show that (1) incorporating visual and action information into pre-training can improve navigation performance; (2) Pre-training can generalize across different new navigation tasks.</p>
<p>Comparison with vision-language pre-training (VLP). The differences are in Table 8 (b). Though the proposed methodology generally follows self supervised learning such as VLP or BERT, our research scope and problem setups are different, which renders existing pre-models are not readily applicable.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Prevalent (Proposed)</th>
<th style="text-align: left;">Press</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: left;">Augmented R2R dataset</td>
<td style="text-align: left;">Generic language</td>
</tr>
<tr>
<td style="text-align: left;">Modality</td>
<td style="text-align: left;">Vision-language-action triplets</td>
<td style="text-align: left;">Language</td>
</tr>
<tr>
<td style="text-align: left;">Learning</td>
<td style="text-align: left;">Train from scratch</td>
<td style="text-align: left;">Off-the-shelf (BERT)</td>
</tr>
<tr>
<td style="text-align: left;">Downstream</td>
<td style="text-align: left;">Three navigation tasks</td>
<td style="text-align: left;">R2R</td>
</tr>
</tbody>
</table>
<p>(a) Press</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Prevalent (Proposed)</th>
<th style="text-align: left;">VLP</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Visual Input</td>
<td style="text-align: left;">Panoramic views <br> (Size: $36 \times 640 \times 480)$</td>
<td style="text-align: left;">Single image <br> (Size: $640 \times 480)$</td>
</tr>
<tr>
<td style="text-align: left;">Visual Features</td>
<td style="text-align: left;">ResNet (View-level)</td>
<td style="text-align: left;">Fast RCNN (Object-level)</td>
</tr>
<tr>
<td style="text-align: left;">Objectives</td>
<td style="text-align: left;">Attentive MLM \&amp; <br> Action Prediction</td>
<td style="text-align: left;">Masking on VL \&amp; <br> Same-Pair Prediction</td>
</tr>
<tr>
<td style="text-align: left;">Downstream</td>
<td style="text-align: left;">RL: Navigation in sequential <br> decision-making environments</td>
<td style="text-align: left;">Single-step prediction</td>
</tr>
</tbody>
</table>
<p>Table 8: Comparison with related works.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ The full list of leaderboard is publicly available: https:// evalai.cloudcv.org/web/challenges/challenge-page/ 463/leaderboard/1292&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
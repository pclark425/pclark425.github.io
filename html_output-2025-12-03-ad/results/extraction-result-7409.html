<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7409 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7409</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7409</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-264147046</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.10141v1.pdf" target="_blank">A Search for Prompts: Generating Structured Answers from Contracts</a></p>
                <p><strong>Paper Abstract:</strong> In many legal processes being able to action on the concrete implication of a legal question can be valuable to automating human review or signalling certain conditions (e.g., alerts around automatic renewal). To support such tasks, we present a form of legal question answering that seeks to return one (or more) fixed answers for a question about a contract clause. After showing that unstructured generative question answering can have questionable outcomes for such a task, we discuss our exploration methodology for legal question answering prompts using OpenAI's \textit{GPT-3.5-Turbo} and provide a summary of insights. Using insights gleaned from our qualitative experiences, we compare our proposed template prompts against a common semantic matching approach and find that our prompt templates are far more accurate despite being less reliable in the exact response return. With some additional tweaks to prompts and the use of in-context learning, we are able to further improve the performance of our proposed strategy while maximizing the reliability of responses as best we can.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7409.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7409.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SemanticSimilarity-ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot semantic similarity via sequence embeddings (LegalBERT / OpenAI Ada)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-generative baseline that embeds clause text and answer-option phrasings with a sentence/sequence embedding model (LegalBERT or OpenAI Ada) and selects the option with maximum cosine similarity to the clause.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LegalBERT; OpenAI Ada (embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LegalBERT: transformer encoder trained/fine-tuned on legal corpora (Pile-Of-Law). OpenAI Ada: OpenAI's Ada embedding model (training details not disclosed). Both produce sequence embeddings used with cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Structured multiple-choice question answering (indemnity; information-sharing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a legal clause, select which predefined answer option(s) the clause implies (single‑choice for indemnity, multi‑select for information‑sharing).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot embedding-based multiple-choice: embed clause and each option and pick option with highest cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type / prompt style (baseline non-generative)</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot (no examples); answer-option phrasings varied (S1–S4, T1–T2) to test sensitivity; clauses limited to <20k characters to satisfy token limits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Used two embedding models (LegalBERT trained on Pile-Of-Law; OpenAI Ada). Evaluated on two tasks: 121 environmental indemnity clauses and 143 permitted-use-of-confidential-information clauses. Cosine similarity argmax decision rule.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7409.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7409.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Generative-P1-ZeroShot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative prompting using P1 (zero-shot) with GPT-3.5-Turbo / GPT-4 / PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generation-based structured-answer approach: a carefully worded zero-shot prompt (P1) instructs the model to select exactly one numbered option from a provided list and to respond only with that option (or the exact phrase 'The clause is silent').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo; GPT-4; PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large autoregressive transformer chat models (GPT-3.5-Turbo and GPT-4 by OpenAI) and PaLM2 by Google; used in generation mode to output structured answers per prompt instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Structured multiple-choice question answering (indemnity; information-sharing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as for semantic baseline: map clause text to one of predefined answer options; P1 restricts output to a single option or a special escape phrase.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language instruction prompt (P1) with embedded clause and explicit answer options (single‑choice); zero‑shot generation.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / instruction framing</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>P1: 'Referring only to the information contained in the clause below, only select which one of the below numbered options is implied by the clause, without providing any other information or justification. If you cannot determine which of the conditions are implied, respond with the exact text: "The clause is silent".' Options inserted (S1–S4 or T1–T2). Temperature generally set to 0 in practice.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Indemnity (GPT-3.5-Turbo, P1, zero-shot): 0.74 accuracy; Information-sharing (GPT-3.5-Turbo, P1, zero-shot): 0.55 accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Semantic similarity baseline (LegalBERT / OpenAI Ada); generative P1 reported to substantially improve over semantic similarity (exact numeric baselines not reported in text for all comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative: generative prompting (P1) non-trivially and substantially outperforms semantic similarity baseline; exact absolute change not uniformly reported. GPT-4 and PaLM2 perform better than GPT-3.5-Turbo (no precise numbers given in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Used GPT-3.5-Turbo (primary), GPT-4 and PaLM2 (preliminary). Temperature generally 0. Clauses <20k characters. Options sets S1–S4 (indemnity), T1–T2 (information-sharing). Parsing/post-processing applied to handle varied output formats.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7409.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7409.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PromptVariants-P1P2P3P4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt template variants (P1, P2, P3, P4) and their effects</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparing small and large changes to the prompt template shows that small wording tweaks (P2) often have little effect, while more explicit rules and multi-select capable prompts (P3/P4) can substantially improve correctness and reduce unwanted generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo (primary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following chat model evaluated under different prompt templates (P1–P4).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Structured multiple-choice question answering (indemnity; information-sharing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>As above; tests how changes in prompt phrasing, rules (e.g., explicit 'respond only with number', escape hatches), and whether multiple selections are allowed affect model output.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt style variations: single-choice constrained generation (P1/P2), multi-select with explicit rules and escape hatches (P3), role-play + explicit question + multi-select (P4).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / instruction framing</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>P2: small rewording of P1 (little effect). P3: explicit rules including 'If it cannot be determined... respond with "Unable to determine."' and allow multiple selections — produced substantive improvements in correct answers but required more post-processing. P4 (role-play): asking model to 'pretend you are a party' and including the explicit question + multi-select improved information-sharing accuracy markedly.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>number correct / accuracy (reported qualitatively)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>P1 (baseline prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>P2: no substantive change vs P1; P3: substantive improvements in correctness vs P1 (qualitative); P4: substantial accuracy improvement for information-sharing task vs simpler prompts (qualitative).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-3.5-Turbo; temperature 0; tested on indemnity and information-sharing datasets; shuffling option order had no effect; changes in prompt wording, adding rules, allowing multi-select and role-play tested.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7409.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7409.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OptionsPhrasing-S1S4-T1T2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Answer-option phrasing variants (S1–S4 for indemnity, T1–T2 for information-sharing)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different phrasings of answer options (complete sentences vs fragments, using synonyms, explicitly equating legal terms) materially affect both embedding similarity baselines and generative model accuracy; clearer, complete-sentence options tend to produce higher accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to embedding baselines (LegalBERT, OpenAI Ada) and generative models (GPT-3.5-Turbo, GPT-4, PaLM2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>N/A (this entity concerns prompt/option formatting rather than a single model architecture).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Structured multiple-choice question answering (indemnity; information-sharing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Selecting correct option(s) given a clause; investigates how wording of provided answer options affects selection.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple-choice option phrasing variations: S1–S4 (different ways to express indemnity answers, including explicit synonyms Lessee/Tenant etc.); T1 vs T2 (information-sharing options, T2 uses clear complete sentences).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / answer-option design</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Findings: options that are 'as complete a thought or sentence as possible' reduce opportunities for the model to generate extra text; wrapping options in punctuation (periods/quotes) sometimes mitigates extra generation; explicitly including term equivalences (e.g., Lessee=Tenant) reduced output variability; T2 (complete sentences) yielded substantive improvement over T1 for information‑sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / consistency (qualitative)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>T2 > T1: substantive improvement in accuracy for information-sharing (qualitative); different S1–S4 forms produce different accuracies across methods (no single form uniformly optimal).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Across both embedding and generative approaches; tested S1–S4 and T1–T2 option sets on the two tasks; often used with prompts P1–P4.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7409.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7409.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InContextFewShot-E1E2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context learning / few-shot seeding with example sets E1, E2 (GPT-3.5-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Providing a small number of human-labelled example clause+answer pairs in the chat history (in-context examples) substantially improves generative model accuracy and reduces the need for post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-Turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned chat model using in-conversation few-shot demonstrations (in-context learning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Structured multiple-choice question answering (indemnity; information-sharing)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks; experiments add 4 labeled example clauses (E1/E2) to seed the model before asking the target clause question.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot in-context learning: P1 prompt augmented with randomly selected human-labelled examples (E1 and E2). Experiments include 0, E1, E2, and E1+E2 conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot vs zero-shot</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>E1 and E2 each contain 4 example clauses (one example per answer option). Also tested E1+E2 (two examples per option).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Indemnity (GPT-3.5-Turbo): No examples 0.74 accuracy; E1 0.83; E2 0.79; E1+E2 0.87. Information-sharing (GPT-3.5-Turbo): No examples 0.55; E1 0.61; E2 0.62; E1+E2 0.64.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot P1 (no examples): Indemnity 0.74; Information-sharing 0.55.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Indemnity: +0.09 absolute for E1 vs no-examples, +0.13 absolute for E1+E2 vs no-examples. Information-sharing: +0.06 absolute for E1+E2 vs no-examples.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-3.5-Turbo chat API; temperature typically 0; examples randomly selected from labeled dataset; P1 prompt style retained; observed large drop in post-processing needs (e.g., responses needing cleanup dropped from 83 to 22 for indemnity when adding E1+E2).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Making pre-trained language models better few-shot learners <em>(Rating: 2)</em></li>
                <li>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing <em>(Rating: 2)</em></li>
                <li>Prompt programming for large language models: Beyond the few-shot paradigm <em>(Rating: 2)</em></li>
                <li>Rethinking the role of demonstrations: What makes in-context learning work <em>(Rating: 1)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7409",
    "paper_id": "paper-264147046",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "SemanticSimilarity-ZeroShot",
            "name_full": "Zero-shot semantic similarity via sequence embeddings (LegalBERT / OpenAI Ada)",
            "brief_description": "A non-generative baseline that embeds clause text and answer-option phrasings with a sentence/sequence embedding model (LegalBERT or OpenAI Ada) and selects the option with maximum cosine similarity to the clause.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LegalBERT; OpenAI Ada (embeddings)",
            "model_description": "LegalBERT: transformer encoder trained/fine-tuned on legal corpora (Pile-Of-Law). OpenAI Ada: OpenAI's Ada embedding model (training details not disclosed). Both produce sequence embeddings used with cosine similarity.",
            "model_size": null,
            "task_name": "Structured multiple-choice question answering (indemnity; information-sharing)",
            "task_description": "Given a legal clause, select which predefined answer option(s) the clause implies (single‑choice for indemnity, multi‑select for information‑sharing).",
            "problem_format": "Zero-shot embedding-based multiple-choice: embed clause and each option and pick option with highest cosine similarity.",
            "format_category": "question type / prompt style (baseline non-generative)",
            "format_details": "Zero-shot (no examples); answer-option phrasings varied (S1–S4, T1–T2) to test sensitivity; clauses limited to &lt;20k characters to satisfy token limits.",
            "performance_metric": "accuracy",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Used two embedding models (LegalBERT trained on Pile-Of-Law; OpenAI Ada). Evaluated on two tasks: 121 environmental indemnity clauses and 143 permitted-use-of-confidential-information clauses. Cosine similarity argmax decision rule.",
            "statistical_significance": null,
            "uuid": "e7409.0"
        },
        {
            "name_short": "Generative-P1-ZeroShot",
            "name_full": "Generative prompting using P1 (zero-shot) with GPT-3.5-Turbo / GPT-4 / PaLM2",
            "brief_description": "A generation-based structured-answer approach: a carefully worded zero-shot prompt (P1) instructs the model to select exactly one numbered option from a provided list and to respond only with that option (or the exact phrase 'The clause is silent').",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo; GPT-4; PaLM2",
            "model_description": "Instruction-tuned large autoregressive transformer chat models (GPT-3.5-Turbo and GPT-4 by OpenAI) and PaLM2 by Google; used in generation mode to output structured answers per prompt instructions.",
            "model_size": null,
            "task_name": "Structured multiple-choice question answering (indemnity; information-sharing)",
            "task_description": "Same as for semantic baseline: map clause text to one of predefined answer options; P1 restricts output to a single option or a special escape phrase.",
            "problem_format": "Natural-language instruction prompt (P1) with embedded clause and explicit answer options (single‑choice); zero‑shot generation.",
            "format_category": "prompt style / instruction framing",
            "format_details": "P1: 'Referring only to the information contained in the clause below, only select which one of the below numbered options is implied by the clause, without providing any other information or justification. If you cannot determine which of the conditions are implied, respond with the exact text: \"The clause is silent\".' Options inserted (S1–S4 or T1–T2). Temperature generally set to 0 in practice.",
            "performance_metric": "accuracy",
            "performance_value": "Indemnity (GPT-3.5-Turbo, P1, zero-shot): 0.74 accuracy; Information-sharing (GPT-3.5-Turbo, P1, zero-shot): 0.55 accuracy",
            "baseline_performance": "Semantic similarity baseline (LegalBERT / OpenAI Ada); generative P1 reported to substantially improve over semantic similarity (exact numeric baselines not reported in text for all comparisons).",
            "performance_change": "Qualitative: generative prompting (P1) non-trivially and substantially outperforms semantic similarity baseline; exact absolute change not uniformly reported. GPT-4 and PaLM2 perform better than GPT-3.5-Turbo (no precise numbers given in main text).",
            "experimental_setting": "Used GPT-3.5-Turbo (primary), GPT-4 and PaLM2 (preliminary). Temperature generally 0. Clauses &lt;20k characters. Options sets S1–S4 (indemnity), T1–T2 (information-sharing). Parsing/post-processing applied to handle varied output formats.",
            "statistical_significance": null,
            "uuid": "e7409.1"
        },
        {
            "name_short": "PromptVariants-P1P2P3P4",
            "name_full": "Prompt template variants (P1, P2, P3, P4) and their effects",
            "brief_description": "Comparing small and large changes to the prompt template shows that small wording tweaks (P2) often have little effect, while more explicit rules and multi-select capable prompts (P3/P4) can substantially improve correctness and reduce unwanted generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo (primary)",
            "model_description": "Instruction-following chat model evaluated under different prompt templates (P1–P4).",
            "model_size": null,
            "task_name": "Structured multiple-choice question answering (indemnity; information-sharing)",
            "task_description": "As above; tests how changes in prompt phrasing, rules (e.g., explicit 'respond only with number', escape hatches), and whether multiple selections are allowed affect model output.",
            "problem_format": "Prompt style variations: single-choice constrained generation (P1/P2), multi-select with explicit rules and escape hatches (P3), role-play + explicit question + multi-select (P4).",
            "format_category": "prompt style / instruction framing",
            "format_details": "P2: small rewording of P1 (little effect). P3: explicit rules including 'If it cannot be determined... respond with \"Unable to determine.\"' and allow multiple selections — produced substantive improvements in correct answers but required more post-processing. P4 (role-play): asking model to 'pretend you are a party' and including the explicit question + multi-select improved information-sharing accuracy markedly.",
            "performance_metric": "number correct / accuracy (reported qualitatively)",
            "performance_value": null,
            "baseline_performance": "P1 (baseline prompt)",
            "performance_change": "P2: no substantive change vs P1; P3: substantive improvements in correctness vs P1 (qualitative); P4: substantial accuracy improvement for information-sharing task vs simpler prompts (qualitative).",
            "experimental_setting": "GPT-3.5-Turbo; temperature 0; tested on indemnity and information-sharing datasets; shuffling option order had no effect; changes in prompt wording, adding rules, allowing multi-select and role-play tested.",
            "statistical_significance": null,
            "uuid": "e7409.2"
        },
        {
            "name_short": "OptionsPhrasing-S1S4-T1T2",
            "name_full": "Answer-option phrasing variants (S1–S4 for indemnity, T1–T2 for information-sharing)",
            "brief_description": "Different phrasings of answer options (complete sentences vs fragments, using synonyms, explicitly equating legal terms) materially affect both embedding similarity baselines and generative model accuracy; clearer, complete-sentence options tend to produce higher accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Applied to embedding baselines (LegalBERT, OpenAI Ada) and generative models (GPT-3.5-Turbo, GPT-4, PaLM2)",
            "model_description": "N/A (this entity concerns prompt/option formatting rather than a single model architecture).",
            "model_size": null,
            "task_name": "Structured multiple-choice question answering (indemnity; information-sharing)",
            "task_description": "Selecting correct option(s) given a clause; investigates how wording of provided answer options affects selection.",
            "problem_format": "Multiple-choice option phrasing variations: S1–S4 (different ways to express indemnity answers, including explicit synonyms Lessee/Tenant etc.); T1 vs T2 (information-sharing options, T2 uses clear complete sentences).",
            "format_category": "prompt style / answer-option design",
            "format_details": "Findings: options that are 'as complete a thought or sentence as possible' reduce opportunities for the model to generate extra text; wrapping options in punctuation (periods/quotes) sometimes mitigates extra generation; explicitly including term equivalences (e.g., Lessee=Tenant) reduced output variability; T2 (complete sentences) yielded substantive improvement over T1 for information‑sharing.",
            "performance_metric": "accuracy / consistency (qualitative)",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": "T2 &gt; T1: substantive improvement in accuracy for information-sharing (qualitative); different S1–S4 forms produce different accuracies across methods (no single form uniformly optimal).",
            "experimental_setting": "Across both embedding and generative approaches; tested S1–S4 and T1–T2 option sets on the two tasks; often used with prompts P1–P4.",
            "statistical_significance": null,
            "uuid": "e7409.3"
        },
        {
            "name_short": "InContextFewShot-E1E2",
            "name_full": "In-context learning / few-shot seeding with example sets E1, E2 (GPT-3.5-Turbo)",
            "brief_description": "Providing a small number of human-labelled example clause+answer pairs in the chat history (in-context examples) substantially improves generative model accuracy and reduces the need for post-processing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-Turbo",
            "model_description": "Instruction-tuned chat model using in-conversation few-shot demonstrations (in-context learning).",
            "model_size": null,
            "task_name": "Structured multiple-choice question answering (indemnity; information-sharing)",
            "task_description": "Same tasks; experiments add 4 labeled example clauses (E1/E2) to seed the model before asking the target clause question.",
            "problem_format": "Few-shot in-context learning: P1 prompt augmented with randomly selected human-labelled examples (E1 and E2). Experiments include 0, E1, E2, and E1+E2 conditions.",
            "format_category": "prompt style / few-shot vs zero-shot",
            "format_details": "E1 and E2 each contain 4 example clauses (one example per answer option). Also tested E1+E2 (two examples per option).",
            "performance_metric": "accuracy",
            "performance_value": "Indemnity (GPT-3.5-Turbo): No examples 0.74 accuracy; E1 0.83; E2 0.79; E1+E2 0.87. Information-sharing (GPT-3.5-Turbo): No examples 0.55; E1 0.61; E2 0.62; E1+E2 0.64.",
            "baseline_performance": "Zero-shot P1 (no examples): Indemnity 0.74; Information-sharing 0.55.",
            "performance_change": "Indemnity: +0.09 absolute for E1 vs no-examples, +0.13 absolute for E1+E2 vs no-examples. Information-sharing: +0.06 absolute for E1+E2 vs no-examples.",
            "experimental_setting": "GPT-3.5-Turbo chat API; temperature typically 0; examples randomly selected from labeled dataset; P1 prompt style retained; observed large drop in post-processing needs (e.g., responses needing cleanup dropped from 83 to 22 for indemnity when adding E1+E2).",
            "statistical_significance": null,
            "uuid": "e7409.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Making pre-trained language models better few-shot learners",
            "rating": 2,
            "sanitized_title": "making_pretrained_language_models_better_fewshot_learners"
        },
        {
            "paper_title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing",
            "rating": 2,
            "sanitized_title": "pretrain_prompt_and_predict_a_systematic_survey_of_prompting_methods_in_natural_language_processing"
        },
        {
            "paper_title": "Prompt programming for large language models: Beyond the few-shot paradigm",
            "rating": 2,
            "sanitized_title": "prompt_programming_for_large_language_models_beyond_the_fewshot_paradigm"
        },
        {
            "paper_title": "Rethinking the role of demonstrations: What makes in-context learning work",
            "rating": 1,
            "sanitized_title": "rethinking_the_role_of_demonstrations_what_makes_incontext_learning_work"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        }
    ],
    "cost": 0.013648,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Search for Prompts: Generating Structured Answers from Contracts
16 Oct 2023</p>
<p>Adam Roegiest 
Zuva Inc
TorontoOntarioCanada</p>
<p>Radha Chitta radha.chitta@zuva.ai 
Zuva Inc
TorontoOntarioCanada</p>
<p>Jonathan Donnelly jonny.donnelly@zuva.ai 
Zuva Inc
TorontoOntarioCanada</p>
<p>Maya Lash 
Zuva Inc
TorontoOntarioCanada</p>
<p>Alexandra Vtyurina 
Zuva Inc
TorontoOntarioCanada</p>
<p>François Longtin francois@zuva.ai 
Zuva Inc
TorontoOntarioCanada</p>
<p>A Search for Prompts: Generating Structured Answers from Contracts
16 Oct 2023F29FF277D45701D2265EE9943F2F3DC4arXiv:2310.10141v1[cs.CV]Structured answersLegal Question AnsweringGenerative AIPrompt Engineering
In many legal processes being able to action on the concrete implication of a legal question can be valuable to automating human review or signalling certain conditions (e.g., alerts around automatic renewal).To support such tasks, we present a form of legal question answering that seeks to return one (or more) fixed answers for a question about a contract clause.After showing that unstructured generative question answering can have questionable outcomes for such a task, we discuss our exploration methodology for legal question answering prompts using OpenAI's GPT-3.5-Turbo and provide a summary of insights.Using insights gleaned from our qualitative experiences, we compare our proposed template prompts against a common semantic matching approach and find that our prompt templates are far more accurate despite being less reliable in the exact response return.With some additional tweaks to prompts and the use of in-context learning, we are able to further improve the performance of our proposed strategy while maximizing the reliability of responses as best we can.</p>
<p>Introduction</p>
<p>Understanding obligations and restrictions in contracts is as useful to lawyers as it is to lay-people 1 and goes beyond just finding relevant information in a contract to understanding the underlying implications (e.g., whether a tenant can sublet an apartment).On the basis of a single contract, this is not too tedious to perform manually when relevant clauses are extracted for the reader but quickly becomes onerous and time-consuming at scale or when legal expertise is not a strong suit.This understanding of not just the clauses but the implications present in contracts can allow individuals and organizations to make smarter decisions and have more confidence when doing so.This is especially paramount when certain implications can have potentially very negative effects (e.g., unlimited liability guarantees, lowest price guarantees) depending on which side of the implication one is on.</p>
<p>With the increasing popularity of Generative AI and applications to "chat" with one's documents (e.g., ChatPDF 2 , docGPT 3 , PDFGPT.IO 4 , CaseText 5 ), there is the potential to more easily facilitate this understanding of legal obligations and restrictions at scale.Inspired by this, we sought to investigate how we might leverage Generative AI, specifically OpenAI's GPT-3.5-Turbo, to help us generate structured answers to questions about clauses in legal documents.Taking inspiration from closed question-answering [10] and textual entailment [33], our task is such that we are provided a set of contextual clauses about which a user would like to ask questions with predefined outputs (e.g., multiple choice) rather than a free-form answer.Such outputs can facilitate the creation of business workflows that can easily take action on the results of the questions without resorting to complex understanding of a generated answer or manual human review.While such outputs are perhaps less compelling than having a "conversation" with a document, we believe that this approach scales better to a document collection and workflow automation.Moreover, we show that more natural prompts can very easily exhibit consistency and reliability issues when examining the responses generated in a "chat" interaction paradigm (Section 3).</p>
<p>While prompt engineering is a new area of research [16,22,36,30] and book publication 6 , it is a task that neither we nor the users of our product actually desire to do.Our end goal then is to find and use prompts that provide consistent answers for the same clause and reliable answers across clauses.In particular, we are interested in finding reusable templates that allow end users to "fill in answer options" and not worry about overall prompt engineering.This allows end users to customize and modify options to their needs but it also results in system builders not needing to foresee all possible information needs and build bespoke prompts for each of them.To find such templates, four team members tested over 700 prompt and clause combinations to discover critical aspects of prompts and prompt templates that we detail in Section 4. Using the insights gleaned, we tested our prompt templates in a more rigorous evaluation in Section 5, where we compare our approach to more traditional semantic matching techniques and find that our prompts can outperform such techniques.We conclude with a discussion of limitations and future work.</p>
<p>Background</p>
<p>Generative models such as LaMDA7 , LLaMA 8 , and GPT-n series9 are selfsupervised models which learn to predict the next token in a sequence of tokens [21,26].These large language models have billions to trillions of parameters and are pre-trained on massive corpora of texts.Unlike the early pre-trained language models, they require little to no fine-tuning and perform well in zero-shot and few-shot settings.</p>
<p>In a zero-shot generation task, the model is provided with the inputs and an instruction in natural language, and the response of the model can be parsed to obtain the answer.For open-domain QA the only input is the question but for closed-domain the input also includes the context.In a few-shot setting, the model is also provided with a few examples, which have the indirect effect of "fine-tuning" the model [4].Recent large language models (e.g.GPT-3.5-Turbo,Vicuna-13B, Alpaca) have been fine-tuned, either using supervised or reinforcement learning, with prompts for a diverse set of natural language tasks [24,20,29,25,6].They have generally outperformed earlier language models on most natural language generation benchmarks including question answering, summarization, and translation [35].Instruction-tuned models achieve better performance than fine-tuned models but can suffer from two major issues [15]:</p>
<p>-Inconsistency: Results generated have been found to be overly reliant on the phrasing of the prompt (e.g., small prompt changes producing inexplicable differences in generated responses, multiple runs yielding different results) -Hallucination: Models were found to generate false/irrelevant information in some instances, which introduces risk when accuracy is important.</p>
<p>Prompt engineering techniques [12,16,30] aim to design prompts that mitigate these issues.For instance, in [12] a smaller language model, such as T5, is tuned to "auto-complete" the prompts for the large language model.Prompt templates are created in [16] and [30] which can be auto-filled.While these methods have had some success, this research is still ongoing.In our work, we use an OpenAI provided large language model, GPT-3.5-Turbo, and determine if it is possible to create prompt templates that can be used to consistently answer questions based on contract clauses.</p>
<p>Open-Ended Response Generation</p>
<p>In many legal processes, lawyers look to take an action on a particular answer option rather than a summary or the raw text of a clause itself.For example, while dealing with environmental indemnification, a lawyer (representing a landlord) may wish to know in what contracts the tenants indemnify the landlord rather than any explicit conditions around the indemnification (which may be relevant later).By identifying which contracts do not have such a clause, the lawyer may have particular advice for the landlord.When only indemnification clauses are presented, however, the lawyer would be required to read them to answer these questions (e.g., direction of indemnification).An ideal system would produce a concise and consistent answer when asked "who indemnifies whom?" with respect to the clause.-The Lessor indemnifies the Lessee Indemnified Parties, and the Lessee indemnifies the Lessor Indemnified Parties.-Lessor indemnifies Lessee Indemnified Parties for any and all Environmental Liabilities, except those caused by the grossly negligent or wilful misconduct of Lessee, Manager, or subtenants of Lessee or Manager, and their respective employees, agents or independent contractors.Lessee indemnifies Lessor Indemnified Parties for Environmental Liabilities caused by the grossly negligent or wilful misconduct of Lessee, Manager, or subtenants of Lessee or Manager, and their respective employees, agents or independent contractors.-Lessor indemnifies Lessee Indemnified Parties.Lessee indemnifies Lessor Indemnified Parties.</p>
<p>While the first and third are close, they aren't identical and the second is more of an overall summary.We note that these are all correct responses but also not something one could plugin in to a workflow automation tool without a lot of additional post-processing (e.g., in the flavour of nugget matching [9]).To further illustrate this problem, we present the responses from the same prompt with a different mutual environmental indemnity clause:</p>
<p>-Tenant indemnifies Landlord for claims arising from the introduction of hazardous material on or after the Effective Date, and Landlord indemnifies Tenant for claims arising from the existence of hazardous material or violation of Environmental Requirements prior to the Effective Date.-In the first paragraph Tenant indemnifies and defends Landlord, and in the second paragraph Landlord indemnifies and defends Tenant.-Tenant indemnifies Landlord.Landlord indemnifies Tenant.</p>
<p>Again, we see responses that are factually correct but are also quite different from the responses for the previous clause.As should be apparent by now, this inconsistency and unreliability as well as response verbosity is not ideal for simplifying work process and may just exacerbate things in the long run.</p>
<p>Finding Reusable Prompt Templates</p>
<p>Before the launch of OpenAI's ChatGPT in December 2022, we had spent time investigating the utility of MNLI [31] pre-trained encoder models 10 for structured question answering (described in Section 5.1) but found that the technique was only suitable when clauses and options could fit in relatively small context windows of such models [28,33].Using similar clauses to the previous section but modelling the question closer to one of entailment, we were able to achieve a more desirable result with ChatGPT on a small set of examples.From this success, we built a tool11 that would allow internal experts to try various prompt and option combinations using previously annotated legal clauses as the context for the prompt.The remainder of this section discusses the overall methodology used and the key findings from that process on how prompt templates might be structured.</p>
<p>Prompt Exploration Methodology</p>
<p>Over a 3.5 week period, several members of our team tried over 700 prompt and clause combinations to explore how sensitive prompts were to variations in clauses. 12Prompts were based on an existing set of questions that we curated ourselves but also includes some end-user provided examples.All of the prompt contexts were previously annotated for supervised machine learning tasks for the purposes of content extraction, previously described in [23,11] and derived from legal documents primarily from EDGAR (https://www.sec.gov/edgar) and SEDAR (https://www.sedar.com/).While using the tool, a team member would select a clause type (e.g., assignment, environmental indemnity) and craft a prompt to answer a related question.They would then run this prompt on as many examples of this clause type as they desired, rating the responses, and making tweaks as they went along to improve the consistency and reliability of responses.We note that some tweaks were made to the tool's interface during this period with the goal of simplifying the testing and evaluation experience with no material effect made to the outcomes discussed.</p>
<p>Team members were free to progress at their own rate and develop their own strategies to avoid locking anyone into a particular style.Though active discussions took place around different prompting styles and how certain edge cases could be avoided or addressed more generally.Invariably this approach has meant that some redundant work has likely occurred which is not ideal but was unlikely to be avoided.This also means that different understandings of the various prompting strategies have also developed which we believe is worth the additional effort.</p>
<p>Prompt Template Key Aspects</p>
<p>Using the logs from our tool, one of the authors conducted a very light "thematic analysis" inspired review of the logs to examine trends in both prompts and in the notes themselves.While we have not "cracked the code" of one-size-fits-all prompt templates and suspect that doing so is a Sisyphean task, we have learned some interesting things on how to steer a generative AI model in returning answers in the structured format that we find desirable.In the following, we provide the insights and key aspects we have gleaned from the analysis of logs and group discussions that occurred while undertaking this search for prompt templates.</p>
<p>Justification As their name implies, generative models are designed to generate text and when answering questions one such source of generation is in justifying a choice.Conceptually this is useful but as a component in workflow/process automation, it is not.Unlike other approaches [34], we very consistently added a constraint instructing the model to respond "without providing justification" or "without providing other information" to nudge the model away from generating text that may or may not be reliable and further complicate downstream processing.</p>
<p>Option Selection An additional aspect of our prompts was in specifying responses should only be from a predefined list of options which we found helps to guide the model to actually select one of the options.Unfortunately, this had a confounding factor in that depending on how this was specified (e.g., "only select the numbered option that is implied by the clause"), the generative model may respond with just the number, "Option i ", or the formatted option (e.g., (3), 3.), or the entire option, or just the answer text of the option (e.g., "Landlord indemnifies Tenant").None of these responses are bad but does mean that some downstream processing is necessary to clean-up responses to a canonical format but could be resolved by setting max_tokens when generating a response.</p>
<p>Options over Prompts</p>
<p>The overarching structure of a prompt can certainly have an effect on how succinct and accurate a response can be and is why we have devoted so much time to finding good templates.But throughout the process we noticed that how answers were phrased had more impact on the end response generated by the model.In particular, we found that options should be as complete a thought or sentence as possible to avoid leaving openings for the model to begin generating additional text (e.g., often elaborating on an option to finish the sentence).We also saw examples where ending options in full-stops or wrapping them in quotations could mitigate generation even with sentence fragments but this was not consistent.</p>
<p>Escape Hatches One of the most interesting aspects of our early trials and even subsequent more thorough investigations with our prompt tool is the ability of the model to make assumptions when lacking sufficient data or context in a clause.As an example, in the case of environmental indemnities where the parties might actually be named (e.g., SmithCo, Joe Smith) rather than a defined term (e.g., landlord, lessee).In such a case, the model often just made assumptions about which party fulfilled a specific role without any prompting or evidence.Interestingly, the model managed to get it right a fair number of times but we hesitate to attribute this to any particular reason.Such aspects can be remediated by ensuring those details are also provided as context but the willingness of the model to make assumptions is not ideal.</p>
<p>To combat this and prevent the model from making assumptions or trying to free-form respond with an "unable to determine"-style response, we tried to provide additional "rules" or "escape hatches" in our prompts to allow the model to default to a safe choice.The most generic rule that we used was: If you cannot determine which of the conditions are implied, respond with the exact text: 'The clause is silent.'This statement required a bit of fine-tuning since an initial attempt to use "answer with ..." often resulted in the model then generating a response along the desired lines but the exact phrase.We posit that this is due to the nature of how the model was trained and less due to explicit phrasing (i.e., prompts asking the model to answer would often be generative in nature in the training set).We have also tried similar but less generic escape hatches where needed (e.g., the aforementioned indemnity situation) and the model will make use of them with sufficient wordsmithing.</p>
<p>Multiple Selections Having a model produce multiple selections is critical when it is possible for multiple conditions to be true and specifying all combinations is not reasonably feasible (i.e., more than a handful of combinations).While we did not spend quite as much time on these types of prompts as others, we did determine that it is possible to do this and that there might be several possible ways to perform the task.</p>
<p>The most obvious is just to direct the model to choose multiple options (e.g., "If multiple options are implied by the clause, select all of the numbered options that apply.")but this has a surprising potential implication.The model can be "too smart" and perhaps rely too much on a non-legal understanding of English and conflate multiple terms of art in the legal world.For example, prior to refinement of options, the model would often conflate "requiring to give notice" and "requiring consent" and would tend to default towards the latter when determining obligations around the assignment of a contract to another party.</p>
<p>An alternative approach that we found had potential is to frame each option as a sub-prompt that required the model to answer "yes" or "no" (as to whether the clause supported the option).In our basic exploration, this often felt easier to get working more quickly but we have yet to expend a lot of effort exploring further.The downside is that the aforementioned problems around assumptions and language conflation were more pronounced and a bit harder to tackle.Moreover, it is not clear whether or not just iteratively asking the options as individual prompts (in the style of [34]) either independently or as part of a larger "conversation" would yield superior results.</p>
<p>Structured Answer Generation</p>
<p>Before we address our evaluation of structured generation, we first discuss some context setting for choosing this avenue of research.It would natural to believe that training a discriminative multi-class classifier to obtain structured answers to the questions presented in this work is a reasonable first choice.Such solutions (e.g., SVMs, logistic regression) are well understood and highly optimized but come with a flaw, they require sufficient examples of each class to be effective (i.e., low prevalence classes can be hard to overcome).For example, landlord indemnification is a relatively rare occurrence in our data and one for which data augmentation is not guaranteed to work.Indeed, we have attempted to address this in prior work [7], it did not consistently work well for especially low prevalence or nuanced differences between options.It has been our experience that such cases arise naturally in the legal domain where rare outcomes tend to be the most problematic and equally hard to identify.</p>
<p>Inspired by recent uses of large language models and textual entailment to facilitate "zero-shot" question-answering, we put such methods to the test.We begin with an examination of semantic matching as a baseline before comparing with generative model prompting techniques and the differences in option choices.We then explore the consistency and reliability of our empirical results before providing some initial experiments detailing how in-context learning [17,32] can improve even bad prompts.6) 2 (71) 3 (39) 4 (5) A 1 (6) 2 (71) 3 (39) 4 (5) A 1 (6) 2 (71) 3 (39) 4 (5) A 1 (6) 2 (71) 3 (39) 4 ( 5 phrasings of the answer options and the prompts using the zero-shot semantic similarity approach (rows 1-2) and generation (rows 3-5) for the indemnity question.The true distribution of the answers is listed along with the answer options in brackets.</p>
<p>Semantic Similarity and Answer Prediction
S1 S2 S3 S4 1 (
Given that our work was inspired by natural language inference (Section 3), we investigate the utility of a baseline method that is not generative but is phrasings of the answer options and the prompts using the zero-shot semantic similarity approach (rows 1-2) and generation (rows 3-5) for the information-sharing question.The true distribution of the answers is listed along with the answer option in brackets.</p>
<p>less restricted by the minimal context length in many popular encoder models (e.g., BERT and its variants).We focus on zero-shot question answering using semantic similarity, which aims to identify which of the options from a given set of options is best supported by the given text [28,33].</p>
<p>Given the text of the clause T , and a set of answer options H, we use a sequence embedding model M to obtain the embeddings for the clause and the answer options and then predict the answer which has the highest similarity with the clause as follows: arg max h∈H cos (M (T ) , M (h)).</p>
<p>We use two legal questions "In the clause below, who indemnifies whom?" ("indemnity question") and "For what purpose are the parties sharing information according to the clause below?" ("information-sharing question") to test the effectiveness of this method.The test datasets for the two questions consisted of a set of legal clauses, which we assume are previously extracted from the contracts through manual means detailed in prior work [23].For the indemnity question, we used a set of 121 "environmental indemnity" clauses, which describe the provisions for security or protection from losses or damage caused by environmental contamination or disasters.For the information-sharing question, our test dataset comprised of 143 "Permitted Use of Confidential Information" clauses, which describe the purposes for which information shared between the parties may be used.From our observations in Section 3 that the phrasing of the question and the answer options plays an important role in the answers generated by the models, we tested out four sets of answer options (S1-S4) for the indemnity question and two sets of answer options (T1-T2) for the informationsharing question, shown in Figures 2 and 3 respectively.</p>
<p>The test clauses for the indemnity question have the distribution of the answers: 6, 71, 39, and 5 for each of the four options.Unlike the indemnity question where only one of the options is applicable, the information-sharing question is a multi-select question (i.e., multiple options may be applicable).The test clauses for this question have the answers distributed as 4, 81, 40, 2, and 57, respectively.There are also 13 clauses that do not contain sufficient information to determine the correct answer.For the information-sharing question, we perform a lenient evaluation of the results and consider the prediction correct if at least one of the correct options was predicted.Such lenient evaluation is to make overall comparisons between methods easier without having to also attempt to learn (or guess) a threshold in the case of semantic similarity.</p>
<p>We ensured all clauses were less than 20, 000 characters in length in order to adhere to the token limits of the OpenAPI models.As we can see, both datasets exhibit the prevalence problem we described at the beginning of this  section, which means that traditional classifiers are unlikely to handle this scenario well [14,5,7].</p>
<p>We used two large language models to compare the performance of this method: (1) the LegalBERT model trained on the Pile-Of-Law dataset [13] 13 and ( 2) the OpenAI Ada model [18].</p>
<p>For a given model, we used cosine similarity between options and clause embeddings to choose the most similar option to a given clause.We then computed the accuracy of option to ground truth for each embedding model and present the results of this evaluation in Tables 1 and 2. We observe that the LegalBERT embeddings, which were trained on contracts, performed moderately better than the OpenAPI Ada embeddings, for which the exact training details are not known.For instance, the terms Lessee and Tenant have a 90% similarity using LegalBERT embeddings, whereas the similarity drops to 87% when using the OpenAI Ada embeddings.Though this may not seem like a substantive difference, it is apparent that explicitly including the terms Lessee and Tenant in the answer options S4 makes a moderate difference in the performance using the Ada embeddings but less so for LegalBERT.Unsurprisingly, phrasing the answer options differently leads to different accuracy regardless of model choice.For example, to identify mutual indemnification, the option "There is mutual indemnification" in S1 and S3 leads to a better result than any other form of the answer option.</p>
<p>Generating Structured Answers</p>
<p>From our manual exploration for reusable prompts, we had sufficient evidence to indicate that we should more thoroughly and empirically investigate this approach for structured answer generation.Accordingly, we evaluate the effectiveness of our approach on the two questions and examine how our approach holds up against the semantic similarity approach of the previous section.To test the effectiveness of structured answer generation, we employed OpenAI's GPT-3.5-Turbo[20] and GPT-4 [19], and Google's PaLM2 [2] Generative AI.</p>
<p>We started with the following prompt: P1: Referring only to the information contained in the clause below, only select which one of the below numbered options is implied by the clause, without providing any other information or justification.If you cannot determine which of the conditions are implied, respond with the exact text: "The clause is silent".{{Options}} {{Clause}} {{Options}} is replaced by the options in each set from the previous section and {{Clause}} is replaced by the text of the clause.A prompt styled in this manner aims to ensure that only one of the answer options is generated, and that the option generated is not followed by any additional text, thereby rendering a structured response.The prompt provides an escape hatch for the model (returning "The clause is silent.")if all the relevant answer options were not provided or the clause does not contain sufficient information.The response from the API is parsed to obtain the structured answer to the question.</p>
<p>From Tables 1 and 2, we can easily see that the performance of this approach non-trivially and substantially improves over the semantic matching approach.Consistent with the similarity results, different forms of answer options perform differently even for generative models but with still generally okay performance in the worst case.Interestingly, there does not appear to be any strong correlation in how well answer options will perform between either approach (e.g., S3 is the second best performing for GPT-3.5-Turbobut the worst for semantic similarity).We also note that the rare clauses where either the "Landlord indemnifies Tenant" or there is "No indemnification" are correctly answered by the generative models.Options T2 yield substantive improvement over options T1 showing that clear, complete sentences as options lead to better accuracy.Among the generative models, the more complex GPT-4 and PaLM2 obviously perform better than GPT-3.5-Turbo.On the indemnity question, both these models are able to correctly identify all instances of mutual indemnification.However, they don't do as well on the information-sharing question, most likely because the prompts and the options are not well-structured.A prompt styled for one model, despite performing relatively well, is not guaranteed to yield optimal results with other generative models.While not surprising, it does suggest a developer ought to choose a model and stick with it.</p>
<p>Despite styling the prompt to ensure that only one of the answer options was generated, we found that there was some lack of consistency in the responses (particularly those of GPT-3.5-Turbo)across clauses.For instance, when provided with options S1 for the indemnity question, the response, "The clause implies that Tenant indemnifies Landlord.",was generated for some clauses instead of "Tenant indemnifies Landlord.".The model also generated "Lessee indemnifies Lessor " for some clauses instead of "Tenant indemnifies Landlord.",which we posit in due to the nature of the language used in some clauses (i.e., the model tries to match the language of the clause).In two to three instances, the model also provided a summary of the clause in addition to the answer option.For example, one of the responses generated was "Lessee indemnifies Lessor and its Affiliates for any and all Environmental Costs incurred in connection with the presence or alleged presence of Hazardous Substances or Mold on or near the Leased Property."</p>
<p>This necessitated some post-processing (e.g., substring matching) of the response to determine which answer option applied where possible.Such an outcome is not what we desired to have happen but is an aspect we are willing to bend on, especially when such post-processing is not complex (i.e., substring or synonym substitution is preferable to nugget-matching).</p>
<p>There were 16, 15, 12, and 9 unique responses generated by GPT-3.5-Turbo for each of the four answer options for the indemnity question.It is our best guess that S4 generates the least number of unique responses because the answer options explicitly note that the terms Lessee, Tenant, and Seller and the terms Lessor, Landlord, and Buyer represent the same entities.Surprisingly, the answers generated for the information-sharing question were more consistent.There were only two instances where a summary of the clause was generated.</p>
<p>While this approach yielded unintended response creativity by the generative model, we think this is a strong indicator that such an approach may be viable for more use cases.To further attempt to refine our approach and prompt phrasings, we detail in the next section how "prompt sensitive" a generative model can be and how we could limit some of the post-processing.For brevity and more focused analysis, we present results only for GPT-3.5-Turbohenceforth.</p>
<p>Sensitivity to Prompts</p>
<p>As we saw in the previous section, one glaring issue with using a generative model for structured answer generation is that it can still get creative in its responses despite our attempts to limit this creativity.Our first idea was that perhaps the order of options would play a part but shuffling the order of options did not change any outcomes and so we do not report on them further.Instead, we report on how changing aspects of the prompt template affect the outcome of the results by introducing new rules or adding more explicit rules for the model to follow.</p>
<p>To address the issues around the model getting too creative in its responses when using P1, we sought to limit the results by requiring the model to respond only with the numbered option rather than the entire option: We also created a prompt, which would allow the model to make multiple selections and making the rules and options more explicit: P3: Using the text provided, follow the subsequent instructions: {{Clause}} Respond with all options which are implied by the provided text, without providing any other information or justification and by following the rules.</p>
<p>Rules:</p>
<p>-If it cannot be determined which of the conditions are implied or if it is required to make assumptions, respond with the exact text: "Unable to determine."-If the terms in the options are not used in the text, respond with the exact text: "Unable to determine."Options: {{Options}} Table 3 shows the difference in the number of correct answers generated for the indemnity question using our three prompt templates and the answer options S1.Perhaps unsurprisingly, the small modifications made to P2 do not substantively change the model's ability to produce the correct answer nor did it particularly limit the creativity of responses generated by the model.The much larger changes in P3, on the other hand, brought about substantive improvements to the model's responses but also meant that we had slightly more post-processing to do to accommodate multiple responses being returned.While the latter was not unexpected, it does illustrate a further complication that could spur more model creativity in other situations, especially those where the answer options may be sentence fragments.</p>
<p>For the information-sharing question, we found that asking the model to pretend to be a party to the agreement, explicitly including the question in the prompt, and allowing for multiple option selection improved the accuracy substantially, as seen in Table 4.This type of prompt styling does not lend as easily to the creation of templates, as it requires the user to provide a question along with the options.However, the improvement in accuracy is worth the additional effort.Throughout our investigation, we have regularly found that using a consistent template and aiming for option correctness has tended to produce more viable results in the longer term but does have some periods of inconsistency and unreliability in the results until the right options are found.Indeed, trying to min-max the prompt template may not always yield the most optimal outcomes as P2 is ever so slightly worse than P1 in our tests.While P3 and P4 did substantially improve the overall effectiveness, this improvement was based on a decent amount of trial and error in our prompt tool and is not an approach we want to undertake for every possible legal question, especially if it risks the model becoming creative in its responses.Nor is this an approach that we want an end-user to have to replicate as it would be far too onerous for them.</p>
<p>All that being said, in our tests throughout this section and the previous one, we have found that once a prompt template and a set of options have hit a steady state in consistency then they do, in fact, stay consistent (i.e., regenerating the response for the same combination of clause, prompt, and options produces the same result).Using P1, we used the GPT-3.5-TurboAPI to rerun the generation of responses 5 times and saw that the results were extremely consistent with no more than a couple of responses changing.Unfortunately, the reliability of responses is not ideal as we discussed earlier but does show that the right combination of prompt and options can non-trivially reduce the amount of variation (e.g., P4 having slightly more than half the variations than P1).</p>
<p>In-Context Learning</p>
<p>If we set aside issues with the model generating the exact response we want and focus solely on the effectiveness of the technique, we see that our structured answer generation approach is definitely better than using semantic similarity (Section 5.2) but that does not mean the technique is ready for end-user consumption (i.e., business processes likely want better than 0.74 accuracy).A simple way to potentially help the model "understand" the task that we are instructing it to perform is to use some form of few-shot learning, specifically in-context learning [17,32], which has been shown to improve model effectiveness [4,12].</p>
<p>In our case, we leverage the fact that GPT-3.5-Turbo is a chat-style model and can "seed" it with labelled examples as part of the conversation (i.e., we pre-populate earlier parts of the conversation with both prompt and answer).To do so, we augment a simple prompt, P1, and answer set, S1, with randomly selected human labelled examples for each of the four options.We create two example sets for the indemnity question, E1 and E2, each containing 4 randomly selected example clauses, one for each answer option.As seen in Table 5, there is reasonable improvement in the accuracy with both example sets.But when augmented with both example sets (two examples for each answer option), there is a stark improvement in accuracy.We find that the model is able to identify tricky cases like mutual indemnification when previously it had struggled with just P1 and S1.Moreover, this technique can dramatically reduce the amount of post-processing necessary to map responses back to options.For example, for the indemnity question, the number of responses needing cleanup drop by at least 50 percent; 83 responses with S1 to 22 responses with S1 and the two example sets.</p>
<p>No examples E1 E2 E1 + E2 Indemnity 0.74 0.83 0.79 0.87 Information-sharing 0.55 0.61 0.62 0.64 Table 5: Accuracy when GPT-3.5-Turbo is provided in-context with different sets of examples, prompt P1, and answer sets S1 and T1 for the indemnity and information-sharing questions.</p>
<p>Utilizing in-context learning has allowed us to improve response generation substantively while incurring the small expense of annotating some additional clauses for the appropriate answer.This indicates that there may be a valuable trade-off to identify between prompt (or option) engineering and just spending the time providing a small handful of examples.Exploring methods to determine when prompt engineering has diminishing results and it would be better to provide examples is an interesting avenue of future work that we plan to explore.</p>
<p>Limitations</p>
<p>For the sake of experimental simplicity and general ease of use, we have focused most of our testing on the GPT-3.5-Turbomodel.Preliminary results on GPT-4 and PaLM2 in Tables 1 and 2 show that the prompts that work well with GPT-3.5-Turbo can be strong starting points but may not be optimal out of the gate.Based on this, we feel comfortable saying that our proposed solution is viable on other generative AI models with the reasonable caveat that no one prompt is a golden ticket for success.Moreover, as there are increasing numbers of models being made available for commercial use [8,3,1,27], we focused on GPT-3.5-Turbo because of its reasonable costs and general widespread adoption.</p>
<p>We have also largely operated under the assumption that the user of the underlying system has the correct context for a clause available to them to ask questions about.While the end-to-end effectiveness will matter in a final system (e.g., using some other information retrieval systems to first identify the clauses), we believe that our prompt structure and escape hatches will help a model overcome false positives but leave investigation to future work.</p>
<p>Conclusions and Future Work</p>
<p>We have presented a structured answer generation task based on identifying the correct answer for a legal question given an associated clause from a document, detailed some issues with relying solely on natural language question answering, and then present insights gleaned from a manual exploration of the prompt space over 3.5 weeks.Using these insights, we compared the reliability and consistency of structured answer generation for two legal questions and several hundred examples and found that our proposed technique is superior to semantic matching between clauses and answer options, especially through small tweaks to prompts and the use of in-context learning.</p>
<p>Looking forward, we plan to more exhaustively investigate how many different prompt templates might be needed for our use cases and whether or not we can identify good and bad answer options (i.e., to avoid users having to guess and check).Additionally, being able to automatically detect whether or not further refinement of options or providing examples for in-context learning is a lucrative avenue as it may reduce user frustration to just provide labelled examples.As the generative model space stabilizes, we also plan to investigate the transferability of our prompts to other models to determine their actual reusability.</p>
<p>Fig. 1 :
1
Fig. 1: An example of an environmental indemnity clause with mutual indemnification.</p>
<p>Figure 1
1
Figure 1 is an example indemnity clause from an arbitrary lease from our collection of legal documents.As an example, if one prompts OpenAI's GPT-3.5-Turbomodel (with temperature set to 0) with the following: In the clause below, who indemnifies whom? and then append the example clause, we get the following responses after regenerating a handful of times (with varying repetitions):</p>
<p>Fig. 2 :
2
Fig. 2: Four sets of answer options for the indemnity question.</p>
<p>Fig. 3 :
3
Fig. 3: Two sets of answer options for the information-sharing question.</p>
<p>P2:</p>
<p>Referring only to the information contained in the clause below, only select the numbered option that is implied by the clause, without providing any other information or justification.If you cannot determine which of the conditions are implied, respond with the exact text: "The clause is silent".{{Options}} {{Clause}}</p>
<p>Table 1 :
1
Number of correct predictions for each answer option and accuracy (A) with different
) A</p>
<p>Table 2 :
2
Number of correct predictions for each answer option and accuracy (A) with different
T1T21 (4) 2 (81) 3 (40) 4 (2) 5 (57) A 1 (4) 2 (81) 3 (40) 4 (2) 5 (57) APIL-LegalBert01122000.23041021 0.18OpenAI Ada000060.04000010.01OpenAI GPT-3.5 Turbo35619100.5506618010.59OpenAI GPT-412133120.4324116000.48PaLM206526010.6506017024 0.71</p>
<p>Table 3 :
3
P4: Read the following permitted use of confidential information legal clause: {{Clause}} Pretend you are a party to the agreement in which the permitted use of confidential information legal clause you have read exists in.You only know what you have read in this prompt.For what purposes are you allowed to use the confidential information?If the clause does not specify the purpose for which you may use the confidential information, respond with: "Unable to determine".Number of correct predictions for each answer option and accuracy (A) with different prompts and options S1 for the indemnity question.The true distribution of the answers is listed along with the answer options in brackets.
In your response, only include the</p>
<p>Table 4 :
4
Number
of correct predictions foreach answer option and accuracy (A) withdifferent prompts and options T2 for theinformation-sharing question. The true dis-tribution of the answers is listed along withthe answer options in brackets.
Especially for lay-people, it is probably more useful as a means to avoid the costs of dealing with a lawyer.
https://blog.google/technology/ai/lamda/
https://ai.facebook.com/blog/large-language-model-llama-meta-ai/
https://platform.openai.com/docs/models
Specifically https://huggingface.co/valhalla/distilbart-mnli-12-1 and related models.
11 Description of the tool is omitted here for brevity. It is available at https://github. com/zuvaai/gpt-
tool.12 We note that for all practical purposes, outside of some initial testing, temperature was set fairly consistently to 0.
https://huggingface.co/pile-of-law/legalbert-large-1.7M-2</p>
<p>E Almazrouei, H Alobeidli, A Alshamsi, A Cappelli, R Cojocaru, M Debbah, E Goffinet, D Heslow, J Launay, Q Malartic, B Noune, B Pannier, Falcon-40B: an open large language model. Penedo, G.2023with state-of-the-art performance</p>
<p>. R Anil, A M Dai, O Firat, M Johnson, D Lepikhin, A Passos, S Shakeri, E Taropa, P Bailey, Z Chen, Palm. 22023</p>
<p>S Biderman, H Schoelkopf, Q Anthony, H Bradley, K O'brien, E Hallahan, M A Khan, S Purohit, U S Prashanth, E Raff, A Skowron, L Sutawika, O Van Der Wal, Pythia: A suite for analyzing large language models across training and scaling. 2023</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, 2020</p>
<p>SMOTE: Synthetic minority over-sampling technique. N V Chawla, K W Bowyer, L O Hall, W P Kegelmeyer, Journal of Artificial Intelligence Research. 162002</p>
<p>W L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. March 2023</p>
<p>A reliable and accurate multiple choice question answering system for due diligence. R Chitta, A K Hudek, Proceedings of the International Conference on Artificial Intelligence and Law. the International Conference on Artificial Intelligence and Law2019</p>
<p>Hello dolly: Democratizing the magic of chatgpt with open models. M Conover, M Hayes, A Mathur, X Meng, J Xie, J Wan, A Ghodsi, P Wendell, M Zaharia, March 2023</p>
<p>Overview of the trec 2006 question answering track. H T Dang, J Lin, D Kelly, Proceedings of the Text Retrieval Conference. the Text Retrieval Conference2006</p>
<p>A survey on question answering systems over linked data and documents. E Dimitrakis, K Sgontzos, Y Tzitzikas, Journal of intelligent information systems. 552020</p>
<p>The utility of context when extracting entities from legal documents. J Donnelly, A Roegiest, Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management. the 29th ACM International Conference on Information &amp; Knowledge Management2020</p>
<p>Making pre-trained language models better fewshot learners. T Gao, A Fisch, D Chen, Proceedings of the International Joint Conference on Natural Language Processing. the International Joint Conference on Natural Language Processing2021</p>
<p>Pile of law: Learning responsible data filtering from the law and a 256gb open-source legal dataset. P Henderson, M S Krass, L Zheng, N Guha, C D Manning, D Jurafsky, D E Ho, 2022</p>
<p>The class imbalance problem: Significance and strategies. N Japkowicz, Proceedings of the International Conference on artificial intelligence. the International Conference on artificial intelligence200056</p>
<p>Survey of hallucination in natural language generation. Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii, Y J Bang, A Madotto, P Fung, ACM Computing Surveys. 55122023</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Computing Surveys. 5592023</p>
<p>Rethinking the role of demonstrations: What makes in-context learning work. S Min, X Lyu, A Holtzman, M Artetxe, M Lewis, H Hajishirzi, L Zettlemoyer, 2022</p>
<p>. A Neelakantan, T Xu, R Puri, A Radford, J M Han, J Tworek, Q Yuan, N Tezak, J W Kim, C Hallacy, J Heidecke, P Shyam, B Power, T E Nekoul, G Sastry, G Krueger, D Schnurr, F P Such, K Hsu, M Thompson, T Khan, T Sherbakov, J Jang, P Welinder, L Weng, 2022Text and code embeddings by contrastive pre-training</p>
<p>OpenAI: Gpt-4 technical report. 2023</p>
<p>. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, 2022Training language models to follow instructions with human feedback</p>
<p>Improving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, 2018</p>
<p>Prompt programming for large language models: Beyond the few-shot paradigm. L Reynolds, K Mcdonell, 2021</p>
<p>A dataset and an examination of identifying passages for due diligence. A Roegiest, A K Hudek, A Mcnulty, The 41st international ACM SIGIR conference on research &amp; development in information retrieval. 2018</p>
<p>Multitask prompted training enables zero-shot task generalization. V Sanh, A Webson, C Raffel, S H Bach, L Sutawika, Z Alyafeai, A Chaffin, A Stiegler, T L Scao, A Raja, 2021</p>
<p>R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I Kloumann, A Korenev, P S Koura, M A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X E Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, 2023Scialom, T.: Llama 2: Open foundation and fine-tuned chat models</p>
<p>Using semantic similarity for multi-label zero-shot classification of text documents. S P Veeranna, J Nam, E L Mencıa, J Fürnkranz, Proceeding of European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning. eeding of European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning2016</p>
<p>Benchmarking generalization via in-context instructions on 1. Y Wang, S Mishra, P Alipoormolabashi, Y Kordi, A Mirzaei, A Arunkumar, A Ashok, A S Dhanasekaran, A Naik, D Stap, 2022600+ language tasks</p>
<p>J White, Q Fu, S Hays, M Sandborn, C Olea, H Gilbert, A Elnashar, J Spencer-Smith, D C Schmidt, A prompt pattern catalog to enhance prompt engineering with chatgpt. 2023</p>
<p>A broad-coverage challenge corpus for sentence understanding through inference. A Williams, N Nangia, S Bowman, Proceedings of the Conference of the North American Chapter. the Conference of the North American ChapterHuman Language Technologies2018</p>
<p>An explanation of in-context learning as implicit bayesian inference. S M Xie, A Raghunathan, P Liang, T Ma, 2022</p>
<p>Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach. W Yin, J Hay, D Roth, 2019</p>
<p>Legal prompting: Teaching a language model to think like a lawyer. F Yu, L Quartey, F Schilder, 2022</p>
<p>. T Zhang, F Ladhak, E Durmus, P Liang, K Mckeown, T B Hashimoto, 2023Benchmarking large language models for news summarization</p>
<p>Large language models are human-level prompt engineers. Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, 2022</p>            </div>
        </div>

    </div>
</body>
</html>
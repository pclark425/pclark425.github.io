<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1657 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1657</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1657</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-53046511</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1810.05687v3.pdf" target="_blank">Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</a></p>
                <p><strong>Paper Abstract:</strong> We consider the problem of transferring policies to the real world by training on a distribution of simulated scenarios. Rather than manually tuning the randomization of simulations, we adapt the simulation parameter distribution using a few real world roll-outs interleaved with policy training. In doing so, we are able to change the distribution of simulations to improve the policy transfer by matching the policy behavior in simulation and the real world. We show that policies trained with our method are able to reliably transfer to different robots in two real world tasks: swing-peg-in-hole and opening a cabinet drawer. The video of our experiments can be found at https://sites.google.com/view/simopt</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1657.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1657.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimOpt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simulation Optimization (SimOpt) — adaptive simulation randomization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative framework that adapts a parameterized distribution of simulation parameters using a small number of real-world roll-outs interleaved with reinforcement learning in simulation, updating the simulator parameter distribution under a KL constraint to improve sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>SimOpt (algorithm)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A closed-loop algorithmic system (not a physical robot) that interleaves policy training in simulation (PPO) with iterative updates to a Gaussian simulation-parameter distribution using real-world observation roll-outs and a KL-constrained sampling-based optimizer (REPS-style).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation / sim-to-real transfer</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>NVIDIA Flex (GPU-based physics simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>GPU-accelerated physics simulator (Flex) used to simulate rigid-body and soft-body dynamics, contact interactions, and multi-instance scenes in parallel on GPUs; used as the parameterized world model for RL roll-outs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity physics (GPU-based maximal-coordinate simulator supporting rigid and soft-body dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>rigid-body dynamics, contact dynamics, soft-body/rope dynamics (Flex rope model), joint and actuator effects, full physical state in simulation, deterministic simulator with parameter sampling to induce stochasticity</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No exact replication of real-world rope bending state in closed-loop at every timestep (real world bending not fully observable); potential lack of sensor noise and unmodeled fine-grained properties (e.g. exact rope micro-structure, unmodelled unobserved real-world disturbances); reward computed only in simulation (real reward not instrumented).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical robot setups used for roll-outs: ABB Yumi and Franka Panda manipulators with DART-based tracking from depth images to provide 3D positions of key objects (peg, drawer handle); only partial real observations are used for SimOpt cost.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>General approach to transfer closed-loop manipulation policies (applied to swing-peg-in-hole and drawer opening tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (PPO) in simulation interleaved with simulation-parameter distribution updates via sampling-based relative-entropy-constrained optimization (REPS-style).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Improvement in discrepancy metric between simulated and real observation trajectories and downstream task success rate when executing the new policy in the real world (measured as task success rate in physical trials).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Start from a parameterized Gaussian distribution over simulation parameters (means and full or diagonal covariances); parameters include physical properties (rope properties, peg size, object poses), robot parameters (action scaling), object poses (cabinet position), and contact/friction-related parameters; SimOpt adapts these distributions using real roll-outs rather than relying on hand-tuned wide randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatches in rope dynamics and compliance, object sizes and clearances (e.g. peg/hole size), object poses (cabinet position offsets), robot action scaling and resulting contact forces, gripper orientation and contact interactions; overly-wide randomization producing infeasible scenarios and simulator breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of a high-fidelity GPU physics simulator (Flex), parameterized simulation distributions (Gaussian), KL-constrained iterative updates (REPS-style) using a discrepancy between simulated and real observation trajectories, a small number of real roll-outs per iteration, closed-loop policies (so policy behavior changes with simulation parameters), and massively parallel simulation to evaluate many parameter samples.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Accurate modeling of contact dynamics and soft-rope dynamics (rope compliance/physical parameters) and correct robot action scaling are important for transfer; overly wide parameter variance is harmful—starting from a conservative distribution and adapting it via real roll-outs yields better transfer than naive very-wide randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Rather than fine-tuning policy weights on the real robot, the method uses a small number of real roll-outs (e.g., 3 roll-outs per SimOpt iteration in the reported experiments) to update the simulation parameter distribution, then retrains/continues RL in the (updated) simulation; typical inner-loop numbers reported: 3 real roll-outs per SimOpt iteration, many simulation samples per distribution update (e.g., 9600 samples per update) and multiple REPS updates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Not a direct comparison of low- vs high-fidelity simulators; paper shows that increasing variance of randomized parameters (e.g., cabinet position standard deviation from 2cm to 10cm) degrades learning and can prevent solving the task — i.e., overly-wide randomized distributions (intended to cover mismatch) can hinder learning and transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adapting a simulation-parameter distribution using a few real roll-outs (SimOpt) yields successful sim-to-real transfer while requiring far fewer real trials than naive approaches; starting from a conservative distribution and iteratively shifting it is more effective than training on very wide randomizations (which can include infeasible scenarios); rope/contact dynamics and robot action scaling were critical factors for successful transfer in the tested tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1657.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1657.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Swing-peg-in-hole (Yumi)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Swing-peg-in-hole task executed on ABB 7-DoF Yumi robot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A manipulation task where a peg attached to the robot by a soft rope must be swung and inserted into an angled hole; used to evaluate sim-to-real transfer when soft-body dynamics (rope) and contact dynamics are important.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>ABB Yumi 7-DoF robot (real), multi-agent simulated Yumi in NVIDIA Flex</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>7-DoF dual-arm industrial research robot (ABB Yumi) used to execute closed-loop joint-velocity policies that control the arm and manipulate a peg suspended on a soft rope to insert it into a hole.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic manipulation (non-rigid / soft-body manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>NVIDIA Flex (GPU-accelerated physics simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulates rigid and soft-body dynamics (including rope/soft-body behavior), contacts and collisions, and full state dynamics; used to run many parallel simulated agents for RL and to sample parameterized scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity physics for rigid and soft bodies (GPU Flex); simulates rope swing dynamics and contacts.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rope/soft-body dynamics (compliance, swinging), peg and hole geometry, contact interactions between peg and hole, robot joint dynamics and actions, full-state physics in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Real rope bending configuration is not fully observable/trackable in the real setup and thus not set continuously in sim; possible absence of real-world sensor noise and unmodeled micro-physical properties of the rope; reward only computed in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical ABB Yumi robot manipulating a peg attached via a rope; DART-based tracking (from depth images) provides 3D peg position to initialize and compute discrepancy; experiments run with a real peg and a 45-degree oriented hole.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Swinging a peg on a soft rope to align and insert it into a hole (dynamic non-prehensile manipulation involving soft-body dynamics and contact insertion).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (PPO) in simulation with simulation parameter randomization and iterative SimOpt updates using a few real roll-outs (closed-loop policies).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate on real robot trials (percentage of successful peg insertions over a fixed number of trials).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>90% success (18/20) after two SimOpt iterations when evaluated on 20 trials.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Initial randomization included parameters such as rope physical parameters (diameter, compliance), peg size, peg-box size, robot parameters including action scaling, and other physical properties; SimOpt shifted the distribution, notably rope physical parameters and robot action scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatch in rope dynamics and compliance, incorrect action scaling of the robot policy, and geometric/size mismatches (peg/hole clearances); inability to set continuous rope state from real data limits certain trajectory-based parameter identification approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>High-fidelity simulation of rope and contact dynamics (Flex), closed-loop policies (policy behavior informative for parameter updates even when some dimensions unobserved), small number of real roll-outs per iteration (3), large-scale parallel simulation (9600 samples per update), and KL-constrained distribution updates leading to adapted rope and robot parameter means.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Accurate modeling of rope physical parameters (which significantly changed during SimOpt) and correct action scaling for the robot are critical for insertion success; contact and dynamic rope modeling are required for reliable transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>No direct policy weight fine-tuning on the real robot; instead, 3 real roll-outs per SimOpt iteration were used to compute observation discrepancy and update simulation distributions; each SimOpt iteration included ~100 RL iterations (≈7 minutes) and 3 real roll-outs, with 3 distribution update steps using 9600 simulation samples per update.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Not a direct fidelity-level comparison; demonstrated that adjusting simulation-parameter distribution (esp. rope parameters) via SimOpt produced high real-world success while naive wide randomization often yielded infeasible instances and failed policies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>After two SimOpt iterations (using only a few real roll-outs each iteration and large-scale parallel simulation), the policy trained in simulation using the adapted parameter distribution transferred to the real Yumi robot with ~90% success; the most significant parameter adaptations were rope physical properties and robot action scaling, highlighting those fidelity aspects as critical for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1657.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1657.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Drawer-opening (Panda)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cabinet drawer opening task executed on Franka Emika 7-DoF Panda robot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contact-rich manipulation task where a Franka Panda robot must grasp a drawer handle with two fingers and pull to open the drawer; used to evaluate sim-to-real transfer where contact dynamics and gripper orientation are important.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Franka Emika Panda 7-DoF robot (real), simulated Panda agents in NVIDIA Flex</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>7-DoF robotic arm that outputs 7 joint velocity commands plus a gripper command; policy must reach, correctly orient the gripper to grasp the handle with both fingers, and pull to open the drawer without losing grip.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic manipulation (contact-rich grasping and pulling)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>NVIDIA Flex (GPU-accelerated physics simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulates rigid-body dynamics and contacts for the robot, cabinet, handle and gripper; provides full-state rewards and supports parameterized randomization of object poses and physical properties.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity rigid-body/contact physics with parameterized randomization (Flex), but simplified/no full sensor noise modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Contact dynamics between gripper fingers and drawer handle, rigid-body kinematics/dynamics of robot and cabinet, drawer motion, object poses (drawer handle 3D position), robot joint angles.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Real-world sensor noise and some unobserved internal states not modeled; certain real-world failure modes (e.g., subtle frictional/tactile differences causing fingers to open) may be approximated rather than exactly modeled; reward only computed in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real Franka Panda robot in front of a cabinet with a tracked drawer handle; DART-based tracking provides 3D positions for initialization and discrepancy computation; experiments evaluate the policy on 20 physical trials.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Reaching, grasping the drawer handle with both fingers while maintaining orthogonal gripper orientation, and pulling to open the drawer.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (PPO) in simulation with domain randomization and iterative SimOpt updates using a few real roll-outs to adapt simulation-parameter distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Task success rate on real trials (percentage of successful drawer openings over 20 trials) and qualitative improvement in gripper orientation / failure mode reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>100% success on 20 trials after SimOpt updates (paper states robot 'can open the drawer at all times' when evaluated on 20 trials).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomized parameters included cabinet pose (position along lateral X axis), robot parameters (action scaling), gripper/handle physical properties; initial Gaussian parameterization (mean and diagonal covariance) was adapted by SimOpt. Experiments showed sensitivity to the variance of cabinet position (σ).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatch in cabinet pose (position offsets), gripper orientation control and finger contact forces (leading to fingers opening under excessive push), friction/contact property mismatches, and overly-wide randomization producing infeasible training scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Adapting the cabinet position and robot parameter distributions via SimOpt using a few real roll-outs (3 per SimOpt iteration), high-fidelity contact simulation (Flex), DART-based tracking to measure real observations, KL-constrained updates and large-scale parallel simulation for sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Contact dynamics and correct modeling/adjustment of robot action scaling and object pose distributions (cabinet position) are important; low variance around cabinet position (e.g., std dev ~2cm) allowed successful RL training, while larger variance (e.g., std dev =10cm) hindered learning.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>No policy fine-tuning on real hardware; used 3 real roll-outs per SimOpt iteration and 20 update steps per SimOpt iteration with 9600 simulation samples per update; RL during iterations used 200 PPO iterations (~22 minutes) per SimOpt iteration (with option to warm-start policy weights).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Paper reports sensitivity to variance in parameter distributions: policies trained with small variance in cabinet position (σ≈2cm) succeed in simulation and transfer, while higher variance (σ≈10cm) yields conservative policies that fail to open the drawer; SimOpt can adapt a conservative source distribution toward the target and enable transfer where naïve wide randomization would fail.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>SimOpt-adapted simulation parameter distributions corrected robot and drawer parameters (including action scaling and cabinet position) and eliminated a failure mode where the gripper would open due to poor orientation/force control; after a small number of SimOpt iterations and few real roll-outs the Panda robot opened the drawer reliably (20/20 trials).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience', 'publication_date_yy_mm': '2018-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 2)</em></li>
                <li>EPOpt: Learning robust neural network policies using model ensembles <em>(Rating: 2)</em></li>
                <li>Preparing for the unknown: Learning a universal policy with online system identification <em>(Rating: 1)</em></li>
                <li>Transfer from simulation to real world through learning deep inverse dynamics model <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1657",
    "paper_id": "paper-53046511",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "SimOpt",
            "name_full": "Simulation Optimization (SimOpt) — adaptive simulation randomization",
            "brief_description": "An iterative framework that adapts a parameterized distribution of simulation parameters using a small number of real-world roll-outs interleaved with reinforcement learning in simulation, updating the simulator parameter distribution under a KL constraint to improve sim-to-real transfer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "SimOpt (algorithm)",
            "agent_system_description": "A closed-loop algorithmic system (not a physical robot) that interleaves policy training in simulation (PPO) with iterative updates to a Gaussian simulation-parameter distribution using real-world observation roll-outs and a KL-constrained sampling-based optimizer (REPS-style).",
            "domain": "general robotics manipulation / sim-to-real transfer",
            "virtual_environment_name": "NVIDIA Flex (GPU-based physics simulator)",
            "virtual_environment_description": "GPU-accelerated physics simulator (Flex) used to simulate rigid-body and soft-body dynamics, contact interactions, and multi-instance scenes in parallel on GPUs; used as the parameterized world model for RL roll-outs.",
            "simulation_fidelity_level": "high-fidelity physics (GPU-based maximal-coordinate simulator supporting rigid and soft-body dynamics)",
            "fidelity_aspects_modeled": "rigid-body dynamics, contact dynamics, soft-body/rope dynamics (Flex rope model), joint and actuator effects, full physical state in simulation, deterministic simulator with parameter sampling to induce stochasticity",
            "fidelity_aspects_simplified": "No exact replication of real-world rope bending state in closed-loop at every timestep (real world bending not fully observable); potential lack of sensor noise and unmodeled fine-grained properties (e.g. exact rope micro-structure, unmodelled unobserved real-world disturbances); reward computed only in simulation (real reward not instrumented).",
            "real_environment_description": "Physical robot setups used for roll-outs: ABB Yumi and Franka Panda manipulators with DART-based tracking from depth images to provide 3D positions of key objects (peg, drawer handle); only partial real observations are used for SimOpt cost.",
            "task_or_skill_transferred": "General approach to transfer closed-loop manipulation policies (applied to swing-peg-in-hole and drawer opening tasks).",
            "training_method": "Reinforcement learning (PPO) in simulation interleaved with simulation-parameter distribution updates via sampling-based relative-entropy-constrained optimization (REPS-style).",
            "transfer_success_metric": "Improvement in discrepancy metric between simulated and real observation trajectories and downstream task success rate when executing the new policy in the real world (measured as task success rate in physical trials).",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Start from a parameterized Gaussian distribution over simulation parameters (means and full or diagonal covariances); parameters include physical properties (rope properties, peg size, object poses), robot parameters (action scaling), object poses (cabinet position), and contact/friction-related parameters; SimOpt adapts these distributions using real roll-outs rather than relying on hand-tuned wide randomization.",
            "sim_to_real_gap_factors": "Mismatches in rope dynamics and compliance, object sizes and clearances (e.g. peg/hole size), object poses (cabinet position offsets), robot action scaling and resulting contact forces, gripper orientation and contact interactions; overly-wide randomization producing infeasible scenarios and simulator breakdowns.",
            "transfer_enabling_conditions": "Use of a high-fidelity GPU physics simulator (Flex), parameterized simulation distributions (Gaussian), KL-constrained iterative updates (REPS-style) using a discrepancy between simulated and real observation trajectories, a small number of real roll-outs per iteration, closed-loop policies (so policy behavior changes with simulation parameters), and massively parallel simulation to evaluate many parameter samples.",
            "fidelity_requirements_identified": "Accurate modeling of contact dynamics and soft-rope dynamics (rope compliance/physical parameters) and correct robot action scaling are important for transfer; overly wide parameter variance is harmful—starting from a conservative distribution and adapting it via real roll-outs yields better transfer than naive very-wide randomization.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "Rather than fine-tuning policy weights on the real robot, the method uses a small number of real roll-outs (e.g., 3 roll-outs per SimOpt iteration in the reported experiments) to update the simulation parameter distribution, then retrains/continues RL in the (updated) simulation; typical inner-loop numbers reported: 3 real roll-outs per SimOpt iteration, many simulation samples per distribution update (e.g., 9600 samples per update) and multiple REPS updates.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": "Not a direct comparison of low- vs high-fidelity simulators; paper shows that increasing variance of randomized parameters (e.g., cabinet position standard deviation from 2cm to 10cm) degrades learning and can prevent solving the task — i.e., overly-wide randomized distributions (intended to cover mismatch) can hinder learning and transfer.",
            "key_findings": "Adapting a simulation-parameter distribution using a few real roll-outs (SimOpt) yields successful sim-to-real transfer while requiring far fewer real trials than naive approaches; starting from a conservative distribution and iteratively shifting it is more effective than training on very wide randomizations (which can include infeasible scenarios); rope/contact dynamics and robot action scaling were critical factors for successful transfer in the tested tasks.",
            "uuid": "e1657.0",
            "source_info": {
                "paper_title": "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Swing-peg-in-hole (Yumi)",
            "name_full": "Swing-peg-in-hole task executed on ABB 7-DoF Yumi robot",
            "brief_description": "A manipulation task where a peg attached to the robot by a soft rope must be swung and inserted into an angled hole; used to evaluate sim-to-real transfer when soft-body dynamics (rope) and contact dynamics are important.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "ABB Yumi 7-DoF robot (real), multi-agent simulated Yumi in NVIDIA Flex",
            "agent_system_description": "7-DoF dual-arm industrial research robot (ABB Yumi) used to execute closed-loop joint-velocity policies that control the arm and manipulate a peg suspended on a soft rope to insert it into a hole.",
            "domain": "robotic manipulation (non-rigid / soft-body manipulation)",
            "virtual_environment_name": "NVIDIA Flex (GPU-accelerated physics simulator)",
            "virtual_environment_description": "Simulates rigid and soft-body dynamics (including rope/soft-body behavior), contacts and collisions, and full state dynamics; used to run many parallel simulated agents for RL and to sample parameterized scenarios.",
            "simulation_fidelity_level": "high-fidelity physics for rigid and soft bodies (GPU Flex); simulates rope swing dynamics and contacts.",
            "fidelity_aspects_modeled": "Rope/soft-body dynamics (compliance, swinging), peg and hole geometry, contact interactions between peg and hole, robot joint dynamics and actions, full-state physics in simulation.",
            "fidelity_aspects_simplified": "Real rope bending configuration is not fully observable/trackable in the real setup and thus not set continuously in sim; possible absence of real-world sensor noise and unmodeled micro-physical properties of the rope; reward only computed in simulation.",
            "real_environment_description": "Physical ABB Yumi robot manipulating a peg attached via a rope; DART-based tracking (from depth images) provides 3D peg position to initialize and compute discrepancy; experiments run with a real peg and a 45-degree oriented hole.",
            "task_or_skill_transferred": "Swinging a peg on a soft rope to align and insert it into a hole (dynamic non-prehensile manipulation involving soft-body dynamics and contact insertion).",
            "training_method": "Reinforcement learning (PPO) in simulation with simulation parameter randomization and iterative SimOpt updates using a few real roll-outs (closed-loop policies).",
            "transfer_success_metric": "Task success rate on real robot trials (percentage of successful peg insertions over a fixed number of trials).",
            "transfer_performance_sim": null,
            "transfer_performance_real": "90% success (18/20) after two SimOpt iterations when evaluated on 20 trials.",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Initial randomization included parameters such as rope physical parameters (diameter, compliance), peg size, peg-box size, robot parameters including action scaling, and other physical properties; SimOpt shifted the distribution, notably rope physical parameters and robot action scaling.",
            "sim_to_real_gap_factors": "Mismatch in rope dynamics and compliance, incorrect action scaling of the robot policy, and geometric/size mismatches (peg/hole clearances); inability to set continuous rope state from real data limits certain trajectory-based parameter identification approaches.",
            "transfer_enabling_conditions": "High-fidelity simulation of rope and contact dynamics (Flex), closed-loop policies (policy behavior informative for parameter updates even when some dimensions unobserved), small number of real roll-outs per iteration (3), large-scale parallel simulation (9600 samples per update), and KL-constrained distribution updates leading to adapted rope and robot parameter means.",
            "fidelity_requirements_identified": "Accurate modeling of rope physical parameters (which significantly changed during SimOpt) and correct action scaling for the robot are critical for insertion success; contact and dynamic rope modeling are required for reliable transfer.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "No direct policy weight fine-tuning on the real robot; instead, 3 real roll-outs per SimOpt iteration were used to compute observation discrepancy and update simulation distributions; each SimOpt iteration included ~100 RL iterations (≈7 minutes) and 3 real roll-outs, with 3 distribution update steps using 9600 simulation samples per update.",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": "Not a direct fidelity-level comparison; demonstrated that adjusting simulation-parameter distribution (esp. rope parameters) via SimOpt produced high real-world success while naive wide randomization often yielded infeasible instances and failed policies.",
            "key_findings": "After two SimOpt iterations (using only a few real roll-outs each iteration and large-scale parallel simulation), the policy trained in simulation using the adapted parameter distribution transferred to the real Yumi robot with ~90% success; the most significant parameter adaptations were rope physical properties and robot action scaling, highlighting those fidelity aspects as critical for transfer.",
            "uuid": "e1657.1",
            "source_info": {
                "paper_title": "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience",
                "publication_date_yy_mm": "2018-10"
            }
        },
        {
            "name_short": "Drawer-opening (Panda)",
            "name_full": "Cabinet drawer opening task executed on Franka Emika 7-DoF Panda robot",
            "brief_description": "A contact-rich manipulation task where a Franka Panda robot must grasp a drawer handle with two fingers and pull to open the drawer; used to evaluate sim-to-real transfer where contact dynamics and gripper orientation are important.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Franka Emika Panda 7-DoF robot (real), simulated Panda agents in NVIDIA Flex",
            "agent_system_description": "7-DoF robotic arm that outputs 7 joint velocity commands plus a gripper command; policy must reach, correctly orient the gripper to grasp the handle with both fingers, and pull to open the drawer without losing grip.",
            "domain": "robotic manipulation (contact-rich grasping and pulling)",
            "virtual_environment_name": "NVIDIA Flex (GPU-accelerated physics simulator)",
            "virtual_environment_description": "Simulates rigid-body dynamics and contacts for the robot, cabinet, handle and gripper; provides full-state rewards and supports parameterized randomization of object poses and physical properties.",
            "simulation_fidelity_level": "high-fidelity rigid-body/contact physics with parameterized randomization (Flex), but simplified/no full sensor noise modeling.",
            "fidelity_aspects_modeled": "Contact dynamics between gripper fingers and drawer handle, rigid-body kinematics/dynamics of robot and cabinet, drawer motion, object poses (drawer handle 3D position), robot joint angles.",
            "fidelity_aspects_simplified": "Real-world sensor noise and some unobserved internal states not modeled; certain real-world failure modes (e.g., subtle frictional/tactile differences causing fingers to open) may be approximated rather than exactly modeled; reward only computed in simulation.",
            "real_environment_description": "Real Franka Panda robot in front of a cabinet with a tracked drawer handle; DART-based tracking provides 3D positions for initialization and discrepancy computation; experiments evaluate the policy on 20 physical trials.",
            "task_or_skill_transferred": "Reaching, grasping the drawer handle with both fingers while maintaining orthogonal gripper orientation, and pulling to open the drawer.",
            "training_method": "Reinforcement learning (PPO) in simulation with domain randomization and iterative SimOpt updates using a few real roll-outs to adapt simulation-parameter distributions.",
            "transfer_success_metric": "Task success rate on real trials (percentage of successful drawer openings over 20 trials) and qualitative improvement in gripper orientation / failure mode reduction.",
            "transfer_performance_sim": null,
            "transfer_performance_real": "100% success on 20 trials after SimOpt updates (paper states robot 'can open the drawer at all times' when evaluated on 20 trials).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomized parameters included cabinet pose (position along lateral X axis), robot parameters (action scaling), gripper/handle physical properties; initial Gaussian parameterization (mean and diagonal covariance) was adapted by SimOpt. Experiments showed sensitivity to the variance of cabinet position (σ).",
            "sim_to_real_gap_factors": "Mismatch in cabinet pose (position offsets), gripper orientation control and finger contact forces (leading to fingers opening under excessive push), friction/contact property mismatches, and overly-wide randomization producing infeasible training scenarios.",
            "transfer_enabling_conditions": "Adapting the cabinet position and robot parameter distributions via SimOpt using a few real roll-outs (3 per SimOpt iteration), high-fidelity contact simulation (Flex), DART-based tracking to measure real observations, KL-constrained updates and large-scale parallel simulation for sample efficiency.",
            "fidelity_requirements_identified": "Contact dynamics and correct modeling/adjustment of robot action scaling and object pose distributions (cabinet position) are important; low variance around cabinet position (e.g., std dev ~2cm) allowed successful RL training, while larger variance (e.g., std dev =10cm) hindered learning.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": "No policy fine-tuning on real hardware; used 3 real roll-outs per SimOpt iteration and 20 update steps per SimOpt iteration with 9600 simulation samples per update; RL during iterations used 200 PPO iterations (~22 minutes) per SimOpt iteration (with option to warm-start policy weights).",
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": "Paper reports sensitivity to variance in parameter distributions: policies trained with small variance in cabinet position (σ≈2cm) succeed in simulation and transfer, while higher variance (σ≈10cm) yields conservative policies that fail to open the drawer; SimOpt can adapt a conservative source distribution toward the target and enable transfer where naïve wide randomization would fail.",
            "key_findings": "SimOpt-adapted simulation parameter distributions corrected robot and drawer parameters (including action scaling and cabinet position) and eliminated a failure mode where the gripper would open due to poor orientation/force control; after a small number of SimOpt iterations and few real roll-outs the Panda robot opened the drawer reliably (20/20 trials).",
            "uuid": "e1657.2",
            "source_info": {
                "paper_title": "Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience",
                "publication_date_yy_mm": "2018-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 2,
            "sanitized_title": "domain_randomization_for_transferring_deep_neural_networks_from_simulation_to_the_real_world"
        },
        {
            "paper_title": "EPOpt: Learning robust neural network policies using model ensembles",
            "rating": 2,
            "sanitized_title": "epopt_learning_robust_neural_network_policies_using_model_ensembles"
        },
        {
            "paper_title": "Preparing for the unknown: Learning a universal policy with online system identification",
            "rating": 1,
            "sanitized_title": "preparing_for_the_unknown_learning_a_universal_policy_with_online_system_identification"
        },
        {
            "paper_title": "Transfer from simulation to real world through learning deep inverse dynamics model",
            "rating": 1,
            "sanitized_title": "transfer_from_simulation_to_real_world_through_learning_deep_inverse_dynamics_model"
        }
    ],
    "cost": 0.0136235,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</p>
<p>Yevgen Chebotar 
Ankur Handa 
Viktor Makoviychuk 
Miles Macklin 
Jan Issac 
Nathan Ratliff 
Dieter Fox 
Closing the Sim-to-Real Loop: Adapting Simulation Randomization with Real World Experience</p>
<p>Fig. 1. Policies for opening a cabinet drawer and swing-peg-in-hole tasks trained by alternatively performing reinforcement learning with multiple agents in simulation and updating simulation parameter distribution using a few real world policy executions.Abstract-We consider the problem of transferring policies to the real world by training on a distribution of simulated scenarios. Rather than manually tuning the randomization of simulations, we adapt the simulation parameter distribution using a few real world roll-outs interleaved with policy training. In doing so, we are able to change the distribution of simulations to improve the policy transfer by matching the policy behavior in simulation and the real world. We show that policies trained with our method are able to reliably transfer to different robots in two real world tasks: swing-peg-in-hole and opening a cabinet drawer. The video of our experiments can be found at https: //sites.google.com/view/simopt.
 Fig. 1
. Policies for opening a cabinet drawer and swing-peg-in-hole tasks trained by alternatively performing reinforcement learning with multiple agents in simulation and updating simulation parameter distribution using a few real world policy executions.</p>
<p>Abstract-We consider the problem of transferring policies to the real world by training on a distribution of simulated scenarios. Rather than manually tuning the randomization of simulations, we adapt the simulation parameter distribution using a few real world roll-outs interleaved with policy training. In doing so, we are able to change the distribution of simulations to improve the policy transfer by matching the policy behavior in simulation and the real world. We show that policies trained with our method are able to reliably transfer to different robots in two real world tasks: swing-peg-in-hole and opening a cabinet drawer. The video of our experiments can be found at https: //sites.google.com/view/simopt.</p>
<p>I. INTRODUCTION</p>
<p>Learning continuous control in real world complex environments has seen a wide interest in the past few years and in particular focusing on learning policies in simulators and transferring to real world, as we still struggle with finding ways to acquire the necessary amount of experience and data in the real world directly. While there have been recent attempts on learning by collecting large scale data directly on real robots [1,2,3,4], such an approach still remains challenging as collecting real world data is prohibitively laborious and expensive. Simulators offer several advantages, e.g. they can run faster than real-time and allow for acquiring large diversity of training data. However, due to the imprecise simulation models and lack of high fidelity replication of real world scenes, policies learned in simulations often cannot be directly applied on real-world systems, a phenomenon also known as the reality gap [5]. In this work, we focus on closing the reality gap by learning policies on distributions of simulated scenarios that are optimized for a better policy transfer.</p>
<p>Training policies on a large diversity of simulated scenarios by randomizing relevant parameters, also known as domain randomization, has shown a considerable promise for the real world transfer in a range of recent works [6,7,8,9]. However, design of the appropriate simulation parameter distributions remains a tedious task and often requires a substantial expert knowledge. Moreover, there are no guarantees that the applied randomization would actually lead to a sensible real world policy as the design choices made in randomizing the parameters tend to be somewhat biased by the expertise of the practitioner. In this work, we apply a data-driven approach and use real world data to adapt simulation randomization such that the behavior of the policies trained in simulation better matches their behavior in the real world. Therefore, starting with some initial distribution of the simulation parameters, we can perform learning in simulation and use real world roll-outs of learned policies to gradually change the simulation randomization such that the learned policies transfer better to the real world without requiring the exact replication of the real world scene in simulation. This approach falls into the domain of modelbased reinforcement learning. However, we leverage recent developments in physics simulations to provide a strong prior of the world model in order to accelerate the learning process. Our system uses partial observations of the real world and only needs to compute rewards in simulation, therefore lifting the requirement for full state knowledge or reward instrumentation in the real world.</p>
<p>II. RELATED WORK</p>
<p>The problem of finding accurate models of the robot and the environment that can facilitate the design of robotic controllers in real world dates back to the original works on system identification [10]. In the context of reinforcement learning (RL), model-based RL explored optimizing policies using learned models [11]. In [12,13], the data from realworld policy executions is used to fit a probabilistic dynamics model, which is then used for learning an optimal policy. Although our work follows the general principle of modelbased reinforcement learning, we aim at using a simulation engine as a form of parameterized model that can help us to embed prior knowledge about the world.</p>
<p>Overcoming the discrepancy between simulated models and the real world has been addressed through identifying simulation parameters [14], finding common feature representations of real and synthetic data [15], using generative models to make synthetic images more realistic [16], fine-tuning the policies trained in simulation in the real world [17], learning inverse dynamics models [18], multiobjective optimization of task fitness and transferability to the real world [19], training on ensembles of dynamics models [20] and training on a large variety of simulated scenarios [6]. Domain randomization of textures was used in [7] to learn to fly a real quadcopter by training an image based policy entirely in simulation. Peng et al. [21] use randomization of physical parameters of the scene to learn a policy in simulation and transfer it to real robot for pushing a puck to a target position. In [9], randomization of physical properties and object appearance is used to train a dexterous robotic hand to perform in-hand manipulation. Yu et al. [22] propose to not only train a policy on a distribution of simulated parameters, but also learn a component that predicts the system parameters from the current states and actions, and use the prediction as an additional input to the policy. In [23], an upper confidence bound on the estimated simulation optimization bias is used as a stopping criterion for a robust training with domain randomization. In [24], an auxiliary reward is used to encourage policies trained in source and target environments to visit the same states.</p>
<p>Combination of system identification and dynamics randomization has been used in the past to learn locomotion for a real quadruped [25], non-prehensile object manipulation [26] and in-hand object pivoting [27]. In our work, we recognize domain randomization and system identification as powerful tools for training general policies in simulation. However, we address the problem of automatically learning simulation parameter distributions that improve policy transfer, as it remains challenging to do it manually. Furthermore, as also noticed in [28], simulators have an advantage of providing a full state of the system compared to partial observations of the real world, which is also used in our work for designing better reward functions.</p>
<p>The closest to our approach are the methods from [29,30,31,32,33] that propose to iteratively learn simulation parameters and train policies. In [29], an iterative system identification framework is used to optimize trajectories of a bipedal robot in simulation and calibrate the simulation parameters by minimizing the discrepancy between the real world and simulated execution of the trajectories. Although we also use the real world data to compute the discrepancy of the simulated executions, we are able to use partial observations of the real world instead of the full states and we concentrate on learning general policies by finding simulation parameter distribution that leads to a better transfer without the need for exact replication of the real world environment. [30] suggests to optimize the simulation parameters such that the value function is well approximated in simulation without replicating the real world dynamics. We also recognize that exact replication of the real world dynamics might not be feasible, however a suitable randomization of the simulated scenarios can still lead to a successful policy transfer. In addition, our approach does not require estimating the reward in the real world, which might be challenging if some of the reward components can not be observed. [31] and [32] consider grounding the simulator using real world data. However, [31] requires a human in the loop to select the best simulation parameters, and [32] needs to fit additional models for the real robot forward dynamics and simulator inverse dynamics. Finally, our work is closest to the adaptive EPOpt framework of Rajeswaran et al. [33], which optimizes a policy over an ensemble of models and adapts the model distribution using data from the target domain. EPOpt optimizes a risk-sensitive objective to obtain robust policies, whereas we optimize the average performance which is a risk-neutral objective. Additionally, EPOpt updates the model distribution by employing Bayesian inference with a particle filter, whereas we update the model distribution using an iterative KLdivergence constrained procedure. More importantly, they focus on simulated environments while in our work, we develop an approach that is shown to work in real world and apply it to two real robot tasks.</p>
<p>III. CLOSING THE SIM-TO-REAL LOOP A. Simulation randomization</p>
<p>Let M = (S, A, P, R, p 0 , γ, T ) be a finite-horizon Markov Decision Process (MDP), where S and A are state and action spaces, P : S × A × S → R + is a state-transition probability function or probabilistic system dynamics, R : S × A → R a reward function, p 0 : S → R + an initial state distribution, γ a reward discount factor, and T a fixed horizon. Let τ = (s 0 , a 0 , . . . , s T , a T ) be a trajectory of states and actions and R(τ ) = T t=0 γ t R(s t , a t ) the trajectory reward. The goal of reinforcement learning methods is to find parameters θ of a policy π θ (a|s) that maximize the expected discounted reward over trajectories induced by the policy:
E π θ [R(τ )]
where s 0 ∼ p 0 , s t+1 ∼ P (s t+1 |s t , a t ) and a t ∼ π θ (a t |s t ).</p>
<p>In our work, the system dynamics are either induced by a simulation engine or real world. As the simulation engine itself is deterministic, a reparameterization trick [34] can be applied to introduce probabilistic dynamics. In particular, we define a distribution of simulation parameters ξ ∼ p φ (ξ) parameterized by φ. The resulting probabilistic system dynamics of the simulation engine are P ξ∼p φ = P (s t+1 |s t , a t , ξ).</p>
<p>As it was shown in [6,7,9], it is possible to design a distribution of simulation parameters p φ (ξ), such that a policy trained on P ξ∼p φ would perform well on a realworld dynamics distribution. This approach is also known as domain randomization and the policy training maximizes the expected reward under the dynamics induced by the distribution of simulation parameters p φ (ξ):
max θ E P ξ∼p φ <a href="1">E π θ [R(τ )]</a>
Domain randomization requires a significant expertise and tedious manual fine-tuning to design the simulation param- After training a policy on current distribution, we sample the policy both in the real world and for a range of parameters in simulation. The discrepancy between the simulated and real observations is used to update the simulation parameter distribution in SimOpt. eter distribution p φ (ξ). Furthermore, as we show in our experiments, it is often disadvantageous to use overly wide distributions of simulation parameters as they can include scenarios with infeasible solutions that hinder successful policy learning, or lead to exceedingly conservative policies. Instead, in the next section, we present a way to automate the learning of p φ (ξ) that makes it possible to shape a suitable randomization without the need to train on very wide distributions.</p>
<p>B. Learning simulation randomization</p>
<p>The goal of our framework is to find a distribution of simulation parameters that brings observations or partial observations induced by the policy trained under this distribution closer to the observations of the real world. Let π θ,p φ be a policy trained under the simulated dynamics distribution P ξ∼p φ as in the objective (1), and let D(τ ob ξ , τ ob real ) be a measure of discrepancy between real world observation trajectories τ ob real = (o 0,real . . . , o T,real ) and simulated observation trajectories τ ob ξ = (o 0,ξ . . . , o T,ξ ) sampled using policy π θ,p φ and the dynamics distribution P ξ∼p φ . It should be noted that the inputs of the policy π θ,p φ and observations used to compute D(τ ob ξ , τ ob real ) are not required to be the same. The goal of optimizing the simulation parameter distribution is to minimize the following objective:
min φ E P ξ∼p φ E π θ,p φ D(τ ob ξ , τ ob real )(2)
This optimization would entail training and real robot evaluation of the policy π θ,p φ for each φ. This would require a large amount of RL iterations and more critically real robot trials. Hence, we develop an iterative approach to approximate the optimization by training a policy π θ,p φ i on the simulation parameter distribution from the previous iteration and using it 
π θ,p φ i ← RL(env) 6: τ ob real ∼ RealRollout(π θ,p φ i ) 7: ξ ∼ Sample(p φi ) 8: τ ob ξ ∼ SimRollout(π θ,p φ i , ξ) 9: c(ξ) ← D(τ ob ξ , τ ob real ) 10: p φi+1 ← UpdateDistribution(p φi , ξ, c(ξ), )
for both, sampling the real world observations and optimizing the new simulation parameter distribution p φi+1 :
min φi+1 E P ξ i+1 ∼p φ i+1 E π θ,p φ i D(τ ob ξi+1 , τ ob real )(3)s.t. D KL p φi+1 p φi ≤ ,
where we introduce a KL-divergence step between the old simulation parameter distribution p φi and the updated distribution p φi+1 to avoid going out of the trust region of the policy π θ,p φ i trained on the old simulation parameter distribution. Fig. 3 shows the general structure of our algorithm that we call SimOpt.</p>
<p>C. Implementation</p>
<p>Here we describe particular implementation choices for the components of our framework used in this work. However, it should be noted that each of the components is replaceable. Algorithm 1 describes the order of running all the components in our implementation.</p>
<p>The RL training is performed on a GPU based simulator using a parallelized version of proximal policy optimization (PPO) [35] on a multi-GPU cluster [36]. We parameterize our simulation parameter distribution as a Gaussian, i.e. p φ (ξ) ∼ N (µ, Σ) with φ = (µ, Σ). We choose weighted 1 and 2 norms between simulation and real world observations for our observation discrepancy function D:
D(τ ob ξ , τ ob real ) = (4) w 1 T i=0 |W (o i,ξ − o i,real )| + w 2 T i=0 W (o i,ξ − o i,real ) 2 2 ,
where w 1 and w 2 are the weights of the 1 and 2 norms, and W are the importance weights for each observation dimension. We additionally apply a Gaussian filter to the distance computation to account for misalignments of the trajectories.</p>
<p>As we use a non-differentiable simulator we employ a sampling-based gradient-free algorithm based on relative entropy policy search [37] for optimizing the objective (3), which is able to perform updates of p φ with an upper bound on the KL-divergence step. By doing so, the simulator can be treated as a black-box, as in this case p φ can be optimized directly by only using samples ξ ∼ p φ and the corresponding costs c(ξ) coming from D(τ ob ξ , τ ob real ).</p>
<p>Sampling of simulation parameters and the corresponding policy roll-outs is highly parallelizable, which we use in our experiments to evaluate large amounts of simulation parameter samples. As noted above, single components of our framework can be exchanged. In case of availability of a differentiable simulator, the objective (3) can be defined as a loss function for optimizing with gradient descent. Furthermore, for cases where 1 and 2 norms are not applicable, we can employ other forms of discrepancy functions, e.g. to account for potential domain shifts between observations [15,38,39]. Alternatively, real world and simulation data can be additionally used to train D(τ ob ξ , τ ob real ) to discriminate between the observations by minimizing the prediction loss of classifying observations as simulated or real, similar to the discriminator training in the generative adversarial framework [40,41,42]. Finally, a higher-dimensional generative model p φ (ξ) can be employed to provide a multi-modal randomization of the simulated environments.</p>
<p>IV. EXPERIMENTS</p>
<p>In our experiments we aim at answering the following questions: (1) How does our method compare to pure domain randomization? (2) How learning a simulation parameter distribution compares to training on a very wide parameter distribution? (3) How many SimOpt iterations and real world trials are required for a successful transfer of robotic manipulation policies? (4) Does our method work for different real world tasks and robots?</p>
<p>We start by performing an ablation study in simulation by transferring policies between scenes with different initial state distributions, such as different poses of the cabinet in the drawer opening task. We demonstrate that updating the distribution of the simulation parameters leads to a successful policy transfer in contrast to just using an initial distribution of the parameters without any updates. As we observe, training on very wide parameter distributions is significantly more difficult and prone to fail than updating a conservative initial distribution.</p>
<p>Next, we show that we can successfully transfer policies to real robots, such as ABB Yumi and Franka Panda, for complex articulated tasks such as cabinet drawer opening, and tasks with non-rigid bodies and complex dynamics, such as swing-peg-in-hole task with the peg swinging on a soft rope. The policies can be transferred with a very small amount of real robot trials and leveraging large-scale training on a multi-GPU cluster.</p>
<p>A. Tasks</p>
<p>We evaluate our approach on two robot manipulation tasks: cabinet drawer opening and swing-peg-in-hole.</p>
<p>1) Swing-peg-in-hole: The goal of this task is to put a peg attached to a robot hand on a rope into a hole placed at a 45 degrees angle. Manipulating a soft rope leads to a swinging motion of the peg, which makes the dynamics of the task more challenging. The task set up in the simulation and real world using a 7-DoF Yumi robot from ABB is depicted in Fig. 4. An example of a wide distribution of simulation parameters in the swing-peg-in-hole task where it is not possible to find a solution for many of the task instances. 2) Drawer opening: In the drawer opening task, the robot has to open a drawer of a cabinet by grasping and pulling it with its fingers. This task involves an ability to handle contact dynamics when grasping the drawer handle. For this task, we use a 7-DoF Panda arm from Franka Emika. Simulated and real world settings are shown in Fig. 1 on the left. This task is operated on a 10D observation space: 7D robot joint angles and 3D position of the cabinet drawer handle. The reward function consists of the distance penalty between the handle and end-effector positions, the angle alignment of the end-effector and the drawer handle, opening distance of the drawer and indicator function ensuring that both robot fingers are on the handle.</p>
<p>We would like to emphasize that our method does not require the full state information of the real world, e.g. we do not need to estimate the rope diameter, rope compliance etc. to update the simulation parameter distribution in the swingpeg-in-hole task. The output of our policies consists of 7 joint velocity commands and an additional gripper command for the drawer opening task.</p>
<p>B. Simulation engine</p>
<p>We use NVIDIA Flex as a high-fidelity GPU based physics simulator that uses maximal coordinate representation to simulate rigid body dynamics. Flex allows a highly parallel implementation and can simulate multiple instances of the scene on a single GPU. We use the multi-GPU based RL infrastructure developed in [36] to leverage the highly parallel nature of the simulator.</p>
<p>C. Simulated experiments</p>
<p>We aim at understanding what effect a wide simulation parameter distribution can have on learning robust policies, and how we can improve the learning performance and the  transferability of the policies using our method to adjust simulation randomization. Fig. 4 shows an example of a significantly wide distribution of simulation parameters for the swing-peg-in-hole task. In this case, peg size, rope properties and size of the peg box were randomized. As we can observe, a large part of the randomized instances does not have a feasible solution, i.e. when the peg is too large for the hole or the rope is too short. Finding a suitably wide parameter distribution would require manual fine-tuning of the randomization parameters.</p>
<p>Moreover, learning performance depends strongly on the variance of the parameter distribution. We investigate this in a simulated cabinet drawer opening task with a Franka arm which is placed in front of a cabinet. We randomize the position of the cabinet along the lateral direction (Xcoordinate) while keeping all other simulation parameters constant. We train our policies on a 2 layer neural network with fully connected layers of 64 units each with PPO for 200 iterations. As we increase the variance of the cabinet position, we observe that the policies learned tend to be conservative i.e. they do end up reaching the handle of the drawer but fail to open it. This is shown in Fig. 5 where we plot the reward as a function of number of iterations used to train the RL policy. We start with a standard deviation of 2cm (σ 2 = 7e − 4) and increase it to 10cm (σ 2 = 0.01). As shown in the plot, the policy is sensitive to the choice of this parameter and only manages to open the drawer when the standard deviation is 2cm. We note that the reward difference may not seem that significant but realize that it is dominated by reaching reward. Increasing variance further, in an attempt to cover a wider operating range, can often lead to simulating unrealistic scenarios and catastrophic breakdown of the physics simulation with various joints of the robot reaching their limits. We also observed that the policy is extremely sensitive to variance in all three axes of the cabinet position i.e. policy only ever converges when the standard deviation is 2cm and fails to learn even reaching the handle otherwise.</p>
<p>In our next set of experiments, we perform policy transfer from the source to target drawer opening scene where position of the cabinet in the target scene is offset by a distance of 15cm and 22cm. After training the policy with RL, it is run on the target scene to collect roll-outs. These roll-outs are then used to perform several SimOpt iterations to optimize simulation parameters that best explain the current roll-outs. We noticed that the RL training can be sped up by initializing the policy with the weights from the previous SimOpt iteration, effectively reducing the number of needed PPO iterations from 200 to 10 after the first SimOpt iteration. The whole process is repeated until the learned policy starts to successfully open the drawer in the target scene. We found that it took overall 3 iterations of doing RL and SimOpt to learn to open the drawer when the cabinet was offset by 15cm. Such a large distance of 15cm would have required the standard deviation of the cabinet position to be 10cm for any naïve domain randomization based training which fails to produce a policy that opens the drawer as shown in Fig. 5. Our method leverages roll-outs from the target scene and changes the distribution of cabinet position such that the training on this new distribution allows opening the drawer. We further note that the number of iterations increases to 5 as we increase the target cabinet distance to 22cm highlighting that our method is able to operate on a wider range of mismatch between the current scene and the target scene. Fig. 6 shows how the source distribution variance adapts to the target distribution variance for this experiment and Fig. 7 shows that our method starts with a conservative guess of the initial distribution of the parameters and changes it using target scene roll-outs until policy behaviour in target and source scene starts to match.</p>
<p>D. Real robot experiments</p>
<p>In our real robot experiments, SimOpt is used to learn simulation parameter distribution of the manipulated objects and the robot. We run our experiments on 7-DoF Franka Panda and ABB Yumi robots. The RL training and SimOpt simulation parameter sampling is performed using a cluster of 64 GPUs for running the simulator with 150 simulated agents per GPU. In the real world, we use object tracking Fig. 7. Policy performance in the target drawer opening environment trained on randomized simulation parameters at different iterations of SimOpt. As the source environment distribution gets adjusted, the policy transfer improves until the robot can successfully solve the task in the fourth SimOpt iteration. Fig. 8. Running policies trained in simulation at different iterations of SimOpt for real world swing-peg-in-hole and drawer opening tasks. Left: SimOpt adjusts physical parameter distribution of the soft rope, peg and the robot, which results in a successful execution of the task on a real robot after two SimOpt iterations. Right: SimOpt adjusts physical parameter distribution of the robot and the drawer. Before updating the parameters, the robot pushes too much on the drawer handle with one of its fingers, which leads to opening the gripper. After one SimOpt iteration, the robot can better control its gripper orientation, which leads to an accurate task execution.   with DART [43] to continuously track the 3D positions of the peg in the swing-peg-in-hole task and the handle of the cabinet drawer in the drawer opening task, as well as initialize positions of the peg box and the cabinet in simulation. DART operates on depth images and requires 3D articulated models of the objects. We learn multi-variate Gaussian distributions of the simulation parameters parameterized by a mean and a full covariance matrix, and perform several updates of the simulation parameter distribution per SimOpt iteration using the same real world roll-outs to minimize the number of real world trials.
µ init diag(Σ init ) µ f inal
1) Swing-peg-in-hole: Fig. 8 (left) demonstrates the behavior of the real robot execution of the policy trained in simulation over 3 iterations of SimOpt. At each iteration, we perform 100 iterations of RL in approximately 7 minutes and 3 roll-outs on the real robot using the currently trained policy to collect real-world observations. Then, we run 3 update  steps of the simulation parameter distribution with 9600 simulation samples per update. In the beginning, the robot misses the hole due to the discrepancy of the simulation parameters and the real world. After a single SimOpt iteration, the robot is able to get much closer to the hole, however not being able to insert the peg as it requires a slight angle to go into the hole, which is non-trivial to achieve using a soft rope. Finally, after two SimOpt iterations, the policy trained on a resulting simulation parameter distribution is able to swing the peg into the hole in 90% of the times when evaluated on 20 trials. Table I shows the initial mean and the diagonal of the covariance matrix of the swing-peg-in-hole simulation parameters, and the shifted mean at the end of the SimOpt training. We observe that the most significant changes occur in the physical parameters of the rope that influence its dynamical behavior and the robot parameters, especially scaling of the policy actions.
µ init diag(Σ init ) µ f inal
2) Drawer opening: For drawer opening, we learn a Gaussian distribution of simulation parameters initialized with a mean and a diagonal covariance matrix described in Table II. Fig. 8 (right) shows the drawer opening behavior before and after performing a SimOpt update. During each SimOpt iteration, we run 200 iterations of RL for approximately 22 minutes, perform 3 real robot roll-outs and 20 update steps of the simulation distribution using 9600 samples per update step. Before updating the parameter distribution, the robot is able to reach the handle and start opening the drawer. However, it cannot exactly replicate the learned behavior from simulation and does not keep the gripper orthogonal to the drawer, which results in pushing too much on the handle from the bottom with one of the robot fingers. As the finger gripping force is limited, the fingers begin to open due to a larger pushing force. After adjusting the parameter distribution including robot and drawer properties, as shown in Table II, the robot is able to better control its gripper orientation and by evaluating on 20 trials can open the drawer at all times keeping the gripper orthogonal to the handle.</p>
<p>E. Comparison to trajectory-based parameter learning</p>
<p>In our work, we run a closed-loop policy in simulation to obtain simulated roll-outs for SimOpt optimization. Alternatively, we could directly set the simulator to states and execute actions from the real world trajectories as proposed in [29,30]. However, such a setting is not always possible as we might not be able to observe all required variables for setting the internal state of the simulator at each time point, e.g. the current bending configuration of the rope in the swing-peg-in-hole task, which we are able to initialize but can not continually track with our real world set up.</p>
<p>Without being able to set the simulator to the real world states continuously, we still can try to copy the real world actions and execute them in an open-loop manner in simulation. However, in our simulated experiments we notice that especially when making particular state dimensions unobservable for SimOpt cost computation, such as X-position of the cabinet in the drawer opening task, executing a closedloop policy still leads to meaningful simulation parameter updates compared to the open-loop execution. We believe in this case the robot behavior is still dependent on the particular simulated scenario due to the closed-loop nature of the policy, which also reflects in the joint trajectories of the robot that are still included in the SimOpt cost function. This means that by using a closed-loop policy we can still update the simulation parameter distribution even without explicitly including some of the relevant observations in the SimOpt cost computation.</p>
<p>V. CONCLUSIONS</p>
<p>Closing the simulation to reality transfer loop is an important component for a robust transfer of robotic policies.</p>
<p>In this work, we demonstrated that adapting simulation randomization using real world data can help in learning simulation parameter distributions that are particularly suited for a successful policy transfer without the need for exact replication of the real world environment. In contrast to trying to learn policies using very wide distributions of simulation parameters, which can simulate infeasible scenarios, we are able to start with distributions that can be efficiently learned with reinforcement learning, and modify them for a better transfer to the real scenario. Our framework does not require full state of the real environment and reward functions are only needed in simulation. We showed that updating simulation distributions is possible using partial observations of the real world while the full state still can be used for the reward computation in simulation. We evaluated our approach on two real world robotic tasks and showed that policies can be transferred with only a few iterations of simulation updates using a small number of real robot trials.</p>
<p>In this work, we applied our method to learning uni-modal simulation parameter distributions. We plan to extend our framework to multi-modal distributions and more complex generative simulation models in future work. Furthermore, we plan to incorporate higher-dimensional sensor modalities, such as vision and touch, for both policy observations and factors of simulation randomization. Tables III and IV show the SimOpt distribution update parameters for swing-peg-in-hole and drawer opening tasks including REPS [37] parameters, settings of the discrepancy function D(τ ob ξ , τ ob real ), weights of each observation dimension in the discrepancy function, and reinforcement learning settings such as parallelized PPO [35,36] training parameters and task reward weights.  </p>
<p>VI. APPENDIX</p>
<p>Fig. 3 .
3The pipeline for optimizing the simulation parameter distribution.</p>
<p>Fig. 1
1on the right. Our observation space consists of 7-DoF arm joint configurations and 3D position of the peg. The reward function for the RL training in simulation includes the distance of the peg from the hole, angle alignment with the hole and a binary reward for solving the task.</p>
<p>Fig. 5 .
5Performance of the policy training with domain randomization for different variances of the distribution of the cabinet position along the X-axis in the drawer opening task.</p>
<p>Fig. 6 .
6Initial distribution of the cabinet position in the source environment, located at extreme left, slowly starts to change to the target environment distribution as a function of running 5 iterations of SimOpt.</p>
<p>scaling (7D) [0.5 . . . 0.</p>
<p>action scaling (7D) [0.26 . . . 0.26] 0.01 [0.19 . . .</p>
<p>Algorithm 1 SimOpt framework 1: p φ0 ← Initial simulation parameter distribution 2: ← KL-divergence step for updating p φ 3: for iteration i ∈ {0, . . . , N } do4: </p>
<p>env ← Simulation(p φi ) </p>
<p>5: </p>
<p>TABLE I
ISWING-PEG-IN-HOLE: SIMULATION PARAMETER DISTRIBUTION.</p>
<p>TABLE II DRAWER
IIOPENING: SIMULATION PARAMETER DISTRIBUTION.</p>
<p>TABLE III SWING
III-PEG-IN-HOLE: SIMOPT PARAMETERS. distribution update parameters Number of REPS updates per SimOpt iteration 20 Number of simulation parameter samples per update 9600 L2-distance between end-effector and drawer handle -0.5 Angular alignment of end-effector with drawer handle -0.07TABLE IV DRAWER OPENING: SIMOPT PARAMETERS.Simulation Timesteps per simulation parameter sample 
453 </p>
<p>KL-threshold 
1.0 </p>
<p>Minimum temperature of sample weights 
0.001 
Discrepancy function parameters </p>
<p>L1-cost weight 
0.5 </p>
<p>L2-cost weight 
1.0 </p>
<p>Gaussian smoothing standard deviation (timesteps) 
5 </p>
<p>Gaussian smoothing truncation (timesteps) 
4 
Observation dimensions cost weights </p>
<p>Joint angles (7D) 
0.5 </p>
<p>Drawer position (3D) 
1.0 
PPO parameters </p>
<p>Number of agents 
400 </p>
<p>Episode length 
150 </p>
<p>Timesteps per batch 
151 </p>
<p>Clip parameter 
0.2 
γ 
0.99 
λ 
0.95 </p>
<p>Entropy coefficient 
0.0 </p>
<p>Optimization epochs 
5 </p>
<p>Optimization batch size per agent 
8 </p>
<p>Optimization step size 
5e-4 </p>
<p>Desired KL-step 
0.01 
RL reward weights </p>
<p>Opening distance of the drawer 
-0.4 </p>
<p>Keeping fingers around the drawer handle bonus 
0.005 </p>
<p>Action penalty 
-0.005 </p>
<p>ACKNOWLEDGEMENTSWe would like to thank Alexander Lambert, Balakumar Sundaralingam and Giovanni Sutanto for their help with the robot experiments, and David Ha, James Davidson, Lerrel Pinto and Fabio Ramos for their helpful feedback on the draft of the paper. We would also like to thank the GPU cluster and infrastucture team at NVIDIA for their help all the way through this project.
Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection. S Levine, P Pastor, A Krizhevsky, J Ibarz, D Quillen, I. J. Robotics Res. 374-5S. Levine, P. Pastor, A. Krizhevsky, J. Ibarz, and D. Quillen. Learning hand-eye coordination for robotic grasping with deep learning and large-scale data col- lection. I. J. Robotics Res., 37(4-5):421-436, 2018.</p>
<p>Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. L Pinto, A Gupta, ICRA. L. Pinto and A. Gupta. Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours. In ICRA, 2016.</p>
<p>Collective robot reinforcement learning with distributed asynchronous guided policy search. A Yahya, A Li, M Kalakrishnan, Y Chebotar, S Levine, IROS. A. Yahya, A. Li, M. Kalakrishnan, Y. Chebotar, and S. Levine. Collective robot reinforcement learning with distributed asynchronous guided policy search. In IROS, 2017.</p>
<p>Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, S Levine, CoRR, abs/1806.10293D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Her- zog, E. Jang, D. Quillen, E. Holly, M. Kalakrishnan, V. Vanhoucke, and S. Levine. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manip- ulation. CoRR, abs/1806.10293, 2018.</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. N Jakobi, P Husbands, I Harvey, European Conference on Artificial Life. SpringerN. Jakobi, P. Husbands, and I. Harvey. Noise and the reality gap: The use of simulation in evolutionary robotics. In European Conference on Artificial Life. Springer, 1995.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, IROS. J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In IROS, 2017.</p>
<p>Cad2rl: Real single-image flight without a single real image. F Sadeghi, S Levine, RSS. F. Sadeghi and S. Levine. Cad2rl: Real single-image flight without a single real image. RSS, 2017.</p>
<p>Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task. S James, A J Davison, E Johns, abs/1707.02267CoRRS. James, A. J. Davison, and E. Johns. Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task. CoRR, abs/1707.02267, 2017.</p>
<p>Learning dexterous in-hand manipulation. M Andrychowicz, B Baker, M Chociej, R Jozefowicz, B Mcgrew, J Pachocki, A Petron, M Plappert, G Powell, A Ray, J Schneider, S Sidor, J Tobin, P Welinder, L Weng, W Zaremba, abs/1808.00177CoRRM. Andrychowicz, B. Baker, M. Chociej, R. Jozefow- icz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba. Learning dex- terous in-hand manipulation. CoRR, abs/1808.00177, 2018.</p>
<p>System identification -theory for the user. L Ljung, Prentice HallL. Ljung. System identification -theory for the user. Prentice Hall, 1999.</p>
<p>A survey on policy search for robotics. Foundations and Trends in Robotics. M P Deisenroth, G Neumann, J Peters, M. P. Deisenroth, G. Neumann, and J. Peters. A survey on policy search for robotics. Foundations and Trends in Robotics, pages 388-403, 2013.</p>
<p>Pilco: A modelbased and data-efficient approach to policy search. M P Deisenroth, C E Rasmussen, ICML. M. P. Deisenroth and C. E. Rasmussen. Pilco: A model- based and data-efficient approach to policy search. In ICML, 2011.</p>
<p>Learning to control a low-cost manipulator using dataefficient reinforcement learning. M P Deisenroth, C E Rasmussen, D Fox, RSS. M. P. Deisenroth, C. E. Rasmussen, and D. Fox. Learning to control a low-cost manipulator using data- efficient reinforcement learning. In RSS, 2011.</p>
<p>Physically consistent state estimation and system identification for contacts. S Kolev, E Todorov, Humanoids. S. Kolev and E. Todorov. Physically consistent state estimation and system identification for contacts. In Humanoids, 2015.</p>
<p>Towards adapting deep visuomotor representations from simulated to real environments. E Tzeng, C Devin, J Hoffman, C Finn, X Peng, S Levine, K Saenko, T Darrell, abs/1511.07111CoRRE. Tzeng, C. Devin, J. Hoffman, C. Finn, X. Peng, S. Levine, K. Saenko, and T. Darrell. Towards adapting deep visuomotor representations from simulated to real environments. CoRR, abs/1511.07111, 2015.</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. K Bousmalis, A Irpan, P Wohlhart, Y Bai, M Kelcey, M Kalakrishnan, L Downs, J Ibarz, P Pastor, K Konolige, S Levine, V Vanhoucke, abs/1709.07857CoRRK. Bousmalis, A. Irpan, P. Wohlhart, Y. Bai, M. Kel- cey, M. Kalakrishnan, L. Downs, J. Ibarz, P. Pastor, K. Konolige, S. Levine, and V. Vanhoucke. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. CoRR, abs/1709.07857, 2017.</p>
<p>Sim-to-real robot learning from pixels with progressive nets. A A Rusu, M Vecerik, T Rothrl, N Heess, R Pascanu, R Hadsell, CoRLA. A. Rusu, M. Vecerik, T. Rothrl, N. Heess, R. Pas- canu, and R. Hadsell. Sim-to-real robot learning from pixels with progressive nets. In CoRL, 2017.</p>
<p>Transfer from simulation to real world through learning deep inverse dynamics model. P F Christiano, Z Shah, I Mordatch, J Schneider, T Blackwell, J Tobin, P Abbeel, W Zaremba, abs/1610.03518CoRRP. F. Christiano, Z. Shah, I. Mordatch, J. Schneider, T. Blackwell, J. Tobin, P. Abbeel, and W. Zaremba. Transfer from simulation to real world through learning deep inverse dynamics model. CoRR, abs/1610.03518, 2016.</p>
<p>Crossing the reality gap in evolutionary robotics by promoting transferable controllers. S Koos, J.-B Mouret, S Doncieux, GECCO. ACM. S. Koos, J.-B. Mouret, and S. Doncieux. Crossing the reality gap in evolutionary robotics by promoting transferable controllers. In GECCO. ACM, 2010.</p>
<p>Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids. I Mordatch, K Lowrey, E Todorov, IROS. I. Mordatch, K. Lowrey, and E. Todorov. Ensemble- cio: Full-body dynamic motion planning that transfers to physical humanoids. In IROS, 2015.</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, In ICRA. X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic control with dynamics randomization. In ICRA, 2018.</p>
<p>Preparing for the unknown: Learning a universal policy with online system identification. W Yu, J Tan, C K Liu, G Turk, RSS. W. Yu, J. Tan, C. K. Liu, and G. Turk. Preparing for the unknown: Learning a universal policy with online system identification. In RSS, 2017.</p>
<p>Domain randomization for simulation-based policy optimization with transferability assessment. F Muratore, F Treede, M Gienger, J Peters, CoRLF. Muratore, F. Treede, M. Gienger, and J. Peters. Domain randomization for simulation-based policy op- timization with transferability assessment. In CoRL, 2018.</p>
<p>Mutual alignment transfer learning. M Wulfmeier, I Posner, P Abbeel, abs/1707.07907CoRRM. Wulfmeier, I. Posner, and P. Abbeel. Mutual alignment transfer learning. CoRR, abs/1707.07907, 2017.</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, RSS. J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. In RSS, 2018.</p>
<p>Reinforcement learning for non-prehensile manipulation: Transfer from simulation to physical system. K Lowrey, S Kolev, J Dao, A Rajeswaran, E Todorov, SIMPAR. K. Lowrey, S. Kolev, J. Dao, A. Rajeswaran, and E. Todorov. Reinforcement learning for non-prehensile manipulation: Transfer from simulation to physical sys- tem. In SIMPAR, 2018.</p>
<p>Reinforcement learning for pivoting task. R Antonova, S Cruciani, C Smith, D Kragic, abs/1703.00472CoRRR. Antonova, S. Cruciani, C. Smith, and D. Kragic. Reinforcement learning for pivoting task. CoRR, abs/1703.00472, 2017.</p>
<p>Asymmetric actor critic for image-based robot learning. L Pinto, M Andrychowicz, P Welinder, W Zaremba, P Abbeel, abs/1710.06542CoRRL. Pinto, M. Andrychowicz, P. Welinder, W. Zaremba, and P. Abbeel. Asymmetric actor critic for image-based robot learning. CoRR, abs/1710.06542, 2017.</p>
<p>Simulationbased design of dynamic controllers for humanoid balancing. J Tan, Z Xie, B Boots, C K Liu, IROS. J. Tan, Z. Xie, B. Boots, and C. K. Liu. Simulation- based design of dynamic controllers for humanoid balancing. In IROS, 2016.</p>
<p>Fast model identification via physics engines for dataefficient policy search. S Zhu, A Kimmel, K E Bekris, A Boularias, IJCAI. ijcai.org. S. Zhu, A. Kimmel, K. E. Bekris, and A. Boularias. Fast model identification via physics engines for data- efficient policy search. In IJCAI. ijcai.org, 2018.</p>
<p>Humanoid robots learning to walk faster: From the real world to simulation and back. A Farchy, S Barrett, P Macalpine, P Stone, AAMAS. A. Farchy, S. Barrett, P. MacAlpine, and P. Stone. Humanoid robots learning to walk faster: From the real world to simulation and back. In AAMAS, 2013.</p>
<p>Grounded action transformation for robot learning in simulation. J Hanna, P Stone, AAAI. J. Hanna and P. Stone. Grounded action transformation for robot learning in simulation. In AAAI, 2017.</p>
<p>Epopt: Learning robust neural network policies using model ensembles. A Rajeswaran, S Ghotra, S Levine, B Ravindran, abs/1610.01283CoRRA. Rajeswaran, S. Ghotra, S. Levine, and B. Ravindran. Epopt: Learning robust neural network policies using model ensembles. CoRR, abs/1610.01283, 2016.</p>
<p>Auto-encoding variational Bayes. CoRR, abs/1312. D P Kingma, M Welling, 6114D. P. Kingma and M. Welling. Auto-encoding varia- tional Bayes. CoRR, abs/1312.6114, 2013.</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, abs/1707.06347CoRRJ. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.</p>
<p>Gpu-accelerated robotic simulation for distributed reinforcement learning. J Liang, V Makoviychuk, A Handa, N Chentanez, M Macklin, D Fox, CoRL. J. Liang, V. Makoviychuk, A. Handa, N. Chentanez, M. Macklin, and D. Fox. Gpu-accelerated robotic sim- ulation for distributed reinforcement learning. CoRL, 2018.</p>
<p>Relative entropy policy search. J Peters, K Mlling, Y Altun, AAAI. J. Peters, K. Mlling, and Y. Altun. Relative entropy policy search. In AAAI, 2010.</p>
<p>Simultaneous deep transfer across domains and tasks. E Tzeng, J Hoffman, T Darrell, K Saenko, ICCV. E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultaneous deep transfer across domains and tasks. In ICCV, 2015.</p>
<p>Time-contrastive networks: Self-supervised learning from video. P Sermanet, C Lynch, Y Chebotar, J Hsu, E Jang, S Schaal, S Levine, In ICRA. P. Sermanet, C. Lynch, Y. Chebotar, J. Hsu, E. Jang, S. Schaal, and S. Levine. Time-contrastive networks: Self-supervised learning from video. In ICRA, 2018.</p>
<p>I J Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A C Courville, Y Bengio, Generative adversarial nets. NIPSI. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and Y. Ben- gio. Generative adversarial nets. In NIPS, 2014.</p>
<p>Generative adversarial imitation learning. J Ho, S Ermon, NIPS. J. Ho and S. Ermon. Generative adversarial imitation learning. In NIPS, 2016.</p>
<p>Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets. K Hausman, Y Chebotar, S Schaal, G S Sukhatme, J J Lim, NIPS. K. Hausman, Y. Chebotar, S. Schaal, G. S. Sukhatme, and J. J. Lim. Multi-modal imitation learning from un- structured demonstrations using generative adversarial nets. In NIPS, 2017.</p>
<p>Dart: Dense articulated real-time tracking. T Schmidt, R A Newcombe, D Fox, RSS. T. Schmidt, R. A. Newcombe, and D. Fox. Dart: Dense articulated real-time tracking. In RSS, 2014.</p>            </div>
        </div>

    </div>
</body>
</html>
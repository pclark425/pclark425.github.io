<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6213 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6213</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6213</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-7abb0536a8faa0f7b40d64d4b1f8a7ee33d9e8fe</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7abb0536a8faa0f7b40d64d4b1f8a7ee33d9e8fe" target="_blank">Evaluating Open-QA Evaluation</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> A new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA, and the pitfalls of current methods and methods to improve LLM-based evaluators are discussed.</p>
                <p><strong>Paper Abstract:</strong> This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this area. All resources are available at \url{https://github.com/wangcunxiang/QA-Eval} and it is under the Apache-2.0 License.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6213.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6213.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 evaluator vs Human</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (text-davinci-003) used as an LLM-as-a-judge compared to human evaluation on EVOUNA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper uses GPT-3.5 (text-davinci-003) as an automated evaluator of Open-QA answers (QA-Eval) on the EVOUNA dataset and compares its judgments to human annotations, reporting quantitative gaps, qualitative failure modes, and prompt-based mitigation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Open Question Answering (Open-QA) evaluation / QA-Eval (EVOUNA)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-3.5 (text-davinci-003)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human annotation performed by the authors: dedicated annotators judged subsets (one annotator per generator-group for convenience), with cross-checking and inter-annotator agreement measured on 500 samples per subset. Cohen's Kappa scores reported per subset (all > 0.86, many > 0.9). Guidelines were provided and edge cases (time-sensitive questions, malformed gold answers) were filtered per stated rules.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Accuracy, Macro-F1, precision, recall, confusion-matrix proportions (TP/TN/FP/FN), and relative ranking of QA systems (model ranks assigned by each evaluator compared to human ranks); inter-annotator agreement (Cohen's Kappa) for humans.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Quantitative: GPT-3.5 evaluator attains high but consistently lower performance than human annotators (examples: on EVOUNA-NQ subsets GPT-3.5 accuracy ranges ~67–94% depending on subset vs 'Another Human' accuracy ~96–100%; see Table 5). GPT-3.5 has lower Macro-F1 than humans and differs in relative ranking of QA systems (none of the automatic evaluators, including GPT-3.5, reproduced human relative ranks across the five QA models). Qualitative: GPT-3.5 is more sensitive to long, verbose answers and to extraneous formatting/source references (e.g., BingChat outputs), producing more errors on those subsets; it rarely produces many false positives but makes systematic mistakes (paraphrase sensitivity, over-reliance on its internal knowledge), while humans show near-perfect consistency and better contextual judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Reported limitations include sensitivity to prompt phrasing; susceptibility to extraneous context/formatting (BingChat-style source citations and appended questions); difficulty with long answers containing extra information (verbosity hurts judgments); paraphrase/paraphrasing errors (mistaking paraphrased correct answers as incorrect or vice versa); literal-interpretation errors; overgeneralization (treating near-correct or related-but-incorrect answers as correct); and 'unknowable reasons' where the evaluator gives judgments inconsistent with both gold answers and human judgment, possibly due to internal knowledge conflicts.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Concrete failures reported: (1) A case where the gold answer 'Prafulla Chandra Ghosh' and a generated (incorrect) answer 'Dr. Bidhan Chandra Roy' were judged by GPT-3.5 as correct—GPT-3.5 apparently used its internal/world knowledge or ignored the gold answer. (2) Systematic poor performance on the BingChat subset attributed to extra formatting and sources in BingChat outputs; excluding BingChat substantially improves GPT-3.5 performance. (3) GPT-3.5 and other automatic evaluators disagree with humans on the relative ranking of QA systems (e.g., humans rank BingChat high on NQ while GPT-3.5 ranks it low). The paper also documents common error categories for LLM-evaluators: Paraphrasing Error, Literal Interpretation Error, Overgeneralization Error, Misleading Emphasis Error, and Unknowable Reasons Error.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>The paper tests several mitigation approaches via prompt engineering and preprocessing: (a) Preprocessing BingChat outputs (removing source citations and trailing 'Do you want to know more?' prompts) to reduce noise. (b) Prompt strategies with GPT-3.5: 'Ignore background information' (often degraded performance on long-answer subsets), 'Give reasons' (asking for rationale decreased performance overall), 'Chain-of-Thoughts' prompting (improved performance on many long-answer subsets—notably NQ-ChatGPT35, NQ-ChatGPT4, NQ-BingChat—but degraded on NQ-FiD), and 'In-Context Learning' with representative examples selected by cross-domain clustering (improved performance on some subsets). The paper emphasizes that these strategies have dataset-dependent effects and no universal fix was found; careful prompt selection and preprocessing (especially normalization of sources/formatting) can reduce some failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Open-QA Evaluation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Is chatgpt a general-purpose natural language processing task solver? <em>(Rating: 2)</em></li>
                <li>GPTScore: Evaluate as you desire <em>(Rating: 2)</em></li>
                <li>Automatic chain of thought prompting in large language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6213",
    "paper_id": "paper-7abb0536a8faa0f7b40d64d4b1f8a7ee33d9e8fe",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 evaluator vs Human",
            "name_full": "GPT-3.5 (text-davinci-003) used as an LLM-as-a-judge compared to human evaluation on EVOUNA",
            "brief_description": "This paper uses GPT-3.5 (text-davinci-003) as an automated evaluator of Open-QA answers (QA-Eval) on the EVOUNA dataset and compares its judgments to human annotations, reporting quantitative gaps, qualitative failure modes, and prompt-based mitigation experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Open Question Answering (Open-QA) evaluation / QA-Eval (EVOUNA)",
            "llm_judge_model": "GPT-3.5 (text-davinci-003)",
            "human_evaluation_setup": "Human annotation performed by the authors: dedicated annotators judged subsets (one annotator per generator-group for convenience), with cross-checking and inter-annotator agreement measured on 500 samples per subset. Cohen's Kappa scores reported per subset (all &gt; 0.86, many &gt; 0.9). Guidelines were provided and edge cases (time-sensitive questions, malformed gold answers) were filtered per stated rules.",
            "metrics_compared": "Accuracy, Macro-F1, precision, recall, confusion-matrix proportions (TP/TN/FP/FN), and relative ranking of QA systems (model ranks assigned by each evaluator compared to human ranks); inter-annotator agreement (Cohen's Kappa) for humans.",
            "reported_differences": "Quantitative: GPT-3.5 evaluator attains high but consistently lower performance than human annotators (examples: on EVOUNA-NQ subsets GPT-3.5 accuracy ranges ~67–94% depending on subset vs 'Another Human' accuracy ~96–100%; see Table 5). GPT-3.5 has lower Macro-F1 than humans and differs in relative ranking of QA systems (none of the automatic evaluators, including GPT-3.5, reproduced human relative ranks across the five QA models). Qualitative: GPT-3.5 is more sensitive to long, verbose answers and to extraneous formatting/source references (e.g., BingChat outputs), producing more errors on those subsets; it rarely produces many false positives but makes systematic mistakes (paraphrase sensitivity, over-reliance on its internal knowledge), while humans show near-perfect consistency and better contextual judgment.",
            "llm_specific_limitations": "Reported limitations include sensitivity to prompt phrasing; susceptibility to extraneous context/formatting (BingChat-style source citations and appended questions); difficulty with long answers containing extra information (verbosity hurts judgments); paraphrase/paraphrasing errors (mistaking paraphrased correct answers as incorrect or vice versa); literal-interpretation errors; overgeneralization (treating near-correct or related-but-incorrect answers as correct); and 'unknowable reasons' where the evaluator gives judgments inconsistent with both gold answers and human judgment, possibly due to internal knowledge conflicts.",
            "notable_failure_cases": "Concrete failures reported: (1) A case where the gold answer 'Prafulla Chandra Ghosh' and a generated (incorrect) answer 'Dr. Bidhan Chandra Roy' were judged by GPT-3.5 as correct—GPT-3.5 apparently used its internal/world knowledge or ignored the gold answer. (2) Systematic poor performance on the BingChat subset attributed to extra formatting and sources in BingChat outputs; excluding BingChat substantially improves GPT-3.5 performance. (3) GPT-3.5 and other automatic evaluators disagree with humans on the relative ranking of QA systems (e.g., humans rank BingChat high on NQ while GPT-3.5 ranks it low). The paper also documents common error categories for LLM-evaluators: Paraphrasing Error, Literal Interpretation Error, Overgeneralization Error, Misleading Emphasis Error, and Unknowable Reasons Error.",
            "mitigation_strategies": "The paper tests several mitigation approaches via prompt engineering and preprocessing: (a) Preprocessing BingChat outputs (removing source citations and trailing 'Do you want to know more?' prompts) to reduce noise. (b) Prompt strategies with GPT-3.5: 'Ignore background information' (often degraded performance on long-answer subsets), 'Give reasons' (asking for rationale decreased performance overall), 'Chain-of-Thoughts' prompting (improved performance on many long-answer subsets—notably NQ-ChatGPT35, NQ-ChatGPT4, NQ-BingChat—but degraded on NQ-FiD), and 'In-Context Learning' with representative examples selected by cross-domain clustering (improved performance on some subsets). The paper emphasizes that these strategies have dataset-dependent effects and no universal fix was found; careful prompt selection and preprocessing (especially normalization of sources/formatting) can reduce some failure modes.",
            "uuid": "e6213.0",
            "source_info": {
                "paper_title": "Evaluating Open-QA Evaluation",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Is chatgpt a general-purpose natural language processing task solver?",
            "rating": 2
        },
        {
            "paper_title": "GPTScore: Evaluate as you desire",
            "rating": 2
        },
        {
            "paper_title": "Automatic chain of thought prompting in large language models",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        }
    ],
    "cost": 0.011186749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating Open-QA Evaluation</h1>
<p>Cunxiang Wang ${ }^{1}$; Sirui Cheng ${ }^{2}$; Qipeng Guo ${ }^{3}$, Yuanhao Yue ${ }^{4}$, Bowen Ding ${ }^{1}$, Zhikun Xu ${ }^{4}$, Yidong Wang ${ }^{1}$, Xiangkun $\mathrm{Hu}^{5}$, Zheng Zhang ${ }^{3}$, and Yue Zhang ${ }^{1 \dagger}$<br>${ }^{1}$ School of Engineering, Westlake University, China<br>${ }^{2}$ Northeastern University, China; ${ }^{3}$ Amazon AWS AI; ${ }^{4}$ Fudan University, China<br>{wangcunxiang, zhangyue}@westlake.edu.cn</p>
<h4>Abstract</h4>
<p>This study focuses on the evaluation of the Open Question Answering (OpenQA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes humanannotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this area. All resources are available at https://github.com/wangcunxiang/QA-Eval and it is under the Apache-2.0 License.</p>
<h2>1 Introduction</h2>
<p>Open Question Answering (Open-QA) [Chen et al., 2017, Yang et al., 2019], refers to the generation of precise responses to broad, open-ended queries. While Open-QA is useful for downstream applications, such as digital assistant and customer support, it also serves as an essential measure of a model's competency in dealing with factuality [Zhu et al., 2021]. With the advent of Large Language Models (LLMs), such as ChatGPT [OpenAI, 2022a] and BARD [Google, 2023], significant strides have been made in tackling NLP tasks [Qin et al., 2023, Kung et al., 2022]. However, evidence has indicated that LLMs can generate hallucinations or other contents that contradict reality [Bang et al., 2023, Ji et al., 2022, Zheng et al., 2023, Wang et al., 2023a]. Consequently, ensuring factuality has become a prime concern, and Open-QA can be a valuable benchmark task for detecting hallucinations.</p>
<p>The Open-QA task is traditionally evaluated via the Exact Match (EM) score [Izacard and Grave, 2021, Lewis et al., 2020, Chen et al., 2017], which uses whether the model output and one of golden answers are an exact character match to determine whether it is correct. Hence, EM has certain limitations as it does not adequately account for the variation in expression of the answers. For instance, 'Lionel Messi' can also be expressed as 'Lionel Andrés Messi', 'Messi', or 'Leo Messi', none of which would produce an exact match. Moreover, the EM score is inapplicable for detailed responses from LLMs such as ChatGPT [OpenAI, 2022a] and GPT-3.5 [OpenAI, 2022b], where formulating a gold standard answer could excessively restrict model performance.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To understand the influence of EM mismatch, we manually evaluate the output of five Open-QA models including the classic Dense Passage Retriever (DPR) [Karpukhin et al., 2020] + Fusion-inDecoder (FiD) [Izacard and Grave, 2021], LLMs including GPT-3.5 [OpenAI, 2022b] and ChatGPT [OpenAI, 2022a], as well as the retrieval-assisted LLM BingChat [Microsoft, 2023] on the standard benchmarks Natural Questions (NQ) [Kwiatkowski et al., 2019] and TriviaQA (TQ) [Joshi et al., 2017] [3], following Karpukhin et al. [2020] and Izacard and Grave [2021]. We focus on aligning model responses with gold standard answers and assessing their factual correctness. Our results show that exact match underestimates model performances by a significant margin. In particular, on the NQ dataset, the DPR + FiD model gives a result of 59.2 by exact match, but 68.9 by human evaluation. In addition, the models’ relative performance ranks change according to human evaluation. These show the importance of finding a correct evaluation metric for Open-QA.</p>
<p>Our human evaluation records can serve as a benchmark for investigating what evaluation methods are best for evaluating the performance of various models on Open-QA. As a result, we consider the novel task of Evaluating Open-QA Evaluation (QA-Eval) by making our human evaluation results as the QA-Eval dataset EVOUNA (EValuating Open qUestion aNswering evAluation). The main idea is to calculate the correlation between evaluator results on the models on the dataset and the human annotation. If a model has a higher correlation with human annotation, then it can be regarded as being more reliable, and vice versa.</p>
<p>Using EVOUNA, we examine several commonly used automatic evaluation metrics, including Lexical Matching, Neural-Evaluation [Zhang* et al., 2020], and LLMs [OpenAI, 2022b]. The last two methods have been widely used in evaluating NLG tasks [Sellam et al., 2020, Zhao et al., 2020, Qin et al., 2023] but not yet for Open-QA. Results on EVOUNA show that, firstly, although these methods are somewhat effective, they still fall short compared to human evaluators, and none of them ranks the five QA models’ outputs in the same relative ranks as humans. In addition, the LLM-evaluators tends to perform much worse on long answers with much additional information.</p>
<p>In our study, we assess the strengths and limitations of different automatic evaluators, identifying shared and unique error types for each. We’ve manually categorized these errors, revealing that evaluators often exhibit over-strictness for varied reasons. Our findings provide key insights for optimizing Open-QA evaluation using these evaluators.</p>
<p>To our knowledge, we are the first in manually assessing the correctness of answers generated by models on Open-QA and create a benchmark to evaluate evaluators on Open-QA. All resources of our benchmark EVOUNA is released at https://github.com/wangcunxiang/QA-Eval.</p>
<h1>2 Related Work</h1>
<p>Large Language Models (LLMs) , such as the GPT-series [Radford et al., 2018, 2019, Brown et al., 2020], including GPT-3.5 [OpenAI, 2022b], ChatGPT-3.5 [OpenAI, 2022a], and Bing Chat [Microsoft, 2023], have been at the forefront of recent research due to their capability to generate coherent and contextually accurate textual output. GPT-3.5, an evolution of OpenAI’s GPT-3 [Brown et al., 2020], houses an immense 175 billion parameters, enhancing its capabilities in understanding and navigating complex linguistic structures. ChatGPT, a conversational variant of GPT-3.5, is specifically tailored for dialog-based applications [OpenAI, 2022a]. Functioning as an interactive AI chatbot, it provides contextually aware and fluent responses to human inputs. Bing Chat, a development by Microsoft, integrates the language comprehension of ChatGPT and an extensive retrieval system to serve as a conversational search engine [Microsoft, 2023].In this study, we utilize GPT-3.5, ChatGPT, and Bing Chat for Open-QA and GPT-3.5 for QA-Eval.</p>
<p>Open-QA Datasets: Natural Questions (NQ) [Kwiatkowski et al., 2019] and TriviaQA (TQ) [Joshi et al., 2017] are widely used datasets in the Open-Domain Question Answering (Open-QA) community, offering unique advantages for training and testing models’ capabilities. The Natural Questions (NQ) dataset, is designed to reflect real-world information-seeking questions and their answers. TriviaQA consists of questions from trivia and quiz-league websites. We choose to use both NQ and TQ in our work for several reasons. Firstly, these datasets provide a diverse range of questions and answers [Kwiatkowski et al., 2019, Joshi et al., 2017]. This diversity helps us to better understand the strengths and weaknesses of different models and evaluation methods across various</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>question types. Furthermore, both NQ and TQ are well-known and commonly used in the research community [Petroni et al., 2021, Izacard and Grave, 2021, Ju et al., 2022, Wang et al., 2023b]. Last, they are both under the Apache-2.0 license.</p>
<p>Evaluation on LLM. Recently, numerous research have conducted on LLMs. For instance, some researchers conduct a systematic analysis of ChatGPT's zero-shot capabilities on representative NLP tasks, concluding that while ChatGPT excels in inferential tasks, it still struggles with specific tasks such as sequence labeling [Qin et al., 2023]. Some researchers carry out a comprehensive evaluation of ChatGPT's robustness from adversarial and out-of-distribution (OOD) perspectives with findings indicate that the model's absolute performance is far from ideal, suggesting that adversarial and OOD robustness remains a significant challenge for foundational models [Wang et al., 2023c]. In this paper, we assess the performance of GPT-3.5, ChatGPT, GPT-4, and Bing Chat by examining their response accuracy ans their ability of evaluating correct responses on the NQ and TQ datasets within the Open-QA task and QA-Evaluation.</p>
<h1>3 Open Question Answering (Open-QA)</h1>
<p>The Open-QA task mandates that for any given open-domain question $q$, the model $\mathcal{M}$ must generate a corresponding answer $\hat{a}$ while the golden answers denotes as $A$. For models utilizing a retriever-reader mechanism, a database $D$ consisting documents is also accessible for information retrieval.</p>
<h3>3.1 Data</h3>
<p>Following [Lewis et al., 2020, Izacard and Grave, 2021], we use the Natural Questions (NQ) [Kwiatkowski et al., 2019] and TriviaQA (TQ) [Joshi et al., 2017] for the Open-QA task. Specifically, we use the development set of NQ and a portion of the test set of TQ. There are 3610 and 2000 cases in the NQ split and TQ split, respectively.</p>
<p>Given that certain questions have answers that change over time, such as 'Who is the current US president?', and some question pairs have answers that are evidently incorrect, we filter out these instances from the dataset. The quantity of the remaining dataset is shown in Table 1.</p>
<p>It is noteworthy that some gold standard answers contain inaccuracies. Some may include factual errors, and we retain these answers and base our evaluations upon them. Conversely, when the errors are structure-related or format-related or other severe ones, we exclude these from our evaluation process. We list four examples with incorrect golden in Section Appendix C.3.1</p>
<p>Additionally, there are instances where the LLMs refuse to answer certain questions. We record these refusals as answers as well.</p>
<h3>3.2 Open-QA Models</h3>
<p>There are two methods currently being employed in this field.
The first approach involves using pretrained language models to generate answers, leveraging their internal knowledge learned from a vast amount of text data. These models generate answers directly, without the need for external databases or retrieval systems [Roberts et al., 2020, Wang et al., 2021, Ye et al., 2022].
The second approach comprises of a two-tiered system: a retriever and a reader [Izacard and Grave, 2021, Yu et al., 2022, Wang et al., 2023d]. The retriever module fetches pertinent information from a predefined database, while the reader uses this fetched data to generate an appropriate answer. Notable representatives of the retriever module include Dense Passage Retrieval (DPR) [Karpukhin et al., 2020] and BM25, and for the reader module include Fusion-in-Decoder (FiD) [Izacard and Grave, 2021, 2020, Raffel et al., 2020] and Retrieval-Augmented Generation (RAG) [Lewis et al., 2020]. In this study, we adopt both of these approaches, to increase the data amount and the variety of models, which have their own advantages</p>
<p>Retriever-Reader Models For this task, we adopt the DPR model [Karpukhin et al., 2020] as the retriever model and the FiD [Izacard and Grave, 2021] model as the reader model. We make a detailed introduction to the architectures of DPR and FiD in the Appendix Section D.1. For the DPR+FiD</p>
<p>Table 1: Statistics of data for Open-QA and QA-Eval. We annotate the results of Open-QA as the EVOUNA dataset for QA-Eval. In each cell, the right is the amount of original samples while the left the remaining amount after the filtration. We only annotate the remained data.</p>
<table>
<thead>
<tr>
<th></th>
<th>NaturalQuestions</th>
<th></th>
<th>TriviaQA</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>remained</td>
<td>original</td>
<td>remained</td>
<td>original</td>
</tr>
<tr>
<td>DPR + FiD</td>
<td>3020</td>
<td>3610</td>
<td>1938</td>
<td>2000</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>3020</td>
<td>3610</td>
<td>1938</td>
<td>2000</td>
</tr>
<tr>
<td>ChatGPT-3.5</td>
<td>3020</td>
<td>3610</td>
<td>1938</td>
<td>2000</td>
</tr>
<tr>
<td>GPT-4</td>
<td>3020</td>
<td>3610</td>
<td>1938</td>
<td>2000</td>
</tr>
<tr>
<td>Bing Chat</td>
<td>3020</td>
<td>3610</td>
<td>1938</td>
<td>2000</td>
</tr>
</tbody>
</table>
<p>model, we first use a publicly available DPR checkpoint to retrieve 100 passages for each question. Subsequently, we train a FiD model from a T5-large model [Raffel et al., 2020] with the retrieved 100 passages for each question using the FiD source code and the suggested hyper-parameters.</p>
<p>Large Language Models We directly use the question as a prompt, feeding it into the model to generate an answer $\hat{a}$ :</p>
<p>$$
\hat{a}=\mathcal{M}_{l l m}(q)
$$</p>
<p>Where $\mathcal{M}_{l l m}$ could be GPT-3.5 (text-davinci-003), ChatGPT-3.5 (gpt-3.5-turbo), ChatGPT-4, or Bing Chat. We obtain the answers using either the API or the webpage. For instance, for GPT-3.5 and ChatGPT-3.5, we utilize the OpenAI text-davinci-003 and gpt-3.5-turbo APIs, respectively, and we set the temperature to 0 to ensure consistent outputs. For ChatGPT-4 and Bing Chat, we use their respective webpages. The full Open-QA experiments for GPT-3.5 and ChatGPT-3.5 were conducted from April 15 to April 17, 2023. For ChatGPT-4 and Bing Chat, the Open-QA experiments were primarily conducted in April 2023 due to their daily access limit.</p>
<h1>3.3 Evaluation Methods</h1>
<p>We employ three representative methods for evaluating the correctness of Open-QA systems' responses: the lexical matching, the Neural-evaluation, and the large language model.</p>
<p>Lexical Matching We utilize the traditional lexical match method as it is popular in the Open-QA evaluation [Chen et al., 2017, Izacard and Grave, 2021, Lewis et al., 2020]. We follow [Izacard and Grave, 2021, Lewis et al., 2020] to use the Exact Match method for answers generated by the DPR+FiD model. If the generated answer $\hat{a}$ exactly matches one golden answer $a \in A$, we classify it as correct, otherwise as incorrect. Since LLM-generated answers are typically long, the Exact Match is not applicable, so if at least one golden answer $a \in A$ appears in the AI-generated answer $\hat{a}$, we classify it as correct, otherwise as incorrect.</p>
<p>Large Language Models Large Language Models (LLMs) have exhibited impressive linguistic capabilities, indicating significant ability in the assessment of QA results. Consequently, we adopt them into this task. We design a prompt filled with a question $q$, an AI-generated answer $\hat{a}$, and a list of golden answers $A$, and then feed it into the LLM to obtain the prediction $\hat{y}$ :</p>
<p>$$
\hat{y}=\mathcal{M}_{l l m}(\text { prompt })
$$</p>
<p>where prompt $=$ "Here is a question, a set of golden answers (split with /), an AI-generated answer. Can you judge whether the AI-generated answer is correct according to the question and golden answers, simply answer Yes or No." + 'Question: '+q+'; ' + 'Golden Answers: ' + A+'; '+ AIgenerated answer: '+à+'; '+A:". In this work, we utilize GPT-3.5 (text-davinci-003) as $\mathcal{M}_{l l m}$.
We obtain the judgement using the OpenAI text-davinci-003 API for GPT-3.5, and we set the temperature to 0 to ensure consistent outputs. The experiments is conducted on April 24, 2023.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2: Inter-annotator agreement for different subsets of EVOUNA. The left is for NaturalQuestions (NQ) subsets while the right is for TriviaQA (TQ) subsets.</p>
<table>
<thead>
<tr>
<th>NQ subsets</th>
<th>Cohen’s Kappa score</th>
<th>TQ subsets</th>
<th>Cohen’s Kappa score</th>
</tr>
</thead>
<tbody>
<tr>
<td>NQ-Fid</td>
<td>91.1</td>
<td>TQ-FiD</td>
<td>100</td>
</tr>
<tr>
<td>NQ-GPT35</td>
<td>92.4</td>
<td>TQ-GPT35</td>
<td>98.4</td>
</tr>
<tr>
<td>NQ-ChatGPT35</td>
<td>90.4</td>
<td>TQ-ChatGPT35</td>
<td>96.6</td>
</tr>
<tr>
<td>NQ-ChatGPT4</td>
<td>88.8</td>
<td>TQ-ChatGPT4</td>
<td>99.1</td>
</tr>
<tr>
<td>NQ-BingChat</td>
<td>86.4</td>
<td>TQ-BingChat</td>
<td>99.2</td>
</tr>
</tbody>
</table>
<p>Table 3: An illustrative example from the EVOUNA dataset. This example includes responses from FiD, GPT-3.5, ChatGPT-3.5, ChatGPT-4 and BingChat models (We omit some details due to space constraints). The question in focus is "where do the greasers live in the outsiders?", with the correct answer being "Tulsa, Oklahoma". The table presents the responses generated by different models and the corresponding human judgments on the accuracy of these responses.</p>
<table>
<thead>
<tr>
<th>Models</th>
<th>Generated Answer</th>
<th>Human Judgement</th>
</tr>
</thead>
<tbody>
<tr>
<td>FiD</td>
<td>Tulsa, Oklahoma</td>
<td>correct</td>
</tr>
<tr>
<td>GPT35</td>
<td>The Greasers live in a poor part of town called the East Side. They live in rundown houses, abandoned buildings, and alleys.</td>
<td>incorrect</td>
</tr>
<tr>
<td>ChatGPT35</td>
<td>The greasers live in the East Side of town in The Outsiders...</td>
<td>incorrect</td>
</tr>
<tr>
<td>ChatGPT4</td>
<td>In the novel "The Outsiders" by S.E. Hinton...</td>
<td>correct</td>
</tr>
<tr>
<td>BingChat</td>
<td>According to , The Outsiders takes place in Tulsa, Oklahoma in the 1960s. The greasers live on the poorer East Side of town...</td>
<td>correct</td>
</tr>
</tbody>
</table>
<p>Neural Evaluation Methods play a pivotal role in gauging the efficacy of NLG tasks. One such model deployed is BERT-score [Zhang<em> et al., 2020]. It has been applied for evaluating several machine generation tasks, including machine translation [Zhang</em> et al., 2020], dialogue [Sellam et al., 2020] and summarization [Zhao et al., 2020]. In the context of QA-Eval, we use BERT-score as our neural-evaluation mechanism. We have also considered BART-Score [Yuan et al., 2021] and GPT-Score [Fu et al., 2023], but they are inapplicable since they provide a continuous score that measures the similarity between the generated text and the reference text. It doesn’t explicitly differentiate between correct and incorrect answers in a binary fashion.</p>
<p>BERT-Score evaluates the similarity between two text sequences, typically between a reference and a hypothesis. We take the reference as the concatenation of a question (q) and the golden answer (A), and the hypothesis is the concatenation of the same question (q) and the AI-generated answer (à). The BERT-score is computed using contextualized word embeddings from a pre-trained BERT model. We introduce the detailed description of the BERT-score algorithm as Sec D.2 For the BERT-Score approach, we set the threshold $\tau$ at 0.5, considering it as the most natural choice.</p>
<h2>4 The EVOUNA Dataset</h2>
<p>The section gives detailed information about the EVOUNA dataset for the QA-Eval task. The EVOUNA dataset is constructed from the results of different Open-QA models, including FiD, GPT-3.5, ChatGPT-3.5/4 and BingChat, on Natural Questions (NQ) and TriviaQA (TQ) with their human annotations. This dataset consists of various components that are divided based on the original dataset and the generator model used, and they are well-detailed in Table 1.</p>
<p>Formally, in the QA-Eval task, an evaluating model $\mathcal{M}$ is presented with an open-domain question $q$, an AI-generated answer $\hat{a}$, and a collection of gold standard answers $A$. The task asks the model to evaluate the correctness of the AI-produced answer in relation to the gold standard responses. The prediction $\hat{y}$ should be either positive (indicating correctness) or negative (indicating incorrectness). The task of QA-Eval in this context is seen as a binary classification task, and the performance of evaluators is quantified using two metrics: accuracy and F1 score. Table 3 presents a representative example of NQ subsets of the EVOUNA dataset. This example illustrates the process where different</p>
<p>Table 4: Human evaluated accuracy and Lexical Matching score of AI-models on NaturalQuestions (NQ) and TriviaQA (TQ). In each cell, the left is the human evaluated accuracy while the right the the lexical match score.</p>
<table>
<thead>
<tr>
<th></th>
<th>NaturalQuestions</th>
<th></th>
<th></th>
<th>TriviaQA</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>DPR + FiD</td>
<td>68.9</td>
<td>59.2</td>
<td>81.5</td>
<td>73.5</td>
<td></td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>65.5</td>
<td>50.7</td>
<td>78.4</td>
<td>71.0</td>
<td></td>
</tr>
<tr>
<td>ChatGPT-3.5</td>
<td>73.0</td>
<td>57.9</td>
<td>84.45</td>
<td>76.7</td>
<td></td>
</tr>
<tr>
<td>ChatGPT-4</td>
<td>78.8</td>
<td>61.8</td>
<td>90.2</td>
<td>82.1</td>
<td></td>
</tr>
<tr>
<td>Bing Chat</td>
<td>79.9</td>
<td>65.4</td>
<td>89.6</td>
<td>81.6</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 5: Performance of Eval-Models on the EVOUNA. In each cell, the left is the accuracy while the right is the Macro-F1.</p>
<table>
<thead>
<tr>
<th></th>
<th>NQ-FiD</th>
<th>NQ-GPT35</th>
<th>NQ-ChatGPT35</th>
<th>NQ-ChatGPT4</th>
<th>NQ-BingChat</th>
</tr>
</thead>
<tbody>
<tr>
<td>Lexical Matching</td>
<td>89.7/92.0</td>
<td>84.8/86.9</td>
<td>80.3/84.9</td>
<td>82.5/87.6</td>
<td>82.3/87.8</td>
</tr>
<tr>
<td>BERT-Score</td>
<td>75.1/83.5</td>
<td>69.5/77.6</td>
<td>72.8/81.2</td>
<td>76.0/84.3</td>
<td>67.5/77.6</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>93.6/95.3</td>
<td>84.1/87.2</td>
<td>82.2/86.9</td>
<td>80.9/86.9</td>
<td>69.5/77.2</td>
</tr>
<tr>
<td>Another Human</td>
<td>96.3/97.4</td>
<td>96.8/97.8</td>
<td>95.6/96.5</td>
<td>96.6/97.9</td>
<td>95.5/97.2</td>
</tr>
<tr>
<td>on EVOUNA-NaturalQuestions</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>TQ-FiD</td>
<td>TQ-GPT35</td>
<td>TQ-ChatGPT35</td>
<td>TQ-ChatGPT4</td>
<td>TQ-BingChat</td>
</tr>
<tr>
<td>Lexical Matching</td>
<td>91.8/94.7</td>
<td>92.3/94.8</td>
<td>92.3/95.2</td>
<td>91.1/94.8</td>
<td>89.8/94.1</td>
</tr>
<tr>
<td>BERT-Score</td>
<td>65.5/75.1</td>
<td>75.7/84.1</td>
<td>80.8/88.4</td>
<td>93.5/90.5</td>
<td>80.4/88.3</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>95.7/97.3</td>
<td>91.2/94.2</td>
<td>92.5/95.5</td>
<td>92.4/95.7</td>
<td>80.9/88.2</td>
</tr>
<tr>
<td>Another Human</td>
<td>100/100</td>
<td>99.4/99.6</td>
<td>98.8/99.2</td>
<td>99.8/99.2</td>
<td>99.8/99.9</td>
</tr>
<tr>
<td>on EVOUNA-TriviaQA</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>models provide answers to the same question, and then a human judge determines the accuracy of each generated answer.</p>
<p>In addition, we normalize answers produced by BingChat. Firstly, we remove their special symbols and referential sources to avoid any potential influence on the performance of evaluating models. Secondly, some Bing Chat answers contain an extra question at the end, such as 'Do you want to know more about xx?', which might induce the evaluating models to answer the question instead of providing judgments. Hence, we also remove these end questions. An example of processing is shown in Section C. 1</p>
<p>Human Annotation The human annotation process for our study was conducted by the authors themselves, eliminating the need for external paid services. The process included the careful removal of inappropriate questions and thorough evaluation of the models' generated responses' correctness. To ensure consistency and precision in the annotation process, we established detailed guidelines, a portion of which can be found in Appendix C.2.</p>
<p>Additionally, we evaluate the inter-annotator agreement on 500 samples from each subset of EVOUNA. The Cohen's Kappa scores [Cohen, 1960] representing inter-annotator agreement for these evaluations are presented in Table 2. With all scores above 80, this indicates strong alignment and agreement among our annotations.</p>
<h1>5 Experiments</h1>
<h3>5.1 Human Evaluation of Open-QA</h3>
<p>The Open-QA results are displayed in Table 4. It's observed that results of the commonly-used lexical match metric on each model's outputs do not align with the accuracy assessed by human evaluators. Moreover, the relative ranking between models also diverge significantly between the lexical match and humans, implying that lexical match metric is not suitable for evaluating Open-QA results, especially those generated by LLMs.</p>
<p>Table 6: Evaluation scores assigned by various evaluation models to different QA models on EVOUNA. In each cell, the left represents the score given by the evaluator (row) to the QA model's performance (column) on the respective dataset, while the value in parentheses is the relative ranks among the five models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">NQ-FiD</th>
<th style="text-align: center;">NQ-GPT35</th>
<th style="text-align: center;">NQ-ChatGPT35</th>
<th style="text-align: center;">NQ-GPT4</th>
<th style="text-align: center;">NQ-BingChat</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical Matching</td>
<td style="text-align: center;">59.2 (3)</td>
<td style="text-align: center;">50.7 (5)</td>
<td style="text-align: center;">57.9 (4)</td>
<td style="text-align: center;">61.8 (2)</td>
<td style="text-align: center;">65.4 (1)</td>
</tr>
<tr>
<td style="text-align: center;">BERT-Score</td>
<td style="text-align: center;">82.5 (1)</td>
<td style="text-align: center;">70.9 (4)</td>
<td style="text-align: center;">71.9 (3)</td>
<td style="text-align: center;">74.5 (2)</td>
<td style="text-align: center;">64.8 (5)</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">67.3 (1)</td>
<td style="text-align: center;">58.8 (4)</td>
<td style="text-align: center;">63.0 (3)</td>
<td style="text-align: center;">66.2 (2)</td>
<td style="text-align: center;">54.1 (5)</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">69.7 (4)</td>
<td style="text-align: center;">70.3 (3)</td>
<td style="text-align: center;">63.0 (5)</td>
<td style="text-align: center;">80.3 (1)</td>
<td style="text-align: center;">78.2 (2)</td>
</tr>
<tr>
<td style="text-align: center;">on EVOUNA-NaturalQuestions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TQ-FiD</td>
<td style="text-align: center;">TQ-GPT35</td>
<td style="text-align: center;">TQ-ChatGPT35</td>
<td style="text-align: center;">TQ-GPT4</td>
<td style="text-align: center;">TQ-BingChat</td>
</tr>
<tr>
<td style="text-align: center;">Lexical Matching</td>
<td style="text-align: center;">73.5 (4)</td>
<td style="text-align: center;">71.0 (5)</td>
<td style="text-align: center;">76.7 (3)</td>
<td style="text-align: center;">82.1 (1)</td>
<td style="text-align: center;">81.6 (2)</td>
</tr>
<tr>
<td style="text-align: center;">BERT-Score</td>
<td style="text-align: center;">56.8 (5)</td>
<td style="text-align: center;">73.9 (4)</td>
<td style="text-align: center;">82.1 (2)</td>
<td style="text-align: center;">84.4 (2)</td>
<td style="text-align: center;">78.0 (3)</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">79.6 (3)</td>
<td style="text-align: center;">72.2 (5)</td>
<td style="text-align: center;">81.0 (2)</td>
<td style="text-align: center;">85.8 (1)</td>
<td style="text-align: center;">72.8 (4)</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">74.0 (4)</td>
<td style="text-align: center;">72.6 (5)</td>
<td style="text-align: center;">76.7 (3)</td>
<td style="text-align: center;">86.8 (1)</td>
<td style="text-align: center;">85.2 (2)</td>
</tr>
<tr>
<td style="text-align: center;">on EVOUNA-TriviaQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Furthermore, ChatGPT-4 and BingChat models show superior performance compared to the other three models on both the Natural Questions (NQ) and TriviaQA (TQ) datasets. However, even these top-performing models, ChatGPT-4 and BingChat, do not achieve perfect accuracy, with scores approximately $80 \%$ on NQ and $86 \%$ on TQ. This indicates that certain questions continue to present challenges. DPR+FiD, GPT-3.5 and ChatGPT-3.5 demonstrate comparable performance on both datasets. Additional analysis of Open-QA results can be found in Appendix Section E.1.</p>
<h1>5.2 Evaluating QA Evaluators on EVOUNA</h1>
<p>Table 5 shows the evaluation performance of different models, namely Lexical Matching, BERTScore, GPT-3.5, and Another Human (used as a reference), on different subsets of the EVOUNA datasets. These subsets are identified by the generator model used to create them, including NQ-FiD, NQ-GPT35, NQ-ChatGPT35, NQ-ChatGPT4, NQ-BingChat, and their TQ equivalents. Besides, we also present the precision and recall performance in the Appendix Table 8.
A few key observations can be made from the data presented in the table 5:
BERT-Score Analysis: The performance of the BERT-Score model is generally lower compared to other models, more noticeably on the TQ datasets. This could imply that the BERT-Score methodology, which utilizes pre-trained language models for embedding comparisons, might struggle to adequately capture the intricate details and sophistication of answer quality, specifically when the AI-generated answers provide more information than the gold standard.</p>
<p>GPT-3.5 Performance: The performance of the GPT-3.5 model is reasonably good across all datasets. However, there is notable variation in its performance depending on the dataset, which underscores the impact of the specific dataset on the evaluation capability of this model.</p>
<p>Human Evaluation: As we mentioned in Section 4, we also conduct an inter-annotator agreement analysis. This secondary evaluation yielded an accuracy and F1 score exceeding $95 \%$, demonstrating superior consistency over all employed AI methods. This result is in line with expectations, as human evaluators, with their innate understanding and assessment capabilities, tend to outperform AI models in accurately gauging the quality of answers. Thus, human evaluators provide an essential benchmark against which the performance of these AI models can be measured.</p>
<p>Assigned Scores: The table 6 displays the evaluation scores that different evaluators, including lexical match, BERT-Score, GPT-3.5, and human (as a reference), give to various QA-models across both NQ and TQ subsets of EVOUNA. The scores illuminate the relative effectiveness of different QA models as evaluated by different evaluators. It's noteworthy that the relative ranks given by evaluators to different QA models vary, indicating evaluators are still not capable to judge the relative level of different models on Open-QA. For example, for NQ, human evaluators rank NQ-BingChat</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Distribution of outcomes for three evaluators on EVOUNA-NQ. Each pie chart aggregates results across EVOUNA-NQ subsets, showing proportions of TP, TN, FP, and FN for each evaluator.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Distribution of error types for the three evaluators on the EVOUNA-NQ dataset, based on manual classification. Each pie chart segment represents the proportion of a specific error type for that evaluator.
second highest while GPT-3.5 ranks it lowest. These discrepancies show no evaluator ranks different QA model outputs on NQ/TQ in the same relative ranks as humans, revealing the complexity and nuance in open-domain QA model evaluation.</p>
<p>In summary, these findings highlight the challenges and complexities involved in evaluating answer quality in open-domain question answering systems. They also underscore the need for further research to enhance the performance of evaluation models, bringing them closer to human-level evaluation capabilities.</p>
<h1>6 Analysis</h1>
<h3>6.1 Distributions of QA-Eval Results</h3>
<p>We explore the evaluation results of various evaluators, including Lexical Matching, Neural Evaluation (BERT-Score), and LLM-as-evaluator (GPT-3.5) and illustrate the results (gathering all subsets) of three evaluators on the EVOUNA-NQ dataset using pie charts in Figure 1. Notably, both Lexical Matching and GPT-3.5 exhibit low proportions of False Positives, indicating they rarely misclassify incorrect answers as correct. In contrast, BERT-Score evenly distributes its errors between the two types. Lexical Matching maintains consistent performance across all EVOUNA-NQ subsets, while GPT-3.5 struggles specifically with the BingChat subset. This discrepancy is likely due to BingChat answers containing extraneous information and unique formatting, which may disrupt the LLM's performance. Lexical Matching remains largely unaffected by these factors. Excluding BingChat data significantly improves GPT-3.5's performance.</p>
<p>In the following sections, we delve deeper into the specific errors made by each evaluator and explore the underlying reasons.</p>
<h3>6.2 Error Analysis in QA-Eval</h3>
<p>We begin by delving into the inherent limitations of the three evaluator types, considering both their intrinsic mechanisms and the observed error cases. Due to space limit, a comprehensive discussion of these limitations is provided in Section E.4.1 of the Appendix.</p>
<p>Based on those limitations, we have designed a set of fine-grained Evaluator Error categories. This includes two common errors found across all evaluators, namely Paraphrasing Error and Synonym Error, as well as specific errors unique to each type of evaluator. In detail, Lexical Matching has Partial Match Error, Structure Variation Error and Overall Misleading Error; Neural Evaluation has Contextual Misunderstanding Error, Threshold Sensitivity and Extended Answer Error; LLM evaluators have Literal Interpretation Error, Literal Interpretation Error, Overgeneralization Error, Misleading Emphasis Error and Unknowable Reasons Error. The detailed definition and examples of Error Categories can be found in Section E.4.2 in the Appendix.</p>
<p>Based on the Error Categories, we have manually classified the errors produced by Lexical Matching, BERT-Score, and GPT-3.5 on each subset of our EVOUNA-NQ. For each subset, we selected 100 errors. We present a unified result (gathering all subsets) in Figure 2. The more detailed result can be found in Table 11 in the Appendix.</p>
<h1>6.3 Insights from QA-Eval Results</h1>
<p>Drawing from each evaluator's limitations and error classifications results, we offer these insights:
Lexical Matching: While lexical matching remains a simple and effective method for Open-QA evaluation, it struggles with issues of limited contextual understanding, low recall, and structural variations. It often marks answers that humans consider correct as incorrect, but rarely does the opposite. This makes Lexical Matching a strict metric, suitable for environments requiring high error recall. When it does mark a human-considered correct answer as wrong, it's usually because the generated answer contains the golden answer, but the overall meaning doesn't support it. For instance, it might negate the golden answer or only use it as part of the response. Lexical Matching struggles with "Structure Variation" errors. For example, if the golden answer is "8 September 2010" and the generated answer is "Amnesia: The Dark Descent was released on September 8, 2010.", Lexical Matching can't recognize it. The other two evaluators rarely have this issue. Due to its inability to handle semantics, it can't manage Synonym or Contextual understanding situations.</p>
<p>Neural Evaluation (BERT-Score and BLEURT in this work): Overall, they aren't well-suited for this QA-Eval task, with the poorest performance among the three types. They can only measure the similarity between two text segments. So, they handle Synonym errors well. However, if the generated answer contains extra information (common with larger models), this can easily influence the BERT-score. BERT-Score isn't great at Contextual Understanding. If the generated answer explains the golden answer without including its entities, BERT-Score can easily get it wrong. Adapting BERT-Score to this QA-Eval task by adjusting the threshold is another issue. Many datasets are highly sensitive to threshold settings.</p>
<p>LLM-evaluator (GPT-3.5 in this work): Overall, the LLM-evaluator can serve as a complement to lexical matching and is valuable for assessing the accuracy of generated answers, it remains sensitive to prompts and the impact of additional contexts, especially for BingChat answers. Its most common error is the "Paraphrasing error", possibly because it's easily influenced by other contexts. It has its own issues, like the "Overgeneralization error", which doesn't appear in the other two evaluators, although they are minor concerns. Sometimes, the LLM-evaluator makes clear mistakes that humans wouldn't. For example, for the question, "Who was the first chief minister of West Bengal?" with the golden answer being "Prafulla Chandra Ghosh", the generated answer was "The first Chief Minister of West Bengal was Dr. Bidhan Chandra Roy." GPT-3.5 marked the generated answer as correct, even though Dr. Bidhan Chandra Roy is not Prafulla Chandra Ghosh. This might be because the evaluator uses its inherent knowledge, overlooking the golden answer, or for other undetermined reasons. Such issues don't appear with the other two evaluators.</p>
<p>In summary, while lexical matching and LLM-evaluators are relatively more effective than neuralevaluations, they still underperform compared to human evaluators, often misjudging correct samples. Each evaluator has its own strengths and weaknesses.</p>
<h3>6.4 Enhancing QA-Eval through Prompt Engineering</h3>
<p>We also examine strategies to improve LLM' (specifically, GPT-3.5) performance in QA-Eval via prompt engineering. Four distinct methods were explored: Ignoring Background Information;</p>
<p>Table 7: GPT-3.5 evaluator performance with different prompt strategies on the EVOUNA-NQ set. Each cell displays accuracy (left) and F1 score (right).</p>
<table>
<thead>
<tr>
<th style="text-align: right;"></th>
<th style="text-align: right;">NQ-FiD</th>
<th style="text-align: right;">NQ-GPT35</th>
<th style="text-align: right;">NQ-ChatGPT35</th>
<th style="text-align: right;">NQ-ChatGPT4</th>
<th style="text-align: right;">NQ-BingChat</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: right;">Original</td>
<td style="text-align: right;">$93.6 / 95.3$</td>
<td style="text-align: right;">$84.1 / 87.2$</td>
<td style="text-align: right;">$82.2 / 86.9$</td>
<td style="text-align: right;">$80.9 / 86.9$</td>
<td style="text-align: right;">$69.5 / 77.2$</td>
</tr>
<tr>
<td style="text-align: right;">Ignoring Background</td>
<td style="text-align: right;">$93.9 / 95.5$</td>
<td style="text-align: right;">$82.9 / 86.0$</td>
<td style="text-align: right;">$80.8 / 85.5$</td>
<td style="text-align: right;">$79.6 / 85.7$</td>
<td style="text-align: right;">$65.7 / 73.4$</td>
</tr>
<tr>
<td style="text-align: right;">Giving Reasons</td>
<td style="text-align: right;">$89.6 / 91.9$</td>
<td style="text-align: right;">$76.2 / 78.4$</td>
<td style="text-align: right;">$73.3 / 78.2$</td>
<td style="text-align: right;">$64.3 / 71.2$</td>
<td style="text-align: right;">$55.6 / 62.2$</td>
</tr>
<tr>
<td style="text-align: right;">Chain-of-Thoughts</td>
<td style="text-align: right;">$90.2 / 92.9$</td>
<td style="text-align: right;">$84.0 / 88.0$</td>
<td style="text-align: right;">$\mathbf{8 4 . 5 / 8 9 . 4}$</td>
<td style="text-align: right;">$\mathbf{8 6 . 0 / 9 1 . 2}$</td>
<td style="text-align: right;">$\mathbf{8 0 . 4 / 8 7 . 1}$</td>
</tr>
<tr>
<td style="text-align: right;">In-Context-Learning</td>
<td style="text-align: right;">$93.1 / 94.9$</td>
<td style="text-align: right;">$\mathbf{8 4 . 5 / 8 8 . 3}$</td>
<td style="text-align: right;">$83.4 / 88.1$</td>
<td style="text-align: right;">$83.2 / 88.6$</td>
<td style="text-align: right;">$75.3 / 82.5$</td>
</tr>
</tbody>
</table>
<p>Providing Reasons for Judgments; Chain of Thoughts [Wei et al., 2022]; In-Context Learning [Dong et al., 2023].
Table 12 outlines the specific prompts used for each method with GPT-3.5 in QA-Eval. The prompts are designed to elicit different model behaviors or responses.
We adopt an approach from Auto-Cot [Zhang et al., 2023] using K-Means clustering [Hartigan and Wong, 1979] to select representative examples for in-context learning. To avoid data leakage, we employ cross-domain clustering; we cluster NQ sets for TQ experiments and vice versa. For example, we select representative examples from NQ-ChatGPT4 for experiments on TQ-ChatGPT4. Four representative examples are chosen for each dataset.
Table 7 presents the performance of GPT-3.5 evaluator with different prompts on the EVOUNA-NQ dataset. Here are the insights: Directing GPT-3.5 to ignore the background information degrades performance on four datasets with long answers (NQ-GPT35/ChatGPT35/ChatGPT4/BingChat). Requiring the model to reason its judgments negatively impacts performance across all datasets. The effects of Chain-of-Thoughts and In-Context-Learning vary. For instance, both methods significantly improve performance on four datasets with long answers, but Chain-of-Thoughts shows a substantial decline on the NQ-FiD. This variability suggests that the influence of these techniques depends on the data distribution.</p>
<h1>7 Limitations</h1>
<p>Our study comes with a few limitations. Firstly, our data, sourced via OpenAI's API or webpage, is subject to frequent model updates which precludes full reproducibility. Secondly, due to the constraints on OpenAI GPT-4's API, we could neither gather an ample amount of open-QA data based on GPT-4, nor utilize GPT-4 for our QA-Eval experiments. Finally, owing to resource limitations, both human and financial, we only managed to label the NQ dev set and a portion of the TQ test set, while the dev set and train set remain unlabeled.
As the gold standard answers in the Natural Questions and TriviaQA datasets occasionally contain inaccuracies, our dataset also carries the risk of inadvertently disseminating misinformation since we are not able to completely get rid of them.</p>
<h2>8 Conclusion</h2>
<p>In this study, we developed the EVOUNA dataset, crafted with the specific intention of evaluating Open-QA system outputs, with an emphasis on large language models. Our critical observation was the apparent deficiency in existing evaluators - from traditional lexical match metrics to neuralevaluation models and large language models - in providing reliable evaluations for these outputs. The EVOUNA dataset offers a robust tool for comprehensive scrutiny of Open-QA models. We've examined the strengths and weaknesses of each evaluator type within our QA-Eval task and have manually categorized the errors they produce on our dataset.</p>
<h2>Acknowledgement</h2>
<p>This publication has emanated from research conducted with the financial support of the Pioneer and "Leading Goose" R\&amp;D Program of Zhejiang under Grant Number 2022SDXHDX0003.</p>
<h1>References</h1>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to answer open-domain questions. In Association for Computational Linguistics (ACL), 2017.</p>
<p>Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. End-to-end open-domain question answering with BERTserini. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 72-77, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4013. URL https://aclanthology.org/N19-4013.</p>
<p>Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. Retrieving and reading: A comprehensive survey on open-domain question answering, 2021.</p>
<p>OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022a.
Google. Bard. bard.google.com, 2023.
Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476, 2023.</p>
<p>Tiffany H. Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, and Victor Tseng. Performance of chatgpt on usmle: Potential for ai-assisted medical education using large language models. PLOS Digital Health, 2, 2022.</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. ArXiv, abs/2302.04023, 2023.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Wenliang Dai, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55:1 - 38, 2022.</p>
<p>Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. Why does chatgpt fall short in answering questions faithfully? ArXiv, abs/2304.10513, 2023.</p>
<p>Zezhong Wang, Fan Yang, Pu Zhao, Lu Wang, Jue Zhang, Mohit Garg, Qingwei Lin, and Dongmei Zhang. Empower large language model to perform better on industrial domain-specific question answering. ArXiv, abs/2305.11541, 2023a.</p>
<p>Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.74. URL https://aclanthology.org/2021.eacl-main. 74.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.</p>
<p>OpenAI. Gpt-3.5 - openai. https://platform.openai.com/docs/models/gpt-3-5, 2022b.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://www.aclweb.org/anthology/2020. emnlp-main. 550 .</p>
<p>Microsoft. Bing chat. https://www.bing.com/new, 2023.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Vancouver, Canada, July 2017. Association for Computational Linguistics.</p>
<p>Tianyi Zhang<em>, Varsha Kishore</em>, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=SkeHuCVFDr.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.704. URL https://aclanthology.org/2020.acl-main. 704.</p>
<p>Jinming Zhao, Ming Liu, Longxiang Gao, Yuan Jin, Lan Du, He Zhao, He Zhang, and Gholamreza Haffari. Summpip: Unsupervised multi-document summarization with sentence graph compression. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1949-1952, 2020.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.</p>
<p>Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523-2544, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.200. URL https://aclanthology.org/2021.naacl-main. 200.</p>
<p>Mingxuan Ju, Wenhao Yu, Tong Zhao, Chuxu Zhang, and Yanfang Ye. Grape: Knowledge graph enhanced passage reader for open-domain question answering. In Findings of Empirical Methods in Natural Language Processing, 2022.</p>
<p>Cunxiang Wang, Zhikun Xu, Qipeng Guo, Xiangkun Hu, Xuefeng Bai, Zheng Zhang, and Yue Zhang. Exploiting abstract meaning representation for open-domain question answering, 2023b.</p>
<p>Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. arXiv preprint arXiv:2302.12095, 2023c.</p>
<p>Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL https://aclanthology.org/2020.emnlp-main. 437.</p>
<p>Cunxiang Wang, Pai Liu, and Yue Zhang. Can generative pre-trained language models serve as knowledge bases for closed-book QA? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3241-3251, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.251. URL https://aclanthology.org/2021.acl-long. 251.</p>
<p>Deming Ye, Yankai Lin, Peng Li, Maosong Sun, and Zhiyuan Liu. A simple but effective pluggable entity lookup table for pre-trained language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 523-529, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.57. URL https://aclanthology.org/2022.acl-short.57.</p>
<p>Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren, Yiming Yang, and Michael Zeng. KG-FiD: Infusing knowledge graph in fusion-in-decoder for open-domain question answering. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4961-4974, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.340. URL https://aclanthology.org/2022.acl-long. 340.</p>
<p>Cunxiang Wang, Haofei Yu, and Yue Zhang. Rfid: Towards rational fusion-in-decoder for opendomain question answering, 2023d.</p>
<p>Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering, 2020. URL https://arxiv.org/abs/2012.04584.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 27263-27277. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166, 2023.</p>
<p>Jacob Cohen. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37 - 46, 1960.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=<em>VjQ1MeSB</em> J.</p>
<p>Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. A survey on in-context learning, 2023.</p>
<p>Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In The Eleventh International Conference on Learning Representations (ICLR 2023), 2023.
J. A. Hartigan and M. A. Wong. A k-means clustering algorithm. JSTOR: Applied Statistics, 28(1): $100-108,1979$.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota,</p>
<p>June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //www.aclweb.org/anthology/N19-1423.</p>
<p>Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot Learning with Retrieval Augmented Language Models. 2022. URL http://arxiv.org/abs/2208.03299.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023.</p>
<p>Tatsunori B. Hashimoto, Hugh Zhang, and Percy Liang. Unifying human and statistical evaluation for natural language generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1689-1701, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1169. URL https://aclanthology.org/ N19-1169.</p>
<h1>A Appendix</h1>
<ol>
<li>Submission introducing new datasets must include the following in the supplementary materials:
(a) Dataset documentation and intended uses. Recommended documentation frameworks include datasheets for datasets, dataset nutrition labels, data statements for NLP, and accountability frameworks.
(b) URL to website/platform where the dataset/benchmark can be viewed and downloaded by the reviewers.
(c) Author statement that they bear all responsibility in case of violation of rights, etc., and confirmation of the data license.
(d) Hosting, licensing, and maintenance plan. The choice of hosting platform is yours, as long as you ensure access to the data (possibly through a curated interface) and will provide the necessary maintenance.</li>
<li>To ensure accessibility, the supplementary materials for datasets must include the following:
(a) Links to access the dataset and its metadata. This can be hidden upon submission if the dataset is not yet publicly available but must be added in the camera-ready version. In select cases, e.g when the data can only be released at a later date, this can be added afterward. Simulation environments should link to (open source) code repositories.
(b) The dataset itself should ideally use an open and widely used data format. Provide a detailed explanation on how the dataset can be read. For simulation environments, use existing frameworks or explain how they can be used.
(c) Long-term preservation: It must be clear that the dataset will be available for a long time, either by uploading to a data repository or by explaining how the authors themselves will ensure this.
(d) Explicit license: Authors must choose a license, ideally a CC license for datasets, or an open source license for code (e.g. RL environments).
(e) Add structured metadata to a dataset's meta-data page using Web standards (like schema.org and DCAT): This allows it to be discovered and organized by anyone. If you use an existing data repository, this is often done automatically.
(f) Highly recommended: a persistent dereferenceable identifier (e.g. a DOI minted by a data repository or a prefix on identifiers.org) for datasets, or a code repository (e.g. GitHub, GitLab,...) for code. If this is not possible or useful, please explain why.</li>
<li>For benchmarks, the supplementary materials must ensure that all results are easily reproducible. Where possible, use a reproducibility framework such as the ML reproducibility checklist, or otherwise guarantee that all results can be easily reproduced, i.e. all necessary datasets, code, and evaluation procedures must be accessible and documented.</li>
<li>For papers introducing best practices in creating or curating datasets and benchmarks, the above supplementary materials are not required.</li>
</ol>
<h2>B Data License and Maintenance Plan</h2>
<p>The EVOUNA data we create is open sourced at https://github.com/wangcunxiang/QA-Eval. The test data is under Apache License 2.0. We plan to collect more data from various datasets, including Natural Questions, Trivia and WebQuestions, and open source it for future research.</p>
<h2>C Data</h2>
<h2>C. 1 An example of Processing BingChat Answer</h2>
<p>Raw Answer:
The revolution period of Venus around the sun is ${ }^{<em> * 224.7}$ Earth days</em>* 1. Is there anything else you would like to know about Venus?</p>
<p>Source: Conversation with Bing, 2023/3/31(1) Planet Venus: 20 interesting facts about the scorching world. https://www.space.com/15988-venus-planet-weird-facts.html Accessed</p>
<p>2023/3/31. (2) What Is The Revolution Period Of Venus In Earth Years? Here's The .... https://www.odysseymagazine.com/what-is-the-revolution-period-of-venus-in-earth-years/ Accessed 2023/3/31. (3) What Is Venus's Revolution Period in Earth Days? I Sciencing. https://sciencing.com/venuss-revolution-period-earth-days-2411.html Accessed 2023/3/31. (4) Top 7 what is the revolution period of venus in earth years 2022. https://entornoit.com/top-7-what-is-the-revolution-period-of-venus-in-earth-years-2022/ Accessed 2023/3/31. (5) The length of a day on Venus keeps changing I Astronomy.com. https://astronomy.com/news/2020/01/the-length-of-a-day-on-venus-keeps-changing Accessed 2023/3/31. (6) Your Age on Other Worlds I Exploratorium. https://www.exploratorium.edu/ronh/age/ Accessed 2023/3/31.</p>
<p>Processed Answer:
The revolution period of Venus around the sun is 224.7 Earth days.</p>
<h1>C. 2 Human Annotation Guidelines</h1>
<p>Here is a question, a set of golden answers (split with /), an AI-generated answer. You are required to judge (1) whether the question have answers that change over time, simply annotate Yes or No; (2) whether the golden answer contain severe errors; (3) whether the AI-generated answer is correct according to the question and golden answers, simply annotate Yes or No.
Here is a set of guidelines for task (1) whether the question have answers that change over time:</p>
<ul>
<li>If the question is clearly time-sensitive, then it is Yes.</li>
<li>If there are words closely related to the current time node such as "this year", "last year", "next time" and "last time" in this question, then it is Yes.</li>
<li>If the question contains values that change over decades, such as "who is the player with the most goals in the World Cup so far", then it is Yes.</li>
<li>If the question contains values that do not change in decades, such as "what is the tallest mountain in the world", then it is No.</li>
</ul>
<p>If the answer to task (1) is Yes, skip to the next.
Here is a set of guidelines for task (2) whether the golden answer contain severe errors:</p>
<ul>
<li>If the golden answer has structure errors, then it is Yes.</li>
</ul>
<p>Example: Question: the south west wind blows across nigeria between? Golden: till September</p>
<ul>
<li>If the golden answer is obviously not what is asked, then it is Yes.</li>
<li>If the golden answer has format errors, then it is Yes.</li>
</ul>
<p>Example: Question: what season does bart bass die in gossip girl? Golden: (</p>
<ul>
<li>If the golden answer has only factual errors, then it is No.
(We also present some examples shown in Section C.3.1.) If the answer to task (2) is Yes, skip to the next.</li>
</ul>
<p>Here is a set of guidelines for task (3):</p>
<ul>
<li>If the question specifies a number (e.g., names of four people), and the response does not meet this requirement (e.g., provides only one name), the answer is deemed incorrect.</li>
<li>Spelling errors in the responses are considered mistakes. For example, if "golden answer" is misspelled as "gloden answer," the response is marked as incorrect.</li>
<li>For questions related to specific times, such as "When was the term social justice first used?" a response of "1840s" would be considered correct. However, if the answer needs to be precise to a specific day, month, and year, each time component needs to be factually accurate for the response to be marked as correct.</li>
<li>
<p>For location-based queries, like "Where was Oak Island filmed?", a response of "Canada" would be deemed correct. But, if the answer requires specific details like state, city, or county, each geographical component must be accurate for the answer to be considered correct.</p>
</li>
<li>
<p>If there is a direct answer and subsequent explanation in the response, then only focus on whether the direct answer is correct, not whether the subsequent explanation is correct</p>
</li>
</ul>
<p>These guidelines were strictly followed to maintain the reliability and validity of the evaluation process.</p>
<h1>C. 3 Supplements to the Annotation</h1>
<h2>AI-generated answers.</h2>
<p>For local-deployed models (DPR+FiD) and models can be accessed with APIs (text-davinci-003 for GPT-3.5 and gpt-3.5-turbo for ChatGPT-3.5), we generate the answers locally. For models that can only be interacted within he webpage, including ChatGPT-4 (we do not have API permissions) and BingChat, we ask the annotators to get the answer by interacting in the webpage and make judgement for the three tasks.</p>
<h2>Data assignment.</h2>
<p>We ask one annotator to judge samples with answers generated by DPR+FiD, GPT-3.5 and ChatGPT3.5; one for samples by ChatGPT-4; one for samples by Bing Chat, for convenience.</p>
<p>Improper questions or goldens. If a sample has an improper question or improper goldens, we mark the sample as improper. Since we have three different annotators to judge improper questions and goldens, if at least two annotators mark the improper result as True, we mark it as True, then we ask the left annotator (if there exists) to re-annotate the sample.</p>
<h2>C.3.1 Golden Error Examples</h2>
<p>Here some examples whose golden answer has obvious mistake. The first two have factual errors while the next one has the structure error and the last one has format error.</p>
<p>Question: was star wars a book or a movie first? Golden: film
Question: what is the democracy of the united states? Golden: federal republic
Question: the south west wind blows across nigeria between? Golden: till September
Question: what season does bart bass die in gossip girl? Golden: (</p>
<h2>D Methods</h2>
<h2>D. 1 DPR and FiD</h2>
<p>The DPR model retrieves relevant documents from all given documents to answer a specific question. Given a question $q$ and a database $D$ with each document denoted as $d$, the DPR model comprises two main components: the question encoder $Q_{\text {enc }}$ and the document encoder $D_{\text {enc }}$. Both typically rely on neural networks, such as BERT [Devlin et al., 2019].
The question encoder $Q_{\text {enc }}$ maps a question $q$ to a dense vector representation $q_{e m b}=Q_{\text {enc }}(q)$, and the document encoder $D_{\text {enc }}$ maps each document $d$ in the database $D$ to a dense vector representation $d_{\text {emb }}=D_{\text {enc }}(d)$.
We compute the similarity between the question embedding $q_{e m b}$ and each document embedding $d_{\text {emb }}$ using the dot product:</p>
<p>$$
s(d, q)=q_{e m b} \cdot d_{e m b}
$$</p>
<p>Documents in the database $D$ are ranked based on their similarity scores, and the top $k$ most relevant documents $D_{k}$ are retrieved. These documents are then used as input for the reader model $\mathcal{M}_{\text {reader }}$ to generate an answer $\hat{a}$ to the question $q$ :</p>
<p>$$
\hat{a}=\mathcal{M}<em k="k">{\text {reader }}\left(q, D</em>\right)
$$</p>
<p>Table 8: Performance of Eval-Models on EVOUNA. In each cell, the left is the precision while the right is the recall.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">NQ-FiD</th>
<th style="text-align: center;">NQ-GPT35</th>
<th style="text-align: center;">NQ-ChatGPT35</th>
<th style="text-align: center;">NQ-ChatGPT4</th>
<th style="text-align: center;">NQ-BingChat</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical Matching</td>
<td style="text-align: center;">99.6/85.4</td>
<td style="text-align: center;">99.5/77.1</td>
<td style="text-align: center;">96.0/76.2</td>
<td style="text-align: center;">99.7/78.1</td>
<td style="text-align: center;">97.6/79.8</td>
</tr>
<tr>
<td style="text-align: center;">BERT-Score</td>
<td style="text-align: center;">76.7/91.7</td>
<td style="text-align: center;">74.7/80.8</td>
<td style="text-align: center;">81.9/80.6</td>
<td style="text-align: center;">86.8/82.0</td>
<td style="text-align: center;">86.6/70.2</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">96.5/94.2</td>
<td style="text-align: center;">92.2/82.7</td>
<td style="text-align: center;">93.8/81.0</td>
<td style="text-align: center;">95.1/79.9</td>
<td style="text-align: center;">95.7/64.8</td>
</tr>
<tr>
<td style="text-align: center;">Another Human</td>
<td style="text-align: center;">98.5/96.3</td>
<td style="text-align: center;">97.8/97.8</td>
<td style="text-align: center;">97.8/95.3</td>
<td style="text-align: center;">99.0/96.8</td>
<td style="text-align: center;">98.7/95.8</td>
</tr>
<tr>
<td style="text-align: center;">on EVOUNA-NaturalQuestions</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">TQ-FiD</td>
<td style="text-align: center;">TQ-GPT35</td>
<td style="text-align: center;">TQ-ChatGPT35</td>
<td style="text-align: center;">TQ-ChatGPT4</td>
<td style="text-align: center;">TQ-BingChat</td>
</tr>
<tr>
<td style="text-align: center;">Lexical Matching</td>
<td style="text-align: center;">100/90.0</td>
<td style="text-align: center;">99.8/90.3</td>
<td style="text-align: center;">100/90.8</td>
<td style="text-align: center;">99.5/90.6</td>
<td style="text-align: center;">98.7/89.9</td>
</tr>
<tr>
<td style="text-align: center;">BERT-Score</td>
<td style="text-align: center;">91.5/63.7</td>
<td style="text-align: center;">86.6/81.6</td>
<td style="text-align: center;">89.7/87.2</td>
<td style="text-align: center;">93.6/97.6</td>
<td style="text-align: center;">94.9/82.6</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">98.5/96.1</td>
<td style="text-align: center;">98.2/90.5</td>
<td style="text-align: center;">97.5/93.5</td>
<td style="text-align: center;">98.1/93.4</td>
<td style="text-align: center;">98.4/80.0</td>
</tr>
<tr>
<td style="text-align: center;">Another Human</td>
<td style="text-align: center;">100/100</td>
<td style="text-align: center;">99.4/99.7</td>
<td style="text-align: center;">98.9/99.5</td>
<td style="text-align: center;">99.8/100</td>
<td style="text-align: center;">99.8/100</td>
</tr>
<tr>
<td style="text-align: center;">on EVOUNA-TriviaQA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h1>D. 2 BERT-Score</h1>
<p>Given a reference $r=A$ and a hypothesis $h=\hat{a}$, we first obtain their contextualized word embeddings using a pre-trained BERT model:</p>
<p>$$
\begin{aligned}
&amp; E_{r}=\operatorname{BERT}(r) \
&amp; E_{h}=\operatorname{BERT}(h)
\end{aligned}
$$</p>
<p>Next, we compute the cosine similarity between each token in the reference and each token in the hypothesis:</p>
<p>$$
S_{i, j}=\frac{E_{r_{i}} \cdot E_{h_{j}}}{\left|E_{r_{i}}\right|\left|E_{h_{j}}\right|}
$$</p>
<p>We then find the optimal token matchings using the maximum cosine similarity:</p>
<p>$$
\begin{aligned}
P_{r} &amp; =\frac{1}{|r|} \sum_{i=1}^{|r|} \max <em i_="i," j="j">{j=1}^{|h|} S</em> \
P_{h} &amp; =\frac{1}{|h|} \sum_{j=1}^{|h|} \max <em i_="i," j="j">{i=1}^{|r|} S</em>
\end{aligned}
$$</p>
<p>Finally, the BERT-score is calculated as the F1 score between the reference and hypothesis:</p>
<p>$$
\text { BERT-score }=\frac{2 \cdot P_{r} \cdot P_{h}}{P_{r}+P_{h}}
$$</p>
<p>To decide whether the AI-generated answer is positive or not, we set a threshold $\tau$ and classify the prediction $\hat{y}$ as positive if the BERT-score is above the threshold and as negative otherwise:</p>
<p>$$
\hat{y}=\left{\begin{array}{ll}
\text { Positive, } &amp; \text { BERT-score }&gt;=\tau \
\text { Negative, } &amp; \text { BERT-score }&lt;\tau
\end{array}\right.
$$</p>
<h2>E Analysis</h2>
<h2>E. 1 Additional Analysis for Open-QA</h2>
<p>From the Table 4, we have several additional observations:
All models perform better on TriviaQA compared to Natural Questions. This might suggest that the TriviaQA dataset, which is known for its trivia-style questions, is more aligned with the kind of</p>
<p>Table 9: The Proportions of Evaluation Outcomes Across Three Evaluators on the EVOUNA-NQ Dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">True Positive</th>
<th style="text-align: center;">True Negative</th>
<th style="text-align: center;">False Positive</th>
<th style="text-align: center;">False Negative</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Lexical Matching</td>
<td style="text-align: center;">57.5</td>
<td style="text-align: center;">26.8</td>
<td style="text-align: center;">1.1</td>
<td style="text-align: center;">14.7</td>
</tr>
<tr>
<td style="text-align: center;">BERT-Score</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">13.6</td>
<td style="text-align: center;">14.8</td>
<td style="text-align: center;">14.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 Evaluator</td>
<td style="text-align: center;">57.8</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">3.3</td>
<td style="text-align: center;">14.3</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5 Evaluator</td>
<td style="text-align: center;">59.8</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">9.9</td>
</tr>
<tr>
<td style="text-align: center;">without NQ-BingChat</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>diverse and general knowledge these models have been trained on. In contrast, the Natural Questions dataset, which is derived from real Google search queries, might contain more complex or niche questions that are challenging for the models.</p>
<p>GPT-3.5 vs ChatGPT-3.5 : These two models have comparable performance, both achieving approximately $65-70 \%$ accuracy on NQ and around $80 \%$ on TQ. This similarity is expected, as they are versions of the same base model, with the main difference being that ChatGPT is fine-tuned specifically for conversational contexts.</p>
<p>GPT-4 vs GPT-3.5 and ChatGPT-3.5 : The newer model GPT-4 significantly outperforms both GPT-3.5 and ChatGPT-3.5 on both datasets. This suggests that the improvements incorporated into GPT-4, likely including a larger model size and potentially refined training techniques, have resulted in substantial gains in question answering performance.</p>
<p>ChatGPT-4 vs BingChat : These two models exhibit the highest performance on both datasets. Their performance is remarkably similar, with GPT-4 outperforming Bing Chat by only a small margin on both datasets. This suggests that the two models, despite potentially having quite different architectures and training procedures, have reached similar levels of proficiency in question answering.</p>
<p>LLMs vs. Retrieval-based Methods : The DPR+FiD model, a representative of traditional retrieval-based methods, performs comparably to the earlier language models (GPT-3.5 and ChatGPT3.5), but falls behind the newer ones (ChatGPT-4 and Bing Chat). This indicates that while retrievalbased methods remain competitive, the newer generation of language models have surpassed them in terms of question answering capability. This could be due to the ability of these large models to better understand and generate natural language, enabling them to generate more accurate and contextually appropriate answers.</p>
<h1>E. 2 Supplemental Analysis for QA-Eval</h1>
<p>Table 8 showcases the performance of various evaluation models on EVOUNA-NaturalQuestions and EVOUNA-TriviaQA datasets. The reported metrics are precision and recall.
Looking at the EVOUNA-NaturalQuestions results, we observe that Lexical Matching and GPT-3.5 evaluation models achieve high precision across all QA models. However, the Lexical Matching model tends to have lower recall compared to GPT-3.5. BERT-Score has relatively lower precision but delivers better recall, indicating its ability to identify relevant answers but with a higher false positive rate. Human evaluation, as expected, provides near-perfect precision and recall scores.
For the EVOUNA-TriviaQA results, a similar pattern is observed. Lexical Matching, GPT-3.5, and human evaluation maintain high precision across all QA models. BERT-Score sees a drop in precision but has comparable recall, especially with the TQ-ChatGPT35 and TQ-ChatGPT4. Again, human evaluation shows nearly perfect performance.
The results underscore the different strengths of the evaluation models: Lexical Matching for precision, BERT-Score for recall, and GPT-3.5 and human evaluation for both. However, all models' performance varies with the dataset and QA model, emphasizing the importance of multiple evaluation methods for comprehensive assessment.</p>
<p>Table 10: Distribution of error types across different generative models on the NQ-test dataset. Each cell represents the proportion of the respective error type to all responses generated by the model.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">InAcc</th>
<th style="text-align: center;">InCom</th>
<th style="text-align: center;">IrrA</th>
<th style="text-align: center;">OutInf</th>
<th style="text-align: center;">MisQs</th>
<th style="text-align: center;">Others</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DPR + FiD</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">GPT-3.5</td>
<td style="text-align: center;">25.3</td>
<td style="text-align: center;">5.4</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">ChatGPT-3.5</td>
<td style="text-align: center;">23.2</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">0.2</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">13.3</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">1.2</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: center;">Bing Chat</td>
<td style="text-align: center;">9.5</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.5</td>
</tr>
</tbody>
</table>
<h1>E. 3 Error Analysis in Open-QA</h1>
<p>We classify the errors in the Open-QA scenario into several distinct categories:</p>
<ul>
<li>Inaccurate Information (InAcc): These errors occur when the model's response, while relevant to the question, contains inaccuracies.</li>
<li>Incomplete Answer (InCom): This type of error is characterized by the model providing pertinent information but failing to fully address the question.</li>
<li>Irrelevant Answer (IrrA): The model's response bears no relevance to the posed question.</li>
<li>Outdated Information (OutInf): These errors occur when the model provides information that was correct at some point in the past but is no longer valid or applicable.</li>
<li>Misinterpretation of the Question (MisQs): This category includes errors where the model misinterprets the question's intent or context.</li>
<li>Other Errors: This catch-all category includes any errors that don't fit into the above classifications.</li>
</ul>
<p>To perform this error classification, we initially used ChatGPT-4 to conduct a preliminary categorization of the Open-QA error data. Subsequently, human annotators were engaged to review and correct the classification results. The finalized results are represented in Table 10.
Analyzing the data reveals several interesting patterns. Notably, Bing Chat appears to have the highest rate of 'Incomplete Answer' errors, suggesting that while it generally understands the question, it often fails to provide a comprehensive answer. However, it also has the lowest rate of 'Inaccurate Information' errors, implying that the quality of the information it provides is usually high.
Conversely, DPR + FiD, GPT-3.5, and ChatGPT-3.5 all have similar rates of 'Inaccurate Information' errors, indicating a potential challenge in maintaining accuracy for these models. GPT-4 seems to outperform the other models in both 'Inaccurate Information' and 'Incomplete Answer' errors, suggesting an overall improvement in the quality and completeness of its responses.
It's also worth noting the relatively low incidence of 'Outdated Information' and 'Misinterpretation of the Question' errors across all models, suggesting that these areas are less problematic in current models.</p>
<p>This error analysis is helpful in identifying the strengths and weaknesses of different models and provides valuable insights into the areas that need further improvements.</p>
<h2>E. 4 Error Analysis in QA-Eval</h2>
<h2>E.4.1 Limitations of Each Evaluator</h2>
<p>Based on our theoretical analysis and observations of erroneous cases, we identified the following issues with each type of evaluator:</p>
<h2>Lexical Matching:</h2>
<ul>
<li>
<p>Lack of Semantic Understanding: The exact match metric doesn't take into account the semantic meaning of the answers. It only checks if the predicted answer is exactly the same as the ground truth, even if the predicted answer is semantically correct but phrased differently.</p>
</li>
<li>
<p>Inability to Handle Synonyms: The exact match metric cannot handle synonyms. If the predicted answer uses a different word that has the same meaning as the word in the ground truth answer, the exact match metric will consider it as a wrong answer.</p>
</li>
<li>Inability to Handle Paraphrasing: Similar to the point above, the exact match metric cannot handle paraphrasing. If the predicted answer is a paraphrase of the ground truth answer, the exact match metric will consider it as a wrong answer.</li>
<li>Inability to Handle Partially Correct Answers: The exact match metric cannot handle partially correct answers. If the predicted answer is partially correct, the exact match metric will consider it as a wrong answer.</li>
<li>Inability to Handle Reordered Words: The exact match metric cannot handle reordered words. If the predicted answer has the same words as the ground truth answer but in a different order, the exact match metric will consider it as a wrong answer.</li>
<li>Inability to Handle Different Levels of Detail: The exact match metric cannot handle different levels of detail. If the predicted answer provides more or less detail than the ground truth answer but is still correct, the exact match metric will consider it as a wrong answer.</li>
<li>Inability to Handle Different Formats: The exact match metric cannot handle different formats. If the predicted answer is in a different format than the ground truth answer (for example, dates or numbers), the exact match metric will consider it as a wrong answer.</li>
</ul>
<p>These limitations highlight the need for more sophisticated evaluation metrics that can understand the semantic meaning of the answers and handle synonyms, paraphrasing, partially correct answers, reordered words, different levels of detail, and different formats.</p>
<p>Neural Evaluation: The limitations of neural evaluation methods, such as BERT-Score and BLEURT, are evident. Most crucially, many neural evaluations are primarily designed to measure the similarity between two phrases or sentences. They are not tailored for binary tasks, especially those assessing the factual correctness of answers. Instead, they provide a continuous score that gauges the similarity between the generated text and the reference text, rendering them directly unsuitable for this particular task. In our study, we employed BERT-score and BLEURT for this task by setting a threshold. However, the performance of both BERT-score and BLEURT was suboptimal. The primary shortcoming of neural evaluations for this task is their misalignment with its requirements.
Furthermore, BERT-score has the following limitations:</p>
<ul>
<li>Sensitivity to Verbosity: BERT-score may penalize verbose answers even if they contain the correct information. If the AI-generated answer provides a detailed explanation while the golden answer is concise, the score might be lower than expected.</li>
<li>Mismatched Focus: If the AI-generated answer is correct but emphasizes different aspects or details than the golden answer, BERT-score might not recognize the similarity, leading to a lower score.</li>
<li>Lack of Contextual Understanding: BERT-score measures the similarity between embeddings but might not fully capture the contextual nuances of certain answers, especially when there are multiple valid ways to answer a question.</li>
<li>Synonym and Paraphrasing Issues: BERT-score might not always recognize synonyms or paraphrased answers as being equivalent to the golden answer, leading to potential discrepancies in scoring.</li>
<li>Threshold Limitations: Setting a fixed threshold (e.g., 0.5) for determining correctness can be arbitrary. Some answers might be just below the threshold but still be correct, while others might be just above but incorrect.</li>
<li>Doesn't Account for Minor Details: BERT-score might not be sensitive enough to minor inaccuracies in the AI-generated answer, especially if the overall semantic content is similar to the golden answer.</li>
<li>Lack of Absolute Truth Measure: BERT-score is a relative measure of similarity between two pieces of text. It doesn't provide an absolute measure of the truthfulness or correctness of an answer.</li>
</ul>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ dl.fbaipublicfiles.com/FiD/data/na_passages.tar.gz
${ }^{5}$ github.com/facebookresearch/FiD&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
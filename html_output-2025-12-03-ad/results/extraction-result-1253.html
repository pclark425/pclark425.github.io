<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1253 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1253</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1253</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-247595228</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2203.11024v1.pdf" target="_blank">Multi-view dreaming: multi-view world model with contrastive learning</a></p>
                <p><strong>Paper Abstract:</strong> In this paper, we propose Multi-View Dreaming, a novel reinforcement learning agent for integrated recognition and control from multi-view observations by extending Dreaming. Most current reinforcement learning method assumes a single-view observation space, and this imposes limitations on the observed data, such as lack of spatial information and occlusions. This makes obtaining ideal observational information from the environment difficult and is a bottleneck for real-world robotics applications. In this paper, we use contrastive learning to train a shared latent space between different viewpoints and show how the Products of Experts approach can be used to integrate and control the probability distributions of latent states for multiple viewpoints. We also propose Multi-View DreamingV2, a variant of Multi-View Dreaming that uses a categorical distribution to model the latent state instead of the Gaussian distribution. Experiments show that the proposed method outperforms simple extensions of existing methods in a realistic robot control task. GRAPHICAL ABSTRACT</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1253.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1253.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MVDreaming</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-View Dreaming (Gaussian latent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-based RL world model that extends Dreaming to multi-view observations by learning per-view Gaussian latent posteriors, aligning them with multi-view contrastive learning, and fusing them into a global belief via a Product of Experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-View Dreaming</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent State Space Model (RSSM) backbone with deterministic recurrent state h_t and per-view stochastic Gaussian latent z_v,t. Each view's encoder infers a Gaussian posterior q(z_v,t | h_t, x_v,t). Per-view posteriors are combined into a global posterior z_t via a Product-of-Experts (harmonic-mean formula over means and variances). A transition predictor p(ẑ_t | h_t) provides priors for imagination, and the agent learns a reward predictor, actor and critic on top of the global latent; training optimizes a decoder-free ELBO-derived objective composed of multi-step KL (overshooting) and a contrastive NCE term.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent stochastic latent model)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic control from pixels with multi-view cameras (simulated Reacher, Dual-view Pendulum, Robosuite Lift)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO-derived terms used as internal training objectives: multi-step KL (overshooting) and contrastive NCE; no explicit pixel reconstruction MSE or next-state MSE is reported in this paper; final RL episode return is used as downstream fidelity/task metric.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>The paper does not report explicit numeric prediction/reconstruction errors. Reported downstream RL returns (Table I) for Multi-View Dreaming (Gaussian): Reacher (multi-view) 588.6 ± 356.0, Pendulum (multi-view) 812.2 ± 130.4, Lift (multi-view) 110.1 ± 44.6 (episode-return units used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Primarily a black-box neural latent model; the authors provide qualitative latent inspection by (decoder) reconstruction from the global latent state (Fig. 8) to show combined view information, but do not demonstrate explicit disentanglement or interpretable latent axes.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Qualitative visualization: reconstructed image from the global latent state to show that important information from both viewpoints is embedded; no quantitative probing (e.g., latent→physical variable regression) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified numerically (no parameter counts, training time, or GPU requirements reported). The method uses a decoder-free objective (as in Dreaming) which reduces reconstruction compute during gradient updates; however exact savings are not measured in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>No quantitative runtime or sample-efficiency comparison reported. Qualitatively, authors note that simple image-overlay extensions of single-view baselines are competitive on simple tasks, while the proposed model yields advantages on more complex tasks—no direct compute-vs-performance trade-off numbers are given.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>See fidelity_performance above. Overall findings: Multi-View Dreaming (Gaussian) performs comparably to baselines on simpler tasks (Reacher, Pendulum) and underperforms the categorical (V2) variant on the complex Robosuite Lift multi-view task.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>The world model is useful for integrating complementary partial observations across views to produce a global belief for control; when observation structure and dynamics are relatively simple, simple baselines (e.g., overlaying images) can be competitive, but in tasks with complex imagery and cross-view dynamics (Lift) the model improves downstream policy performance. High internal training objective (low KL/NCE loss) is used as a proxy for representation fidelity but numerical correspondence to downstream RL performance is not explicitly quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Using Gaussian latents with PoE yields a principled probabilistic fusion but the Gaussian variant was sometimes outperformed by the categorical variant (MVDreamingV2) in complex tasks. Decoder-free objectives reduce reconstruction overhead but remove an explicit pixel-fidelity metric. Contrastive multi-view alignment improves multi-view representation but increases representation-learning complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Key choices: RSSM backbone, per-view encoders with Gaussian output, Product-of-Experts fusion implemented as harmonic-mean of means/variances, contrastive NCE across simultaneous multi-view images as positive pairs, decoder-free ELBO-derived loss (KL overshooting + NCE).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared against Dreamer, DreamerV2, Dreaming, DreamingV2, and a naive multi-view baseline that overlays multi-view images in the color channel. On simple tasks overlay baselines and several baselines are competitive; on the complex Robosuite Lift task the Gaussian MVDreaming improves over some baselines but is substantially outperformed by the categorical MVDreamingV2. No GPU/time comparisons are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>The paper suggests that categorical latent models (Multi-View DreamingV2) can be more effective for complex visual-control tasks; no strict optimal hyperparameterization or compute-vs-performance frontier is provided. Authors recommend multi-view contrastive alignment plus PoE fusion for multi-view settings and note further gains may come from embedding domain info (camera coordinates, proprioception) into per-view latents.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-view dreaming: multi-view world model with contrastive learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1253.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1253.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MVDreamingV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-View DreamingV2 (Categorical latent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of Multi-View Dreaming that uses discrete/categorical latent states (inherited from DreamingV2) and averages categorical distributions across views to form a global latent, combined with multi-view contrastive learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-View DreamingV2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RSSM-style recurrent model with deterministic h_t and a categorical/discrete stochastic latent z_t (as in DreamingV2). Each view infers per-view categorical posteriors; the global posterior is formed by averaging categorical distributions across dimensions (Product-of-Experts adapted to categorical latent by averaging). Training uses decoder-free ELBO-style objectives with multi-view contrastive NCE to align per-view embeddings; actor/critic and reward predictors operate on the global categorical latent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent, discrete/categorical latent)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>robotic control from multi-view pixel observations (simulated Reacher, Dual-view Pendulum, Robosuite Lift)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO-derived multi-step KL and contrastive NCE used during training; no explicit pixel reconstruction or next-state MSE reported; downstream episodic returns reported as primary evaluation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No explicit numeric latent prediction/reconstruction errors reported. Downstream RL returns (Table I) for Multi-View DreamingV2 reported in the paper: Reacher (multi-view) 936.1 ± 95.9, Pendulum (multi-view) 256.5 ± 304.5, Lift (multi-view) 345.0 ± 133.6 (episode-return units). Notably MVDreamingV2 achieved the best performance among compared methods on the Robosuite Lift multi-view task.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Categorical latents can be more amenable to discrete inspection, but the paper presents only qualitative reconstruction visuals; no systematic interpretability analysis tying discrete codes to physical variables is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Qualitative reconstruction of global latent to image to confirm combined view information; no quantitative probing or code-interpretation methodology reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified; number of discrete categories / logits sizes and training compute are not reported. The approach inherits decoder-free training advantages from DreamingV2 but no timing or memory measurements are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>No measured compute-time or sample-efficiency comparisons given. Empirically more effective than Gaussian MVDreaming and naive overlay baselines on the complex Lift task, implying better task-utility per sample in that scenario, but raw compute cost is not compared.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported downstream returns (see fidelity_performance). The paper emphasizes MVDreamingV2 outperformed all baselines by a significant margin on the Robosuite Lift multi-view task, while results across Reacher and Pendulum varied.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Discrete latent representation coupled with multi-view contrastive alignment and PoE-style fusion appears to yield better task-relevant representations for complex visual-manipulation tasks (Lift). However, on some simpler tasks performance was mixed, indicating that model choice (Gaussian vs categorical) interacts with task complexity and observation structure.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Categorical latents provided superior task utility in the complex Lift task but not universally superior across all experiments; discrete representation may impose different optimization dynamics and sensitivity to hyperparameters. The decoder-free objective reduces pixel reconstruction overhead at the cost of removing an explicit pixel-fidelity signal.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Discrete/categorical latent state (DreamingV2-style), per-view categorical posteriors, averaging-based fusion for categorical PoE, multi-view contrastive NCE for alignment, RSSM recurrent dynamics, decoder-free training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Outperformed Gaussian MVDreaming and simple overlayed-image baselines on the Robosuite Lift task in this paper. On simpler tasks overlay and baseline methods were sometimes competitive; no computational cost comparisons given.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests categorical (V2) formulation is preferable for complex multi-view visual control; authors do not provide a formal hyperparameter sweep or prescription for category sizes, but recommend multi-view contrastive alignment + discrete latents for complicated observation/dynamics scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-view dreaming: multi-view world model with contrastive learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1253.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1253.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreaming</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dreaming: Model-based reinforcement learning by latent imagination without reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-free world model that trains latent dynamics and uses imagination in latent space to learn control, relying on an ELBO-derived objective that replaces pixel reconstruction with contrastive and KL terms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dreaming: Model-based reinforcement learning by latent imagination without reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreaming</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-free latent dynamics model (RSSM-like) that trains representation and transition models without pixel reconstruction; uses contrastive learning (NCE) and multi-step KL objectives derived from ELBO to align latent representations and learn predictive dynamics for latent imagination-based policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (decoder-free, recurrent)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general visual model-based RL (referenced and used as baseline in multi-view experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Training objectives: NCE contrastive loss and multi-step KL (ELBO-derived); paper does not report pixel MSE or next-state MSE for Dreaming within this manuscript beyond downstream RL scores.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No separate fidelity numbers in this paper; baseline downstream RL returns (from Table I) where Dreaming is included: Reacher (multi-view) ~841.3 ± 316.7, Pendulum (multi-view) ~831.6 ± 126.4, Lift (multi-view) ~177.0 ± 80.9 (approximate values as reported in the table).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box latent model; no interpretability analyses shown in this paper beyond qualitative reconstructions presented for the multi-view methods that build on Dreaming.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported in this paper specific to Dreaming; the decoder-free design removes explicit pixel reconstructions as a training signal.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not quantified in this paper; decoder-free objective removes reconstruction gradients, potentially reducing compute compared to reconstruction-based models, but no numerical comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not quantitatively compared in this paper; treated as a baseline for multi-view extensions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used as baseline in experiments; see fidelity_performance for reported RL returns in Table I.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Dreaming provides a basis for decoder-free multi-view extension; in the authors' experiments simple overlay baselines based on Dreaming-family methods were sometimes competitive on simpler tasks but less effective on the complex Lift task than MVDreamingV2.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Removing reconstruction simplifies and potentially speeds training but removes a direct pixel-fidelity signal; representation alignment must rely on contrastive losses which can have different inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Decoder-free ELBO-derived training, latent imagination for policy learning, NCE contrastive loss and KL overshooting objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared empirically as a baseline to Dreamer variants and multi-view extensions; specific numeric comparisons are provided in Table I of the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not provided in this paper (Dreaming is used as an inherited baseline component).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-view dreaming: multi-view world model with contrastive learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1253.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1253.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamingV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamingV2: Model-based reinforcement learning with discrete world models without reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-free world model variant that uses discrete/categorical latent variables (discrete world models) instead of Gaussian latents to represent state for latent imagination and policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dreamingv2: Model-based reinforcement learning with discrete world models without reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamingV2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RSSM-like recurrent model with discrete/categorical latent representations (discrete world models) trained without pixel reconstruction; uses contrastive and KL-based losses to learn dynamics and representations suitable for latent imagination-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (discrete/categorical latent)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual model-based RL (used as a baseline and as the basis for Multi-View DreamingV2)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Training objectives: multi-step KL and contrastive NCE; no pixel reconstruction MSE reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Baseline RL returns (Table I) assigned to DreamingV2 in experiments: Reacher (multi-view) ~860.9 ± 285.6, Pendulum (multi-view) ~410.4 ± 413.99, Lift (multi-view) ~254.7 ± 104.2 (values are taken from the paper's comparative table).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Discrete latents may admit more interpretable clustering, but the paper reports no explicit interpretability probes for DreamingV2 in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported in this paper specifically for DreamingV2.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not quantitatively compared here; used as a baseline and as the basis for the multi-view categorical extension.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Used as baseline; see fidelity_performance. Authors report that their Multi-View DreamingV2 (their extension) often improves over single-view DreamingV2 baselines on multi-view Lift task when multi-view fusion is used properly.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Discrete latent modeling can provide advantages in some downstream tasks (as seen by the authors' MVDreamingV2 results), but DreamingV2 baseline performance in multi-view settings was mixed in the reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Categorical/discrete latents can change optimization dynamics and performance depending on task complexity; no compute trade-offs are quantified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Discrete categorical latent states, decoder-free ELBO-derived objective, contrastive alignment (in multi-view extension).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Benchmarked versus Dreamer-family and Gaussian latent baselines in the paper's experiments (see Table I).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not provided; authors use DreamingV2 architecture as the basis for their multi-view categorical extension.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-view dreaming: multi-view world model with contrastive learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1253.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1253.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent latent world model (RSSM) that learns latent dynamics with an encoder-decoder and uses latent imagination to train agents; used in this paper as a standard baseline for model-based RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dreamer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RSSM-based model with deterministic recurrent state and stochastic latent components; learns a latent dynamics model from pixel observations and uses imagined trajectories in latent space to train actor/critic/policy. Traditionally includes reconstruction components in original formulations (used here as a representative baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual control from pixels (general RL benchmarks and used here as baseline for multi-view tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Original Dreamer uses reconstruction losses plus latent KL; in this manuscript Dreamer is treated as a baseline and fidelity metrics are downstream RL returns reported in Table I.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Baseline downstream RL returns reported in Table I (approximate): Reacher (multi-view) ~685.0 ± 410.7, Pendulum (multi-view) ~801.2 ± 110.4, Lift (multi-view) ~102.9 ± 82.8.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Model is a neural latent model without explicit interpretability analyses in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not quantified in this manuscript; Dreamer is used as a representative model-based baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>See fidelity_performance above; Dreamer performed variably across tasks in the paper's multi-view experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Dreamer-style RSSM provides a strong baseline for single-view model-based RL; when naively extended to multi-view by image overlay it can be competitive on simpler tasks but fails to capture cross-view structure as effectively as multi-view contrastive+PoE approaches for complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Reconstruction-based training provides pixel fidelity but increases compute; decoder-free variants remove reconstruction expense at the cost of losing pixel reconstruction as an explicit objective.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>RSSM latent dynamics, latent imagination for policy optimization, (original Dreamer) pixel reconstruction + latent objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared empirically to DreamerV2, Dreaming-family, and the proposed multi-view methods; see Table I.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not discussed in this paper beyond use as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-view dreaming: multi-view world model with contrastive learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1253.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1253.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mastering atari with discrete world models.</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A discrete latent variant of Dreamer (often called DreamerV2) that uses discrete world models to improve performance on long-horizon/complex tasks; used as a baseline in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mastering atari with discrete world models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DreamerV2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A world model that replaces continuous Gaussian latents with discrete/categorical latent representations (discrete world models) to enable more expressive or stable latent dynamics; used here as a baseline and point of comparison for categorical latent choices.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (discrete/categorical latent)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general visual RL benchmarks (Atari originally) and used here as a baseline for multi-view control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Original DreamerV2 uses discrete-latent training objectives; in this paper fidelity is evaluated via downstream RL returns shown in Table I.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Baseline downstream RL returns (approximate from Table I): Reacher (multi-view) ~42.2 ± 73.3, Pendulum (multi-view) ~10.8 ± 24.5, Lift (multi-view) ~120.8 ± 51.5 (values as reported in the comparative table).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Discrete latents could be more interpretable in principle, but no interpretability analyses are provided in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not quantified in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported as a baseline with mixed performance on the reported multi-view tasks; in this paper the custom Multi-View DreamingV2 (authors' extension) outperformed DreamerV2 baselines on the complex Lift task.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Discrete latent models can be advantageous in some settings; the paper shows that a multi-view categorical design (their MVDreamingV2) is especially effective on complex multi-view manipulation tasks compared to naive single-view discrete baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Discrete latents can change learning dynamics and may be more effective when combined with multi-view contrastive alignment; however, performance depends on task and how multi-view fusion is implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Discrete/categorical latents, RSSM-style recurrence, latent imagination for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Benchmarked versus Dreamer and Dreaming-family models in Table I; authors' multi-view categorical extension exceeded DreamerV2 baseline in complex task.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-view dreaming: multi-view world model with contrastive learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1253.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1253.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Product of Experts (PoE) fusion</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic method to combine multiple per-view latent distributions into a single global posterior by multiplying expert densities; applied here to fuse per-view posteriors into a global latent state.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Training products of experts by minimizing contrastive divergence.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Product of Experts fusion</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Per-view posterior distributions (Gaussian or categorical) are combined into a single global posterior. For Gaussian latents the paper implements PoE by computing a weighted harmonic-mean style combination of means and precisions (equations provided); for categorical latents the paper averages per-dimension categorical distributions to form a fused discrete posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>probabilistic fusion method</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-view latent integration for world models in RL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>No explicit fidelity metric reported for fusion itself; success of fusion measured indirectly via downstream RL performance and qualitative reconstructions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not numerically reported; authors attribute improved multi-view integration (qualitatively demonstrated via reconstructions) and improved performance in complex tasks to PoE fusion combined with contrastive alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Mathematically interpretable: PoE provides a principled probabilistic combination and integrates uncertainty across views; the implementation details (harmonic-mean for Gaussian) are explicit in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Analytic formulae for Gaussian fusion and averaging for categorical fusion; qualitative reconstructed images shown from the fused latent.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Minimal additional compute to combine distributions; no explicit benchmarks provided.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not quantified; presented as an efficient, principled fusion technique compared to naive concatenation/overlay.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Contributes to capability of system to aggregate partial observations across views; authors show that PoE fusion as part of MVDreaming family helps on complex multi-view tasks (empirical evidence in Lift).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>PoE enables combining heterogeneous partial observations into a single belief; utility depends on correct per-view posteriors and contrastive alignment to ensure posteriors represent the same latent.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Requires reliable per-view posteriors; if per-view encoders are poorly calibrated, multiplying densities could overconfidently collapse beliefs. The paper does not measure this effect quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Gaussian PoE implemented via harmonic-mean for mean/variance; categorical fusion implemented by averaging categorical logits/probabilities per dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared implicitly against naive overlay of images and simple concatenation baselines; PoE fusion combined with multi-view contrastive learning outperforms naive approaches in complex multi-view tasks in the experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not explicitly specified; authors adopt standard PoE formulas and recommend combining it with contrastive multi-view alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-view dreaming: multi-view world model with contrastive learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1253.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1253.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent State-Space Model (RSSM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent latent-dynamics architecture used to learn stochastic latent states and deterministic recurrence for model-based RL; serves as the backbone dynamics model in Multi-View Dreaming and baseline Dreamer methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dream to control: Learning behaviors by latent imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RSSM (recurrent stochastic latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Architecture includes deterministic recurrent state h_t = f_phi(h_{t-1}, z_{t-1}, a_{t-1}), a representation model q_phi(z_t | h_t, x_t) producing stochastic latents, and a transition predictor p_phi(ẑ_t | h_t) generating priors used for imagination; used together with reward predictors, actor and critic.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (recurrent/stochastic)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>visual control and model-based RL from pixels (multi-view extension in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Trained using multi-step KL overshooting and contrastive NCE objectives; no explicit predictive-MSE metrics reported in this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>No isolated fidelity statistics for RSSM alone; overall system performance reported via downstream returns in Table I.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Black-box neural recurrent model; no interpretability analyses for RSSM components provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>None reported beyond qualitative reconstructions from global latent.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not specified numerically; RSSM is standard component in world models and computational cost depends on chosen network sizes (not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not directly compared in paper; RSSM is used because prior works (Dreamer family) successfully used it for latent imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>RSSM-based models (Dreamer-family and MVDreaming variants) achieve the reported RL returns; RSSM enables latent prediction/imagination used by actor/critic.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>RSSM provides the temporal dynamics model enabling imagination and multi-step planning; in multi-view setting per-view encoders feed into the RSSM to permit temporal integration across views.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>RSSM introduces stochastic/deterministic splitting that requires careful training (KL balance, overshooting). Paper inherits these issues but does not analyze them in depth.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Deterministic recurrent core for temporal context; stochastic latent for uncertainty; representation model conditions on h_t and current images; transition predictor provides prior for latent imagination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Chosen as standard backbone due to prior success (Dreamer); specific alternatives (e.g., pure feedforward predictors, transformers) are not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified; authors use RSSM in the same manner as Dreaming/Dreamer family implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-view dreaming: multi-view world model with contrastive learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1253.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e1253.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-view contrastive</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-View Contrastive Learning for shared latent space</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contrastive representation-learning procedure that treats simultaneous images from different viewpoints of the same scene as positive pairs and images from different times as negatives, used to align per-view representations in a shared latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multi-view contrastive learning (NCE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Extends standard image-augmentation contrastive objectives by using cross-view positive pairs: for images x_v1,t and x_v2,t taken at the same time from different cameras the representation model is trained to bring their latents closer (via NCE/cross-entropy discriminative loss), while images from other times serve as negatives. This alignment is integrated into the decoder-free ELBO-style objective.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>representation learning / auxiliary objective</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>multi-view visual representation learning for model-based RL</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Contrastive NCE loss (J_NCE_k) used as a training objective; no standalone numeric contrastive accuracy or AUROC is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not numerically reported beyond its impact on downstream RL returns where multi-view contrastive learning contributes to improved multi-view fusion and better policy performance in complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Aligns representations across views so that global latent is view-invariant; interpretability relies on qualitative reconstructions rather than quantitative disentanglement analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of reconstructions from global latent and qualitative claims that latents from different views are close when they represent same scene.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Not reported; contrastive learning typically requires sampling negative examples and additional encoder passes but no precise compute figures are given.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Not quantified; authors argue multi-view contrastive learning is beneficial especially when naive image-overlay baselines fail on complex tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Improves downstream RL performance in multi-view complex tasks when combined with PoE and categorical latent variants (empirical claim supported by Lift task results).</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Key to learning a shared latent across viewpoints, enabling PoE fusion to produce a coherent global belief; effective particularly when different views provide complementary partial observations.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Requires careful selection of positives/negatives (simultaneous multi-view frames as positives); increases representation learning complexity and reliance on negative sampling strategies; exact sensitivity not quantified in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Positive pairs: different views at same time + random crop augmentations; negatives: images from other times (same/different views); integrated into ELBO-derived objective as J_NCE_k terms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared qualitatively to time-contrastive and overlay baselines; multi-view contrastive + PoE provides improved performance on complex tasks compared to naive overlaying of views.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Multi-view dreaming: multi-view world model with contrastive learning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Dreaming: Model-based reinforcement learning by latent imagination without reconstruction. <em>(Rating: 2)</em></li>
                <li>Dreamingv2: Model-based reinforcement learning with discrete world models without reconstruction. <em>(Rating: 2)</em></li>
                <li>Dream to control: Learning behaviors by latent imagination. <em>(Rating: 2)</em></li>
                <li>Mastering atari with discrete world models. <em>(Rating: 2)</em></li>
                <li>PlaNet <em>(Rating: 1)</em></li>
                <li>Training products of experts by minimizing contrastive divergence. <em>(Rating: 1)</em></li>
                <li>CURL: Contrastive unsupervised representations for reinforcement learning. <em>(Rating: 1)</em></li>
                <li>Multi-modal mutual information (MUMMI) training for robust self-supervised deep reinforcement learning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1253",
    "paper_id": "paper-247595228",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "MVDreaming",
            "name_full": "Multi-View Dreaming (Gaussian latent)",
            "brief_description": "A model-based RL world model that extends Dreaming to multi-view observations by learning per-view Gaussian latent posteriors, aligning them with multi-view contrastive learning, and fusing them into a global belief via a Product of Experts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multi-View Dreaming",
            "model_description": "Recurrent State Space Model (RSSM) backbone with deterministic recurrent state h_t and per-view stochastic Gaussian latent z_v,t. Each view's encoder infers a Gaussian posterior q(z_v,t | h_t, x_v,t). Per-view posteriors are combined into a global posterior z_t via a Product-of-Experts (harmonic-mean formula over means and variances). A transition predictor p(ẑ_t | h_t) provides priors for imagination, and the agent learns a reward predictor, actor and critic on top of the global latent; training optimizes a decoder-free ELBO-derived objective composed of multi-step KL (overshooting) and a contrastive NCE term.",
            "model_type": "latent world model (recurrent stochastic latent model)",
            "task_domain": "robotic control from pixels with multi-view cameras (simulated Reacher, Dual-view Pendulum, Robosuite Lift)",
            "fidelity_metric": "ELBO-derived terms used as internal training objectives: multi-step KL (overshooting) and contrastive NCE; no explicit pixel reconstruction MSE or next-state MSE is reported in this paper; final RL episode return is used as downstream fidelity/task metric.",
            "fidelity_performance": "The paper does not report explicit numeric prediction/reconstruction errors. Reported downstream RL returns (Table I) for Multi-View Dreaming (Gaussian): Reacher (multi-view) 588.6 ± 356.0, Pendulum (multi-view) 812.2 ± 130.4, Lift (multi-view) 110.1 ± 44.6 (episode-return units used in experiments).",
            "interpretability_assessment": "Primarily a black-box neural latent model; the authors provide qualitative latent inspection by (decoder) reconstruction from the global latent state (Fig. 8) to show combined view information, but do not demonstrate explicit disentanglement or interpretable latent axes.",
            "interpretability_method": "Qualitative visualization: reconstructed image from the global latent state to show that important information from both viewpoints is embedded; no quantitative probing (e.g., latent→physical variable regression) reported.",
            "computational_cost": "Not specified numerically (no parameter counts, training time, or GPU requirements reported). The method uses a decoder-free objective (as in Dreaming) which reduces reconstruction compute during gradient updates; however exact savings are not measured in the paper.",
            "efficiency_comparison": "No quantitative runtime or sample-efficiency comparison reported. Qualitatively, authors note that simple image-overlay extensions of single-view baselines are competitive on simple tasks, while the proposed model yields advantages on more complex tasks—no direct compute-vs-performance trade-off numbers are given.",
            "task_performance": "See fidelity_performance above. Overall findings: Multi-View Dreaming (Gaussian) performs comparably to baselines on simpler tasks (Reacher, Pendulum) and underperforms the categorical (V2) variant on the complex Robosuite Lift multi-view task.",
            "task_utility_analysis": "The world model is useful for integrating complementary partial observations across views to produce a global belief for control; when observation structure and dynamics are relatively simple, simple baselines (e.g., overlaying images) can be competitive, but in tasks with complex imagery and cross-view dynamics (Lift) the model improves downstream policy performance. High internal training objective (low KL/NCE loss) is used as a proxy for representation fidelity but numerical correspondence to downstream RL performance is not explicitly quantified.",
            "tradeoffs_observed": "Using Gaussian latents with PoE yields a principled probabilistic fusion but the Gaussian variant was sometimes outperformed by the categorical variant (MVDreamingV2) in complex tasks. Decoder-free objectives reduce reconstruction overhead but remove an explicit pixel-fidelity metric. Contrastive multi-view alignment improves multi-view representation but increases representation-learning complexity.",
            "design_choices": "Key choices: RSSM backbone, per-view encoders with Gaussian output, Product-of-Experts fusion implemented as harmonic-mean of means/variances, contrastive NCE across simultaneous multi-view images as positive pairs, decoder-free ELBO-derived loss (KL overshooting + NCE).",
            "comparison_to_alternatives": "Compared against Dreamer, DreamerV2, Dreaming, DreamingV2, and a naive multi-view baseline that overlays multi-view images in the color channel. On simple tasks overlay baselines and several baselines are competitive; on the complex Robosuite Lift task the Gaussian MVDreaming improves over some baselines but is substantially outperformed by the categorical MVDreamingV2. No GPU/time comparisons are provided.",
            "optimal_configuration": "The paper suggests that categorical latent models (Multi-View DreamingV2) can be more effective for complex visual-control tasks; no strict optimal hyperparameterization or compute-vs-performance frontier is provided. Authors recommend multi-view contrastive alignment plus PoE fusion for multi-view settings and note further gains may come from embedding domain info (camera coordinates, proprioception) into per-view latents.",
            "uuid": "e1253.0",
            "source_info": {
                "paper_title": "Multi-view dreaming: multi-view world model with contrastive learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "MVDreamingV2",
            "name_full": "Multi-View DreamingV2 (Categorical latent)",
            "brief_description": "A variant of Multi-View Dreaming that uses discrete/categorical latent states (inherited from DreamingV2) and averages categorical distributions across views to form a global latent, combined with multi-view contrastive learning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multi-View DreamingV2",
            "model_description": "RSSM-style recurrent model with deterministic h_t and a categorical/discrete stochastic latent z_t (as in DreamingV2). Each view infers per-view categorical posteriors; the global posterior is formed by averaging categorical distributions across dimensions (Product-of-Experts adapted to categorical latent by averaging). Training uses decoder-free ELBO-style objectives with multi-view contrastive NCE to align per-view embeddings; actor/critic and reward predictors operate on the global categorical latent.",
            "model_type": "latent world model (recurrent, discrete/categorical latent)",
            "task_domain": "robotic control from multi-view pixel observations (simulated Reacher, Dual-view Pendulum, Robosuite Lift)",
            "fidelity_metric": "ELBO-derived multi-step KL and contrastive NCE used during training; no explicit pixel reconstruction or next-state MSE reported; downstream episodic returns reported as primary evaluation metric.",
            "fidelity_performance": "No explicit numeric latent prediction/reconstruction errors reported. Downstream RL returns (Table I) for Multi-View DreamingV2 reported in the paper: Reacher (multi-view) 936.1 ± 95.9, Pendulum (multi-view) 256.5 ± 304.5, Lift (multi-view) 345.0 ± 133.6 (episode-return units). Notably MVDreamingV2 achieved the best performance among compared methods on the Robosuite Lift multi-view task.",
            "interpretability_assessment": "Categorical latents can be more amenable to discrete inspection, but the paper presents only qualitative reconstruction visuals; no systematic interpretability analysis tying discrete codes to physical variables is provided.",
            "interpretability_method": "Qualitative reconstruction of global latent to image to confirm combined view information; no quantitative probing or code-interpretation methodology reported.",
            "computational_cost": "Not specified; number of discrete categories / logits sizes and training compute are not reported. The approach inherits decoder-free training advantages from DreamingV2 but no timing or memory measurements are provided.",
            "efficiency_comparison": "No measured compute-time or sample-efficiency comparisons given. Empirically more effective than Gaussian MVDreaming and naive overlay baselines on the complex Lift task, implying better task-utility per sample in that scenario, but raw compute cost is not compared.",
            "task_performance": "Reported downstream returns (see fidelity_performance). The paper emphasizes MVDreamingV2 outperformed all baselines by a significant margin on the Robosuite Lift multi-view task, while results across Reacher and Pendulum varied.",
            "task_utility_analysis": "Discrete latent representation coupled with multi-view contrastive alignment and PoE-style fusion appears to yield better task-relevant representations for complex visual-manipulation tasks (Lift). However, on some simpler tasks performance was mixed, indicating that model choice (Gaussian vs categorical) interacts with task complexity and observation structure.",
            "tradeoffs_observed": "Categorical latents provided superior task utility in the complex Lift task but not universally superior across all experiments; discrete representation may impose different optimization dynamics and sensitivity to hyperparameters. The decoder-free objective reduces pixel reconstruction overhead at the cost of removing an explicit pixel-fidelity signal.",
            "design_choices": "Discrete/categorical latent state (DreamingV2-style), per-view categorical posteriors, averaging-based fusion for categorical PoE, multi-view contrastive NCE for alignment, RSSM recurrent dynamics, decoder-free training.",
            "comparison_to_alternatives": "Outperformed Gaussian MVDreaming and simple overlayed-image baselines on the Robosuite Lift task in this paper. On simpler tasks overlay and baseline methods were sometimes competitive; no computational cost comparisons given.",
            "optimal_configuration": "Paper suggests categorical (V2) formulation is preferable for complex multi-view visual control; authors do not provide a formal hyperparameter sweep or prescription for category sizes, but recommend multi-view contrastive alignment + discrete latents for complicated observation/dynamics scenarios.",
            "uuid": "e1253.1",
            "source_info": {
                "paper_title": "Multi-view dreaming: multi-view world model with contrastive learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Dreaming",
            "name_full": "Dreaming: Model-based reinforcement learning by latent imagination without reconstruction.",
            "brief_description": "A decoder-free world model that trains latent dynamics and uses imagination in latent space to learn control, relying on an ELBO-derived objective that replaces pixel reconstruction with contrastive and KL terms.",
            "citation_title": "Dreaming: Model-based reinforcement learning by latent imagination without reconstruction.",
            "mention_or_use": "use",
            "model_name": "Dreaming",
            "model_description": "Decoder-free latent dynamics model (RSSM-like) that trains representation and transition models without pixel reconstruction; uses contrastive learning (NCE) and multi-step KL objectives derived from ELBO to align latent representations and learn predictive dynamics for latent imagination-based policy learning.",
            "model_type": "latent world model (decoder-free, recurrent)",
            "task_domain": "general visual model-based RL (referenced and used as baseline in multi-view experiments)",
            "fidelity_metric": "Training objectives: NCE contrastive loss and multi-step KL (ELBO-derived); paper does not report pixel MSE or next-state MSE for Dreaming within this manuscript beyond downstream RL scores.",
            "fidelity_performance": "No separate fidelity numbers in this paper; baseline downstream RL returns (from Table I) where Dreaming is included: Reacher (multi-view) ~841.3 ± 316.7, Pendulum (multi-view) ~831.6 ± 126.4, Lift (multi-view) ~177.0 ± 80.9 (approximate values as reported in the table).",
            "interpretability_assessment": "Black-box latent model; no interpretability analyses shown in this paper beyond qualitative reconstructions presented for the multi-view methods that build on Dreaming.",
            "interpretability_method": "None reported in this paper specific to Dreaming; the decoder-free design removes explicit pixel reconstructions as a training signal.",
            "computational_cost": "Not quantified in this paper; decoder-free objective removes reconstruction gradients, potentially reducing compute compared to reconstruction-based models, but no numerical comparisons provided.",
            "efficiency_comparison": "Not quantitatively compared in this paper; treated as a baseline for multi-view extensions.",
            "task_performance": "Used as baseline in experiments; see fidelity_performance for reported RL returns in Table I.",
            "task_utility_analysis": "Dreaming provides a basis for decoder-free multi-view extension; in the authors' experiments simple overlay baselines based on Dreaming-family methods were sometimes competitive on simpler tasks but less effective on the complex Lift task than MVDreamingV2.",
            "tradeoffs_observed": "Removing reconstruction simplifies and potentially speeds training but removes a direct pixel-fidelity signal; representation alignment must rely on contrastive losses which can have different inductive biases.",
            "design_choices": "Decoder-free ELBO-derived training, latent imagination for policy learning, NCE contrastive loss and KL overshooting objectives.",
            "comparison_to_alternatives": "Compared empirically as a baseline to Dreamer variants and multi-view extensions; specific numeric comparisons are provided in Table I of the paper.",
            "optimal_configuration": "Not provided in this paper (Dreaming is used as an inherited baseline component).",
            "uuid": "e1253.2",
            "source_info": {
                "paper_title": "Multi-view dreaming: multi-view world model with contrastive learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DreamingV2",
            "name_full": "DreamingV2: Model-based reinforcement learning with discrete world models without reconstruction.",
            "brief_description": "A decoder-free world model variant that uses discrete/categorical latent variables (discrete world models) instead of Gaussian latents to represent state for latent imagination and policy learning.",
            "citation_title": "Dreamingv2: Model-based reinforcement learning with discrete world models without reconstruction.",
            "mention_or_use": "use",
            "model_name": "DreamingV2",
            "model_description": "RSSM-like recurrent model with discrete/categorical latent representations (discrete world models) trained without pixel reconstruction; uses contrastive and KL-based losses to learn dynamics and representations suitable for latent imagination-based RL.",
            "model_type": "latent world model (discrete/categorical latent)",
            "task_domain": "visual model-based RL (used as a baseline and as the basis for Multi-View DreamingV2)",
            "fidelity_metric": "Training objectives: multi-step KL and contrastive NCE; no pixel reconstruction MSE reported here.",
            "fidelity_performance": "Baseline RL returns (Table I) assigned to DreamingV2 in experiments: Reacher (multi-view) ~860.9 ± 285.6, Pendulum (multi-view) ~410.4 ± 413.99, Lift (multi-view) ~254.7 ± 104.2 (values are taken from the paper's comparative table).",
            "interpretability_assessment": "Discrete latents may admit more interpretable clustering, but the paper reports no explicit interpretability probes for DreamingV2 in this manuscript.",
            "interpretability_method": "None reported in this paper specifically for DreamingV2.",
            "computational_cost": "Not specified in this paper.",
            "efficiency_comparison": "Not quantitatively compared here; used as a baseline and as the basis for the multi-view categorical extension.",
            "task_performance": "Used as baseline; see fidelity_performance. Authors report that their Multi-View DreamingV2 (their extension) often improves over single-view DreamingV2 baselines on multi-view Lift task when multi-view fusion is used properly.",
            "task_utility_analysis": "Discrete latent modeling can provide advantages in some downstream tasks (as seen by the authors' MVDreamingV2 results), but DreamingV2 baseline performance in multi-view settings was mixed in the reported experiments.",
            "tradeoffs_observed": "Categorical/discrete latents can change optimization dynamics and performance depending on task complexity; no compute trade-offs are quantified in the paper.",
            "design_choices": "Discrete categorical latent states, decoder-free ELBO-derived objective, contrastive alignment (in multi-view extension).",
            "comparison_to_alternatives": "Benchmarked versus Dreamer-family and Gaussian latent baselines in the paper's experiments (see Table I).",
            "optimal_configuration": "Not provided; authors use DreamingV2 architecture as the basis for their multi-view categorical extension.",
            "uuid": "e1253.3",
            "source_info": {
                "paper_title": "Multi-view dreaming: multi-view world model with contrastive learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Dreamer",
            "name_full": "Dream to control: Learning behaviors by latent imagination.",
            "brief_description": "A recurrent latent world model (RSSM) that learns latent dynamics with an encoder-decoder and uses latent imagination to train agents; used in this paper as a standard baseline for model-based RL.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination.",
            "mention_or_use": "use",
            "model_name": "Dreamer",
            "model_description": "RSSM-based model with deterministic recurrent state and stochastic latent components; learns a latent dynamics model from pixel observations and uses imagined trajectories in latent space to train actor/critic/policy. Traditionally includes reconstruction components in original formulations (used here as a representative baseline).",
            "model_type": "latent world model (recurrent)",
            "task_domain": "visual control from pixels (general RL benchmarks and used here as baseline for multi-view tasks)",
            "fidelity_metric": "Original Dreamer uses reconstruction losses plus latent KL; in this manuscript Dreamer is treated as a baseline and fidelity metrics are downstream RL returns reported in Table I.",
            "fidelity_performance": "Baseline downstream RL returns reported in Table I (approximate): Reacher (multi-view) ~685.0 ± 410.7, Pendulum (multi-view) ~801.2 ± 110.4, Lift (multi-view) ~102.9 ± 82.8.",
            "interpretability_assessment": "Model is a neural latent model without explicit interpretability analyses in this paper.",
            "interpretability_method": "None reported in this paper.",
            "computational_cost": "Not specified here.",
            "efficiency_comparison": "Not quantified in this manuscript; Dreamer is used as a representative model-based baseline.",
            "task_performance": "See fidelity_performance above; Dreamer performed variably across tasks in the paper's multi-view experiments.",
            "task_utility_analysis": "Dreamer-style RSSM provides a strong baseline for single-view model-based RL; when naively extended to multi-view by image overlay it can be competitive on simpler tasks but fails to capture cross-view structure as effectively as multi-view contrastive+PoE approaches for complex tasks.",
            "tradeoffs_observed": "Reconstruction-based training provides pixel fidelity but increases compute; decoder-free variants remove reconstruction expense at the cost of losing pixel reconstruction as an explicit objective.",
            "design_choices": "RSSM latent dynamics, latent imagination for policy optimization, (original Dreamer) pixel reconstruction + latent objectives.",
            "comparison_to_alternatives": "Compared empirically to DreamerV2, Dreaming-family, and the proposed multi-view methods; see Table I.",
            "optimal_configuration": "Not discussed in this paper beyond use as baseline.",
            "uuid": "e1253.4",
            "source_info": {
                "paper_title": "Multi-view dreaming: multi-view world model with contrastive learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DreamerV2",
            "name_full": "Mastering atari with discrete world models.",
            "brief_description": "A discrete latent variant of Dreamer (often called DreamerV2) that uses discrete world models to improve performance on long-horizon/complex tasks; used as a baseline in this work.",
            "citation_title": "Mastering atari with discrete world models.",
            "mention_or_use": "use",
            "model_name": "DreamerV2",
            "model_description": "A world model that replaces continuous Gaussian latents with discrete/categorical latent representations (discrete world models) to enable more expressive or stable latent dynamics; used here as a baseline and point of comparison for categorical latent choices.",
            "model_type": "latent world model (discrete/categorical latent)",
            "task_domain": "general visual RL benchmarks (Atari originally) and used here as a baseline for multi-view control tasks.",
            "fidelity_metric": "Original DreamerV2 uses discrete-latent training objectives; in this paper fidelity is evaluated via downstream RL returns shown in Table I.",
            "fidelity_performance": "Baseline downstream RL returns (approximate from Table I): Reacher (multi-view) ~42.2 ± 73.3, Pendulum (multi-view) ~10.8 ± 24.5, Lift (multi-view) ~120.8 ± 51.5 (values as reported in the comparative table).",
            "interpretability_assessment": "Discrete latents could be more interpretable in principle, but no interpretability analyses are provided in this manuscript.",
            "interpretability_method": "None reported here.",
            "computational_cost": "Not specified.",
            "efficiency_comparison": "Not quantified in this manuscript.",
            "task_performance": "Reported as a baseline with mixed performance on the reported multi-view tasks; in this paper the custom Multi-View DreamingV2 (authors' extension) outperformed DreamerV2 baselines on the complex Lift task.",
            "task_utility_analysis": "Discrete latent models can be advantageous in some settings; the paper shows that a multi-view categorical design (their MVDreamingV2) is especially effective on complex multi-view manipulation tasks compared to naive single-view discrete baselines.",
            "tradeoffs_observed": "Discrete latents can change learning dynamics and may be more effective when combined with multi-view contrastive alignment; however, performance depends on task and how multi-view fusion is implemented.",
            "design_choices": "Discrete/categorical latents, RSSM-style recurrence, latent imagination for policy learning.",
            "comparison_to_alternatives": "Benchmarked versus Dreamer and Dreaming-family models in Table I; authors' multi-view categorical extension exceeded DreamerV2 baseline in complex task.",
            "optimal_configuration": "Not provided here.",
            "uuid": "e1253.5",
            "source_info": {
                "paper_title": "Multi-view dreaming: multi-view world model with contrastive learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "PoE",
            "name_full": "Product of Experts (PoE) fusion",
            "brief_description": "A probabilistic method to combine multiple per-view latent distributions into a single global posterior by multiplying expert densities; applied here to fuse per-view posteriors into a global latent state.",
            "citation_title": "Training products of experts by minimizing contrastive divergence.",
            "mention_or_use": "use",
            "model_name": "Product of Experts fusion",
            "model_description": "Per-view posterior distributions (Gaussian or categorical) are combined into a single global posterior. For Gaussian latents the paper implements PoE by computing a weighted harmonic-mean style combination of means and precisions (equations provided); for categorical latents the paper averages per-dimension categorical distributions to form a fused discrete posterior.",
            "model_type": "probabilistic fusion method",
            "task_domain": "multi-view latent integration for world models in RL",
            "fidelity_metric": "No explicit fidelity metric reported for fusion itself; success of fusion measured indirectly via downstream RL performance and qualitative reconstructions.",
            "fidelity_performance": "Not numerically reported; authors attribute improved multi-view integration (qualitatively demonstrated via reconstructions) and improved performance in complex tasks to PoE fusion combined with contrastive alignment.",
            "interpretability_assessment": "Mathematically interpretable: PoE provides a principled probabilistic combination and integrates uncertainty across views; the implementation details (harmonic-mean for Gaussian) are explicit in the paper.",
            "interpretability_method": "Analytic formulae for Gaussian fusion and averaging for categorical fusion; qualitative reconstructed images shown from the fused latent.",
            "computational_cost": "Minimal additional compute to combine distributions; no explicit benchmarks provided.",
            "efficiency_comparison": "Not quantified; presented as an efficient, principled fusion technique compared to naive concatenation/overlay.",
            "task_performance": "Contributes to capability of system to aggregate partial observations across views; authors show that PoE fusion as part of MVDreaming family helps on complex multi-view tasks (empirical evidence in Lift).",
            "task_utility_analysis": "PoE enables combining heterogeneous partial observations into a single belief; utility depends on correct per-view posteriors and contrastive alignment to ensure posteriors represent the same latent.",
            "tradeoffs_observed": "Requires reliable per-view posteriors; if per-view encoders are poorly calibrated, multiplying densities could overconfidently collapse beliefs. The paper does not measure this effect quantitatively.",
            "design_choices": "Gaussian PoE implemented via harmonic-mean for mean/variance; categorical fusion implemented by averaging categorical logits/probabilities per dimension.",
            "comparison_to_alternatives": "Compared implicitly against naive overlay of images and simple concatenation baselines; PoE fusion combined with multi-view contrastive learning outperforms naive approaches in complex multi-view tasks in the experiments.",
            "optimal_configuration": "Not explicitly specified; authors adopt standard PoE formulas and recommend combining it with contrastive multi-view alignment.",
            "uuid": "e1253.6",
            "source_info": {
                "paper_title": "Multi-view dreaming: multi-view world model with contrastive learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "RSSM",
            "name_full": "Recurrent State-Space Model (RSSM)",
            "brief_description": "A recurrent latent-dynamics architecture used to learn stochastic latent states and deterministic recurrence for model-based RL; serves as the backbone dynamics model in Multi-View Dreaming and baseline Dreamer methods.",
            "citation_title": "Dream to control: Learning behaviors by latent imagination.",
            "mention_or_use": "use",
            "model_name": "RSSM (recurrent stochastic latent dynamics)",
            "model_description": "Architecture includes deterministic recurrent state h_t = f_phi(h_{t-1}, z_{t-1}, a_{t-1}), a representation model q_phi(z_t | h_t, x_t) producing stochastic latents, and a transition predictor p_phi(ẑ_t | h_t) generating priors used for imagination; used together with reward predictors, actor and critic.",
            "model_type": "latent world model (recurrent/stochastic)",
            "task_domain": "visual control and model-based RL from pixels (multi-view extension in this paper)",
            "fidelity_metric": "Trained using multi-step KL overshooting and contrastive NCE objectives; no explicit predictive-MSE metrics reported in this manuscript.",
            "fidelity_performance": "No isolated fidelity statistics for RSSM alone; overall system performance reported via downstream returns in Table I.",
            "interpretability_assessment": "Black-box neural recurrent model; no interpretability analyses for RSSM components provided here.",
            "interpretability_method": "None reported beyond qualitative reconstructions from global latent.",
            "computational_cost": "Not specified numerically; RSSM is standard component in world models and computational cost depends on chosen network sizes (not provided).",
            "efficiency_comparison": "Not directly compared in paper; RSSM is used because prior works (Dreamer family) successfully used it for latent imagination.",
            "task_performance": "RSSM-based models (Dreamer-family and MVDreaming variants) achieve the reported RL returns; RSSM enables latent prediction/imagination used by actor/critic.",
            "task_utility_analysis": "RSSM provides the temporal dynamics model enabling imagination and multi-step planning; in multi-view setting per-view encoders feed into the RSSM to permit temporal integration across views.",
            "tradeoffs_observed": "RSSM introduces stochastic/deterministic splitting that requires careful training (KL balance, overshooting). Paper inherits these issues but does not analyze them in depth.",
            "design_choices": "Deterministic recurrent core for temporal context; stochastic latent for uncertainty; representation model conditions on h_t and current images; transition predictor provides prior for latent imagination.",
            "comparison_to_alternatives": "Chosen as standard backbone due to prior success (Dreamer); specific alternatives (e.g., pure feedforward predictors, transformers) are not evaluated in this paper.",
            "optimal_configuration": "Not specified; authors use RSSM in the same manner as Dreaming/Dreamer family implementations.",
            "uuid": "e1253.7",
            "source_info": {
                "paper_title": "Multi-view dreaming: multi-view world model with contrastive learning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Multi-view contrastive",
            "name_full": "Multi-View Contrastive Learning for shared latent space",
            "brief_description": "A contrastive representation-learning procedure that treats simultaneous images from different viewpoints of the same scene as positive pairs and images from different times as negatives, used to align per-view representations in a shared latent space.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Multi-view contrastive learning (NCE)",
            "model_description": "Extends standard image-augmentation contrastive objectives by using cross-view positive pairs: for images x_v1,t and x_v2,t taken at the same time from different cameras the representation model is trained to bring their latents closer (via NCE/cross-entropy discriminative loss), while images from other times serve as negatives. This alignment is integrated into the decoder-free ELBO-style objective.",
            "model_type": "representation learning / auxiliary objective",
            "task_domain": "multi-view visual representation learning for model-based RL",
            "fidelity_metric": "Contrastive NCE loss (J_NCE_k) used as a training objective; no standalone numeric contrastive accuracy or AUROC is reported.",
            "fidelity_performance": "Not numerically reported beyond its impact on downstream RL returns where multi-view contrastive learning contributes to improved multi-view fusion and better policy performance in complex tasks.",
            "interpretability_assessment": "Aligns representations across views so that global latent is view-invariant; interpretability relies on qualitative reconstructions rather than quantitative disentanglement analyses.",
            "interpretability_method": "Visualization of reconstructions from global latent and qualitative claims that latents from different views are close when they represent same scene.",
            "computational_cost": "Not reported; contrastive learning typically requires sampling negative examples and additional encoder passes but no precise compute figures are given.",
            "efficiency_comparison": "Not quantified; authors argue multi-view contrastive learning is beneficial especially when naive image-overlay baselines fail on complex tasks.",
            "task_performance": "Improves downstream RL performance in multi-view complex tasks when combined with PoE and categorical latent variants (empirical claim supported by Lift task results).",
            "task_utility_analysis": "Key to learning a shared latent across viewpoints, enabling PoE fusion to produce a coherent global belief; effective particularly when different views provide complementary partial observations.",
            "tradeoffs_observed": "Requires careful selection of positives/negatives (simultaneous multi-view frames as positives); increases representation learning complexity and reliance on negative sampling strategies; exact sensitivity not quantified in paper.",
            "design_choices": "Positive pairs: different views at same time + random crop augmentations; negatives: images from other times (same/different views); integrated into ELBO-derived objective as J_NCE_k terms.",
            "comparison_to_alternatives": "Compared qualitatively to time-contrastive and overlay baselines; multi-view contrastive + PoE provides improved performance on complex tasks compared to naive overlaying of views.",
            "uuid": "e1253.8",
            "source_info": {
                "paper_title": "Multi-view dreaming: multi-view world model with contrastive learning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Dreaming: Model-based reinforcement learning by latent imagination without reconstruction.",
            "rating": 2,
            "sanitized_title": "dreaming_modelbased_reinforcement_learning_by_latent_imagination_without_reconstruction"
        },
        {
            "paper_title": "Dreamingv2: Model-based reinforcement learning with discrete world models without reconstruction.",
            "rating": 2,
            "sanitized_title": "dreamingv2_modelbased_reinforcement_learning_with_discrete_world_models_without_reconstruction"
        },
        {
            "paper_title": "Dream to control: Learning behaviors by latent imagination.",
            "rating": 2,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        },
        {
            "paper_title": "Mastering atari with discrete world models.",
            "rating": 2,
            "sanitized_title": "mastering_atari_with_discrete_world_models"
        },
        {
            "paper_title": "PlaNet",
            "rating": 1
        },
        {
            "paper_title": "Training products of experts by minimizing contrastive divergence.",
            "rating": 1,
            "sanitized_title": "training_products_of_experts_by_minimizing_contrastive_divergence"
        },
        {
            "paper_title": "CURL: Contrastive unsupervised representations for reinforcement learning.",
            "rating": 1,
            "sanitized_title": "curl_contrastive_unsupervised_representations_for_reinforcement_learning"
        },
        {
            "paper_title": "Multi-modal mutual information (MUMMI) training for robust self-supervised deep reinforcement learning.",
            "rating": 1,
            "sanitized_title": "multimodal_mutual_information_mummi_training_for_robust_selfsupervised_deep_reinforcement_learning"
        }
    ],
    "cost": 0.02510575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multi-View Dreaming: Multi-View World Model with Contrastive Learning
15 Mar 2022</p>
<p>Akira Kinose kinose.akira@jp.panasonic.com 
Akira Kinose is with Innovation Center
Connected Solutions Com-pany, Panasonic Corporation
Japan</p>
<p>Masashi Okada 
Technology Division
Masashi Okada and Tadahiro Taniguchi are with Digital &amp; AI Technology Center
Panasonic Corporation
Japan</p>
<p>Ryo Okumura 
Technology Division
Masashi Okada and Tadahiro Taniguchi are with Digital &amp; AI Technology Center
Panasonic Corporation
Japan</p>
<p>Tadahiro Taniguchi 
Technology Division
Masashi Okada and Tadahiro Taniguchi are with Digital &amp; AI Technology Center
Panasonic Corporation
Japan</p>
<p>College of Information Science and Engineering
Tadahiro Taniguchi is also with Ritsumeikan University
Japan</p>
<p>Multi-View Dreaming: Multi-View World Model with Contrastive Learning
15 Mar 2022F7A09D8FE06EDB1FEFB2E923171111A9arXiv:2203.11024v1[cs.AI]
In this paper, we propose Multi-View Dreaming, a novel reinforcement learning agent for integrated recognition and control from multi-view observations by extending Dreaming.Most current reinforcement learning method assumes a single-view observation space, and this imposes limitations on the observed data, such as lack of spatial information and occlusions.This makes obtaining ideal observational information from the environment difficult and is a bottleneck for real-world robotics applications.In this paper, we use contrastive learning to train a shared latent space between different viewpoints, and show how the Products of Experts approach can be used to integrate and control the probability distributions of latent states for multiple viewpoints.We also propose Multi-View DreamingV2, a variant of Multi-View Dreaming that uses a categorical distribution to model the latent state instead of the Gaussian distribution.Experiments show that the proposed method outperforms simple extensions of existing methods in a realistic robot control task.</p>
<p>I. INTRODUCTION</p>
<p>It would be desirable to have a vision-based control system that can manipulate objects in environments where there are many blind spots and image observation is limited.In the case of a robot grasping an object on a complicated shelf, the robot must be able to control it by observing images from various cameras.</p>
<p>By contrast, most current reinforcement learning method assumes a single-view observation space, and this imposes limitations on the observed data, such as lack of spatial information and occlusions.This makes obtaining ideal observational data from the environment difficult, resulting in problems like missing observational data.This problem has become a bottleneck for real-world robotics applications.</p>
<p>Therefore, our goal in this research is to realize a method for learning control based on observations from multiple viewpoints.When solving this problem, it will be more useful for robot control in factories where multiple cameras can be installed, as well as automatic driving control where viewing information from multiple directions is required.Multi-view reinforcement learning can also be applied to research problems such as robustness to sensor degradation and multimodal data fusion.To address this problem, it is crucial to develop a model-based reinforcement learning method, which enables integrated recognition and control from multi-view observations.Overview of Multi-View Dreaming, the proposed world models approach.Multi-View Dreaming trains a shared latent space between different viewpoints using contrastive learning.Then, Multi-View Dreaming infers the global latent state by the Product of Experts for multiple latent state distributions.By using the global latent state as input observations, the agent can train a policy based on multiple viewpoint observations through reinforcement learning.</p>
<p>However, model-based reinforcement learning frameworks for multi-view control have not yet been established.TCN [1] and mfTCN [2], but both of them do not consider the partial observability of the environment and do not train latent state dynamics.MuMMI [3] is multimodal reinforcement learning, which is highly relevant to this research.Compared to MuMMI, our focus is to (1) proposing "multi-view" world model which is highly needed in real-world robotics applications, and (2) systematically applying the theory to Dreaming [4] and DreamingV2 [5] to verify its effectiveness.</p>
<p>In this paper, we propose Multi-View Dreaming, a modelbased reinforcement learning for control based on multiview observations.Multi-View Dreaming is a novel world model approach for integrated recognition and control from multi-view observations by extending Dreaming.Fig. 1 shows an overview diagram of Multi-View Learning.We use contrastive learning to train a shared latent space between different viewpoints, and show how the Products of Experts approach can be used to integrate and control the probability distributions of latent states for multiple viewpoints.The proposed method is the extension of Dreaming [4] and DreamingV2 [5] to multi-view control.DreamingV2 focuses on representing latent states as categorical variables, while Dreaming focuses on making the dreamer decoder-free.The goal of this paper is to develop a multi-view approach to these approaches.</p>
<p>The key contributions of this paper are summarized as follows:</p>
<p>• Learning world models from multi-view observations.Using contrastive learning, the proposed Multi-View Dreaming and its categorical version variant, Multi-View DreamingV2, train a shared latent space between different viewpoints.They then use the Product of Experts to infer the global latent state for multiple latent state distributions.</p>
<p>• Practical experiments for visual control.We demonstrate the effectiveness of the proposed method in some scenarios, which correspond to real-world problems and realistic robot control tasks.The remainder of this paper is organized as follows.In Sec.II, key differences from related work are discussed.In Sec.III, our proposed method Multi-View Dreaming / Multi-View DreamingV2 is specified.In Sec.IV, the effectiveness of our proposed methods is demonstrated via simulated evaluations.Finally, In Sec.V, concludes this paper.</p>
<p>II. RELATED WORKS World Models</p>
<p>Our research focused on learning world models and policies from high-dimensional observations in partially observable Markov decision process (POMDP [6]).Several approaches have been proposed to learn latent space dynamcis models and use them to solve POMDP in model-based RL [7], [8].</p>
<p>World models are a model-based reinforcement learning that uses observation data to learn a predictive model based on the agent's behavior.World model trains a latent state dynamics model from the agent's experience, which is used to learn behavior.It is advantageous to learn a compact state representation for high-dimensional input information such as images and to use world models to predict the future in latent space.Representative studies are the World Models [9], SLAC [10], PlaNet [11], PlaNet-Bayes [12] Dreamer [13], DreamerV2 [14], Dreaming [4], DreamingV2 [5], etc. Learning from multiple observations is important in realworld robotics applications, but these methods do not address this issue.</p>
<p>The proposed method can also be seen as an extension of the world model to multi-view control, as it can infer the state representation from image observations and predict the future state of the environment in time series using a latent dynamics model.Our method is especially based on Dreaming [5].</p>
<p>Contrastive Learning in RL</p>
<p>Contrastive learning [15], [16] is a self-supervised learning framework for learning useful representations by imposing similarity constraints on the latent space between training data.</p>
<p>In contrastive learning, the distance of image pairs in the latent space is represented as a loss function.Contrastive Learning trains image pairs with similarity constraints in the latent space so that they are close to each other if they are data augmented instances, and far from each other if they are different instances.</p>
<p>Works that using contrastive learning for reinforcement learning include CURL [17], Dreaming [4], CFM [18], CVRL [19], TPC [20].</p>
<p>Multi-View Learning in RL</p>
<p>Several reinforcement learning methods have been proposed to train a policy based on observed data from multiple modalities [1]- [3], [21], [22].</p>
<p>MuMMI [3] is a research that is particularly relevant to this paper.In contrast to MuMMI, what is particularly important in this work is that (1) this focuses on "multiview", which is highly needed in real applications to robots, and (2) systematically applies the theory to Dreamer(V2) and Dreaming(V2) to verify its effectiveness.</p>
<p>TCN [1] and mfTCN [2] are examples of research dealing with multi-view contrastive learning, but both of them treating multi-view image embeddings as states, and using them to learn a policy.However, they do not sufficiently account for the fact that the environment is a POMDP and do not train a latent dynamics model.The difference between our method and these studies is that our method is model-based and can predict future states in time series, and it can integrate state representations deduced from multiple viewpoints.</p>
<p>III. MULTI-VIEW DREAMING</p>
<p>In this paper, we present Multi-View Dreaming, a modelbased reinforcement learning method with world models that learns latent dynamics and a policy from multi-view observations, as an extension of Dreaming [4].Fig. 2 shows a detailed diagram of the proposed method.In this method, we apply contrastive learning between multi-view observations to train a world model, based on the idea that images obtained from multi-view observations are augmentations of the same environment instance.Therefore, images from multi-view observations are trained to be close to each other in latent space at the same time.To recognize that observations from different viewpoints have the same latent state, the agent learns world models.</p>
<p>World Model learning based on RSSM</p>
<p>The world model can learn a predictive model from the agent's experience and use the prediction model to learn the behavior.Compact state representations are learned when trained on high-dimensional observations as images, allowing forward predictions in the learned latent space.This kind of model that predicts the future on latent space is called the latent dynamics model.By modeling the latent dynamics model of the environment, the agent can predict the long-term future and optimize its behavior without image reconstructions.</p>
<p>Multi-View Dreaming consists of a recurrent model (RSSM) to predict forward dynamics in partially observable environments, and a reward predictor.RSSM is an important component for learning latent dynamics, and it has been used in many world models [4], [13], [14].The model components are:
RSSM                Recurrent model: h t = f φ (h t−1 , z t−1 , a t−1 ) Representation model: z t ∼ q φ (z t | h t , x t ) Transition predictor: ẑt ∼ p φ (ẑ t | h t )(1)
Reward predictor:
rt ∼ p (r t | h t , z t )(2)
Actor: ât ∼ p ψ (â t | ẑt ) t and z 2 t for each viewpoint from current images x 1 t , x 2 t for each viewpoint and recurrence states ht.The global posterior probability state zt is calculated from the posterior probability states z 1 t and z 2 t of each viewpoint by Product of Experts.The Transition predictor calculates ẑt, a prior probability state that attempts to predict the posterior probability state without accessing the current image.This method uses the same decoderfree world model as Dreaming, but we train the decoder experimentally without computing the gradient to the loss function.
Critic: v ξ (ẑ t ) ≈ E p φ ,p ψ   τ ≥t γτ−t rτ  (3
Fig. 3 illustrates the detailed model architecture of Multi-View Dreaming.In the proposed model, latent states of the multiple viewpoints z 1 t , z 2 t are integrated (details in the next section) to global stochastic latent state z t .</p>
<p>Integration of latent state distributions</p>
<p>In this section, we explain how to integrate multiple latent state distributions.</p>
<p>1) Multi-View Dreaming (Gaussian): The RSSM based on Dreamer assumes a Gaussian distribution for the latent state distribution.By integrating the latent states of each of the multiple viewpoints into the global latent state, the global latent state can be seen as representing the true latent state of the environment.The stochastic state z t integrates the states of multiple viewpoints by taking a weighted harmonic mean over the mean µ and variance σ of the normal distribution, as shown in the following equation:
µ V = V v=1 µv σ 2 v V v=1 1 σ 2 v , σ 2 V = 1 V v=1 1 σ 2 v (4)
where V denotes the number of viewpoints.It is inspired by the Products of Experts [23] proposed by Hinton.The idea is to multiply the density functions of multiple probability distributions (experts) to combine them.</p>
<p>論文 Fig.4
View1 View2 View1 View2 View1 View2
Fig. 4. Reacher-easy task uses a continuous action space with 2 dimensions (left).viewpoint 1 blinds the right half of the screen, and viewpoint 2 blinds the left half.Pendulum-swingup task uses a continuous action space with 1 dimensions (center).viewpoint 1 observes the pendulum from directly above, and viewpoint 2 observes the pendulum from directly beside.Lift-Panda task uses a continuous action space with 7 dimensions (right).viewpoint 1 observes the table from the front, and viewpoint 2 observes the table from the side.</p>
<p>In this way, the agent can choose an action based on several dimensions without covering the full dimensionality of the latent state.</p>
<p>2) Multi-View DreamingV2 (Categorical): In this paper, we also propose a variant of Multi-View Dreaming that uses a categorical distribution to model the latent state instead of the Gaussian distribution that was proposed in Dream-erV2.There has been no prior research that uses categorical distributions for latent states of the world model to learn multi-view observations to our knowledge.In this paper, we call this method Multi-View DreamingV2.Multi-View DreamingV2 is based on DreamingV2 instead of Dreaming and is extended to multi-view observations.Because the latent state in Multi-View DreamingV2 is categorical rather than Gaussian, the Product of Experts is calculated by averaging each dimension of the categorical distribution.</p>
<p>Multi-View Contrastive Learning</p>
<p>As described in the previous section, to integrate the latent space in multiple viewpoints, it is necessary to learn a common world model across all viewpoint.We propose to learn a common world model for all viewpoints by using contrastive learning between viewpoints for representation model.</p>
<p>The objective function of Multi-View Dreaming is basically the same as that of Dreaming [4].Dreaming introduces a reconsruction-free objective derived from the ELBO objective:
J Dreaming := K k=0 J NCE k + J KL k (5)
where K represents the overshooting distance.J KL k is a multi-step objective and J N CE k is a categorical cross entropy objective to discriminate positive pair (z t , x t ) and negative pair (z t , x ( = x t )) as shown below:
J N CE k := E p(zt|z t−k ,a&lt;t)q(z t−k |•) log p (z t | x t ) − log x p (z t | x ) (6) Dreaming calculates J N CE k
by random image augmentation using image cropping.</p>
<p>In our method, images from each viewpoint observed at the same time are selected in addition to random cropping data augmentation to increase the number of positive sample pairs.The negative samples are also sampled from images from different viewpoints observed at different times.This is based on the intuition that random cropping in contrastive learning can be regarded as equivalent to a change in viewpoint in a reinforcement learning task.Even when the viewpoint and observation image are different, we believe that the latent space representations of the same scene should be close together.Therefore, by embedding the latent states of different viewpoints at the same time in close proximity, these integrated latent states will be closer to the true latent states.We call this approach as multi-view contrastive learning.</p>
<p>As shown Fig. 2, the image pairs of each viewpoint at the same time are positive samples, and positive samples are image pairs taken from each viewpoint at the same time, and the latent space is learned so that these images are close to each other.Negative samples are image pairs of the same and different viewpoints at different times, and the latent space is learned so that these images are far apart.</p>
<p>IV. EXPERIMENTS</p>
<p>As shown Fig. 4, we evaluated Multi-View Dreaming's effectiveness in two scenarios that mimic real-world prob- lems.We also used Robosuite to demonstrate the proposed method's effectiveness in a real-world robot control task.</p>
<p>Experimental Settings</p>
<p>Our main baselines are Dreamer [13], DreamerV2 [14], Dreaming [4], DreamingV2 [5], the representative modelbased reinforcement learning methods.However, we did not simply compare the single-view and multi-view approaches, but also extended the baseline as follows: a simple extension of the baseline by overlaying multi-view images in the color channel direction as input images.For example, if the image of the 64(height)×64(width)×3(color) array is observed from two viewpoints, the agent will observe the image of the 64(height)×64(width)×6(color) array after overlaying.Multi-View Dreaming is implemented based on DreamingV2.Therefore, the elements proposed in the research up to DreamingV2 will be inherited in this method.</p>
<p>We experimented with the three tasks shown in Fig. 4. In all tasks, observations are pixel inputs (64×64) only.Reacher task and Pendulum task are provided from the DeepMind Control Suite [24], Lift task is provided from the Robosuite [25], but environments were augmented to provide images from multiple viewpoints.</p>
<p>Scenario: Blind Reacher</p>
<p>In this experiment, we assume that occlusion occurs on the observed images and that the critical information required for the task is not available from a single viewpoint.Specifically, as Fig. 4 shows, viewpoint 1 blinds the right half of the screen, and viewpoint 2 blinds the left half.We handle blinds by filling them with black pixels, so the image size is unchanged.In this scenario, learning the policy from a single viewpoint image is insufficient to complete the task; instead, the policy must be learned by combining information from multiple viewpoints.</p>
<p>Scenario: Dual View Pendulum</p>
<p>In this experiment, we assume a scenario where the camera position in the environment is changed from the default and only limited information of the task is available from a single viewpoint.Viewpoint 1 observes the pendulum from directly above, while viewpoint 2 observes the pendulum from directly beside, as shown in Fig. 4. It is difficult to achieve any of these viewpoints with only a single camera in one direction, and it is necessary to learn the policy by combining information from two viewpoints located in different places.</p>
<p>Scenaro: Robosuite Lift</p>
<p>We used Robosuite in this experiment to test the effectiveness of our method in a realistic robot control task.In a realistic robot task, the robot's body and arms act as obstacles in the observation space, necessitating multi-modal control from multiple viewpoints.</p>
<p>Experimental Results</p>
<p>Fig. 5, 6, 7 show the training progress for each scenario, and the final performance scores are shown in Table 1.</p>
<p>In the Blind Reacher scenario, Multi-View DreamingV2 learned policies by using the images from the two viewpoints well.However, the performance was comparable to simple extensions of Dreamer, Dreaming, and DreamingV2.Multi-View Dreaming did not perform as well as the full observation.</p>
<p>In the Dual View Pendulum scenario, all methods except DreamerV2 and Multi-View DreamingV2 performed equally well in learning.</p>
<p>In the Robosuite Lift scenario, Multi-View DreamingV2 outperformed all other approaches by a significant difference.Whereas the previous two experiments did not differ from baseline, Multi-View DreamingV2 was particularly effective in this task.</p>
<p>This suggests that it is difficult to separate the necessary information for a task like Lift, which involves complex image information and different dynamics between viewpoints, using a simple method of overlaying images in the color channel direction, and that the proposed method is effective in estimating the latent state for each viewpoint.</p>
<p>A reconstructed image from the global latent state is shown in Fig. 8.The global latent state combines and embeds both the important information of viewpoint 1 and viewpoint 2. This can be qualitatively confirmed.</p>
<p>V. CONCLUSION</p>
<p>In this paper, we proposed a novel world model approach for integrated recognition and control from multi-view observations by extending Dreaming.We used contrastive learning to train a shared latent space between different viewpoints, and showed how the Products of Experts approach can be used to integrate and control the probability distributions of latent states for multiple viewpoints.</p>
<p>We demonstrated the effectiveness of the world model using multi-view observations in two scenarios that correspond to problems in real environments and in realistic robot control tasks.In conclusion, simple extensions of methods of overlapping images are effective for simple tasks, but a multi-view contrastive learning approach is more effective for tasks with complex images and dynamics.Understanding the features of these methods revealed in this study, as well as making effective use of multi-view, is important for practical applications in robotics.Theoretical causes of these differences will be the subject of future research.</p>
<p>Our method can be seen as an example of a multiview version of the generalized multimodal world model.In addition to multi-view images, world models that incorporate more modalities such as audio, tactile sensing, and depth sensors would be an intriguing one which could be usefully explored in further research.In addition, embedding domain information such as camera coordinates and robot proprioception into the latent state of each viewpoint would be a fruitful area for further work.</p>
<p>We were not able to experiment with the real robot in this paper, but we are currently working on real robotics application, and evaluating it will be a future issue.</p>
<p>Fig. 1.Overview of Multi-View Dreaming, the proposed world models approach.Multi-View Dreaming trains a shared latent space between different viewpoints using contrastive learning.Then, Multi-View Dreaming infers the global latent state by the Product of Experts for multiple latent state distributions.By using the global latent state as input observations, the agent can train a policy based on multiple viewpoint observations through reinforcement learning.</p>
<p>Fig. 2 .
2
Fig. 2. Detailed diagram of multi-view contrastive learning.The image pairs of each viewpoint at the same time (green arrows ↔) are positive samples, and the latent space is learned so that these images are located close to each other.The image pairs of the same and different viewpoints at different times (red arrows ↔) are negative samples, and the latent space is learned so that these images are located far from each other.</p>
<p>Fig. 3 .
3
Fig. 3. Model Architecture of Multi-View Dreaming.In this figure, assume that the model observes images from two viewpoints at the same time.The training image x 1 t , x 2 t for each viewpoint is encoded using a shared encoder.The RSSM uses a sequence of deterministic recurrent states ht .At each step, this model infers global posterior probability states zt and prior probability states ẑt.The representation model infers posterior probability states z 1t and z 2 t for each viewpoint from current images x 1 t , x 2 t for each viewpoint and recurrence states ht.The global posterior probability state zt is calculated from the posterior probability states z 1 t and z 2 t of each viewpoint by Product of Experts.The Transition predictor calculates ẑt, a prior probability state that attempts to predict the posterior probability state without accessing the current image.This method uses the same decoderfree world model as Dreaming, but we train the decoder experimentally without computing the gradient to the loss function.</p>
<p>Fig. 5 .Fig. 6 .Fig. 7 .
567
Fig. 5. Training progress of Blind Reacher scenario</p>
<p>Fig. 8 .
8
Fig. 8. Observed image from viewpoint 1 (left).Observed image from viewpoint 2 (center).Reconstructed image from the global latent state (right).</p>
<p>TABLE I
I
Comparison of Multi-View Dreaming / V2 to simple extension of conventional model-based reinforcement learning by final performance score.
MVDreaming MVDreamingV2DreamerDreamerV2DreamingDreamingV2(ours)(ours)[13][14][4][5]Multi-ViewGaussian LatentCategorical LatentDecoder-freeReacher (multi-view)588.6±356.0936.1±95.9685.0±410.742.2±73.3841.3±316.7860.9±285.6Pendulum (multi-view) 812.2±130.4256.5±304.5801.2±110.410.8±24.5831.6±126.4 410.4±413.99Lift (single-view)--133.5±64.28132.6±62.4150.5±78.5330.9±113.6Lift (multi-view)110.1±44.6345.0±133.6102.9±82.8120.8±51.58177.0±80.9254.7±104.2</p>
<p>Time-contrastive networks: Self-supervised learning from video. P Sermanet, C Lynch, Y Chebotar, J Hsu, E Jang, S Schaal, S Levine, G Brain, International Conference on Robotics and Automation (ICRA). 2018</p>
<p>Learning actionable representations from visual observations. D Dwibedi, J Tompson, C Lynch, P Sermanet, 2018</p>
<p>Multi-modal mutual information (MUMMI) training for robust self-supervised deep reinforcement learning. K Chen, Y Lee, H Soh, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Dreaming: Model-based reinforcement learning by latent imagination without reconstruction. M Okada, T Taniguchi, International Conference on Robotics and Automation (ICRA). 2021</p>
<p>Dreamingv2: Model-based reinforcement learning withdiscrete world models without reconstruction. M Okada, T Taniguchi, arXiv:2203.004942002arXiv preprint</p>
<p>Planning and acting in partially observable stochastic domains. L P Kaelbling, M L Littman, A R Cassandra, Artificial intelligence. 1011-21998</p>
<p>SOLAR: Deep structured representations for model-based reinforcement learning. M Zhang, S Vikram, L Smith, P Abbeel, M Johnson, S Levine, International Conference on Machine Learning (ICML). 2019</p>
<p>Variational temporal abstraction. T Kim, S Ahn, Y Bengio, Neural Information Processing Systems. 2019</p>
<p>Recurrent world models facilitate policy evolution. D Ha, J Schmidhuber, Neural Information Processing Systems. 2018</p>
<p>Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. A X Lee, A Nagabandi, P Abbeel, S Levine, arXiv:1907.009532019arXiv preprint</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, International Conference on Machine Learning (ICML). 2019</p>
<p>PlaNet of the Bayesians: Reconsidering and improving deep planning network by incorporating Bayesian inference. M Okada, N Kosaka, T Taniguchi, 2020</p>
<p>Dream to control: Learning behaviors by latent imagination. D Hafner, T Lillicrap, J Ba, M Norouzi, International Conference on Learning Representations (ICLR). 2020</p>
<p>Mastering atari with discrete world models. D Hafner, T Lillicrap, M Norouzi, J Ba, International Conference on Learning Representations (ICLR). 2021</p>
<p>Representation learning with contrastive predictive coding. A V D Oord, Y Li, O Vinyals, arXiv:1807.037482018</p>
<p>A simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, International Conference on Learning Representations (ICLR). 2020</p>
<p>CURL: Contrastive unsupervised representations for reinforcement learning. A Srinivas, M Laskin, P Abbeel, International Conference on Machine Learning (ICML). 2020</p>
<p>Learning predictive representations for deformable objects using contrastive estimation. W Yan, A Vangipuram, P Abbeel, L Pinto, arXiv:2003.054362020</p>
<p>Contrastive variational model-based reinforcement learning for complex observations. X Ma, S Chen, D Hsu, W S Lee, Conference on Robot Learning (CoRL). 2020</p>
<p>Temporal predictive coding for model-based planning in latent space. T D Nguyen, R Shu, T Pham, H Bui, S Ermon, International Conference on Machine Learning (ICML). 2021</p>
<p>Making sense of vision and touch: Self-supervised learning of multimodal representations for contactrich tasks. M A Lee, Y Zhu, K Srinivasan, P Shah, S Savarese, L Fei-Fei, A Garg, J Bohg, International Conference on Robotics and Automation (ICRA). </p>
<p>Multi-view reinforcement learning. M Li, L Wu, J Wang, H Bou Ammar, Neural Information Processing Systems. 2019</p>
<p>Training products of experts by minimizing contrastive divergence. G E Hinton, Neural computation. 2002</p>
<p>DeepMind control suite. Y Tassa, Y Doron, A Muldal, T Erez, Y Li, arXiv:1801.006902018arXiv preprint</p>
<p>robosuite: A modular simulation framework and benchmark for robot learning. Y Zhu, J Wong, A Mandlekar, R Martín-Martín, arXiv:2009.122932020</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8570 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8570</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8570</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-237940562</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2021.emnlp-main.110.pdf" target="_blank">RuleBERT: Teaching Soft Rules to Pre-Trained Language Models</a></p>
                <p><strong>Paper Abstract:</strong> While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8570.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8570.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (Robustly optimized BERT pretraining approach)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large bidirectional Transformer encoder pre-trained with masked language modeling; used in this paper as the base PLM (RoBERTa-LARGE) and as a baseline for fine-tuning experiments on logical reasoning with soft Horn rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RoBERTa: A robustly optimized BERT pretraining approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-LARGE</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Bidirectional Transformer encoder pretrained with masked language modeling; chosen because it is trained on more data than BERT and has stronger positional/encoding properties for these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈355M parameters (RoBERTa-LARGE; paper reports ~355M total parameters for models used)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Synthetic soft-Horn-rule probabilistic entailment (single-rule / overlapping rules / chaining), plus external tests: bAbI Task #15, Negated LAMA, CheckList QQP</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Binary classification task: given natural-language facts and (soft) Horn rules as context and a hypothesis, predict whether hypothesis holds and (via training target) produce a probability matching a symbolic probabilistic reasoner (LP MLN). Tasks include single-rule inference, overlapping/conflicting soft rules, multi-step (chained) inference up to depth 5, and external behavioral/entailment tests (negation, symmetry, bAbI deductive reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning the pre-trained RoBERTa on synthetic datasets generated from mined Horn rules; baseline uses standard binary cross-entropy (BCE) while the improved approach uses a weighted BCE (wBCE) loss that injects LP MLN-derived probabilistic weights into the training loss.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>As baseline (standard fine-tuning) RoBERTa achieves high raw accuracy on single-rule synthetic tasks (near 0.99 on some rules) but worse calibration/CA@k than wBCE variants; on bAbI Task #15 RoBERTa reached ≈0.676 accuracy after 2 epochs (improving to 0.827 after 3 epochs under same hyperparameters), and on CheckList QQP RoBERTa baseline reached 0.0 on the hard CheckList test in first-epoch comparison reported.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly to the same architecture fine-tuned with the proposed weighted loss (RoBERTa-wBCE / RULEBERT). RoBERTa (standard BCE) is outperformed by the wBCE-fine-tuned models in measures of calibrated confidence (CA@k) and often in accuracy; in external tasks RULEBERT (wBCE-fine-tuned) outperforms RoBERTa on Negated LAMA and (temporarily) on CheckList QQP and bAbI under low-data / few-epoch settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>As a vanilla baseline, standard RoBERTa fine-tuned with BCE shows worse calibration (probability estimation) on soft-rule probabilistic outputs, and worse behavioral performance on negation/symmetry tests compared to the wBCE fine-tuned model; suffers from typical fine-tuning instability on small datasets (high variance reported for small bAbI splits).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Pretrained RoBERTa contains representational capacity for simple deductive patterns but needs targeted fine-tuning and a loss that encodes probabilistic rule uncertainty to produce well-calibrated probabilistic entailments; naive fine-tuning (BCE) yields high raw accuracy but poorer confidence calibration and weaker transfer to negation/symmetry tests.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8570.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8570.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RULEBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RULEBERT (RoBERTa fine-tuned to reason with soft Horn rules)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RoBERTa-LARGE model fine-tuned on large synthetic datasets generated from soft Horn rules, trained with a revised weighted binary cross-entropy loss that uses LP MLN-derived probabilities so the model predicts calibrated probabilities for hypotheses under soft rules.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RULEBERT (fine-tuned RoBERTa-LARGE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa-LARGE architecture (~355M params) fine-tuned on 3.2M synthetic examples derived from 161 mined soft Horn rules; training uses a modified empirical risk (wBCE) where each example is treated as a weighted positive (weight w from LP MLN) and weighted negative (1-w) example so the PLM learns to output confidences aligned with a symbolic probabilistic reasoner (LP MLN).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈355M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Soft-rule probabilistic deductive reasoning (single-rule, overlapping conclusions, rule chaining up to depth 5) plus transfer tests: Negated LAMA (negation), CheckList QQP (symmetry/duplicate detection), bAbI Task #15 (deduction)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Predict whether a natural-language hypothesis follows from a set of natural-language facts and soft Horn rules, and output a probability that matches LP MLN's computed probability; tasks test deduction under uncertain/soft rules, conflict resolution among rules, and multi-hop chaining (depth-limited). External tests challenge predicate negation and symmetry and a standard deductive toy task (bAbI #15).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning on synthetic rule-derived datasets generated by feeding rules+facts to an LP MLN reasoner to obtain target probabilities; training with weighted binary cross-entropy (wBCE) that encodes LP MLN probabilities per example; datasets include single-rule (3.2M examples total for 161 rules), overlapping-rule (300K examples), and chaining (70K examples up to depth 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On synthetic tasks RULEBERT achieves very high accuracy and well-calibrated confidence (single-rule accuracies often ≈0.99 and CA@0.1 often >0.98 on tested rules). On overlapping rules and chaining it attains high F1/accuracy across many conditions though performance degrades for deeper unseen chaining depths. On transfer tasks, RULEBERT outperforms RoBERTa on Negated LAMA (lower mean Spearman ρ and much smaller % overlap between positive and negated answers) and achieves higher bAbI Task #15 accuracy in early epochs (RULEBERT: 0.863 after 2 epochs vs RoBERTa: 0.676), and on CheckList QQP RULEBERT hit 0.422 after one epoch vs RoBERTa 0.0 in the same one-epoch comparison (but catastrophic forgetting occurred after more epochs).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>RULEBERT (wBCE, multi-rule pre-fine-tuned) vs FT-PLM (individual rule-specific fine-tuned models): RULEBERT20 (fine-tuned on 20 rules) outperforms FT-PLM on unseen rules that share predicates with pre-finetuning rules; for rules with unseen predicates FT-PLM can perform better, and further fine-tuning RULEBERT20 on the target rule (FT-RULEBERT20) yields the best results. Compared to RoBERTa with standard BCE, RULEBERT yields better probability calibration (CA@k) and transfer on negation/symmetry tests.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance drops for chaining depths larger than those seen during training (models trained on low-depth chaining underperform on depth-5 chains); catastrophic forgetting observed when subsequently fine-tuning on downstream tasks (e.g., RULEBERT's initial QQP gains disappear after 3 epochs); synthetic-dataset training may not cover all natural language variability; dependence on LP MLN for ground-truth probabilities means the approach inherits any limitations/approximations of that reasoning formalism; mined rules (DBpedia) may introduce bias and incompleteness; some experiments show variability on small datasets (bAbI) requiring multiple runs.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Teaching PLMs soft-rule reasoning is effective when (i) training targets encode rule uncertainty (use of LP MLN weights) and (ii) large synthetic datasets expose diverse triggering and counterexamples; the weighted-loss formulation (wBCE) is crucial for calibrated probability prediction; learned logical notions (negation, symmetry) transfer to other tasks, but the approach needs sufficient coverage for multi-hop reasoning and is vulnerable to catastrophic forgetting when later fine-tuned on different tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8570.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8570.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FT-PLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-tuned PLM (per-rule fine-tuning baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline setup in which the pre-trained PLM (RoBERTa-LARGE) is fine-tuned separately for each target rule using examples specifically generated for that rule (rule-specific fine-tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FT-PLM (rule-specific fine-tuned RoBERTa-LARGE)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RoBERTa-LARGE fine-tuned on examples for a single rule (typically 4k training examples in the unseen-rule transfer experiments), using a standard fine-tuning procedure in the paper's experimental comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈355M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Single-rule probabilistic entailment (per-rule fine-tuning evaluation) and transfer experiments on unseen rules</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>For a given rule, the model is trained to output True/False for hypotheses under contexts generated to exercise that rule; in transfer experiments FT-PLM is a model trained only on the test rule (not on a pool of rules) and evaluated on that rule's examples.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Standard fine-tuning on synthetic examples generated for a single rule (no pre-fine-tuning on a multi-rule corpus), used as a point of comparison to multi-rule pre-finetuned models (RULEBERT20) and to evaluate transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Per-rule FT-PLM often attains high accuracy on its target rule (used as a strong per-rule baseline). In transfer tests on unseen rules that share no predicates with RULEBERT20's pre-fine-tuning set, FT-PLM outperforms RULEBERT20; quantitative per-rule numbers vary by rule but FT-PLM is reported to be better on the second group of unseen-predicate rules in Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to RULEBERT20: FT-PLM is weaker on unseen rules that share predicates with RULEBERT20's pre-finetuning set (RULEBERT20 generalizes better there), but stronger on rules with completely unseen predicates; further fine-tuning RULEBERT20 on the specific rule (FT-RULEBERT20) produces the best results overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>FT-PLM lacks the multi-rule inductive prior learned by RULEBERT and thus generalizes worse to new rules that reuse predicates seen during broader pre-fine-tuning; it requires per-rule training data and does not provide calibrated probabilistic outputs tied to LP MLN unless trained with the weighted loss.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Per-rule fine-tuning is a strong baseline for individual rules but is less efficient and less general than pre-finetuning on a diverse rule corpus; combining pre-finetuning across rules and then light per-rule adaptation (FT-RULEBERT) yields best performance, indicating transfer of logical primitives is possible.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Towards AI-complete question answering: A set of prerequisite toy tasks <em>(Rating: 2)</em></li>
                <li>Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly <em>(Rating: 1)</em></li>
                <li>Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8570",
    "paper_id": "paper-237940562",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "RoBERTa",
            "name_full": "RoBERTa (Robustly optimized BERT pretraining approach)",
            "brief_description": "A large bidirectional Transformer encoder pre-trained with masked language modeling; used in this paper as the base PLM (RoBERTa-LARGE) and as a baseline for fine-tuning experiments on logical reasoning with soft Horn rules.",
            "citation_title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "mention_or_use": "use",
            "model_name": "RoBERTa-LARGE",
            "model_description": "Bidirectional Transformer encoder pretrained with masked language modeling; chosen because it is trained on more data than BERT and has stronger positional/encoding properties for these experiments.",
            "model_size": "≈355M parameters (RoBERTa-LARGE; paper reports ~355M total parameters for models used)",
            "reasoning_task_name": "Synthetic soft-Horn-rule probabilistic entailment (single-rule / overlapping rules / chaining), plus external tests: bAbI Task #15, Negated LAMA, CheckList QQP",
            "reasoning_task_description": "Binary classification task: given natural-language facts and (soft) Horn rules as context and a hypothesis, predict whether hypothesis holds and (via training target) produce a probability matching a symbolic probabilistic reasoner (LP MLN). Tasks include single-rule inference, overlapping/conflicting soft rules, multi-step (chained) inference up to depth 5, and external behavioral/entailment tests (negation, symmetry, bAbI deductive reasoning).",
            "method_or_approach": "Fine-tuning the pre-trained RoBERTa on synthetic datasets generated from mined Horn rules; baseline uses standard binary cross-entropy (BCE) while the improved approach uses a weighted BCE (wBCE) loss that injects LP MLN-derived probabilistic weights into the training loss.",
            "performance": "As baseline (standard fine-tuning) RoBERTa achieves high raw accuracy on single-rule synthetic tasks (near 0.99 on some rules) but worse calibration/CA@k than wBCE variants; on bAbI Task #15 RoBERTa reached ≈0.676 accuracy after 2 epochs (improving to 0.827 after 3 epochs under same hyperparameters), and on CheckList QQP RoBERTa baseline reached 0.0 on the hard CheckList test in first-epoch comparison reported.",
            "baseline_comparison": "Compared directly to the same architecture fine-tuned with the proposed weighted loss (RoBERTa-wBCE / RULEBERT). RoBERTa (standard BCE) is outperformed by the wBCE-fine-tuned models in measures of calibrated confidence (CA@k) and often in accuracy; in external tasks RULEBERT (wBCE-fine-tuned) outperforms RoBERTa on Negated LAMA and (temporarily) on CheckList QQP and bAbI under low-data / few-epoch settings.",
            "limitations_or_failures": "As a vanilla baseline, standard RoBERTa fine-tuned with BCE shows worse calibration (probability estimation) on soft-rule probabilistic outputs, and worse behavioral performance on negation/symmetry tests compared to the wBCE fine-tuned model; suffers from typical fine-tuning instability on small datasets (high variance reported for small bAbI splits).",
            "insights_or_conclusions": "Pretrained RoBERTa contains representational capacity for simple deductive patterns but needs targeted fine-tuning and a loss that encodes probabilistic rule uncertainty to produce well-calibrated probabilistic entailments; naive fine-tuning (BCE) yields high raw accuracy but poorer confidence calibration and weaker transfer to negation/symmetry tests.",
            "uuid": "e8570.0"
        },
        {
            "name_short": "RULEBERT",
            "name_full": "RULEBERT (RoBERTa fine-tuned to reason with soft Horn rules)",
            "brief_description": "A RoBERTa-LARGE model fine-tuned on large synthetic datasets generated from soft Horn rules, trained with a revised weighted binary cross-entropy loss that uses LP MLN-derived probabilities so the model predicts calibrated probabilities for hypotheses under soft rules.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RULEBERT (fine-tuned RoBERTa-LARGE)",
            "model_description": "RoBERTa-LARGE architecture (~355M params) fine-tuned on 3.2M synthetic examples derived from 161 mined soft Horn rules; training uses a modified empirical risk (wBCE) where each example is treated as a weighted positive (weight w from LP MLN) and weighted negative (1-w) example so the PLM learns to output confidences aligned with a symbolic probabilistic reasoner (LP MLN).",
            "model_size": "≈355M parameters",
            "reasoning_task_name": "Soft-rule probabilistic deductive reasoning (single-rule, overlapping conclusions, rule chaining up to depth 5) plus transfer tests: Negated LAMA (negation), CheckList QQP (symmetry/duplicate detection), bAbI Task #15 (deduction)",
            "reasoning_task_description": "Predict whether a natural-language hypothesis follows from a set of natural-language facts and soft Horn rules, and output a probability that matches LP MLN's computed probability; tasks test deduction under uncertain/soft rules, conflict resolution among rules, and multi-hop chaining (depth-limited). External tests challenge predicate negation and symmetry and a standard deductive toy task (bAbI #15).",
            "method_or_approach": "Fine-tuning on synthetic rule-derived datasets generated by feeding rules+facts to an LP MLN reasoner to obtain target probabilities; training with weighted binary cross-entropy (wBCE) that encodes LP MLN probabilities per example; datasets include single-rule (3.2M examples total for 161 rules), overlapping-rule (300K examples), and chaining (70K examples up to depth 5).",
            "performance": "On synthetic tasks RULEBERT achieves very high accuracy and well-calibrated confidence (single-rule accuracies often ≈0.99 and CA@0.1 often &gt;0.98 on tested rules). On overlapping rules and chaining it attains high F1/accuracy across many conditions though performance degrades for deeper unseen chaining depths. On transfer tasks, RULEBERT outperforms RoBERTa on Negated LAMA (lower mean Spearman ρ and much smaller % overlap between positive and negated answers) and achieves higher bAbI Task #15 accuracy in early epochs (RULEBERT: 0.863 after 2 epochs vs RoBERTa: 0.676), and on CheckList QQP RULEBERT hit 0.422 after one epoch vs RoBERTa 0.0 in the same one-epoch comparison (but catastrophic forgetting occurred after more epochs).",
            "baseline_comparison": "RULEBERT (wBCE, multi-rule pre-fine-tuned) vs FT-PLM (individual rule-specific fine-tuned models): RULEBERT20 (fine-tuned on 20 rules) outperforms FT-PLM on unseen rules that share predicates with pre-finetuning rules; for rules with unseen predicates FT-PLM can perform better, and further fine-tuning RULEBERT20 on the target rule (FT-RULEBERT20) yields the best results. Compared to RoBERTa with standard BCE, RULEBERT yields better probability calibration (CA@k) and transfer on negation/symmetry tests.",
            "limitations_or_failures": "Performance drops for chaining depths larger than those seen during training (models trained on low-depth chaining underperform on depth-5 chains); catastrophic forgetting observed when subsequently fine-tuning on downstream tasks (e.g., RULEBERT's initial QQP gains disappear after 3 epochs); synthetic-dataset training may not cover all natural language variability; dependence on LP MLN for ground-truth probabilities means the approach inherits any limitations/approximations of that reasoning formalism; mined rules (DBpedia) may introduce bias and incompleteness; some experiments show variability on small datasets (bAbI) requiring multiple runs.",
            "insights_or_conclusions": "Teaching PLMs soft-rule reasoning is effective when (i) training targets encode rule uncertainty (use of LP MLN weights) and (ii) large synthetic datasets expose diverse triggering and counterexamples; the weighted-loss formulation (wBCE) is crucial for calibrated probability prediction; learned logical notions (negation, symmetry) transfer to other tasks, but the approach needs sufficient coverage for multi-hop reasoning and is vulnerable to catastrophic forgetting when later fine-tuned on different tasks.",
            "uuid": "e8570.1"
        },
        {
            "name_short": "FT-PLM",
            "name_full": "Fine-tuned PLM (per-rule fine-tuning baseline)",
            "brief_description": "A baseline setup in which the pre-trained PLM (RoBERTa-LARGE) is fine-tuned separately for each target rule using examples specifically generated for that rule (rule-specific fine-tuning).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FT-PLM (rule-specific fine-tuned RoBERTa-LARGE)",
            "model_description": "RoBERTa-LARGE fine-tuned on examples for a single rule (typically 4k training examples in the unseen-rule transfer experiments), using a standard fine-tuning procedure in the paper's experimental comparisons.",
            "model_size": "≈355M parameters",
            "reasoning_task_name": "Single-rule probabilistic entailment (per-rule fine-tuning evaluation) and transfer experiments on unseen rules",
            "reasoning_task_description": "For a given rule, the model is trained to output True/False for hypotheses under contexts generated to exercise that rule; in transfer experiments FT-PLM is a model trained only on the test rule (not on a pool of rules) and evaluated on that rule's examples.",
            "method_or_approach": "Standard fine-tuning on synthetic examples generated for a single rule (no pre-fine-tuning on a multi-rule corpus), used as a point of comparison to multi-rule pre-finetuned models (RULEBERT20) and to evaluate transfer.",
            "performance": "Per-rule FT-PLM often attains high accuracy on its target rule (used as a strong per-rule baseline). In transfer tests on unseen rules that share no predicates with RULEBERT20's pre-fine-tuning set, FT-PLM outperforms RULEBERT20; quantitative per-rule numbers vary by rule but FT-PLM is reported to be better on the second group of unseen-predicate rules in Table 5.",
            "baseline_comparison": "Compared to RULEBERT20: FT-PLM is weaker on unseen rules that share predicates with RULEBERT20's pre-finetuning set (RULEBERT20 generalizes better there), but stronger on rules with completely unseen predicates; further fine-tuning RULEBERT20 on the specific rule (FT-RULEBERT20) produces the best results overall.",
            "limitations_or_failures": "FT-PLM lacks the multi-rule inductive prior learned by RULEBERT and thus generalizes worse to new rules that reuse predicates seen during broader pre-fine-tuning; it requires per-rule training data and does not provide calibrated probabilistic outputs tied to LP MLN unless trained with the weighted loss.",
            "insights_or_conclusions": "Per-rule fine-tuning is a strong baseline for individual rules but is less efficient and less general than pre-finetuning on a diverse rule corpus; combining pre-finetuning across rules and then light per-rule adaptation (FT-RULEBERT) yields best performance, indicating transfer of logical primitives is possible.",
            "uuid": "e8570.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Towards AI-complete question answering: A set of prerequisite toy tasks",
            "rating": 2,
            "sanitized_title": "towards_aicomplete_question_answering_a_set_of_prerequisite_toy_tasks"
        },
        {
            "paper_title": "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
            "rating": 1,
            "sanitized_title": "negated_and_misprimed_probes_for_pretrained_language_models_birds_can_talk_but_cannot_fly"
        },
        {
            "paper_title": "Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge",
            "rating": 1,
            "sanitized_title": "leapofthought_teaching_pretrained_models_to_systematically_reason_over_implicit_knowledge"
        }
    ],
    "cost": 0.01433025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RULEBERT: Teaching Soft Rules to Pre-Trained Language Models
Association for Computational LinguisticsCopyright Association for Computational LinguisticsNovember 7-11, 2021. 2021</p>
<p>Mohammed Saeed 
Qatar Computing Research Institute
HBKU
Qatar</p>
<p>Naser Ahmadi 
Qatar Computing Research Institute
HBKU
Qatar</p>
<p>Preslav Nakov pnakov@hbku.edu.qa 
Qatar Computing Research Institute
HBKU
Qatar</p>
<p>Paolo Papotti 
Qatar Computing Research Institute
HBKU
Qatar</p>
<p>† Eurecom 
Qatar Computing Research Institute
HBKU
Qatar</p>
<p>France 
Qatar Computing Research Institute
HBKU
Qatar</p>
<p>RULEBERT: Teaching Soft Rules to Pre-Trained Language Models</p>
<p>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing
the 2021 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational LinguisticsNovember 7-11, 2021. 20211460
While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the finetuned model, yielding state-of-the-art results on external datasets.</p>
<p>Introduction</p>
<p>Pre-trained language models (PLMs) based on transformers (Devlin et al., 2019;Liu et al., 2020) are established tools for capturing both linguistic and factual knowledge (Clark et al., 2019b;Rogers et al., 2020). However, even the largest models fail on basic reasoning tasks. If we consider common relations between entities, we see that such models are not aware of negation, inversion (e.g., parent-child), symmetry (e.g., spouse), implication, and composition. While these are obvious to a human, they are challenging to learn from text corpora as they go beyond linguistic and factual knowledge (Ribeiro et al., 2020;. We claim that such reasoning primitives can be transferred to the PLMs by leveraging logical rules, such as those shown in Figure 1.</p>
<p>Input facts:</p>
<p>Mike is the parent of Anne. Anne lives with Mark. Anne is the child of Laure. Anne lives with Mike. Input rules: (r 1 , .1) Two persons living together are married. (r 2 , .7) Persons with a common child are married. (r 3 , .9) Someone cannot be married to his/her child. (r 4 , 1) Every person is the parent of his/her child. While there have been initial attempts to teach reasoning with rules to PLMs , such approaches model only a subclass of logical rules. In fact, current solutions focus on exact rules, i.e., rules that hold in all cases. In reality, most of the rules are approximate, or soft, and thus have a certain confidence of being correct. For example, across the 7,015 logical rules defined on the DBpedia knowledge graph, only 11% have a confidence above 95%. In the example, rules r 1 -r 3 are soft, i.e., cover knowledge that is not true in all circumstances. Consider rule r 2 , stating that if two persons have a child in common, they are most likely married. As r 2 has a confidence of being correct of 0.7, this uncertainty is reflected in the probability of the prediction.</p>
<p>With the above considerations in mind, here we show how to reason over soft logical rules with PLMs. We provide facts and rules expressed in natural language, and we ask the PLM to come up with a logical conclusion for a hypothesis, together with the probability for it being true.</p>
<p>Unlike previous approaches , we enable deductive reasoning for a large class of soft rules with binary predicates and an unrestricted number of variables. Our model can even reason over settings with conflicting evidence, as shown in Test 3 in Figure 1. In the example, as Anne and Mike live together, they have a 0.1 probability of being married because of soft rule r 1 . However, we can derive from exact rule r 4 that Anne is the child of Mike and therefore they cannot be married, according to soft rule r 3 .</p>
<p>To model uncertainty, we pick one flavor of probabilistic logic programming languages, LP MLN , for reasoning with soft rules (Lee and Wang, 2016). It assigns weights to stable models, similarly to how Markov Logic assigns weights to models. However, our method is independent of the logic programming approach at hand, and different models can be fine-tuned with different programming solutions. Our proposal makes use of synthetic examples that "teach" the desired formal behavior through fine-tuning. In particular, we express the uncertainty in the loss function used for fine-tuning by explicitly mimicking the results for the same problem modeled with LP MLN .</p>
<p>Our contributions can be summarized as follows:</p>
<p>• We introduce the problem of teaching soft rules expressed in a synthetic language to PLMs through fine-tuning (modeled as binary classification).</p>
<p>• We create and release the first dataset for this task, which contains 3.2M examples derived from 161 rules describing real common-sense patterns with the target probability for the task obtained from a formal reasoner (Section 4).</p>
<p>• We introduce techniques to predict the correct probability of the reasoning output for the given soft rules and facts. Our solution relies on a revised loss function that effectively models the uncertainty of the rules (Section 5). Our approach handles multi-variable rules and nicely extends to examples that require reasoning over multiple input rules.</p>
<p>• We show that our approach enables fine-tuned models to yield prediction probability very close to that produced by a formal reasoner (Section 6). Our PLM fine-tuned on soft rules, RULEBERT, can effectively reason with facts and rules that it has not seen at training, even when fine-tuned with only 20 rules.</p>
<p>• We demonstrate that our fine-tuning approach effectively transfers knowledge about predicate negation and symmetry to the lower levels of the transformer, which benefits from the logical notions in the rules. In particular, RULEBERT achieves new state-of-the-art results on three external datasets.</p>
<p>The data, the code, and the fine-tuned model are available at http://github.com/MhmdSaiid/ RuleBert.</p>
<p>Related Work</p>
<p>PLMs have been shown to have some reasoning capabilities (Talmor et al., 2020b), but fail on basic reasoning tasks (Talmor et al., 2020a) and are inconsistent (Elazar et al., 2021), especially when it comes to negation .</p>
<p>Our work focuses on deductive reasoning. Note that it is different from previous work, e.g., on measuring the factual knowledge of PLMs (Petroni et al., 2019), on probing the commonsense capabilities of PLMs at the token or at the sentence level (Zhou et al., 2020), or on testing the reasoning capabilities of PLMs on tasks such as age comparison and taxonomy conjunction (Talmor et al., 2020a). Our work relates to Task #15 in the bAbI dataset (Weston et al., 2016) and to Rule-Takers . However, we differ (i) by using a larger subclass of first-order logic rules (with more variables and various forms), and (ii) by incorporating soft rules.</p>
<p>Our proposal is different from work on Question Answering (QA) with implicit reasoning based on common-sense knowledge (Clark et al., 2019a), as we rely purely on deductive logic from explicitly stated rules.</p>
<p>Our approach also differs from methods that semantically parse natural language into a formal representation on which a formal reasoner can be applied (Liang, 2016), as we directly reason with language. Yet, we are also different from Natural Language Inference (NLI) and textual entailment, which work with text directly, but cannot handle Horn rules (MacCartney and Manning, 2009;Dagan et al., 2013).</p>
<p>Unlike previous work (Hamilton et al., 2018;Yang et al., 2017;Minervini et al., 2020), we do not design a new, ad-hoc module for neural reasoning, but we rely solely on the transformer's capability to emulate algorithms (Wang et al., 2019b;Lample and Charton, 2020).</p>
<p>Background</p>
<p>Language Models. We focus on language models pre-trained with bidirectional transformer encoders using masked language modeling (Devlin et al., 2019). For fine-tuning, we create examples for sequence classification to teach the models how to emulate reasoning given facts and soft rules.</p>
<p>Logical Rules. We rely on existing corpora of declarative Horn rules mined from large RDF knowledge bases (KBs) (Galárraga et al., 2015;Ortona et al., 2018;Ahmadi et al., 2020). An RDF KB is a database representing information with triples (or facts) p(s, o), where a predicate p connects a subject s and an object o. An atom in a rule is a predicate connecting two universally quantified variables. A Horn rule (or clause) has the form: B → h(x, y), where h(x, y) is a single atom (head or conclusion of the rule) and B (body or premise of the rule) is a conjunction of atoms. Positive rules identify relationships between entities, e.g., r 1 , r 2 , r 4 in Figure 1. Negative rules identify contradictions, e.g., r 3 in Figure 1. Rules can contain predicates comparing numerical values, such as &lt;. For example, negative rule r 5 : birthYear(b,d) ∧ foundYear(a,c) ∧ &lt;(c,d) → negfounder(a,b) states that any person (variable b) with a birth year (d) higher than the founding year (c) of a company (a) cannot be its founder. A fact is derived from a rule if all the variables in the rule body are replaced with constants from facts. For r 5 , facts "foundYear (Ford,1903), birthYear(E. Musk,1971), &gt;(1971,1903" trigger the rule that derives the fact negFounder(E. Musk, Ford).</p>
<p>Rule Confidence. Exact rules, such as r 4 , apply in all cases, without exception. However, most rules are approximate, or soft, as they apply with a certain likelihood. For example, r 3 in Figure 1 is true in most cases, but there are historical exceptions in royal families. Rules are annotated with a measure of this likelihood, either manually or with a computed confidence (Galárraga et al., 2015).</p>
<p>Probabilistic Answer Set Programming. As we deal with soft rules, we adopt LP MLN (Lee and Wang, 2016) to create the dataset. LP MLN is a probabilistic extension of answer set programs (ASP) with the concept of weighted rules from Markov Logic (Baral, 2010). In ASP, search problems are reduced to computing stable models (answer sets), a set of beliefs described by the program.</p>
<p>A weight (or confidence) is assigned to each rule, so that the more rules a stable model satisfies, the larger weight it gets, and the probability of the stable model is computed by normalizing its weight among all stable models. Given a set of soft rules and facts, we measure how much the hypothesis is supported by the stable model.</p>
<p>Dataset</p>
<p>We start by defining the reasoning task. We then discuss example generation methods for three scenarios: single rule as input, multiple (possibly conflicting) rules that require reasoning for the same conclusion, and multiple rules that require a sequence (chain) of reasoning steps. Examples of the data generation procedures are in the Appendix.</p>
<p>Reasoning Task</p>
<p>Each example is a triple (context, hypothesis, confidence). Context is a combination of rule(s) and generated facts, such as "If the first person lives together with the second person, then the first person is the spouse of the second person." and "Anne lives with Mike." Hypothesis is the statement to be assessed based on the context, e.g., "Laure is the spouse of Mike." Confidence is the probability that the hypothesis is valid given by the reasoner, e.g., 0.7. As we generate the examples, we know the confidence for each hypothesis.</p>
<p>Single-Rule Dataset Generation</p>
<p>Given a rule, we generate examples of different hypotheses to expose the model to various contexts. Each example contains the context c and a hypothesis h with its probability of being true as obtained for the (c, h) pair from the LP MLN reasoner. The intuition is that the examples show the expected behavior of a formal reasoner for every combination of possible facts for a given rule. This process is not about teaching the model specific facts to recall later, but teaching it reasoning patterns.</p>
<p>Unlike previous work , our rules allow for multiple variables. This introduces additional complexity as examples must show how to deal with the symmetry of the predicate. For example, child(Alice,Bob) and child(Bob,Alice) are not equivalent since child is not symmetric, while spouse(Alice,Bob) and spouse(Bob,Alice) are equivalent as spouse is symmetric. We assume that metadata about the symmetry and the types is available from the KB for the predicates in the rules.</p>
<p>Given as input (i) a rule r, (ii) a desired number n of examples, (iii) an integer m to indicate the maximum number of facts given as a context, and (iv) a pool of values for each type involved in r's predicate pools, Algorithm 1 outputs a dataset D of generated examples. We start at line 3 by generating facts, such as child(Eve,Bob), using the function GenFacts (lines 15-18), which takes as input a rule r, the maximum number of facts m to generate, and the pools. A random integer less than m sets the number of facts in the current context. The generated facts F have predicates from the body of r, their polarity (true or negated atom) is assigned randomly, and variables are instantiated with values sampled from the pool (line 16). Facts are created randomly, as we are not interested in teaching the model specific facts to recall later, but instead we want to teach it how to reason with different combinations of rules and facts. We then ensure that the rule is triggered in every context, eventually adding more facts to F using the function GetRuleFacts in line 17. After obtaining F , we feed rule r along with facts F to the LP MLN reasoner, and we obtain a set O containing all satisfied facts and rule conclusions (line 4).</p>
<p>We generate different hypotheses, where each one leads to an example in dataset D. For each context, we add an example with different facts with respect to the given rule according to three dimensions. A fact can be (i) for a predicate in the premise or in the conclusion of a rule, could be (ii) satisfied or unsatisfied given the rule, and could have (iii) positive or negative polarity. This makes eight different possibilities, thus leading to the generation of eight different hypotheses (one for each context).</p>
<p>The first hypothesis h 1 is obtained by sampling a fact from the set F (line 5). We then produce the counter hypothesis h 2 by altering the fact (line 6) using the function Alter (lines 19-22). Given a hypothesis p(s, o) (line 19), we return its negated form if p is symmetric (line 20). Otherwise, if p is not symmetric, we produce a counter hypothesis either by negation (line 21), or by switching the subject and the object in the triple as the predicate is not symmetric (line 22). We rely on a dictionary to check whether a predicate is symmetric or not.</p>
<p>We then produce hypothesis h 3 (line 7), which is the outcome of triggering rule r with the facts added in line 17. The counter hypothesis h 4 is generated by altering h 3 (line 8). Moreover, we generate hypothesis h 5 by considering any unsatisfied positive fact outside F . Following a closed-world assumption (CWA), we assume that positive triples are false if they cannot be proven, meaning that their negation is true. We sample a fact f l from the set of all possible positive facts that do not have the same predicate of the rule head (line 9). Thus, h 5 will never be in the output O of the reasoner, as it cannot be derived. We then produce h 6 by negating h 5 in line 10. We further derive h 7 by sampling a fact f r that has the same predicate as that of the rule head, but does not belong to the output of the reasoner O (line 11). For a positive (negative) rule, such a fact is labelled as False (True). h 7 is then negated to get the counter hypothesis h 8 (line 12). All generated hypotheses are added to D (line 13), and the process repeats until we obtain n examples.</p>
<p>Finally, we automatically convert the examples to natural language using predefined templates. A basic template for atom predicate p (type t 1 , type t 2 ) is "If the 1 st t 1 is p of the 2 nd t 2 ." ("If the first person is spouse of . . . "). For the single-rule scenario, we release a dataset for 161 rules with a total of 3.2M examples and a 80%:10%:10% split for training, validation, and testing.</p>
<p>Rules with Overlapping Conclusion</p>
<p>When multiple rules are in the context, there could be facts that trigger more than one rule for a given hypothesis. The triggered rules might be all of the same polarity (positive or negative), eventually accumulating their confidence, or could be a mix of positive and negative rules that oppose each other. While the data generation procedure in Section 4.2 can be extended to handle multiple rules, this raises an efficiency problem. Given a set of R rules, it would generate 8 |R| examples for each (facts,rule) pair in order to cover all rule combinations. This is very expensive, e.g., for five rules, it would generate 8 5 = 32, 768 examples for a single context.</p>
<p>Given this challenge, we follow a different approach. We first generate data for each rule individually using Algorithm 1. We then generate more examples only for combinations of two or more rules having all their rule conclusions as hypotheses. For every input context, we produce rule-conclusion hypotheses (positive and negative) while varying the rules being fired. Thus, we generate 2 * |R| x=2</p>
<p>|R|</p>
<p>x examples with at least two rules triggered. Adding the single-rule data, we generate 8 * |R| + 2 * |R| x=2 |R| x for every (facts,rules) pair, which is considerably smaller than 8 r for |R| ≥ 2, according to the binomial theorem. For example, for |R|=5, we generate 92 examples per context. For the overlapping rules scenario, we release a dataset for 5 rules with a total of 300K examples, and a 70%:10%:20% split for training, validation, and testing.</p>
<p>Chaining of Rule Executions</p>
<p>For certain hypotheses, an answer may be obtained by executing rules in a sequence, i.e., one on the result of the other, or in a chain. To be able to evaluate a model in this scenario, we generate hypotheses that can be tested only by chaining a number of rules (an example is shown in Appendix H). Given a pool of rules over different relations and a depth D, we sample a chain of rules with length D. We then generate hypotheses that would require a depth varying between 0 and D. We generate a rule-conclusion hypothesis (h 3 ) and its alteration (h 4 ) for each depth d ≤ D. A depth of 0 means that the hypothesis can be verified using the facts alone without triggering any rule. We also generate counter-hypotheses by altering the hypotheses at a given depth, and we further include hypotheses that are unsatisfied given the input.</p>
<p>For the chaining rules scenario, we start with a pool of 64 soft rules, and we generate hypotheses that would need at most five chained rules to verify them. The dataset for d ≤ 5 contains a total of 70K examples, and a 70%:10%:20% split for training, validation, and testing.</p>
<p>Teaching PLMs to Reason</p>
<p>In this section, we explain how we teach a PLM to reason with one or more soft rules. Note that uncertainty stems from the rule confidence. One approach to teach how to estimate the probability of a prediction is to treat each confidence value (or bucket of confidence values) as a class and to model the problem as a k-way classification instance (or regression), but this is intractable when multiple rules are considered. Instead, we keep the problem as a two-class one by altering how the information is propagated in the model to incorporate uncertainty from the rule confidence.</p>
<p>Let
D = {(x i , y i )} m i=1
be our generated dataset, where x i is one example of the form (context,hypothesis,confidence) and y i is a label indicating whether the hypothesis is validated or not by the context (facts and rules in English), and m is the size of the training set. A classifier f is a function that maps the input to one of the labels in the label space. Let h(x, y) be a classification loss function. The empirical risk of the classifier f is
R h (f ) = ED(h(x, y)) = − 1 m m i=1 h(xi, yi)
We want to introduce uncertainty in our loss function, using the weights computed by the LP MLN solver as a proxy to represent the probability of predicting the hypothesis as being true. To do so, we apply a revised empirical risk:
R h (f ) = ED(h(x, y)) = − 1 m m i=1 (w(xi) * h(xi, 1) + (1 − w(xi)) * h(xi, 0)) where w(x i ) is the probability of x i being True.
We now state that each example is considered as a combination both of a weighted positive example with a weight w(x i ) provided by the LP MLN solver and a weighted negative example with a weight 1 − w(x i ).</p>
<p>When trained to minimize this risk, the model learns to assign the weights to each output class, thus predicting the confidence for the true class when given the satisfied rule head as a hypothesis.</p>
<p>Dataset Total Train Dev Test</p>
<p>Single Rule (Section 6.2) 20K 16K 2K 2K Overlap (Section 6.3) 300K 210K 30K 60K Chaining (Depth=5) (Section 6.4) 70K 56K 4.6K 9.4K RULEBERT (Section 7)</p>
<p>3.2M 2.56M .32M .32M </p>
<p>Experiments</p>
<p>We first describe the experimental setup (Section 6.1). We then evaluate the model on single (Section 6.2) and on multiple rules (Sections 6.3 and 6.4). We show that a PLM fine-tuned on soft rules, namely RULEBERT, makes accurate predictions for unseen rules (Section 6.5), and it is more consistent than existing models on three external datasets (Section 7). We report the values of the hyper-parameters, as well as the results for some ablation experiments in the Appendix. The datasets for all experiments are summarized in Table 1.</p>
<p>Experimental Setup</p>
<p>Rules. We use a corpus of 161 soft rules mined from DBpedia. We chose a pool of distinct rules with varying number of variables, number of predicates, rule conclusions, and confidences.</p>
<p>Reasoner. We use the official implementation 1 of the LP MLN reasoner. We set the reasoner to compute the exact probabilities for the triples.</p>
<p>PLM. We use the HuggingFace pre-trained RoBERTa LARGE (Liu et al., 2020) model as our base model, as it is trained on more data compared to BERT (Devlin et al., 2019), and is better at learning positional embeddings (Wang and Chen, 2020). We fine-tune the PLM 2 with the weighted binary cross-entropy (wBCE) loss from Section 5. More details can be found in Appendix C.</p>
<p>Evaluations Measures.</p>
<p>For the examples in the test set, we use accuracy (Acc) and F1 score (F1) for balanced and unbalanced settings, respectively. As these measures do not take into account the uncertainty of the prediction probability, we further introduce Confidence Accuracy@k (CA@k), which measures the proportion of examples whose absolute error between the predicted and the actual probabilities is less than a threshold k: where x i is the i th example of dataset, w i is the actual confidence of the associated hypothesis given by the LP MLN reasoner,ŵ i is the predicted confidence by the model, and k is a chosen threshold. The measure can be seen as the ordinary accuracy measure, but true positives and negatives are counted only if the condition is satisfied, where lower values for k indicate stricter evaluation.
CA@k = #{x i , |w i −ŵ i | &lt; k} #{x i }</p>
<p>Single Soft Rule</p>
<p>We fine-tune 16 models for 16 different positive and negative rules (one model per rule) using 16k training samples per rule. We compare the accuracy of each model (i) without teaching uncertainty using binary cross-entropy (RoBERTa), and (ii) with teaching soft rules using wBCE. Table 2 shows a rule with its confidence, followed by accuracy and CA@k (for k = 0.1 and k = 0.01) for both loss functions. We see that models fine-tuned using RoBERTa-wBCE perform better on CA@k. In terms of accuracy, both models perform well, with RoBERTa-wBCE performing better for all rules. Interestingly, the best performing rules are two rules that involve comparison of numerical values (birth years against death and founding years), which suggests that our method can handle comparison predicates.</p>
<p>Results. Every row in</p>
<p>Rules Overlapping on Conclusion</p>
<p>The dataset contains five soft rules with spouse or negspouse in the rule conclusion, and a confidence between 0.30 and 0.87 (shown in Figure 2). We train a model on the dataset and test it (i) on a test set for each of the five rules separately, (ii) on test sets with U triggered rules, where U ∈ {2, 3, 4, 5}. Results. Table 3 shows that the model achieves high scores both on the single test sets (top five rows) and on the sets with interacting rules. The test sets with U = 2 and U = 3 are most challenging, as they contain 5 2 = 10 and 5 3 = 10 combinations of rules, respectively, while the one with U = 5 has only one possible rule combination. The high scores indicate that PLMs can actually learn the interaction between multiple soft rules.   Table 3: Results for a model trained on five rules sharing the same predicate, and tested on multiple test sets.
(r1, .87) child(a,c) ∧ parent(c,b) → spouse(A,B) (r2, .64) child(a,b) → negspouse(a,b) (r3, .3) relative(a,b) → spouse(a,b) (r4, ,78) child(a,c) ∧ child(b,c) → spouse(a,b) (r5, .67) predecessor(a,b) → negspouse(a,b)</p>
<p>Rule Chaining</p>
<p>Here, we assess models fine-tuned on various chaining depths. We construct six datasets for this scenario with increasing depths (D = 0, D ≤ 1, D ≤ 2, D ≤ 3, D ≤ 4, D ≤ 5), i.e., dataset D ≤ x contains hypotheses that need at most x chained rules. We thus train six models (one per dataset), and we test them (i) on their own test dataset (Test), (ii) on the test set with D ≤ 5 that contains all examples up to depth 5 (All), and (iii) on test sets with a chaining of depth x (Depx).</p>
<p>Results.</p>
<p>The results are shown in Table 4. We can see that the models achieve high F1 scores on the respective test sets for Depth 0. The red borderline indicates F1 scores for models tested on chaining depths higher than the ones they have been trained on. We see that Mod3 and Mod4 do fairly well on Depth 5. However, there is a decrease for higher depths, possibly due to the need for more training examples in order to learn such depths.  Moreover, since we sample a chain of rules each time, it is likely that every model has been trained on certain chains of rules. This yields lower scores in the constant-depth test sets as the models are being tested on unseen rule chains.</p>
<p>Note that Mod0 shows a counter-intuitive increase in the F1 score for higher unseen depths. Chaining soft rules may lead to a low probability for the associated hypothesis, and thus eventually to a False label. However, Mod0 is not trained on chaining and sees a hypothesis that requires chaining as an unsatisfied fact, thus eventually labelling it as False, while in fact it is the chaining of the soft rules that is the cause for this label. This is never the case with hard rules, as the actual label there would be True.</p>
<p>Testing RULEBERT on Unseen Rules</p>
<p>We have seen that a PLM can be successfully finetuned with rules. We now study the performance on the PLM after it has been fine-tuned on 161 (single) rules. We call this fine-tuned model RULEBERT.</p>
<p>Rule</p>
<p>FT-PLM RULEBERT20 FT-RULEBERT20  Table 5: Evaluation on unseen rules (accuracy). The first group contains rules with predicates seen by RULEBERT among the 20 rules used for fine-tuning, while the second group has rules with unseen predicates. We first evaluate RULEBERT on unseen rules. We fine-tune it with only twenty randomly selected rules (shown in Figure 3) and call it RULEBERT 20 . We then select ten new rules divided into two groups: (i) five rules containing predicates that were used in the rules for fine-tuning RULEBERT 20 , and (ii) five rules that share no predicates with the fine-tuning rules. For each rule in the test sets, we run a model fine-tuned (with 4k examples) only for that rule (FT-PLM), the model fine-tuned on the twenty original rules (RULEBERT 20 ), and the same model fine-tuned again for the rule at hand (FT-RULEBERT 20 ).
child(a,b) → negparent(a,b) child(a,b) → nespouse(a,b) child(a,b) → negchild(b,a) child(a,b) → negrelation(b,a) parent(a,b) → negparent(b,a) parent(a,b) → nespouse(a,b) spouse(a,b) → relative(b,a) successor(a,b) → predecessor(b,a) predecessor(a,b) → negsuccessor(a,b) successor(a,b) → negspouse(a,b) predecessor(a,b) → negspouse(a,b) child(a,c) ∧ parent(c,b) → spouse(a,b) child(b,a) ∧ child(c,a) → spouse(b,c) parent(a,b) ∧ parent(b,c) → negparent(a,c) parent(a,b) ∧ child(c,a) → spouse(b,c) spouse(a,b) ∧ parent(c,a) → negspouse(b,c) spouse(a,b) ∧ child(a,c) → negspouse(b,c) successor(a,
Results. Table 5 shows that RULEBERT 20 outperforms the fine-tuned model (FT-PLM) on the first group. Even though fine-tuned on 20 rules, it learned enough about (i) symmetric/transitive predicates and (ii) rule confidence to predict correctly, even better than rule-specific models.</p>
<p>For the second rule group, the accuracy of RULEBERT 20 is high, but FT-PLM performs better. Applying the same fine-tuning on RULEBERT 20 yields the best results in all scenarios.</p>
<p>RULEBERT on External Datasets</p>
<p>As our fine-tuning propagates information in the layers of the encoder, we hypothesize that RULE-BERT effectively "learns" logical properties of the concepts represented in the rules, such as negation and symmetry, and thus it could perform better on tasks testing such properties of PLMs. To study the negation of predicates, we use the Negated LAMA datasets, which test how PLMs distinguish a Cloze question and its negation . In most cases, PLMs make the same prediction both for a positive statement ("Relativity was developed by Einstein.") and for its negation ("Relativity was not developed by Einstein."). To test the symmetry relationship between predicates, we use the SRL test in CheckList (Ribeiro et al., 2020), which focuses on behavioral testing of NLP models; we use its test set for the duplicate-question detection task (QQP) (Wang et al., 2019a). Finally, we test deductive reasoning on the bAbI dataset and its Task #15 (Weston et al., 2016).</p>
<p>Negated LAMA Experiments</p>
<p>For Negated LAMA, we do not fine-tune RULE-BERT for the task; instead, we replace its original classification layer by an MLM head with weights identical to those of RoBERTa (not fine-tuned). Note that this configuration is biased in favor of RoBERTa, as the parameters of the MLM head and of the RoBERTa encoder have been trained in conjunction and thus good values have been found for this combination, which is not the case for our RULEBERT.  Results Yet, even in this arguably unfair setting, RULEBERT outperforms RoBERTa on all datasets of Negated LAMA, as shown in Table 7. We can see that RULEBERT performs better on both evaluation measures used in . It achieves a lower mean Spearman rank correlation (ρ) and a much smaller percentage of positive and negated answers overlap (%).</p>
<p>CheckList QQP Experiments</p>
<p>The CheckList tests (Ribeiro et al., 2020) have shown that PLMs fail in many basic cases. We hypothesize that RULEBERT can perform better on tasks and examples that deal with symmetric and asymmetric predicates, if such predicates have been shown to it during pre-fine-tuning. We experiment with the QQP dataset, which asks to detect whether two questions are duplicates. We identify a few rules that can teach a model about symmetric predicates, and we pre-fine-tune RULEBERT on them; then, we fine-tune it on the QQP dataset.</p>
<p>Results Table 6 shows the results on the challenging CheckList QQP test set: we can see that RULE-BERT achieves accuracy of 0.422 after one epoch, while RoBERTa is at 0.0. However, after three epochs RULEBERT is also at 0.0, 3 i.e., it started to unlearn what it had learned at pre-fine-tuning (Kirkpatrick et al., 2017;Kemker et al., 2018;Biesialska et al., 2020). Learning a new task often leads to such catastrophic forgetting (Ke et al., 2021). While there are ways to alleviate this (Ke et al., 2021), this is beyond the scope of this paper.</p>
<p>bAbI Task #15 Experiments</p>
<p>Finally, we experiment with task #15 of the bAbI dataset, where the goal is to assess whether a model can perform deductive reasoning. However, as mentioned in the original bAbI paper (Weston et al., 2016), it is not only desirable to perform well on the task, but also to use the fewest examples.</p>
<p>3 On the much easier QQP test set, RULEBERT achieved 0.89 accuracy after one epoch, and 0.91 after three epochs.  Thus, we use the smallest dataset consisting of about 2,000 data points. We hypothesize that under the same conditions and hyper-parameters, RULE-BERT should be able to generalize faster and to learn in fewer epochs. As PLMs produce varying scores when fine-tuned on small datasets, we repeat the experiment ten times and we report the average scores. We then compare to RoBERTa. Both models contain two classification layers to predict start and end spans of the input context.</p>
<p>Results</p>
<p>We can see in Table 6 that RULEBERT achieves accuracy of 0.863 in two epochs, while RoBERTa achieves 0.676. On the third epoch, RoBERTa catches up with accuracy of 0.827, while RULEBERT starts to overfit (goes down to 0.825), indicating that fewer epochs should be used, probably due to catastrophic forgetting.</p>
<p>Conclusion and Future Work</p>
<p>We studied whether PLMs could reason with soft rules over natural language. We experimented with one flavor of probabilistic answer set programming (LP MLN ), but other semantics can be also used with the proposed methodology. We further explored the inference capabilities of Transformerbased PLMs, focusing on positive and negative textual entailment.</p>
<p>We leave non-entailment for future work. We also leave open the development of explainable models. Some approaches use occlusion that removes parts of the input and checks the impact on the output  or build proof iteratively using 1-hop inference (Tafjord et al., 2021).</p>
<p>Ethics and Broader Impact</p>
<p>Data Collection While we generated the facts in our examples, the logical rules have been mined from the data in the DBpedia knowledge graph, which in turn has been generated from Wikipedia.</p>
<p>Biases We are aware of (i) the biases and abusive language patterns (Sheng et al., 2019;Bender et al., 2021;Liang et al., 2021) that PLMs impose, and (ii) the imperfectness and the biases of our rules as data from Wikipedia has been used to mine the rules and compute their confidences (Janowicz et al., 2018;Demartini, 2019). However, our goal is to study PLM's capability of deductive soft reasoning. For (i), there has been some work on debiasing PLMs (Liang et al., 2020), while for (ii), we used mined rules to have more variety, but could resort to user-specified rules validated by consensus to relieve the bias.</p>
<p>Environmental Impact The use of large-scale Transformers requires a lot of computations and GPUs/TPUs for training, which contributes to global warming (Strubell et al., 2019;Schwartz et al., 2020). This is a smaller issue in our case, as we do not train such models from scratch; rather, we fine-tune them on relatively small datasets. Moreover, running on a CPU for inference, once the model is fine-tuned, is less problematic as CPUs have a much lower environmental impact.</p>
<p>A More on Reasoning with Soft Rules</p>
<p>Let σ be a signature as in first-order logic. An LP MLN program Π is a finite set of weighted rules of the form:
w : A ← B (1)
where A is a disjunction of atoms of σ, B is a conjunction of literals (atoms and negated atoms) of σ, and w is a real number or the symbol α. When A is ⊥ (the empty disjunction), the rule asserts that B should be false in the stable model. An LP MLN rule (1) is called soft if w is a real number or hard if w is α. An LP MLN program is ground if its rules contain no variables. An LP MLN program Π that contains variables is identified with a ground LP MLN program gr σ [Π], which is obtained from Π by replacing every variable with every ground term of σ. The weight of a ground rule in gr σ [Π] is the same as the weight of the corresponding rule in Π. By Π we denote the unweighted logic program obtained from Π, i.e., Π = {R | w : R ∈ Π}.</p>
<p>For a ground LP MLN program Π, Π I denotes the set of rules w : R in Π such that I satisfies R (denoted I |= R) and SM[Π] denotes the set {I | I is a (deterministic) stable model of Π I }. The (unnormalized) weight of I under Π is defined as follows:
W Π (I) =    exp( w:R∈Π I w) if I ∈ SM[Π]; 0
otherwise.</p>
<p>The probability of I under Π is the normalized weight defined as follows:
P Π (I) = lim α→∞ W Π (I) J∈SM[Π] W Π (J)
.</p>
<p>In Answer Set programming (ASP), search problems are reduced to computing stable models (a.k.a. answer sets), a set of beliefs described by the program. In the case of a Horn program, the stable models coincide with the minimal models. LP MLN programs are transformed to meet the needs of an ASP solver (Lee et al., 2017;Gebser et al., 2014).</p>
<p>B Rule Support</p>
<p>We designed an experiment to show the impact of increasing the number of overlapping rules on the same target predicate. The goal is to measure how often multiple rules are triggered for the same target triple. We measure this with the support of a rule, i.e., the number of triples in the knowledge base that satisfy all the atoms in the rule.</p>
<p>To compute the support for more than one rule, we combine the premises of the rules. In this experiment, we picked three predicates (spouse, child and relative), and for each one we selected ten rules randomly. Next, we used DBpedia online endpoint 4 to compute the support for each combination of n (n=1,2,...,5) rules for each predicate. The results in Figure 4 show that by increasing the number of rules, the support decreases for all predicates. For combinations with more than three rules, the support is very small.</p>
<p>C More Experimental Details</p>
<p>For fine-tuning our models, we use Google Colaboratory (Bisong, 2019), which assigns random GPU clusters of various types. The number of parameters of our models is about 355M. We select the values of our hyper-parameters (shown in Table 8) on the development sets, by maximizing accuracy.</p>
<p>The execution times vary largely depending on the GPU at hand and on the scenario, with finetuning on a Tesla V100 taking from one hour for a single rule to a few hours for all the chaining experiments. The training/validation/testing splits are shown in Table 1. Table 9 shows the sizes of the used test datasets.</p>
<p>D Ablation</p>
<p>D.1 Impact of the Data Size Setting. We report the impact of the size of the fine-tuning data on the model performance. As shown in Table 2, the accuracy of the fine-tuned model is higher for rules with higher confidence.   We therefore divide the rules in three categories: High contains rules with confidence greater than 0.8, Medium has rules with confidence between 0.4 and 0.8, and Low is for the rest. There are six rules in the Medium category and the other two categories have five rules each. For each rule, we fine-tune seven models with 1k, 2k, 5k, 10k, 15k, 20k, and 30k examples.</p>
<p>Results. Figure 5 shows that having more training data improves the accuracy in all scenarios. For all categories, there is a sizable increase going from 10k to 15k examples; the impact is smaller for higher values. The highest increase is for rules with high confidence, and rules with medium confidence demonstrate larger increase than low confidence.   Results. The results in Table 10 show that the model performance does not depend heavily on using the same fact format for training and testing. With examples using letters in training, the results are slightly better in the case with two formats. We ultimately use names for testing and training in our default configuration as it yields better results.</p>
<p>E Impact of the Random Seed</p>
<p>Pre-trained transformers often suffer from instability of the results across multiple reruns with different random seeds. This usually happens with small training datasets (Dodge et al., 2020;Mosbach et al., 2021). In such cases, typically multiple reruns are performed, and the average value over these reruns is reported. However, the numbers for the main experiments we report in this paper are not averaged over multiple reruns as our datasets are considerably large and the models did not suffer from instability due to random seeds. For example, when we reran RULE-BERT on a single-rule experiment three times, we obtained accuracy of 0.98959, 0.99551, 0.99636 with a standard deviation of only 0.003.</p>
<p>Yet, for the small dataset bAbI, we observed a much higher standard deviation of 0.17. Thus, in this case we report results that are averaged over ten reruns.</p>
<p>F Data Generation Example</p>
<p>We show an example of data generation for Algorithm 1. For simplicity, here we show an example of a hard rule, i.e., one whose confidence is implicitly set to one. 5 We begin by setting the values of the input parameters:</p>
<p>Algorithm 1 Input:
• r = child(A,C) ∧ parent(C,B) → spouse(A,B) • n = 8 • m = 5 • pools ={Alice,Bob,Carl,David,Eve}
We set n = 8 to generate all the eight hypotheses. We start by generating a set of facts F (line 3), having predicates from the body of the rule with random polarity. We ensure that there are facts that trigger the rule. Their number should not exceed m. Here is an example of generated facts F : Generated Facts F:
• f 1 : negparent(Eve,Carl) • f 2 : child(Eve,David) • f 3 : parent(Carl,Bob) • f 4 : child(Alice,Carl)
Four facts are generated in total. Facts f 3 and f 4 trigger rule r. We then feed the rule r and facts F into the LP MLN reasoner (line 4 Hypothesis h 1 is obtained by sampling from F (line 5), and thus it is a valid hypothesis. Then, the hypothesis h 2 is generated by altering h 1 with the function Alter (line 19-22). In this example, since child is not symmetric, h 2 is produced using a switch of the subject and the object of h 1 to generate a false hypothesis (line 6).</p>
<p>Hypothesis h 3 is the outcome of rule r being triggered by facts f 3 and f 4 (line 7). In a similar fashion to h 2 , we produce h 4 (line 8).</p>
<p>Hypothesis h 5 is sampled from the universe of all unsatisfied positive facts having a different predicate than that of the rule body (line 9), which makes it an invalid hypothesis, as it is not found in the O. Hypothesis h 6 is the negation of h 5 , and, following CWA, it is a valid hypothesis (line 10).</p>
<p>Finally, hypothesis h 7 is sampled from the universe of unsatisfied rule-head atoms (line 11), and it is negated to produce hypothesis h 8 . We then convert each example to synthetic English using a set of pre-defined templates for the facts and for the rules. Here is the above Example #1, but now rewritten in synthetic English:</p>
<p>Example #1 (Synthetic English):</p>
<p>• Rule r = If the child of the first person is the third person, and the parent of the third person is the second person, then the first person is the spouse of the second person.</p>
<p>• F acts F :</p>
<p>f 1 : The parent of Eve is not Carl.</p>
<p>f 2 : The child of Eve is David.</p>
<p>f 3 : The parent of Carl is Bob.</p>
<p>f 4 : The child of Alice is Carl.</p>
<p>• Hypothesis h 3 : The spouse of Alice is Bob.</p>
<p>The Context is defined as the combined set of facts and rule(s). Both Context and Hypothesis are fed as an input to the model.</p>
<p>Example #1 (Model Input):</p>
<p>• Context : The parent of Eve is not Carl.</p>
<p>The child of Eve is David. If the child of the first person is the third person, and the parent of the third person is the second person, then the first person is the spouse of the second person. The parent of Carl is Bob. The child of Alice is Carl.</p>
<p>• Hypothesis : The spouse of Alice is Bob.</p>
<p>G Rule Overlap Example</p>
<p>After generating the data for every rule in Figure 2, we generate additional examples using combinations of rules. Below, we show how to handle the interaction of two rules: r 2 and r 3 . We follow the procedure in Algorithm 1 by generating facts that trigger the rules, but we only take into consideration hypotheses that deal with rule conclusions. For example, consider the following facts:</p>
<p>Generated Facts We can generate an example that triggers two rules: f 2 triggers r 2 , and f 3 triggers r 3 . Feeding the above facts and rules r 2 and r 3 to the reasoner, we obtain the following output (the numbers in parentheses indicate the likelihood of the triple): We produce hypotheses that trigger both rules together. For example, here we generate two hypotheses coming from o 4 and o 5 . The confidence (weight) of a hypothesis is given by the LP MLN reasoner. Taking o 5 as a hypothesis, we feed the following example to the model:</p>
<p>Example #2 (Model Input):</p>
<p>• Context : The parent of Eve is not Carl.</p>
<p>The child of Eve is David. If the child of the first person is the second person, then the first person is not the spouse of the second person. The relative of Eve is David. If the relative of the first person is the second person, then the first person is the spouse of the second person. The predecessor of Eve is David. • Hypothesis : The spouse of Eve is not David. • W eight : 0.55</p>
<p>We also generate an example, where three rules are triggered: In addition to r 2 and r 3 , r 5 is triggered by f 4 . We then repeat the same procedure to generate the following example:</p>
<p>Example #3 (Model Input):</p>
<p>• Context : The parent of Eve is not Carl.</p>
<p>The child of Eve is David. If the child of the first person is the second person, then the first person is not the spouse of the second person. The relative of Eve is David. If the relative of the first person is the second person, then the first person is the spouse of the second person. The predecessor of Eve is David. If the predecessor of the first person is the second person, then the first person is not the spouse of the second person. • Hypothesis : The spouse of Eve is not David. • W eight : 0.6</p>
<p>Test 1 :Figure 1 :
11Laure and Mike are married. Answer: True with probability 0.7 [r 4 , r 2 ] Test 2: Anne and Mark are married. Answer: False with probability 0.9 [r 1 ]Test 3: Anne and Mike are married. Answer: False with probability 0.9 [r 1 , r 3 , r 4 ] Examples of hypotheses that require reasoning using facts and possibly conflicting soft rules (rule id and confidence shown in brackets).</p>
<p>Algorithm 1 : 3 FFunction
13Generate Synthetic Data Input: rule r ; // child(a,b)→parent(b,a) n ; // # of examples m ; // max # of facts pools ; // pools of names Output: Generated Dataset D 1 D = {}, i = 1 ; // initialize 2 while i ≤ ceiling(n/8) do = GenF acts(r, m, pools) ; // child(Eve,Bob),parent(Amy,Sam) 4 O = LP M LN (r, F ) ; // reasoner output 5 h1 = f ∈ F ; // child(Eve,Bob) 6 h2 = Alter(f ) ; // negchild(Eve,Bob) 7 h3 = r(F ) ; // parent(Bob,Eve) 8 h4 = Alter(r(F )) ; // parent(Eve,Bob) 9 h5 = pos.f l / ∈ F ; // child(Joe,Garry) Alter(p(s, o)): 20 if p is symmetric then return ¬p(s, o) ; 21 if random()&gt;0.5 then return ¬p(s, o) ; 22 else return p(o, s) ;</p>
<p>Figure 2 :
2The five overlapping soft rules.</p>
<p>Figure 3 :
3c) ∧ successor(b,c) → negspouse(a,b) publisher(c,b) ∧ subsequentwork(c,a) → publisher(a,b) publisher(c,b) ∧ previouswork(c,a) → publisher(a,b) The 20 random rules used for RULEBERT 20 .</p>
<p>Figure 4 :
4Support of the overlapping rules.</p>
<p>Figure 5 :
5Impact of the training data size.</p>
<p>Overall, we obtain eight different examples represented in symbolic knowledge, where each example contains the set of generated facts F , the rule r, and a single hypothesis h i . The following is one example in symbolic knowledge: Example #1 (Symbolic): • Rule r = child(A,C) ∧ parent(C,B) → spouse(A,B) • F acts F : f 1 : negparent(Eve,Carl) f 2 : child(Eve,David) f 3 : parent(Carl,Bob) f 4 : child(Alice,Carl) • Hypothesis h 3 : spouse(Alice,Bob)</p>
<p>4 : spouse(Eve,David) (0.134) • o 5 : negspouse(Eve,David) (0.55) • o 6 : predecessor(Eve,David) (1.0)</p>
<p>Table 1 :
1Datasets for the experiments and their splits.</p>
<p>Table 2 :
2Evaluation results for single-rule models.Test Size 
F1 
CA@.15 CA@.1 CA@.05 </p>
<p>r1 
1.6k .990 
.987 
.986 
.954 
r2 
1.6k .999 
.997 
.996 
.946 
r3 
1.6k .995 
.994 
.994 
.992 
r4 
1.6k .990 
.989 
.988 
.935 
r5 
1.6k 
1 
.999 
.998 
.979 
U=2 20k .985 
.997 
.993 
.968 
U=3 20k .925 
1 
.998 
.949 
U=4 10k .956 
1 
1 
.988 
U=5 
2k 
1 
1 
1 
.980 </p>
<p>Table 4 :
4F1 scores for models trained on varying depths and tested on six datasets. The boxed area indicates models tested on unseen chaining depths.</p>
<p>Table 6 :
6Evaluation on external datasets (accuracy).</p>
<p>Table 7 :
7Negated LAMA: Mean Spearman rank cor-
relation (ρ) and mean percentage of overlap in the first 
ranked predictions (%) for original vs. negated queries. </p>
<p>Table 8 :
8Hyper-parameters for fine-tuning our model.Dataset 
Size </p>
<p>Mod0 Test(own) 
2,667 
Mod1 Test(own) 
4,000 
Mod2 Test(own) 
5,334 
Mod3 Test(own) 
6,667 
Mod4 Test(own) 
8,000 
Mod5 Test(own) 
9,334 
Test(D≤5) 
9,334 
Depth=0 
16,057 
Depth=1 
6,608 
Depth=2 
5,389 
Depth=3 
3,993 
Depth=4 
2,619 
Depth=5 
1,336 </p>
<p>Table 9 :
9Number of examples in each of the test datasets for the chaining experiment.</p>
<p>D.2 Role of the Example FormatSetting. When we teach rules to PLMs, we rely on examples with real names from a fixed pool. However, our goal is to teach PLMs the semantics of the soft rule, not the facts in our examples. Thus, we further design an experiment to assess the impact of the format used in the example facts on the behavior of the model. We distinguish two formats for the generated facts: (i) real names such as Alice and IBM, and (ii) letters such as A and B. We first use each format in fine-tuning and we then test both formats. We end up with two test/train scenarios: one with the same format and one with different formats. For this study, we use just one rule: child(a,c) ∧ parent(c,b) → spouse(a,b), with 30K examples for fine-tuning, and 2k for testing.Train Letter Train Name </p>
<p>Test Letter 
.981 
.932 
Test Name 
.977 
.985 </p>
<p>Table 10 :
10Impact of the example format on accuracy.</p>
<p>). The output O is then: LP MLN Reasoner Output O: • o 1 : child(Eve,David) • o 2 : child(Alice,Carl) • o 3 : parent(Carl,Bob) • o 4 : spouse(Alice,Bob) • o 5 : negchild(Eve,Carl) We start generating the hypotheses: Generated Hypotheses H: • h 1 : child(Eve,David) • h 2 : child(David,Eve) • h 3 : spouse(Alice,Bob) • h 4 : negspouse(Alice,Bob) • h 5 : child(David,Carl) • h 6 : negchild(David,Carl) • h 7 : spouse(Bob,Eve) • h 8 : negspouse(Bob,Eve)5  We show an example of a soft rule in Section G below.</p>
<p>• f 1
f:negparent(Eve,Carl) • f 2 : child(Eve,David) • f 3 : relative(Eve,David) • f 4 : predecessor(Eve,David)
http://dbpedia.org/sparql
AcknowledgmentsThis work is partially supported by a Google Faculty Research Award and the ANR JCJC Grant InfClean.This procedure is repeated for all combinations of two or more rules. In case when all rules have the same head polarity, we generate a false example by altering the hypothesis and finding the complement of the initial (1-W eight) weight. For example, r 2 and r 5 can occur together and both have the same rule head, and thus no conflict occurs. The generated valid example would be as follows:Example #4 (Model Input):• Context : The parent of Eve is not Carl.The child of Eve is David. If the child of the first person is the second person, then the first person is not the spouse of the second person. The relative of Eve is David. The predecessor of Eve is David. If the predecessor of the first person is the second person, then the first person is not the spouse of the second person.• Hypothesis : The spouse of Eve is not David.• W eight : 0.64An invalid example is generated from the valid example by altering the hypothesis. Here is an invalid example:Example #4 (Model Input):• Context : The parent of Eve is not Carl.The child of Eve is David. If the child of the first person is the second person, then the first person is not the spouse of the second person. The relative of Eve is David. The predecessor of Eve is David. If the predecessor of the first person is the second person, then the first person is not the spouse of the second person.H Rule Chaining ExampleHere is an example that illustrates rule chaining:Example #5 (Symbolic):• Rules R = r 1 : f 2 triggers r 2 which produces t = child(Bob,Carl). t and f 3 trigger r 1 to validate the hypothesis h. r 1 and r 2 have been chained to validate the hypothesis. Since we used two rules to validate the hypothesis, we say that this is a chain of depth = 2.
RuleHub: A public corpus of rules for knowledge graphs. Naser Ahmadi, Thi-Thuy-Duyen Truong, Le-Hong-Mai Dao, Stefano Ortona, Paolo Papotti, 10.1145/3409384J. Data and Information Quality. 124Naser Ahmadi, Thi-Thuy-Duyen Truong, Le-Hong- Mai Dao, Stefano Ortona, and Paolo Papotti. 2020. RuleHub: A public corpus of rules for knowledge graphs. J. Data and Information Quality, 12(4).</p>
<p>Knowledge Representation, Reasoning and Declarative Problem Solving. Chitta Baral, Cambridge University PressUSA1st editionChitta Baral. 2010. Knowledge Representation, Rea- soning and Declarative Problem Solving, 1st edition. Cambridge University Press, USA.</p>
<p>On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21. the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21Virtual Event, Canada. Association for Computing MachineryEmily M. Bender, Timnit Gebru, Angelina McMillan- Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Confer- ence on Fairness, Accountability, and Transparency, FAccT '21, page 610-623, Virtual Event, Canada. Association for Computing Machinery.</p>
<p>Continual lifelong learning in natural language processing: A survey. Magdalena Biesialska, Katarzyna Biesialska, Marta R Costa-Jussà, 10.18653/v1/2020.coling-main.574Proceedings of the 28th International Conference on Computational Linguistics, COLING '20. the 28th International Conference on Computational Linguistics, COLING '20Barcelona, Spain (OnlineInternational Committee on Computational LinguisticsMagdalena Biesialska, Katarzyna Biesialska, and Marta R. Costa-jussà. 2020. Continual lifelong learning in natural language processing: A survey. In Proceedings of the 28th International Confer- ence on Computational Linguistics, COLING '20, pages 6523-6541, Barcelona, Spain (Online). Inter- national Committee on Computational Linguistics.</p>
<p>Google colaboratory. Ekaba Bisong, 10.1007/978-1-4842-4470-8_7Building Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners. ApressEkaba Bisong. 2019. Google colaboratory. In Build- ing Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners, pages 59-64. Apress.</p>
<p>BoolQ: Exploring the surprising difficulty of natural yes/no questions. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, Kristina Toutanova, 10.18653/v1/N19-1300Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, Minnesota, USALong and Short Papers1Association for Computational LinguisticsChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019a. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceed- ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long and Short Papers), NAACL-HLT '19, pages 2924-2936, Minneapolis, Minnesota, USA. Associ- ation for Computational Linguistics.</p>
<p>What does BERT look at? An analysis of BERT's attention. Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D Manning, 10.18653/v1/W19-4828Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP '19. the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP '19Florence, ItalyAssociation for Computational LinguisticsKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019b. What does BERT look at? An analysis of BERT's attention. In Pro- ceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, BlackboxNLP '19, pages 276-286, Florence, Italy. Association for Computational Linguistics.</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, 10.24963/ijcai.2020/537Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI '20. the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI '20Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over lan- guage. In Proceedings of the Twenty-Ninth Inter- national Joint Conference on Artificial Intelligence,, IJCAI '20, pages 3882-3890, Online. International Joint Conferences on Artificial Intelligence Organi- zation.</p>
<p>Recognizing Textual Entailment: Models and Applications. Synthesis Lectures on Human Language Technologies. Dan Ido Dagan, Mark Roth, Fabio Massimo Sammons, Zanzotto, Morgan and Claypool publishersIdo Dagan, Dan Roth, Mark Sammons, and Fabio Mas- simo Zanzotto. 2013. Recognizing Textual Entail- ment: Models and Applications. Synthesis Lec- tures on Human Language Technologies. Morgan and Claypool publishers.</p>
<p>Implicit bias in crowdsourced knowledge graphs. Gianluca Demartini, 10.1145/3308560.3317307Proceedings of the 2019 World Wide Web Conference: Companion Volume, WWW '19. the 2019 World Wide Web Conference: Companion Volume, WWW '19San Francisco, California, USAAssociation for Computing MachineryGianluca Demartini. 2019. Implicit bias in crowd- sourced knowledge graphs. In Proceedings of the 2019 World Wide Web Conference: Companion Vol- ume, WWW '19, page 624-630, San Francisco, Cal- ifornia, USA. Association for Computing Machin- ery.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, Minnesota, USAAssociation for Computational Linguistics1NAACL-HLT '19Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), NAACL-HLT '19, pages 4171-4186, Minneapolis, Minnesota, USA. Association for Computational Linguistics.</p>
<p>Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, Noah A Smith, arXiv:2002.06305Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah A. Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stop- ping. arXiv:2002.06305.</p>
<p>Yanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard H Hovy, arXiv:2102.01017Hinrich Schütze, and Yoav Goldberg. 2021. Measuring and improving consistency in pretrained language models. Yanai Elazar, Nora Kassner, Shauli Ravfogel, Ab- hilasha Ravichander, Eduard H. Hovy, Hinrich Schütze, and Yoav Goldberg. 2021. Measuring and improving consistency in pretrained language mod- els. arXiv:2102.01017.</p>
<p>Fast rule mining in ontological knowledge bases with AMIE++. Luis Galárraga, Christina Teflioudi, Katja Hose, Fabian M Suchanek, 10.1007/s00778-015-0394-1The VLDB Journal. 246Luis Galárraga, Christina Teflioudi, Katja Hose, and Fabian M. Suchanek. 2015. Fast rule mining in onto- logical knowledge bases with AMIE++. The VLDB Journal, 24(6):707-730.</p>
<p>. Martin Gebser, Roland Kaminski, Benjamin Kaufmann, Torsten Schaub, arXiv:1405.3694Clingo = ASP + control: Preliminary report. Martin Gebser, Roland Kaminski, Benjamin Kauf- mann, and Torsten Schaub. 2014. Clingo = ASP + control: Preliminary report. arXiv:1405.3694.</p>
<p>Embedding logical queries on knowledge graphs. William L Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, Jure Leskovec, https:/dl.acm.org/doi/10.5555/3326943.3327131Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18. the 32nd International Conference on Neural Information Processing Systems, NIPS'18Montréal, CanadaCurran Associates IncWilliam L. Hamilton, Payal Bajaj, Marinka Zitnik, Dan Jurafsky, and Jure Leskovec. 2018. Embedding logical queries on knowledge graphs. In Proceed- ings of the 32nd International Conference on Neu- ral Information Processing Systems, NIPS'18, page 2030-2041, Montréal, Canada. Curran Associates Inc.</p>
<p>Debiasing knowledge graphs: Why female presidents are not like female popes. Krzysztof Janowicz, Bo Yan, Blake Regalia, Rui Zhu, Gengchen Mai, Proceedings of the International Semantic Web Conference, ISWC '18. the International Semantic Web Conference, ISWC '18Monterey, California, USAKrzysztof Janowicz, Bo Yan, Blake Regalia, Rui Zhu, and Gengchen Mai. 2018. Debiasing knowledge graphs: Why female presidents are not like female popes. In Proceedings of the International Semantic Web Conference, ISWC '18, Monterey, California, USA.</p>
<p>Are pretrained language models symbolic reasoners over knowledge?. Nora Kassner, Benno Krojer, Hinrich Schütze, 10.18653/v1/2020.conll-1.45Proceedings of the 24th Conference on Computational Natural Language Learning, CoNLL '20. the 24th Conference on Computational Natural Language Learning, CoNLL '20Association for Computational LinguisticsOnlineNora Kassner, Benno Krojer, and Hinrich Schütze. 2020. Are pretrained language models symbolic reasoners over knowledge? In Proceedings of the 24th Conference on Computational Natural Lan- guage Learning, CoNLL '20, pages 552-564, On- line. Association for Computational Linguistics.</p>
<p>Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. Nora Kassner, Hinrich Schütze, 10.18653/v1/2020.acl-main.698Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20. the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20Online. Association for Computational LinguisticsNora Kassner and Hinrich Schütze. 2020. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Proceedings of the 58th Annual Meeting of the Association for Com- putational Linguistics, ACL '20, pages 7811-7818, Online. Association for Computational Linguistics.</p>
<p>Adapting BERT for continual learning of a sequence of aspect sentiment classification tasks. Zixuan Ke, Hu Xu, Bing Liu, 10.18653/v1/2021.naacl-main.378Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '21. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '21Online. Association for Computational LinguisticsZixuan Ke, Hu Xu, and Bing Liu. 2021. Adapting BERT for continual learning of a sequence of as- pect sentiment classification tasks. In Proceedings of the 2021 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT '21, pages 4746-4755, Online. Association for Compu- tational Linguistics.</p>
<p>Measuring catastrophic forgetting in neural networks. Ronald Kemker, Marc Mcclure, Angelina Abitino, Tyler L Hayes, Christopher Kanan, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI '18. the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI '18New Orleans, Louisiana, USAAAAI PressRonald Kemker, Marc McClure, Angelina Abitino, Tyler L. Hayes, and Christopher Kanan. 2018. Mea- suring catastrophic forgetting in neural networks. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, AAAI '18, pages 3390- 3398, New Orleans, Louisiana, USA. AAAI Press.</p>
<p>Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, 10.1073/pnas.1611835114Proceedings of the National Academy of Sciences. 11413James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Ag- nieszka Grabska-Barwinska, Demis Hassabis, Clau- dia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114(13):3521-3526.</p>
<p>Deep learning for symbolic mathematics. Guillaume Lample, François Charton, Proceedings of the 8th International Conference on Learning Representations, ICLR '20. the 8th International Conference on Learning Representations, ICLR '20Addis Ababa, EthiopiaOpenReview.netGuillaume Lample and François Charton. 2020. Deep learning for symbolic mathematics. In Proceed- ings of the 8th International Conference on Learning Representations, ICLR '20, Addis Ababa, Ethiopia. OpenReview.net.</p>
<p>Computing LPMLN using ASP and MLN solvers. Theory and Practice of Logic Programming. Joohyung Lee, Samidh Talsania, Y Wang, 10.1017/S147106841700040017Joohyung Lee, Samidh Talsania, and Y. Wang. 2017. Computing LPMLN using ASP and MLN solvers. Theory and Practice of Logic Programming, 17(5- 6):942-960.</p>
<p>Weighted rules under the stable model semantics. Joohyung Lee, Yi Wang, https:/dl.acm.org/doi/10.5555/3032027.3032045Proceedings of the Fifteenth International Conference on Principles of Knowledge Representation and Reasoning, KR '16. the Fifteenth International Conference on Principles of Knowledge Representation and Reasoning, KR '16Cape Town, South AfricaAAAI PressJoohyung Lee and Yi Wang. 2016. Weighted rules under the stable model semantics. In Proceedings of the Fifteenth International Conference on Prin- ciples of Knowledge Representation and Reasoning, KR '16, page 145-154, Cape Town, South Africa. AAAI Press.</p>
<p>Towards debiasing sentence representations. Irene Mengze Paul Pu Liang, Emily Li, Zheng, Chong Yao, Ruslan Lim, Louis-Philippe Salakhutdinov, Morency, 10.18653/v1/2020.acl-main.488Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20. the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20Online. Association for Computational LinguisticsPaul Pu Liang, Irene Mengze Li, Emily Zheng, Yao Chong Lim, Ruslan Salakhutdinov, and Louis- Philippe Morency. 2020. Towards debiasing sen- tence representations. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, ACL '20, pages 5502-5515, Online. As- sociation for Computational Linguistics.</p>
<p>Towards understanding and mitigating social biases in language models. Chiyu Paul Pu Liang, Louis-Philippe Wu, Ruslan Morency, Salakhutdinov, Proceedings of the International Conference on Machine Learning, ICML '21. the International Conference on Machine Learning, ICML '21Online. PMLRPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2021. Towards under- standing and mitigating social biases in language models. In Proceedings of the International Confer- ence on Machine Learning, ICML '21, pages 6565- 6576, Online. PMLR.</p>
<p>Learning executable semantic parsers for natural language understanding. Commun. Percy Liang, 10.1145/2866568ACM59Percy Liang. 2016. Learning executable semantic parsers for natural language understanding. Com- mun. ACM, 59(9):68-76.</p>
<p>RoBERTa: A robustly optimized BERT pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin , Proceedings of the 8th International Conference on Learning Representations, ICLR '20. the 8th International Conference on Learning Representations, ICLR '20Addis Ababa, EthiopiaOpenReview.netYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin. 2020. RoBERTa: A robustly optimized BERT pretraining approach. In Proceedings of the 8th International Conference on Learning Representations, ICLR '20, Addis Ababa, Ethiopia. OpenReview.net.</p>
<p>An extended model of natural logic. Bill Maccartney, Christopher D Manning, Proceedings of the Eight International Conference on Computational Semantics, IWCS-WS '09. the Eight International Conference on Computational Semantics, IWCS-WS '09Tilburg, The NetherlandsAssociation for Computational LinguisticsBill MacCartney and Christopher D. Manning. 2009. An extended model of natural logic. In Proceedings of the Eight International Conference on Computa- tional Semantics, IWCS-WS '09, pages 140-156, Tilburg, The Netherlands. Association for Computa- tional Linguistics.</p>
<p>Differentiable reasoning on large knowledge bases and natural language. Pasquale Minervini, Matko Bošnjak, Tim Rocktäschel, Sebastian Riedel, Edward Grefenstette, 10.1609/aaai.v34i04.5962Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Pasquale Minervini, Matko Bošnjak, Tim Rocktäschel, Sebastian Riedel, and Edward Grefenstette. 2020. Differentiable reasoning on large knowledge bases and natural language. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):5182- 5190.</p>
<p>On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines. Marius Mosbach, Maksym Andriushchenko, Dietrich Klakow, Proceedings of the 9th International Conference on Learning Representations, ICLR '21, Virtual Event. the 9th International Conference on Learning Representations, ICLR '21, Virtual EventMarius Mosbach, Maksym Andriushchenko, and Diet- rich Klakow. 2021. On the stability of fine-tuning bert: Misconceptions, explanations, and strong base- lines. In Proceedings of the 9th International Con- ference on Learning Representations, ICLR '21, Vir- tual Event, Austria. OpenReview.net.</p>
<p>Robust discovery of positive and negative rules in knowledge bases. Stefano Ortona, Paolo Venkata Vamsikrishna Meduri, Papotti, 10.1109/ICDE.2018.00108Proceedings of the 2018 IEEE 34th International Conference on Data Engineering, ICDE '18. the 2018 IEEE 34th International Conference on Data Engineering, ICDE '18Paris, FranceIEEEStefano Ortona, Venkata Vamsikrishna Meduri, and Paolo Papotti. 2018. Robust discovery of positive and negative rules in knowledge bases. In Proceed- ings of the 2018 IEEE 34th International Conference on Data Engineering, ICDE '18, pages 1168-1179, Paris, France. IEEE.</p>
<p>Language models as knowledge bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingHong Kong, ChinaAssociation for Computational Linguistics19Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowl- edge bases? In Proceedings of the 2019 Confer- ence on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing, EMNLP- IJCNLP '19, pages 2463-2473, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Beyond accuracy: Behavioral testing of NLP models with CheckList. Tongshuang Marco Tulio Ribeiro, Carlos Wu, Sameer Guestrin, Singh, 10.18653/v1/2020.acl-main.442Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20. the 58th Annual Meeting of the Association for Computational Linguistics, ACL '20Online. Association for Computational LinguisticsMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Behav- ioral testing of NLP models with CheckList. In Pro- ceedings of the 58th Annual Meeting of the Associa- tion for Computational Linguistics, ACL '20, pages 4902-4912, Online. Association for Computational Linguistics.</p>
<p>A primer in BERTology: What we know about how BERT works. Anna Rogers, Olga Kovaleva, Anna Rumshisky, 10.1162/tacl_a_00349Transactions of the Association for Computational Linguistics. 8Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. A primer in BERTology: What we know about how BERT works. Transactions of the Associ- ation for Computational Linguistics, 8:842-866.</p>
<p>. Roy Schwartz, Jesse Dodge, Noah A Smith, Oren Etzioni, 10.1145/33818312020. Green AI. Commun. ACM. 6312Roy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. 2020. Green AI. Commun. ACM, 63(12):54-63.</p>
<p>The woman worked as a babysitter: On biases in language generation. Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng, 10.18653/v1/D19-1339Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP '19. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP '19Hong Kong, ChinaAssociation for Computational LinguisticsEmily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. In Proceedings of the 2019 Conference on Empiri- cal Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP '19, pages 3407-3412, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Energy and policy considerations for deep learning in NLP. Emma Strubell, Ananya Ganesh, Andrew Mccallum, 10.18653/v1/P19-1355Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL '19. the 57th Annual Meeting of the Association for Computational Linguistics, ACL '19Florence, ItalyAssociation for Computational LinguisticsEmma Strubell, Ananya Ganesh, and Andrew McCal- lum. 2019. Energy and policy considerations for deep learning in NLP. In Proceedings of the 57th Annual Meeting of the Association for Computa- tional Linguistics, ACL '19, pages 3645-3650, Flo- rence, Italy. Association for Computational Linguis- tics.</p>
<p>ProofWriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, 10.18653/v1/2021.findings-acl.317Findings of the Association for Computational Linguistics, ACL-IJCNLP '21. Online. Association for Computational LinguisticsOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating implications, proofs, and abductive statements over natural language. In Find- ings of the Association for Computational Linguis- tics, ACL-IJCNLP '21, pages 3621-3634, Online. Association for Computational Linguistics.</p>
<p>Alon Talmor, Yanai Elazar, Yoav Goldberg, Jonathan Berant, 10.1162/tacl_a_003422020a. oLMpics-on what language model pre-training captures. Transactions of the Association for Computational Linguistics. 8Alon Talmor, Yanai Elazar, Yoav Goldberg, and Jonathan Berant. 2020a. oLMpics-on what language model pre-training captures. Transactions of the As- sociation for Computational Linguistics, 8:743-758.</p>
<p>Leap-of-thought: Teaching pre-trained models to systematically reason over implicit knowledge. Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Goldberg, Jonathan Berant, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. Online33Alon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold- berg, and Jonathan Berant. 2020b. Leap-of-thought: Teaching pre-trained models to systematically rea- son over implicit knowledge. In Advances in Neu- ral Information Processing Systems 33: Annual Con- ference on Neural Information Processing Systems 2020, volume 33 of NeurIPS '20, pages 20227- 20237, Online.</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Proceedings of the 7th International Conference on Learning Representations, ICLR '19. the 7th International Conference on Learning Representations, ICLR '19New Orleans, Louisiana, USAOpenReview.netAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019a. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In Pro- ceedings of the 7th International Conference on Learning Representations, ICLR '19, New Orleans, Louisiana, USA. OpenReview.net.</p>
<p>Learning deep transformer models for machine translation. Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, Lidia S Chao, 10.18653/v1/P19-1176Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, ACL '19. the 57th Annual Meeting of the Association for Computational Linguistics, ACL '19Florence, ItalyAssociation for Computational LinguisticsQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao. 2019b. Learning deep transformer models for ma- chine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, ACL '19, pages 1810-1822, Florence, Italy. Association for Computational Linguistics.</p>
<p>What do position embeddings learn? An empirical study of pretrained language model positional encoding. Yu-An Wang, Yun-Nung Chen, 10.18653/v1/2020.emnlp-main.555Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP '20. the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP '20Online. Association for Computational LinguisticsYu-An Wang and Yun-Nung Chen. 2020. What do po- sition embeddings learn? An empirical study of pre- trained language model positional encoding. In Pro- ceedings of the 2020 Conference on Empirical Meth- ods in Natural Language Processing, EMNLP '20, pages 6840-6849, Online. Association for Compu- tational Linguistics.</p>
<p>Towards AI-complete question answering: A set of prerequisite toy tasks. Jason Weston, Antoine Bordes, Sumit Chopra, Tomás Mikolov, Proceedings of the 4th International Conference on Learning Representations, ICLR '16. the 4th International Conference on Learning Representations, ICLR '16San Juan, Puerto RicoJason Weston, Antoine Bordes, Sumit Chopra, and Tomás Mikolov. 2016. Towards AI-complete ques- tion answering: A set of prerequisite toy tasks. In Proceedings of the 4th International Conference on Learning Representations, ICLR '16, San Juan, Puerto Rico.</p>
<p>Differentiable learning of logical rules for knowledge base reasoning. Fan Yang, Zhilin Yang, William W Cohen, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems. Long Beach, California, USA, NeurIPS17Fan Yang, Zhilin Yang, and William W. Cohen. 2017. Differentiable learning of logical rules for knowl- edge base reasoning. In Advances in Neural Infor- mation Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, California, USA, NeurIPS '17, pages 2319-2328.</p>
<p>Hurtful words: Quantifying biases in clinical contextual word embeddings. Haoran Zhang, Amy X Lu, Mohamed Abdalla, Matthew Mcdermott, Marzyeh Ghassemi, 10.1145/3368555.3384448Proceedings of the ACM Conference on Health, Inference, and Learning, CHIL '20. the ACM Conference on Health, Inference, and Learning, CHIL '20Toronto, Ontario, CanadaAssociation for Computing MachineryHaoran Zhang, Amy X. Lu, Mohamed Abdalla, Matthew McDermott, and Marzyeh Ghassemi. 2020. Hurtful words: Quantifying biases in clinical con- textual word embeddings. In Proceedings of the ACM Conference on Health, Inference, and Learn- ing, CHIL '20, page 110-120, Toronto, Ontario, Canada. Association for Computing Machinery.</p>
<p>Evaluating commonsense in pretrained language models. Xuhui Zhou, Yue Zhang, Leyang Cui, Dandan Huang, 10.1609/aaai.v34i05.6523Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceOnline. AAAI Press34Xuhui Zhou, Yue Zhang, Leyang Cui, and Dandan Huang. 2020. Evaluating commonsense in pre- trained language models. In Proceedings of the AAAI Conference on Artificial Intelligence, vol- ume 34 of AAAI '20, pages 9733-9740, Online. AAAI Press.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9163 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9163</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9163</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-278339524</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.03049v2.pdf" target="_blank">34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are reshaping many aspects of materials science and chemistry research, enabling advances in molecular property prediction, materials design, scientific automation, knowledge extraction, and more. Recent developments demonstrate that the latest class of models are able to integrate structured and unstructured data, assist in hypothesis generation, and streamline research workflows. To explore the frontier of LLM capabilities across the research lifecycle, we review applications of LLMs through 34 total projects developed during the second annual Large Language Model Hackathon for Applications in Materials Science and Chemistry, a global hybrid event. These projects spanned seven key research areas: (1) molecular and material property prediction, (2) molecular and material design, (3) automation and novel interfaces, (4) scientific communication and education, (5) research data management and automation, (6) hypothesis generation and evaluation, and (7) knowledge extraction and reasoning from the scientific literature. Collectively, these applications illustrate how LLMs serve as versatile predictive models, platforms for rapid prototyping of domain-specific tools, and much more. In particular, improvements in both open source and proprietary LLM performance through the addition of reasoning, additional training data, and new techniques have expanded effectiveness, particularly in low-data environments and interdisciplinary research. As LLMs continue to improve, their integration into scientific workflows presents both new opportunities and new challenges, requiring ongoing exploration, continued refinement, and further research to address reliability, interpretability, and reproducibility.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9163.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9163.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-phonon-regressor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 (fine-tuned text regressor for phonon DOS peak prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuned Llama 3 models were trained on textual descriptions of crystal structures (Robocrystallographer and LobsterPy orbital/bonding analyses) to directly predict the highest-frequency phonon DOS peak (a vibrational/thermal material property).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama 3 family models fine-tuned using the Alpaca prompt format on textual descriptions of crystal structures; specific model size not stated in the manuscript. Fine-tuning used 1,264 examples (train/val/test split 0.64/0.2/0.16), 10 epochs, text->numeric output conversion.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computational materials science / vibrational properties of crystalline solids (phonon density of states prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Predict the highest-frequency peak in the phonon density of states (phonon DOS) from text descriptions of crystalline structures, including orbital-based bonding analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) between predicted and reference peak frequencies (units: cm^-1).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Robocrystallographer-only input: MAE = 44 cm^-1; Robocrystallographer + LobsterPy (orbital/bonding) input: MAE = 38 cm^-1. These values lie within the MatBench range reported at time of writing (29–68 cm^-1).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Inclusion of orbital-based bonding information (LobsterPy) improved accuracy; dataset size (1264 examples initially, later extended to ~13,000), prompt/Alpaca format, textual-to-numeric output conversion, model architecture/parameter count and fine-tuning strategy (encoder-decoder vs encoder-only), input sequence length limits, and choice of training hyperparameters all identified as relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to MatBench test-suite models (benchmark MAE range 29–68 cm^-1) and prior Random Forest analyses that showed bond strengths are predictive for this target; the fine-tuned Llama3 models are comparable to mid-range MatBench performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Preliminary results: models not exhaustively analyzed or optimized; textual output needed conversion to numeric which may introduce errors; limited initial dataset (1264) may limit generalization; model architecture choices for regression (e.g., encoder-only adaptations) were not explored in this pilot; potential sensitivity to prompt format and numeric parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend further model adaptation for regression tasks (e.g., encoder-only modifications), experimenting with models of different parameter counts (smaller models sometimes fine-tune better for property regression), expanding training data (they extended to ~13,000 materials), testing other thermal/elastic properties, and enriching LobsterPy-generated descriptions (e.g., computed charges) to further improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9163.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9163.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangSim atomistic agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangSim — LLM-driven interface/agent for atomistic simulation and inverse materials design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based agent (integrated via LangChain) that calls pre-defined atomistic simulation workflows (pyiron/ASE with EMT and MACE forcefields) to perform inverse design tasks such as finding alloy compositions that match a user-specified bulk modulus.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The paper does not specify a particular conversational LLM model family used for the LangSim agent; LangChain is used to let an LLM call Python functions that run pyiron workflows. The project leverages atomistic foundation models/forcefields (MACE foundation model, EMT) to compute properties.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Atomistic materials simulation / computational materials science (alloy property prediction/inverse design)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Inverse design: identify binary alloy concentration (Cu–Au) such that computed bulk modulus matches a user-specified target (~145 GPa ± 2 GPa) by iteratively calling atomistic simulation workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Direct computed bulk modulus (GPa) from atomistic simulation (EMT/MACE); success measured by difference from target bulk modulus and whether result falls within requested uncertainty bounds (± GPa).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported example: agent returned Cu–Au composition of ~73.8% Cu / 26.2% Au giving a computed bulk modulus ≈ 145.58 GPa, which is within the target 145 GPa ± 2 GPa (i.e., within the requested tolerance).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Accuracy depends on fidelity of underlying simulation workflow (EMT and MACE forcefields), correctness and limits of pre-defined workflows, quality of the LLM's tool-calling and planning, the numerical precision of simulation codes, and how well the forcefield captures alloy physics; constraining the LLM to validated workflows reduces hallucination and improves reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Validation performed by running the actual atomistic simulation (EMT/MACE) to compute bulk modulus and compare to the target; no direct comparison to other LLMs or purely predictive surrogates is reported in this example.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Model identity and sensitivity not reported; reliance on forcefield approximations (EMT/MACE) means computed property accuracy is limited by those models; potential for LLM hallucination if allowed to construct arbitrary workflows was mitigated by restricting available tools; scalability and generality beyond the demonstrated example not exhaustively evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors advocate restricting LLMs to pre-defined, validated simulation workflows (reduces hallucinations), integrating LLMs into active learning/closed-loop discovery with on-the-fly simulations, and validating LLM-suggested designs by executing real simulations; they highlight the promise of LLMs as controllers of external high-fidelity simulators rather than standalone simulators.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9163.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9163.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct-MOF-Agent (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Leveraging AI Agents for Designing Low Band Gap Metal–Organic Frameworks (ReAct agent using GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A ReAct-style AI agent that uses GPT-4 (temperature 0.1) with retrieval-augmented generation over research papers to suggest SMILES modifications to MOFs for lowering band gap; proposed candidates are validated for chemical feasibility (RDKit) and evaluated for band gap using a surrogate transformer ensemble (MOFormer).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 used as the central language model in a ReAct agent pipeline (temperature 0.1). The LLM is used for literature-guided design, natural-language reasoning, and proposing structure modifications (SMILES), while property prediction is handled by a transformer surrogate (MOFormer ensemble).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Materials design / computational materials chemistry — band gap engineering of metal–organic frameworks (MOFs)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-driven generation of new MOF candidates (SMILES) aimed at reducing electronic band gap; subsequent property inference via a surrogate model ensemble that predicts band gap and associated uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Surrogate model Mean Absolute Error (MAE) in eV for band gap prediction; surrogate ensemble mean and standard deviation used for uncertainty quantification; chemical validity checks by RDKit.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported surrogate transformer ensemble MAE ≈ 0.467 eV (trained/fine-tuned on QMOF labels); baseline MOFormer (pre-trained on 400k entries) achieves MAE ≈ 0.387 eV. The agent uses the surrogate and uncertainty estimates to guide design but no end-to-end experimental validation of band gaps reported.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Surrogate pretraining dataset size and transfer learning (pretraining on structure embeddings), ensemble construction and uncertainty estimation, quality of retrieved literature guidelines (RAG), chemical feasibility enforcement (RDKit), LLM temperature and prompt engineering, and limits imposed on modifications (e.g., constrained metal-node changes).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared the team's transformer surrogate (MAE ≈ 0.467 eV) against MOFormer pre-trained on 400k entries (MAE ≈ 0.387 eV); GPT-4 is noted as a hyperparameter – other LLMs could be substituted but were not benchmarked here.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Surrogate error is non-negligible (≈0.4–0.5 eV); agent's metal-node modification ability was limited by prompt/input constraints; ultimate validation requires higher-fidelity computations or experiments; LLM itself is a generator not the property predictor, so design quality depends on surrogate accuracy and RAG quality.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend integrating uncertainty quantification (as done with ensemble std dev), using RAG to extract design heuristics from literature, validating candidates with higher-fidelity computations or experiments, and treating the LLM choice as a tunable hyperparameter — also implying improvements via larger/better-pretrained surrogates or more extensive pretraining datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llm-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions <em>(Rating: 2)</em></li>
                <li>A Quantum-Chemical Bonding Database for Solid-State Materials (JSONS: Part 1) <em>(Rating: 2)</em></li>
                <li>Lobsterpy: A package to automatically analyze lobster runs <em>(Rating: 2)</em></li>
                <li>A foundation model for atomistic materials chemistry <em>(Rating: 2)</em></li>
                <li>pyiron: An integrated development environment for computational materials science <em>(Rating: 1)</em></li>
                <li>MoFormer: self-supervised transformer model for metal-organic framework property prediction <em>(Rating: 2)</em></li>
                <li>QMOF database <em>(Rating: 1)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9163",
    "paper_id": "paper-278339524",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "Llama3-phonon-regressor",
            "name_full": "Llama 3 (fine-tuned text regressor for phonon DOS peak prediction)",
            "brief_description": "Fine-tuned Llama 3 models were trained on textual descriptions of crystal structures (Robocrystallographer and LobsterPy orbital/bonding analyses) to directly predict the highest-frequency phonon DOS peak (a vibrational/thermal material property).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3",
            "model_description": "Llama 3 family models fine-tuned using the Alpaca prompt format on textual descriptions of crystal structures; specific model size not stated in the manuscript. Fine-tuning used 1,264 examples (train/val/test split 0.64/0.2/0.16), 10 epochs, text-&gt;numeric output conversion.",
            "scientific_subdomain": "Computational materials science / vibrational properties of crystalline solids (phonon density of states prediction)",
            "simulation_task": "Predict the highest-frequency peak in the phonon density of states (phonon DOS) from text descriptions of crystalline structures, including orbital-based bonding analysis.",
            "evaluation_metric": "Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) between predicted and reference peak frequencies (units: cm^-1).",
            "simulation_accuracy": "Robocrystallographer-only input: MAE = 44 cm^-1; Robocrystallographer + LobsterPy (orbital/bonding) input: MAE = 38 cm^-1. These values lie within the MatBench range reported at time of writing (29–68 cm^-1).",
            "factors_affecting_accuracy": "Inclusion of orbital-based bonding information (LobsterPy) improved accuracy; dataset size (1264 examples initially, later extended to ~13,000), prompt/Alpaca format, textual-to-numeric output conversion, model architecture/parameter count and fine-tuning strategy (encoder-decoder vs encoder-only), input sequence length limits, and choice of training hyperparameters all identified as relevant.",
            "comparison_baseline": "Compared to MatBench test-suite models (benchmark MAE range 29–68 cm^-1) and prior Random Forest analyses that showed bond strengths are predictive for this target; the fine-tuned Llama3 models are comparable to mid-range MatBench performance.",
            "limitations_or_failure_cases": "Preliminary results: models not exhaustively analyzed or optimized; textual output needed conversion to numeric which may introduce errors; limited initial dataset (1264) may limit generalization; model architecture choices for regression (e.g., encoder-only adaptations) were not explored in this pilot; potential sensitivity to prompt format and numeric parsing.",
            "author_recommendations_or_insights": "Authors recommend further model adaptation for regression tasks (e.g., encoder-only modifications), experimenting with models of different parameter counts (smaller models sometimes fine-tune better for property regression), expanding training data (they extended to ~13,000 materials), testing other thermal/elastic properties, and enriching LobsterPy-generated descriptions (e.g., computed charges) to further improve accuracy.",
            "uuid": "e9163.0",
            "source_info": {
                "paper_title": "34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LangSim atomistic agent",
            "name_full": "LangSim — LLM-driven interface/agent for atomistic simulation and inverse materials design",
            "brief_description": "An LLM-based agent (integrated via LangChain) that calls pre-defined atomistic simulation workflows (pyiron/ASE with EMT and MACE forcefields) to perform inverse design tasks such as finding alloy compositions that match a user-specified bulk modulus.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "",
            "model_description": "The paper does not specify a particular conversational LLM model family used for the LangSim agent; LangChain is used to let an LLM call Python functions that run pyiron workflows. The project leverages atomistic foundation models/forcefields (MACE foundation model, EMT) to compute properties.",
            "scientific_subdomain": "Atomistic materials simulation / computational materials science (alloy property prediction/inverse design)",
            "simulation_task": "Inverse design: identify binary alloy concentration (Cu–Au) such that computed bulk modulus matches a user-specified target (~145 GPa ± 2 GPa) by iteratively calling atomistic simulation workflows.",
            "evaluation_metric": "Direct computed bulk modulus (GPa) from atomistic simulation (EMT/MACE); success measured by difference from target bulk modulus and whether result falls within requested uncertainty bounds (± GPa).",
            "simulation_accuracy": "Reported example: agent returned Cu–Au composition of ~73.8% Cu / 26.2% Au giving a computed bulk modulus ≈ 145.58 GPa, which is within the target 145 GPa ± 2 GPa (i.e., within the requested tolerance).",
            "factors_affecting_accuracy": "Accuracy depends on fidelity of underlying simulation workflow (EMT and MACE forcefields), correctness and limits of pre-defined workflows, quality of the LLM's tool-calling and planning, the numerical precision of simulation codes, and how well the forcefield captures alloy physics; constraining the LLM to validated workflows reduces hallucination and improves reliability.",
            "comparison_baseline": "Validation performed by running the actual atomistic simulation (EMT/MACE) to compute bulk modulus and compare to the target; no direct comparison to other LLMs or purely predictive surrogates is reported in this example.",
            "limitations_or_failure_cases": "Model identity and sensitivity not reported; reliance on forcefield approximations (EMT/MACE) means computed property accuracy is limited by those models; potential for LLM hallucination if allowed to construct arbitrary workflows was mitigated by restricting available tools; scalability and generality beyond the demonstrated example not exhaustively evaluated.",
            "author_recommendations_or_insights": "Authors advocate restricting LLMs to pre-defined, validated simulation workflows (reduces hallucinations), integrating LLMs into active learning/closed-loop discovery with on-the-fly simulations, and validating LLM-suggested designs by executing real simulations; they highlight the promise of LLMs as controllers of external high-fidelity simulators rather than standalone simulators.",
            "uuid": "e9163.1",
            "source_info": {
                "paper_title": "34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "ReAct-MOF-Agent (GPT-4)",
            "name_full": "Leveraging AI Agents for Designing Low Band Gap Metal–Organic Frameworks (ReAct agent using GPT-4)",
            "brief_description": "A ReAct-style AI agent that uses GPT-4 (temperature 0.1) with retrieval-augmented generation over research papers to suggest SMILES modifications to MOFs for lowering band gap; proposed candidates are validated for chemical feasibility (RDKit) and evaluated for band gap using a surrogate transformer ensemble (MOFormer).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4 used as the central language model in a ReAct agent pipeline (temperature 0.1). The LLM is used for literature-guided design, natural-language reasoning, and proposing structure modifications (SMILES), while property prediction is handled by a transformer surrogate (MOFormer ensemble).",
            "scientific_subdomain": "Materials design / computational materials chemistry — band gap engineering of metal–organic frameworks (MOFs)",
            "simulation_task": "Text-driven generation of new MOF candidates (SMILES) aimed at reducing electronic band gap; subsequent property inference via a surrogate model ensemble that predicts band gap and associated uncertainty.",
            "evaluation_metric": "Surrogate model Mean Absolute Error (MAE) in eV for band gap prediction; surrogate ensemble mean and standard deviation used for uncertainty quantification; chemical validity checks by RDKit.",
            "simulation_accuracy": "Reported surrogate transformer ensemble MAE ≈ 0.467 eV (trained/fine-tuned on QMOF labels); baseline MOFormer (pre-trained on 400k entries) achieves MAE ≈ 0.387 eV. The agent uses the surrogate and uncertainty estimates to guide design but no end-to-end experimental validation of band gaps reported.",
            "factors_affecting_accuracy": "Surrogate pretraining dataset size and transfer learning (pretraining on structure embeddings), ensemble construction and uncertainty estimation, quality of retrieved literature guidelines (RAG), chemical feasibility enforcement (RDKit), LLM temperature and prompt engineering, and limits imposed on modifications (e.g., constrained metal-node changes).",
            "comparison_baseline": "Compared the team's transformer surrogate (MAE ≈ 0.467 eV) against MOFormer pre-trained on 400k entries (MAE ≈ 0.387 eV); GPT-4 is noted as a hyperparameter – other LLMs could be substituted but were not benchmarked here.",
            "limitations_or_failure_cases": "Surrogate error is non-negligible (≈0.4–0.5 eV); agent's metal-node modification ability was limited by prompt/input constraints; ultimate validation requires higher-fidelity computations or experiments; LLM itself is a generator not the property predictor, so design quality depends on surrogate accuracy and RAG quality.",
            "author_recommendations_or_insights": "Authors recommend integrating uncertainty quantification (as done with ensemble std dev), using RAG to extract design heuristics from literature, validating candidates with higher-fidelity computations or experiments, and treating the LLM choice as a tunable hyperparameter — also implying improvements via larger/better-pretrained surrogates or more extensive pretraining datasets.",
            "uuid": "e9163.2",
            "source_info": {
                "paper_title": "34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llm-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions",
            "rating": 2,
            "sanitized_title": "llmprop_predicting_physical_and_electronic_properties_of_crystalline_solids_from_their_text_descriptions"
        },
        {
            "paper_title": "A Quantum-Chemical Bonding Database for Solid-State Materials (JSONS: Part 1)",
            "rating": 2,
            "sanitized_title": "a_quantumchemical_bonding_database_for_solidstate_materials_jsons_part_1"
        },
        {
            "paper_title": "Lobsterpy: A package to automatically analyze lobster runs",
            "rating": 2,
            "sanitized_title": "lobsterpy_a_package_to_automatically_analyze_lobster_runs"
        },
        {
            "paper_title": "A foundation model for atomistic materials chemistry",
            "rating": 2,
            "sanitized_title": "a_foundation_model_for_atomistic_materials_chemistry"
        },
        {
            "paper_title": "pyiron: An integrated development environment for computational materials science",
            "rating": 1,
            "sanitized_title": "pyiron_an_integrated_development_environment_for_computational_materials_science"
        },
        {
            "paper_title": "MoFormer: self-supervised transformer model for metal-organic framework property prediction",
            "rating": 2,
            "sanitized_title": "moformer_selfsupervised_transformer_model_for_metalorganic_framework_property_prediction"
        },
        {
            "paper_title": "QMOF database",
            "rating": 1,
            "sanitized_title": "qmof_database"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 1,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        }
    ],
    "cost": 0.0158285,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Leveraging Orbital-Based Bonding Analysis Information in LLMs
16 May 2025</p>
<p>Yoel Zimmermann 0009-0003-1720-4368
Adib Bazgir 0000-0001-6475-8505
University of Missouri-Columbia</p>
<p>Alexander Al-Feghali 0009-0004-8377-7049
McGill University</p>
<p>Mehrad Ansari 0000-0001-5696-9193
Acceleration Consortium</p>
<p>Joshua Bocarsly 0000-0002-7523-152X
University of Houston</p>
<p>L Catherine Brinson 0000-0003-2551-1563
Duke University</p>
<p>Yuan Chiang 0000-0002-4017-7084
University of California at Berkeley</p>
<p>Lawrence Berkeley National Laboratory</p>
<p>Defne Circi 0000-0002-5761-0198
Duke University</p>
<p>Min-Hsueh Chiu 0000-0003-0637-7856
University of Southern California</p>
<p>Nathan Daelman 0000-0002-7647-1816
Humboldt University of Berlin</p>
<p>Matthew L Evans 0000-0002-1182-9098
Université catholique de Louvain</p>
<p>Abhijeet S Gangan 
University of California
Los Angeles</p>
<p>Janine George 0000-0001-8907-0336
Friedrich-Schiller-Universität Jena 13 Federal Institute of Materials Research and Testing (BAM</p>
<p>Hassan Harb 0000-0002-6016-3122
Argonne National Laboratory</p>
<p>Ghazal Khalighinejad 0009-0005-2476-8043
Duke University</p>
<p>Sartaaj Takrim Khan 0009-0009-2131-9700
University of Toronto</p>
<p>Sascha Klawohn 0000-0003-4850-776X
Humboldt University of Berlin</p>
<p>Magdalena Lederbauer 0009-0008-0665-1839
Soroush Mahjoubi 0000-0001-8879-5431
Massachusetts Institute of Technology</p>
<p>Bernadette Mohr 0000-0003-0903-0073
Humboldt University of Berlin</p>
<p>University of Amsterdam 18 KU
Leuven</p>
<p>Seyed Mohamad Moosavi 0000-0002-0357-5729
Acceleration Consortium</p>
<p>University of Toronto</p>
<p>Aakash Naik 0000-0002-6071-6786
Friedrich-Schiller-Universität Jena 13 Federal Institute of Materials Research and Testing (BAM</p>
<p>Aleyna Beste Ozhan 0000-0002-0281-3860
Massachusetts Institute of Technology</p>
<p>Dieter Plessers 0000-0001-8906-8447
PiyushAritra Roy 0000-0003-0243-9124
London South Bank University
20 EPFL</p>
<p>Fabian Schöppach 
Humboldt University of Berlin</p>
<p>Philippe Schwaller 0000-0003-3046-6576
Carla Terboven 
Helmholtz-Zentrum Berlin für Materialien und Energie GmbH</p>
<p>Katharina Ueltzen 0009-0003-2967-1182
Friedrich-Schiller-Universität Jena 13 Federal Institute of Materials Research and Testing (BAM</p>
<p>Yue Wu 0000-0003-2874-8267
Independent Researcher</p>
<p>Shang Zhu 0000-0002-8433-8599
University of Michigan
Ann Arbor</p>
<p>Jan Janssen 0000-0001-9948-7119
Max-Planck Institute for Sustainable Materials 24 Fum Technologies
Inc</p>
<p>Calvin Li 
Ian Foster 0000-0003-2129-5269
Argonne National Laboratory</p>
<p>University of Chicago</p>
<p>Ben Blaiszik blaiszik@uchicago.edu 0000-0002-5326-4902
Argonne National Laboratory</p>
<p>University of Chicago</p>
<p>Eth Zurich 
Janine George Github 
Federico Ottomano 
Elena Patyukova 
Judith Clymo 
Dmytro Antypov 
Chi Zhang 
Ranjan Maharana 
Weijie Zhang 
Xuefeng Liu 
Erik Bitzek Github 
Anastasiia Tsymbal 
Oleksandr Narykov 
Dana O'connor 
Shagun Maheshwari 
Stanley Lo 
Archit Vasan 
Zartashia Afzal 
Kevin Shen Github 
Jan Weinreich 
Ankur K Gupta 
Amirhossein D Naghdi 
Tyler Josephson 
Fariha Agbere 
Kevin Ishimwe 
Colin Jones 
Charishma Puli 
Samiha Sharlin 
Hao Liu Github 
Andres M Bran 
Anna Borisova 
Marcel M Calderon 
Mark Tropin 
Takrim Sartaaj 
Mahyar Khan 
SeyedMohamad Rajabi 
Amro Aswad Moosavi 
Github 
Alessandro Canalicchio 
Alexander Moßhammer 
Tehseen Rug 
Christoph Völker Github 
Yuan Langsim 
Giuseppe Chiang 
Greg Fisicaro 
Sarom Juhasz 
Bernadette Leang 
Utkarsh Mohr 
Francesco Pratiush 
Leopold Ricci 
Pablo A Talirz 
Trung Unzueta 
Gabriel Vo 
Sebastian Vogel 
Jan Pagel 
Janssen Github 
Marcel Schloz 
Jose C Gonzalez Github 
Chiku Parida 
Martin H Petersen Github 
Luis Pinto 
Xuan Vu Nguyen 
Tirtha Vinchurkar 
Pradip Si 
Kuman Suneel </p>
<p>3D Molecular Feature Vectors for Large Language Models</p>
<p>Imran GitHub LLMSpectrometry</p>
<p>Rob Mills
Philippe Schwaller GitHub Leveraging AI Agents for Designing Low Band Gap Metal-Organic Frameworks</p>
<p>How Low Can You Go? Leveraging Small LLMs for Material Design</p>
<p>Leveraging Orbital-Based Bonding Analysis Information in LLMs
16 May 202538D9586097FCFED3455C45FD15C3CAFFarXiv:2505.03049v2[cs.LG]Towards AutomationAssistantsAgentsand Accelerated Scientific Discovery Harnessing Language Model for Density Functional Theory (DFT) Parameter Suggestion
Large Language Models (LLMs) are reshaping many aspects of materials science and chemistry research, enabling advances in molecular property prediction, materials design, scientific automation, knowledge extraction, and more.Recent developments demonstrate that the latest class of models are able to integrate structured and unstructured data, assist in hypothesis generation, and streamline research workflows.To explore the frontier of LLM capabilities across the research lifecycle, we review applications of LLMs through 34 total projects developed during the second annual Large Language Model Hackathon for Applications in Materials Science and Chemistry, a global hybrid event.These projects spanned seven key research areas: (1) molecular and material property prediction, (2) molecular and material design, (3) automation and novel interfaces, (4) scientific communication and education, (5) research data management and automation, (6) hypothesis generation and evaluation, and (7) knowledge extraction and reasoning from the scientific literature.Collectively, these applications illustrate how LLMs serve as versatile predictive models, platforms for rapid prototyping of domain-specific tools, and much more.In particular, improvements in both open source and proprietary LLM performance through the addition of reasoning, additional training data, and new techniques have expanded effectiveness, particularly in low-data environments and interdisciplinary research.As LLMs continue to improve, their integration into scientific workflows presents both new opportunities and new challenges, requiring ongoing exploration, continued refinement, and further research to address reliability, interpretability, and reproducibility.</p>
<p>Introduction</p>
<p>The integration of large language models (LLMs) into scientific workflows is reshaping how researchers approach data-driven discovery, automation, and even scientific reasoning and hypothesis generation [1,2,3,4].In chemistry and materials science, fields characterized by complex data modalities, heterogeneous data formats, sparse experimental datasets, and fragmented knowledge ecosystems, LLMs are emerging as versatile tools capable of bridging gaps between computational methods, experimental data, literature and text sources, and domain expertise [5,6,7,8,9,10,11,12]. Early applications have already demonstrated potential applicability in tasks ranging from molecular property prediction [13,14,15] to automated laboratory workflows [16,17] and development of novel user interfaces [18,19].As illustrated in Figure 1, we note that there is a significant opportunity for these broad new capabilities to be incorporated throughout the scientific research lifecycle; from initial ideation through experimental execution to communication, learning, and further iteration.</p>
<p>However, the rapidity of change and the nearly constant release of models with higher performance, lower cost, and wider application spaces, and release of other platform capabilities (e.g., agentic tools, deep research modalities) make it challenging to keep pace, necessitating a collaborative and interdisciplinary effort to identify high-impact use cases, address specific limitations, and prototype applications to catalyze deeper study [20,21,22,23,24,25,26].Towards this goal, we believe that accessing the wisdom of the crowd via science hackathons provides a powerful, and dynamic framework for fostering collaboration building, knowledge exchange, innovation, and incentivizing the rapid problem-solving and exploration needed to realize the benefit of these new models for scientific discovery in materials science and chemistry [27,28,29,30].</p>
<p>In this work, we describe and analyze select applications developed as part of the second Large Language Model Hackathon for Applications in Materials Science and Chemistry [30], detailing the broad classes of problems addressed by teams and highlighting trends in the approaches taken.We categorize the 34 submissions into seven key research areas and provide an overview of team contributions with highlights drawn from exemplar projects in each research area.We also present a summary table containing team details and code repository links for all submissions to offer a comprehensive view of the innovations demonstrated during the event.</p>
<p>Finally, we discuss the broader conclusions of the hackathon, emphasizing its role in fostering interdisciplinary collaboration, accelerating the adoption of artificial intelligence (AI) in scientific research [27,28,29], and identifying key challenges that require further investigation.By examining these contributions, we provide insight into how structured collaborative frameworks can drive the systematic integration of LLMs into chemistry and materials science to accelerate research, improve researcher efficiency, and shape the future of AI-driven discovery.</p>
<p>Overview of Submissions</p>
<p>The hackathon resulted in 34 team submissions (with 32 submissions providing detailed descriptions), covering a broad spectrum of materials science and chemistry applications.The submissions and links to the respective source code repositories are listed in Table 1.We categorized projects based on their primary objectives, clustering them into seven key areas, forming a constellation of new capabilities across the research lifecycle:</p>
<ol>
<li>
<p>Molecular and Material Property Prediction: Forecasting chemical and physical properties of molecules and materials using LLMs, particularly excelling in low-data environments and combining structured and unstructured data.</p>
</li>
<li>
<p>Molecular and Material Design: Generating and optimizing novel molecules and materials using LLMs, including peptides, metal-organic frameworks, and sustainable construction materials.</p>
</li>
<li>
<p>Automation and Novel Interfaces: Developing natural language interfaces and LLM-powered automated workflows to simplify complex scientific tasks, making advanced tools and techniques more accessible to researchers.</p>
</li>
</ol>
<p>Scientific Communication and Education:</p>
<p>Enhancing academic communication, automating educational content creation, and supporting learning in materials science and chemistry.</p>
<p>Research Data Management and Automation:</p>
<p>Streamlining the handling, organization, and processing of scientific data through LLM-powered tools and multimodal agents.</p>
<p>Hypothesis Generation and Evaluation:</p>
<p>Using LLMs to generate, assess, and refine scientific hypotheses, leveraging multiple AI agents and statistical approaches.</p>
<p>Knowledge Extraction and Reasoning:</p>
<p>Extracting structured information from scientific literature and performing sophisticated reasoning about chemical and materials science concepts through knowledge graphs and multimodal approaches.</p>
<p>Collectively, this constellation of capabilities, shown in Figure 1, is applicable to long-standing challenges across the research lifecycle, creating a flywheel of improvements that promises to empower researchers with new capabilities and to speed the research process.</p>
<p>Table 1: Overview of the tools developed by the various tools, and links to source code repositories.Full descriptions of the projects can be found in Ref. [31].</p>
<p>Project Authors Links</p>
<p>LLMy</p>
<p>GitHub</p>
<p>We next discuss the constellation of capabilities in more detail and highlight exemplar projects across each key application area.</p>
<p>Molecular and Material Property Prediction</p>
<p>LLMs have rapidly advanced in molecular and material property prediction, employing both textual and numerical data to forecast a wide range of properties.Recent studies [1,15,13,3] show LLMs performing comparably to, or even surpassing, conventional machine learning methods, particularly in low-data environments.The flexibility in processing both structured and unstructured data [32], as well as their general applicability to regression tasks [33], make LLMs a powerful tool for diverse predictive tasks in molecular and materials science.</p>
<p>Leveraging orbital-based bonding analysis information in LLMs for material property predictions</p>
<p>Previous studies have used different strategies to learn material properties using LLMs, such as enriching graph neural network (GNN) features with LLM embeddings [4], training domain-specific LLMs and customizing model architectures [5,6,7], or fine-tuning general-purpose LLMs [8,14].While exact strategies have differed, existing models predominantly operate on string representations of crystal structures primarily consisting of compositional and structural information commonly found in crystallographic information files (CIFs).Multiple studies have successfully utilized the text descriptions of structures [7,14,8] that can be generated using the Robocrystallographer package [9].These descriptions consist of structural features like bond lengths, coordination polyhedra, lattice parameters, coordinates, structure type, and other descriptors.</p>
<p>Other studies explored different string representations of compositional and structural information [4,6,14].The team behind this submission emphasizes that, to their knowledge, no previous studies investigated including orbital-based bonding analysis information in LLMs for materials property prediction tasks.Thus, in this pilot study, the team tested including such descriptions in LLMs to predict the highest-frequency peak in their phonon density of states (DOS) [11,10].This target is relevant to the thermal properties of materials and it is a tracked component of the MatBench benchmark project.A key hypothesis is that the inclusion of the bonding analysis information for this vibrational property will improve the LLM's performance, as previous studies demonstrated the importance of such bonding information for the same target via a Random Forest model [34].</p>
<p>To test this hypothesis, the team fine-tuned multiple Llama 3 models on the textual description of 1264 crystal structures in the benchmark dataset.The text descriptions were generated using two packages: the Robocrystallographer and LobsterPy package [35].The text descriptions from Lobsterpy consist of orbitalbased bonding analyses containing information on covalent bond strengths and antibonding states.The data used here is available on Zenodo [36] and was generated as part of an earlier dataset publication [34].</p>
<p>During the hackathon, one Llama model was fine-tuned with the Alpaca prompt format using both Robocrystallographer and LobsterPy text descriptions, and another one using solely Robocrystallographer input.Figure 2 depicts the prompt used to fine-tune an LLM to predict the last phonon DOS peak.The train/test/validation split was 0.64/0.2/0.16.The models were trained for 10 epochs with a validation step after each epoch.The textual output was converted back into numerical frequency values for the computation of MAEs and RMSEs.The results show that including bonding-based information improved the model's  prediction.The results also corroborate the team's previous finding that quantum-chemical bond strengths are relevant for this particular target property.Both model performances (Robocrystallographer: 44 cm −1 , Robocrystallographer+LobsterPy: 38 cm −1 ) are comparable to other models of the MatBench test suite, with MAEs ranging from 29 cm −1 to 68 cm −1 as per the time of writing [37].</p>
<p>Although the preliminary results seem promising, the models have not yet been exhaustively analyzed, validated, or optimized yet.As the prediction of a numerical value and not its text embedding is of interest to the task, further model adaptation might be beneficial.For example, Rubungo et al. [7] modified T5, an encoder-decoder model, for regression tasks by removing its decoder and adding a linear layer on top of its encoder.Halving the number of model parameters allowed them to fine-tune on longer input sequences, improving model performance.A recently published benchmark for LLMs in materials property prediction also suggests that fine-tuning models with fewer parameters improves the prediction of materials properties [14].</p>
<p>With the available easy-to-use packages like Unsloth, [38] the team was able to integrate their materials data into fine-tuning an LLM for property prediction with very limited resources and time.Since these initial results, the work has been extended to a dataset of bonding-based text descriptions including 13,000 crystalline materials.In the future, the team aims to (1) test these text descriptions further to learn other thermal and elastic material properties like elastic constants and lattice thermal conductivity and (2) to extend further the text descriptions generated with the LobsterPy package to include, e.g., information on computed charges.</p>
<p>Molecular and Material Design</p>
<p>LLMs have also been applied to molecular and material design, proving capable in both settings [2,39,40,41,42], especially if pre-trained or fine-tuned with domain-specific data [43].However, despite these advancements, LLMs still face limitations in practical applications [44].</p>
<p>Leveraging AI Agents for Designing Low Band Gap Metal-Organic Frameworks</p>
<p>Metal-organic frameworks (MOFs) are known to be excellent candidates for electrocatalysis due to their large surface area, high adsorption capacity at low CO 2 concentrations, and the ability to fine-tune the spatial arrangement of active sites within their crystalline structure [45].Low band gap MOFs are crucial as they efficiently absorb visible light and exhibit higher electrical conductivity, making them suitable for photocatalysis, solar energy conversion, sensors, and optoelectronics.This submission aims at using chemistry-informed ReAct [46] AI Agents to optimize the band gap property of MOFs.The overview of the workflow is presented 3a.The agent takes as inputs a textual representation of the initial MOF structure as a SMILES (Simplified Molecular Input Line-Entry System) string representation, and a short description of the property optimization task (i.e., reducing band gap), all in natural language.This is followed by an iterative closed-loop suggestion of new MOF candidates with a lower band gap with uncertainty quantification, by adjusting the initial MOF given a set of design guidelines automatically obtained from the scientific literature.A detailed analysis of this methodology, including its application to various classes of materials such as surfactants, ligands, and peptides can be found in reference [47], which supports both closed-loop and human-in-the-loop feedback cycles and thus enables real-time property inference for human-AI collaboration in molecular design.</p>
<p>The agent, powered by an LLM, is augmented with a set of tools allowing for chemistry-informed decisionmaking.These tools are as follows:</p>
<ol>
<li>Retrieval-Augmented Generation (RAG): This tool allows the agent to obtain design guidelines on how to adapt the MOF structure from unstructured text.Specifically, in this prototype, the agent has access to a fixed set of 7 MOF research papers (see Refs. [48,49,50,51,52,53,54]) as PDFs.This tool is designed to extract the most relevant sentences from papers in response to a given query.It works by embedding both the paper and the query into numerical vectors using OpenAI's text-ada-002 [55], then identifying the top k passages within the document that either explicitly mention or implicitly suggest the adaptations required for the specified band gap property for a MOF.Inspired by the team's earlier work [56], k is set to 9, but is dynamically adjusted based on the relevant context's length to avoid OpenAI's token limitation.</li>
</ol>
<p>Surrogate Band Gap Predictor</p>
<p>The surrogate model used is a transformer (MOFormer [57]) that takes as input the MOF as a SMILES string.This model is pre-trained using a self-supervised learning technique known as Barlow-Twin [58], where representation learning is done against structure-based embeddings from a crystal graph convolutional neural network (CGCNN) [59].This was done against 16,000 BW20K entries [60].The pre-trained weights are then transferred and fine-tuned to predict the band gap labels taken from 7,450 entries from the QMOF database [61].From a 5-fold training, an ensemble of five transformers are trained to return the mean band gap and the standard deviation, which is used to assess uncertainty for predictions.For comparison, the team's transformer's mean absolute error (MAE) is approximately 0.467, whereas MOFormer, which was pre-trained on 400,000 entries, achieves an MAE of approximately 0.387.</p>
<p>Chemical Feasibility Evaluator</p>
<p>This tool primarily uses RDKit [62] to convert a SMILES string into an RDKit Mol object, and performs several validation steps to ensure chemical feasibility.First, it parses the SMILES string to confirm correct syntax.Next, it validates the atoms and bonds, ensuring they are chemically valid and recognized.It then checks atomic valences to ensure each atom forms a reasonable number of bonds.For ring structures, RDKit verifies the correct ring closure notation.Additionally, it adds implicit hydrogens to satisfy valence requirements and detects aromatic systems, marking relevant atoms and bonds as aromatic.These steps collectively ensure the molecule's basic chemical validity.</p>
<p>The team has used OpenAI's GPT-4 [63] with a temperature of 0.1 as the preferred LLM and LangChain [64] for the application framework development (nonetheless, the team confirms that the choice of LLM is only a hyperparameter and other LLMs can drive the agent).</p>
<p>The new MOF candidates and their corresponding inferred band gap are represented in Figure 3b.The agent starts by retrieving the following design guidelines for low band gap MOFs from research papers: 1. Increasing the conjugation in the linker.2. Selecting electron-rich metal nodes.3. Functionalizing the linker with nitro and amino groups.4. Altering linker length.5. Substitute functional groups (i.e., substituting hydrogen with electron-donating groups on the organic linker).Note that the metal node adaptations are restrained by simply changing the system input prompt.The agent iteratively implements the above strategies, makes changes to the initial MOF, and suggests a new SMILES.The new SMILES is validated using the Chemical Feasibility Evaluator tool, and if found invalid, the agent uses a self-correction feedback loop to suggest new candidates, accounting for the extracted design guidelines.After each valid modification, the band gap of the new MOF is then assessed using the fine-tuned ensemble of surrogate MOFormers to ensure a lower band gap.The self-correction feedback loop also handles new MOFs with undesired higher band gaps with respect to the initial MOF, by reverting to the most recent valid MOF candidate with the lowest band gap identified throughout the iterations.</p>
<p>Automation and Novel Interfaces</p>
<p>LLMs are increasingly important to the modern scientific workflow, enabling the development of more intuitive interfaces for users dealing with complex digital tools.For example, platforms such as ChemCrow [19], RestGPT [65], and HoneyComb [18] allow researchers to input commands in natural language to interact with and analyze complex software and databases.With LLMs, democratized access and dramatically simpler interfaces are possible for programs like specialized computational techniques or command-line interfaces that may previously have required deep expertise.LLMs excel at autonomous planning and task execution in multistep scenarios [16] by breaking complex processes into smaller actions, making experimental or computational workflows controllable by models with less need for direct oversight.Such behavior may include but is not limited to: simple interaction with laboratory robotic systems [66,17], where difficult scientific objectives can be converted into precise, callable commands: the basis of precision and consistency.The integration of LLMs and robotics promises to improve operational efficiency and enable new designs of experimental workflows with increased flexibility.</p>
<p>LangSim -Large Language Model Interface for Atomistic Simulation</p>
<p>LLMs can augment scientists with their common workflows, dramatically simplifying the interactions across systems using natural language input to understand and implement the intent of the user.The LangSim project [67] prototyped an interface to showcase the ability of LLMs to autonomously start atomistic simulations to study material properties on an atomistic scale.This provides the LLM with a way to request and then use novel scientific data and insights that were previously not available in published databases.One might imagine, e.g., the on-the-fly calculation of defect properties, e.g., grain boundary segregation energies In addition, by integrating the LLM in the active learning cycle of an autonomous materials discovery loop, with the option to calculate different material properties and access existing databases, the LLM becomes an AI scientist on a quest to discover novel materials.In this project, straightforward atomistic simulation and agentic scientific reasoning were explored as a natural language interface to users without programming skills.</p>
<p>The LangSim project implements atomistic simulation agents based on both pyiron [68] and LangChain [69].LangChain enables the LLM to call any kind of Python function and include the output in the thought process of the next iteration.In the case of LangSim, these Python functions represent simulation workflows implemented in the pyiron [68] workflow framework to calculate material properties with atomistic simulations.By restricting the LLM to pre-defined simulation workflows, the risk of hallucination is reduced compared to generative approaches, which request the LLM to define and generate the simulation workflow.Based on the MACE [70] foundation model for atomistic simulation, LangSim was used to predict the binary concentration of solid solution alloy required to match a user-defined bulk modulus, demonstrating an inverse materials design approach to enable application-specific alloy design.</p>
<p>LLMicroscopilot: assisting microscope operations through LLMs</p>
<p>While the state-of-the-art microscopes in materials science are crucial for high-resolution imaging and analysis, they are still rather addressed by expert operators due to their complex and steep-cost ownership.Their manipulation involves delicate tasks, mostly involving precision alignment, guaranteed optimal performances, and shifting between different operational modes to address various research questions that require extensive training and experience.This unobtainable quality has not only slowed down routine experimental procedures but has also formed a serious roadblock to opportunities for broadening access and allowing an acceleration of scientific discovery.With progress in natural language processing, LLMs opened the way for a Copernican revolution in this landscape.Integration of LLMs to the microscope interface will allow complex operations to be done through natural language commands.Similar to modern chatbots, which allow even those with no programming knowledge to generate complex computer programs [71], LLMs can become intuitive intermediaries assisting users in traversing the manifold control procedures of advanced microscopes.Early studies of scanning probe microscopy have shown that LLMs can facilitate remote access [72] and even direct control [73] of these instruments, lessening the workload for expert operators.A promising approach is to use an LLM agent that accesses and operates some concrete external tools.These agents also interpret user commands and use observations in real time to make decisions, reducing the hallucinations, or wrong outputs, that sometimes appear with a standalone LLM.This would streamline the user experience, further relieving researchers from having to navigate through complex, tool-specific APIs, thus broadening the reach of advanced microscopes, especially to non-experts.</p>
<p>LangSim Atomistic Simulation Agents</p>
<p>Text, AtomDict  An illustration of such is the work performed by the LLMicroscopilot team, an LLM-based agent partially automating the operation of a scanning transmission electron microscope.LLMicroscopilot, its prototype, combines a generally trained foundation model, which is then tailored to specific people and domains through dedicated control tools.This agent operates quite well at first, utilizing the API for a microscope experiment simulation tool [74], by performing such important tasks as estimating experimental parameters and executing the actual experiments.Therefore, this automation reduced dependence on personnel highly trained in operating such systems, thus increasing the opportunities for wider engagement in materials science due to the impact on usability.In the future, though, developments in the field are expected with LLMicroscopilot.In the future, they would involve integrating open-source microscope hardware control tools [75] and include capabilities for database access.Consequently, the system will be able to utilize Retrieval-Augmented Generation techniques to further inform parameter estimation and aid in the data analysis.Effectively, this will allow researchers to integrate LLMs in user interfaces at high-end microscopes and, instead of working on tedious, routine operational tasks, invest their energy in high-level scientific research and innovation, democratizing access to advanced experimental techniques.</p>
<p>Scientific Communication and Education</p>
<p>LLMs are transforming how scientific and educational content is created and shared, enhancing accessibility and personalized learning [76,77,78,79].By automating tasks like question generation, feedback, and grading, LLMs streamline educational processes, freeing educators to focus on individual learning needs.Additionally, LLMs assist in translating complex scientific findings into accessible formats, broadening public engagement [79].However, technological readiness, transparency, and ethical concerns around data privacy and bias remain critical challenges to address [78,76].</p>
<p>MaSTeA: Materials Science Teaching Assistant</p>
<p>This team selected 650 questions from the materials science question answering dataset (MaScQA) [80], requiring undergraduate-level understanding to solve.These questions are classified into four types based on their structure: Multiple Choice Questions (MCQs), Match the Following (MATCH), Numerical Questions with Given Options (MCQN), and Numerical Questions (NUM).MCQs are generally conceptual, with four options, where mostly one is correct, though occasionally multiple answers are valid.MATCH questions involve two lists of entities that need to be correctly paired, with four answer choices provided, one of which contains the correct set of matches.MCQN questions present a numerical problem with four answer choices, requiring a solution to identify the correct option, while NUM questions have numerical answers rounded to the nearest integer or floating-point number as specified.The team aimed to automate the evaluation of open-source and proprietary LLMs on MaScQA and develop an interactive interface for students to engage with these questions.Various models, including LLAMA3-8B, HAIKU, SONNET, GPT-4, and OPUS, were evaluated across 14 subject categories, such as characterization, applications, properties, and behavior.The evaluation results, summarized in Table 2, show that the OPUS variant of Claude consistently outperformed other models, achieving the highest accuracy in most categories.GPT-4 also demonstrated strong performance, particularly in material processing and fluid mechanics.As expected from prior studies, larger models such as OPUS and GPT-4 outperformed the smaller LLAMA3-8B, reinforcing the significance of model size in performance [81].The results suggest that there is significant room for improvement to enhance the accuracy of language models in answering scientific questions.</p>
<p>The evaluation involved:</p>
<p>• Extracting corresponding values: For MCQs, correct choices were identified using regular expressions and compared to model predictions.</p>
<p>• Prediction verification: Numerical predictions were validated against exact or acceptable ranges, while MCQ responses were matched to correct answer choices.</p>
<p>• Calculating accuracy: Accuracy was computed per question type and topic, followed by an overall assessment across all questions.</p>
<p>The evaluation results, summarized in Table 2, show that the OPUS variant of Claude consistently outperformed other models, achieving the highest accuracy in most categories.GPT-4 also demonstrated strong performance, particularly in material processing and fluid mechanics.As expected from prior studies, larger models such as OPUS and GPT-4 outperformed the smaller LLAMA3-8B, reinforcing the significance of model size in performance [81].The results suggest that there is significant room for improvement to enhance the accuracy of language models in answering scientific questions.The interactive web app, MaSTeA (Materials Science Teaching Assistant), developed using Streamlit, allows easy model testing to identify LLMs' strengths and weaknesses in different materials science subfields.The interface can be seen in Figure 6.With MaSTeA, the team demonstrated the potential of interactive tools to help students practice answering questions and learn the steps to reach the correct solution.By evaluating LLM performance, the goal was to guide future model development and identify areas for improvement.The results suggest that LLMs can benefit from strategies such as self-consistency [82] and retrieval-augmented generation (RAG) [83], which have been shown to reduce hallucinations and increase accuracy.Additionally, integrating advanced reasoning models could further improve performance.Recent advancements in domain-specific LLMs, such as LLaMat [84], highlight the potential of specialized training to enhance scientific reasoning.</p>
<p>Research Data Management and Automation</p>
<p>Various submissions were received that attempt to enhance the management, accessibility, and automation of scientific data workflows using LLMs.These efforts, often leveraging multimodal agents, aim to simplify complex data handling, improve reproducibility, and accelerate insights across diverse scientific disciplines.We highlight two exemplar projects: "yeLLowhaMmer" a multimodal LLM-based data management agent that automates data handling within electronic lab notebooks (ELNs) and laboratory information manage- ment systems (LIMS), and "NOMAD Query Reporter", an LLM-based agent that uses RAG to generate context-aware summaries from large materials science repositories like NOMAD [85]</p>
<p>yeLLowhaMMer: A Multi-modal Tool-calling Agent for Accelerated Research Data Management</p>
<p>As scientific data continues to grow in volume and complexity, there is a need for tools that can simplify the job of managing this data to draw insights, increase reproducibility, and accelerate discovery.Digital systems of record, such as electronic lab notebooks (ELNs) or laboratory information management systems (LIMS), have been a great advancement in this area.However, capturing data using, e.g., electronic lab notebooks (ELNs) or laboratory information management systems (LIMS) is laborious, or simply impossible, to accomplish using graphical user interfaces alone.Recent advances in AI present an opportunity to augment how researchers interact with their data, improving scientific data management and allowing scientists to ask scientific questions of these data sources in new ways.YeLLowhaMmer explored how large language models can be used to simplify and accelerate data handling tasks in order to generate new insights, improve reproducibility, and save time for researchers using the open-source datalab [86] ELN/LIMS.Previously, the team had made progress toward this goal by developing a conversational assistant, Whinchat [30], that allows users to ask questions about their data.However, this assistant was unable to take action with a user's data or seek additional information as is often needed for scientific tasks.Thus, the team developed yeLLowhaMmer as a multimodal large language model (MLLM)-based data management agent capable of taking free-form text and image instructions from users and executing a variety of complex scientific data management tasks.</p>
<p>The agent is powered by commercial MLLMs used within an agentic framework capable of iteratively writing and executing Python code that interacts with datalab instances via the datalab-api package.In typical usage, a yeLLowhaMmer user might instruct the agent: "Pull up my 10 most recent sample entries and summarize the synthetic approaches used."In this case, the agent will attempt to write datalab python API code to query for the user's samples in the datalab instance and write a human-readable summary based on the result.If the code it generates gives an error (or does not give sufficient information), the agent can iteratively rewrite the program until the task is accomplished successfully.Importantly, this paradigm is enabled by the presence of a structured API for diverse forms of scientific data; which is provided by datalab in its open-source schemas and API documentation.</p>
<p>In developing yeLLowhaMmer, the team found that simply copying documentation for the new datalabapi package into the system prompt produced poor code.Creating a simplified version with concrete examples and abridged JSON Schema formats proved more effective.The 12,000-character prompt (ca.3,200 tokens) works well with modern large context models like Claude 3 Haiku.Future scientific libraries might benefit from maintaining both standard documentation and condensed "agents.txt"files optimized for ML agents.</p>
<p>This work shows the opportunity to integrate more tightly into scientific data management workflows to allow researchers to quickly handle complex tasks and efficiently ask questions of all collected data.An important challenge is to find ways to ensure that data curated or modified by such agents will be appropriately 'credited' by, for example, visually demarcating AI-generated content, and providing UI pathways for human users to verify or relabel such data in an efficient manner.Finally, recent progress in MLLM's ability to handle audio and video content in addition to text and images will allow agents to use audiovisual data in real time to provide even more comprehensive user interfaces.</p>
<p>NOMAD Query Reporter: Automating Research Data Narratives</p>
<p>Research data management (RDM) in materials science includes a wide variety of schemas and data structures.Databases such as NOMAD [85,87] support extensible context-aware schemas.Hence, the results of a single query may in fact contain various schemas, complicating the data analysis process.NOMAD Query Reporter is a proof-of-concept application built to produce a written summary of the common methodological parameters and standout results in a scientific style.These may serve as the first step in an analysis workflow, or as progenitors of a journal article's "methods" section.</p>
<p>Given the large size -over 19 million entries-and dynamic nature -open public uploads-of the NOMAD database, retraining or fine-tuning strategies are challenging.Instead, this prototype implements a retrieval-Figure 7: The yeLLowhaMmer multimodal agent can be used for a variety of data management tasks.Here, it is shown automatically adding an entry into the datalab lab data management system based on an image of a handwritten lab notebook page.augmented generation (RAG) approach, as defined by Gao et al. [88], to enrich Llama3 (70B version) model's [89] knowledge base.The team progressively fed data by field into the LLM's chat-completion API as context.Subsequently, the construction of the summary was completed by topic (i.e., properties, techniques, material composition) in a multi-turn conversation style with the "roles" feature clearly distinguishing the LLM's tasks from the data provided.Alignment with earlier versions of the chat history is enforced both via low-temperature settings as well as prompt engineering.For a step-by-step overview, see Figure 8.This work highlights the ability of LLMs to augment research data management systems via returning information in formats that are easily understandable by users.While the prototype NOMAD Query Reporter as able to manage homogenized hits well, attempts at extending to manually annotated, heterogeneous data from ELNs proved challenging.Thus, follow-up work should consider more performant models and advanced RAG and other strategies to improve model context.</p>
<p>Hypothesis Generation and Evaluation</p>
<p>LLMs can be leveraged to streamline scientific inquiry, hypothesis generation, and verification.Recent work across psychology, astronomy, and biomedical research demonstrates their capacity to generate novel, validated hypotheses by integrating domain-specific data structures like causal graphs [90,91,92,93,94].Although still largely untapped in chemistry and materials science, this approach holds substantial promise for accelerating discovery and innovation in these fields [95,96,97].</p>
<p>Multi-Agent Hypothesis Generation and Verification through Tree of Thoughts and Retrieval Augmented Generation</p>
<p>Scientific discovery thrives on the ability to generate and evaluate new hypotheses efficiently.However, the process of forming meaningful and testable hypotheses often requires extensive background research, domain knowledge, and iterative refinement.Advances in large language models offer an opportunity to assist researchers in streamlining this process, particularly through structured, multi-agent frameworks that systematically generate, evaluate, and refine ideas.The Thoughtful Beavers team (Soroush Mahjoubi, Aleyna B. Ozhan) designed a multi-agent system to enhance scientific inquiry in materials science.Similar systems have proven useful in social sciences [98], and the system was adapted specifically for hypothesis generation in the domain of cement and concrete.The system consists of specialized agents that work in tandem: retrieving background knowledge, generating inspirations, formulating hypotheses, and evaluating their feasibility, utility, and novelty.By leveraging a combination of retrieval-augmented generation, tree-of-thoughts reasoning [99], and LLM-as-a-judge frameworks, this pipeline, which is illustrated in Figure 9, ensures that only the most promising hypotheses emerge from the process.</p>
<p>To test this pipeline, the authors focused on sustainability challenges in concrete design.By processing 66,000 abstracts related to the field, an embedding-based retrieval system was built to extract relevant insights and generate research questions.From this dataset, the approach produced 1,000 structured hypotheses, which were then subjected to rigorous evaluation.The results showed that 243 hypotheses were deemed feasible based on current scientific knowledge, 175 demonstrated practical utility, and 12 stood out as highly novel.</p>
<p>Looking ahead, this framework can be adapted to other material systems or even cross-disciplinary applications.By adjusting the background retrieval process, researchers could apply this method to areas such as ceramics, composites, or biomedical materials.Additionally, cross-pollination of ideas between domains could inspire new lines of research.As LLM capabilities continue to evolve, integrating AI-assisted hypothesis generation with expert validation could significantly accelerate scientific progress while maintaining the critical role of human creativity in innovation.</p>
<p>Knowledge Extraction and Reasoning</p>
<p>Extraction of structured scientific knowledge from unstructured text using LLMs to assisting researchers in navigating complex academic content is of wide interest [100,101,102,103].These systems streamline tasks like named entity recognition and relation extraction, offering flexible solutions tailored to materials science and chemistry [101].Tool-augmented frameworks help LLMs address complex reasoning by leveraging scientific tools and resources, expanding their utility as assistants in scientific research [104].</p>
<p>ActiveScience</p>
<p>Extracting and refining knowledge in hard sciences is crucial.While large language models excel in summarization and dialogue generation, they are also prone to generating false information, a phenomenon known as hallucination.This presents a significant challenge for researchers leveraging LLMs in scientific fields.</p>
<p>Various strategies exist to mitigate hallucinations.One approach involves fine-tuning models or constructing additional lightweight models after pretraining, but these methods require substantial computational resources, making them impractical in many cases.A more accessible alternative is retrieval-augmented generation (RAG), which enhances LLMs by incorporating external information.Conceptually, if a fine-tuned model resembles a domain expert with deep knowledge, a pre-trained model is akin to a generalist with broad understanding.By supplying additional context, pre-trained models can generate more accurate and reliable outputs.To address this challenge, Min-Hsueh Chiu introduced an automated framework Active-Science that leverages large language models to ingest scientific articles into a knowledge graph and enable natural language queries for domain knowledge extraction.The framework integrates three key components: a data source API, a large language model, and a graph database.While these components can be replaced with equivalent technologies, this work specifically utilizes the ArXiv API [105], GPT-3.5 Turbo [25], and Neo4j [106].</p>
<p>For structured representation of knowledge and relationships, ActiveScience employs an ontology that defines key entities such as Application, Property, Material, Element, and Metadata.The ontology design is adaptable and scalable to specific use cases.ActiveScience constructs its knowledge graph by extracting relevant triples from scientific articles.Specifically, prompts are generated using the predefined ontology and the introduction sections of articles to produce Cypher import statements containing structured triples, such as (Material: "Nanowire") -[HAS ELEMENT] -(Element: "Aluminum") and (Material: "Nanowire") -[HAS FORMULA] -(Formula: "Al-Si alloy").These triples are then imported into a Neo4j graph database.To facilitate RAG, the GraphCypherQAChain module from LangChain is employed.For instance, given the query, "Retrieve the top three reference URLs where the Property contains 'opti'?",GraphCypherQAChain dynamically generates a Cypher query based on the predefined ontology schema, executes it within Neo4j, and returns the relevant results.The processes of query generation and natural language processing are handled by LLMs.The pipeline and output are illustrated in Figure 10.</p>
<p>GlossaGen</p>
<p>Academic literature, particularly review articles and grant applications, would substantially benefit from the inclusion of comprehensive glossaries elucidating complex terminology and discipline-specific nomenclature.However, the manual generation of such reference materials is a labor-intensive and redundant process.To address this limitation, Lederbauer et al. developed GlossaGen, which leverages large language models to automate the creation of glossaries for academic articles and grant proposals, eliminating the need for time-consuming manual compilation.To efficiently process PDF or TeX articles, a pre-processing step automatically extracts the title and DOI, and chunks the text into smaller, context-preserving sections for LLM analysis.LLMs such as GPT-3.5-Turbo[25] and GPT-4-Turbo [24] then identify and define scientific terms with the help of Typed Predictors [107] and Chain-of-Thought [108] prompting, ensuring well-structured, contextually relevant, and accurate outputs.The generated glossary is not merely presented as a list of terms but also as an ontology-based knowledge graph using Neo4J [106] and Graph Maker [109], visualizing the intricate relationships between various technical concepts (Figure 11).A user-friendly interface prototype, developed with Gradio [110], enables seamless interaction and customization, making the system accessible to researchers.</p>
<p>Future enhancements could focus on improving glossary output through LLM fine-tuning, integrating retrieval-augmented generation, and enabling article image parsing.Additionally, the system can better support users by allowing them to input specific terms for glossary explanations, ensuring comprehensive coverage even when LLMs omit key concepts.Overall, GlossaGen's rapid development and promising capabilities highlight the potential of LLMs to assist researchers in their scientific outreach.</p>
<p>ChemQA</p>
<p>Foundation models exhibit strong capabilities in chemistry reasoning, yet their performance across different input modalities -text, images, and their combination, remains underexplored.Building upon prior benchmarks such as IsoBench [111] and ChemLLMBench [112], the VizChem team (Khalighinejad et al.) introduced ChemQA [113], a multimodal question-answering dataset designed to assess chemistry reasoning in language models.ChemQA comprises five distinct QA tasks: atom counting, molecular weight calculation, name conversion, molecule captioning, and retrosynthesis planning.Each task is formulated with both molecular images and textual SMILES representations, enabling a systematic study of multimodal reasoning in chemistry.</p>
<p>The evaluation results, shown in Figure 12, reveal that the models achieve higher accuracy when provided with both text and images, while the performance drops significantly with image-only inputs.Notably, Claude 3 Opus demonstrates superior performance in text-based tasks, whereas Gemini Pro and GPT-4 Turbo excel in multimodal settings [114,115,116].These findings highlight the limitations of current models in processing visual chemistry data independently.</p>
<p>By introducing ChemQA, the VizChem team underscored the need for enhanced multimodal reasoning in chemistry.Future work should focus on improving the integration of textual and visual representations</p>
<p>Hackathon Event Overview</p>
<p>The second annual Large Language Model (LLM) Hackathon for Applications in Materials Science and Chemistry was held on May 9, 2024, bringing together a global network of researchers, students, and industry professionals.With 556 registered participants and over 120 active contributors forming 34 teams, the event spanned multiple time zones and research domains, underscoring the broad interest in applying LLMs to scientific discovery(Figure 13).This hackathon built on the success of the previous year's event, described in detail in [30].The hybrid format included physical hubs in Toronto, Montreal, San Francisco, Berlin, Lausanne, and Tokyo, fostering interdisciplinary collaboration across institutions and time zones.The event began with a kickoff panel featuring experts Elsa Olivetti (MIT), Jon Reifsneider (Duke), Michael Craig (Valence Laboratories), and Marwin Segler (Microsoft), who discussed the evolving role of LLMs in scientific research.</p>
<p>The charge of the hackathon was intentionally open-ended: to explore the vast potential application space and create tangible demonstrations of the most innovative, impactful, and scalable solutions within a constrained timeframe.Participants leveraged open-source and best-in-class multimodal models to tackle challenges in materials science and chemistry.These teams submitted projects covering molecular property prediction, materials design, automation, hypothesis generation, and knowledge extraction, demonstrating the versatility of LLMs in scientific research.Many incorporated retrieval-augmented generation (RAG), multi-agent reasoning, and natural language interfaces, showcasing AI's expanding role in scientific discovery.</p>
<p>Beyond technical contributions, the hackathon fostered a global research community, with 483 researchers continuing collaborations via Slack.The event demonstrated the value of structured collaboration in accelerating AI-driven discovery and bridging computational scientists, experimentalists, and AI researchers.</p>
<p>LLM HACKATHON FOR APPLICATIONS IN MATERIALS AND CHEMISTRY</p>
<p>Conclusion</p>
<p>The LLM Hackathon for Applications in Materials Science and Chemistry has demonstrated the dual utility and immense promise of LLMs to impact materials science and chemistry research across the entire lifecycle.Together, the projects 1) demonstrate the promise of a new set of tools that together form a cohesive patchwork to perform tasks ranging from hypothesis generation to data extraction, novel interface design, analysis of results, and more; and 2) showcase the ability of LLMs to enable rapid prototyping and exploration of the application space.Participants effectively utilized LLMs to explore solutions to specific challenges while rapidly evaluating their ideas over just a short 24-hour period, highlighting compelling abilities to enhance the efficiency and creativity of research processes across many applications.It's important to note that many projects benefited from significant advancements in LLM performance since the previous year's hackathon.That is, the performance across the application space was improved simply via the release of more powerful versions of Gemini, ChatGPT, Claude, Llama, and other models and more easily accessible APIs and examples.If this trend continues, we expect to see even broader applications in subsequent hackathons and in materials science and chemistry more generally.We note that reliance on proprietary APIs raises reproducibility concerns as models evolve or are deprecated, while infrastructure demands for training, fine-tuning, or running inference on models with parameters reaching hundreds of billions require yet more computational resources, leading to significant infrastructure roadblocks to further academic work.</p>
<p>Importantly, the hybrid hackathon format itself proved to be an effective mechanism to foster interdisciplinary collaboration, accelerate the prototyping of AI-driven tools, and create a global community of researchers engaged in exploring LLM applications.The hybrid format, combining physical hubs with virtual participation, facilitated knowledge exchange across continents, highlighting the importance of accessible, multimodal, and scalable approaches to scientific innovation.</p>
<p>Figure 1 :
1
Figure 1: The LLM-Powered Research Constellation.At each stage of the research process, from initial ideation through experimental execution and communication of results, LLMs provide a constellation of capabilities spanning hypothesis generation, property prediction, novel interfaces, education, material design, automation, data management, scientific communication, and more.This constellation demonstrates the possibility of LLMs and multimodal models to drive a more efficient, rapid, and creative scientific discovery process through integrations across the research lifecycle.</p>
<p>Figure 2 :
2
Figure 2: Schematic depicting the prompt for fine-tuning the LLM with Alpaca prompt format.</p>
<p>Figure 3 :
3
Figure 3: Workflow overview.The ReAct agent looks up guidelines for designing low band gap MOFs from research papers and suggests a new MOF (likely with a lower band gap).It then checks the validity of the new SMILES candidate and predicts the band gap with epistemic uncertainty estimation using an ensemble of surrogate fine-tuned MOFormers.b.Band gap predictions for new MOF candidates as a function of agent iterations</p>
<p>nd the concentration of an Copper Gold Alloy with a bulk modulus around 145 GPa with an error of plus or minus 2 GPa using the EMT simulation code.Validate your prediction by computing the bulk modulus and do not stop until you calculate the bulk modulus with the de ned uncertainty.</p>
<p>bulk modulus for the Cu-Au alloy with 73.8% Copper and 26.2% Gold is approximately 145.58 GPa, which is within the desired range of 145 GPa ± 2 GPa.</p>
<p>Figure 4 :
4
Figure 4: LangSim framework for atomistic simulation and inverse design.Custom atomistic modeling tools (such as pyiron, ASE python package functions with underlying EMT and MACE-MP-0 forcefields) are integrated using LangChain @tool decorator.Pydantic model is used to exchange atomic information in a structured format between LLM and tools.The emerging agentic capability for inverse alloy design is demonstrated.LLM agent is able to find the target composition of Cu-Au alloy with the desired bulk modulus.</p>
<p>Figure 5 :
5
Figure 5: Schematic overview of the LLMicroscopilot assistant.The microscope user interface allows the user to input queries, which are then processed by the LLM.The LLM executes appropriate tools to provide domain-specific knowledge, support data analysis, or operate the microscope.</p>
<p>Figure 6 :
6
Figure 6: MaSTeA interface demonstrating a numerical question task.The model arrives at the correct answer by reasoning through the problem, providing students with a step-by-step solution if they struggle to solve it independently.</p>
<p>Figure 8 :
8
Figure 8: Flowchart of the Query Reporter usage, including the back-end interaction with external resources,i.e., NOMAD and Llama.Intermediate steps managing hallucinations or token limits are marked in red and orange, respectively.</p>
<p>Figure 9 :
9
Figure 9: Multi-Agent Hypothesis Generation and Verification Framework.The system uses Retrieval-Augmented Generation, Tree of Thoughts, and Feasibility, Utility, and Novelty evaluation agents to generate and refine hypotheses for sustainable concrete design.</p>
<p>Figure 10 :
10
Figure10: ActiveScience Framework for Knowledge Extraction.The system combines ontology-driven prompts, large language models, and a Neo4j knowledge graph to enable natural language queries and retrieval-augmented generation (RAG) for scientific research insights.Additionally, a code snippet demonstrating the use of LangChain is shown.</p>
<p>Figure 11 :
11
Figure 11: Schematic overview of the GlossaGen project.Textual information is extracted from PDF and LaTeX files and a glossary is generated with terms and their definition.From this, a knowledge graph is created, showing entities and relationships between terms.</p>
<p>Figure 12 :
12
Figure 12: Performance of Gemini Pro, GPT-4 Turbo, and Claude3 Opus on text, visual, and text+visual representations.The plot shows that models achieve higher accuracy with combined text and visual inputs compared to visual-only inputs.</p>
<p>Figure 13 :
13
Figure 13: LLM Hackathon for Applications in Materials and Chemistry hybrid hackathon.Researchers were able to participate from both remote and in-person locations (purple pins).</p>
<p>Table 2 :
2
Accuracy of LLMs for each topic
Topic# Questions LLaMA-3-8b Haiku Sonnet OPUS GPT4Thermodynamics11437.7247.3755.2673.6857.02Atomic structure1003240496459Mechanical behavior9622.9241.6752.0871.8843.75Material manufacturing9143.9657.1456.0480.2268.13Material applications5352.8364.1577.3692.4586.79Phase transition4131.7146.3465.8570.7363.41Electrical properties3633.332555.5672.2244.44Material processing3548.5754.2974.2988.5788.57Transport phenomena2437.570.8358.3387.562.5Magnetic properties1526.6746.6746.6766.6760Material characterization1478.5757.1485.7192.8671.43Fluid mechanics1421.435057.1478.5785.71Material testing977.7866.67100100100Miscellaneous862.562.562.57562.5
AcknowledgmentsPlanning for this event was supported by NSF Awards #2226419 and #2209892.We would like to thank event sponsors who provided platform credits and prizes for teams, including RadicalAI, Iteratec, Reincarnate, Acceleration Consortium, and Neo4j.Site coordinators include: Brandon Lines, Philippe Schwaller, Pepe Marquez, Mehrad Ansari and Seyed Mohamad Moosavi.Mohamad Moosavi acknowledges support from the Data Science Institute at the University of Toronto for organizing events related to LLMs.Mehrad Ansari acknowledges Mahyar Rajabi, Seyed Mohamad Moosavi, and Amro Aswad for their feedback on the project.Aakash Naik, Katharina Ueltzen, and Janine George would like to acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding their work on this project by providing generous computing time on the GCS Supercomputer SuperMUC-NG at Leibniz Super computing Centre (www.lrz.de)(project pn73da).Conflicts of InterestThe authors declare no conflicts of interest.
Leveraging large language models for predictive chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, Nature Machine Intelligence. 6Feb. 2024</p>
<p>Large language models as molecular design engines. D Bhattacharya, H J Cassady, M A Hickner, W F Reinhart, Journal of Chemical Information and Modeling. 642024</p>
<p>AtomGPT: Atomistic Generative Pretrained Transformer for Forward and Inverse Materials Design. K Choudhary, The Journal of Physical Chemistry Letters. 152024</p>
<p>Comparative study of large language model architectures on frontier. J Yin, A Bose, G Cong, I Lyngaas, Q Anthony, 2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS). IEEE2024</p>
<p>Matscibert: A materials domain language model for text mining and information extraction. T Gupta, M Zaki, N A Krishnan, Mausam , Computational Materials. 811022022</p>
<p>Alchembert: Exploring lightweight language models for materials informatics. X Liu, Y Wang, T Yang, X Liu, X Wen, ChemRxiv. 2025</p>
<p>Llm-prop: Predicting physical and electronic properties of crystalline solids from their text descriptions. A N Rubungo, C Arnold, B P Rand, A B Dieng, arXiv:2310.140292023arXiv preprint</p>
<p>Explainable synthesizability prediction of inorganic crystal polymorphs using large language models. S Kim, J Schrier, Y Jung, Angewandte Chemie International Edition. e2024239502024</p>
<p>Robocrystallographer: automated crystal structure text descriptions and analysis. A M Ganose, A Jain, MRS Communications. 932019</p>
<p>Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm. A Dunn, Q Wang, A Ganose, D Dopp, A Jain, Computational Materials. 611382020</p>
<p>High-throughput density-functional perturbation theory phonons for inorganic materials. G Petretto, S Dwaraknath, H Miranda, D Winston, M Giantomassi, M J Van Setten, X Gonze, K A Persson, G Hautier, G.-M Rignanese, Scientific data. 512018</p>
<p>Matagent: A human-in-the-loop multiagent llm framework for accelerating the material science discovery cycle. A Bazgir, R Chandra Praneeth Madugula, Y Zhang, AI for Accelerated Materials Design-ICLR 2025. 2025</p>
<p>Regression with large language models for materials and molecular property prediction. R Jacobs, M P Polak, L E Schultz, H Mahdavi, V Honavar, D Morgan, 2024</p>
<p>Llm4mat-bench: benchmarking large language models for materials property prediction. A N Rubungo, K Li, J Hattrick-Simpers, A B Dieng, arXiv:2411.001772024arXiv preprint</p>
<p>Can large language models empower molecular property prediction?. C Qian, H Tang, Z Yang, H Liang, Y Liu, 2023</p>
<p>Autonomous chemical research with large language models. D A Boiko, R Macknight, B Kline, G Gomes, Nature. 6242023</p>
<p>Self-driving laboratories for chemistry and materials science. G Tom, S P Schmid, S G Baird, Y Cao, K Darvish, H Hao, S Lo, S Pablo-García, E M Rajaonson, M Skreta, N Yoshikawa, S Corapi, G D Akkoc, F Strieth-Kalthoff, M Seifrid, A Aspuru-Guzik, Chemical Reviews. 1242024</p>
<p>Honeycomb: A flexible llm-based agent system for materials science. H Zhang, Y Song, Z Hou, S Miret, B Liu, 2024</p>
<p>Augmenting large language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, Nature Machine Intelligence. 652024</p>
<p>The llama 3 herd of models. A Grattafiori, A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Vaughan, A Yang, A Fan, A Goyal, A Hartshorn, A Yang, A Mitra, A Sravankumar, A Korenev, A Hinsvark, A Rao, A Zhang, A Rodriguez, A Gregerson, A Spataru, B Roziere, B Biron, B Tang, B Chern, C Caucheteux, C Nayak, C Bi, C Marra, C Mcconnell, C Keller, C Touret, C Wu, C Wong, C C Ferrer, C Nikolaidis, D Allonsius, D Song, D Pintz, D Livshits, D Wyatt, D Esiobu, D Choudhary, D Mahajan, D Garcia-Olano, D Perino, D Hupkes, E Lakomkin, E Albadawy, E Lobanova, E Dinan, E M Smith, F Radenovic, F Guzmán, F Zhang, G Synnaeve, G Lee, G L Anderson, G Thattai, G Nail, G Mialon, G Pang, G Cucurell, H Nguyen, H Korevaar, H Xu, H Touvron, I Zarov, I A Ibarra, I Kloumann, I Misra, I Evtimov, J Zhang, J Copet, J Lee, J Geffert, J Vranes, J Park, J Mahadeokar, J Shah, J Van Der Linde, J Billock, J Hong, J Lee, J Fu, J Chi, J Huang, J Liu, J Wang, J Yu, J Bitton, J Spisak, J Park, J Rocca, J Johnstun, J Saxe, J Jia, K V Alwala, K Prasad, K Upasani, K Plawiak, K Li, K Heafield, K Stone, K El-Arini, K Iyer, K Malik, K Chiu, K Bhalla, K Lakhotia, L Rantala-Yeary, L Van Der Maaten, L Chen, L Tan, L Jenkins, L Martin, L Madaan, L Malo, L Blecher, L Landzaat, L Oliveira, M Muzzi, M Pasupuleti, M Singh, M Paluri, M Kardas, M Tsimpoukelli, M Oldham, M Rita, M Pavlova, M Kambadur, M Lewis, M Si, M K Singh, M Hassan, N Goyal, N Torabi, N Bashlykov, N Bogoychev, N Chatterji, N Zhang, O Duchenne, O ¸elebi, P Alrassy, P Zhang, P Li, P Vasic, P Weng, P Bhargava, P Dubal, P Krishnan, P S Koura, P Xu, Q He, Q Dong, R Srinivasan, R Ganapathy, R Calderer, R S Cabral, R Stojnic, R Raileanu, R Maheswari, R Girdhar, R Patel, R Sauvestre, R Polidoro, R Sumbaly, R Taylor, R Silva, R Hou, R Wang, S Hosseini, S Chennabasappa, S Singh, S Bell, S S Kim, S Edunov, S Nie, S Narang, S Raparthy, S Shen, S Wan, S Bhosale, S Zhang, S Vandenhende, S Batra, S Whitman, S Sootla, S Collot, S Gururangan, S Borodinsky, T Herman, T Fowler, T Sheasha, T Georgiou, T Scialom, T Speckbacher, T Mihaylov, T Xiao, U Karn, V Goswami, V Gupta, V Ramanathan, V Kerkez, V Gonguet, V Do, V Vogeti, V Albiero, V Petrovic, W Chu, W Xiong, W Fu, W Meers, X Martinet, X Wang, X Wang, X E Tan, X Xia, X Xie, X Jia, X Wang, Y Goldschlag, Y Gaur, Y Babaei, Y Wen, Y Song, Y Zhang, Y Li, Y Mao, Z D Coudert, Z Yan, Z Chen, Z Papakipos, A Singh, A Srivastava, A Jain, A Kelsey, A Shajnfeld, A Gangidi, A Victoria, A Goldstand, A Menon, A Sharma, A Boesenberg, A Baevski, A Feinstein, A Kallet, A Sangani, A Teo, A Yunus, A Lupu, A Alvarado, A Caples, A Gu, A Ho, A Poulton, A Ryan, A Ramchandani, A Dong, A Franco, A Goyal, A Saraf, A Chowdhury, A Gabriel, A Bharambe, A Eisenman, A Yazdan, B James, B Maurer, B Leonhardi, B Huang, B Loyd, B D Paola, B Paranjape, B Liu, B Wu, B Ni, B Hancock, B Wasti, B Spence, B Stojkovic, B Gamido, B Montalvo, C Parker, C Burton, C Mejia, C Liu, C Wang, C Kim, C Zhou, C Hu, C.-H Chu, C Cai, C Tindal, C Feichtenhofer, C Gao, D Civin, D Beaty, D Kreymer, D Li, D Adkins, D Xu, D Testuggine, D David, D Parikh, D Liskovich, D Foss, D Wang, D Le, D Holland, E Dowling, E Jamil, E Montgomery, E Presani, E Hahn, E Wood, E.-T Le, E Brinkman, E Arcaute, E Dunbar, E Smothers, F Sun, F Kreuk, F Tian, F Kokkinos, F Ozgenel, F Caggioni, F Kanayet, F Seide, G M Florez, G Schwarz, G Badeer, G Swee, G Halpern, G Herman, G Sizov, Guangyi, G Zhang, H Lakshminarayanan, H Inan, H Shojanazeri, H Zou, H Wang, H Zha, H Habeeb, H Rudolph, H Suk, H Aspegren, H Goldman, I Zhan, I Damlaj, I Molybog, I Tufanov, I.-E Leontiadis, I Veliche, J Gat, J Weissman, J Geboski, J Kohli, J Lam, J.-B Asher, J Gaya, J Marcus, J Tang, J Chan, J Zhen, J Reizenstein, J Teboul, J Zhong, J Jin, J Yang, L Chen, L Garg, L A , L Silva, L Bell, L Zhang, L Guo, L Yu, L Moshkovich, L Wehrstedt, M Khabsa, M Avalani, M Bhatt, M Mankus, M Hasson, M Lennie, M Reso, M Groshev, M Naumov, M Lathi, M Keneally, M Liu, M L Seltzer, M Valko, M Restrepo, M Patel, M Vyatskov, M Samvelyan, M Clark, M Macey, M Wang, M J Hermoso ; Salpekar, O Kalinli, P Kent, P Parekh, P Saab, P Balaji, P Rittner, P Bontrager, P Roux, P Dollar, P Zvyagina, P Ratanchandani, P Yuvraj, Q Liang, R Alao, R Rodriguez, R Ayub, R Murthy, R Nayani, R Mitra, R Parthasarathy, R Li, R Hogan, R Battey, R Wang, R Howes, R Rinott, S Mehta, S Siby, S J Bondu, S Datta, S Chugh, S Hunt, S Dhillon, S Sidorov, S Pan, S Mahajan, S Verma, S Yamamoto, S Ramaswamy, S Lindsay, S Lindsay, S Feng, S Lin, S C Zha, S Patil, S Shankar, S Zhang, S Zhang, S Wang, S Agarwal, S Sajuyigbe, S Chintala, S Max, S Chen, S Kehoe, S Satterfield, S Govindaprasad, S Gupta, S Deng, S Cho, S Virk, S Subramanian, S Choudhury, S Goldman, T Remez, T Glaser, T Best, T Koehler, T Robinson, T Li, T Zhang, T Matthews, T Chou, T Shaked, V Vontimitta, V Ajayi, V Montanez, V Mohan, V S Kumar, V Mangla, V Ionescu, V Poenaru, V T Mihailescu, V Ivanov, W Li, W Wang, W Jiang, W Bouaziz, W Constable, X Tang, X Wu, X Wang, X Wu, X Gao, Y Kleinman, Y Chen, Y Hu, Y Jia, Y Qi, Y Li, Y Zhang, Y Zhang, Y Adi, Y Nam, Yu, Y Wang, Y Zhao, Y Hao, Y Qian, Y Li, Z He, Z Rait, Z Devito, Z Rosnbrick, Z Wen, Z Yang, Z Zhao, Ma, Cummings, J. Carvill, J. Shepard, J. McPhie, J. Torres, J. Ginsburg, J. Wang, K. Wu, K. H. U, K. Saxena, K. Khandelwal, K. Zand, K. Matosich, K. Veeraraghavan, K. Michelena, K. Li, K. Jagadeesh, K. Huang, K. Chawla, K. Huang,2024Santhanam, N. Parks, N. White, N. Bawa, N. Singhal, N. Egebo, N. Usunier, N. Mehta, N. P. Laptev</p>
<p>. M Abdin, J Aneja, H Awadalla, A Awadallah, A A Awan, N Bach, A Bahree, A Bakhtiari, J Bao, H Behl, A Benhaim, M Bilenko, J Bjorck, S Bubeck, M Cai, Q Cai, V Chaudhary, D Chen, D Chen, W Chen, Y.-C Chen, Y.-L Chen, H Cheng, P Chopra, X Dai, M Dixon, R Eldan, V Fragoso, J Gao, M Gao, M Gao, A Garg, A D Giorno, A Goswami, S Gunasekar, E Haider, J Hao, R J Hewett, W Hu, J Huynh, D Iter, S A Jacobs, M Javaheripi, X Jin, N Karampatziakis, P Kauffmann, M Khademi, D Kim, Y J Kim, L Kurilenko, J R Lee, Y T Lee, Y Li, Y Li, C Liang, L Liden, X Lin, Z Lin, C Liu, L Liu, M Liu, W Liu, X Liu, C Luo, P Madan, A Mahmoudzadeh, D Majercak, M Mazzola, C C T Mendes, A Mitra, H Modi, A Nguyen, B Norick, B Patra, D Perez-Becker, T Portet, R Pryzant, H Qin, M Radmilac, L Ren, G De Rosa, C Rosset, S Roy, O Ruwase, O Saarikivi, A Saied, A Salim, M Santacroce, S Shah, N Shang, H Sharma, Y Shen, S Shukla, X Song, M Tanaka, A Tupini, P Vaddamanu, C Wang, G Wang, L Wang, S Wang, X Wang, Y Wang, R Ward, W Wen, P Witte, H Wu, X Wu, M Wyatt, B Xiao, C Xu, J Xu, W Xu, J Xue, S Yadav, F Yang, J Yang, Y Yang, Z Yang, D Yu, L Yuan, C Zhang, C Zhang, J Zhang, L L Zhang, Y Zhang, Y Zhang, Y Zhang, X Zhou, Phi-3 technical report: A highly capable language model locally on your phone," 2024</p>
<p>The claude 3 model family: Opus, sonnet, haiku. Anthropic, 2024</p>
<p>. A Q Jiang, A Sablayrolles, A Roux, A Mensch, B Savary, C Bamford, D S Chaplot, D De Las Casas, E B Hanna, F Bressand, G Lengyel, G Bour, G Lample, L R Lavaud, L Saulnier, M.-A , </p>
<p>. P Lachaux, S Stock, S Subramanian, S Yang, T L Antoniak, T Scao, T Gervet, T Lavril, T Wang, W E Lacroix, Sayed, Mixtral of experts," 2024</p>
<p>Gpt-4-turbo and gpt-4. Openai, 2023. February 24. 2025</p>
<p>Gpt-3.5-turbo. Openai, 2023. February 24. 2025</p>
<p>. J Openai, S Achiam, S Adler, L Agarwal, I Ahmad, F L Akkaya, D Aleman, J Almeida, S Altenschmidt, S Altman, R Anadkat, I Avila, S Babuschkin, V Balaji, P Balcom, H Baltescu, M Bao, J Bavarian, I Belgum, J Bello, G Berdine, C Bernadett-Shapiro, L Berner, O Bogdonoff, M Boiko, A.-L Boyd, G Brakman, T Brockman, M Brooks, K Brundage, T Button, R Cai, A Campbell, B Cann, C Carey, R Carlson, B Carmichael, C Chan, F Chang, D Chantzis, S Chen, R Chen, J Chen, M Chen, B Chen, C Chess, C Cho, H W Chu, D Chung, J Cummings, Y Currier, C Dai, T Decareaux, N Degry, D Deutsch, A Deville, D Dhar, S Dohan, S Dowling, A Dunning, A Ecoffet, T Eleti, D Eloundou, L Farhi, N Fedus, S P Felix, J Fishman, I Forte, L Fulford, E Gao, C Georges, V Gibson, T Goel, G Gogineni, R Goh, J Gontijo-Lopes, M Gordon, S Grafstein, R Gray, J Greene, S S Gross, Y Gu, C Guo, J Hallacy, J Han, Y Harris, M He, J Heaton, C Heidecke, A Hesse, W Hickey, P Hickey, B Hoeschele, K Houghton, S Hsu, X Hu, J Hu, S Huizinga, S Jain, J Jain, A Jang, R Jiang, H Jiang, D Jin, S Jin, B Jomoto, H Jonn, T Jun, Lukasz Kaftan, A Kaiser, I Kamali, N S Kanitscheider, T Keskar, L Khan, J W Kilpatrick, C Kim, Y Kim, J H Kim, J Kirchner, M Kiros, D Knight, Lukasz Kokotajlo, A Kondraciuk, A Kondrich, K Konstantinidis, G Kosic, V Krueger, M Kuo, I Lampe, T Lan, J Lee, J Leike, D Leung, C M Levy, R Li, M Lim, S Lin, M Lin, T Litwin, R Lopez, P Lowe, A Lue, K Makanju, S Malfacini, T Manning, Y Markov, B Markovski, K Martin, A Mayer, B Mayne, S M Mcgrew, C Mckinney, P Mcleavey, J Mcmillan, D Mcneil, A Medina, J Mehta, L Menick, A Metz, P Mishchenko, V Mishkin, E Monaco, D Morikawa, T Mossing, M Mu, O Murati, D Murk, A Mély, R Nair, R Nakano, A Nayak, R Neelakantan, H Ngo, L Noh, C Ouyang, J O'keefe, A Pachocki, J Paino, A Palermo, G Pantuliano, J Parascandolo, E Parish, A Parparita, M Passos, ; D Pavlov, K Selsam, T Sheppard, J Sherbakov, S Shieh, P Shoker, S Shyam, E Sidor, M Sigler, J Simens, K Sitkin, I Slama, B Sohl, Y Sokolowsky, N Song, F P Staudacher, N Such, I Summers, J Sutskever, N Tang, M B Tezak, P Thompson, A Tillet, E Tootoonchian, P Tseng, N Tuggle, J Turley, J F C Tworek, A Uribe, A Vallone, C Vijayvergiya, C Voss, J J Wainwright, A Wang, B Wang, J Wang, J Ward, C Wei, A Weinmann, P Welihinda, J Welinder, L Weng, M Weng, D Wiethoff, C Willner, S Winter, H Wolrich, L Wong, S Workman, J Wu, M Wu, K Wu, T Xiao, S Xu, K Yoo, Q Yu, W Yuan, R Zaremba, C Zellers, M Zhang, S Zhang, T Zhao, J Zheng, W Zhuang, B Zhuk, Zoph, Peng, A. Perelman, F. de Avila Belbute Peres, M. Petrov, H. P. de Oliveira Pinto, Michael, Pokorny, M. Pokrass, V. H. Pong, T. Powell, A. Power, B. Power, E. Proehl, R. Puri, A. Radford, J. Rae, A. Ramesh, C. Raymond, F. Real, K. Rimbach, C. Ross, B. Rotsted, H. Roussez, N. Ryder, M. Saltarelli, T. Sanders, S. Santurkar, G. Sastry, H. Schmidt, D. Schnurr, J. Schulman,Gpt-4 technical report," 2024</p>
<p>How to support newcomers in scientific hackathons -an action research study on expert mentoring. A Nolte, L B Hayden, J D Herbsleb, Proceedings of the ACM on Human-Computer Interaction. 42020</p>
<p>Understanding hackathons for science: Collaboration, affordances, and outcomes. E P P Pe-Than, J D Herbsleb, Lecture Notes in Computer Science. 2019Springer International Publishing</p>
<p>Hack your organizational innovation: literature review and integrative model for running hackathons. B Heller, A Amir, R Waxman, Y Maaravi, Journal of Innovation and Entrepreneurship. 122023</p>
<p>14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. K M Jablonka, Q Ai, A Al-Feghali, S Badhwar, J D Bocarsly, A M Bran, S Bringuier, L C Brinson, K Choudhary, D Circi, S Cox, W A Jong, M L Evans, N Gastellu, J Genzling, M V Gil, A K Gupta, Z Hong, A Imran, S Kruschwitz, A Labarre, J Lála, T Liu, S Ma, S Majumdar, G W Merz, N Moitessier, E Moubarak, B Mouriño, B Pelkie, M Pieler, M C Ramos, B Ranković, S G Rodriques, J N Sanders, P Schwaller, M Schwarting, J Shi, B Smit, B E Smith, J Van Herck, C Völker, L Ward, S Warren, B Weiser, S Zhang, X Zhang, G A Zia, A Scourtas, K J Schmidt, I Foster, A D White, B Blaiszik, Digital Discovery. 252023</p>
<p>. Y Zimmermann, A Bazgir, Z Afzal, F Agbere, Q Ai, N Alampara, A Al-Feghali, M Ansari, D Antypov, A Aswad, J Bai, V Baibakova, D D Biswajeet, E Bitzek, J D Bocarsly, A Borisova, A M Bran, L C Brinson, M M Calderon, A Canalicchio, V Chen, Y Chiang, D Circi, B Charmes, V Chaudhary, Z Chen, M.-H Chiu, J Clymo, K Dabhadkar, N Daelman, A Datar, W A Jong, M L Evans, M G Fard, G Fisicaro, A S Gangan, J George, J D C Gonzalez, M Götte, A K Gupta, H Harb, P Hong, A Ibrahim, A Ilyas, A Imran, K Ishimwe, R Issa, K M Jablonka, C Jones, T R Josephson, G Juhasz, S Kapoor, R Kang, G Khalighinejad, S Khan, S Klawohn, S Kuman, A N Ladines, S Leang, M Lederbauer, Sheng-Lun, H Liao, X Liu, S Liu, S Lo, P R Madireddy, S Maharana, S Maheshwari, J A Mahjoubi, R Márquez, T Mills, B Mohanty, S M Mohr, A Moosavi, A D Moßhammer, A Naghdi, O Naik, H Narykov, X V Näsström, X Nguyen, D Ni, T O'connor, F Olayiwola, A B Ottomano, S Ozhan, C Pagel, J Parida, V Park, E Patel, M H Patyukova, L Petersen, J M Pinto, D Pizarro, T Plessers, U Pradhan, C Pratiush, ; S Puli, K Sharlin, J Shen, P Shi, J Si, T Souza, S Sparks, L Sudhakar, D Talirz, O Tang, C Taran, M Terboven, A Tropin, K Tsymbal, P A Ueltzen, A Unzueta, T Vasan, T Vinchurkar, G Vo, C Vogel, J Völker, F Weinreich, M Yang, C Zaki, S Zhang, W Zhang, R Zhang, S Zhu, J Zhu, C Janssen, I Li, B Foster, Blaiszik, Qin, M. Rajabi, F. Ricci, E. Risch, M. Ríos-García, A. Roy, T. Rug, H. M. Sayeed, M. Scheidgen, M. Schilling-Wilhelmi, M. Schloz, F. Schöppach, J. Schumann, P. Schwaller, M. Schwarting,2025Reflections from the 2024 large language model (llm) hackathon for applications in materials science and chemistry</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, 2020</p>
<p>From words to numbers: Your large language model is secretly a capable regressor when given in-context examples. R Vacareanu, V A Negru, V Suciu, M Surdeanu, First Conference on Language Modeling. 2024</p>
<p>A quantum-chemical bonding database for solid-state materials. A A Naik, C Ertural, N Dhamrait, P Benner, J George, Scientific Data. 1016102023</p>
<p>Lobsterpy: A package to automatically analyze lobster runs. A A Naik, K Ueltzen, C Ertural, A J Jackson, J George, Journal of Open Source Software. 99462862024</p>
<p>A Quantum-Chemical Bonding Database for Solid-State Materials (JSONS: Part 1). A A Naik, C Ertural, N Dhamrait, P Benner, J George, 2023</p>
<p>The matbench test suite, phonon dataset. Matbench, 2024</p>
<p>The unsloth package. M H Daniel Han, U Team, 2024</p>
<p>Multimodal large language models for inverse molecular design with retrosynthetic planning. G Liu, M Sun, W Matusik, M Jiang, J Chen, 2024</p>
<p>Llmatdesign: Autonomous materials discovery with large language models. S Jia, C Zhang, V Fung, 2024</p>
<p>Can llms generate diverse molecules? towards alignment with structural diversity. H Jang, Y Jang, J Kim, S Ahn, 2025</p>
<p>Generative design of functional metal complexes utilizing the internal knowledge of large language models. J Lu, Z Song, Q Zhao, Y Du, Y Cao, H Jia, C Duan, 2024</p>
<p>A sober look at LLMs for material discovery: Are they actually good for Bayesian optimization over molecules?. A Kristiadi, F Strieth-Kalthoff, M Skreta, P Poupart, A Aspuru-Guzik, G Pleiss, Proceedings of the 41st International Conference on Machine Learning. R Salakhutdinov, Z Kolter, K Heller, A Weller, N Oliver, J Scarlett, F Berkenkamp, the 41st International Conference on Machine LearningPMLRJul 2024235Proceedings of Machine Learning Research</p>
<p>Are llms ready for real-world materials discovery?. S Miret, N M A Krishnan, 2024</p>
<p>Review on applications of metal-organic frameworks for co2 capture and the performance enhancement mechanisms. L Li, H S Jung, J W Lee, Y T Kang, Renewable and Sustainable Energy Reviews. 1621124412022</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.036292022arXiv preprint</p>
<p>dziner: Rational inverse design of materials with ai agents. M Ansari, J Watchorn, C E Brown, J S Brown, arXiv:2410.039632024arXiv preprint</p>
<p>Semiconductor metal-organic frameworks: future lowbandgap materials. M Usman, S Mendiratta, K.-L Lu, Advanced Materials. 29616050712017</p>
<p>Band gap modulations in uio metalorganic frameworks. E Flage-Larsen, A Røyset, J H Cavka, K Thorshaug, The Journal of Physical Chemistry C. 117402013</p>
<p>Band gap engineering of paradigm mof-5. L.-M Yang, G.-Y Fang, J Ma, E Ganz, S S Han, Crystal growth &amp; design. 1452014</p>
<p>Theoretical investigations on the chemical bonding, electronic structure, and optical properties of the metal-organic framework mof-5. L.-M Yang, P Vajeeston, P Ravindran, H Fjellvag, M Tilset, Inorganic chemistry. 49222010</p>
<p>Recent advancements in mof-based catalysts for applications in electrochemical and photoelectrochemical water splitting: A review. M Ali, E Pervaiz, T Noor, O Rabi, R Zahra, M Yang, International Journal of Energy Research. 4522021</p>
<p>Tuning electrical and mechanical properties of metal-organic frameworks by metal substitution. Y Yan, C Wang, Z Cai, X Wang, F Xuan, ACS Applied Materials &amp; Interfaces. 15362023</p>
<p>Tunability of band gaps in metal-organic frameworks. C.-K Lin, D Zhao, W.-Y Gao, Z Yang, J Ye, T Xu, Q Ge, S Ma, D.-J Liu, Inorganic chemistry. 51162012</p>
<p>New and improved embedding model. R Greene, T Sanders, L Weng, A Neelakantan, 2022</p>
<p>Agent-based learning of materials datasets from the scientific literature. M Ansari, S M Moosavi, Digital Discovery. 3122024</p>
<p>Moformer: self-supervised transformer model for metal-organic framework property prediction. Z Cao, R Magar, Y Wang, A Barati, Farimani, Journal of the American Chemical Society. 14552023</p>
<p>Barlow twins: Self-supervised learning via redundancy reduction. J Zbontar, L Jing, I Misra, Y Lecun, S Deny, International conference on machine learning. PMLR2021</p>
<p>Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. T Xie, J C Grossman, Physical review letters. 120141453012018</p>
<p>Understanding the diversity of the metal-organic framework ecosystem. S M Moosavi, A Nandy, K M Jablonka, D Ongari, J P Janet, P G Boyd, Y Lee, B Smit, H J Kulik, Nature communications. 1112020</p>
<p>Machine learning the quantum-chemical properties of metal-organic frameworks for accelerated materials discovery. A S Rosen, S M Iyer, D Ray, Z Yao, A Aspuru-Guzik, L Gagliardi, J M Notestein, R Q Snurr, Matter. 452021</p>
<p>Rdkit documentation. G Landrum, Release. 201314</p>
<p>Gpt-4 technical report. Openai, 2023</p>
<p>Langchain. H Chase, 102022</p>
<p>Restgpt: Connecting large language models with real-world restful apis. Y Song, W Xiong, D Zhu, W Wu, H Qian, M Song, H Huang, C Li, K Wang, R Yao, Y Tian, S Li, 2023</p>
<p>Organa: A robotic assistant for automated chemistry experimentation and characterization. K Darvish, M Skreta, Y Zhao, N Yoshikawa, S Som, M Bogdanovic, Y Cao, H Hao, H Xu, A Aspuru-Guzik, A Garg, F Shkurti, 2025</p>
<p>LangSim Project. 2024Langsim</p>
<p>pyiron: An integrated development environment for computational materials science. J Janssen, S Surendralal, Y Lysogorskiy, M Todorova, T Hickel, R Drautz, J Neugebauer, Computational Materials Science. 1632019</p>
<p>Langchain. H Chase, </p>
<p>A foundation model for atomistic materials chemistry. I Batatia, P Benner, Y Chiang, A M Elena, D P Kovács, J Riebesell, X R Advincula, M Asta, M Avaylon, W J Baldwin, arXiv:2401.000962023arXiv preprint</p>
<p>Roadmap on data-centric materials science. S Bauer, P Benner, T Bereau, V Blum, M Boley, C Carbogno, C R A Catlow, G Dehm, S Eibl, R Ernstorfer, Modelling and Simulation in Materials Science and Engineering. 326633012024</p>
<p>Leveraging large language models and social media for automation in scanning probe microscopy. Z Diao, H Yamashita, M Abe, arXiv:2405.154902024arXiv preprint</p>
<p>Synergizing human expertise and ai efficiency with language model for microscopy operation and automated experiment design. M C Liu, R K Vasudevan, Machine Learning Science and Technology. 522024</p>
<p>The abtem code: transmission electron microscopy from first principles. J Madsen, T Susi, 2021Open Research Europe</p>
<p>Nion swift: Open source image processing software for instrument control, data acquisition, organization, visualization, and analysis using python. C Meyer, N Dellby, J A Hachtel, T Lovejoy, A Mittelberger, O Krivanek, Microscopy and Microanalysis. 25S22019</p>
<p>Practical and ethical challenges of large language models in education: A systematic scoping review. L Yan, L Sha, L Zhao, Y Li, R Martinez-Maldonado, G Chen, X Li, Y Jin, D Gašević, British Journal of Educational Technology. 552023</p>
<p>Large language models for education: A survey and outlook. S Wang, T Xu, H Li, C Zhang, J Liang, J Tang, P S Yu, Q Wen, 2024</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. E Kasneci, K Sessler, S Küchemann, M Bannert, D Dementieva, F Fischer, U Gasser, G Groh, S Günnemann, E Hüllermeier, S Krusche, G Kutyniok, T Michaeli, C Nerdel, J Pfeffer, O Poquet, M Sailer, A Schmidt, T Seidel, M Stadler, J Weller, J Kuhn, G Kasneci, Learning and Individual Differences. 1032023</p>
<p>The notorious gpt: science communication in the age of artificial intelligence. M S Schäfer, Journal of Science Communication. 222023</p>
<p>Mascqa: investigating materials science knowledge of large language models. M Zaki, N A Krishnan, Digital Discovery. 322024</p>
<p>Benchmarking large language models for math reasoning tasks. K Seßler, Y Rong, E Gözlüklü, E Kasneci, arXiv:2408.108392024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, D Zhou, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W Yih, T Rocktäschel, S Riedel, Advances in Neural Information Processing Systems (NeurIPS). 202033</p>
<p>V Mishra, S Singh, D Ahlawat, M Zaki, V Bihani, H S Grover, B Mishra, S Miret, N Krishnan, arXiv:2412.09560Foundational large language models for materials research. 2024arXiv preprint</p>
<p>The nomad laboratory: from data sharing to artificial intelligence. C Draxl, M Scheffler, Journal of Physics: Materials. 22019</p>
<p>. M L Evans, J D Bocarsly, datalab," 2024</p>
<p>Nomad: A distributed web-based platform for managing materials science research data. M Scheidgen, L Himanen, A N Ladines, D Sikter, M Nakhaee, Á Fekete, T Chang, A Golparvar, J A Márquez, S Brockhauser, Journal of Open Source Software. 89053882023</p>
<p>Retrievalaugmented generation for large language models: A survey. Y Gao, Y Xiong, X Gao, K Jia, J Pan, Y Bi, Y Dai, J Sun, H Wang, H Wang, arXiv:2312.1099720232arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.139712023arXiv preprint</p>
<p>Hypothesis generation with large language models. Y Zhou, H Liu, T Srivastava, H Mei, C Tan, Proceedings of the 1st Workshop on NLP for Science (NLP4Science). the 1st Workshop on NLP for Science (NLP4Science)Association for Computational Linguistics2024</p>
<p>Scientific hypothesis generation by a large language model: Laboratory validation in breast cancer treatment. A Abdel-Rehim, H Zenil, O Orhobor, M Fisher, R J Collins, E Bourne, G W Fearnley, E Tate, H X Smith, L N Soldatova, R D King, 2024</p>
<p>Automating psychological hypothesis generation with ai: when large language models meet causal graph. S Tong, K Mao, Z Huang, Y Zhao, K Peng, Humanities and Social Sciences Communications. 112024</p>
<p>Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy. I Ciucȃ, Y.-S Ting, S Kruk, K Iyer, 2023</p>
<p>Proteinhypothesis: A physics-aware chain of multi-agent rag llm for hypothesis generation in protein science. A Bazgir, R Chandra Praneeth Madugula, Y Zhang, Towards Agentic AI for Science: Hypothesis Generation, Comprehension, Quantification, and Validation. 2025</p>
<p>Beyond designer's knowledge: Generating materials design hypotheses via large language models. Q Liu, M P Polak, S Y Kim, M A A Shuvo, H S Deodhar, J Han, D Morgan, H Oh, 2024</p>
<p>Towards ai research agents in the chemical sciences. O Shir, ChemRxiv. 12024</p>
<p>Agentichypothesis: A survey on hypothesis generation using llm systems. A Bazgir, R Chandra Praneeth Madugula, Y Zhang, Towards Agentic AI for Science: Hypothesis Generation, Comprehension, Quantification, and Validation. 2025</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Z Yang, X Du, J Li, J Zheng, S Poria, E Cambria, 2024</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336</p>
<p>Large language models for scientific information extraction: An empirical study for virology. M Shamsabadi, J Souza, S Auer, 2024</p>
<p>Structured information extraction from scientific text with large language models. J Dagdelen, A Dunn, S Lee, N Walker, A S Rosen, G Ceder, K A Persson, A Jain, Nature Communications. 152024</p>
<p>Large language models for generative information extraction: A survey. D Xu, W Chen, W Peng, C Zhang, T Xu, X Zhao, X Wu, Y Zheng, Y Wang, E Chen, 2024</p>
<p>Generative ai for self-adaptive systems: State of the art and research roadmap. J Li, M Zhang, N Li, D Weyns, Z Jin, K Tei, ACM Transactions on Autonomous and Adaptive Systems. 192024</p>
<p>Sciagent: Tool-augmented language models for scientific reasoning. Y Ma, Z Gou, J Hao, R Xu, S Wang, L Pan, Y Yang, Y Cao, A Sun, H Awadalla, W Chen, 2024</p>
<p>Neo4j. Neo4j, February 24, 2025</p>
<p>Dspy: Compiling declarative language model calls into self-improving pipelines. O Khattab, A Singhvi, P Maheshwari, Z Zhang, K Santhanam, S Vardhamanan, S Haq, A Sharma, T T Joshi, H Moazam, H Miller, M Zaharia, C Potts, 2023</p>
<p>Chainof-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, 2023</p>
<p>Graph maker. Graph Maker, February 24, 2025</p>
<p>. " Gradio, Gradio, 2024. February 24, 2025</p>
<p>Isobench: Benchmarking multimodal foundation models on isomorphic representations. D Fu, G Khalighinejad, O Liu, B Dhingra, D Yogatama, R Jia, W Neiswanger, 2024</p>
<p>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. T Guo, K Guo, B Nan, Z Liang, Z Guo, N V Chawla, O Wiest, X Zhang, 2023</p>
<p>Chemqa: a multimodal question-and-answering dataset on chemistry reasoning. S Zhu, X Liu, G Khalighinejad, 2024</p>
<p>Gpt-4 technical report. Openai, 2024</p>
<p>Gemini: A family of highly capable multimodal models. G Team, 2024</p>
<p>The claude 3 model family: Opus, sonnet, haiku. </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9257 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9257</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9257</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-274131315</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.10541v1.pdf" target="_blank">Does Prompt Formatting Have Any Impact on LLM Performance?</a></p>
                <p><strong>Paper Abstract:</strong> In the realm of Large Language Models (LLMs), prompt optimization is crucial for model performance. Although previous research has explored aspects like rephrasing prompt contexts, using various prompting techniques (like in-context learning and chain-of-thought), and ordering few-shot examples, our understanding of LLM sensitivity to prompt templates remains limited. Therefore, this paper examines the impact of different prompt templates on LLM performance. We formatted the same contexts into various human-readable templates, including plain text, Markdown, JSON, and YAML, and evaluated their impact across tasks like natural language reasoning, code generation, and translation using OpenAI's GPT models. Experiments show that GPT-3.5-turbo's performance varies by up to 40\% in a code translation task depending on the prompt template, while larger models like GPT-4 are more robust to these variations. Our analysis highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9257.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9257.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 code-translation sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo series sensitivity on code translation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that GPT-3.5-turbo family models show large performance variability depending on prompt template when performing code translation tasks, with reported variations up to tens of percent depending on format choice.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-35-turbo (GPT-3.5-turbo series)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>code translation (Code2Code / general code translation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translating code from one programming language to another; evaluated in the paper using Code2Code benchmarks (e.g., CODEXGLUE, HumanEval-X) and BLEU as an evaluation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompts formatted in one of four global template styles (plain text, Markdown, YAML, JSON); content (persona, instructions, examples, output format instructions, user ask) held constant across formats while only structure/syntax changed.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared plain text, Markdown, YAML, and JSON templates against each other.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Reported up to 40% variation in performance for GPT-3.5-turbo on a code translation task depending on prompt template (paper statement in abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>The authors attribute such sensitivity to how models parse and act on structured vs. unstructured prompt syntax; larger models appear less affected. No single definitive mechanistic explanation is proven in the paper for the 40% claim beyond empirical observation.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompt templates: plain text, Markdown, YAML, JSON; the content of placeholders was identical across templates. Metrics for code translation used BLEU (for CODEXGLUE/HumanEval-X). Models evaluated via Azure OpenAI endpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9257.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9257.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FIND Markdown->PlainText effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large performance change on FIND (NL2Code) when switching from Markdown to plain text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>On the FIND benchmark (natural-language-to-code reverse engineering of Python functions), GPT-3.5-turbo variants exhibited very large relative improvements when the prompt was changed from Markdown to plain text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-35-turbo-0613 and gpt-35-turbo-16k-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>FIND (Function Interpretation and Description)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given 5 input-output examples for a Python function, generate the underlying function implementation; evaluated using a 'string indicator' metric counting passed test cases.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompts containing 5 examples and the same instruction content presented in either Markdown or plain text templates (and also YAML/JSON in other conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Markdown versus plain text (also included comparisons to YAML and JSON across experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Reported a dramatic 200% improvement for GPT-35-turbo variants when switching prompts from Markdown to plain text (paper statement in Sensitivity section).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors note that format structure can strongly influence how the model maps examples and instructions to generation behavior; no single universal cause is proven, but smaller models show greater sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>FIND strings category: 500 functions, each given 5 I/O example pairs sampled as few-shot context; evaluation used the string indicator metric from Schwettmann et al.; models compared included 4 GPT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9257.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9257.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 HumanEval JSON->Plain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-32k-0613 performance change on HumanEval when switching JSON to plain text</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The GPT-4-32k-0613 configuration showed a very large relative performance increase on the HumanEval code generation benchmark when prompts were changed from JSON format to plain text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-32k-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HumanEval (NL2Code)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>A set of Python programming problems evaluated with pass@1 metric (does a single generated solution pass the unit tests).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompts provided in JSON vs plain text (content held constant).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>JSON compared to plain text (also other formats included in full experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Paper reports an over 300% performance boost for GPT-4-32k-0613 when switching the prompt format from JSON to plain text on HumanEval (Sensitivity section).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Analysis of outputs suggested the JSON template sometimes caused the model to produce chain-of-thought text without continuing to generate the code (for this model/format pairing). Authors hypothesize this may relate to known model behavior/fixes (OpenAI 'laziness' fixes) for some GPT-4 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>HumanEval dataset: 164 problems, pass@1 metric, some model/format pairs produced chain-of-thought instead of code, likely reducing pass@1. JSON template was one of four global templates compared.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9257.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9257.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMLU JSON vs Markdown (GPT-3.5-16k)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-35-turbo-16k-0613 accuracy difference on MMLU: JSON vs Markdown</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concrete per-example showing that for a subset of MMLU multiple-choice items, changing prompt template from Markdown to JSON (or vice versa) produced large accuracy differences; a cited example reports a 42% accuracy increase for JSON compared to Markdown for GPT-35-turbo-16k-0613 on an international law MMLU subset.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-35-turbo-16k-0613</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (Massive Multitask Language Understanding)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>57-subject multiple-choice benchmark testing world knowledge and problem solving; evaluated using accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Few-shot prompts with 5 dev examples per subject, content consistent; templates were JSON vs Markdown (and others).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Markdown vs JSON (also plain text and YAML in broader experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>accuracy: (example reported) JSON increased accuracy by 42% relative to Markdown on the illustrated MMLU example set (figure caption statement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>42% relative accuracy increase (JSON vs Markdown) in the example highlighted for GPT-35-turbo-16k-0613.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors argue that prompt format changes can materially alter how models interpret task structure and answer selection; no single optimal template generalizes across models.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>MMLU: dev set used as 5-shot examples per subject, test set size 14,079 questions; accuracy metric; in some MMLU experiments temperature was set to 0 to remove stochasticity for consistency measures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9257.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9257.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency: GPT-3.5 low</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Low cross-template answer consistency of GPT-3.5-turbo series</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Across format pairs (e.g., Markdown vs JSON) GPT-3.5-turbo models produced non-identical answers a majority of the time; an example reported only 16% identical responses between Markdown and JSON on MMLU.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-35-turbo series (gpt-35-turbo-0613, gpt-35-turbo-16k-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU (consistency measurement)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measuring proportion of test items with identical answers across two prompt templates (consistency metric C(Pa,Pb)).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Same content presented in two different templates (e.g., Markdown vs JSON); MMLU runs used temperature=0 to remove sampling variance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Pairs of templates (Markdown, JSON, plain text, YAML) compared pairwise.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>consistency: 0.16 (16% identical responses between Markdown and JSON reported for GPT-3.5-turbo series on MMLU).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Smaller GPT-3.5 models are less consistent across template changes, indicating the format influences the model's mapping from prompt to answer; larger models show better consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Consistency metric from Shu et al. (2023) used; temperature set to 0 for MMLU experiments; comparisons shown in Figures 2 and 8 in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9257.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9257.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Consistency: GPT-4 higher</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Greater cross-template output consistency for GPT-4 models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 models exhibited higher consistency between outputs produced from different prompt templates (consistency metric > 0.5 on MMLU and better scores on FIND) compared to GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 family (gpt-4-32k-0613, gpt-4-1106-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MMLU and FIND (consistency measurement)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Proportion of test items with identical answers across templates (consistency metric); also measured across FIND settings.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Same content in different templates; MMLU used deterministic setting (temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Markdown vs JSON vs plain text vs YAML.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>consistency: >0.5 (paper reports GPT-4 consistently exceeded 0.5 on MMLU; precise values per pair in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Larger models like GPT-4 appear more robust and produce more uniform outputs across prompt structures, suggesting improved generalization over input formatting.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Consistency evaluated on MMLU (temperature=0) and FIND (following FIND settings). Figures 2 and 8 present aggregated consistency comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9257.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9257.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Robustness (CMD) across models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coefficient of Mean Deviation (CMD) robustness comparison across GPT variants</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper computes CMD (dispersion of scalar metrics across prompt templates) and finds GPT-4-1106-preview has the smallest CMD (<0.036), GPT-4-32k-0613 has CMD up to ~0.043, and GPT-3.5 series CMD ranges from 0.035 to 0.176, indicating greater sensitivity in GPT-3.5.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4-1106-preview, gpt-4-32k-0613, gpt-35-turbo series</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate across all benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CMD is the coefficient of mean deviation of scalar task metrics across prompt templates; used as an aggregate measure of robustness to template variation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Performance across the four template formats (plain text, Markdown, YAML, JSON) aggregated into CMD per model/benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Cross-model comparison of CMD values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Reported CMDs: GPT-4-1106-preview < 0.036; GPT-4-32k-0613 up to 0.043; GPT-3.5 series range 0.035–0.176.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors suggest the improved robustness of GPT-4 variants may stem from better capacity to process diverse formats and possibly differences in prompt embedding behavior; the 1106-preview variant appeared particularly robust.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>CMD computed across all benchmarks (MMLU, NER Finance, HumanEval, FIND, CODEXGLUE, HumanEval-X). Figures and tables (e.g., Figure 6, Table 4) provide per-benchmark CMDs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9257.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9257.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transferability / IoU</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Intersection-over-Union (IoU) of top-performing templates between models (prompt transferability)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper measures IoU of sets of top-performing templates between model pairs and finds low transferability across model series (IoU often below 0.2), but high overlap within closely related variants (IoU > 0.7 for same-subseries pairs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>cross-model comparison (GPT-3.5 and GPT-4 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>NER Finance and other benchmarks (template transferability)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>IoU between sets of statistically indistinguishable top templates for each model, indicating how well a template that is top for one model transfers to another.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Top templates identified per model from the set {plain text, Markdown, YAML, JSON}; IoU computed pairwise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Pairwise model comparisons (within sub-series and across series).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>IoU often < 0.2 between different model series; IoU > 0.7 within same-version pairs (e.g., gpt-35-turbo-0613 vs gpt-35-turbo-16k-0613).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Findings show no single prompt format generalizes across distinct GPT model series; prompt engineering must be model-specific. The authors interpret low IoU as evidence that prompt preference is not transferable across model families.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>IoU defined as |P_m1 ∩ P_m2| / |P_m1 ∪ P_m2| where P_m are top templates per model determined by matched-pairs t-test; thresholds discussed (0.5, 0.7). Figure 3 and Appendix B present IoU heatmaps (NER Finance shown as example).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9257.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9257.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model-format preference (GPT-3.5 vs GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observed prompt-format preferences across GPT model families</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirically, GPT-3.5-turbo preferred JSON-format prompts while GPT-4 models tended to favor Markdown-format prompts in the authors' experiments, implying model-specific format optima.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-35-turbo series and GPT-4 series</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Aggregate across benchmarks (format preference analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Analysis of which prompt templates yielded top performance per model across multiple benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Four global prompt templates (plain text, Markdown, YAML, JSON) compared while holding content identical.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Comparative ranking of the four templates per model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Authors report that no single template is universally optimal; preferences vary by model family (GPT-3.5 preferring JSON, GPT-4 preferring Markdown). This is presented as empirical observation rather than a proven causal mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Top-template identification done via matched-pairs t-tests across benchmarks; IoU and ranking figures (Figure 5 and related) summarize the model-specific preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Does Prompt Formatting Have Any Impact on LLM Performance?', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting <em>(Rating: 2)</em></li>
                <li>Mind your format: Towards consistent evaluation of in-context learning improvements <em>(Rating: 2)</em></li>
                <li>You don't need a personality test to know these models are unreliable: Assessing the reliability of large language models on psychometric instruments <em>(Rating: 1)</em></li>
                <li>A systematic survey of prompt engineering in large language models: Techniques and applications <em>(Rating: 1)</em></li>
                <li>Table meets llm: Can large language models understand structured table data? a benchmark and empirical study <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9257",
    "paper_id": "paper-274131315",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "GPT-3.5 code-translation sensitivity",
            "name_full": "GPT-3.5-turbo series sensitivity on code translation tasks",
            "brief_description": "The paper reports that GPT-3.5-turbo family models show large performance variability depending on prompt template when performing code translation tasks, with reported variations up to tens of percent depending on format choice.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-35-turbo (GPT-3.5-turbo series)",
            "model_size": null,
            "task_name": "code translation (Code2Code / general code translation)",
            "task_description": "Translating code from one programming language to another; evaluated in the paper using Code2Code benchmarks (e.g., CODEXGLUE, HumanEval-X) and BLEU as an evaluation metric.",
            "presentation_format": "Prompts formatted in one of four global template styles (plain text, Markdown, YAML, JSON); content (persona, instructions, examples, output format instructions, user ask) held constant across formats while only structure/syntax changed.",
            "comparison_format": "Compared plain text, Markdown, YAML, and JSON templates against each other.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": "Reported up to 40% variation in performance for GPT-3.5-turbo on a code translation task depending on prompt template (paper statement in abstract).",
            "explanation_or_hypothesis": "The authors attribute such sensitivity to how models parse and act on structured vs. unstructured prompt syntax; larger models appear less affected. No single definitive mechanistic explanation is proven in the paper for the 40% claim beyond empirical observation.",
            "null_or_negative_result": false,
            "experimental_details": "Prompt templates: plain text, Markdown, YAML, JSON; the content of placeholders was identical across templates. Metrics for code translation used BLEU (for CODEXGLUE/HumanEval-X). Models evaluated via Azure OpenAI endpoints.",
            "uuid": "e9257.0",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "FIND Markdown-&gt;PlainText effect",
            "name_full": "Large performance change on FIND (NL2Code) when switching from Markdown to plain text",
            "brief_description": "On the FIND benchmark (natural-language-to-code reverse engineering of Python functions), GPT-3.5-turbo variants exhibited very large relative improvements when the prompt was changed from Markdown to plain text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-35-turbo-0613 and gpt-35-turbo-16k-0613",
            "model_size": null,
            "task_name": "FIND (Function Interpretation and Description)",
            "task_description": "Given 5 input-output examples for a Python function, generate the underlying function implementation; evaluated using a 'string indicator' metric counting passed test cases.",
            "presentation_format": "Few-shot prompts containing 5 examples and the same instruction content presented in either Markdown or plain text templates (and also YAML/JSON in other conditions).",
            "comparison_format": "Markdown versus plain text (also included comparisons to YAML and JSON across experiments).",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": "Reported a dramatic 200% improvement for GPT-35-turbo variants when switching prompts from Markdown to plain text (paper statement in Sensitivity section).",
            "explanation_or_hypothesis": "Authors note that format structure can strongly influence how the model maps examples and instructions to generation behavior; no single universal cause is proven, but smaller models show greater sensitivity.",
            "null_or_negative_result": false,
            "experimental_details": "FIND strings category: 500 functions, each given 5 I/O example pairs sampled as few-shot context; evaluation used the string indicator metric from Schwettmann et al.; models compared included 4 GPT variants.",
            "uuid": "e9257.1",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GPT-4 HumanEval JSON-&gt;Plain",
            "name_full": "GPT-4-32k-0613 performance change on HumanEval when switching JSON to plain text",
            "brief_description": "The GPT-4-32k-0613 configuration showed a very large relative performance increase on the HumanEval code generation benchmark when prompts were changed from JSON format to plain text.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4-32k-0613",
            "model_size": null,
            "task_name": "HumanEval (NL2Code)",
            "task_description": "A set of Python programming problems evaluated with pass@1 metric (does a single generated solution pass the unit tests).",
            "presentation_format": "Prompts provided in JSON vs plain text (content held constant).",
            "comparison_format": "JSON compared to plain text (also other formats included in full experiments).",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": "Paper reports an over 300% performance boost for GPT-4-32k-0613 when switching the prompt format from JSON to plain text on HumanEval (Sensitivity section).",
            "explanation_or_hypothesis": "Analysis of outputs suggested the JSON template sometimes caused the model to produce chain-of-thought text without continuing to generate the code (for this model/format pairing). Authors hypothesize this may relate to known model behavior/fixes (OpenAI 'laziness' fixes) for some GPT-4 variants.",
            "null_or_negative_result": false,
            "experimental_details": "HumanEval dataset: 164 problems, pass@1 metric, some model/format pairs produced chain-of-thought instead of code, likely reducing pass@1. JSON template was one of four global templates compared.",
            "uuid": "e9257.2",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "MMLU JSON vs Markdown (GPT-3.5-16k)",
            "name_full": "GPT-35-turbo-16k-0613 accuracy difference on MMLU: JSON vs Markdown",
            "brief_description": "A concrete per-example showing that for a subset of MMLU multiple-choice items, changing prompt template from Markdown to JSON (or vice versa) produced large accuracy differences; a cited example reports a 42% accuracy increase for JSON compared to Markdown for GPT-35-turbo-16k-0613 on an international law MMLU subset.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-35-turbo-16k-0613",
            "model_size": null,
            "task_name": "MMLU (Massive Multitask Language Understanding)",
            "task_description": "57-subject multiple-choice benchmark testing world knowledge and problem solving; evaluated using accuracy.",
            "presentation_format": "Few-shot prompts with 5 dev examples per subject, content consistent; templates were JSON vs Markdown (and others).",
            "comparison_format": "Markdown vs JSON (also plain text and YAML in broader experiments).",
            "performance": "accuracy: (example reported) JSON increased accuracy by 42% relative to Markdown on the illustrated MMLU example set (figure caption statement).",
            "performance_comparison": null,
            "format_effect_size": "42% relative accuracy increase (JSON vs Markdown) in the example highlighted for GPT-35-turbo-16k-0613.",
            "explanation_or_hypothesis": "Authors argue that prompt format changes can materially alter how models interpret task structure and answer selection; no single optimal template generalizes across models.",
            "null_or_negative_result": false,
            "experimental_details": "MMLU: dev set used as 5-shot examples per subject, test set size 14,079 questions; accuracy metric; in some MMLU experiments temperature was set to 0 to remove stochasticity for consistency measures.",
            "uuid": "e9257.3",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Consistency: GPT-3.5 low",
            "name_full": "Low cross-template answer consistency of GPT-3.5-turbo series",
            "brief_description": "Across format pairs (e.g., Markdown vs JSON) GPT-3.5-turbo models produced non-identical answers a majority of the time; an example reported only 16% identical responses between Markdown and JSON on MMLU.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-35-turbo series (gpt-35-turbo-0613, gpt-35-turbo-16k-0613)",
            "model_size": null,
            "task_name": "MMLU (consistency measurement)",
            "task_description": "Measuring proportion of test items with identical answers across two prompt templates (consistency metric C(Pa,Pb)).",
            "presentation_format": "Same content presented in two different templates (e.g., Markdown vs JSON); MMLU runs used temperature=0 to remove sampling variance.",
            "comparison_format": "Pairs of templates (Markdown, JSON, plain text, YAML) compared pairwise.",
            "performance": "consistency: 0.16 (16% identical responses between Markdown and JSON reported for GPT-3.5-turbo series on MMLU).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Smaller GPT-3.5 models are less consistent across template changes, indicating the format influences the model's mapping from prompt to answer; larger models show better consistency.",
            "null_or_negative_result": false,
            "experimental_details": "Consistency metric from Shu et al. (2023) used; temperature set to 0 for MMLU experiments; comparisons shown in Figures 2 and 8 in paper.",
            "uuid": "e9257.4",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Consistency: GPT-4 higher",
            "name_full": "Greater cross-template output consistency for GPT-4 models",
            "brief_description": "GPT-4 models exhibited higher consistency between outputs produced from different prompt templates (consistency metric &gt; 0.5 on MMLU and better scores on FIND) compared to GPT-3.5.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 family (gpt-4-32k-0613, gpt-4-1106-preview)",
            "model_size": null,
            "task_name": "MMLU and FIND (consistency measurement)",
            "task_description": "Proportion of test items with identical answers across templates (consistency metric); also measured across FIND settings.",
            "presentation_format": "Same content in different templates; MMLU used deterministic setting (temperature=0).",
            "comparison_format": "Markdown vs JSON vs plain text vs YAML.",
            "performance": "consistency: &gt;0.5 (paper reports GPT-4 consistently exceeded 0.5 on MMLU; precise values per pair in figures).",
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Larger models like GPT-4 appear more robust and produce more uniform outputs across prompt structures, suggesting improved generalization over input formatting.",
            "null_or_negative_result": false,
            "experimental_details": "Consistency evaluated on MMLU (temperature=0) and FIND (following FIND settings). Figures 2 and 8 present aggregated consistency comparisons.",
            "uuid": "e9257.5",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Robustness (CMD) across models",
            "name_full": "Coefficient of Mean Deviation (CMD) robustness comparison across GPT variants",
            "brief_description": "The paper computes CMD (dispersion of scalar metrics across prompt templates) and finds GPT-4-1106-preview has the smallest CMD (&lt;0.036), GPT-4-32k-0613 has CMD up to ~0.043, and GPT-3.5 series CMD ranges from 0.035 to 0.176, indicating greater sensitivity in GPT-3.5.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4-1106-preview, gpt-4-32k-0613, gpt-35-turbo series",
            "model_size": null,
            "task_name": "Aggregate across all benchmarks",
            "task_description": "CMD is the coefficient of mean deviation of scalar task metrics across prompt templates; used as an aggregate measure of robustness to template variation.",
            "presentation_format": "Performance across the four template formats (plain text, Markdown, YAML, JSON) aggregated into CMD per model/benchmark.",
            "comparison_format": "Cross-model comparison of CMD values.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": "Reported CMDs: GPT-4-1106-preview &lt; 0.036; GPT-4-32k-0613 up to 0.043; GPT-3.5 series range 0.035–0.176.",
            "explanation_or_hypothesis": "Authors suggest the improved robustness of GPT-4 variants may stem from better capacity to process diverse formats and possibly differences in prompt embedding behavior; the 1106-preview variant appeared particularly robust.",
            "null_or_negative_result": false,
            "experimental_details": "CMD computed across all benchmarks (MMLU, NER Finance, HumanEval, FIND, CODEXGLUE, HumanEval-X). Figures and tables (e.g., Figure 6, Table 4) provide per-benchmark CMDs.",
            "uuid": "e9257.6",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Transferability / IoU",
            "name_full": "Intersection-over-Union (IoU) of top-performing templates between models (prompt transferability)",
            "brief_description": "The paper measures IoU of sets of top-performing templates between model pairs and finds low transferability across model series (IoU often below 0.2), but high overlap within closely related variants (IoU &gt; 0.7 for same-subseries pairs).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "cross-model comparison (GPT-3.5 and GPT-4 variants)",
            "model_size": null,
            "task_name": "NER Finance and other benchmarks (template transferability)",
            "task_description": "IoU between sets of statistically indistinguishable top templates for each model, indicating how well a template that is top for one model transfers to another.",
            "presentation_format": "Top templates identified per model from the set {plain text, Markdown, YAML, JSON}; IoU computed pairwise.",
            "comparison_format": "Pairwise model comparisons (within sub-series and across series).",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": "IoU often &lt; 0.2 between different model series; IoU &gt; 0.7 within same-version pairs (e.g., gpt-35-turbo-0613 vs gpt-35-turbo-16k-0613).",
            "explanation_or_hypothesis": "Findings show no single prompt format generalizes across distinct GPT model series; prompt engineering must be model-specific. The authors interpret low IoU as evidence that prompt preference is not transferable across model families.",
            "null_or_negative_result": false,
            "experimental_details": "IoU defined as |P_m1 ∩ P_m2| / |P_m1 ∪ P_m2| where P_m are top templates per model determined by matched-pairs t-test; thresholds discussed (0.5, 0.7). Figure 3 and Appendix B present IoU heatmaps (NER Finance shown as example).",
            "uuid": "e9257.7",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Model-format preference (GPT-3.5 vs GPT-4)",
            "name_full": "Observed prompt-format preferences across GPT model families",
            "brief_description": "Empirically, GPT-3.5-turbo preferred JSON-format prompts while GPT-4 models tended to favor Markdown-format prompts in the authors' experiments, implying model-specific format optima.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-35-turbo series and GPT-4 series",
            "model_size": null,
            "task_name": "Aggregate across benchmarks (format preference analysis)",
            "task_description": "Analysis of which prompt templates yielded top performance per model across multiple benchmarks.",
            "presentation_format": "Four global prompt templates (plain text, Markdown, YAML, JSON) compared while holding content identical.",
            "comparison_format": "Comparative ranking of the four templates per model.",
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Authors report that no single template is universally optimal; preferences vary by model family (GPT-3.5 preferring JSON, GPT-4 preferring Markdown). This is presented as empirical observation rather than a proven causal mechanism.",
            "null_or_negative_result": false,
            "experimental_details": "Top-template identification done via matched-pairs t-tests across benchmarks; IoU and ranking figures (Figure 5 and related) summarize the model-specific preferences.",
            "uuid": "e9257.8",
            "source_info": {
                "paper_title": "Does Prompt Formatting Have Any Impact on LLM Performance?",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
            "rating": 2,
            "sanitized_title": "quantifying_language_models_sensitivity_to_spurious_features_in_prompt_design_or_how_i_learned_to_start_worrying_about_prompt_formatting"
        },
        {
            "paper_title": "Mind your format: Towards consistent evaluation of in-context learning improvements",
            "rating": 2,
            "sanitized_title": "mind_your_format_towards_consistent_evaluation_of_incontext_learning_improvements"
        },
        {
            "paper_title": "You don't need a personality test to know these models are unreliable: Assessing the reliability of large language models on psychometric instruments",
            "rating": 1,
            "sanitized_title": "you_dont_need_a_personality_test_to_know_these_models_are_unreliable_assessing_the_reliability_of_large_language_models_on_psychometric_instruments"
        },
        {
            "paper_title": "A systematic survey of prompt engineering in large language models: Techniques and applications",
            "rating": 1,
            "sanitized_title": "a_systematic_survey_of_prompt_engineering_in_large_language_models_techniques_and_applications"
        },
        {
            "paper_title": "Table meets llm: Can large language models understand structured table data? a benchmark and empirical study",
            "rating": 1,
            "sanitized_title": "table_meets_llm_can_large_language_models_understand_structured_table_data_a_benchmark_and_empirical_study"
        }
    ],
    "cost": 0.01489675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Does Prompt Formatting Have Any Impact on LLM Performance?
15 Nov 2024</p>
<p>Jia He hejia@microsoft.com 
Mukund Rungta rungtamukund@microsoft.com 
David Koleczek dkoleczek@microsoft.com 
Arshdeep Sekhon 
Franklin X Wang fxwang@mit.edu 
Sadid Hasan 
Microsoft 
Josh Achiam 
Steven Adler 
Sandhini Agarwal 
Lama Ahmad 
Ilge Akkaya 
Florencia Leoni Aleman 
Diogo Almeida 
Janko Altenschmidt 
Sam Altman 
Tom B Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Amanda Askell 
Ariel Herbert-Voss 
Gretchen Krueger 
Tom Henighan 
Rewon Child 
Aditya Ramesh 
Daniel M Ziegler 
Jeffrey Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Mateusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Sam Mc- Candlish 
Alec Radford 
Ilya Sutskever 
Dario 2020 Amodei 
Nicholas Carlini 
Daniel Paleka 
Krishnamurthy Dj 
Thomas Steinke 
Jonathan Hayase 
A Feder Cooper 
Katherine Lee 
Matthew Jagielski 
Milad Nasr 
Arthur Conmy 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Henrique Ponde 
Oliveira Pinto 
Jared Ka- Plan 
Harri Edwards 
Yuri Burda 
Nicholas Joseph 
Greg Brockman 
Alex Ray 
Raul Puri 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Andrew N Carr 
Jan Leike 
Vedant Misra 
Evan Morikawa 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Sam Mccandlish 
Wojciech 2021 Zaremba 
Evaluating 
Aakanksha Chowdhery 
Sharan Narang 
Jacob Devlin 
Maarten Bosma 
Gaurav Mishra 
Adam Roberts 
HyungPaul Barham 
Won Chung 
Charles Sutton 
Sebastian Gehrmann 
Parker Schuh 
Kensen Shi 
Sasha Tsvyashchenko 
Joshua Maynez 
Abhishek Rao 
Parker Barnes 
Yi Tay 
Noam Shazeer 
Vin- Odkumar Prabhakaran 
Emily Reif 
Nan Du 
Ben Hutchinson 
Reiner Pope 
James Bradbury 
Jacob Austin 
Michael Isard 
Guy Gur-Ari 
Pengcheng Yin 
Toju Duke 
Anselm Levskaya 
Sanjay Ghemawat 
Sunipa Dev 
Henryk Michalewski 
Xavier Garcia 
Kevin Robinson 
Liam Fedus 
Denny Zhou 
Daphne Ippolito 
David Luan 
Hyeontaek Lim 
Barret Zoph 
Alexander Spiridonov 
Ryan Sepassi 
David Dohan 
Shivani Agrawal 
Mark Omernick 
An- Drew M Dai 
Thanumalayan Sankaranarayana 
Marie Pellat 
Aitor Lewkowycz 
Erica Moreira 
Oleksandr Polozov 
Zongwei Zhou 
Xuezhi Wang 
Brennan Saeta 
Mark Diaz 
Orhan Firat 
Michele Catasta 
Jason Wei 
Kathy Meier-Hellstern 
Douglas Eck 
Jeff Dean 
Slav Petrov 
Does Prompt Formatting Have Any Impact on LLM Performance?
15 Nov 2024CDCC3C46F3A364588D91B5E29C7DAB1AarXiv:2411.10541v1[cs.CL]
In the realm of Large Language Models (LLMs), prompt optimization is crucial for model performance.Although previous research has explored aspects like rephrasing prompt contexts, using various prompting techniques (like in-context learning and chain-ofthought), and ordering few-shot examples, our understanding of LLM sensitivity to prompt templates remains limited.Therefore, this paper examines the impact of different prompt templates on LLM performance.We formatted the same contexts into various human-readable templates, including plain text, Markdown, JSON, and YAML, and evaluated their impact across tasks like natural language reasoning, code generation, and translation using Ope-nAI's GPT models.Experiments show that GPT-3.5-turbo'sperformance varies by up to 40% in a code translation task depending on the prompt template, while larger models like GPT-4 are more robust to these variations.Our analysis highlights the need to reconsider the use of fixed prompt templates, as different formats can significantly affect model performance.</p>
<p>Introduction</p>
<p>The emergence of LLMs marks a significant advancement in AI, revolutionizing natural language processing, understanding, and generation ((Brown et al., 2020;Ouyang et al., 2022;Chowdhery et al., 2022;Achiam et al., 2023)).Prompt engineering has become crucial, focusing on crafting inputs that guide LLMs to produce desired outputs, leveraging a nuanced understanding of how these models interpret and respond to prompts ( (Sahoo et al., 2024)).Effective prompt design generally includes clear instructions, Retrieval-Augmented Generation (RAG) or other prompting approaches for enhancing in-context learning (ICL), and appropriate formatting.</p>
<ul>
<li>Equal Contribution Figure 1: An example to demonstrate how prompt formatting impacts GPT-35-turbo-16k-0613 model's performance based on our experiments on multiple choice questions related to international law from the MMLU benchmark ( (Hendrycks et al., 2020)).Texts inside "&lt;&gt;" are replaced by actual contexts.Accuracy goes up by 42% for JSON compared to Markdown.</li>
</ul>
<p>Often overlooked, prompt format can significantly impact model performance, contrary to the assumption that it remains stable across different templates.There exist limited research and anecdotal evidence ((Aghajanyan, June 2023;Sclar et al., 2023;Voronov et al., 2024)), which suggest that prompt format choices may lead to substantial performance variations, raising concerns about current evaluation standards that ignore this factor.For example, one study showed that LLMs are sensitive to minor fine-grained prompt modifications, such as separators or capitalization changes ( (Sclar et al., 2023)).Also, existing evaluation approaches typically use fixed templates, potentially leading to misleading conclusions ( (Voronov et al., 2024)).</p>
<p>Inspired by these findings, our study investigates whether broader changes in prompt format affect model efficacy.We evaluate the impact of prompt templates on OpenAI's four GPT models across six benchmarks, using plain text, Markdown, YAML, and JSON formats, as illustrated in Figure 1.This comprehensive approach contrasts with prior research that primarily examined minor template alterations.Our research focuses on the GPT model series for two main reasons: the lack of comparative analyses of behavioral patterns across different GPT model iterations, especially the latest GPT-4turbo, and the need to identify effective interaction methods and optimal input formats for these models, which do not disclose their training methodologies or data.</p>
<p>Our study is designed to investigate the following key questions:</p>
<p>• Sensitivity: To what extent does the performance of GPT models vary with different prompt formats?</p>
<p>• Consistency: Are GPT models capable of producing uniform responses to identical queries when presented with varying prompt structures?</p>
<p>• Transferability: Is there an optimal prompt format that is universally effective across diverse GPT models, thereby ensuring peak performance?</p>
<p>In addition to our primary questions, we explore the correlation between prompt format efficacy and task-specific competencies, as well as the impact of model size on performance.OpenAI's GPT models including GPT-35-turbo and GPT-4 (Achiam et al., 2023) show unpredictable sensitivity to prompt format changes, with significant performance discrepancies across all models and benchmarks.Notably, there is no universally optimal format, even within the same generational lineage.However, GPT-4turbo demonstrates greater resilience to prompt format changes compared to its predecessors and contemporaries.In summary, our key contributions are as follows:</p>
<p>• This study is the first to compare the impact of different prompt formats on GPT models' performance across various tasks, examining plain text, Markdown, YAML, and JSON.</p>
<p>• Our research provides an extensive analysis of prompt formatting effects on GPT models across a wide range of tasks, including multiple-choice questions, code generation, and translation.</p>
<p>• We present an evaluation of the GPT model iterations via Azure OpenAI, revealing that GPT-4-turbo is less susceptible to prompt structure variations compared to earlier models.</p>
<p>2 Experimental Setup</p>
<p>Datasets</p>
<p>Our experiments span various tasks and datasets, categorized into three main groups:</p>
<p>• Natural Language to Natural Language (NL2NL): Includes Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020) and NER Finance from OpenAI Evals (OpenAI, 2023).</p>
<p>• Natural Language to Code (NL2Code): Includes HumanEval (Chen et al., 2021) and FIND (Schwettmann et al., 2023).</p>
<p>• Code to Code (Code2Code): Includes CODEXGLUE (Lu et al., 2021) and HumanEval-X (Zheng et al., 2023).</p>
<p>We initially assess model performance using task-specific scalar scoring functions, followed by metrics from Sections 3 to 5 to address our research questions.Detailed dataset descriptions and metrics are in Appendix B.</p>
<p>Prompt Design</p>
<p>We use various input formats: plain text, markdown, YAML, and JSON.Prompts include five components: persona, task instructions, examples, output format instructions, and user ask.We ensure the content of each placeholder stays the same across different prompt formats.The only differences are in structure and syntax.To avoid confounding variables, we design the prompts so that the context and meaning remain consistent, regardless of the format.Examples are in Appendix C.</p>
<p>Models</p>
<p>Experiments were conducted on OpenAI's GPT-3.5 and GPT-4 models via Azure (Microsoft, 2024).For GPT-3.5, we used "gpt-35-turbo-0613" and "gpt-35-turbo-16k-0613" to compare context window sizes (4k vs. 16k).For GPT-4, we used "gpt-4-32k-0613" and "gpt-4-1106-preview" to test the newer, faster variant with a 128k context window.</p>
<p>Sensitivity</p>
<p>Metrics Definition</p>
<p>Sensitivity.To evaluate how much the choice of prompt template impacts a model's performance on a task T, we look at a variety of templates {p 1 , p 2 , . . ., p n } and measure their performance 3.2 Does prompt format impact the performance of language models and how significant is the performance variation when switching prompt formats?</p>
<p>We begin by analyzing if model performance is sensitive to any changes in the prompt format at all.To assess this, we conducted a one-sided matched pair t-test, comparing the best and worst performing formats for each model across various benchmarks.The resulting p-values, which are shown in Table 1, are mostly below 0.01.This suggests that the differences in model performance due to format changes are statistically significant.</p>
<p>Figure 4 visualizes how the models fare across all benchmarks, highlighting a considerable range in performance.For instance, in the FIND dataset, both GPT-35-turbo-0613 and GPT-35-turbo-16k-0613 show a dramatic 200% improvement when prompts are switched from Markdown to plain text.Similarly, for the HumanEval benchmark, the GPT-4 model with a 32k-0613 configuration exhibits an impressive performance boost of over 300% when the prompt format is changed from JSON to plain text.This suggests, LLM performance may not be robust to the choice of prompt format.</p>
<p>Consistency</p>
<p>Metrics Definition</p>
<p>Following the sensitivity measurement, we quantify the extent of answer variation due to prompt changes using the consistency metric from (Shu et al., 2023).This metric calculates the proportion of test samples that yield identical responses for two prompt templates.The consistency C(P a , P b ) for templates P a and P b is defined as:
C(P a , P b ) = 1 N N i=1 1 (A Pa (x i ) = A P b (x i ))
where N is the test set size and A represents the model's answer.A higher score indicates greater answer consistency between prompts.</p>
<p>4.2 Are larger models more consistent in generated outputs between templates?</p>
<p>Our study assessed the consistency of model outputs using the MMLU and FIND datasets, as shown in Figures 2 and 8.For MMLU, we set the temperature to zero to eliminate response variability.The GPT-3.5-turbo series displayed low consistency, with scores below 0.5, and only 16% identical responses between Markdown and JSON formats.In contrast, GPT-4's consistency scores surpassed 0.5, indicating better reliability across different prompts.For the FIND dataset, following the settings from (Schwettmann et al., 2023), GPT-4 again outperformed the GPT-3.5-turboseries in consistency.These findings suggest that larger models like GPT-4 are more consistent, but there is still a need for model improvements to achieve reliable performance across various formats.In summary, the consistency of model responses varies with size, with larger models like GPT-4 providing more uniform outputs across different prompts.</p>
<p>Transferability</p>
<p>Metrics Definition</p>
<p>Intersection-over-Union. To assess the transferability of prompt templates between models, we calculate the Intersection-over-Union (IoU) for the sets of top-performing templates between model pairs.Top-performing templates are those with statistically indistinguishable performance, determined by a matched pairs t-test.The IoU is defined as:
IoU = |P m1 ∩ P m2 | |P m1 ∪ P m2 |
where P m1 and P m2 represent the sets of top templates for models m1 and m2, respectively.An IoU threshold of 0.5 is common, but a higher threshold like 0.7 indicates greater overlap.</p>
<p>5.2 Do models from same family exhibit similar trend across prompt formats?</p>
<p>Our research into Large Language Models (LLMs), GPT-based models in particular, reveals that prompt formatting preferences vary by model.As demonstrated in Figure 5, GPT-3.5-turboprefers JSON, whereas GPT-4 favors Markdown.When examining prompt transferability using Intersection- over-Union (IoU) metrics (Figure 3 and Appendix B), we found low compatibility between different model series, with IoU often below 0.2.However, models from the same sub-series, like GPT-35-turbo-16k-0613 and GPT-35-turbo-0613, show high IoU over 0.7.These insights highlight that even with common architectures and training goals, GPT-models react differently to identical prompts.Optimal performance requires model-specific prompt engineering, as no single format works universally across various GPT models, even within the same family.This underscores the necessity for tailored prompt engineering due to the nontransferability of prompt formats across different GPT models.</p>
<p>Conclusion</p>
<p>Our study reveals that the way prompts are formatted significantly impacts GPT-based models' performance, with no single format excelling universally.This finding questions current evaluation methods that often ignore prompt structure, potentially misjudging a model's true abilities.We advocate for diverse prompt formats in future LLM testing to accurately gauge and enhance their performance.</p>
<p>Regarding explainability, we observe that model size affects model's responses to prompt variations.For instance, GPT-4's performance is less influenced by prompt changes compared to GPT-3.5, suggesting that larger models may process prompts more consistently.This discovery prompts further research into LLM interpretability, aiming to refine AI adaptability and human-AI interaction.</p>
<p>Limitations</p>
<p>This study was focused on GPT-based models, however, we plan to examine the impact of prompt formats on other models, such as LLaMA (Touvron et al., 2023), Gemini (Team et al., 2023), PaLM (Chowdhery et al., 2022), or smaller models like Phi (Li et al., 2023) in the future.This would provide a more holistic understanding of the influence that prompt formatting exerts across different LLM families.</p>
<p>Moreover, there is an opportunity to enhance the breadth of template exploration in subsequent studies.Our research did not include formats like HTML or XML, which are prevalent in the training datasets of many models.Incorporating these formats could yield a more exhaustive examination of prompt format effects.</p>
<p>Lastly, our experimental design maintained all other prompt design elements constant, isolating prompt format as the sole variable.It would be intriguing for future work to investigate how the sensitivity of models to prompt format might shift when other prompt engineering techniques are modified.This includes varying the number of fewshot examples provided or refining the precision of prompt instructions.Such research could offer valuable insights into the interplay between prompt structure and model responsiveness, potentially informing more effective prompt engineering practices.Yao et al., 2023)) have been introduced.A thorough examination of these methodologies can be found in the survey by (Sahoo et al., 2024).</p>
<p>In recent developments, a novel prompt programming framework ((Microsoft)) has been introduced, which offers greater control and efficiency in generating structured outputs.Our study diverges from this approach by examining the effects of more prevalent and established prompt formats on LLMs, as opposed to investigating formats that are newly proposed and not widely adopted yet.Furthermore, it is important to note that third-party tools are predominantly designed for integration with opensource models, which may not seamlessly extend to proprietary models such as GPT.Another similar vein of research is dedicated to the structural design of prompts, aiming to optimize task performance without altering the inherent semantic content.This includes investigations into the sequential arrangement of context ( (Liu et al., 2023;Zhao et al., 2021;Lu et al., 2022)) and the design of prompt formats ( (Sclar et al., 2023;Voronov et al., 2024;Shu et al., 2023)).Our work contributes to this growing body of literature by examining the impact of prompt formatting on the performance of LLMs.</p>
<p>Prompt Format The sensitivity of LLMs to prompt construction is a well-documented phenomenon, yet research on the impact of prompt formats on model performance remains sparse.Pioneering studies ( (Sclar et al., 2023;Voronov et al., 2024;Shu et al., 2023)) have conducted rigorous investigations, revealing that widely used opensource LLMs exhibit extreme sensitivity to variations in prompt format.These studies, however, primarily focus on subtle, local changes to the format-such as the number of colons following a question, the insertion of newlines, or the selection of input/output verbalizers.Besides, their experimental designs are confined to classification tasks, limiting the generalizability of findings across diverse tasks.</p>
<p>Our research diverges from these existing studies by examining the effects of global prompt format modifications on model performance, offering insights that are applicable to a broad spectrum of LLM-based tasks that necessitate prompt engineering.The closest related work to ours is by (Sui et al., 2024), which however only provides a cursory exploration of format influence and is restricted to tabular data.To the best of our knowledge, our study is the first effort to systematically investigate the impact of global prompt format variations -an inescapable aspect of prompt engineering design decisions.</p>
<p>B Datasets</p>
<p>We evaluate six distinct benchmarks and classify them according to the nature of the task involved.</p>
<p>NL2NL</p>
<p>• Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020) covers 57 subjects including 20 STEM subjects, 13 humanities subjects, 12 social sciences subjects and 12 other subjects.Each subject contains at least 100 multiple choice questions, which tests both world knowledge and problem solving ability.We use the dev set which contains 5 questions per subjects as few-shot examples, and use test set containing 14,079 questions with different levels of difficulty to evaluation model performance.We use accuracy score to measure model performance.</p>
<p>• NER Finance: OpenAI Evals (OpenAI, 2023) is a framework containing a registry of evaluations to test LLMs where NER Finance is one of those.This dataset contains samples between one sentence to one paragraph long from financial documents.The task is to extract the all of the entities in the document, with the evaluation being if the LLM outputs each entity, in order.We randomly sample 500 examples from this dataset.</p>
<p>NL2Code</p>
<p>• HumanEval (Chen et al., 2021) is a benchmark dataset consisting of a collection of Python programming problems, each accompanied by a function signature, a docstring outlining the problem to be solved, and a set of unit tests that the correct implementation must pass.We use the evaluation metric pass@1, which checks if the the generated code passes the unit given unit tests in one attempt.We use all 164 samples in this dataset.</p>
<p>• FIND (Schwettmann et al., 2023): The Function Interpretation and Description (FIND) benchmark dataset is a natural language-tocode generation task.The LLM is given 5 example inputs and outputs to an unknown Python function and is tasked with reverse engineering the original Python code.We evaluate the benchmark by comparing the output of test cases on a ground truth function with the output from LLM generated functions.We use the "strings" category of functions for the task consisting of 500 functions.We provide the LLM with 5 pairs of input and output for each function.To select these examples, we randomly sample from a dataset provided by (Schwettmann et al., 2023) that contains example test strings for each function.To evaluate the generated function code, we use the string indicator metric introduced by (Schwettmann et al., 2023) that measures the number of test cases passed by the function.</p>
<p>Code2Code</p>
<p>• CODEXGLUE (Lu et al., 2021) stands for General Language Understanding Evaluation benchmark for CODE.It was originally introduced to address the lack of diversified benchmarks in code intelligence by providing a diverse set of tasks, including code translation.We use the parallel code for Java and C-Sharp and vice versa.We use the test set containing 1000 parallel code in Java and C-Sharp to experiment the capabilities of the LLMs in translating code from one programming language to another.The performance of the LLMs is evaluated using the BLEU (Papineni et al., 2002) score, which compares the generated code to the reference code.</p>
<p>• HumanEval-X (Zheng et al., 2023) dataset is a benchmark designed to evaluate the multilingual capabilities of code generative models.It contains 820 high-quality, human-crafted data samples, each accompanied by test cases.</p>
<p>The dataset supports a variety of programming languages, including Python, C++, Java, JavaScript, and Go.In this we experiment with one of the above dimension of codetranslation focusing on Java to Python.To accomplish this task, we combine the "declaration" and "canonical-solution" together to finally get the overall function in the respective language."Declaration" contains the function declaration for the respective language and "canonical solution" has the human-crafted example solution for the language.Similar to CODEXGLUE, we use the BLEU (Papineni et al., 2002) score for measuring the performance.</p>
<p>C Prompt Templates</p>
<p>In this section we provide examples of the four prompt templates we used for the NER Finance task.Prompts for all other tasks followed identical formatting.Variables that are injected into the prompt are denoted by blue text wrapped in braces.</p>
<p>For example a user ask being injected is denoted as {USER ASK}.</p>
<p>D Additional Research Questions</p>
<p>D.1 How does the format in which information is structured and presented influence the ability to solve problems that require different skill sets?</p>
<p>We analyze whether model's sensitivity to prompt format changes is related to the skills required to solve the task using the MMLU benchmark which comprises 57 subjects, categorized into four domains: humanities, social science, STEM, and others.Each domain encompasses various disciplines, necessitating distinct skill sets and knowledge for accurate question answering.</p>
<p>Figure 5 breaks down the performance on MMLU dataset by domain.We observe the per-formance spread exists across different tasks, and it's not signified nor eliminated by specific skills required.This suggests that the model's sensitivity to prompt formatting is a general characteristic, rather than being contingent on the specific skills or reasoning abilities required by different tasks.Performance is influenced by how information is presented to it, regardless of the complexity or na-</p>
<p>Markdown</p>
<p>System: ## Persona -You are a annotator working for large financial data company are tasked with extracting named entities from financial documents who follows strict guidelines for quality and formatting.## Instructions -You will be given a sentence from a financial document.-List the named entities in the order they appear in the sentence.</p>
<p>-If an entity appears multiple times, list it multiples times.</p>
<p>-Provide your chain of thought first and then respond with your final answer.Description: You are a annotator working for large financial data company are tasked with extracting named entities from financial documents who follows strict guidelines for quality and formatting.</p>
<p>Instructions:</p>
<p>-You will be given a sentence from a financial document.</p>
<p>-List the named entities in the order they appear in the sentence.</p>
<p>-If an entity appears multiple times, list it multiples times.</p>
<p>-Provide your chain of thought first and then respond with your final answer.</p>
<p>Output_Format:</p>
<p>Entities should be stated in the format NAME -TYPE where TYPE can be PERSON, ORGANIZATION, or LOCATION.State your final answer as a comma-separated list of entities enclosed in square brackets.</p>
<p>Examples:</p>
<ul>
<li>trained on more data than GPT-3.5, and is clearly the overall more capable model ((Achiam et al., 2023;Bubeck et al., 2023;Carlini et al., 2024)).In this section, we aim to ascertain whether an expansion in general capability translates to enhanced stability in response to changes in templates.The CMDs for all the models across benchmarks are presented in Figure 6.</li>
</ul>
<p>A lower value of CMD indicates more robustness to template variation.The results indicate that the GPT-4-1106-preview model exhibits superior robustness to format changes, maintaining a performance dispersion consistently below 0.036 across all benchmarks.In contrast, the GPT-4-32k-0613 model demonstrates less robustness relative to the GPT-4-1106-preview, yet it outperforms the GPT-3.5 series, with CMDs not exceeding 0.043.The GPT-3.5 series displays a broader range of CMDs, from 0.035 to 0.176, signifying a higher degree of performance variability under different prompt formats.GPT-4's observed improvements may be attributed to its enhanced ability to process data in diverse formats.Moreover, it is possible that the robustness of the model is not adversely impacted by format variations at the level of the last hidden layer of prompt embedding.Notably, the GPT-4-1106preview model achieves greater robustness compared to the GPT-4-32k-0613, corroborating existing evidence that suggests the former has a heightened proficiency in comprehending and generating content in specific formats as instructed (OpenAI, November 2023).Further examining GPT-4-32k-0613's performance, we notice the CMD on Hu-manEval benchmark is extremely high, this is due the extremely low score using JSON format, see Table 4 for results.Analyzing the model outputs, we find the poor performance is because most of the time the model would generate chain of thought in plain text, but did not continue with actually generating the code.The other models did not exhibit this behavior for the JSON template.We hypothesize that this may be related to the OpenAI's claim about fixing laziness in task completion in the 0125 version of GPT-4-turbo (OpenAI, 2024).In summary, larger models are more robust to template variation.</p>
<p>E Complete Results</p>
<p>E.1 Additional results on model performance under all templates across benchamrks.</p>
<p>E.2 IoU scores on all benchmarks.</p>
<p>E.3 Dotplot on all benchmark datasets</p>
<p>Figure 2: Consistency comparison for MMLU dataset: GPT-3.5 models show consistency scores below 0.5 across format pairs, whereas GPT-4 consistently exceeds 0.5, indicating greater reliability.</p>
<p>Figure 3 :
3
Figure 3: Intersection over Union (IoU) scores for top templates on the NER Finance benchmark across models.Higher IoU is observed within same-version model pairs, whereas cross-version pairs exhibit lower IoU.</p>
<h1></h1>
<h1>Output Format -Entities should be stated in the format NAME -TYPE where TYPE can be PERSON, ORGANIZATION, or LOCATION.-State your final answer as a comma-separated list of entities enclosed in square brackets.Example: [Bank -ORGANIZATION, Borrower -PERSON].-Ifthere are no entities found, state your final answer as 'No entities found'.</h1>
<p>Figure 6 :
6
Figure 6: Coefficient of mean deviation (CMD) of scalar metrics for all the prompt templates.Figure shows the CMDs across models and datasets.GPT-3.5 series exhibit larger CMD scores across benchmarks than GPT-4 series, indicating higher sensitivity to the choice of format.</p>
<p>Figure 7 :
7
Figure 7: Heatmap of IoU values for other benchmarks.</p>
<p>Figure 8 :
8
Figure 8: Performance of Consistency for FIND dataset across models.</p>
<p>Figure 9 :
9
Figure 9: Dotplot of model performance across prompt formats on all benchmarks.</p>
<p>Zhuosheng Zhang, Aston Zhang,Mu Li, and Alex  Smola.2022.Automatic chain of thought prompting in large language models.Preprint, arXiv:2210.03493.Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.2021.Calibrate before use: Improving few-shot performance of language models.
Preprint, arXiv:2102.09690.Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, ShanWang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang,Yang Li, Teng Su, Zhilin Yang, and Jie Tang. 2023.Codegeex: A pre-trained model for code genera-tion with multilingual evaluations on humaneval-x.Preprint, arXiv:2303.17568.A Related WorkPrompt Engineering The field of prompt engi-neering has garnered significant interest in recentyears, in parts due to the emergent capabilities ofthe most capable LLMs, while also trying to bet-ter control their still unpredictable outcomes. Aprominent strand of research within this domainconcentrates on innovative prompting methodolo-gies. These include few-shot prompting ((Brownet al., 2020)), which enables models to adapt tonew tasks without extensive retraining, and Chain-of-Thought prompting ((Wei et al., 2023)), both ofwhich are designed to enhance the reasoning capa-bilities of LLMs. Additionally, Automatic Chain-of-Thought (Auto-CoT) ((Zhang et al., 2022)) andSelf-Consistency ((Wang et al., 2023)) approacheshave been developed to further refine these reason-ing processes. To mitigate hallucinations in LLMoutputs, techniques such as Retrieval AugmentedGeneration (RAG) ((Lewis et al., 2021)) and Re-Act ((</p>
<p>Table 2 :
2
Prompt templates considered in this paper.Placeholders are denoted with {variable name} and get replaced with task specific context.You are a annotator working for large financial data company and are tasked with extracting named entities from financial documents who follows strict guidelines for quality and formatting.The following sentence is from a financial document.List the named entities in the order they appear in the sentence.If an entity appears multiple times, list it multiples times.Entities should be stated in the format NAME -TYPE where TYPE can be PERSON, ORGANIZATION, or LOCATION.State your final answer as a comma-separated list of entities enclosed in square brackets.Example: [Bank -ORGANIZATION, Borrower -PERSON].If there are no entities found, state your final answer as 'No entities found'.Provide your chain of thought first and then respond with your final answer.Here is an example: {ICL EXAMPLE INPUT} {ICL EXAMPLE SOLUTION}
Prompt FormatPrompt TemplatePlain text{persona} {instructions} {examples} {output format instructions} {user ask}## Persona{persona}## Instructions{instructions}Markdown## Examples {examples}## Output Format{Output format instructions}## User Question{user ask}Persona-{persona}Instructions-{instructions}YAMLExamples -{examples}Output format-{output format instructions}User question-{user ask}{"Persona": "{persona}","Instructions": "{instructions}","Examples": "{examples}",JSON"Output format": "{output format instructions}"}{"User ask": "{user ask}"}PlaintextSystem:User:{INPUT}
JSONSystem: { "Persona": "You are a annotator working for large financial data company are tasked with extracting named entities from financial documents who follows strict guidelines for quality and formatting.","Instructions": [ "You will be given a sentence from a financial document.","List the named entities in the order they appear in the sentence.sensitivity to prompt format.While the architectural details and exact size of GPT-4 are not published, it is assumed that GPT-4 contains significantly more parameters, was
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Retrieval-augmented generation for knowledgeintensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen Tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela, arXiv:2005.114012021Preprint</p>
<p>Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, Yin Tat, Lee , arXiv:2309.05463Textbooks are all you need ii: phi-1.5 technical report. 2023arXiv preprint</p>
<p>Lost in the middle: How language models use long contexts. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang, arXiv:2307.031722023Preprint</p>
<p>Codexglue: A machine learning benchmark dataset for code understanding and generation. Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, Dawn Drain, Daxin Jiang, Duyu Tang, arXiv:2102.046642021arXiv preprint</p>
<p>Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.087862022Preprint</p>
<p>Azure openai service models. Microsoft</p>
<p>Evals. 2023OpenAI</p>
<p>New embedding models and api updates. 2024OpenAI</p>
<p>Improved instruction following and json mode. Openai, November 2023</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, arXiv:2203.02155Jan Leike, and Ryan Lowe. 2022Preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>A systematic survey of prompt engineering in large language models: Techniques and applications. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha, arXiv:2402.079272024Preprint</p>
<p>Sarah Schwettmann, Tamar Rott Shaham, Joanna Materzynska, Neil Chowdhury, Shuang Li, Jacob Andreas, David Bau, Antonio Torralba, arXiv:2309.03886Find: A function description benchmark for evaluating interpretability methods. 2023Preprint</p>
<p>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr, arXiv:2310.113242023arXiv preprint</p>
<p>You don't need a personality test to know these models are unreliable: Assessing the reliability of large language models on psychometric instruments. Bangzhao Shu, Lechen Zhang, Minje Choi, Lavinia Dunagan, Dallas Card, David Jurgens, arXiv:2311.097182023arXiv preprint</p>
<p>Table meets llm: Can large language models understand structured table data? a benchmark and empirical study. Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, Dongmei Zhang, The 17th ACM International Conference on Web Search and Data Mining. 2024WSDM '24</p>
<p>Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, arXiv:2312.11805Gemini: a family of highly capable multimodal models. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Mind your format: Towards consistent evaluation of in-context learning improvements. Anton Voronov, Lena Wolf, Max Ryabinin, arXiv:2401.067662024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, Denny Zhou, arXiv:2203.111712023Preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.119032023Preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.036292023Preprint</p>            </div>
        </div>

    </div>
</body>
</html>
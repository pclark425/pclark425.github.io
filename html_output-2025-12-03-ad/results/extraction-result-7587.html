<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7587 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7587</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7587</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-a9fe5bd8da2d9603cf2cf6c6ea8b0f83c6d3a4f9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a9fe5bd8da2d9603cf2cf6c6ea8b0f83c6d3a4f9" target="_blank">How many data points is a prompt worth?</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> It is found that prompting does indeed provide a benefit, and that this benefit can be quantified per task, and results show that prompting is often worth 100s of data points on average across classification tasks.</p>
                <p><strong>Paper Abstract:</strong> When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7587.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7587.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt vs Head</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt-based (pattern + verbalizer) fine-tuning versus head-based classifier fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Comparison of adapting a pretrained LM by converting inputs to a cloze/prompt (pattern + verbalizer) and fine-tuning to predict a masked token versus attaching a generic classifier head on top of pretrained representations and fine-tuning it; evaluated across SuperGLUE tasks and MNLI to quantify how many training examples prompting is worth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>roberta-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large pretrained Transformer (Roberta) checkpoint used for fine-tuning; experiments load the standard roberta-large weights from Hugging Face.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>355M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MNLI and SuperGLUE tasks (BoolQ, CB, COPA, MultiRC, RTE, WiC, WSC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Varied classification and reasoning tasks: entailment (MNLI, RTE, CB), yes/no QA (BoolQ), multiple-choice commonsense (COPA), multi-sentence reading comprehension (MultiRC), word-in-context sense disambiguation (WiC), and Winograd-style coreference (WSC).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt/cloze (fill-in-the-blank masked token predicted by LM) versus classifier head (vector -> softmax over labels).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompt format decomposed into pattern (text with <MASK>) and verbalizer mapping predicted token(s) to classes (pattern-verbalizer pairs, PVPs). Multiple human-written prompts per task reused from Schick & Schütze; prompt for BoolQ examples: passage + 'Question: q? Answer: <MASK>'. Head format: generic classification head on top of pretrained representations. Fine-tuning hyperparameters: low LR 1e-5, at least 250 steps (often >100 epochs), up to full dataset; experiments run across exponentially increasing training-set sizes (from 10 up to full); 4 random seed runs per experimental point; reported curves use accumulated-maximum over runs unless otherwise stated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Per-task prompting advantage measured as an equivalent number of extra training examples (P vs H, average advantage ± std): MNLI: 3506 ± 536 data points; BoolQ: 752 ± 46; CB: 90 ± 2; COPA: 288 ± 242; MultiRC: 384 ± 378; RTE: 282 ± 34; WiC: -424 ± 74 (prompt underperforms); WSC: 281 ± 137. (These numbers quantify how many training examples the prompt is worth relative to head-based fine-tuning.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Head-based classifier fine-tuning (generic head on pretrained representations); baseline is the head curve used to compute the area-based advantage metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Expressed as an equivalent increase in training data for the head model (absolute): e.g., MNLI: +3506 ± 536 data points advantage for prompting; BoolQ: +752 ± 46 data points; WiC: -424 ± 74 (prompt worse by equivalent of 424 data points).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Pretrained roberta-large; fine-tuned at LR=1e-5 for >=250 steps; training sizes from 10 up to full dataset; 4 runs per data point; accumulated-maximum reporting across runs; bootstrapped confidence by holding out one of 4 runs combinations (16 combinations) to estimate std.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Reported as ± values (standard deviation across bootstrap of run-holdouts). No p-values reported; confidence intervals reported as ± (e.g., MNLI ±536).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How many data points is a prompt worth?', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7587.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7587.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Null-verbalizer prompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt with a non-informative (null) verbalizer (random token-class mapping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Control prompt format where the verbalizer mapping from predicted token to class is made semantically non-informative (e.g., replaced by random first names) to separate the contribution of the natural-language pattern from the semantic verbalizer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>roberta-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Roberta-large fine-tuned with prompts where the verbalizer is non-semantic (random tokens) so class meaning must be learned from supervised signal rather than lexically via verbalizer tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>355M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MNLI and SuperGLUE tasks (tasks that fit single-token verbalizers: MNLI, BoolQ, CB, MultiRC, RTE, WiC, ... excluding some multi-token tasks like COPA/WSC where control not applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as above but with verbalizer replaced by meaningless tokens to evaluate contribution of pattern language vs lexical verbalizer.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Cloze/prompt format with null verbalizer (masked token still predicted but mapping to labels is randomly assigned names).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt ablation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Null verbalizer replaces semantically meaningful verbalizer tokens ('yes','no','true','false') with random first names (or other non-informative tokens). Patterns unchanged. Comparison pairs reported: P (prompt with normal verbalizer) vs N (null-verbalizer), and N vs H (null-verbalizer vs head).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (and computed equivalent data-point advantage metric)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>P vs N average advantage (prompt with real verbalizer compared to null): MNLI: 150 ± 252 data points; BoolQ: 299 ± 81; CB: 78 ± 2; MultiRC: 74 ± 56; RTE: 404 ± 68; WiC: -354 ± 166. N vs H (null-verbalizer vs head) shows pattern-only inductive bias: MNLI: 3355 ± 612; BoolQ: 453 ± 90; CB: 12 ± 1; MultiRC: 309 ± 320; RTE: -122 ± 62; WiC: -70 ± 160. These indicate that for many tasks the pattern alone confers a large data advantage (N vs H), while the lexical verbalizer adds further gains in some tasks (P vs N).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Prompt with informative verbalizer (P) and head-based classifier (H) used as comparison baselines for P vs N and N vs H respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Reported as absolute equivalent data-point differences: e.g., P vs N on RTE: +404 ± 68 data points; N vs H on MNLI: +3355 ± 612 data points. Signs indicate direction (positive means prompt or null-verbalizer better than comparator).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same fine-tuning settings as main experiments; null-verbalizer applied only to tasks where single-token verbalizer is meaningful; 4 runs per data point; accumulated-maximum reporting; bootstrapped std reported.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Reported as ± values (std across bootstrap of run combinations); no formal hypothesis-test p-values given.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How many data points is a prompt worth?', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7587.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7587.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt design variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of different human-written prompt patterns (pattern choice) on fine-tuning performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Assessment of how varying the natural-language prompt pattern (different PVPs) affects model performance; finds that prompt choice is generally not a dominant hyperparameter compared to random-seed variance, with some task- and data-regime exceptions (e.g., specific few-shot advantage on BoolQ).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>roberta-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Roberta-large fine-tuned multiple times using different human-written patterns (PVPs) per task drawn from Schick & Schütze prompt sets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>355M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MultiRC, BoolQ, and other SuperGLUE tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same SuperGLUE tasks; here focus is on sensitivity to the particular prompt wording/pattern used to produce the masked token.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple alternative prompt patterns (PVPs) for the same underlying classification task; each pattern maps input to a cloze with masked token(s).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / pattern selection</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Multiple human-written prompts per task were evaluated; median performance across runs per prompt plotted. Observed substantial overlap in confidence intervals across different prompts; an exception: in low-data few-shot BoolQ, one prompt gave a noticeable few-shot advantage. Example patterns for MultiRC are listed in Appendix A. Reporting used median and per-prompt run distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (median and run-confidence intervals)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: for most tasks prompt-to-prompt differences were small and overlapped with run variance; for BoolQ few-shot one prompt had a statistically noticeable advantage in a few-shot regime (numerical few-shot advantage reported qualitatively in figure; no single scalar equivalence provided).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Comparison across different prompt patterns; baseline implicitly the median/typical prompt performance or seed-averaged performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Typically small and overshadowed by run variance; exception: specific BoolQ few-shot prompt provided non-negligible absolute gains in the few-shot regime (visualized in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Multiple PVPs reused from Schick & Schütze; plots show median across runs; 4 runs per data point; low-data regimes emphasized (e.g., 10–100 examples).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>No formal p-values; conclusions drawn from overlapping confidence intervals and visualization of median ± run variance; one strong few-shot exception noted for BoolQ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How many data points is a prompt worth?', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7587.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7587.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reporting method sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Influence of run-reporting/aggregation method on measured prompting advantage</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Demonstrates that the method used to aggregate across random-seed runs (accumulated-maximum, maximum, or mean reporting) affects the numerical estimate of how many data points a prompt is worth, altering reported advantage magnitudes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>roberta-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same roberta-large experiments aggregated under different reporting conventions to test sensitivity of the advantage metric to reporting choices.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>355M</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MNLI and SuperGLUE tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks as above; here focus on how aggregation across runs affects the computed 'average advantage' metric.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Aggregation/reporting format: accumulated-maximum across runs (primary), maximum, and mean performance at each data point.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>evaluation/reporting style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Three reporting styles compared: 'accumulated maximum' (primary used in paper: keep best performance attained at that amount of data or lower), 'maximum' (best at that point), and 'mean' (average across runs). The distribution of per-run outcomes is left-skewed and sometimes bimodal, so mean reporting can materially change conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>average advantage (equivalent data points) computed from curves; accuracy curves underlying the metric are aggregated differently per reporting method.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example differences for P vs H (MNLI): accumulated-maximum 3506 ± 536, maximum 3506 ± 536, mean 4526 ± 266 (note mean reporting inflates advantage in MNLI). Other tasks similarly show numeric shifts (e.g., BoolQ P vs H: accmax 752 ± 46, max 737 ± 53, mean 1139 ± 285).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Baseline remains head-based classifier; here baseline advantage numbers vary depending on aggregation method used for curves.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Aggregation choice changes reported advantage by large absolute amounts (hundreds to thousands of equivalent data points depending on task).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same experimental runs; three aggregation/presentation methods applied post-hoc to compute and report advantage; bootstrapped std reported per method.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td>Reported ± values per aggregation method (std across bootstrap/run combinations); no p-values. Authors note mean reporting can make results vary significantly due to skewed/bimodal run distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How many data points is a prompt worth?', 'publication_date_yy_mm': '2021-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploiting cloze questions for few-shot text classification and natural language inference <em>(Rating: 2)</em></li>
                <li>It's not just size that matters: Small language models are also few-shot learners <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Exploring the limits of transfer learning with a unified text-to-text transformer <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7587",
    "paper_id": "paper-a9fe5bd8da2d9603cf2cf6c6ea8b0f83c6d3a4f9",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Prompt vs Head",
            "name_full": "Prompt-based (pattern + verbalizer) fine-tuning versus head-based classifier fine-tuning",
            "brief_description": "Comparison of adapting a pretrained LM by converting inputs to a cloze/prompt (pattern + verbalizer) and fine-tuning to predict a masked token versus attaching a generic classifier head on top of pretrained representations and fine-tuning it; evaluated across SuperGLUE tasks and MNLI to quantify how many training examples prompting is worth.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "roberta-large",
            "model_description": "A large pretrained Transformer (Roberta) checkpoint used for fine-tuning; experiments load the standard roberta-large weights from Hugging Face.",
            "model_size": "355M",
            "task_name": "MNLI and SuperGLUE tasks (BoolQ, CB, COPA, MultiRC, RTE, WiC, WSC)",
            "task_description": "Varied classification and reasoning tasks: entailment (MNLI, RTE, CB), yes/no QA (BoolQ), multiple-choice commonsense (COPA), multi-sentence reading comprehension (MultiRC), word-in-context sense disambiguation (WiC), and Winograd-style coreference (WSC).",
            "problem_format": "Prompt/cloze (fill-in-the-blank masked token predicted by LM) versus classifier head (vector -&gt; softmax over labels).",
            "format_category": "prompt style / question type",
            "format_details": "Prompt format decomposed into pattern (text with &lt;MASK&gt;) and verbalizer mapping predicted token(s) to classes (pattern-verbalizer pairs, PVPs). Multiple human-written prompts per task reused from Schick & Schütze; prompt for BoolQ examples: passage + 'Question: q? Answer: &lt;MASK&gt;'. Head format: generic classification head on top of pretrained representations. Fine-tuning hyperparameters: low LR 1e-5, at least 250 steps (often &gt;100 epochs), up to full dataset; experiments run across exponentially increasing training-set sizes (from 10 up to full); 4 random seed runs per experimental point; reported curves use accumulated-maximum over runs unless otherwise stated.",
            "performance_metric": "accuracy",
            "performance_value": "Per-task prompting advantage measured as an equivalent number of extra training examples (P vs H, average advantage ± std): MNLI: 3506 ± 536 data points; BoolQ: 752 ± 46; CB: 90 ± 2; COPA: 288 ± 242; MultiRC: 384 ± 378; RTE: 282 ± 34; WiC: -424 ± 74 (prompt underperforms); WSC: 281 ± 137. (These numbers quantify how many training examples the prompt is worth relative to head-based fine-tuning.)",
            "baseline_performance": "Head-based classifier fine-tuning (generic head on pretrained representations); baseline is the head curve used to compute the area-based advantage metric.",
            "performance_change": "Expressed as an equivalent increase in training data for the head model (absolute): e.g., MNLI: +3506 ± 536 data points advantage for prompting; BoolQ: +752 ± 46 data points; WiC: -424 ± 74 (prompt worse by equivalent of 424 data points).",
            "experimental_setting": "Pretrained roberta-large; fine-tuned at LR=1e-5 for &gt;=250 steps; training sizes from 10 up to full dataset; 4 runs per data point; accumulated-maximum reporting across runs; bootstrapped confidence by holding out one of 4 runs combinations (16 combinations) to estimate std.",
            "statistical_significance": "Reported as ± values (standard deviation across bootstrap of run-holdouts). No p-values reported; confidence intervals reported as ± (e.g., MNLI ±536).",
            "uuid": "e7587.0",
            "source_info": {
                "paper_title": "How many data points is a prompt worth?",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Null-verbalizer prompt",
            "name_full": "Prompt with a non-informative (null) verbalizer (random token-class mapping)",
            "brief_description": "Control prompt format where the verbalizer mapping from predicted token to class is made semantically non-informative (e.g., replaced by random first names) to separate the contribution of the natural-language pattern from the semantic verbalizer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "roberta-large",
            "model_description": "Roberta-large fine-tuned with prompts where the verbalizer is non-semantic (random tokens) so class meaning must be learned from supervised signal rather than lexically via verbalizer tokens.",
            "model_size": "355M",
            "task_name": "MNLI and SuperGLUE tasks (tasks that fit single-token verbalizers: MNLI, BoolQ, CB, MultiRC, RTE, WiC, ... excluding some multi-token tasks like COPA/WSC where control not applicable)",
            "task_description": "Same tasks as above but with verbalizer replaced by meaningless tokens to evaluate contribution of pattern language vs lexical verbalizer.",
            "problem_format": "Cloze/prompt format with null verbalizer (masked token still predicted but mapping to labels is randomly assigned names).",
            "format_category": "prompt style / prompt ablation",
            "format_details": "Null verbalizer replaces semantically meaningful verbalizer tokens ('yes','no','true','false') with random first names (or other non-informative tokens). Patterns unchanged. Comparison pairs reported: P (prompt with normal verbalizer) vs N (null-verbalizer), and N vs H (null-verbalizer vs head).",
            "performance_metric": "accuracy (and computed equivalent data-point advantage metric)",
            "performance_value": "P vs N average advantage (prompt with real verbalizer compared to null): MNLI: 150 ± 252 data points; BoolQ: 299 ± 81; CB: 78 ± 2; MultiRC: 74 ± 56; RTE: 404 ± 68; WiC: -354 ± 166. N vs H (null-verbalizer vs head) shows pattern-only inductive bias: MNLI: 3355 ± 612; BoolQ: 453 ± 90; CB: 12 ± 1; MultiRC: 309 ± 320; RTE: -122 ± 62; WiC: -70 ± 160. These indicate that for many tasks the pattern alone confers a large data advantage (N vs H), while the lexical verbalizer adds further gains in some tasks (P vs N).",
            "baseline_performance": "Prompt with informative verbalizer (P) and head-based classifier (H) used as comparison baselines for P vs N and N vs H respectively.",
            "performance_change": "Reported as absolute equivalent data-point differences: e.g., P vs N on RTE: +404 ± 68 data points; N vs H on MNLI: +3355 ± 612 data points. Signs indicate direction (positive means prompt or null-verbalizer better than comparator).",
            "experimental_setting": "Same fine-tuning settings as main experiments; null-verbalizer applied only to tasks where single-token verbalizer is meaningful; 4 runs per data point; accumulated-maximum reporting; bootstrapped std reported.",
            "statistical_significance": "Reported as ± values (std across bootstrap of run combinations); no formal hypothesis-test p-values given.",
            "uuid": "e7587.1",
            "source_info": {
                "paper_title": "How many data points is a prompt worth?",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Prompt design variability",
            "name_full": "Effect of different human-written prompt patterns (pattern choice) on fine-tuning performance",
            "brief_description": "Assessment of how varying the natural-language prompt pattern (different PVPs) affects model performance; finds that prompt choice is generally not a dominant hyperparameter compared to random-seed variance, with some task- and data-regime exceptions (e.g., specific few-shot advantage on BoolQ).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "roberta-large",
            "model_description": "Roberta-large fine-tuned multiple times using different human-written patterns (PVPs) per task drawn from Schick & Schütze prompt sets.",
            "model_size": "355M",
            "task_name": "MultiRC, BoolQ, and other SuperGLUE tasks",
            "task_description": "Same SuperGLUE tasks; here focus is on sensitivity to the particular prompt wording/pattern used to produce the masked token.",
            "problem_format": "Multiple alternative prompt patterns (PVPs) for the same underlying classification task; each pattern maps input to a cloze with masked token(s).",
            "format_category": "prompt style / pattern selection",
            "format_details": "Multiple human-written prompts per task were evaluated; median performance across runs per prompt plotted. Observed substantial overlap in confidence intervals across different prompts; an exception: in low-data few-shot BoolQ, one prompt gave a noticeable few-shot advantage. Example patterns for MultiRC are listed in Appendix A. Reporting used median and per-prompt run distributions.",
            "performance_metric": "accuracy (median and run-confidence intervals)",
            "performance_value": "Qualitative: for most tasks prompt-to-prompt differences were small and overlapped with run variance; for BoolQ few-shot one prompt had a statistically noticeable advantage in a few-shot regime (numerical few-shot advantage reported qualitatively in figure; no single scalar equivalence provided).",
            "baseline_performance": "Comparison across different prompt patterns; baseline implicitly the median/typical prompt performance or seed-averaged performance.",
            "performance_change": "Typically small and overshadowed by run variance; exception: specific BoolQ few-shot prompt provided non-negligible absolute gains in the few-shot regime (visualized in figures).",
            "experimental_setting": "Multiple PVPs reused from Schick & Schütze; plots show median across runs; 4 runs per data point; low-data regimes emphasized (e.g., 10–100 examples).",
            "statistical_significance": "No formal p-values; conclusions drawn from overlapping confidence intervals and visualization of median ± run variance; one strong few-shot exception noted for BoolQ.",
            "uuid": "e7587.2",
            "source_info": {
                "paper_title": "How many data points is a prompt worth?",
                "publication_date_yy_mm": "2021-03"
            }
        },
        {
            "name_short": "Reporting method sensitivity",
            "name_full": "Influence of run-reporting/aggregation method on measured prompting advantage",
            "brief_description": "Demonstrates that the method used to aggregate across random-seed runs (accumulated-maximum, maximum, or mean reporting) affects the numerical estimate of how many data points a prompt is worth, altering reported advantage magnitudes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "roberta-large",
            "model_description": "Same roberta-large experiments aggregated under different reporting conventions to test sensitivity of the advantage metric to reporting choices.",
            "model_size": "355M",
            "task_name": "MNLI and SuperGLUE tasks",
            "task_description": "Tasks as above; here focus on how aggregation across runs affects the computed 'average advantage' metric.",
            "problem_format": "Aggregation/reporting format: accumulated-maximum across runs (primary), maximum, and mean performance at each data point.",
            "format_category": "evaluation/reporting style",
            "format_details": "Three reporting styles compared: 'accumulated maximum' (primary used in paper: keep best performance attained at that amount of data or lower), 'maximum' (best at that point), and 'mean' (average across runs). The distribution of per-run outcomes is left-skewed and sometimes bimodal, so mean reporting can materially change conclusions.",
            "performance_metric": "average advantage (equivalent data points) computed from curves; accuracy curves underlying the metric are aggregated differently per reporting method.",
            "performance_value": "Example differences for P vs H (MNLI): accumulated-maximum 3506 ± 536, maximum 3506 ± 536, mean 4526 ± 266 (note mean reporting inflates advantage in MNLI). Other tasks similarly show numeric shifts (e.g., BoolQ P vs H: accmax 752 ± 46, max 737 ± 53, mean 1139 ± 285).",
            "baseline_performance": "Baseline remains head-based classifier; here baseline advantage numbers vary depending on aggregation method used for curves.",
            "performance_change": "Aggregation choice changes reported advantage by large absolute amounts (hundreds to thousands of equivalent data points depending on task).",
            "experimental_setting": "Same experimental runs; three aggregation/presentation methods applied post-hoc to compute and report advantage; bootstrapped std reported per method.",
            "statistical_significance": "Reported ± values per aggregation method (std across bootstrap/run combinations); no p-values. Authors note mean reporting can make results vary significantly due to skewed/bimodal run distributions.",
            "uuid": "e7587.3",
            "source_info": {
                "paper_title": "How many data points is a prompt worth?",
                "publication_date_yy_mm": "2021-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploiting cloze questions for few-shot text classification and natural language inference",
            "rating": 2
        },
        {
            "paper_title": "It's not just size that matters: Small language models are also few-shot learners",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "rating": 1
        }
    ],
    "cost": 0.0116735,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>How Many Data Points is a Prompt Worth?</h1>
<p>Teven Le Scao<br>Hugging Face<br>teven@huggingface.co</p>
<h4>Abstract</h4>
<p>When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting taskspecific guidance, which is beneficial in lowdata regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.</p>
<h2>1 Introduction</h2>
<p>The main paradigm for adapting pretrained models for classification (Radford, 2018; Dong et al., 2019; Devlin et al., 2018) is fine-tuning via an explicit classifier head. However, an alternative approach has arisen: adapting the pretrained language model directly as a predictor through autoregressive text generation (Radford et al., 2019) or completion of a cloze task (Trinh and Le, 2018). This method is notably used in T5 fine-tuning (Raffel et al., 2019) leading to state-of-the-art results on the SuperGLUE benchmark (Wang et al., 2019).</p>
<p>One argument made for classification by direct language generation is that it allows us to pick custom prompts for each task (McCann et al., 2018). While this approach can be used for zero-shot classification (Puri and Catanzaro, 2019) or priming (Brown et al., 2020), it can also be used in fine-tuning to provide extra task information to the classifier, especially in the low-data regime (Schick and Schütze, 2020a,b).</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Alexander M. Rush</h2>
<p>Hugging Face
sasha@huggingface.co</p>
<p>If this argument is indeed true, it is natural to ask how it impacts the sample efficiency of the model, or more directly, how many data points is a prompt worth? As with many low-data and pretrainingbased problems, this question is complicated by the fine-tuning setup, training procedure, and prompts themselves. We attempt to isolate these variables through diverse prompts, multiple runs, and best practices in low-training data fine-tuning. We introduce a metric, the average data advantage, for quantifying the impact of a prompt in practice.</p>
<p>Our experiments find that the impact of tasktargeted prompting can nicely be quantified in terms of direct training data, and that it varies over the nature of different tasks. On MNLI (Williams et al., 2018), we find that using a prompt contributes approximately 3500 data points. On SuperGLUE, it adds approximately 280 data points on RTE (Dagan et al., 2005) and up to 750 on BoolQ (Clark et al., 2019). In low- to medium-data settings, this advantage can be a real contribution to training a model.</p>
<h2>2 Related Work</h2>
<p>Prompting has been used both for zero-shot and fine-tuning based methods. Zero-shot approaches attempt to answer a task with a prompt without finetuning through generation (Radford et al., 2019). GPT3 (Brown et al., 2020) extends this approach to a supervised priming method by taking in training data as priming at inference time, so it can attend to them while answering. T5 (Raffel et al., 2019) and other sequence-to-sequence pretrained models use standard word-based fine-tuning with a marker prompt to answer classification tasks with strong empirical success. Our setting differs in that we are interested in using task-based prompts and finetuning, in-between the T5 and GPT2 setting.</p>
<p>Our setting most closely resembles PET (Schick and Schütze, 2020a,b), which claims that taskspecific prompting helps transfer learning, espe-</p>
<p>cially in the low-data regime. However, in order to reach the best possible results on SuperGLUE, PET introduces several other extensions: semisupervision via additional pseudo-labeled data, ensembling models trained with several different prompts, and finally distilling the ensemble into a linear classifier rather than a language model. Our aim is to isolate the specific contributions of prompting within supervised fine-tuning.</p>
<p>Finally, recent papers have experimented with discovering prompts through automated processes tailored to the language model (Jiang et al., 2020; Schick et al., 2020). We limit ourselves to humanwritten prompts, as we are interested into whether prompting itself specifically adds information to the supervised task. It is an interesting question as to whether automatic prompts can have this same impact (relative to the training data they require).</p>
<h2>3 Comparison: Heads vs Prompts</h2>
<p>Consider two transfer learning settings for text classification: head-based, where a generic head layer takes in pretrained representations to predict an output class; prompt-based, where a task-specific pattern string is designed to coax the model into producing a textual output corresponding to a given class. Both can be utilized for fine-tuning with supervised training data, but prompts further allow the user to customize patterns to help the model.</p>
<p>For the prompt model we follow the notation from PET (Schick and Schütze, 2020a) and decompose a prompt into a pattern and a verbalizer. The pattern turns the input text into a cloze task, i.e. a sequence with a masked token or tokens that need to be filled. Let us use as example an excerpt from SuperGLUE task BoolQ (Clark et al., 2019), in which the model must answer yes-or-no binary questions. In order to let a language model answer the question in italics, our pattern is in bold (Schick and Schütze, 2020b):
"Posthumous marriage - Posthumous marriage (or necrogamy) is a marriage in which one of the participating members is deceased. It is legal in France and similar forms are practiced in Sudan and China. Since World War I, France has had hundreds of requests each year, of which many have been accepted. Based on the previous passage, can u marry a dead person in france ? <MASK>"</p>
<p>The masked word prediction is mapped to a verbalizer which produces a class. (here "Yes":</p>
<p>True. "No": False ${ }^{1}$ ). Several pattern-verbalizer pairs (PVPs) could be used for a single task, differing either through the pattern, the verbalizer, or both. Fine-tuning is done by training the model to produce the correct verbalization. The loss is the cross-entropy loss between the correct answer and the distribution of probabilities amongst the tokens in the verbalizer. We re-use pattern choices from Schick and Schütze (2020b); examples are available in Appendix A.</p>
<h2>4 Experimental Setting</h2>
<p>We run all experiments with the same pretrained checkpoint, roberta-large ( 355 M parameters) from RoBERTa (Liu et al., 2019), which we load from the transformers (Wolf et al., 2020) library. ${ }^{2}$ In line with previous observations (McCoy et al., 2019; Dodge et al., 2020; Lee et al., 2020), head-based fine-tuning performance varies considerably. We follow recommendations of Mosbach et al. (2020) and Zhang et al. (2020) to train at a low learning rate $\left(10^{-5}\right)$ for a large number of steps (always at least 250 , possibly for over 100 epochs).</p>
<p>We perform our evaluation on SuperGLUE and MNLI (Williams et al., 2018). These datasets comprise a variety of tasks, all in English, including entailment (MNLI, RTE (Dagan et al., 2005), CB (de Marneffe et al., 2019)), multiple choice question answering (BoolQ (Clark et al., 2019), MultiRC (Khashabi et al., 2018)), and commonsense reasoning (WSC (Levesque et al., 2012), COPA (Roemmele et al., 2011), WiC (Pilehvar and Camacho-Collados, 2018)). We do not include ReCoRD (Zhang et al., 2018) in our comparisons as there is no head model to compare with, since it is already a cloze task. Data sizes range from 250 data points for CB to 392, 702 for MNLI. As test data is not publicly available for SuperGLUE tasks, we set aside part of training (from 50 for CB, COPA and MultiRC to 500 for BoolQ) to use for development, and evaluate on their original validation sets. For MNLI, we use the available matched validation and test sets.</p>
<p>We compare models across a scale of available data, starting with 10 data points and increasing exponentially (as high-data performance tends to</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Prompting vs head (classifier) performance across data scales, up to the full dataset, for six SuperGLUE tasks. Compares the best prompt and head performance at each level of training data across 4 runs. Highlighted region shows the accuracy difference of the models. Cross-hatch region highlights the lowest- and highest- accuracy matched region in the curves. The highlighted area in this region is used to estimate the data advantage.</p>
<p>saturate) to the full dataset. For example, for MultiRC, which has 969 data points initially, we start by reserving 50 data points for development. This leaves us with 919 training points, and we train models with 10, 15, 20, 32, 50, 70, 100, 150, 200, 320, 500, 750, and 919 training points. We run every experiment 4 times in order to reduce variance, for a total of 1892 training runs across all tasks. At every point, we report the best performance that has been achieved at that amount of data or lower. Full graphs are presented in Appendix B.</p>
<h1>5 Results</h1>
<p>Figure 1 shows the main results comparing head- and prompt-based fine-tuning with the best-performing pattern on that task. Prompting enjoys a substantial advantage on every task, except for WiC as is reported in previous results (Schick and Schütze, 2020b). Both approaches improve with more training data, but prompting remains better by a varying amount. Many tasks in SuperGLUE have relatively few data points, but we also see an advantage in large datasets like BoolQ and MNLI.</p>
<p>To quantify how many data points the prompt is worth, we first isolate the y-axis band of the lowest- and highest- accuracy where the two curves match in accuracy. The horizontal line at these points represents the advantage of prompting. We then take the integral in this region, i.e. area between the linearly-interpolated curves, divided by the height of the band. The area has the dimension of a quantity of data points times the metric unit, so dividing by the performance range yields a # of data points advantage. As low data training is sensitive to noise, in addition to following best training practices we run several different experiments for each x-point. We use a bootstrapping approach to estimate confidence over these runs. Specifically, we hold out one of the 4 head runs and 4 prompt runs (16 combinations total), and compute the standard deviation of those outcomes.</p>
<p>We report these quantities for every task in Table 1 as <em>Average advantage</em>. For almost all the tasks, we see that prompting gives a substantial advantage in terms of data efficiency, adding the equivalent of hundreds of data points on average.</p>
<h1>6 Analysis</h1>
<p><strong>Impact of Pattern vs Verbalizer</strong> The intuition of prompts is that they introduce a task description in natural language, even with few training points. To better understand the zero-shot versus adaptive nature of prompts, we consider a <em>null verbalizer</em>, a control with a verbalizer that cannot yield semantic information without training. For every task that requires filling in one word (which excludes</p>
<p><sup>3</sup>We assume asymptotically the two curves would match, but are limited by data.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Average Advantage (# Training Points)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">MNLI</td>
<td style="text-align: center;">BoolQ</td>
<td style="text-align: center;">CB</td>
<td style="text-align: center;">COPA</td>
<td style="text-align: center;">MultiRC*</td>
<td style="text-align: center;">RTE</td>
<td style="text-align: center;">WiC</td>
<td style="text-align: center;">WSC</td>
</tr>
<tr>
<td style="text-align: left;">$P$ vs $H$</td>
<td style="text-align: center;">$3506 \pm 536$</td>
<td style="text-align: center;">$752 \pm 46$</td>
<td style="text-align: center;">$90 \pm 2$</td>
<td style="text-align: center;">$288 \pm 242$</td>
<td style="text-align: center;">$384 \pm 378$</td>
<td style="text-align: center;">$282 \pm 34$</td>
<td style="text-align: center;">$-424 \pm 74$</td>
<td style="text-align: center;">$281 \pm 137$</td>
</tr>
<tr>
<td style="text-align: left;">$P$ vs $N$</td>
<td style="text-align: center;">$150 \pm 252$</td>
<td style="text-align: center;">$299 \pm 81$</td>
<td style="text-align: center;">$78 \pm 2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$74 \pm 56$</td>
<td style="text-align: center;">$404 \pm 68$</td>
<td style="text-align: center;">$-354 \pm 166$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">$N$ vs $H$</td>
<td style="text-align: center;">$3355 \pm 612$</td>
<td style="text-align: center;">$453 \pm 90$</td>
<td style="text-align: center;">$12 \pm 1$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$309 \pm 320$</td>
<td style="text-align: center;">$-122 \pm 62$</td>
<td style="text-align: center;">$-70 \pm 160$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 1: Average prompting advantage in number of data points for MNLI \&amp; SuperGLUE tasks. $P$ denotes the prompt model, $H$ the head model. On average across performance levels, an MNLI prompt model yields the results of an MNLI head model trained with 3500 additional data points. Confidence levels are based on a multiple random runs (see text). $N$ indicates a null-verbalizer prompting task that replaces the verbalizer with a non-sensical mapping. *The comparison band of MultiRC is too small as the head baseline fails to learn beyond majority class; we use the full region for a lower-bound result.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Comparison of full prompt and null verbalizer advantage on MNLI at lower data scales.
the more free-form COPA and WSC), we replace the verbalizers, for example, "yes", "no", "maybe", "right" or "wrong", with random first names.</p>
<p>Table 1 shows the advantage of the standard prompt over the null verbalizer to estimate this control. We see that for small data tasks such as CB, the null verbalizer removes much of the benefits of prompting. However, with more training data, the model seems to adapt the verbalizer while still gaining the inductive bias benefits of the pattern. Figure 2 showcases this dynamic on MNLI. This result further shows that prompting yields data efficiency even if it is not directly analogous to the generation process of training.</p>
<p>Impact of Different Prompts If the prompt acts as a description of the task, one would expect different valid descriptions to vary in their benefits. In order to compare the different prompts we used on each task, we chart the median performance for each of them under different runs. In nearly every experiment, we find that the confidence intervals of those curves largely overlap, implying that prompt choice is not a dominant hyperparameter, i.e. the variance across random seeds usually outweighs the possible benefits of prompt choice. One ex-
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Median performance on MultiRC across runs for three prompts. Differences are inconsistent and eclipsed by the variance within one prompt's runs.
ception is the low-data regime of BoolQ, where one of the prompts enjoys a significant few-shot advantage over the others. We plot this curve for MultiRC in Figure 3 and the rest in Appendix C.</p>
<p>Metric sensitivity We treat each metric linearly in calculating advantage; alternatively, we could reparameterize the $y$ axis for each task. This choice does not have a consistent effect for or against prompting. For example, emphasizing gains close to convergence increases prompting advantage on CB and MNLI but decreases it on COPA or BoolQ.</p>
<h2>7 Conclusion</h2>
<p>We investigate prompting through a systematic study of its data advantage. Across tasks, prompting consistently yields a varying improvement throughout the training process. Analysis shows that prompting is mostly robust to pattern choice, and can even learn without an informative verbalizer. On large datasets, prompting is similarly helpful in terms of data points, although they are less beneficial in performance. In future work, we hope to study the mechanism and training dynamics of the prompting benefits.</p>
<h2>8 Impact statement</h2>
<p>Significant compute resources were used to run this paper's experiments. A single experiment (defined as one model run, at one data level, on one task) was quite light-weight, taking usually a little under an hour on a single Nvidia V100. However, as we computed a little under two thousand runs, this adds up to about 1800 GPU hours, to which one must add around 400 GPU hours of prototyping and hyper-parameter searching. Those 2200 GPU hours would usually have necessitated the release of about 400 kg of CO 2 , about $40 \%$ of a transatlantic flight for a single passenger, in the country where we ran the experiments, although we used a carbon-neutral cloud compute provider.</p>
<p>The main benefit of prompting, rather than compute efficiency, is data efficiency. Although we ran all of our experiments on English, we hope that this property will be especially helpful in low-resource language applications. In a sense, a practitioner could then remedy the lack of task-specific data in their language by introducing information through a prompt. However, this comes with the inherent risk of introducing human biases into the model. Prompt completion also suffers from biases already present within the language model (Sheng et al., 2019). This could cause a prompted model to repeat those biases in classification, especially in the few-shot setting where prompting mostly relies on the pretrained model.</p>
<h2>9 Acknowledgments</h2>
<p>We thank Steven Cao and Joe Davison for the discussions about prompting that initially spurred this paper. We further thank Timo Schick for making the code for PET available and for discussions about performance replication. We lastly thank Canwen Xu, Yacine Jernite, Victor Sanh, Dimitri Lozeve and Antoine Ogier for their help with the figures and writing of this draft.</p>
<h2>References</h2>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario</p>
<p>Amodei. 2020. Language models are few-shot learners.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. CoRR, abs/1905.10044.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. pages 177-190.</p>
<p>Marie-Catherine de Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. Proceedings of Sins und Bedeutung, 23(2):107-124.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805.</p>
<p>Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stopping.</p>
<p>Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation.</p>
<p>Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know?</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 252-262. Association for Computational Linguistics.</p>
<p>Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. ALBERT: A lite BERT for selfsupervised learning of language representations. CoRR, abs/1909.11942.</p>
<p>Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang. 2020. Mixout: Effective regularization to finetune large-scale pretrained language models.</p>
<p>Hector J. Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Principles of Knowledge Representation and Reasoning: Proceedings of the Thirteenth International Conference, KR 2012, Rome, Italy, June 10-14, 2012. AAAI Press.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.</p>
<p>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language decathlon: Multitask learning as question answering.
R. Thomas McCoy, Junghyun Min, and Tal Linzen. 2019. Berts of a feather do not generalize together: Large variability in generalization across models with similar test set performance.</p>
<p>Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. 2020. On the stability of fine-tuning bert: Misconceptions, explanations, and strong baselines.</p>
<p>Mohammad Taher Pilehvar and José CamachoCollados. 2018. Wic: 10, 000 example pairs for evaluating context-sensitive representations. CoRR, abs/1808.09121.</p>
<p>Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative language models. CoRR, abs/1912.10165.</p>
<p>Alec Radford. 2018. Improving language understanding by generative pre-training.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer.</p>
<p>Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S. Gordon. 2011. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical Report SS-11-06, Stanford, California, USA, March 21-23, 2011. AAAI.</p>
<p>Timo Schick, Helmut Schmid, and Hinrich Schütze. 2020. Automatically identifying words that can serve as labels for few-shot text classification.</p>
<p>Timo Schick and Hinrich Schütze. 2020a. Exploiting cloze questions for few shot text classification and natural language inference.</p>
<p>Timo Schick and Hinrich Schütze. 2020b. It's not just size that matters: Small language models are also few-shot learners.</p>
<p>Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. CoRR, abs/1909.01326.</p>
<p>Trieu H. Trinh and Quoc V. Le. 2018. A simple method for commonsense reasoning.</p>
<p>Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. CoRR, abs/1905.00537.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122. Association for Computational Linguistics.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Huggingface's transformers: State-of-the-art natural language processing.</p>
<p>Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and machine commonsense reading comprehension.</p>
<p>Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, and Yoav Artzi. 2020. Revisiting fewsample bert fine-tuning.</p>
<h2>A Choice of prompts</h2>
<p>We use a subset of prompts from (Schick and Schütze, 2020b).</p>
<ul>
<li>For entailment tasks (CB, MNLI, RTE) given a premise $p$ and hypothesis $h$ :
h? I <MASK>, p
"h"? I <MASK>. "p"
with "yes" as a verbalizer for entailment, "no" for contradiction, "maybe" for neutrality.</li>
<li>For BoolQ, given a passage $p$ and question $q$ :
$p$. Question: $q$ ? Answer: <MASK>.
$p$. Based on the previous passage, $q$ ? $&lt;$ MASK&gt;.</li>
</ul>
<p>Based on the following passage, $q$ ? <MASK>.
$p$
with "yes" or "no" as verbalizers for True and False.</p>
<ul>
<li>For COPA, given an effect $e$ and possible causes $c_{1}$ and $c_{2}$ :
" $c_{1}$ " or " $c_{2}$ "? $e$, so <MASK> .
$c_{1}$ or $c_{2}$ ? $e$, so $&lt;$ MASK $&gt;$.
and a cause $c$ and possible effects $e_{1}$ and $e_{2}$ :
" $e_{1}$ " or " $e_{2}$ "? <MASK>, because $c$.
$e_{1}$ or $e_{2}$ ? <MASK>, because $c$.
and the verbalizer is the identity function.</li>
<li>For MultiRC, given a passage $p$, question $q$ and answer $a$, we estimate whether $a$ is a proper answer with:
$p$. Question: $q$ ? Is it $a$ ? <MASK>.
$p$. Based on the previous passage, $q$ ? Is the correct answer $a$ ? <MASK>.
$p$. Question: $q$ ? Is the correct answer $a$ ? $&lt;$ MASK $&gt;$.</li>
<li>For WiC, given two sentences $s_{1}$ and $s_{2}$ and a word $w$, we classify whether $w$ was used in the same sense.
" $s_{1}$ " / " $s_{2}$ ". Similar sense of " $w$ "? <MASK>.
$s_{1} s_{2}$ Does $w$ have the same meaning in both sentences? <MASK>.</li>
</ul>
<p>With "yes" and "no" as verbalizers.</p>
<ul>
<li>For WSC, given a sentence $s$ with a marked pronoun * $p$ * and noun $n$ :
$s$ The pronoun '<em> $p$ </em>' refers to <MASK>.
$s$ In the previous sentence, the pronoun '<em> $p$ </em>' refers to <MASK>.
$s$ In the passage above, what does the pronoun'<em> $p$ </em>' refer to? Answer: <MASK>.</li>
</ul>
<p>With the identity function as a verbalizer.</p>
<h2>B Influence of the reporting method over runs</h2>
<p>We chose to report the accumulated maximum performance across runs for every model. This means that if the maximum performance over random seeds is smaller than a maximum previously attained with less data points, we use the previous value. This appendix presents results with the maximum and mean at every point to condense several runs instead. Using either maximum is equivalent; using the mean, however, can make results vary significantly, as the distribution of outcomes is heavily left-skewed, or even bimodal, with poor-performance outliers.</p>
<p>Average Advantage (accumulated maximum reporting)</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MNLI</th>
<th style="text-align: center;">BoolQ</th>
<th style="text-align: center;">CB</th>
<th style="text-align: center;">COPA</th>
<th style="text-align: center;">MultiRC*</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">WiC</th>
<th style="text-align: center;">WSC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$P v s H$</td>
<td style="text-align: center;">$3506 \pm 536$</td>
<td style="text-align: center;">$752 \pm 46$</td>
<td style="text-align: center;">$90 \pm 2$</td>
<td style="text-align: center;">$288 \pm 242$</td>
<td style="text-align: center;">$384 \pm 378$</td>
<td style="text-align: center;">$282 \pm 34$</td>
<td style="text-align: center;">$-424 \pm 74$</td>
<td style="text-align: center;">$281 \pm 137$</td>
</tr>
<tr>
<td style="text-align: center;">$P v s N$</td>
<td style="text-align: center;">$150 \pm 252$</td>
<td style="text-align: center;">$299 \pm 81$</td>
<td style="text-align: center;">$78 \pm 2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$74 \pm 56$</td>
<td style="text-align: center;">$404 \pm 68$</td>
<td style="text-align: center;">$-354 \pm 166$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$N v s H$</td>
<td style="text-align: center;">$3355 \pm 612$</td>
<td style="text-align: center;">$453 \pm 90$</td>
<td style="text-align: center;">$12 \pm 1$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$309 \pm 320$</td>
<td style="text-align: center;">$-122 \pm 62$</td>
<td style="text-align: center;">$-70 \pm 160$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 2: Average prompting advantage in number of data points for MNLI \&amp; SuperGLUE tasks with accmax reporting. $P$ denotes the prompt model, $H$ the head model. On average across performance levels, an MNLI prompt model yields the results of an MNLI head model trained with 3500 additional data points. Confidence levels are based on a multiple random runs (see text). $N$ indicates a null-verbalizer prompting task that replaces the verbalizer with a non-sensical mapping. *The comparison band of MultiRC is too small as the head baseline fails to learn beyond majority class; we use the full region for a lower-bound result.</p>
<p>Average Advantage (maximum reporting)</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MNLI</th>
<th style="text-align: center;">BoolQ</th>
<th style="text-align: center;">CB</th>
<th style="text-align: center;">COPA</th>
<th style="text-align: center;">MultiRC*</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">WiC</th>
<th style="text-align: center;">WSC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$P v s H$</td>
<td style="text-align: center;">$3506 \pm 536$</td>
<td style="text-align: center;">$737 \pm 53$</td>
<td style="text-align: center;">$86 \pm 1$</td>
<td style="text-align: center;">$226 \pm 189$</td>
<td style="text-align: center;">$448 \pm 314$</td>
<td style="text-align: center;">$218 \pm 24$</td>
<td style="text-align: center;">$-133 \pm 65$</td>
<td style="text-align: center;">$297 \pm 448$</td>
</tr>
<tr>
<td style="text-align: center;">$P v s N$</td>
<td style="text-align: center;">$150 \pm 252$</td>
<td style="text-align: center;">$249 \pm 80$</td>
<td style="text-align: center;">$75 \pm 3$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$77 \pm 64$</td>
<td style="text-align: center;">$331 \pm 64$</td>
<td style="text-align: center;">$-341 \pm 187$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$N v s H$</td>
<td style="text-align: center;">$3355 \pm 612$</td>
<td style="text-align: center;">$488 \pm 90$</td>
<td style="text-align: center;">$12 \pm 2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$371 \pm 305$</td>
<td style="text-align: center;">$-113 \pm 61$</td>
<td style="text-align: center;">$209 \pm 176$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Average prompting advantage in number of data points for MNLI \&amp; SuperGLUE tasks with maximum reporting.</p>
<p>Average Advantage (mean reporting)</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MNLI</th>
<th style="text-align: center;">BoolQ</th>
<th style="text-align: center;">CB</th>
<th style="text-align: center;">COPA</th>
<th style="text-align: center;">MultiRC*</th>
<th style="text-align: center;">RTE</th>
<th style="text-align: center;">WiC</th>
<th style="text-align: center;">WSC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$P v s H$</td>
<td style="text-align: center;">$4526 \pm 266$</td>
<td style="text-align: center;">$1139 \pm 285$</td>
<td style="text-align: center;">$81 \pm 2$</td>
<td style="text-align: center;">$292 \pm 212$</td>
<td style="text-align: center;">$399 \pm 77$</td>
<td style="text-align: center;">$72 \pm 71$</td>
<td style="text-align: center;">$447 \pm 190$</td>
<td style="text-align: center;">$-490 \pm 402$</td>
</tr>
<tr>
<td style="text-align: center;">$P v s N$</td>
<td style="text-align: center;">$185 \pm 307$</td>
<td style="text-align: center;">$514 \pm 287$</td>
<td style="text-align: center;">$80 \pm 2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$58 \pm 2$</td>
<td style="text-align: center;">$621 \pm 215$</td>
<td style="text-align: center;">$10 \pm 104$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$N v s H$</td>
<td style="text-align: center;">$4341 \pm 347$</td>
<td style="text-align: center;">$625 \pm 402$</td>
<td style="text-align: center;">$1 \pm 2$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$342 \pm 78$</td>
<td style="text-align: center;">$-549 \pm 203$</td>
<td style="text-align: center;">$437 \pm 189$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 4: Average prompting advantage in number of data points for MNLI \&amp; SuperGLUE tasks with mean reporting.</p>
<h1>C Curves on all tasks</h1>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Prompting vs head (classifier) performance across data scales, up to the full dataset, for seven SuperGLUE tasks \&amp; MNLI. Compares the best prompt and head performance at each level of training data across 4 runs. Highlighted region shows the accuracy difference of the models. Cross-hatch region highlights the lowestand highest- accuracy matched region in the curves. The highlighted area in this region is used to estimate the data advantage from prompting.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Median performance across runs for each prompt on every task.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ The correct answer here is, of course, yes. Originated in 1803 as Napoleon rose to power, this practice was mainly to the benefit of war widows.
${ }^{2}$ After experimenting with RoBERTa, AIBERT (Lan et al., 2019) and BERT (Devlin et al., 2018), we found roberta-large to have the most consistent performance.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
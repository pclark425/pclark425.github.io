<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1309 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1309</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1309</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-221971078</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2009.13303v2.pdf" target="_blank">Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey</a></p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning has recently seen huge success across multiple areas in the robotics domain. Owing to the limitations of gathering real-world data, i.e., sample inefficiency and the cost of collecting it, simulation environments are utilized for training the different agents. This not only aids in providing a potentially infinite data source, but also alleviates safety concerns with real robots. Nonetheless, the gap between the simulated and real worlds degrades the performance of the policies once the models are transferred into real robots. Multiple research efforts are therefore now being directed towards closing this sim-to-real gap and accomplish more efficient policy transfer. Recent years have seen the emergence of multiple methods applicable to different domains, but there is a lack, to the best of our knowledge, of a comprehensive review summarizing and putting into context the different methods. In this survey paper, we cover the fundamental background behind sim-to-real transfer in deep reinforcement learning and overview the main methods being utilized at the moment: domain randomization, domain adaptation, imitation learning, meta-learning and knowledge distillation. We categorize some of the most relevant recent works, and outline the main application scenarios. Finally, we discuss the main opportunities and challenges of the different approaches and point to the most promising directions.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1309.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1309.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AirSim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Airsim</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-fidelity visual and physical simulator for autonomous vehicles providing realistic rendering and physics to enable sim-to-real experiments for vehicle/robot agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Airsim: High-fidelity visual and physical simulation for autonomous vehicles.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>AirSim</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulator that provides high-fidelity visual rendering and physical dynamics aimed at autonomous vehicle research (visual + physics).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / vehicle dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>high-fidelity (visual and physical simulation as described by the AirSim reference)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Realistic photorealistic rendering and physics engine for vehicles; explicitly described as providing high-fidelity visual and physical simulation; no per-study numeric fidelity metrics provided in this survey.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world deployment (aim stated in survey: enable direct deployment with minimal mismatches)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Survey states higher realism in simulators (like AirSim) generally improves prospects for direct deployment, but gives no explicit minimal required features.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1309.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1309.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open urban driving simulator used for autonomous driving research with realistic urban scenes and sensors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Carla: An open urban driving simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>CARLA</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Open-source urban driving simulator providing realistic city environments, sensors and vehicle dynamics for autonomous driving research.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / autonomous driving</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high fidelity for urban driving scenarios (realistic urban visuals and vehicle dynamics as provided by CARLA)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes urban scene rendering, simulated sensors and vehicle dynamics; survey does not list numerical fidelity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world deployment (general aim)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Survey notes simulators like CARLA aim to reduce sim-to-real mismatch but gives no explicit minimal fidelity requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1309.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1309.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MuJoCo (Multi-Joint dynamics with Contact)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics engine designed for model-based control and robotics that provides relatively accurate dynamics simulation and fast execution for RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mujoco: A physics engine for model-based control.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Physics engine used for model-based control and reinforcement learning experiments; often used for dynamics-heavy robotics tasks requiring relatively accurate contact and multibody dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics / dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high fidelity for rigid-body dynamics and contact modelling (optimized for speed and accuracy in RL/control tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Accurate multibody dynamics, contact modelling and fast simulation suitable for control; survey notes MuJoCo integrates well with RL toolkits and is often used with rendering engines for visual sims (e.g., Ogre3D) but does not provide numeric fidelity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world robots or higher-fidelity simulation depending on the referenced study</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Survey mentions that more realistic simulators (e.g., MuJoCo combined with good rendering) help, but does not specify a minimal set of features required.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1309.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1309.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python module wrapping the Bullet physics engine for fast physics simulation targeted at games, robotics and ML experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pybullet, a python module for physics simulation for games, robotics and machine learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Physics simulator that enables fast training loops for reinforcement learning and robotics by simulating rigid-body dynamics and contacts; commonly used with RL libraries and gym environments.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics / dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity (trade-off between speed and physical realism; suitable for large-scale RL training)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Simulates rigid-body dynamics and contact; optimized for runtime speed; survey contrasts it with Gazebo (more integrated with ROS) and notes it typically enables faster training but may be less feature-complete for some robot stacks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world robots (as used in referenced sim-to-real works)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Survey suggests PyBullet/MuJoCo provide faster training while Gazebo provides tighter integration with ROS; no minimal fidelity specification.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1309.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1309.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source multi-robot simulator tightly integrated with ROS, used for simulating complex robotic systems and full robot stacks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Design and use paradigms for gazebo, an open-source multi-robot simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Simulator offering complex scenarios, sensor models and tight ROS integration, enabling simulation of full robot software stacks and perception pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>robotics / perception / dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high fidelity for full-system robotics stacks (good sensor/stack integration; may be slower than PyBullet/MuJoCo)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Integrated with ROS for realistic sensor/actuator pipelines and robot middleware; suited for complex scenarios though training speed may be slower than physics-only engines.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world robot stacks (via ROS integration)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Survey highlights Gazebo's advantage for system-in-the-loop simulations but does not give a minimum fidelity prescription.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1309.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1309.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Witman et al. (plasma jet)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study that applied sim-to-real RL to control thermal effects of an atmospheric pressure plasma jet; the survey lists it as an example of sim-to-real for a thermodynamics-related control problem.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo + Ogre3D (as reported in the survey's table)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Combination of MuJoCo physics engine for dynamics and Ogre3D for rendering used to simulate the plasma-jet control environment (as reported in the survey table).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>thermodynamics / thermal control (plasma jet thermal effects)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>not explicitly quantified; simulation aimed to model thermal control dynamics using MuJoCo (physics) and Ogre3D (rendering) — implies medium-fidelity dynamics plus visual rendering</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Survey lists MuJoCo+Ogre3D as simulator used; the paper is presented as a sim-to-real application for thermal control, but the survey does not provide detailed fidelity metrics (e.g., thermal conduction modelling accuracy) or time-step/resolution details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agent (survey entry indicates RL used for thermal control task; exact algorithm detail not provided in survey entry).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Control of thermal effects of an atmospheric pressure plasma jet (thermal-regulation/control task)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real-world plasma jet hardware (implied sim-to-real transfer intent in paper title and survey listing)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>No explicit statement in the survey about required minimum thermal-model fidelity for successful transfer in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1309.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1309.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Matas et al. (deformable objects)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real reinforcement learning for deformable object manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper applying domain randomization and stochastic grasping in simulation to train RL policies for deformable object manipulation and then transferring to real robots; survey highlights limitations in simulating object deformability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real reinforce- ment learning for deformable object manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PyBullet (as reported in the survey table)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>PyBullet-based simulation with stochastic grasping and domain randomization used to train manipulation policies for deformable objects.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / deformable object manipulation (materials / contact mechanics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>limited/medium-fidelity for deformable-object physics — survey explicitly notes inability to properly simulate degree of deformability as a drawback</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes randomized visual/physics parameters and stochastic grasping in simulation; however, survey reports simulator could not accurately model object deformability (stiffness mismatch) which impacted real-world transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning agent trained with domain randomization and stochastic grasping; specific RL algorithm in original paper not enumerated in the survey entry.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Dexterous manipulation of deformable objects (grasping/manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>real robot manipulation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Survey points out that accurate deformability modeling is important — lack of it caused performance issues on real hardware, indicating a non-trivial minimum fidelity for deformable properties.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Survey explicitly reports that inability to simulate degree of deformability caused failures: the real robot could not grasp stiffer objects that were mis-modeled in simulation; also notes that excessive visual randomization harmed performance in a related experiment.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1309.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e1309.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI dexterous in-hand</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning dexterous in-hand manipulation (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large-scale sim-to-real RL work that trained dexterous in-hand manipulation policies with extensive dynamics randomization and demonstrated successful sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dexterous in-hand manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>MuJoCo (implied in the survey reference to randomizing physical parameters in simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Physics-based simulation where many physical parameters (masses, friction, damping, actuator gains, object dimensions) were randomized to produce robust policies for a five-fingered hand.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / dexterous manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-to-high for rigid-body/contact dynamics; fidelity for contacts and actuators was augmented by randomized parameter distributions rather than perfect deterministic accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Simulation randomized object dimensions, masses, friction coefficients, joint damping, actuator gains etc. to cover real-world variability; survey reports success of this dynamics randomization for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Deep reinforcement learning agent for dexterous in-hand manipulation (large-scale RL with domain/dynamics randomization).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learn robust dexterous in-hand manipulation policies transferable to a physical five-fingered hand</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>physical five-fingered robot hand (real hardware)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>reported as successful sim-to-real transfer in the surveyed literature (qualitative success; no numeric metrics in survey)</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Survey highlights that dynamics randomization (broadening parameter distributions) can enable transfer even if exact simulator fidelity is imperfect; no strict minimal fidelity threshold is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet. <em>(Rating: 2)</em></li>
                <li>Sim-to-real reinforce- ment learning for deformable object manipulation. <em>(Rating: 2)</em></li>
                <li>Learning dexterous in-hand manipulation. <em>(Rating: 2)</em></li>
                <li>Airsim: High-fidelity visual and physical simulation for autonomous vehicles. <em>(Rating: 2)</em></li>
                <li>Mujoco: A physics engine for model-based control. <em>(Rating: 2)</em></li>
                <li>Pybullet, a python module for physics simulation for games, robotics and machine learning. <em>(Rating: 2)</em></li>
                <li>Design and use paradigms for gazebo, an open-source multi-robot simulator. <em>(Rating: 2)</em></li>
                <li>Carla: An open urban driving simulator. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1309",
    "paper_id": "paper-221971078",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "AirSim",
            "name_full": "Airsim",
            "brief_description": "A high-fidelity visual and physical simulator for autonomous vehicles providing realistic rendering and physics to enable sim-to-real experiments for vehicle/robot agents.",
            "citation_title": "Airsim: High-fidelity visual and physical simulation for autonomous vehicles.",
            "mention_or_use": "mention",
            "simulator_name": "AirSim",
            "simulator_description": "Simulator that provides high-fidelity visual rendering and physical dynamics aimed at autonomous vehicle research (visual + physics).",
            "scientific_domain": "robotics / vehicle dynamics",
            "fidelity_level": "high-fidelity (visual and physical simulation as described by the AirSim reference)",
            "fidelity_characteristics": "Realistic photorealistic rendering and physics engine for vehicles; explicitly described as providing high-fidelity visual and physical simulation; no per-study numeric fidelity metrics provided in this survey.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": "real-world deployment (aim stated in survey: enable direct deployment with minimal mismatches)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Survey states higher realism in simulators (like AirSim) generally improves prospects for direct deployment, but gives no explicit minimal required features.",
            "failure_cases": "",
            "uuid": "e1309.0"
        },
        {
            "name_short": "CARLA",
            "name_full": "CARLA",
            "brief_description": "Open urban driving simulator used for autonomous driving research with realistic urban scenes and sensors.",
            "citation_title": "Carla: An open urban driving simulator.",
            "mention_or_use": "mention",
            "simulator_name": "CARLA",
            "simulator_description": "Open-source urban driving simulator providing realistic city environments, sensors and vehicle dynamics for autonomous driving research.",
            "scientific_domain": "robotics / autonomous driving",
            "fidelity_level": "medium-to-high fidelity for urban driving scenarios (realistic urban visuals and vehicle dynamics as provided by CARLA)",
            "fidelity_characteristics": "Includes urban scene rendering, simulated sensors and vehicle dynamics; survey does not list numerical fidelity metrics.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": "real-world deployment (general aim)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Survey notes simulators like CARLA aim to reduce sim-to-real mismatch but gives no explicit minimal fidelity requirements.",
            "failure_cases": "",
            "uuid": "e1309.1"
        },
        {
            "name_short": "MuJoCo",
            "name_full": "MuJoCo (Multi-Joint dynamics with Contact)",
            "brief_description": "A physics engine designed for model-based control and robotics that provides relatively accurate dynamics simulation and fast execution for RL training.",
            "citation_title": "Mujoco: A physics engine for model-based control.",
            "mention_or_use": "mention",
            "simulator_name": "MuJoCo",
            "simulator_description": "Physics engine used for model-based control and reinforcement learning experiments; often used for dynamics-heavy robotics tasks requiring relatively accurate contact and multibody dynamics.",
            "scientific_domain": "mechanics / robotics / dynamics",
            "fidelity_level": "medium-to-high fidelity for rigid-body dynamics and contact modelling (optimized for speed and accuracy in RL/control tasks)",
            "fidelity_characteristics": "Accurate multibody dynamics, contact modelling and fast simulation suitable for control; survey notes MuJoCo integrates well with RL toolkits and is often used with rendering engines for visual sims (e.g., Ogre3D) but does not provide numeric fidelity metrics.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": "real-world robots or higher-fidelity simulation depending on the referenced study",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Survey mentions that more realistic simulators (e.g., MuJoCo combined with good rendering) help, but does not specify a minimal set of features required.",
            "failure_cases": "",
            "uuid": "e1309.2"
        },
        {
            "name_short": "PyBullet",
            "name_full": "PyBullet",
            "brief_description": "A Python module wrapping the Bullet physics engine for fast physics simulation targeted at games, robotics and ML experiments.",
            "citation_title": "Pybullet, a python module for physics simulation for games, robotics and machine learning.",
            "mention_or_use": "mention",
            "simulator_name": "PyBullet",
            "simulator_description": "Physics simulator that enables fast training loops for reinforcement learning and robotics by simulating rigid-body dynamics and contacts; commonly used with RL libraries and gym environments.",
            "scientific_domain": "mechanics / robotics / dynamics",
            "fidelity_level": "medium-fidelity (trade-off between speed and physical realism; suitable for large-scale RL training)",
            "fidelity_characteristics": "Simulates rigid-body dynamics and contact; optimized for runtime speed; survey contrasts it with Gazebo (more integrated with ROS) and notes it typically enables faster training but may be less feature-complete for some robot stacks.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": "real-world robots (as used in referenced sim-to-real works)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Survey suggests PyBullet/MuJoCo provide faster training while Gazebo provides tighter integration with ROS; no minimal fidelity specification.",
            "failure_cases": "",
            "uuid": "e1309.3"
        },
        {
            "name_short": "Gazebo",
            "name_full": "Gazebo",
            "brief_description": "An open-source multi-robot simulator tightly integrated with ROS, used for simulating complex robotic systems and full robot stacks.",
            "citation_title": "Design and use paradigms for gazebo, an open-source multi-robot simulator.",
            "mention_or_use": "mention",
            "simulator_name": "Gazebo",
            "simulator_description": "Simulator offering complex scenarios, sensor models and tight ROS integration, enabling simulation of full robot software stacks and perception pipelines.",
            "scientific_domain": "robotics / perception / dynamics",
            "fidelity_level": "medium-to-high fidelity for full-system robotics stacks (good sensor/stack integration; may be slower than PyBullet/MuJoCo)",
            "fidelity_characteristics": "Integrated with ROS for realistic sensor/actuator pipelines and robot middleware; suited for complex scenarios though training speed may be slower than physics-only engines.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": "real-world robot stacks (via ROS integration)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Survey highlights Gazebo's advantage for system-in-the-loop simulations but does not give a minimum fidelity prescription.",
            "failure_cases": "",
            "uuid": "e1309.4"
        },
        {
            "name_short": "Witman et al. (plasma jet)",
            "name_full": "Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet",
            "brief_description": "Study that applied sim-to-real RL to control thermal effects of an atmospheric pressure plasma jet; the survey lists it as an example of sim-to-real for a thermodynamics-related control problem.",
            "citation_title": "Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet.",
            "mention_or_use": "mention",
            "simulator_name": "MuJoCo + Ogre3D (as reported in the survey's table)",
            "simulator_description": "Combination of MuJoCo physics engine for dynamics and Ogre3D for rendering used to simulate the plasma-jet control environment (as reported in the survey table).",
            "scientific_domain": "thermodynamics / thermal control (plasma jet thermal effects)",
            "fidelity_level": "not explicitly quantified; simulation aimed to model thermal control dynamics using MuJoCo (physics) and Ogre3D (rendering) — implies medium-fidelity dynamics plus visual rendering",
            "fidelity_characteristics": "Survey lists MuJoCo+Ogre3D as simulator used; the paper is presented as a sim-to-real application for thermal control, but the survey does not provide detailed fidelity metrics (e.g., thermal conduction modelling accuracy) or time-step/resolution details.",
            "model_or_agent_name": null,
            "model_description": "Reinforcement learning agent (survey entry indicates RL used for thermal control task; exact algorithm detail not provided in survey entry).",
            "reasoning_task": "Control of thermal effects of an atmospheric pressure plasma jet (thermal-regulation/control task)",
            "training_performance": null,
            "transfer_target": "real-world plasma jet hardware (implied sim-to-real transfer intent in paper title and survey listing)",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "No explicit statement in the survey about required minimum thermal-model fidelity for successful transfer in this study.",
            "failure_cases": "",
            "uuid": "e1309.5"
        },
        {
            "name_short": "Matas et al. (deformable objects)",
            "name_full": "Sim-to-real reinforcement learning for deformable object manipulation",
            "brief_description": "Paper applying domain randomization and stochastic grasping in simulation to train RL policies for deformable object manipulation and then transferring to real robots; survey highlights limitations in simulating object deformability.",
            "citation_title": "Sim-to-real reinforce- ment learning for deformable object manipulation.",
            "mention_or_use": "mention",
            "simulator_name": "PyBullet (as reported in the survey table)",
            "simulator_description": "PyBullet-based simulation with stochastic grasping and domain randomization used to train manipulation policies for deformable objects.",
            "scientific_domain": "mechanics / deformable object manipulation (materials / contact mechanics)",
            "fidelity_level": "limited/medium-fidelity for deformable-object physics — survey explicitly notes inability to properly simulate degree of deformability as a drawback",
            "fidelity_characteristics": "Includes randomized visual/physics parameters and stochastic grasping in simulation; however, survey reports simulator could not accurately model object deformability (stiffness mismatch) which impacted real-world transfer.",
            "model_or_agent_name": null,
            "model_description": "Reinforcement learning agent trained with domain randomization and stochastic grasping; specific RL algorithm in original paper not enumerated in the survey entry.",
            "reasoning_task": "Dexterous manipulation of deformable objects (grasping/manipulation)",
            "training_performance": null,
            "transfer_target": "real robot manipulation tasks",
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Survey points out that accurate deformability modeling is important — lack of it caused performance issues on real hardware, indicating a non-trivial minimum fidelity for deformable properties.",
            "failure_cases": "Survey explicitly reports that inability to simulate degree of deformability caused failures: the real robot could not grasp stiffer objects that were mis-modeled in simulation; also notes that excessive visual randomization harmed performance in a related experiment.",
            "uuid": "e1309.6"
        },
        {
            "name_short": "OpenAI dexterous in-hand",
            "name_full": "Learning dexterous in-hand manipulation (OpenAI)",
            "brief_description": "Large-scale sim-to-real RL work that trained dexterous in-hand manipulation policies with extensive dynamics randomization and demonstrated successful sim-to-real transfer.",
            "citation_title": "Learning dexterous in-hand manipulation.",
            "mention_or_use": "mention",
            "simulator_name": "MuJoCo (implied in the survey reference to randomizing physical parameters in simulation)",
            "simulator_description": "Physics-based simulation where many physical parameters (masses, friction, damping, actuator gains, object dimensions) were randomized to produce robust policies for a five-fingered hand.",
            "scientific_domain": "mechanics / dexterous manipulation",
            "fidelity_level": "medium-to-high for rigid-body/contact dynamics; fidelity for contacts and actuators was augmented by randomized parameter distributions rather than perfect deterministic accuracy",
            "fidelity_characteristics": "Simulation randomized object dimensions, masses, friction coefficients, joint damping, actuator gains etc. to cover real-world variability; survey reports success of this dynamics randomization for transfer.",
            "model_or_agent_name": null,
            "model_description": "Deep reinforcement learning agent for dexterous in-hand manipulation (large-scale RL with domain/dynamics randomization).",
            "reasoning_task": "Learn robust dexterous in-hand manipulation policies transferable to a physical five-fingered hand",
            "training_performance": null,
            "transfer_target": "physical five-fingered robot hand (real hardware)",
            "transfer_performance": "reported as successful sim-to-real transfer in the surveyed literature (qualitative success; no numeric metrics in survey)",
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Survey highlights that dynamics randomization (broadening parameter distributions) can enable transfer even if exact simulator fidelity is imperfect; no strict minimal fidelity threshold is provided.",
            "failure_cases": "",
            "uuid": "e1309.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet.",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_reinforcement_learning_for_control_of_thermal_effects_of_an_atmospheric_pressure_plasma_jet"
        },
        {
            "paper_title": "Sim-to-real reinforce- ment learning for deformable object manipulation.",
            "rating": 2,
            "sanitized_title": "simtoreal_reinforce_ment_learning_for_deformable_object_manipulation"
        },
        {
            "paper_title": "Learning dexterous in-hand manipulation.",
            "rating": 2,
            "sanitized_title": "learning_dexterous_inhand_manipulation"
        },
        {
            "paper_title": "Airsim: High-fidelity visual and physical simulation for autonomous vehicles.",
            "rating": 2,
            "sanitized_title": "airsim_highfidelity_visual_and_physical_simulation_for_autonomous_vehicles"
        },
        {
            "paper_title": "Mujoco: A physics engine for model-based control.",
            "rating": 2,
            "sanitized_title": "mujoco_a_physics_engine_for_modelbased_control"
        },
        {
            "paper_title": "Pybullet, a python module for physics simulation for games, robotics and machine learning.",
            "rating": 2,
            "sanitized_title": "pybullet_a_python_module_for_physics_simulation_for_games_robotics_and_machine_learning"
        },
        {
            "paper_title": "Design and use paradigms for gazebo, an open-source multi-robot simulator.",
            "rating": 2,
            "sanitized_title": "design_and_use_paradigms_for_gazebo_an_opensource_multirobot_simulator"
        },
        {
            "paper_title": "Carla: An open urban driving simulator.",
            "rating": 2,
            "sanitized_title": "carla_an_open_urban_driving_simulator"
        }
    ],
    "cost": 0.016837249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey</p>
<p>Wenshuai Zhao 
Turku Intelligent Embedded and Robotic Systems Lab
University of Turku
Finland</p>
<p>Jorge Peña Queralta 
Turku Intelligent Embedded and Robotic Systems Lab
University of Turku
Finland</p>
<p>Tomi Westerlund 
Turku Intelligent Embedded and Robotic Systems Lab
University of Turku
Finland</p>
<p>Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: a Survey
Index Terms-Deep Reinforcement LearningRoboticsSim- to-RealTransfer LearningMeta LearningDomain Random- izationKnowledge DistillationImitation Learning
Deep reinforcement learning has recently seen huge success across multiple areas in the robotics domain. Owing to the limitations of gathering real-world data, i.e., sample inefficiency and the cost of collecting it, simulation environments are utilized for training the different agents. This not only aids in providing a potentially infinite data source, but also alleviates safety concerns with real robots. Nonetheless, the gap between the simulated and real worlds degrades the performance of the policies once the models are transferred into real robots. Multiple research efforts are therefore now being directed towards closing this sim-toreal gap and accomplish more efficient policy transfer. Recent years have seen the emergence of multiple methods applicable to different domains, but there is a lack, to the best of our knowledge, of a comprehensive review summarizing and putting into context the different methods. In this survey paper, we cover the fundamental background behind sim-to-real transfer in deep reinforcement learning and overview the main methods being utilized at the moment: domain randomization, domain adaptation, imitation learning, meta-learning and knowledge distillation. We categorize some of the most relevant recent works, and outline the main application scenarios. Finally, we discuss the main opportunities and challenges of the different approaches and point to the most promising directions.</p>
<p>I. INTRODUCTION</p>
<p>Reinforcement learning (RL) algorithms have been increasingly adopted by the robotics community over the past years to control complex robots or multi-robot systems [1], [2], or provide end-to-end policies from perception to control [3]. Inspired by the way we learn through trial-and-error processes, RL algorithms base their knowledge acquisition in the rewards that agents obtain when they act in certain manners given different experiences. This naturally requires a large number of episodes, and therefore the learning limitations in terms of time and experience variability in real-world scenarios is evident. Moreover, learning with real robots requires the consideration of potentially dangerous or unexpected behaviors in safety-critical applications [4]. Deep reinforcement learning (DRL) algorithms have been successfully deployed in various types of simulation environments, yet their success beyond simulated worlds has been limited. An exception to this is, however, robotic tasks involving object manipulation [5], [6]. In this survey, we review the most relevant works that try to answer a key research question in this direction: how to exploit One of the most common methods is domain randomization, through which different parameters of the simulator (e..g, colors, textures, dynamics) are randomized to produce more robust policies.</p>
<p>simulation-based training in real-world settings by transferring the knowledge and adapting the policies accordingly ( Fig. 1). Simulation-based training provides data at low-cost, but involves inherent mismatches with real-world settings. Bridging the gap between simulation and reality requires, first of all, methods that are able to account for mismatches in both sensing and actuation. The former aspect has been widely studied in recent years within the deep learning field, for instance with adversarial attacks on computer vision algorithms [7]. The latter risk can be minimized through more realistic simulation. In both of these cases, some of the current approaches include works that introduce perturbances in the environment [8] or focus on domain randomization [9]. Another key aspect to take into account is that an agent deployed in the real world will potentially be exposed to novel experiences that were not present in the simulations [10], as well as the potential need to adapt their policies to encompass wider sets of tasks. Some of the approaches to bridge the gap in this direction rely on meta learning [11] or continual learning [12], among others.</p>
<p>The methods described above focus on extracting knowledge from simulation-trained agents in order to deploy them in real-life scenarios. However, other approaches exist to the same end. In recent years, simulators have been progressing towards more realistic scenarios and physics engines: Airsim [13], CARLA [14], RotorS [15], [16], and others [17]. With some of these simulators, part of the aim is to be able to deploy the robotic agents directly into the real world by providing training data and experiences with minimal mismatches between real and simulated settings. Other research efforts have been directed towards increasing safety during training in real-settings. Safety is one of the main challenges towards achieving online training of complex agents in the real-world, from robot arms to self-driving cars [4]. In this direction, recent works have shown promising results towards safe DRL that is able to ensure convergence even while reducing the exploration space [3]. In this survey, we do not cover specific simulators or techniques for direct learning in real-world settings, but instead focus on describing the main methods for transferring knowledge learned in simulation towards their deployment in real robotic platforms. This is, to the best of our knowledge, the first survey that describes the different methods being utilized towards closing the simulation-to-reality gap in DRL for robotics. We also concentrate on describing the main application fields of current research efforts. We discuss recent works from a wider point of view by including related research directions in the areas of transfer learning and domain adaptation, knowledge distillation, and meta reinforcement learning. While other surveys have focused on transfer learning techniques [18] or safe reinforcement learning [4], we provide a different point of view with an emphasis on DRL policy transfer in the robotics domain. Finally, there is also a significant amount of publications deploying DRL policies on real robots. In this survey, nonetheless, we focus on those works that specifically tackle issues in sim-to-real transfer. The focus is mostly in end-to-end approaches, but we also describe relevant research where sim-to-real transfer techniques are applied to the sensing aspects of robotic operation, primarily the transfer of DL vision algorithms to real robots.</p>
<p>The rest of this paper is organized as follows. In Section II, we briefly introduce the main approaches to DRL, together with related research directions in knowledge distillation, transfer, adaptation and meta learning. Section III then delves into the different approaches being taken towards closing the simulation-to-reality gap, with Section IV focusing on the most relevant application areas. Then, we discuss open challenges and promising research directions in Section V. Finally, Section VI concludes this survey.</p>
<p>II. BACKGROUND</p>
<p>Sim-to-real is a very comprehensive concept and applied in many fields including robotics and classic machine vision tasks. Thereby quite a few methods and concepts intersect with this aim including transfer learning, robust RL, and meta learning. In this section, we briefly introduce the concepts of  deep reinforcement learning, knowledge distillation, transfer learning and domain adaption, before going into more details about sim-to-real transfer methods for DRL. The relationship between there concepts is illustrated in Fig. 2.</p>
<p>A. Deep Reinforcement Learning</p>
<p>A standard reinforcement learning (RL) task can be regarded as a sequential decision making setup which consists of an agent interacting with an environment in discrete steps. The agent takes an action a t at each timestep t, causing the environment to change its state from s t to s t+1 with a transition probability p(s t+1 |s t , a t ). This setup can be regarded as a Markov decision process (MDP) with a set of states s ∈ S, actions a ∈ A, transitions p ∈ P and rewards r ∈ R. Therefore we can define this MDP as a tuple (1).
D ≡ (S, A, P, R)(1)
The objective of reinforcement learning is to maximize the expected reward by choosing an optimal policy which will be represented via a deep neural network in DRL. Accelerated by modern computation capacity, DRL has shown significant success on various applications [1], [19], but particular in the simulated environment [20]. Therefore, how to transfer this success from simulation to reality is drawing more and more attention, which is also the motivation of this paper.</p>
<p>B. Sim-to-Real Transfer</p>
<p>Transferring DRL policies form simulation environments to reality is a necessary step towards more complex robotic systems that have DL-defined controllers. This, however, is not a problem specific to DRL algorithms, but ML in general. While most DRL algorithms provide end-to-end policies, i.e., control mechanisms that take raw sensor data as inputs and produce direct actuation commands as outputs, these two dimensions of robotics can be separated. Closing the gap between simulation and reality gap in terms of actuation requires simulators to be more accurate, and to account for variability in agent dynamics. On the sensing part, however, the problem can be considered wider, as it also involves the more general ML problem of facing situations in the real world that have not appeared in simulation [10]. In this paper, we focus mostly on end-to-end models, and overview both research directed towards system modeling and dynamics randomization, as well as research introducing randomization from the sensing point of view.</p>
<p>C. Transfer Learning and Domain Adaptation</p>
<p>Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains [18]. In this way, transfer learning can reduce the dependence of target domain data when constructing target learners.</p>
<p>Domain adaptation is a subset of transfer learning methods. It specifies the situation when we have sufficient source domain labeled data and the same single task as the target task, but without or very few target domain data. In sim-to-real robotics, researchers tend to employ a simulator to train the RL model and then deploy it in the realistic environment, where we should take advantage of the domain adaptation techniques in order to transfer the simulation based model well.</p>
<p>D. Knowledge Distillation</p>
<p>Large networks are typical in DRL with high-dimensional input data (e.g, complex visual tasks). Policy distillation is the process of extracting knowledge to train a new network that is able to maintain a similarly expert level while being significantly smaller and more efficient [21]. In these set-ups, the two networks are typically called teacher and student. The student is trained in a supervised manner with data generated by the teacher network. In [12], the authors presented Dis-CoRL, a modular, effective and scalable pipeline for continual DRL. DisCoRL has been succesfully applied to multiple tasks learned by different teachers, with their knowledge being distilled to a single student network.</p>
<p>E. Meta Reinforcement Learning</p>
<p>Meta Learning, namely learning to learn, aims to learn the adaptation ability to unseen test tasks from multiple training tasks. A good meta learning model should be trained across a variety of learning tasks and optimized for the best performance over a distribution of tasks, including potentially unseen tasks when tested. This spirit can be applied on both supervised learning and reinforcement learning, and in the latter case it is called meta reinforcement learning (MetaRL) [22].</p>
<p>The overall configuration of MetaRL is similar to an ordinary RL algorithm, except that MetaRL usually implements an LSTM policy and incorporates the last reward r t−1 and last action a t−1 into the current policy observation. In this case, the LSTM's hidden states serve as a memory for tracking characteristics of the trajectories. Therefore, MetaRL draws knowledge from past training.</p>
<p>F. Robust RL and Imitation Learning</p>
<p>Robust RL [23] was proposed quite early as a new RL paradigm that explicitly takes into account input disturbances as well as modeling errors. It considers a bad, or even adversarial model and tries to maximize the reward as a optimization problem [24], [25].</p>
<p>Imitation learning proposes to employ expert demonstration or trajectories instead of manually constructing a fixed reward function to train RL agents. The methods of imitation learning can be broadly classified into two key areas: behaviour cloning where an agent learns a mapping from observations to actions given demonstrations [26], [27] and inverse reinforcement learning where an agent attempts to estimate a reward function that describes the given demonstrations [28]. Because it aims to give a robust reward for RL agents, sometimes imitation learning can be utilized to obtain robust RL or sim-to-real transfer [29].</p>
<p>III. METHODOLOGIES FOR SIM-TO-REAL TRANSFER</p>
<p>Research in sim-to-real transfer has resulted in an increase of several orders of magnitude in the number of publications over the past few years. Multiple research directions have been followed, and we summarize in this section the most representative methods for sim-to-real transfer. Table I lists some of the most relevant and recent works in this field. The most widely used method for learning transfer is domain randomization, with other relevant examples including policy distillation, system identification, or meta-RL. The variability in terms of learning algorithms is higher, with DRL using proximal policy optimization (PPO) [45], trust region policy optimization (TRPO) [46], maximum aposteriori policy optimization (MPO) [47], asynchronous actor critic (A3C) methods [48], soft actor critic (SAC) [49], or deep deterministic policy gradient (DDPG) [50], among others.</p>
<p>A. Zero-shot Transfer</p>
<p>The most straightforward way of transferring knowledge from simulation to reality is to build a realistic simulator, or to have enough simulated experience, so that the model can be directly applied in real-world settings. This strategy is commonly referred to as zero-shot or direct transfer. System identification to build precise models of the real world and domain randomization are techniques that can be seen as one-shot transfer. We discuss both of these separately in Sections III-B and III-C.</p>
<p>B. System Identification</p>
<p>It is of note that simulators are not faithful representation of the real world. System identification [51] is exactly to build a precise mathematical model for a physical system and to make the simulator more realistic careful calibration is necessary. Nonetheless, challenges for obtaining a realistic enough simulator are still existing. For example, it is hard to build high-quality rendered image to simulate the real vision. Furthermore, many physical parameters of the same robot might vary significantly due to temperature, humidity, positioning or its wear-and-tear in time, which brings more difficulty for system identification.</p>
<p>C. Domain Randomization Methods</p>
<p>Domain randomization is the idea that [52], instead of carefully modeling all the parameters of the real world, we Traore et al. [12] Continual RL with policy distillation and sim-to-real transfer.</p>
<p>Continual learning with policy distillation.</p>
<p>PyBullet</p>
<p>Multi-task Distillation</p>
<p>PPO2</p>
<p>Small mobile platform</p>
<p>Robotic navigation</p>
<p>Kaspar et al. [31] Sim-to-real transfer for RL without Dynamics Randomization.</p>
<p>System identification and a highquality robot model.</p>
<p>PyBullet</p>
<p>SAC KUKA LBR iiwa +WSG50 gripper</p>
<p>Peg-in-Hole manipulation</p>
<p>Matas et al. [6] Sim-to-real RL for deformable object manipulation.</p>
<p>Stochastic grasping and domain radomization.</p>
<p>(sim)</p>
<p>PyBullet</p>
<p>DDPGfD 7DOF Kinova</p>
<p>Mico Arm</p>
<p>Dexterous manipulation</p>
<p>Witman et al. [32] Sim-to-real RL for thermal effects of an atmospheric pressure plasma jet. Van Baar et al. [35] Sim-to-real transfer with robustified policies for robot tasks.</p>
<p>Variation of appearance and/ or physics parameters.</p>
<p>(sim)</p>
<p>MuJoCo +Ogre 3D</p>
<p>A3C (sim) +Off-policy</p>
<p>Mitsubishi</p>
<p>Melfa RV-6SL</p>
<p>Marble Maze Manipulation</p>
<p>Bassani et al. [36] Sim2Real RL for robotic soccer competitions.</p>
<p>Domain adaptation and custom simulator for transfer.</p>
<p>VSSS-RL</p>
<p>DDPG /DQN</p>
<p>VSSS Robot</p>
<p>Robotic Navigation</p>
<p>Qin et al. [37] Sim2Real for six-legged robots with DRL and curriculum learning.</p>
<p>Curriculum learning with inverse kinematics.</p>
<p>V-Rep</p>
<p>PPO Six-legged robot Navigation and obstacle avoid.</p>
<p>Vacaro et al. [38]</p>
<p>Sim-to-real in reinforcement learning for everyone Domain randomization (light + color + textures).</p>
<p>(sim) could highly randomize the simulation in order to cover the real distribution of the real-world data despite the bias between the model and real world. Fig. 3a shows the paradigm of domain randomization. According to the components of the simulator randomized, we divide the methods of domain randomization into two kinds: visual randomization and dynamics randomization. In robotic vision tasks including object localization [53], object detection [54], pose estimation [55], and semantic segmentation [56], the training data from simulator always have different textures, lighting, and camera positions from the realistic environments. Therefore, visual domain randomization aims to provide enough simulated variability of the visual parameters at training time such that at test time the model is able to generalize to real-world data. In addition to adding randomization to the visual input, dynamics randomization could also help acquire a robust policy particularly where the controlling policy is needed. To learn dexterous in-hand manipulation policies for a physical five-fingered hand, [57] randomizes various physical parameters in the simulator, such as object dimensions, objects and robot link masses, surface friction coefficients, robot joint damping coefficients and actuator force gains. Their successful sim-to-real transfer experiments show the powerful effect of domain randomization.</p>
<p>Besides usually making the simulated data randomized to cover the real-world data distribution, [58] provides another interesting angle to apply domain randomization. They proposes to translate the randomized simulated image and realworld into the canonical sim images and demonstrate the effectiveness of this sim-to-real approach by training a visionbased closed-loop grasping RL agent in simulation.</p>
<p>D. Domain Adaptation Methods</p>
<p>Domain adaptation methods use data from source domain to improve the performance of a learned model on a different target domain where data is always less available. Since usually there are different feature spaces between the source domain and target domain, in order to better transfer the knowledge from source data, we should attempt to make these two feature space unified. This is the main spirit of domain adaptation, and can be described by the diagram in Fig. 3b.</p>
<p>The research of domain adaptation is broadly conducted recently in vision-based tasks, such as image classification and semantic segmentation [59], [60]. However, in this paper we focus on the tasks related with reinforcement learning and the ones applied to robotics. In these scenarios, the pure vision related tasks employing domain adaptation play as priors to the succeeding building reinforcement learning agents or other controlling tasks [58], [61], [29]. There is also some image-topolicy work using domain adaptation to generalize the policy learned by synthetic data or speed up the learning on realworld robots [61]. Sometimes domain adaptation is used to directly transfer the policy between agents [62].</p>
<p>Specifically, we now formalize the domain adaptation scenarios in a reinforcement learning setting [63]. Based on the definition of MDP in equation (1), we denote the source domain as D S ≡ (S S , A S , P S , R S ) and target domain as D T ≡ (S T , A T , P T , R T ), respectively. In reinforcement learning scenarios, the states S of the source and target domain can be quite different (S S = S T ) due to the perceptual-reality gap [64], while both domains share the action spaces and the transitions P (A S ≈ A T , P S ≈ P T )and their reward functions R have structural similarity (R S ≈ R T ).</p>
<p>From the literature, we summarize three common methods for domain adaptation regardless of their tasks. They are discrepancy-based, adversarial-based, and reconstructionbased methods, which can be also used crossly. Discrepancybased methods measure the feature distance between source and target domain by calculating pre-defined statistical metrics, in order to align their feature spaces [65], [66], [67]. Adversarial-based methods build a domain classifier to distinguish whether the features come from source domain or target domain. After being trained, the extractor could produce invariant feature from both source domain and target domain [68], [69], [70]. Reconstruction-based methods also aim to find the invariant or shared features between domains. However, they realize this goal by constructing one auxiliary reconstruction task and employ the shared feature to recover the original input [71]. In this way, the shared feature should be invariant and independent with the domains. These three methods provide different angles to make the features from different domains unified, and can be utilized in both vision tasks and RL-based control tasks.</p>
<p>E. Learning with Disturbances</p>
<p>Domain randomization and dynamics randomization methods focus on introducing perturbations in the simulation environments with the aim of making the agents less susceptible to the mismatches between simulation and reality [30], [38], [40]. The same conceptual idea has been extended in other works, where perturbances has been introduced to obtain more robust agents. For example, in [72], the authors consider noisy rewards. While not directly related to sim-to-real transfer, noisy rewards can better emulate real-world training of agents. Also, in some of our recent works [8], [73], we have considered environmental perturbations that affect differently different agents that are learning in parallel. This is an aspect that needs to be considered when multiple real agents are to be deployed or trained with a common policy.</p>
<p>F. Simulation Environments</p>
<p>A key aspect in sim-to-real transfer is the choice of simulation. Independently of the techniques utilized for efficiently transferring knowledge to real robots, the more realistic a simulation is the better results that can be expected. The most widely used simulators in the literature are Gazebo [74], Unity3D, and PyBullet [75] or MuJoCo [17]. Gazebo has the advantage of being widely integrated with the Robot Operating System (ROS) middleware, and therefore can be used together with part of the robotics stack that is present in real robots. PyBullet and MuJoCo, on the other hand, present wider integration with DL and RL libraries and gym environments. In  general, Gazebo suits more complex scenarios while PyBullet and MuJoCo provide faster training.</p>
<p>In those cases where system identification for one-shot transfer is the objective, researchers have often built or customized specific simulations that meet problem-specific requirements and constraints [32], [36], [41].</p>
<p>IV. APPLICATION SCENARIOS</p>
<p>Some of the most common applications for DRL in robotics are navigation and dexterous manipulation [1], [76]. Owing to the limited operational space in which most robotic arms operate, simulation environments for dexterous manipulation are relatively easier to generate than those for more complex robotic systems. For instance, the Open AI Gym [77], one of the most widely used frameworks for reinforcement learning, provides multiple environments for dexterous manipulation.</p>
<p>A. Dexterous Robotic Manipulation</p>
<p>Robotic manipulation tasks that have been possible with DRL ranging from learning peg-in-hole tasks [40] to deformable object manipulation [6], and including more dexterous manipulation with multi-fingered hands [5], or learning force control policies [78]. The latter is particularly relevant for sim-to-real: applying excessive force to real objects might cause damage, while grasping can fail with a lack of force.</p>
<p>In [6], Matas et al. utilize domain randomization for learning manipulation of deformable objects. The authors identify as one of the main drawbacks of the simulation environment the inability to properly simulate the degree of deformability of the objects, with the real robot being unable to grasp stiffer objects. Moreover, a relevant conclusion from this work is that excessive domain randomization can be detrimental. Specifically, when the number of different colors that were being used for each texture was too large, the performance of the real robot was significantly worse.</p>
<p>B. Robotic Navigation</p>
<p>While learning navigation with reinforcement learning has been a topic of increasing research interest over the past years [79], [80], the literature focusing on sim-to-real transfer methods is sparse. The first difference with respect to moreestablished research in learning manipulation is perhaps the lack of standard simulation environments. Owing to the more specific environment and sensor suites that are required for different navigation tasks, custom simulators have often been used [36], [37], or simulation worlds have been created using Unity, Unreal Engine, or Gazebo [39], [42].</p>
<p>Sim-to-real transfer for DRL policies can be applied to complex navigation tasks: from six-legged robots [37] to depth-based mapless navigation [39], including robots for soccer competitions [36]. In order to achieve a successful transfer to the real world, different methods have been applied in the literature. Of particular interest due to their potential and novelty are the following methods: curriculum learning [37], incremental environment complexity [39], and continual learning and policy distillation for multiple tasks [12].</p>
<p>C. Other Applications</p>
<p>Some other applications of DRL and sim-to-real transfer in robotics that have emerged over the past years are the control of a plasma jet [32], tactile sensing [43], or multiagent manipulation [44].</p>
<p>V. MAIN CHALLENGES AND FUTURE DIRECTIONS</p>
<p>Albeit the progress presented in the papers we reviewed, sim-to-real remains challenging based on existing methods. For domain randomization, researchers tend to study empirically examining which randomization to add, but it is hard to explain formally how and why it works, which thereby brings the difficulty of designing efficiently simulations and randomization distributions. For domain adaptation, most existing algorithms focus on homogeneous deep domain adaptation, which assumes that the feature spaces between the source and target domains are the same. However, this assumption may not be true in many applications. Thus we expect more exploration to transfer knowledge without this limitation.</p>
<p>Two of the most promising research directions are: (i) integration of different existing methods for more efficient transfer (e.g., domain randomization and domain adaptation); and (ii) incremental complexity learning, continual learning, and reward shaping for complex or multi-step tasks.</p>
<p>VI. CONCLUSION</p>
<p>Reinforcement learning algorithms often rely on simulated data to meet their need for vast amounts of labeled experiences. The mismatch between the simulation environments and real-world scenarios, however, requires further attention to be put to methods for sim-to-real transfer of the knowledge acquired in simulation. This is, to the best of our knowledge, the first survey that focuses on the different approaches being taken for sim-to-real transfer in DRL for robotics.</p>
<p>Domain randomization has been identified as the most widely adopted method for increasing the realism of simulation and better prepare for the real world. However, we have discussed alternative research directions showing promising results. For instance, policy distillation is enabling multi-task learning and more efficient and smaller networks, while metalearning methods allow for wider variability of tasks.</p>
<p>Multiple challenges remain in this field. While practical implementations show the efficiency of the different methods, wider theoretical and empirical studies are required to better understand the effect of these techniques in the learning process. Moreover, generalization of existing results with a more comprehensive analysis is also lacking in the literature.</p>
<p>Fig. 1 :
1Conceptual view of a simulation-to-reality transfer process.</p>
<p>Fig. 2 :
2Illustration of the different methods related to sim-to-real transfer in deep reinforcement learning and their relationships.</p>
<p>behind the domain adaptation paradigm.</p>
<p>Fig. 3 :
3Illustration of two of the most widely used methods for sim-to-real transfer in DRL. Domain randomization and domain adaptation are often applied as separate techniques, but they can also be applied together.</p>
<p>TABLE I :
IClassification of the most relevant publications in Sim2Real Transfer.Description </p>
<p>Sim-to-real transfer </p>
<p>Multi-agent </p>
<p>Simulator </p>
<p>Knowledge </p>
<p>Learning </p>
<p>Real </p>
<p>Application </p>
<p>and learning details </p>
<p>learning 
/ Engine </p>
<p>Transfer </p>
<p>Algorithm </p>
<p>Robot/Platform </p>
<p>Balaji et al. [30] </p>
<p>DeepRacer: </p>
<p>an </p>
<p>educational </p>
<p>autonomous racing platform. </p>
<p>Random colors and parallel do-
main randomization </p>
<p>(sim only) 
Distr. rollout </p>
<p>Gazebo 
RoboMaker </p>
<p>PPO </p>
<p>DeepRacer </p>
<p>4WD 1:18 Car </p>
<p>Autonomous 
racing </p>
<p>ACKNOWLEDGEMENTSThis work was supported by the Academy of Finland's AutoSOS project with grant number 328755.
Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony Bharath, arXiv:1708.05866A brief survey of deep reinforcement learning. Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil An- thony Bharath. A brief survey of deep reinforcement learning. arXiv:1708.05866, 2017.</p>
<p>Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications. Thanh Thi Nguyen, Ngoc Duy Nguyen, Saeid Nahavandi, IEEE transactions on cybernetics. Thanh Thi Nguyen, Ngoc Duy Nguyen, and Saeid Nahavandi. Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications. IEEE transactions on cybernetics, 2020.</p>
<p>End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. Richard Cheng, Gábor Orosz, M Richard, Joel W Murray, Burdick, AAAI Artificial Intelligence. 33Richard Cheng, Gábor Orosz, Richard M Murray, and Joel W Burdick. End-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks. In AAAI Artificial Intelligence, volume 33, 2019.</p>
<p>A comprehensive survey on safe reinforcement learning. Javier Garcıa, Fernando Fernández, Journal of Machine Learning Research. 161Javier Garcıa and Fernando Fernández. A comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1), 2015.</p>
<p>Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, Sergey Levine, arXiv:1709.10087Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. arXiv:1709.10087, 2017.</p>
<p>Sim-to-real reinforcement learning for deformable object manipulation. Jan Matas, Stephen James, Andrew J Davison, arXiv:1806.07851Jan Matas, Stephen James, and Andrew J Davison. Sim-to-real reinforce- ment learning for deformable object manipulation. arXiv:1806.07851, 2018.</p>
<p>Threat of adversarial attacks on deep learning in computer vision: A survey. Naveed Akhtar, Ajmal Mian, IEEE Access. 6Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision: A survey. IEEE Access, 6, 2018.</p>
<p>Towards closing the sim-to-real gap in collaborative multi-robot deep reinforcement learning. Wenshuai Zhao, Jorge Peña Queralta, Li Qingqing, Tomi Westerlund, In 5th ICRAEWenshuai Zhao, Jorge Peña Queralta, Li Qingqing, and Tomi Wester- lund. Towards closing the sim-to-real gap in collaborative multi-robot deep reinforcement learning. In 5th ICRAE, 2020.</p>
<p>Bayesian domain randomization for sim-to-real transfer. Fabio Muratore, Christian Eilers, Michael Gienger, Jan Peters, arXiv:2003.02471Fabio Muratore, Christian Eilers, Michael Gienger, and Jan Pe- ters. Bayesian domain randomization for sim-to-real transfer. arXiv:2003.02471, 2020.</p>
<p>Blind spot detection for safe sim-to-real transfer. Ramya Ramakrishnan, Ece Kamar, Debadeepta Dey, Eric Horvitz, Julie Shah, Journal of Artificial Intelligence Research. 67Ramya Ramakrishnan, Ece Kamar, Debadeepta Dey, Eric Horvitz, and Julie Shah. Blind spot detection for safe sim-to-real transfer. Journal of Artificial Intelligence Research, 67, 2020.</p>
<p>Meta reinforcement learning for sim-to-real domain adaptation. Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, Ville Kyrki, arXiv:1909.12906Karol Arndt, Murtaza Hazara, Ali Ghadirzadeh, and Ville Kyrki. Meta reinforcement learning for sim-to-real domain adaptation. arXiv:1909.12906, 2019.</p>
<p>Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer. René Traoré, Hugo Caselles-Dupré, Timothée Lesort, Te Sun, Natalia Díaz-Rodríguez, David Filliat, arXiv:1906.04452René Traoré, Hugo Caselles-Dupré, Timothée Lesort, Te Sun, Natalia Díaz-Rodríguez, and David Filliat. Continual reinforcement learning deployed in real-life using policy distillation and sim2real transfer. arXiv:1906.04452, 2019.</p>
<p>Airsim: High-fidelity visual and physical simulation for autonomous vehicles. Shital Shah, Debadeepta Dey, Chris Lovett, Ashish Kapoor, Field and service robotics. Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. Airsim: High-fidelity visual and physical simulation for autonomous vehicles. In Field and service robotics, 2018.</p>
<p>Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, Carla, arXiv:1711.03938An open urban driving simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. arXiv:1711.03938, 2017.</p>
<p>Rotors-a modular gazebo mav simulator framework. Fadri Furrer, Michael Burri, Markus Achtelik, Roland Siegwart, Robot Operating System (ROS). Fadri Furrer, Michael Burri, Markus Achtelik, and Roland Siegwart. Ro- tors-a modular gazebo mav simulator framework. In Robot Operating System (ROS). 2016.</p>
<p>Distributed progressive formation control for multi-agent systems: 2d and 3d deployment of uavs in ros/gazebo with rotors. Cassandra Mccord, Jorge Peña Queralta, Tuan Nguyen Gia, Tomi Westerlund, ECMR. Cassandra McCord, Jorge Peña Queralta, Tuan Nguyen Gia, and Tomi Westerlund. Distributed progressive formation control for multi-agent systems: 2d and 3d deployment of uavs in ros/gazebo with rotors. In ECMR, 2019.</p>
<p>Mujoco: A physics engine for model-based control. Emanuel Todorov, Tom Erez, Yuval Tassa, IROS. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In IROS, 2012.</p>
<p>A comprehensive survey on transfer learning. Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, Qing He, Proceedings of the IEEE. the IEEEFuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 2020.</p>
<p>Collaborative multi-robot systems for search and rescue: Coordination and perception. Jorge Peña Queralta, Jussi Taipalmaa, Bilge Can Pullinen, Victor Kathan, Tuan Sarker, Hannu Nguyen Gia, Moncef Tenhunen, Jenni Gabbouj, Tomi Raitoharju, Westerlund, arXiv:2008.12610arXiv preprintJorge Peña Queralta, Jussi Taipalmaa, Bilge Can Pullinen, Victor Kathan Sarker, Tuan Nguyen Gia, Hannu Tenhunen, Moncef Gabbouj, Jenni Raitoharju, and Tomi Westerlund. Collaborative multi-robot systems for search and rescue: Coordination and perception. arXiv preprint arXiv:2008.12610, 2020.</p>
<p>. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, D Przemysław, Christy Ebiak, David Dennison, Quirin Farhi, Shariq Fischer, Chris Hashme, Hesse, arXiv:1912.06680et al. Dota 2 with large scale deep reinforcement learningChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysław D ebiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv:1912.06680, 2019.</p>
<p>Sergio Gomez Andrei A Rusu, Caglar Colmenarejo, Guillaume Gulcehre, James Desjardins, Razvan Kirkpatrick, Pascanu, arXiv:1511.06295Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. Andrei A Rusu, Sergio Gomez Colmenarejo, Caglar Gulcehre, Guil- laume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, and Raia Hadsell. Policy distillation. arXiv:1511.06295, 2015.</p>
<p>X Jane, Zeb Wang, Dhruva Kurth-Nelson, Hubert Tirumala, Joel Z Soyer, Remi Leibo, Munos, arXiv:1611.05763Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn. arXiv:1611.05763, 2016.</p>
<p>Robust reinforcement learning. Jun Morimoto, Kenji Doya, Neural computation. 172Jun Morimoto and Kenji Doya. Robust reinforcement learning. Neural computation, 17(2), 2005.</p>
<p>Action robust reinforcement learning and applications in continuous control. Chen Tessler, Yonathan Efroni, Shie Mannor, arXiv:1901.09184Chen Tessler, Yonathan Efroni, and Shie Mannor. Action ro- bust reinforcement learning and applications in continuous control. arXiv:1901.09184, 2019.</p>
<p>Robust reinforcement learning for continuous control with model misspecification. J Daniel, Nir Mankowitz, Rae Levine, Yuanyuan Jeong, Jackie Shi, Abbas Kay, Jost Tobias Abdolmaleki, Timothy Springenberg, Todd Mann, Martin Hester, Riedmiller, arXiv:1906.07516Daniel J Mankowitz, Nir Levine, Rae Jeong, Yuanyuan Shi, Jackie Kay, Abbas Abdolmaleki, Jost Tobias Springenberg, Timothy Mann, Todd Hester, and Martin Riedmiller. Robust reinforcement learning for continuous control with model misspecification. arXiv:1906.07516, 2019.</p>
<p>Alvinn: An autonomous land vehicle in a neural network. A Dean, Pomerleau, Advances in neural information processing systems. Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. In Advances in neural information processing systems, 1989.</p>
<p>A reduction of imitation learning and structured prediction to no-regret online learning. Stéphane Ross, Geoffrey Gordon, Drew Bagnell, AISTATS. Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In AISTATS, 2011.</p>
<p>Algorithms for inverse reinforcement learning. Y Andrew, Stuart J Ng, Russell, Icml. 1Andrew Y Ng, Stuart J Russell, et al. Algorithms for inverse reinforce- ment learning. In Icml, volume 1, 2000.</p>
<p>Sim-toreal transfer of accurate grasping with eye-in-hand observations and continuous control. Mengyuan Yan, Iuri Frosio, Stephen Tyree, Jan Kautz, arXiv:1712.03303Mengyuan Yan, Iuri Frosio, Stephen Tyree, and Jan Kautz. Sim-to- real transfer of accurate grasping with eye-in-hand observations and continuous control. arXiv:1712.03303, 2017.</p>
<p>Bharathan Balaji, Sunil Mallya, Sahika Genc, Saurabh Gupta, Leo Dirac, Vineet Khare, Gourav Roy, Tao Sun, Yunzhe Tao, Brian Townsend, arXiv:1911.01562Educational autonomous racing platform for experimentation with sim2real reinforcement learning. Bharathan Balaji, Sunil Mallya, Sahika Genc, Saurabh Gupta, Leo Dirac, Vineet Khare, Gourav Roy, Tao Sun, Yunzhe Tao, Brian Townsend, et al. Deepracer: Educational autonomous racing platform for experimentation with sim2real reinforcement learning. arXiv:1911.01562, 2019.</p>
<p>Manuel Kaspar, Juan David Munoz Osorio, Jürgen Bock, arXiv:2002.11635Sim2real transfer for reinforcement learning without dynamics randomization. Manuel Kaspar, Juan David Munoz Osorio, and Jürgen Bock. Sim2real transfer for reinforcement learning without dynamics randomization. arXiv:2002.11635, 2020.</p>
<p>Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet. Matthew Witman, Dogan Gidon, B David, Berend Graves, Ali Smit, Mesbah, Plasma Sources Science and Technology. 289Matthew Witman, Dogan Gidon, David B Graves, Berend Smit, and Ali Mesbah. Sim-to-real transfer reinforcement learning for control of thermal effects of an atmospheric pressure plasma jet. Plasma Sources Science and Technology, 28(9), 2019.</p>
<p>Modelling generalized forces with reinforcement learning for simto-real transfer. Rae Jeong, Jackie Kay, Francesco Romano, Thomas Lampe, Tom Rothorl, Abbas Abdolmaleki, Tom Erez, Yuval Tassa, Francesco Nori, arXiv:1910.09471Rae Jeong, Jackie Kay, Francesco Romano, Thomas Lampe, Tom Rothorl, Abbas Abdolmaleki, Tom Erez, Yuval Tassa, and Francesco Nori. Modelling generalized forces with reinforcement learning for sim- to-real transfer. arXiv:1910.09471, 2019.</p>
<p>Flexible robotic grasping with sim-to-real transfer based reinforcement learning. Michel Breyer, Fadri Furrer, Tonci Novkovic, Roland Siegwart, Juan Nieto, ArXiv e-printsMichel Breyer, Fadri Furrer, Tonci Novkovic, Roland Siegwart, and Juan Nieto. Flexible robotic grasping with sim-to-real transfer based reinforcement learning. ArXiv e-prints, 2018.</p>
<p>Simulation to real transfer learning with robustified policies for robot tasks. R Van Baar, Corcodel, Sullivan, Jha, D Romeres, Nikovski, J van Baar, R Corcodel, A Sullivan, D Jha, D Romeres, and D Nikovski. Simulation to real transfer learning with robustified policies for robot tasks. 2018.</p>
<p>Learning to play soccer by reinforcement and applying sim-to-real to compete in the real world. F Hansenclever, Renie A Bassani, Jose Delgado, Nilton De O Lima Junior, R Heitor, Medeiros, H M Pedro, Alain Braga, Tapp, arXiv:2003.11102Hansenclever F Bassani, Renie A Delgado, Jose Nilton de O Lima Junior, Heitor R Medeiros, Pedro HM Braga, and Alain Tapp. Learning to play soccer by reinforcement and applying sim-to-real to compete in the real world. arXiv:2003.11102, 2020.</p>
<p>Sim-to-real: Six-legged robot control with deep reinforcement learning and curriculum learning. Yue Bangyu Qin, Yi Gao, Bai, ICRAE. Bangyu Qin, Yue Gao, and Yi Bai. Sim-to-real: Six-legged robot control with deep reinforcement learning and curriculum learning. In ICRAE, 2019.</p>
<p>Sim-to-real in reinforcement learning for everyone. Juliano Vacaro, Guilherme Marques, Bruna Oliveira, Gabriel Paz, Thomas Paula, Wagston Staehler, David Murphy, LARS-SBR-WREJuliano Vacaro, Guilherme Marques, Bruna Oliveira, Gabriel Paz, Thomas Paula, Wagston Staehler, and David Murphy. Sim-to-real in reinforcement learning for everyone. In LARS-SBR-WRE, 2019.</p>
<p>Sim-to-real transfer with incremental environment complexity for reinforcement learning of depth-based robot navigation. Thomas Chaffre, Julien Moras, Adrien Chan-Hon-Tong, Julien Marzat, arXiv:2004.14684Thomas Chaffre, Julien Moras, Adrien Chan-Hon-Tong, and Julien Marzat. Sim-to-real transfer with incremental environment com- plexity for reinforcement learning of depth-based robot navigation. arXiv:2004.14684, 2020.</p>
<p>Reinforcement learning with cartesian commands and sim to real transfer for peg in hole tasks. Manuel Kaspar, Jürgen Bock, Manuel Kaspar and Jürgen Bock. Reinforcement learning with cartesian commands and sim to real transfer for peg in hole tasks.</p>
<p>Andrew Hundt, Benjamin Killeen, Heeyeon Kwon, Chris Paxton, Gregory D Hager, arXiv:1909.11730Efficient reinforcement learning for multi-step visual tasks via reward shaping. Andrew Hundt, Benjamin Killeen, Heeyeon Kwon, Chris Paxton, and Gregory D Hager. " good robot!": Efficient reinforcement learning for multi-step visual tasks via reward shaping. arXiv:1909.11730, 2019.</p>
<p>Sim-to-real transfer of robotic gripper pose estimation-using deep reinforcement learning, generative adversarial networks, and visual servoing. Ole-Magnus Pedersen, NTNUMaster's thesisOle-Magnus Pedersen. Sim-to-real transfer of robotic gripper pose estimation-using deep reinforcement learning, generative adversarial networks, and visual servoing. Master's thesis, NTNU, 2019.</p>
<p>Sim-to-real transfer for optical tactile sensing. Zihan Ding, F Nathan, Edward Lepora, Johns, arXiv:2004.00136Zihan Ding, Nathan F Lepora, and Edward Johns. Sim-to-real transfer for optical tactile sensing. arXiv:2004.00136, 2020.</p>
<p>Multi-agent manipulation via locomotion using hierarchical sim2real. Ofir Nachum, Michael Ahn, Hugo Ponte, Shixiang Gu, Vikash Kumar, arXiv:1908.05224Ofir Nachum, Michael Ahn, Hugo Ponte, Shixiang Gu, and Vikash Kumar. Multi-agent manipulation via locomotion using hierarchical sim2real. arXiv:1908.05224, 2019.</p>
<p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv:1707.06347, 2017.</p>
<p>Trust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, ICML. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In ICML, 2015.</p>
<p>Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, arXiv:1806.06920Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv:1806.06920, 2018.</p>
<p>Asynchronous methods for deep reinforcement learning. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu, ICML. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In ICML, 2016.</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, arXiv:1801.01290Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv:1801.01290, 2018.</p>
<p>P Timothy, Jonathan J Lillicrap, Alexander Hunt, Nicolas Pritzel, Tom Heess, Yuval Erez, David Tassa, Daan Silver, Wierstra, arXiv:1509.02971Continuous control with deep reinforcement learning. Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv:1509.02971, 2015.</p>
<p>System identification and control using genetic algorithms. Kristinn Kristinsson, Guy Albert Dumont, IEEE Transactions on Systems, Man, and Cybernetics. 225Kristinn Kristinsson and Guy Albert Dumont. System identification and control using genetic algorithms. IEEE Transactions on Systems, Man, and Cybernetics, 22(5), 1992.</p>
<p>Real-World Robotic Perception and Control Using Synthetic Data. P Joshua, Tobin, UC BerkeleyPhD thesisJoshua P Tobin. Real-World Robotic Perception and Control Using Synthetic Data. PhD thesis, UC Berkeley, 2019.</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, Pieter Abbeel, IROS. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In IROS, 2017.</p>
<p>Training deep networks with synthetic data: Bridging the reality gap by domain randomization. Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, Stan Birchfield, CVPR Workshops. Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield. Training deep networks with synthetic data: Bridging the reality gap by domain randomization. In CVPR Workshops, 2018.</p>
<p>Implicit 3d orientation learning for 6d object detection from rgb images. Martin Sundermeyer, Maximilian Zoltan-Csaba Marton, Manuel Durner, Rudolph Brucker, Triebel, ECCV. Martin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3d orientation learning for 6d object detection from rgb images. In ECCV, 2018.</p>
<p>Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data. Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, Boqing Gong, ICCV. Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni- Vincentelli, Kurt Keutzer, and Boqing Gong. Domain randomization and pyramid consistency: Simulation-to-real generalization without ac- cessing target domain data. In ICCV, 2019.</p>
<p>Learning dexterous in-hand manipulation. Bowen Openai: Marcin Andrychowicz, Maciek Baker, Rafal Chociej, Bob Jozefowicz, Jakub Mcgrew, Arthur Pachocki, Matthias Petron, Glenn Plappert, Alex Powell, Ray, The International Journal of Robotics Research. 3912020OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1), 2020.</p>
<p>Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, Konstantinos Bousmalis, CVPR. Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalash- nikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, and Konstantinos Bousmalis. Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks. In CVPR, 2019.</p>
<p>Deep visual domain adaptation: A survey. Mei Wang, Weihong Deng, Neurocomputing. 312Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312, 2018.</p>
<p>Cycada: Cycle-consistent adversarial domain adaptation. Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, Trevor Darrell, ICML. Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, 2018.</p>
<p>Using simulation and domain adaptation to improve efficiency of deep robotic grasping. Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, ICRA. Konstantinos Bousmalis, Alex Irpan, Paul Wohlhart, Yunfei Bai, Matthew Kelcey, Mrinal Kalakrishnan, Laura Downs, Julian Ibarz, Peter Pastor, Kurt Konolige, et al. Using simulation and domain adaptation to improve efficiency of deep robotic grasping. In ICRA, 2018.</p>
<p>Learning invariant feature spaces to transfer skills with reinforcement learning. Abhishek Gupta, Coline Devin, Yuxuan Liu, Pieter Abbeel, Sergey Levine, arXiv:1703.02949Abhishek Gupta, Coline Devin, YuXuan Liu, Pieter Abbeel, and Sergey Levine. Learning invariant feature spaces to transfer skills with rein- forcement learning. arXiv:1703.02949, 2017.</p>
<p>Darla: Improving zero-shot transfer in reinforcement learning. Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, P Christopher, Alexander Burgess, Matthew Pritzel, Charles Botvinick, Alexander Blundell, Lerchner, arXiv:1707.08475Irina Higgins, Arka Pal, Andrei A Rusu, Loic Matthey, Christopher P Burgess, Alexander Pritzel, Matthew Botvinick, Charles Blundell, and Alexander Lerchner. Darla: Improving zero-shot transfer in reinforce- ment learning. arXiv:1707.08475, 2017.</p>
<p>Sim-to-real robot learning from pixels with progressive nets. Matej Andrei A Rusu, Thomas Večerík, Nicolas Rothörl, Razvan Heess, Raia Pascanu, Hadsell, Conference on Robot Learning. Andrei A Rusu, Matej Večerík, Thomas Rothörl, Nicolas Heess, Razvan Pascanu, and Raia Hadsell. Sim-to-real robot learning from pixels with progressive nets. In Conference on Robot Learning, 2017.</p>
<p>Deep domain confusion: Maximizing for domain invariance. Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, Trevor Darrell, arXiv:1412.3474Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv:1412.3474, 2014.</p>
<p>Learning transferable features with deep adaptation networks. Mingsheng Long, Yue Cao, Jianmin Wang, Michael Jordan, ICML. Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learn- ing transferable features with deep adaptation networks. In ICML, 2015.</p>
<p>Return of frustratingly easy domain adaptation. Baochen Sun, Jiashi Feng, Kate Saenko, arXiv:1511.05547Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. arXiv:1511.05547, 2015.</p>
<p>Domain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, Victor Lempitsky, The Journal of Machine Learning Research. 171Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1), 2016.</p>
<p>Simultaneous deep transfer across domains and tasks. Eric Tzeng, Judy Hoffman, Trevor Darrell, Kate Saenko, ICCV. Eric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simulta- neous deep transfer across domains and tasks. In ICCV, 2015.</p>
<p>Unsupervised pixel-level domain adaptation with generative adversarial networks. Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan, CVPR. Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In CVPR, 2017.</p>
<p>Domain separation networks. Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, Dumitru Erhan, Advances in neural information processing systems. Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. Domain separation networks. In Advances in neural information processing systems, 2016.</p>
<p>Reinforcement learning with perturbed rewards. Jingkang Wang, Yang Liu, Bo Li, AAAI. Jingkang Wang, Yang Liu, and Bo Li. Reinforcement learning with perturbed rewards. In AAAI, 2020.</p>
<p>Ubiquitous distributed deep reinforcement learning at the edge: Analyzing byzantine agents in discrete action spaces. Wenshuai Zhao, Jorge Peña Queralta, Li Qingqing, Tomi Westerlund, The 11th International Conference on Emerging Ubiquitous Systems and Pervasive Networks. 2020EUSPN 2020Wenshuai Zhao, Jorge Peña Queralta, Li Qingqing, and Tomi West- erlund. Ubiquitous distributed deep reinforcement learning at the edge: Analyzing byzantine agents in discrete action spaces. In The 11th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2020), 2020.</p>
<p>Design and use paradigms for gazebo, an open-source multi-robot simulator. Nathan Koenig, Andrew Howard, IROS. 3Nathan Koenig and Andrew Howard. Design and use paradigms for gazebo, an open-source multi-robot simulator. In IROS, volume 3, 2004.</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. Erwin Coumans, Yunfei Bai, Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games, robotics and machine learning. 2016.</p>
<p>Reinforcement learning in robotics: A survey. J Kober, The International Journal of Robotics Research. 3211J. Kober et al. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research, 32(11), 2013.</p>
<p>Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, arXiv:1606.01540Jie Tang, and Wojciech Zaremba. Openai gym. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv:1606.01540, 2016.</p>
<p>Learning force control policies for compliant manipulation. M Kalakrishnan, IROS. M. Kalakrishnan et al. Learning force control policies for compliant manipulation. In IROS, 2011.</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, J Joseph, Abhinav Lim, Li Gupta, Ali Fei-Fei, Farhadi, ICRA. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, 2017.</p>
<p>A survey on visual navigation for artificial agents with deep reinforcement learning. Fanyu Zeng, Chen Wang, Shuzhi Sam Ge, IEEE Access. 8Fanyu Zeng, Chen Wang, and Shuzhi Sam Ge. A survey on visual navigation for artificial agents with deep reinforcement learning. IEEE Access, 8, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>